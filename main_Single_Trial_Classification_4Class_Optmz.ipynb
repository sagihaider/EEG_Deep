{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "main_Single_Trial_Classification_4Class_Optmz.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sagihaider/EEG_Deep/blob/master/main_Single_Trial_Classification_4Class_Optmz.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X110PE9MjrlE",
        "colab_type": "code",
        "outputId": "9e825c0c-429b-4f48-8f7e-87a62560383c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        }
      },
      "source": [
        "!git clone https://github.com/sagihaider/EEG_Deep.git"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'EEG_Deep'...\n",
            "remote: Enumerating objects: 64, done.\u001b[K\n",
            "remote: Counting objects: 100% (64/64), done.\u001b[K\n",
            "remote: Compressing objects: 100% (62/62), done.\u001b[K\n",
            "remote: Total 117 (delta 27), reused 6 (delta 2), pack-reused 53\u001b[K\n",
            "Receiving objects: 100% (117/117), 600.02 MiB | 15.36 MiB/s, done.\n",
            "Resolving deltas: 100% (41/41), done.\n",
            "Checking out files: 100% (33/33), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ioLzMoiS60Qf",
        "colab_type": "code",
        "outputId": "3d7e9a6f-babf-4f40-f492-3b1dcfe8a784",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 65
        }
      },
      "source": [
        "import scipy.io as spio\n",
        "import numpy as np\n",
        "from importlib.machinery import SourceFileLoader\n",
        "\n",
        "# EEGNet-specific imports\n",
        "from EEG_Deep.EEGModelsHR import EEGNet, ShallowConvNet, DeepConvNet\n",
        "from tensorflow.keras import utils as np_utils\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras import backend as K\n",
        "K.set_image_data_format('channels_first')\n",
        "\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# tools for plotting confusion matrices\n",
        "from matplotlib import pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZjtnE_6RSpum",
        "colab_type": "code",
        "outputId": "423ae0e2-16af-4151-ee0a-209a3653ca86",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "x= 1         \n",
        "fName = 'EEG_Deep/Data2A/Data_A0' + str(x) + 'T.mat'  # Load Data\n",
        "print(fName)\n",
        "mat = spio.loadmat(fName)\n",
        "X_tr = mat['cleanRawEEGData']\n",
        "y_tr = mat['cleanClassLabels']"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "EEG_Deep/Data2A/Data_A01T.mat\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3c11RQCoS80S",
        "colab_type": "code",
        "outputId": "99c3743b-72ee-4755-916c-e4f5897c35b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(np.shape(X_tr))\n",
        "print(np.shape(y_tr))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(288, 22, 1875)\n",
            "(288, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tgWnQSY5T4op",
        "colab_type": "code",
        "outputId": "144f1b19-683f-42a9-fb2b-bd75d2b21e03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "\n",
        "# take 50/25/25 percent of the data to train/validate/test\n",
        "X_train      = X_tr[0:240,:,500:1250]\n",
        "Y_train      = y_tr[0:240]\n",
        "#X_validate   = X[151:200,:,500:1250]\n",
        "#Y_validate   = y[151:200]\n",
        "X_val       = X_tr[241:,:,500:1250]\n",
        "Y_val       = y_tr[241:]\n",
        "\n",
        "print(np.shape(X_train))\n",
        "print(np.shape(Y_train))\n",
        "# print(np.shape(X_validate))\n",
        "# print(np.shape(Y_validate))\n",
        "print(np.shape(X_val))\n",
        "print(np.shape(Y_val))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(240, 22, 750)\n",
            "(240, 1)\n",
            "(47, 22, 750)\n",
            "(47, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bfC8z4I-UnF6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# convert labels to one-hot encodings.\n",
        "Y_train      = np_utils.to_categorical(Y_train-1)\n",
        "# Y_validate   = np_utils.to_categorical(Y_validate-1)\n",
        "Y_val       = np_utils.to_categorical(Y_val-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NqmsOU1BXOZJ",
        "colab_type": "code",
        "outputId": "94f8bc37-489c-4cd7-ebe9-e6987715552d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "kernels, chans, samples = 1, 22, 750\n",
        "# convert data to NCHW (trials, kernels, channels, samples) format. Data \n",
        "# contains 22 channels and 500 time-points. Set the number of kernels to 1.\n",
        "X_train      = X_train.reshape(X_train.shape[0], kernels, chans, samples)\n",
        "X_val       = X_val.reshape(X_val.shape[0], kernels, chans, samples)\n",
        "   \n",
        "print('X_train shape:', X_train.shape)\n",
        "print(X_train.shape[0], 'train samples')\n",
        "print(X_val.shape[0], 'val samples')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train shape: (240, 1, 22, 750)\n",
            "240 train samples\n",
            "47 val samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvVfScxP1juN",
        "colab_type": "code",
        "outputId": "03bfdd64-d5f9-4210-9fa3-3e77d6284748",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Load test data\n",
        "x= 1         \n",
        "fName = 'EEG_Deep/Data2A/Data_A0' + str(x) + 'E.mat'  # Load Data\n",
        "print(fName)\n",
        "mat = spio.loadmat(fName)\n",
        "X_ts = mat['cleanRawEEGData']\n",
        "y_ts = mat['cleanClassLabels']"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "EEG_Deep/Data2A/Data_A01E.mat\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XdP4eTcM1wN3",
        "colab_type": "code",
        "outputId": "ee924ec6-8116-4df6-a5ee-4f5022d3f208",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "print(np.shape(X_ts))\n",
        "print(np.shape(y_ts))\n",
        "\n",
        "# take 50/25/25 percent of the data to train/validate/test\n",
        "X_test      = X_ts[:,:,500:1250]\n",
        "Y_test      = y_ts[:]\n",
        "print(np.shape(X_test))\n",
        "print(np.shape(Y_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(288, 22, 1875)\n",
            "(288, 1)\n",
            "(288, 22, 750)\n",
            "(288, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9vVDMLZp2uX6",
        "colab_type": "code",
        "outputId": "1b0e2802-8a8f-49a1-ec7f-95018e5aabfd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "\n",
        "#convert labels to one-hot encodings.\n",
        "Y_test      = np_utils.to_categorical(Y_test-1)\n",
        "\n",
        "kernels, chans, samples = 1, 22, 750\n",
        "# convert data to NCHW (trials, kernels, channels, samples) format. Data \n",
        "# contains 22 channels and 500 time-points. Set the number of kernels to 1.\n",
        "X_test      = X_test.reshape(X_test.shape[0], kernels, chans, samples)\n",
        "\n",
        "   \n",
        "print('X_train shape:', X_test.shape)\n",
        "print(X_test.shape[0], 'train samples')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train shape: (288, 1, 22, 750)\n",
            "288 train samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rUlJdZk_X1OY",
        "colab_type": "code",
        "outputId": "be62012b-5363-40b5-d2e6-687d9206f5f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 855
        }
      },
      "source": [
        "# configure the EEGNet-8,2,16 model with kernel length of 32 samples (other \n",
        "# model configurations may do better, but this is a good starting point)\n",
        "model = EEGNet(nb_classes = 4, Chans = 22, Samples = 750, \n",
        "             dropoutRate = 0.5, kernLength = 25, F1 = 8, \n",
        "             D = 2, F2 = 16, norm_rate = 0.25, dropoutType = 'Dropout')\n",
        "model.summary()\n",
        "\n",
        "# compile the model and set the optimizers\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', \n",
        "              metrics = ['accuracy'])\n",
        "\n",
        "# count number of parameters in the model\n",
        "numParams    = model.count_params() \n",
        "\n",
        "# set a valid path for your system to record model checkpoints\n",
        "checkpointer = ModelCheckpoint(filepath='/tmp/checkpoint.h5', verbose=1,\n",
        "                               save_best_only=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 1, 22, 750)]      0         \n",
            "_________________________________________________________________\n",
            "conv2d (Conv2D)              (None, 8, 22, 750)        200       \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 8, 22, 750)        32        \n",
            "_________________________________________________________________\n",
            "depthwise_conv2d (DepthwiseC (None, 16, 1, 750)        352       \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 16, 1, 750)        64        \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 16, 1, 750)        0         \n",
            "_________________________________________________________________\n",
            "average_pooling2d (AveragePo (None, 16, 1, 187)        0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 16, 1, 187)        0         \n",
            "_________________________________________________________________\n",
            "separable_conv2d (SeparableC (None, 16, 1, 187)        512       \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 16, 1, 187)        64        \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 16, 1, 187)        0         \n",
            "_________________________________________________________________\n",
            "average_pooling2d_1 (Average (None, 16, 1, 23)         0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 16, 1, 23)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 368)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 64)                23616     \n",
            "_________________________________________________________________\n",
            "dense1 (Dense)               (None, 4)                 260       \n",
            "_________________________________________________________________\n",
            "softmax (Activation)         (None, 4)                 0         \n",
            "=================================================================\n",
            "Total params: 25,100\n",
            "Trainable params: 25,020\n",
            "Non-trainable params: 80\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CwvCZhYUfIx9",
        "colab_type": "code",
        "outputId": "89d123ff-3575-4d92-b5da-ba05ff21d98f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# the syntax is {class_1:weight_1, class_2:weight_2,...}. Here just setting\n",
        "# the weights all to be 1\n",
        "class_weights = {0:1, 1:1, 2:1, 3:1}\n",
        "\n",
        "################################################################################\n",
        "# fit the model. Due to very small sample sizes this can get\n",
        "# pretty noisy run-to-run, but most runs should be comparable to xDAWN + \n",
        "# Riemannian geometry classification (below)\n",
        "################################################################################\n",
        "history = model.fit(X_train, Y_train, batch_size = 16, epochs = 100, \n",
        "                        verbose = 2, validation_data=(X_val, Y_val),\n",
        "                        callbacks=[checkpointer], class_weight = class_weights)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 240 samples, validate on 47 samples\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 1.37843, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 4s - loss: 1.4070 - acc: 0.2708 - val_loss: 1.3784 - val_acc: 0.3404\n",
            "Epoch 2/100\n",
            "\n",
            "Epoch 00002: val_loss improved from 1.37843 to 1.36857, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.3544 - acc: 0.3458 - val_loss: 1.3686 - val_acc: 0.2979\n",
            "Epoch 3/100\n",
            "\n",
            "Epoch 00003: val_loss improved from 1.36857 to 1.35139, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.3197 - acc: 0.4125 - val_loss: 1.3514 - val_acc: 0.4043\n",
            "Epoch 4/100\n",
            "\n",
            "Epoch 00004: val_loss improved from 1.35139 to 1.33083, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.2808 - acc: 0.5083 - val_loss: 1.3308 - val_acc: 0.3617\n",
            "Epoch 5/100\n",
            "\n",
            "Epoch 00005: val_loss improved from 1.33083 to 1.31723, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.2517 - acc: 0.4875 - val_loss: 1.3172 - val_acc: 0.4468\n",
            "Epoch 6/100\n",
            "\n",
            "Epoch 00006: val_loss improved from 1.31723 to 1.29167, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.2098 - acc: 0.5583 - val_loss: 1.2917 - val_acc: 0.4681\n",
            "Epoch 7/100\n",
            "\n",
            "Epoch 00007: val_loss improved from 1.29167 to 1.27924, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.1867 - acc: 0.5375 - val_loss: 1.2792 - val_acc: 0.4255\n",
            "Epoch 8/100\n",
            "\n",
            "Epoch 00008: val_loss improved from 1.27924 to 1.25395, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.1556 - acc: 0.5625 - val_loss: 1.2539 - val_acc: 0.4255\n",
            "Epoch 9/100\n",
            "\n",
            "Epoch 00009: val_loss improved from 1.25395 to 1.23838, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.1216 - acc: 0.5750 - val_loss: 1.2384 - val_acc: 0.4043\n",
            "Epoch 10/100\n",
            "\n",
            "Epoch 00010: val_loss improved from 1.23838 to 1.20940, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.0544 - acc: 0.6625 - val_loss: 1.2094 - val_acc: 0.4468\n",
            "Epoch 11/100\n",
            "\n",
            "Epoch 00011: val_loss improved from 1.20940 to 1.20313, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.0403 - acc: 0.6208 - val_loss: 1.2031 - val_acc: 0.4043\n",
            "Epoch 12/100\n",
            "\n",
            "Epoch 00012: val_loss improved from 1.20313 to 1.18049, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.9983 - acc: 0.6875 - val_loss: 1.1805 - val_acc: 0.4468\n",
            "Epoch 13/100\n",
            "\n",
            "Epoch 00013: val_loss improved from 1.18049 to 1.17454, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.9913 - acc: 0.6458 - val_loss: 1.1745 - val_acc: 0.4468\n",
            "Epoch 14/100\n",
            "\n",
            "Epoch 00014: val_loss improved from 1.17454 to 1.15577, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.9349 - acc: 0.7208 - val_loss: 1.1558 - val_acc: 0.4468\n",
            "Epoch 15/100\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 1.15577\n",
            "240/240 - 0s - loss: 0.9119 - acc: 0.7250 - val_loss: 1.1594 - val_acc: 0.4681\n",
            "Epoch 16/100\n",
            "\n",
            "Epoch 00016: val_loss improved from 1.15577 to 1.12966, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.9182 - acc: 0.7083 - val_loss: 1.1297 - val_acc: 0.4894\n",
            "Epoch 17/100\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 1.12966\n",
            "240/240 - 0s - loss: 0.9016 - acc: 0.7083 - val_loss: 1.1676 - val_acc: 0.4681\n",
            "Epoch 18/100\n",
            "\n",
            "Epoch 00018: val_loss improved from 1.12966 to 1.12092, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.8536 - acc: 0.7542 - val_loss: 1.1209 - val_acc: 0.5532\n",
            "Epoch 19/100\n",
            "\n",
            "Epoch 00019: val_loss improved from 1.12092 to 1.11653, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.8037 - acc: 0.7708 - val_loss: 1.1165 - val_acc: 0.4681\n",
            "Epoch 20/100\n",
            "\n",
            "Epoch 00020: val_loss improved from 1.11653 to 1.05155, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.7583 - acc: 0.7958 - val_loss: 1.0515 - val_acc: 0.5745\n",
            "Epoch 21/100\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 1.05155\n",
            "240/240 - 0s - loss: 0.7990 - acc: 0.7625 - val_loss: 1.1690 - val_acc: 0.4043\n",
            "Epoch 22/100\n",
            "\n",
            "Epoch 00022: val_loss improved from 1.05155 to 0.97201, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.7677 - acc: 0.7667 - val_loss: 0.9720 - val_acc: 0.5957\n",
            "Epoch 23/100\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.97201\n",
            "240/240 - 0s - loss: 0.7461 - acc: 0.8000 - val_loss: 1.0000 - val_acc: 0.5745\n",
            "Epoch 24/100\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.97201\n",
            "240/240 - 0s - loss: 0.7419 - acc: 0.7708 - val_loss: 1.0119 - val_acc: 0.5957\n",
            "Epoch 25/100\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.97201\n",
            "240/240 - 0s - loss: 0.6933 - acc: 0.8083 - val_loss: 0.9733 - val_acc: 0.5532\n",
            "Epoch 26/100\n",
            "\n",
            "Epoch 00026: val_loss improved from 0.97201 to 0.91594, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.7066 - acc: 0.8000 - val_loss: 0.9159 - val_acc: 0.6383\n",
            "Epoch 27/100\n",
            "\n",
            "Epoch 00027: val_loss improved from 0.91594 to 0.88128, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.6988 - acc: 0.8250 - val_loss: 0.8813 - val_acc: 0.6596\n",
            "Epoch 28/100\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.88128\n",
            "240/240 - 0s - loss: 0.6823 - acc: 0.7875 - val_loss: 0.9316 - val_acc: 0.5957\n",
            "Epoch 29/100\n",
            "\n",
            "Epoch 00029: val_loss improved from 0.88128 to 0.85330, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.6680 - acc: 0.8167 - val_loss: 0.8533 - val_acc: 0.5745\n",
            "Epoch 30/100\n",
            "\n",
            "Epoch 00030: val_loss improved from 0.85330 to 0.84588, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.6750 - acc: 0.8125 - val_loss: 0.8459 - val_acc: 0.7021\n",
            "Epoch 31/100\n",
            "\n",
            "Epoch 00031: val_loss improved from 0.84588 to 0.83937, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.6464 - acc: 0.8583 - val_loss: 0.8394 - val_acc: 0.6383\n",
            "Epoch 32/100\n",
            "\n",
            "Epoch 00032: val_loss improved from 0.83937 to 0.83834, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.6416 - acc: 0.8375 - val_loss: 0.8383 - val_acc: 0.7021\n",
            "Epoch 33/100\n",
            "\n",
            "Epoch 00033: val_loss improved from 0.83834 to 0.77556, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.6395 - acc: 0.8417 - val_loss: 0.7756 - val_acc: 0.7660\n",
            "Epoch 34/100\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.77556\n",
            "240/240 - 0s - loss: 0.6440 - acc: 0.8500 - val_loss: 0.8012 - val_acc: 0.6596\n",
            "Epoch 35/100\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.77556\n",
            "240/240 - 0s - loss: 0.6074 - acc: 0.8542 - val_loss: 0.8146 - val_acc: 0.6596\n",
            "Epoch 36/100\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.77556\n",
            "240/240 - 0s - loss: 0.6069 - acc: 0.8667 - val_loss: 0.7811 - val_acc: 0.7021\n",
            "Epoch 37/100\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.77556\n",
            "240/240 - 0s - loss: 0.5916 - acc: 0.8625 - val_loss: 0.8730 - val_acc: 0.6596\n",
            "Epoch 38/100\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.77556\n",
            "240/240 - 0s - loss: 0.5937 - acc: 0.8708 - val_loss: 0.8015 - val_acc: 0.6596\n",
            "Epoch 39/100\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.77556\n",
            "240/240 - 0s - loss: 0.6100 - acc: 0.8542 - val_loss: 0.8307 - val_acc: 0.6596\n",
            "Epoch 40/100\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.77556\n",
            "240/240 - 0s - loss: 0.5796 - acc: 0.8500 - val_loss: 0.8026 - val_acc: 0.7234\n",
            "Epoch 41/100\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.77556\n",
            "240/240 - 0s - loss: 0.6054 - acc: 0.8333 - val_loss: 0.8039 - val_acc: 0.6809\n",
            "Epoch 42/100\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.77556\n",
            "240/240 - 0s - loss: 0.5731 - acc: 0.8625 - val_loss: 0.8273 - val_acc: 0.6170\n",
            "Epoch 43/100\n",
            "\n",
            "Epoch 00043: val_loss improved from 0.77556 to 0.75434, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.5616 - acc: 0.8667 - val_loss: 0.7543 - val_acc: 0.7872\n",
            "Epoch 44/100\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.75434\n",
            "240/240 - 0s - loss: 0.5640 - acc: 0.8917 - val_loss: 0.8469 - val_acc: 0.6596\n",
            "Epoch 45/100\n",
            "\n",
            "Epoch 00045: val_loss improved from 0.75434 to 0.72081, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.5699 - acc: 0.8500 - val_loss: 0.7208 - val_acc: 0.7872\n",
            "Epoch 46/100\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.72081\n",
            "240/240 - 0s - loss: 0.5453 - acc: 0.8750 - val_loss: 0.8175 - val_acc: 0.6596\n",
            "Epoch 47/100\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.72081\n",
            "240/240 - 0s - loss: 0.5268 - acc: 0.8958 - val_loss: 0.7976 - val_acc: 0.6809\n",
            "Epoch 48/100\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.72081\n",
            "240/240 - 0s - loss: 0.5385 - acc: 0.8875 - val_loss: 0.8327 - val_acc: 0.7021\n",
            "Epoch 49/100\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.72081\n",
            "240/240 - 0s - loss: 0.5420 - acc: 0.8875 - val_loss: 0.7746 - val_acc: 0.6596\n",
            "Epoch 50/100\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.72081\n",
            "240/240 - 0s - loss: 0.5432 - acc: 0.9042 - val_loss: 0.8363 - val_acc: 0.6596\n",
            "Epoch 51/100\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.72081\n",
            "240/240 - 0s - loss: 0.5287 - acc: 0.8875 - val_loss: 0.7509 - val_acc: 0.7234\n",
            "Epoch 52/100\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.72081\n",
            "240/240 - 0s - loss: 0.5339 - acc: 0.9083 - val_loss: 0.8216 - val_acc: 0.6596\n",
            "Epoch 53/100\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.72081\n",
            "240/240 - 0s - loss: 0.5335 - acc: 0.8542 - val_loss: 0.8395 - val_acc: 0.6383\n",
            "Epoch 54/100\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.72081\n",
            "240/240 - 0s - loss: 0.5390 - acc: 0.8958 - val_loss: 0.8183 - val_acc: 0.6596\n",
            "Epoch 55/100\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.72081\n",
            "240/240 - 0s - loss: 0.5349 - acc: 0.8958 - val_loss: 0.8162 - val_acc: 0.6596\n",
            "Epoch 56/100\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.72081\n",
            "240/240 - 0s - loss: 0.5271 - acc: 0.8958 - val_loss: 0.7730 - val_acc: 0.7021\n",
            "Epoch 57/100\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.72081\n",
            "240/240 - 0s - loss: 0.5050 - acc: 0.9042 - val_loss: 0.8359 - val_acc: 0.6809\n",
            "Epoch 58/100\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.72081\n",
            "240/240 - 0s - loss: 0.4953 - acc: 0.9042 - val_loss: 0.7712 - val_acc: 0.6809\n",
            "Epoch 59/100\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.72081\n",
            "240/240 - 0s - loss: 0.5058 - acc: 0.9083 - val_loss: 0.7573 - val_acc: 0.7021\n",
            "Epoch 60/100\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.72081\n",
            "240/240 - 0s - loss: 0.4990 - acc: 0.8917 - val_loss: 0.7880 - val_acc: 0.7234\n",
            "Epoch 61/100\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.72081\n",
            "240/240 - 0s - loss: 0.4798 - acc: 0.9208 - val_loss: 0.7975 - val_acc: 0.6596\n",
            "Epoch 62/100\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.72081\n",
            "240/240 - 0s - loss: 0.5064 - acc: 0.9042 - val_loss: 0.7978 - val_acc: 0.7021\n",
            "Epoch 63/100\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.72081\n",
            "240/240 - 0s - loss: 0.4837 - acc: 0.9333 - val_loss: 0.8472 - val_acc: 0.6170\n",
            "Epoch 64/100\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.72081\n",
            "240/240 - 0s - loss: 0.4954 - acc: 0.9042 - val_loss: 0.7439 - val_acc: 0.7234\n",
            "Epoch 65/100\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.72081\n",
            "240/240 - 0s - loss: 0.4875 - acc: 0.9083 - val_loss: 0.8172 - val_acc: 0.6383\n",
            "Epoch 66/100\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.72081\n",
            "240/240 - 0s - loss: 0.4996 - acc: 0.8958 - val_loss: 0.8278 - val_acc: 0.6809\n",
            "Epoch 67/100\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.72081\n",
            "240/240 - 0s - loss: 0.4632 - acc: 0.9250 - val_loss: 0.7764 - val_acc: 0.7021\n",
            "Epoch 68/100\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.72081\n",
            "240/240 - 0s - loss: 0.4573 - acc: 0.9250 - val_loss: 0.7517 - val_acc: 0.6809\n",
            "Epoch 69/100\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.72081\n",
            "240/240 - 0s - loss: 0.5044 - acc: 0.9167 - val_loss: 0.8295 - val_acc: 0.6809\n",
            "Epoch 70/100\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.72081\n",
            "240/240 - 0s - loss: 0.4629 - acc: 0.9292 - val_loss: 0.7656 - val_acc: 0.6809\n",
            "Epoch 71/100\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.72081\n",
            "240/240 - 0s - loss: 0.4783 - acc: 0.9208 - val_loss: 0.8252 - val_acc: 0.6809\n",
            "Epoch 72/100\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.72081\n",
            "240/240 - 0s - loss: 0.4650 - acc: 0.9333 - val_loss: 0.7927 - val_acc: 0.7234\n",
            "Epoch 73/100\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.72081\n",
            "240/240 - 0s - loss: 0.4508 - acc: 0.9250 - val_loss: 0.8321 - val_acc: 0.6809\n",
            "Epoch 74/100\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.72081\n",
            "240/240 - 0s - loss: 0.4368 - acc: 0.9458 - val_loss: 0.7671 - val_acc: 0.7021\n",
            "Epoch 75/100\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.72081\n",
            "240/240 - 0s - loss: 0.4406 - acc: 0.9417 - val_loss: 0.8016 - val_acc: 0.7234\n",
            "Epoch 76/100\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.72081\n",
            "240/240 - 0s - loss: 0.4559 - acc: 0.9333 - val_loss: 0.8537 - val_acc: 0.7021\n",
            "Epoch 77/100\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.72081\n",
            "240/240 - 0s - loss: 0.4442 - acc: 0.9417 - val_loss: 0.7740 - val_acc: 0.6809\n",
            "Epoch 78/100\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.72081\n",
            "240/240 - 0s - loss: 0.4354 - acc: 0.9500 - val_loss: 0.7582 - val_acc: 0.7234\n",
            "Epoch 79/100\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.72081\n",
            "240/240 - 0s - loss: 0.4420 - acc: 0.9458 - val_loss: 0.7879 - val_acc: 0.6170\n",
            "Epoch 80/100\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.72081\n",
            "240/240 - 0s - loss: 0.4710 - acc: 0.9250 - val_loss: 0.7675 - val_acc: 0.7021\n",
            "Epoch 81/100\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.72081\n",
            "240/240 - 0s - loss: 0.4347 - acc: 0.9417 - val_loss: 0.7456 - val_acc: 0.6809\n",
            "Epoch 82/100\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.72081\n",
            "240/240 - 0s - loss: 0.4426 - acc: 0.9333 - val_loss: 0.7776 - val_acc: 0.7021\n",
            "Epoch 83/100\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.72081\n",
            "240/240 - 0s - loss: 0.4597 - acc: 0.9417 - val_loss: 0.8445 - val_acc: 0.6596\n",
            "Epoch 84/100\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.72081\n",
            "240/240 - 0s - loss: 0.4404 - acc: 0.9417 - val_loss: 0.7295 - val_acc: 0.7021\n",
            "Epoch 85/100\n",
            "\n",
            "Epoch 00085: val_loss improved from 0.72081 to 0.72074, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.4468 - acc: 0.9292 - val_loss: 0.7207 - val_acc: 0.7021\n",
            "Epoch 86/100\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.72074\n",
            "240/240 - 0s - loss: 0.4498 - acc: 0.9250 - val_loss: 0.7770 - val_acc: 0.6596\n",
            "Epoch 87/100\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.72074\n",
            "240/240 - 0s - loss: 0.4171 - acc: 0.9458 - val_loss: 0.7795 - val_acc: 0.7021\n",
            "Epoch 88/100\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.72074\n",
            "240/240 - 0s - loss: 0.4077 - acc: 0.9583 - val_loss: 0.7931 - val_acc: 0.6809\n",
            "Epoch 89/100\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.72074\n",
            "240/240 - 0s - loss: 0.4342 - acc: 0.9375 - val_loss: 0.7695 - val_acc: 0.6596\n",
            "Epoch 90/100\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.72074\n",
            "240/240 - 0s - loss: 0.4350 - acc: 0.9333 - val_loss: 0.8027 - val_acc: 0.6596\n",
            "Epoch 91/100\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.72074\n",
            "240/240 - 0s - loss: 0.4195 - acc: 0.9458 - val_loss: 0.7285 - val_acc: 0.7447\n",
            "Epoch 92/100\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.72074\n",
            "240/240 - 0s - loss: 0.4460 - acc: 0.9292 - val_loss: 0.7773 - val_acc: 0.6596\n",
            "Epoch 93/100\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.72074\n",
            "240/240 - 0s - loss: 0.4367 - acc: 0.9333 - val_loss: 0.7488 - val_acc: 0.6809\n",
            "Epoch 94/100\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.72074\n",
            "240/240 - 0s - loss: 0.4657 - acc: 0.9458 - val_loss: 0.8086 - val_acc: 0.6809\n",
            "Epoch 95/100\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.72074\n",
            "240/240 - 0s - loss: 0.4300 - acc: 0.9458 - val_loss: 0.7772 - val_acc: 0.6170\n",
            "Epoch 96/100\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.72074\n",
            "240/240 - 0s - loss: 0.4315 - acc: 0.9500 - val_loss: 0.8036 - val_acc: 0.6809\n",
            "Epoch 97/100\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.72074\n",
            "240/240 - 0s - loss: 0.4513 - acc: 0.9333 - val_loss: 0.7476 - val_acc: 0.6809\n",
            "Epoch 98/100\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.72074\n",
            "240/240 - 0s - loss: 0.4095 - acc: 0.9542 - val_loss: 0.7453 - val_acc: 0.7021\n",
            "Epoch 99/100\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.72074\n",
            "240/240 - 0s - loss: 0.4149 - acc: 0.9333 - val_loss: 0.7312 - val_acc: 0.7234\n",
            "Epoch 100/100\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.72074\n",
            "240/240 - 0s - loss: 0.4249 - acc: 0.9583 - val_loss: 0.7591 - val_acc: 0.7021\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4aC5kY7lhys",
        "colab_type": "code",
        "outputId": "6c8ea75c-04ec-4ef2-83dd-ec92dd980381",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "# Plot training & validation accuracy values\n",
        "plt.plot(history.history['acc'])\n",
        "plt.plot(history.history['val_acc'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0P8YE0iV2XwS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ynew = model.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EfAoIy2p3lWB",
        "colab_type": "code",
        "outputId": "5c8f6d0f-13ac-46f5-d725-b46073fbbea1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "# Evaluate the model on the test data using `evaluate`\n",
        "print('\\n# Evaluate on test data')\n",
        "results = model.evaluate(X_test, Y_test, batch_size=1)\n",
        "print('test loss, test acc:', results)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "# Evaluate on test data\n",
            "288/288 [==============================] - 1s 3ms/sample - loss: 0.7663 - acc: 0.7222\n",
            "test loss, test acc: [0.7663404682858123, 0.7222222]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}