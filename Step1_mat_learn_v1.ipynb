{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Step1_mat_learn_v1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sagihaider/EEG_Deep/blob/master/Step1_mat_learn_v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X110PE9MjrlE",
        "colab_type": "code",
        "outputId": "284dca7d-45cd-49ca-96e1-15dfe9862f1b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "!git clone https://github.com/sagihaider/EEG_Deep.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'EEG_Deep'...\n",
            "remote: Enumerating objects: 11, done.\u001b[K\n",
            "remote: Counting objects: 100% (11/11), done.\u001b[K\n",
            "remote: Compressing objects: 100% (9/9), done.\u001b[K\n",
            "remote: Total 11 (delta 1), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (11/11), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ioLzMoiS60Qf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "outputId": "58fe318c-8770-449e-91c8-4d47b4a50479"
      },
      "source": [
        "import scipy.io as spio\n",
        "import numpy as np\n",
        "from importlib.machinery import SourceFileLoader\n",
        "\n",
        "# EEGNet-specific imports\n",
        "from EEG_Deep.EEGModels import EEGNet, ShallowConvNet, DeepConvNet\n",
        "from tensorflow.keras import utils as np_utils\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras import backend as K\n",
        "K.set_image_data_format('channels_first')\n",
        "\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# tools for plotting confusion matrices\n",
        "from matplotlib import pyplot as plt"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZjtnE_6RSpum",
        "colab_type": "code",
        "outputId": "5cd2726f-3a88-40f0-8194-e0676e81c4a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "x= 1         \n",
        "fName = 'EEG_Deep/parsed_A0' + str(x) + 'T.mat'  # Load Data\n",
        "print(fName)\n",
        "mat = spio.loadmat(fName)\n",
        "X = mat['cleanRawEEGData']\n",
        "y = mat['cleanClassLabels']"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "EEG_Deep/parsed_A01T.mat\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3c11RQCoS80S",
        "colab_type": "code",
        "outputId": "3db04260-f84b-4694-c3ca-b870b52766cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(np.shape(X))\n",
        "print(np.shape(y))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(273, 22, 1500)\n",
            "(273, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tgWnQSY5T4op",
        "colab_type": "code",
        "outputId": "092e192a-a570-4b5a-8f89-97e96a195a03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "\n",
        "# take 50/25/25 percent of the data to train/validate/test\n",
        "X_train      = X[0:200,:,500:1250]\n",
        "Y_train      = y[0:200]\n",
        "#X_validate   = X[151:200,:,500:1250]\n",
        "#Y_validate   = y[151:200]\n",
        "X_test       = X[201:,:,500:1250]\n",
        "Y_test       = y[201:]\n",
        "\n",
        "print(np.shape(X_train))\n",
        "print(np.shape(Y_train))\n",
        "# print(np.shape(X_validate))\n",
        "# print(np.shape(Y_validate))\n",
        "print(np.shape(X_test))\n",
        "print(np.shape(Y_test))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(200, 22, 750)\n",
            "(200, 1)\n",
            "(72, 22, 750)\n",
            "(72, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bfC8z4I-UnF6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# convert labels to one-hot encodings.\n",
        "Y_train      = np_utils.to_categorical(Y_train-1)\n",
        "# Y_validate   = np_utils.to_categorical(Y_validate-1)\n",
        "Y_test       = np_utils.to_categorical(Y_test-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NqmsOU1BXOZJ",
        "colab_type": "code",
        "outputId": "677af214-7e66-4372-e73c-b4e09f52204d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "kernels, chans, samples = 1, 22, 750\n",
        "\n",
        "# convert data to NCHW (trials, kernels, channels, samples) format. Data \n",
        "# contains 22 channels and 500 time-points. Set the number of kernels to 1.\n",
        "X_train      = X_train.reshape(X_train.shape[0], kernels, chans, samples)\n",
        "# X_validate   = X_validate.reshape(X_validate.shape[0], kernels, chans, samples)\n",
        "X_test       = X_test.reshape(X_test.shape[0], kernels, chans, samples)\n",
        "   \n",
        "print('X_train shape:', X_train.shape)\n",
        "print(X_train.shape[0], 'train samples')\n",
        "print(X_test.shape[0], 'test samples')"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train shape: (200, 1, 22, 750)\n",
            "200 train samples\n",
            "72 test samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rUlJdZk_X1OY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# configure the EEGNet-8,2,16 model with kernel length of 32 samples (other \n",
        "# model configurations may do better, but this is a good starting point)\n",
        "model = EEGNet(nb_classes = 4, Chans = 22, Samples = 750, \n",
        "             dropoutRate = 0.5, kernLength = 25, F1 = 8, \n",
        "             D = 2, F2 = 16, norm_rate = 0.25, dropoutType = 'Dropout')\n",
        "\n",
        "# compile the model and set the optimizers\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', \n",
        "              metrics = ['accuracy'])\n",
        "\n",
        "# count number of parameters in the model\n",
        "numParams    = model.count_params() \n",
        "\n",
        "# set a valid path for your system to record model checkpoints\n",
        "checkpointer = ModelCheckpoint(filepath='/tmp/checkpoint.h5', verbose=1,\n",
        "                               save_best_only=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CwvCZhYUfIx9",
        "colab_type": "code",
        "outputId": "a2e3653f-18f8-47b9-e446-4d1fa0b3c202",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# the syntax is {class_1:weight_1, class_2:weight_2,...}. Here just setting\n",
        "# the weights all to be 1\n",
        "class_weights = {0:1, 1:1, 2:1, 3:1}\n",
        "\n",
        "################################################################################\n",
        "# fit the model. Due to very small sample sizes this can get\n",
        "# pretty noisy run-to-run, but most runs should be comparable to xDAWN + \n",
        "# Riemannian geometry classification (below)\n",
        "################################################################################\n",
        "history = model.fit(X_train, Y_train, batch_size = 16, epochs = 100, \n",
        "                        verbose = 2, validation_data=(X_test, Y_test),\n",
        "                        callbacks=[checkpointer], class_weight = class_weights)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 200 samples, validate on 72 samples\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 1.37484, saving model to /tmp/checkpoint.h5\n",
            "200/200 - 1s - loss: 1.4009 - acc: 0.2900 - val_loss: 1.3748 - val_acc: 0.3056\n",
            "Epoch 2/100\n",
            "\n",
            "Epoch 00002: val_loss improved from 1.37484 to 1.36296, saving model to /tmp/checkpoint.h5\n",
            "200/200 - 0s - loss: 1.3368 - acc: 0.4350 - val_loss: 1.3630 - val_acc: 0.3750\n",
            "Epoch 3/100\n",
            "\n",
            "Epoch 00003: val_loss improved from 1.36296 to 1.34830, saving model to /tmp/checkpoint.h5\n",
            "200/200 - 0s - loss: 1.3028 - acc: 0.4200 - val_loss: 1.3483 - val_acc: 0.3611\n",
            "Epoch 4/100\n",
            "\n",
            "Epoch 00004: val_loss improved from 1.34830 to 1.33371, saving model to /tmp/checkpoint.h5\n",
            "200/200 - 0s - loss: 1.2632 - acc: 0.4850 - val_loss: 1.3337 - val_acc: 0.4028\n",
            "Epoch 5/100\n",
            "\n",
            "Epoch 00005: val_loss improved from 1.33371 to 1.31925, saving model to /tmp/checkpoint.h5\n",
            "200/200 - 0s - loss: 1.2191 - acc: 0.4800 - val_loss: 1.3193 - val_acc: 0.4167\n",
            "Epoch 6/100\n",
            "\n",
            "Epoch 00006: val_loss improved from 1.31925 to 1.31344, saving model to /tmp/checkpoint.h5\n",
            "200/200 - 0s - loss: 1.1811 - acc: 0.5500 - val_loss: 1.3134 - val_acc: 0.4028\n",
            "Epoch 7/100\n",
            "\n",
            "Epoch 00007: val_loss improved from 1.31344 to 1.30232, saving model to /tmp/checkpoint.h5\n",
            "200/200 - 0s - loss: 1.1616 - acc: 0.5500 - val_loss: 1.3023 - val_acc: 0.4444\n",
            "Epoch 8/100\n",
            "\n",
            "Epoch 00008: val_loss improved from 1.30232 to 1.29607, saving model to /tmp/checkpoint.h5\n",
            "200/200 - 0s - loss: 1.1377 - acc: 0.5650 - val_loss: 1.2961 - val_acc: 0.4167\n",
            "Epoch 9/100\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 1.29607\n",
            "200/200 - 0s - loss: 1.0870 - acc: 0.6000 - val_loss: 1.2982 - val_acc: 0.3889\n",
            "Epoch 10/100\n",
            "\n",
            "Epoch 00010: val_loss improved from 1.29607 to 1.28807, saving model to /tmp/checkpoint.h5\n",
            "200/200 - 0s - loss: 1.1087 - acc: 0.5600 - val_loss: 1.2881 - val_acc: 0.4028\n",
            "Epoch 11/100\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 1.28807\n",
            "200/200 - 0s - loss: 1.0552 - acc: 0.6600 - val_loss: 1.2916 - val_acc: 0.4306\n",
            "Epoch 12/100\n",
            "\n",
            "Epoch 00012: val_loss improved from 1.28807 to 1.28290, saving model to /tmp/checkpoint.h5\n",
            "200/200 - 0s - loss: 1.0220 - acc: 0.6400 - val_loss: 1.2829 - val_acc: 0.4444\n",
            "Epoch 13/100\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 1.28290\n",
            "200/200 - 0s - loss: 0.9965 - acc: 0.6900 - val_loss: 1.2928 - val_acc: 0.4167\n",
            "Epoch 14/100\n",
            "\n",
            "Epoch 00014: val_loss improved from 1.28290 to 1.25880, saving model to /tmp/checkpoint.h5\n",
            "200/200 - 0s - loss: 1.0042 - acc: 0.6650 - val_loss: 1.2588 - val_acc: 0.4306\n",
            "Epoch 15/100\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 1.25880\n",
            "200/200 - 0s - loss: 0.9823 - acc: 0.6600 - val_loss: 1.2699 - val_acc: 0.4444\n",
            "Epoch 16/100\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 1.25880\n",
            "200/200 - 0s - loss: 0.9453 - acc: 0.6900 - val_loss: 1.2668 - val_acc: 0.4444\n",
            "Epoch 17/100\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 1.25880\n",
            "200/200 - 0s - loss: 0.9255 - acc: 0.7100 - val_loss: 1.2697 - val_acc: 0.4306\n",
            "Epoch 18/100\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 1.25880\n",
            "200/200 - 0s - loss: 0.8937 - acc: 0.7500 - val_loss: 1.2676 - val_acc: 0.4028\n",
            "Epoch 19/100\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 1.25880\n",
            "200/200 - 0s - loss: 0.8910 - acc: 0.7150 - val_loss: 1.2597 - val_acc: 0.4167\n",
            "Epoch 20/100\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 1.25880\n",
            "200/200 - 0s - loss: 0.8431 - acc: 0.7800 - val_loss: 1.2636 - val_acc: 0.3750\n",
            "Epoch 21/100\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 1.25880\n",
            "200/200 - 0s - loss: 0.8710 - acc: 0.7150 - val_loss: 1.2988 - val_acc: 0.3750\n",
            "Epoch 22/100\n",
            "\n",
            "Epoch 00022: val_loss improved from 1.25880 to 1.25580, saving model to /tmp/checkpoint.h5\n",
            "200/200 - 0s - loss: 0.8077 - acc: 0.7800 - val_loss: 1.2558 - val_acc: 0.3889\n",
            "Epoch 23/100\n",
            "\n",
            "Epoch 00023: val_loss improved from 1.25580 to 1.24400, saving model to /tmp/checkpoint.h5\n",
            "200/200 - 0s - loss: 0.7584 - acc: 0.7800 - val_loss: 1.2440 - val_acc: 0.3750\n",
            "Epoch 24/100\n",
            "\n",
            "Epoch 00024: val_loss improved from 1.24400 to 1.21625, saving model to /tmp/checkpoint.h5\n",
            "200/200 - 0s - loss: 0.7689 - acc: 0.8050 - val_loss: 1.2163 - val_acc: 0.3889\n",
            "Epoch 25/100\n",
            "\n",
            "Epoch 00025: val_loss improved from 1.21625 to 1.19473, saving model to /tmp/checkpoint.h5\n",
            "200/200 - 0s - loss: 0.7596 - acc: 0.7400 - val_loss: 1.1947 - val_acc: 0.4306\n",
            "Epoch 26/100\n",
            "\n",
            "Epoch 00026: val_loss improved from 1.19473 to 1.18963, saving model to /tmp/checkpoint.h5\n",
            "200/200 - 0s - loss: 0.7794 - acc: 0.7400 - val_loss: 1.1896 - val_acc: 0.4306\n",
            "Epoch 27/100\n",
            "\n",
            "Epoch 00027: val_loss improved from 1.18963 to 1.13559, saving model to /tmp/checkpoint.h5\n",
            "200/200 - 0s - loss: 0.7038 - acc: 0.7750 - val_loss: 1.1356 - val_acc: 0.4444\n",
            "Epoch 28/100\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 1.13559\n",
            "200/200 - 0s - loss: 0.7046 - acc: 0.7950 - val_loss: 1.1483 - val_acc: 0.4444\n",
            "Epoch 29/100\n",
            "\n",
            "Epoch 00029: val_loss improved from 1.13559 to 1.09072, saving model to /tmp/checkpoint.h5\n",
            "200/200 - 0s - loss: 0.7118 - acc: 0.7500 - val_loss: 1.0907 - val_acc: 0.5000\n",
            "Epoch 30/100\n",
            "\n",
            "Epoch 00030: val_loss improved from 1.09072 to 1.08141, saving model to /tmp/checkpoint.h5\n",
            "200/200 - 0s - loss: 0.6557 - acc: 0.8150 - val_loss: 1.0814 - val_acc: 0.4861\n",
            "Epoch 31/100\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 1.08141\n",
            "200/200 - 0s - loss: 0.6622 - acc: 0.8150 - val_loss: 1.0848 - val_acc: 0.5000\n",
            "Epoch 32/100\n",
            "\n",
            "Epoch 00032: val_loss improved from 1.08141 to 1.04927, saving model to /tmp/checkpoint.h5\n",
            "200/200 - 0s - loss: 0.6772 - acc: 0.7900 - val_loss: 1.0493 - val_acc: 0.5278\n",
            "Epoch 33/100\n",
            "\n",
            "Epoch 00033: val_loss improved from 1.04927 to 0.97964, saving model to /tmp/checkpoint.h5\n",
            "200/200 - 0s - loss: 0.6494 - acc: 0.7900 - val_loss: 0.9796 - val_acc: 0.5694\n",
            "Epoch 34/100\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.97964\n",
            "200/200 - 0s - loss: 0.6391 - acc: 0.8100 - val_loss: 0.9879 - val_acc: 0.5556\n",
            "Epoch 35/100\n",
            "\n",
            "Epoch 00035: val_loss improved from 0.97964 to 0.97337, saving model to /tmp/checkpoint.h5\n",
            "200/200 - 0s - loss: 0.6486 - acc: 0.7850 - val_loss: 0.9734 - val_acc: 0.5417\n",
            "Epoch 36/100\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.97337\n",
            "200/200 - 0s - loss: 0.6088 - acc: 0.8050 - val_loss: 0.9764 - val_acc: 0.5556\n",
            "Epoch 37/100\n",
            "\n",
            "Epoch 00037: val_loss improved from 0.97337 to 0.92202, saving model to /tmp/checkpoint.h5\n",
            "200/200 - 0s - loss: 0.6122 - acc: 0.8200 - val_loss: 0.9220 - val_acc: 0.5694\n",
            "Epoch 38/100\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.92202\n",
            "200/200 - 0s - loss: 0.5831 - acc: 0.8050 - val_loss: 0.9278 - val_acc: 0.5694\n",
            "Epoch 39/100\n",
            "\n",
            "Epoch 00039: val_loss improved from 0.92202 to 0.89889, saving model to /tmp/checkpoint.h5\n",
            "200/200 - 0s - loss: 0.5909 - acc: 0.8350 - val_loss: 0.8989 - val_acc: 0.5694\n",
            "Epoch 40/100\n",
            "\n",
            "Epoch 00040: val_loss improved from 0.89889 to 0.89680, saving model to /tmp/checkpoint.h5\n",
            "200/200 - 0s - loss: 0.5936 - acc: 0.8300 - val_loss: 0.8968 - val_acc: 0.5556\n",
            "Epoch 41/100\n",
            "\n",
            "Epoch 00041: val_loss improved from 0.89680 to 0.85410, saving model to /tmp/checkpoint.h5\n",
            "200/200 - 0s - loss: 0.5820 - acc: 0.8400 - val_loss: 0.8541 - val_acc: 0.5972\n",
            "Epoch 42/100\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.85410\n",
            "200/200 - 0s - loss: 0.5674 - acc: 0.8050 - val_loss: 0.9204 - val_acc: 0.5417\n",
            "Epoch 43/100\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.85410\n",
            "200/200 - 0s - loss: 0.5863 - acc: 0.8000 - val_loss: 0.9005 - val_acc: 0.5694\n",
            "Epoch 44/100\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.85410\n",
            "200/200 - 0s - loss: 0.5525 - acc: 0.8400 - val_loss: 0.8950 - val_acc: 0.5556\n",
            "Epoch 45/100\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.85410\n",
            "200/200 - 0s - loss: 0.5935 - acc: 0.8350 - val_loss: 0.8652 - val_acc: 0.5694\n",
            "Epoch 46/100\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.85410\n",
            "200/200 - 0s - loss: 0.5732 - acc: 0.8350 - val_loss: 0.9067 - val_acc: 0.5417\n",
            "Epoch 47/100\n",
            "\n",
            "Epoch 00047: val_loss improved from 0.85410 to 0.83691, saving model to /tmp/checkpoint.h5\n",
            "200/200 - 0s - loss: 0.5698 - acc: 0.8500 - val_loss: 0.8369 - val_acc: 0.6250\n",
            "Epoch 48/100\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.83691\n",
            "200/200 - 0s - loss: 0.5664 - acc: 0.8500 - val_loss: 0.8501 - val_acc: 0.5833\n",
            "Epoch 49/100\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.83691\n",
            "200/200 - 0s - loss: 0.5515 - acc: 0.8550 - val_loss: 0.8423 - val_acc: 0.5833\n",
            "Epoch 50/100\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.83691\n",
            "200/200 - 0s - loss: 0.5486 - acc: 0.8350 - val_loss: 0.8620 - val_acc: 0.5417\n",
            "Epoch 51/100\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.83691\n",
            "200/200 - 0s - loss: 0.5470 - acc: 0.8600 - val_loss: 0.8448 - val_acc: 0.5972\n",
            "Epoch 52/100\n",
            "\n",
            "Epoch 00052: val_loss improved from 0.83691 to 0.82142, saving model to /tmp/checkpoint.h5\n",
            "200/200 - 0s - loss: 0.5242 - acc: 0.8550 - val_loss: 0.8214 - val_acc: 0.6111\n",
            "Epoch 53/100\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.82142\n",
            "200/200 - 0s - loss: 0.5661 - acc: 0.8100 - val_loss: 0.8445 - val_acc: 0.6111\n",
            "Epoch 54/100\n",
            "\n",
            "Epoch 00054: val_loss improved from 0.82142 to 0.82046, saving model to /tmp/checkpoint.h5\n",
            "200/200 - 0s - loss: 0.5393 - acc: 0.8350 - val_loss: 0.8205 - val_acc: 0.6111\n",
            "Epoch 55/100\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.82046\n",
            "200/200 - 0s - loss: 0.5419 - acc: 0.8300 - val_loss: 0.8617 - val_acc: 0.5833\n",
            "Epoch 56/100\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.82046\n",
            "200/200 - 0s - loss: 0.5028 - acc: 0.8400 - val_loss: 0.8283 - val_acc: 0.6111\n",
            "Epoch 57/100\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.82046\n",
            "200/200 - 0s - loss: 0.5302 - acc: 0.8300 - val_loss: 0.8270 - val_acc: 0.6111\n",
            "Epoch 58/100\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.82046\n",
            "200/200 - 0s - loss: 0.5138 - acc: 0.8500 - val_loss: 0.8268 - val_acc: 0.6250\n",
            "Epoch 59/100\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.82046\n",
            "200/200 - 0s - loss: 0.5459 - acc: 0.8600 - val_loss: 0.8312 - val_acc: 0.6389\n",
            "Epoch 60/100\n",
            "\n",
            "Epoch 00060: val_loss improved from 0.82046 to 0.80001, saving model to /tmp/checkpoint.h5\n",
            "200/200 - 0s - loss: 0.5595 - acc: 0.8250 - val_loss: 0.8000 - val_acc: 0.6250\n",
            "Epoch 61/100\n",
            "\n",
            "Epoch 00061: val_loss improved from 0.80001 to 0.76034, saving model to /tmp/checkpoint.h5\n",
            "200/200 - 0s - loss: 0.5215 - acc: 0.8550 - val_loss: 0.7603 - val_acc: 0.6944\n",
            "Epoch 62/100\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.76034\n",
            "200/200 - 0s - loss: 0.4787 - acc: 0.8700 - val_loss: 0.7640 - val_acc: 0.6806\n",
            "Epoch 63/100\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.76034\n",
            "200/200 - 0s - loss: 0.5033 - acc: 0.8700 - val_loss: 0.8011 - val_acc: 0.5972\n",
            "Epoch 64/100\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.76034\n",
            "200/200 - 0s - loss: 0.4879 - acc: 0.8550 - val_loss: 0.7706 - val_acc: 0.6667\n",
            "Epoch 65/100\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.76034\n",
            "200/200 - 0s - loss: 0.5004 - acc: 0.8700 - val_loss: 0.8321 - val_acc: 0.6250\n",
            "Epoch 66/100\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.76034\n",
            "200/200 - 0s - loss: 0.4455 - acc: 0.8800 - val_loss: 0.8157 - val_acc: 0.6806\n",
            "Epoch 67/100\n",
            "\n",
            "Epoch 00067: val_loss improved from 0.76034 to 0.75458, saving model to /tmp/checkpoint.h5\n",
            "200/200 - 0s - loss: 0.4816 - acc: 0.8700 - val_loss: 0.7546 - val_acc: 0.7083\n",
            "Epoch 68/100\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.75458\n",
            "200/200 - 0s - loss: 0.4908 - acc: 0.8500 - val_loss: 0.7585 - val_acc: 0.6667\n",
            "Epoch 69/100\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.75458\n",
            "200/200 - 0s - loss: 0.4949 - acc: 0.8550 - val_loss: 0.8102 - val_acc: 0.6528\n",
            "Epoch 70/100\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.75458\n",
            "200/200 - 0s - loss: 0.4678 - acc: 0.8850 - val_loss: 0.7577 - val_acc: 0.6944\n",
            "Epoch 71/100\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.75458\n",
            "200/200 - 0s - loss: 0.4488 - acc: 0.8950 - val_loss: 0.8028 - val_acc: 0.6528\n",
            "Epoch 72/100\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.75458\n",
            "200/200 - 0s - loss: 0.5050 - acc: 0.8500 - val_loss: 0.7776 - val_acc: 0.7083\n",
            "Epoch 73/100\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.75458\n",
            "200/200 - 0s - loss: 0.4855 - acc: 0.8800 - val_loss: 0.7589 - val_acc: 0.7083\n",
            "Epoch 74/100\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.75458\n",
            "200/200 - 0s - loss: 0.4747 - acc: 0.8650 - val_loss: 0.7812 - val_acc: 0.6806\n",
            "Epoch 75/100\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.75458\n",
            "200/200 - 0s - loss: 0.4628 - acc: 0.8650 - val_loss: 0.7734 - val_acc: 0.6806\n",
            "Epoch 76/100\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.75458\n",
            "200/200 - 0s - loss: 0.4691 - acc: 0.8650 - val_loss: 0.7790 - val_acc: 0.6389\n",
            "Epoch 77/100\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.75458\n",
            "200/200 - 0s - loss: 0.4662 - acc: 0.8650 - val_loss: 0.7630 - val_acc: 0.6944\n",
            "Epoch 78/100\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.75458\n",
            "200/200 - 0s - loss: 0.4693 - acc: 0.8600 - val_loss: 0.7605 - val_acc: 0.6944\n",
            "Epoch 79/100\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.75458\n",
            "200/200 - 0s - loss: 0.4468 - acc: 0.8850 - val_loss: 0.7643 - val_acc: 0.6944\n",
            "Epoch 80/100\n",
            "\n",
            "Epoch 00080: val_loss improved from 0.75458 to 0.67520, saving model to /tmp/checkpoint.h5\n",
            "200/200 - 0s - loss: 0.4598 - acc: 0.8750 - val_loss: 0.6752 - val_acc: 0.7083\n",
            "Epoch 81/100\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.67520\n",
            "200/200 - 0s - loss: 0.4326 - acc: 0.8950 - val_loss: 0.7488 - val_acc: 0.7083\n",
            "Epoch 82/100\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.67520\n",
            "200/200 - 0s - loss: 0.4161 - acc: 0.9150 - val_loss: 0.6843 - val_acc: 0.7361\n",
            "Epoch 83/100\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.67520\n",
            "200/200 - 0s - loss: 0.4553 - acc: 0.8800 - val_loss: 0.7506 - val_acc: 0.6944\n",
            "Epoch 84/100\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.67520\n",
            "200/200 - 0s - loss: 0.4237 - acc: 0.9150 - val_loss: 0.7007 - val_acc: 0.7361\n",
            "Epoch 85/100\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.67520\n",
            "200/200 - 0s - loss: 0.4362 - acc: 0.8800 - val_loss: 0.7197 - val_acc: 0.7083\n",
            "Epoch 86/100\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.67520\n",
            "200/200 - 0s - loss: 0.4089 - acc: 0.9000 - val_loss: 0.7200 - val_acc: 0.7083\n",
            "Epoch 87/100\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.67520\n",
            "200/200 - 0s - loss: 0.3926 - acc: 0.9450 - val_loss: 0.6993 - val_acc: 0.7361\n",
            "Epoch 88/100\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.67520\n",
            "200/200 - 0s - loss: 0.3871 - acc: 0.9100 - val_loss: 0.7020 - val_acc: 0.7222\n",
            "Epoch 89/100\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.67520\n",
            "200/200 - 0s - loss: 0.4134 - acc: 0.9050 - val_loss: 0.7054 - val_acc: 0.7500\n",
            "Epoch 90/100\n",
            "\n",
            "Epoch 00090: val_loss improved from 0.67520 to 0.65600, saving model to /tmp/checkpoint.h5\n",
            "200/200 - 0s - loss: 0.4253 - acc: 0.8750 - val_loss: 0.6560 - val_acc: 0.7778\n",
            "Epoch 91/100\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.65600\n",
            "200/200 - 0s - loss: 0.4425 - acc: 0.8600 - val_loss: 0.6977 - val_acc: 0.7500\n",
            "Epoch 92/100\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.65600\n",
            "200/200 - 0s - loss: 0.4094 - acc: 0.8800 - val_loss: 0.6803 - val_acc: 0.7500\n",
            "Epoch 93/100\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.65600\n",
            "200/200 - 0s - loss: 0.4106 - acc: 0.8850 - val_loss: 0.7188 - val_acc: 0.6667\n",
            "Epoch 94/100\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.65600\n",
            "200/200 - 0s - loss: 0.4139 - acc: 0.9100 - val_loss: 0.6936 - val_acc: 0.7778\n",
            "Epoch 95/100\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.65600\n",
            "200/200 - 0s - loss: 0.4138 - acc: 0.8950 - val_loss: 0.6986 - val_acc: 0.7639\n",
            "Epoch 96/100\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.65600\n",
            "200/200 - 0s - loss: 0.3888 - acc: 0.9300 - val_loss: 0.7883 - val_acc: 0.6806\n",
            "Epoch 97/100\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.65600\n",
            "200/200 - 0s - loss: 0.4065 - acc: 0.9000 - val_loss: 0.6657 - val_acc: 0.7778\n",
            "Epoch 98/100\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.65600\n",
            "200/200 - 0s - loss: 0.3837 - acc: 0.8950 - val_loss: 0.7339 - val_acc: 0.7083\n",
            "Epoch 99/100\n",
            "\n",
            "Epoch 00099: val_loss improved from 0.65600 to 0.64908, saving model to /tmp/checkpoint.h5\n",
            "200/200 - 0s - loss: 0.4624 - acc: 0.8550 - val_loss: 0.6491 - val_acc: 0.7639\n",
            "Epoch 100/100\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.64908\n",
            "200/200 - 0s - loss: 0.4235 - acc: 0.8950 - val_loss: 0.6853 - val_acc: 0.7639\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4aC5kY7lhys",
        "colab_type": "code",
        "outputId": "51c1f2cf-3213-44ca-96d8-f70e3fd0111b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "# Plot training & validation accuracy values\n",
        "plt.plot(history.history['acc'])\n",
        "plt.plot(history.history['val_acc'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTEJTEPCBAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}