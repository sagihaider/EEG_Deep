{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "main_Single_Trial_Classification_PatientData_CrossSubject",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sagihaider/EEG_Deep/blob/master/main_Single_Trial_Classification_PatientData_CrossSubject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X110PE9MjrlE",
        "colab_type": "code",
        "outputId": "7705c440-9ed7-4fcf-fd76-781550ca3c15",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        }
      },
      "source": [
        "!git clone https://github.com/sagihaider/EEG_Deep.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'EEG_Deep'...\n",
            "remote: Enumerating objects: 36, done.\u001b[K\n",
            "remote: Counting objects: 100% (36/36), done.\u001b[K\n",
            "remote: Compressing objects: 100% (32/32), done.\u001b[K\n",
            "remote: Total 189 (delta 18), reused 10 (delta 4), pack-reused 153\u001b[K\n",
            "Receiving objects: 100% (189/189), 857.31 MiB | 15.09 MiB/s, done.\n",
            "Resolving deltas: 100% (71/71), done.\n",
            "Checking out files: 100% (59/59), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ioLzMoiS60Qf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import scipy.io as spio\n",
        "import numpy as np\n",
        "from importlib.machinery import SourceFileLoader\n",
        "\n",
        "# EEGNet-specific imports\n",
        "from EEG_Deep.EEGModels import EEGNet, ShallowConvNet, DeepConvNet\n",
        "from tensorflow.keras import utils as np_utils\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras import backend as K\n",
        "K.set_image_data_format('channels_first')\n",
        "\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# tools for plotting confusion matrices\n",
        "from matplotlib import pyplot as plt\n",
        "from scipy.signal import butter, lfilter\n",
        "\n",
        "from numpy import array\n",
        "from sklearn.model_selection import KFold"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mo80Jg_Pn5lP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Band-pass Filter\n",
        "def butter_bandpass_filter(data, lowcut, highcut, fs, order=4):\n",
        "    nyq = 0.5 * fs\n",
        "    low = lowcut / nyq\n",
        "    high = highcut / nyq\n",
        "    b, a = butter(order, [low, high], btype='band')\n",
        "    y = lfilter(b, a, data)\n",
        "    return y\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZjtnE_6RSpum",
        "colab_type": "code",
        "outputId": "b8bf352a-1935-43dd-ee79-712ba905b39d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from numpy import zeros\n",
        "K.clear_session()\n",
        "cols = 1\n",
        "rows = 10\n",
        "acc_all = zeros([rows, cols])\n",
        "loss_all = zeros([rows, cols])\n",
        "result=[]\n",
        "count = 0\n",
        "# data sample\n",
        "data = array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
        "# prepare cross validation\n",
        "kfold = KFold(10, True, 1)\n",
        "# enumerate splits\n",
        "for train, test in kfold.split(data):\n",
        "  count = count + 1\n",
        "  # print('train: %s, test: %s' % (data[train], data[test]))\n",
        "  r_X_tr = np.empty([0, 12, 4096])\n",
        "  Y_tr = np.empty([0,1])\n",
        "  r_X_ts = np.empty([0, 12, 4096])\n",
        "  Y_ts = np.empty([0,1])\n",
        "  X_tr = np.empty([720, 12, 4096])\n",
        "  X_ts = np.empty([40, 12, 4096])\n",
        "  \n",
        "\n",
        "  for x in data[train]:\n",
        "    print(x)\n",
        "    fName = 'EEG_Deep/Data2A/parsed_P0' + str(x) + 'T.mat'  # Load Data\n",
        "    print(fName)\n",
        "    mat = spio.loadmat(fName)\n",
        "    x_tr = mat['RawEEGData']\n",
        "    y_tr = mat['Labels']\n",
        "    print(r_X_tr.shape)\n",
        "    r_X_tr=np.append(r_X_tr, x_tr, axis=0)\n",
        "    Y_tr=np.append(Y_tr, y_tr, axis=0)\n",
        "    print(r_X_tr.shape)\n",
        "    print(Y_tr.shape)\n",
        "\n",
        "\n",
        "  for x in data[test]:\n",
        "    print(x)\n",
        "    subid = x \n",
        "    fName = 'EEG_Deep/Data2A/parsed_P0' + str(x) + 'E.mat'  # Load Data\n",
        "    print(fName)\n",
        "    mat = spio.loadmat(fName)\n",
        "    x_ts = mat['RawEEGData']\n",
        "    y_ts = mat['Labels']\n",
        "    print(r_X_ts.shape)\n",
        "    r_X_ts=np.append(r_X_ts, x_ts, axis=0)\n",
        "    Y_ts=np.append(Y_ts, y_ts, axis=0)\n",
        "    print(r_X_ts.shape)\n",
        "    print(Y_ts.shape)\n",
        "\n",
        "\n",
        "  ### Filter Training Data ###\n",
        "  print(\"Filtering of training data in progress\")\n",
        "  for t in range(r_X_tr.shape[0]):\n",
        "    tril = r_X_tr[t,:,:]\n",
        "    # tril = tril.transpose()\n",
        "    tril_filtered = butter_bandpass_filter(tril, \n",
        "                                              lowcut=8, \n",
        "                                              highcut=24, \n",
        "                                              fs=512,\n",
        "                                              order=4)\n",
        "    # tril_filtered = tril_filtered.transpose()\n",
        "    X_tr[t,:,:] = tril_filtered\n",
        "  \n",
        "  print(X_tr.shape)\n",
        "\n",
        "  ### Filter Test Data Data ###\n",
        "  print(\"Filtering of test data in progress\")\n",
        "  for t in range(r_X_ts.shape[0]):\n",
        "    tril = r_X_ts[t,:,:]\n",
        "    # tril = tril.transpose()\n",
        "    tril_filtered = butter_bandpass_filter(tril, \n",
        "                                              lowcut=8, \n",
        "                                              highcut=24, \n",
        "                                              fs=512,\n",
        "                                              order=4)\n",
        "    # tril_filtered = tril_filtered.transpose()\n",
        "    X_ts[t,:,:] = tril_filtered\n",
        "\n",
        "  print(X_ts.shape)\n",
        "\n",
        "  indices = np.arange(X_tr.shape[0])\n",
        "  np.random.shuffle(indices)\n",
        "\n",
        "  X_tr = X_tr[indices]\n",
        "  Y_tr = Y_tr[indices]\n",
        "\n",
        "  # split data of each subject in training and validation\n",
        "  X_train = X_tr[0:620,:,2560:4096]\n",
        "  Y_train = Y_tr[0:620].ravel()\n",
        "  X_val   = X_tr[620:,:,2560:4096]\n",
        "  Y_val   = Y_tr[620:].ravel()\n",
        "  print(Y_val)\n",
        "  print(np.shape(X_train))\n",
        "  print(np.shape(Y_train))\n",
        "  print(np.shape(X_val))\n",
        "  print(np.shape(Y_val))\n",
        "\n",
        "  # convert labels to one-hot encodings.\n",
        "  Y_train      = np_utils.to_categorical(Y_train-1, num_classes=4)\n",
        "  Y_val       = np_utils.to_categorical(Y_val-1, num_classes=4)\n",
        "  print(Y_val)\n",
        "\n",
        "  kernels, chans, samples = 1, 12, 1536\n",
        "  # convert data to NCHW (trials, kernels, channels, samples) format. Data \n",
        "  # contains 22 channels and 500 time-points. Set the number of kernels to 1.\n",
        "  X_train = X_train.reshape(X_train.shape[0], kernels, chans, samples)\n",
        "  X_val   = X_val.reshape(X_val.shape[0], kernels, chans, samples)\n",
        "   \n",
        "  print('X_train shape:', X_train.shape)\n",
        "  print(X_train.shape[0], 'train samples')\n",
        "  print(X_val.shape[0], 'val samples')\n",
        "\n",
        "  X_test      = X_ts[:,:,2560:4096]\n",
        "  Y_test      = Y_ts[:]\n",
        "  print(np.shape(X_test))\n",
        "  print(np.shape(Y_test))\n",
        "\n",
        "  #convert labels to one-hot encodings.\n",
        "  Y_test      = np_utils.to_categorical(Y_test-1, num_classes=4)\n",
        "\n",
        "  # convert data to NCHW (trials, kernels, channels, samples) format. Data \n",
        "  # contains 22 channels and 500 time-points. Set the number of kernels to 1.\n",
        "  X_test = X_test.reshape(X_test.shape[0], kernels, chans, samples)\n",
        "  print('X_train shape:', X_test.shape)\n",
        "  print(X_test.shape[0], 'train samples')\n",
        "\n",
        "  # configure the EEGNet-8,2,16 model with kernel length of 32 samples (other \n",
        "  # model configurations may do better, but this is a good starting point)\n",
        "  model = EEGNet(nb_classes = 4, Chans = 12, Samples = 1536,\n",
        "                 dropoutRate = 0.5, kernLength = 25, F1 = 8, \n",
        "                 D = 2, F2 = 16, norm_rate = 0.25, dropoutType = 'Dropout')\n",
        "      \n",
        "  # compile the model and set the optimizers\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='adam', \n",
        "                metrics = ['accuracy'])\n",
        "\n",
        "  # count number of parameters in the model\n",
        "  numParams    = model.count_params() \n",
        "\n",
        "  # set a valid path for your system to record model checkpoints\n",
        "  checkpointer = ModelCheckpoint(filepath='/tmp/checkpoint.h5', verbose=1,\n",
        "                                 save_best_only=True)\n",
        "  \n",
        "  # the syntax is {class_1:weight_1, class_2:weight_2,...}. Here just setting\n",
        "  # the weights all to be 1\n",
        "  class_weights = {0:1, 1:1, 2:1, 3:1}\n",
        "\n",
        "  history = model.fit(X_train, Y_train, batch_size = 16, epochs = 300,\n",
        "                      verbose = 2, validation_data=(X_val, Y_val),\n",
        "                      callbacks=[checkpointer], class_weight = class_weights)\n",
        "  \n",
        "  # Plot training & validation accuracy values\n",
        "  plt.plot(history.history['acc'])\n",
        "  plt.plot(history.history['val_acc'])\n",
        "  plt.title('Model accuracy')\n",
        "  plt.ylabel('Accuracy')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.legend(['Train', 'Test'], loc='upper left')\n",
        "  plt.show()\n",
        "  figName = 'Accuracy_A0' + str(x) + '.png'  \n",
        "  plt.savefig(figName)\n",
        "\n",
        "  print('\\n# Evaluate on test data')\n",
        "  results = model.evaluate(X_test, Y_test, batch_size=1)\n",
        "  print('test loss, test acc:', results)\n",
        "\n",
        "  loss_all[subid - 1, 0] = results[0]\n",
        "  acc_all[subid - 1, 0] = results[1]\n",
        "  \n",
        "  from keras import backend as K \n",
        "  # Do some code, e.g. train and save model\n",
        "  K.clear_session()\n",
        "\n",
        "print(loss_all)\n",
        "print(acc_all)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n",
            "EEG_Deep/Data2A/parsed_P01T.mat\n",
            "(0, 12, 4096)\n",
            "(80, 12, 4096)\n",
            "(80, 1)\n",
            "2\n",
            "EEG_Deep/Data2A/parsed_P02T.mat\n",
            "(80, 12, 4096)\n",
            "(160, 12, 4096)\n",
            "(160, 1)\n",
            "4\n",
            "EEG_Deep/Data2A/parsed_P04T.mat\n",
            "(160, 12, 4096)\n",
            "(240, 12, 4096)\n",
            "(240, 1)\n",
            "5\n",
            "EEG_Deep/Data2A/parsed_P05T.mat\n",
            "(240, 12, 4096)\n",
            "(320, 12, 4096)\n",
            "(320, 1)\n",
            "6\n",
            "EEG_Deep/Data2A/parsed_P06T.mat\n",
            "(320, 12, 4096)\n",
            "(400, 12, 4096)\n",
            "(400, 1)\n",
            "7\n",
            "EEG_Deep/Data2A/parsed_P07T.mat\n",
            "(400, 12, 4096)\n",
            "(480, 12, 4096)\n",
            "(480, 1)\n",
            "8\n",
            "EEG_Deep/Data2A/parsed_P08T.mat\n",
            "(480, 12, 4096)\n",
            "(560, 12, 4096)\n",
            "(560, 1)\n",
            "9\n",
            "EEG_Deep/Data2A/parsed_P09T.mat\n",
            "(560, 12, 4096)\n",
            "(640, 12, 4096)\n",
            "(640, 1)\n",
            "10\n",
            "EEG_Deep/Data2A/parsed_P010T.mat\n",
            "(640, 12, 4096)\n",
            "(720, 12, 4096)\n",
            "(720, 1)\n",
            "3\n",
            "EEG_Deep/Data2A/parsed_P03E.mat\n",
            "(0, 12, 4096)\n",
            "(40, 12, 4096)\n",
            "(40, 1)\n",
            "Filtering of training data in progress\n",
            "(720, 12, 4096)\n",
            "Filtering of test data in progress\n",
            "(40, 12, 4096)\n",
            "[1. 1. 2. 2. 2. 2. 2. 1. 2. 2. 1. 1. 2. 1. 2. 1. 2. 1. 1. 1. 1. 1. 1. 1.\n",
            " 2. 2. 1. 1. 1. 2. 1. 2. 2. 2. 1. 1. 1. 1. 2. 1. 1. 1. 1. 2. 1. 2. 1. 2.\n",
            " 2. 2. 1. 1. 2. 1. 1. 2. 2. 1. 2. 2. 2. 2. 2. 2. 2. 1. 1. 1. 2. 2. 2. 1.\n",
            " 2. 1. 2. 1. 2. 1. 2. 2. 1. 2. 1. 1. 2. 2. 2. 2. 2. 1. 2. 1. 2. 1. 1. 1.\n",
            " 1. 1. 2. 1.]\n",
            "(620, 12, 1536)\n",
            "(620,)\n",
            "(100, 12, 1536)\n",
            "(100,)\n",
            "[[1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]]\n",
            "X_train shape: (620, 1, 12, 1536)\n",
            "620 train samples\n",
            "100 val samples\n",
            "(40, 12, 1536)\n",
            "(40, 1)\n",
            "X_train shape: (40, 1, 12, 1536)\n",
            "40 train samples\n",
            "Train on 620 samples, validate on 100 samples\n",
            "Epoch 1/300\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 1.17172, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.9610 - acc: 0.5210 - val_loss: 1.1717 - val_acc: 0.4900\n",
            "Epoch 2/300\n",
            "\n",
            "Epoch 00002: val_loss improved from 1.17172 to 1.04453, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.7662 - acc: 0.5887 - val_loss: 1.0445 - val_acc: 0.5800\n",
            "Epoch 3/300\n",
            "\n",
            "Epoch 00003: val_loss improved from 1.04453 to 0.94494, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.7202 - acc: 0.6258 - val_loss: 0.9449 - val_acc: 0.5000\n",
            "Epoch 4/300\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.94494 to 0.85751, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.6826 - acc: 0.6629 - val_loss: 0.8575 - val_acc: 0.5300\n",
            "Epoch 5/300\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.85751 to 0.75438, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.6329 - acc: 0.7032 - val_loss: 0.7544 - val_acc: 0.6800\n",
            "Epoch 6/300\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.75438 to 0.69201, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.6035 - acc: 0.7113 - val_loss: 0.6920 - val_acc: 0.6900\n",
            "Epoch 7/300\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.69201 to 0.63714, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.5868 - acc: 0.7323 - val_loss: 0.6371 - val_acc: 0.7200\n",
            "Epoch 8/300\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.63714\n",
            "620/620 - 0s - loss: 0.5690 - acc: 0.7435 - val_loss: 0.6562 - val_acc: 0.6500\n",
            "Epoch 9/300\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.63714\n",
            "620/620 - 0s - loss: 0.5674 - acc: 0.7258 - val_loss: 0.6722 - val_acc: 0.6300\n",
            "Epoch 10/300\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.63714 to 0.58831, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.5648 - acc: 0.7242 - val_loss: 0.5883 - val_acc: 0.7400\n",
            "Epoch 11/300\n",
            "\n",
            "Epoch 00011: val_loss improved from 0.58831 to 0.52763, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.5518 - acc: 0.7274 - val_loss: 0.5276 - val_acc: 0.7600\n",
            "Epoch 12/300\n",
            "\n",
            "Epoch 00012: val_loss improved from 0.52763 to 0.50776, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.5444 - acc: 0.7484 - val_loss: 0.5078 - val_acc: 0.7800\n",
            "Epoch 13/300\n",
            "\n",
            "Epoch 00013: val_loss improved from 0.50776 to 0.48382, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.5416 - acc: 0.7532 - val_loss: 0.4838 - val_acc: 0.7800\n",
            "Epoch 14/300\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.48382\n",
            "620/620 - 0s - loss: 0.5321 - acc: 0.7355 - val_loss: 0.5391 - val_acc: 0.7400\n",
            "Epoch 15/300\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.48382\n",
            "620/620 - 0s - loss: 0.5388 - acc: 0.7306 - val_loss: 0.4908 - val_acc: 0.7700\n",
            "Epoch 16/300\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.48382\n",
            "620/620 - 0s - loss: 0.5294 - acc: 0.7565 - val_loss: 0.4854 - val_acc: 0.7600\n",
            "Epoch 17/300\n",
            "\n",
            "Epoch 00017: val_loss improved from 0.48382 to 0.48302, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.5201 - acc: 0.7548 - val_loss: 0.4830 - val_acc: 0.7800\n",
            "Epoch 18/300\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.48302\n",
            "620/620 - 0s - loss: 0.5193 - acc: 0.7468 - val_loss: 0.5046 - val_acc: 0.7400\n",
            "Epoch 19/300\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.48302\n",
            "620/620 - 0s - loss: 0.5350 - acc: 0.7355 - val_loss: 0.5015 - val_acc: 0.7700\n",
            "Epoch 20/300\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.48302\n",
            "620/620 - 0s - loss: 0.4952 - acc: 0.7597 - val_loss: 0.5197 - val_acc: 0.7300\n",
            "Epoch 21/300\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.48302\n",
            "620/620 - 0s - loss: 0.5143 - acc: 0.7645 - val_loss: 0.5425 - val_acc: 0.7100\n",
            "Epoch 22/300\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.48302\n",
            "620/620 - 0s - loss: 0.4943 - acc: 0.7645 - val_loss: 0.5293 - val_acc: 0.7200\n",
            "Epoch 23/300\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.48302\n",
            "620/620 - 0s - loss: 0.5110 - acc: 0.7855 - val_loss: 0.5182 - val_acc: 0.7200\n",
            "Epoch 24/300\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.48302\n",
            "620/620 - 0s - loss: 0.5120 - acc: 0.7597 - val_loss: 0.6702 - val_acc: 0.6400\n",
            "Epoch 25/300\n",
            "\n",
            "Epoch 00025: val_loss improved from 0.48302 to 0.45646, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.4850 - acc: 0.7839 - val_loss: 0.4565 - val_acc: 0.7700\n",
            "Epoch 26/300\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.45646\n",
            "620/620 - 0s - loss: 0.4649 - acc: 0.7887 - val_loss: 0.5833 - val_acc: 0.7200\n",
            "Epoch 27/300\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.45646\n",
            "620/620 - 0s - loss: 0.4933 - acc: 0.7758 - val_loss: 0.5045 - val_acc: 0.7200\n",
            "Epoch 28/300\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.45646\n",
            "620/620 - 0s - loss: 0.5013 - acc: 0.7516 - val_loss: 0.5878 - val_acc: 0.7200\n",
            "Epoch 29/300\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.45646\n",
            "620/620 - 0s - loss: 0.4891 - acc: 0.7710 - val_loss: 0.5138 - val_acc: 0.7300\n",
            "Epoch 30/300\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.45646\n",
            "620/620 - 0s - loss: 0.4733 - acc: 0.7823 - val_loss: 0.5075 - val_acc: 0.7200\n",
            "Epoch 31/300\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.45646\n",
            "620/620 - 0s - loss: 0.4848 - acc: 0.7629 - val_loss: 0.5395 - val_acc: 0.7200\n",
            "Epoch 32/300\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.45646\n",
            "620/620 - 0s - loss: 0.5157 - acc: 0.7581 - val_loss: 0.5073 - val_acc: 0.7400\n",
            "Epoch 33/300\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.45646\n",
            "620/620 - 0s - loss: 0.4670 - acc: 0.7952 - val_loss: 0.5988 - val_acc: 0.7200\n",
            "Epoch 34/300\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.45646\n",
            "620/620 - 0s - loss: 0.4595 - acc: 0.7806 - val_loss: 0.5060 - val_acc: 0.7400\n",
            "Epoch 35/300\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.45646\n",
            "620/620 - 1s - loss: 0.4607 - acc: 0.7952 - val_loss: 0.4810 - val_acc: 0.7300\n",
            "Epoch 36/300\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.45646\n",
            "620/620 - 0s - loss: 0.4664 - acc: 0.7984 - val_loss: 0.6030 - val_acc: 0.7000\n",
            "Epoch 37/300\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.45646\n",
            "620/620 - 0s - loss: 0.4696 - acc: 0.8000 - val_loss: 0.5306 - val_acc: 0.7500\n",
            "Epoch 38/300\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.45646\n",
            "620/620 - 0s - loss: 0.4645 - acc: 0.8048 - val_loss: 0.4845 - val_acc: 0.7500\n",
            "Epoch 39/300\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.45646\n",
            "620/620 - 0s - loss: 0.4757 - acc: 0.7903 - val_loss: 0.5416 - val_acc: 0.7200\n",
            "Epoch 40/300\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.45646\n",
            "620/620 - 0s - loss: 0.4475 - acc: 0.7806 - val_loss: 0.4718 - val_acc: 0.7700\n",
            "Epoch 41/300\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.45646\n",
            "620/620 - 0s - loss: 0.4616 - acc: 0.7774 - val_loss: 0.5019 - val_acc: 0.7300\n",
            "Epoch 42/300\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.45646\n",
            "620/620 - 0s - loss: 0.4646 - acc: 0.7710 - val_loss: 0.5178 - val_acc: 0.7200\n",
            "Epoch 43/300\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.45646\n",
            "620/620 - 0s - loss: 0.4750 - acc: 0.7742 - val_loss: 0.4625 - val_acc: 0.7700\n",
            "Epoch 44/300\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.45646\n",
            "620/620 - 0s - loss: 0.4424 - acc: 0.7871 - val_loss: 0.5131 - val_acc: 0.7500\n",
            "Epoch 45/300\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.45646\n",
            "620/620 - 0s - loss: 0.4904 - acc: 0.7823 - val_loss: 0.4715 - val_acc: 0.7800\n",
            "Epoch 46/300\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.45646\n",
            "620/620 - 0s - loss: 0.4800 - acc: 0.7694 - val_loss: 0.4947 - val_acc: 0.7400\n",
            "Epoch 47/300\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.45646\n",
            "620/620 - 0s - loss: 0.4390 - acc: 0.8097 - val_loss: 0.5434 - val_acc: 0.7500\n",
            "Epoch 48/300\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.45646\n",
            "620/620 - 0s - loss: 0.4570 - acc: 0.7903 - val_loss: 0.5276 - val_acc: 0.7500\n",
            "Epoch 49/300\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.45646\n",
            "620/620 - 0s - loss: 0.4395 - acc: 0.8000 - val_loss: 0.5956 - val_acc: 0.7300\n",
            "Epoch 50/300\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.45646\n",
            "620/620 - 0s - loss: 0.4467 - acc: 0.8000 - val_loss: 0.6040 - val_acc: 0.6600\n",
            "Epoch 51/300\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.45646\n",
            "620/620 - 0s - loss: 0.4317 - acc: 0.8129 - val_loss: 0.5929 - val_acc: 0.6900\n",
            "Epoch 52/300\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.45646\n",
            "620/620 - 0s - loss: 0.4408 - acc: 0.7984 - val_loss: 0.4963 - val_acc: 0.7600\n",
            "Epoch 53/300\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.45646\n",
            "620/620 - 0s - loss: 0.4585 - acc: 0.7919 - val_loss: 0.6121 - val_acc: 0.6900\n",
            "Epoch 54/300\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.45646\n",
            "620/620 - 0s - loss: 0.4597 - acc: 0.7935 - val_loss: 0.4742 - val_acc: 0.8000\n",
            "Epoch 55/300\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.45646\n",
            "620/620 - 0s - loss: 0.4571 - acc: 0.7871 - val_loss: 0.5544 - val_acc: 0.7100\n",
            "Epoch 56/300\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.45646\n",
            "620/620 - 0s - loss: 0.4523 - acc: 0.7903 - val_loss: 0.5855 - val_acc: 0.7000\n",
            "Epoch 57/300\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.45646\n",
            "620/620 - 0s - loss: 0.4439 - acc: 0.7984 - val_loss: 0.4971 - val_acc: 0.7400\n",
            "Epoch 58/300\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.45646\n",
            "620/620 - 0s - loss: 0.4587 - acc: 0.8081 - val_loss: 0.5098 - val_acc: 0.7400\n",
            "Epoch 59/300\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.45646\n",
            "620/620 - 0s - loss: 0.4233 - acc: 0.8081 - val_loss: 0.4664 - val_acc: 0.7700\n",
            "Epoch 60/300\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.45646\n",
            "620/620 - 0s - loss: 0.4262 - acc: 0.8000 - val_loss: 0.5640 - val_acc: 0.7100\n",
            "Epoch 61/300\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.45646\n",
            "620/620 - 0s - loss: 0.4589 - acc: 0.7935 - val_loss: 0.4886 - val_acc: 0.7700\n",
            "Epoch 62/300\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.45646\n",
            "620/620 - 0s - loss: 0.4330 - acc: 0.8097 - val_loss: 0.4913 - val_acc: 0.7600\n",
            "Epoch 63/300\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.45646\n",
            "620/620 - 0s - loss: 0.4568 - acc: 0.8065 - val_loss: 0.4670 - val_acc: 0.7900\n",
            "Epoch 64/300\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.45646\n",
            "620/620 - 0s - loss: 0.4309 - acc: 0.8097 - val_loss: 0.4624 - val_acc: 0.7900\n",
            "Epoch 65/300\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.45646\n",
            "620/620 - 0s - loss: 0.4362 - acc: 0.7952 - val_loss: 0.4957 - val_acc: 0.7500\n",
            "Epoch 66/300\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.45646\n",
            "620/620 - 0s - loss: 0.4524 - acc: 0.7774 - val_loss: 0.4970 - val_acc: 0.7200\n",
            "Epoch 67/300\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.45646\n",
            "620/620 - 0s - loss: 0.4498 - acc: 0.8016 - val_loss: 0.5170 - val_acc: 0.7200\n",
            "Epoch 68/300\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.45646\n",
            "620/620 - 0s - loss: 0.4258 - acc: 0.8194 - val_loss: 0.4580 - val_acc: 0.8100\n",
            "Epoch 69/300\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.45646\n",
            "620/620 - 0s - loss: 0.4503 - acc: 0.7903 - val_loss: 0.5087 - val_acc: 0.7200\n",
            "Epoch 70/300\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.45646\n",
            "620/620 - 0s - loss: 0.4405 - acc: 0.7968 - val_loss: 0.5041 - val_acc: 0.7400\n",
            "Epoch 71/300\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.45646\n",
            "620/620 - 0s - loss: 0.4374 - acc: 0.8081 - val_loss: 0.5201 - val_acc: 0.7400\n",
            "Epoch 72/300\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.45646\n",
            "620/620 - 0s - loss: 0.4436 - acc: 0.7952 - val_loss: 0.4695 - val_acc: 0.8000\n",
            "Epoch 73/300\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.45646\n",
            "620/620 - 0s - loss: 0.4182 - acc: 0.8081 - val_loss: 0.4757 - val_acc: 0.7700\n",
            "Epoch 74/300\n",
            "\n",
            "Epoch 00074: val_loss improved from 0.45646 to 0.44989, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.4349 - acc: 0.8016 - val_loss: 0.4499 - val_acc: 0.8000\n",
            "Epoch 75/300\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.44989\n",
            "620/620 - 0s - loss: 0.4369 - acc: 0.8048 - val_loss: 0.5073 - val_acc: 0.7500\n",
            "Epoch 76/300\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.44989\n",
            "620/620 - 0s - loss: 0.4190 - acc: 0.8000 - val_loss: 0.5046 - val_acc: 0.7300\n",
            "Epoch 77/300\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.44989\n",
            "620/620 - 0s - loss: 0.3983 - acc: 0.8371 - val_loss: 0.4754 - val_acc: 0.7500\n",
            "Epoch 78/300\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.44989\n",
            "620/620 - 0s - loss: 0.4221 - acc: 0.8145 - val_loss: 0.5903 - val_acc: 0.6800\n",
            "Epoch 79/300\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.44989\n",
            "620/620 - 0s - loss: 0.4272 - acc: 0.8032 - val_loss: 0.5324 - val_acc: 0.7300\n",
            "Epoch 80/300\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.44989\n",
            "620/620 - 0s - loss: 0.4304 - acc: 0.8016 - val_loss: 0.5053 - val_acc: 0.7600\n",
            "Epoch 81/300\n",
            "\n",
            "Epoch 00081: val_loss improved from 0.44989 to 0.43496, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.4030 - acc: 0.8226 - val_loss: 0.4350 - val_acc: 0.8000\n",
            "Epoch 82/300\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.43496\n",
            "620/620 - 0s - loss: 0.3975 - acc: 0.8306 - val_loss: 0.4883 - val_acc: 0.7500\n",
            "Epoch 83/300\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.43496\n",
            "620/620 - 0s - loss: 0.4061 - acc: 0.8177 - val_loss: 0.6204 - val_acc: 0.7200\n",
            "Epoch 84/300\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.43496\n",
            "620/620 - 0s - loss: 0.4017 - acc: 0.8258 - val_loss: 0.5512 - val_acc: 0.7100\n",
            "Epoch 85/300\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.43496\n",
            "620/620 - 0s - loss: 0.4168 - acc: 0.8032 - val_loss: 0.4499 - val_acc: 0.8300\n",
            "Epoch 86/300\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.43496\n",
            "620/620 - 0s - loss: 0.4270 - acc: 0.8016 - val_loss: 0.4709 - val_acc: 0.7500\n",
            "Epoch 87/300\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.43496\n",
            "620/620 - 0s - loss: 0.3996 - acc: 0.8194 - val_loss: 0.4752 - val_acc: 0.7600\n",
            "Epoch 88/300\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.43496\n",
            "620/620 - 0s - loss: 0.4184 - acc: 0.8242 - val_loss: 0.5022 - val_acc: 0.7400\n",
            "Epoch 89/300\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.43496\n",
            "620/620 - 0s - loss: 0.4312 - acc: 0.8129 - val_loss: 0.4977 - val_acc: 0.7600\n",
            "Epoch 90/300\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.43496\n",
            "620/620 - 0s - loss: 0.3801 - acc: 0.8210 - val_loss: 0.5368 - val_acc: 0.7400\n",
            "Epoch 91/300\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.43496\n",
            "620/620 - 0s - loss: 0.4126 - acc: 0.8290 - val_loss: 0.4754 - val_acc: 0.7600\n",
            "Epoch 92/300\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.43496\n",
            "620/620 - 0s - loss: 0.4328 - acc: 0.7935 - val_loss: 0.5102 - val_acc: 0.7300\n",
            "Epoch 93/300\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.43496\n",
            "620/620 - 0s - loss: 0.4096 - acc: 0.8274 - val_loss: 0.4699 - val_acc: 0.7600\n",
            "Epoch 94/300\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.43496\n",
            "620/620 - 0s - loss: 0.4215 - acc: 0.8081 - val_loss: 0.4443 - val_acc: 0.8000\n",
            "Epoch 95/300\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.43496\n",
            "620/620 - 0s - loss: 0.3761 - acc: 0.8306 - val_loss: 0.6469 - val_acc: 0.6900\n",
            "Epoch 96/300\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.43496\n",
            "620/620 - 0s - loss: 0.4039 - acc: 0.7919 - val_loss: 0.4722 - val_acc: 0.7600\n",
            "Epoch 97/300\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.43496\n",
            "620/620 - 0s - loss: 0.3986 - acc: 0.8290 - val_loss: 0.5062 - val_acc: 0.7300\n",
            "Epoch 98/300\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.43496\n",
            "620/620 - 0s - loss: 0.4205 - acc: 0.8113 - val_loss: 0.4858 - val_acc: 0.7500\n",
            "Epoch 99/300\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.43496\n",
            "620/620 - 0s - loss: 0.3956 - acc: 0.8177 - val_loss: 0.4961 - val_acc: 0.7700\n",
            "Epoch 100/300\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.43496\n",
            "620/620 - 0s - loss: 0.4148 - acc: 0.8274 - val_loss: 0.6251 - val_acc: 0.7000\n",
            "Epoch 101/300\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.43496\n",
            "620/620 - 0s - loss: 0.4177 - acc: 0.8242 - val_loss: 0.6116 - val_acc: 0.6900\n",
            "Epoch 102/300\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.43496\n",
            "620/620 - 0s - loss: 0.3978 - acc: 0.8210 - val_loss: 0.4768 - val_acc: 0.7800\n",
            "Epoch 103/300\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.43496\n",
            "620/620 - 0s - loss: 0.4183 - acc: 0.7984 - val_loss: 0.5734 - val_acc: 0.7300\n",
            "Epoch 104/300\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.43496\n",
            "620/620 - 0s - loss: 0.3895 - acc: 0.8290 - val_loss: 0.4600 - val_acc: 0.7700\n",
            "Epoch 105/300\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 0.43496\n",
            "620/620 - 0s - loss: 0.4004 - acc: 0.8242 - val_loss: 0.5261 - val_acc: 0.7300\n",
            "Epoch 106/300\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 0.43496\n",
            "620/620 - 0s - loss: 0.4103 - acc: 0.8242 - val_loss: 0.5166 - val_acc: 0.7500\n",
            "Epoch 107/300\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.43496\n",
            "620/620 - 0s - loss: 0.4165 - acc: 0.8177 - val_loss: 0.4469 - val_acc: 0.7900\n",
            "Epoch 108/300\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 0.43496\n",
            "620/620 - 0s - loss: 0.3916 - acc: 0.8258 - val_loss: 0.4850 - val_acc: 0.7700\n",
            "Epoch 109/300\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.43496\n",
            "620/620 - 0s - loss: 0.4041 - acc: 0.8210 - val_loss: 0.4956 - val_acc: 0.7500\n",
            "Epoch 110/300\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.43496\n",
            "620/620 - 0s - loss: 0.3942 - acc: 0.8323 - val_loss: 0.5015 - val_acc: 0.7500\n",
            "Epoch 111/300\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 0.43496\n",
            "620/620 - 0s - loss: 0.4178 - acc: 0.8097 - val_loss: 0.4776 - val_acc: 0.7600\n",
            "Epoch 112/300\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 0.43496\n",
            "620/620 - 0s - loss: 0.3953 - acc: 0.8161 - val_loss: 0.6224 - val_acc: 0.6900\n",
            "Epoch 113/300\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.43496\n",
            "620/620 - 0s - loss: 0.3901 - acc: 0.8306 - val_loss: 0.5477 - val_acc: 0.7400\n",
            "Epoch 114/300\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.43496\n",
            "620/620 - 0s - loss: 0.4272 - acc: 0.7935 - val_loss: 0.4968 - val_acc: 0.7600\n",
            "Epoch 115/300\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.43496\n",
            "620/620 - 0s - loss: 0.4186 - acc: 0.8000 - val_loss: 0.4519 - val_acc: 0.8100\n",
            "Epoch 116/300\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.43496\n",
            "620/620 - 0s - loss: 0.3754 - acc: 0.8355 - val_loss: 0.5046 - val_acc: 0.7700\n",
            "Epoch 117/300\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.43496\n",
            "620/620 - 0s - loss: 0.3814 - acc: 0.8177 - val_loss: 0.5816 - val_acc: 0.7300\n",
            "Epoch 118/300\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.43496\n",
            "620/620 - 0s - loss: 0.4005 - acc: 0.8194 - val_loss: 0.4756 - val_acc: 0.7400\n",
            "Epoch 119/300\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 0.43496\n",
            "620/620 - 0s - loss: 0.3914 - acc: 0.8290 - val_loss: 0.4507 - val_acc: 0.7700\n",
            "Epoch 120/300\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.43496\n",
            "620/620 - 0s - loss: 0.3990 - acc: 0.8258 - val_loss: 0.6085 - val_acc: 0.7100\n",
            "Epoch 121/300\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.43496\n",
            "620/620 - 0s - loss: 0.3924 - acc: 0.8323 - val_loss: 0.4779 - val_acc: 0.7600\n",
            "Epoch 122/300\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.43496\n",
            "620/620 - 0s - loss: 0.4052 - acc: 0.8242 - val_loss: 0.5220 - val_acc: 0.7300\n",
            "Epoch 123/300\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 0.43496\n",
            "620/620 - 0s - loss: 0.3935 - acc: 0.8274 - val_loss: 0.4535 - val_acc: 0.8100\n",
            "Epoch 124/300\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 0.43496\n",
            "620/620 - 0s - loss: 0.3983 - acc: 0.8048 - val_loss: 0.5340 - val_acc: 0.7400\n",
            "Epoch 125/300\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 0.43496\n",
            "620/620 - 0s - loss: 0.4040 - acc: 0.8081 - val_loss: 0.5392 - val_acc: 0.7200\n",
            "Epoch 126/300\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 0.43496\n",
            "620/620 - 0s - loss: 0.3886 - acc: 0.8113 - val_loss: 0.6719 - val_acc: 0.6500\n",
            "Epoch 127/300\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 0.43496\n",
            "620/620 - 0s - loss: 0.3962 - acc: 0.8258 - val_loss: 0.6092 - val_acc: 0.6800\n",
            "Epoch 128/300\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 0.43496\n",
            "620/620 - 0s - loss: 0.3783 - acc: 0.8371 - val_loss: 0.6446 - val_acc: 0.6900\n",
            "Epoch 129/300\n",
            "\n",
            "Epoch 00129: val_loss improved from 0.43496 to 0.43264, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.3808 - acc: 0.8274 - val_loss: 0.4326 - val_acc: 0.8200\n",
            "Epoch 130/300\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 0.43264\n",
            "620/620 - 0s - loss: 0.4254 - acc: 0.8113 - val_loss: 0.4678 - val_acc: 0.7600\n",
            "Epoch 131/300\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 0.43264\n",
            "620/620 - 0s - loss: 0.3970 - acc: 0.8290 - val_loss: 0.4424 - val_acc: 0.8000\n",
            "Epoch 132/300\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 0.43264\n",
            "620/620 - 0s - loss: 0.4326 - acc: 0.8081 - val_loss: 0.5270 - val_acc: 0.7600\n",
            "Epoch 133/300\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.43264\n",
            "620/620 - 0s - loss: 0.3723 - acc: 0.8339 - val_loss: 0.4457 - val_acc: 0.8100\n",
            "Epoch 134/300\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.43264\n",
            "620/620 - 0s - loss: 0.3709 - acc: 0.8355 - val_loss: 0.5534 - val_acc: 0.7200\n",
            "Epoch 135/300\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 0.43264\n",
            "620/620 - 0s - loss: 0.3857 - acc: 0.8242 - val_loss: 0.6338 - val_acc: 0.7000\n",
            "Epoch 136/300\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.43264\n",
            "620/620 - 0s - loss: 0.3941 - acc: 0.8323 - val_loss: 0.5799 - val_acc: 0.7200\n",
            "Epoch 137/300\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 0.43264\n",
            "620/620 - 0s - loss: 0.3732 - acc: 0.8500 - val_loss: 0.5583 - val_acc: 0.7400\n",
            "Epoch 138/300\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.43264\n",
            "620/620 - 0s - loss: 0.3739 - acc: 0.8468 - val_loss: 0.4458 - val_acc: 0.8100\n",
            "Epoch 139/300\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.43264\n",
            "620/620 - 0s - loss: 0.4037 - acc: 0.8129 - val_loss: 0.4505 - val_acc: 0.7600\n",
            "Epoch 140/300\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 0.43264\n",
            "620/620 - 0s - loss: 0.3850 - acc: 0.8258 - val_loss: 0.5333 - val_acc: 0.7500\n",
            "Epoch 141/300\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.43264\n",
            "620/620 - 0s - loss: 0.3905 - acc: 0.8403 - val_loss: 0.5284 - val_acc: 0.7500\n",
            "Epoch 142/300\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.43264\n",
            "620/620 - 0s - loss: 0.4026 - acc: 0.8129 - val_loss: 0.4447 - val_acc: 0.7900\n",
            "Epoch 143/300\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 0.43264\n",
            "620/620 - 0s - loss: 0.3593 - acc: 0.8371 - val_loss: 0.4464 - val_acc: 0.7700\n",
            "Epoch 144/300\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 0.43264\n",
            "620/620 - 0s - loss: 0.3896 - acc: 0.8242 - val_loss: 0.5655 - val_acc: 0.6900\n",
            "Epoch 145/300\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.43264\n",
            "620/620 - 0s - loss: 0.3798 - acc: 0.8452 - val_loss: 0.4921 - val_acc: 0.7500\n",
            "Epoch 146/300\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.43264\n",
            "620/620 - 0s - loss: 0.3832 - acc: 0.8403 - val_loss: 0.4901 - val_acc: 0.7400\n",
            "Epoch 147/300\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.43264\n",
            "620/620 - 0s - loss: 0.3849 - acc: 0.8339 - val_loss: 0.4563 - val_acc: 0.7800\n",
            "Epoch 148/300\n",
            "\n",
            "Epoch 00148: val_loss improved from 0.43264 to 0.43244, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.3739 - acc: 0.8500 - val_loss: 0.4324 - val_acc: 0.8000\n",
            "Epoch 149/300\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.43244\n",
            "620/620 - 0s - loss: 0.3674 - acc: 0.8403 - val_loss: 0.4787 - val_acc: 0.7800\n",
            "Epoch 150/300\n",
            "\n",
            "Epoch 00150: val_loss improved from 0.43244 to 0.41060, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.3895 - acc: 0.8258 - val_loss: 0.4106 - val_acc: 0.8100\n",
            "Epoch 151/300\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.41060\n",
            "620/620 - 0s - loss: 0.3878 - acc: 0.8274 - val_loss: 0.4746 - val_acc: 0.7500\n",
            "Epoch 152/300\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.41060\n",
            "620/620 - 0s - loss: 0.3909 - acc: 0.8323 - val_loss: 0.4637 - val_acc: 0.7600\n",
            "Epoch 153/300\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 0.41060\n",
            "620/620 - 0s - loss: 0.3769 - acc: 0.8323 - val_loss: 0.5554 - val_acc: 0.7300\n",
            "Epoch 154/300\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.41060\n",
            "620/620 - 0s - loss: 0.3982 - acc: 0.8242 - val_loss: 0.4407 - val_acc: 0.7900\n",
            "Epoch 155/300\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.41060\n",
            "620/620 - 0s - loss: 0.3679 - acc: 0.8387 - val_loss: 0.4176 - val_acc: 0.8100\n",
            "Epoch 156/300\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.41060\n",
            "620/620 - 0s - loss: 0.3996 - acc: 0.8290 - val_loss: 0.4865 - val_acc: 0.7600\n",
            "Epoch 157/300\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.41060\n",
            "620/620 - 0s - loss: 0.3921 - acc: 0.8226 - val_loss: 0.4760 - val_acc: 0.7500\n",
            "Epoch 158/300\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.41060\n",
            "620/620 - 0s - loss: 0.3649 - acc: 0.8565 - val_loss: 0.4842 - val_acc: 0.7700\n",
            "Epoch 159/300\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.41060\n",
            "620/620 - 0s - loss: 0.3781 - acc: 0.8226 - val_loss: 0.5109 - val_acc: 0.7500\n",
            "Epoch 160/300\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.41060\n",
            "620/620 - 1s - loss: 0.3892 - acc: 0.8258 - val_loss: 0.5777 - val_acc: 0.6800\n",
            "Epoch 161/300\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.41060\n",
            "620/620 - 0s - loss: 0.4178 - acc: 0.8048 - val_loss: 0.5492 - val_acc: 0.7000\n",
            "Epoch 162/300\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.41060\n",
            "620/620 - 0s - loss: 0.3930 - acc: 0.8065 - val_loss: 0.5699 - val_acc: 0.6900\n",
            "Epoch 163/300\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.41060\n",
            "620/620 - 0s - loss: 0.3619 - acc: 0.8484 - val_loss: 0.5755 - val_acc: 0.7000\n",
            "Epoch 164/300\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.41060\n",
            "620/620 - 0s - loss: 0.3694 - acc: 0.8290 - val_loss: 0.4395 - val_acc: 0.8100\n",
            "Epoch 165/300\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.41060\n",
            "620/620 - 0s - loss: 0.3676 - acc: 0.8419 - val_loss: 0.4461 - val_acc: 0.8000\n",
            "Epoch 166/300\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 0.41060\n",
            "620/620 - 0s - loss: 0.3895 - acc: 0.8290 - val_loss: 0.5115 - val_acc: 0.7600\n",
            "Epoch 167/300\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.41060\n",
            "620/620 - 0s - loss: 0.3836 - acc: 0.8339 - val_loss: 0.4761 - val_acc: 0.7700\n",
            "Epoch 168/300\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.41060\n",
            "620/620 - 0s - loss: 0.3850 - acc: 0.8242 - val_loss: 0.6717 - val_acc: 0.6500\n",
            "Epoch 169/300\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 0.41060\n",
            "620/620 - 0s - loss: 0.3813 - acc: 0.8290 - val_loss: 0.5182 - val_acc: 0.7400\n",
            "Epoch 170/300\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.41060\n",
            "620/620 - 0s - loss: 0.3643 - acc: 0.8274 - val_loss: 0.4383 - val_acc: 0.7700\n",
            "Epoch 171/300\n",
            "\n",
            "Epoch 00171: val_loss improved from 0.41060 to 0.40382, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.3646 - acc: 0.8323 - val_loss: 0.4038 - val_acc: 0.8500\n",
            "Epoch 172/300\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.40382\n",
            "620/620 - 0s - loss: 0.3927 - acc: 0.8290 - val_loss: 0.4417 - val_acc: 0.8100\n",
            "Epoch 173/300\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.40382\n",
            "620/620 - 0s - loss: 0.3864 - acc: 0.8177 - val_loss: 0.4998 - val_acc: 0.7700\n",
            "Epoch 174/300\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.40382\n",
            "620/620 - 0s - loss: 0.3861 - acc: 0.8306 - val_loss: 0.4224 - val_acc: 0.8100\n",
            "Epoch 175/300\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.40382\n",
            "620/620 - 0s - loss: 0.3795 - acc: 0.8274 - val_loss: 0.4261 - val_acc: 0.7900\n",
            "Epoch 176/300\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 0.40382\n",
            "620/620 - 0s - loss: 0.4120 - acc: 0.8081 - val_loss: 0.4844 - val_acc: 0.7700\n",
            "Epoch 177/300\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.40382\n",
            "620/620 - 0s - loss: 0.4183 - acc: 0.8097 - val_loss: 0.4321 - val_acc: 0.7900\n",
            "Epoch 178/300\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.40382\n",
            "620/620 - 0s - loss: 0.3817 - acc: 0.8274 - val_loss: 0.4676 - val_acc: 0.7600\n",
            "Epoch 179/300\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.40382\n",
            "620/620 - 0s - loss: 0.3750 - acc: 0.8419 - val_loss: 0.4874 - val_acc: 0.7500\n",
            "Epoch 180/300\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.40382\n",
            "620/620 - 0s - loss: 0.3889 - acc: 0.8081 - val_loss: 0.4849 - val_acc: 0.7700\n",
            "Epoch 181/300\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.40382\n",
            "620/620 - 0s - loss: 0.3654 - acc: 0.8323 - val_loss: 0.4643 - val_acc: 0.7800\n",
            "Epoch 182/300\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.40382\n",
            "620/620 - 0s - loss: 0.3830 - acc: 0.8274 - val_loss: 0.4198 - val_acc: 0.8200\n",
            "Epoch 183/300\n",
            "\n",
            "Epoch 00183: val_loss improved from 0.40382 to 0.40173, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.3713 - acc: 0.8419 - val_loss: 0.4017 - val_acc: 0.8300\n",
            "Epoch 184/300\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 0.40173\n",
            "620/620 - 0s - loss: 0.3638 - acc: 0.8387 - val_loss: 0.4062 - val_acc: 0.8300\n",
            "Epoch 185/300\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.40173\n",
            "620/620 - 1s - loss: 0.4064 - acc: 0.8113 - val_loss: 0.5073 - val_acc: 0.7400\n",
            "Epoch 186/300\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.40173\n",
            "620/620 - 0s - loss: 0.3731 - acc: 0.8355 - val_loss: 0.4315 - val_acc: 0.7900\n",
            "Epoch 187/300\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 0.40173\n",
            "620/620 - 0s - loss: 0.3590 - acc: 0.8355 - val_loss: 0.4252 - val_acc: 0.8100\n",
            "Epoch 188/300\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.40173\n",
            "620/620 - 0s - loss: 0.3590 - acc: 0.8581 - val_loss: 0.4512 - val_acc: 0.7900\n",
            "Epoch 189/300\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.40173\n",
            "620/620 - 0s - loss: 0.3776 - acc: 0.8274 - val_loss: 0.4874 - val_acc: 0.7800\n",
            "Epoch 190/300\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.40173\n",
            "620/620 - 0s - loss: 0.4122 - acc: 0.8129 - val_loss: 0.4908 - val_acc: 0.7600\n",
            "Epoch 191/300\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.40173\n",
            "620/620 - 0s - loss: 0.3779 - acc: 0.8387 - val_loss: 0.4916 - val_acc: 0.7400\n",
            "Epoch 192/300\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 0.40173\n",
            "620/620 - 0s - loss: 0.3849 - acc: 0.8210 - val_loss: 0.6596 - val_acc: 0.6800\n",
            "Epoch 193/300\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.40173\n",
            "620/620 - 0s - loss: 0.3776 - acc: 0.8210 - val_loss: 0.4512 - val_acc: 0.7800\n",
            "Epoch 194/300\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.40173\n",
            "620/620 - 0s - loss: 0.3529 - acc: 0.8468 - val_loss: 0.5358 - val_acc: 0.7400\n",
            "Epoch 195/300\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.40173\n",
            "620/620 - 0s - loss: 0.3654 - acc: 0.8419 - val_loss: 0.4107 - val_acc: 0.8000\n",
            "Epoch 196/300\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.40173\n",
            "620/620 - 0s - loss: 0.3412 - acc: 0.8581 - val_loss: 0.4165 - val_acc: 0.8000\n",
            "Epoch 197/300\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.40173\n",
            "620/620 - 0s - loss: 0.3511 - acc: 0.8500 - val_loss: 0.4683 - val_acc: 0.7600\n",
            "Epoch 198/300\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.40173\n",
            "620/620 - 0s - loss: 0.3761 - acc: 0.8323 - val_loss: 0.5012 - val_acc: 0.7800\n",
            "Epoch 199/300\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.40173\n",
            "620/620 - 0s - loss: 0.3538 - acc: 0.8468 - val_loss: 0.4040 - val_acc: 0.8300\n",
            "Epoch 200/300\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.40173\n",
            "620/620 - 0s - loss: 0.3674 - acc: 0.8355 - val_loss: 0.4554 - val_acc: 0.7800\n",
            "Epoch 201/300\n",
            "\n",
            "Epoch 00201: val_loss did not improve from 0.40173\n",
            "620/620 - 0s - loss: 0.3413 - acc: 0.8597 - val_loss: 0.4634 - val_acc: 0.7800\n",
            "Epoch 202/300\n",
            "\n",
            "Epoch 00202: val_loss did not improve from 0.40173\n",
            "620/620 - 0s - loss: 0.3663 - acc: 0.8355 - val_loss: 0.4874 - val_acc: 0.7700\n",
            "Epoch 203/300\n",
            "\n",
            "Epoch 00203: val_loss did not improve from 0.40173\n",
            "620/620 - 0s - loss: 0.4021 - acc: 0.8194 - val_loss: 0.4323 - val_acc: 0.8100\n",
            "Epoch 204/300\n",
            "\n",
            "Epoch 00204: val_loss did not improve from 0.40173\n",
            "620/620 - 0s - loss: 0.3711 - acc: 0.8371 - val_loss: 0.6033 - val_acc: 0.6900\n",
            "Epoch 205/300\n",
            "\n",
            "Epoch 00205: val_loss did not improve from 0.40173\n",
            "620/620 - 0s - loss: 0.3432 - acc: 0.8468 - val_loss: 0.5848 - val_acc: 0.7100\n",
            "Epoch 206/300\n",
            "\n",
            "Epoch 00206: val_loss did not improve from 0.40173\n",
            "620/620 - 0s - loss: 0.3690 - acc: 0.8177 - val_loss: 0.5169 - val_acc: 0.7600\n",
            "Epoch 207/300\n",
            "\n",
            "Epoch 00207: val_loss did not improve from 0.40173\n",
            "620/620 - 0s - loss: 0.3912 - acc: 0.8226 - val_loss: 0.4640 - val_acc: 0.7900\n",
            "Epoch 208/300\n",
            "\n",
            "Epoch 00208: val_loss did not improve from 0.40173\n",
            "620/620 - 0s - loss: 0.3741 - acc: 0.8323 - val_loss: 0.4213 - val_acc: 0.8000\n",
            "Epoch 209/300\n",
            "\n",
            "Epoch 00209: val_loss did not improve from 0.40173\n",
            "620/620 - 0s - loss: 0.3586 - acc: 0.8452 - val_loss: 0.5143 - val_acc: 0.7600\n",
            "Epoch 210/300\n",
            "\n",
            "Epoch 00210: val_loss did not improve from 0.40173\n",
            "620/620 - 0s - loss: 0.3732 - acc: 0.8387 - val_loss: 0.4992 - val_acc: 0.7900\n",
            "Epoch 211/300\n",
            "\n",
            "Epoch 00211: val_loss did not improve from 0.40173\n",
            "620/620 - 0s - loss: 0.3547 - acc: 0.8323 - val_loss: 0.5101 - val_acc: 0.7400\n",
            "Epoch 212/300\n",
            "\n",
            "Epoch 00212: val_loss did not improve from 0.40173\n",
            "620/620 - 0s - loss: 0.3729 - acc: 0.8403 - val_loss: 0.5068 - val_acc: 0.7800\n",
            "Epoch 213/300\n",
            "\n",
            "Epoch 00213: val_loss did not improve from 0.40173\n",
            "620/620 - 0s - loss: 0.3496 - acc: 0.8419 - val_loss: 0.5349 - val_acc: 0.7400\n",
            "Epoch 214/300\n",
            "\n",
            "Epoch 00214: val_loss did not improve from 0.40173\n",
            "620/620 - 0s - loss: 0.3809 - acc: 0.8145 - val_loss: 0.4694 - val_acc: 0.7700\n",
            "Epoch 215/300\n",
            "\n",
            "Epoch 00215: val_loss did not improve from 0.40173\n",
            "620/620 - 0s - loss: 0.3516 - acc: 0.8629 - val_loss: 0.5352 - val_acc: 0.7600\n",
            "Epoch 216/300\n",
            "\n",
            "Epoch 00216: val_loss did not improve from 0.40173\n",
            "620/620 - 0s - loss: 0.3584 - acc: 0.8645 - val_loss: 0.5116 - val_acc: 0.7500\n",
            "Epoch 217/300\n",
            "\n",
            "Epoch 00217: val_loss did not improve from 0.40173\n",
            "620/620 - 0s - loss: 0.3621 - acc: 0.8290 - val_loss: 0.4531 - val_acc: 0.7700\n",
            "Epoch 218/300\n",
            "\n",
            "Epoch 00218: val_loss did not improve from 0.40173\n",
            "620/620 - 0s - loss: 0.3986 - acc: 0.8177 - val_loss: 0.5808 - val_acc: 0.7000\n",
            "Epoch 219/300\n",
            "\n",
            "Epoch 00219: val_loss did not improve from 0.40173\n",
            "620/620 - 0s - loss: 0.3900 - acc: 0.8145 - val_loss: 0.5373 - val_acc: 0.7400\n",
            "Epoch 220/300\n",
            "\n",
            "Epoch 00220: val_loss did not improve from 0.40173\n",
            "620/620 - 0s - loss: 0.3375 - acc: 0.8694 - val_loss: 0.4343 - val_acc: 0.8000\n",
            "Epoch 221/300\n",
            "\n",
            "Epoch 00221: val_loss did not improve from 0.40173\n",
            "620/620 - 0s - loss: 0.3563 - acc: 0.8435 - val_loss: 0.5067 - val_acc: 0.7600\n",
            "Epoch 222/300\n",
            "\n",
            "Epoch 00222: val_loss did not improve from 0.40173\n",
            "620/620 - 0s - loss: 0.3704 - acc: 0.8452 - val_loss: 0.5127 - val_acc: 0.7500\n",
            "Epoch 223/300\n",
            "\n",
            "Epoch 00223: val_loss did not improve from 0.40173\n",
            "620/620 - 0s - loss: 0.3637 - acc: 0.8468 - val_loss: 0.4962 - val_acc: 0.7500\n",
            "Epoch 224/300\n",
            "\n",
            "Epoch 00224: val_loss did not improve from 0.40173\n",
            "620/620 - 0s - loss: 0.3586 - acc: 0.8484 - val_loss: 0.4838 - val_acc: 0.7700\n",
            "Epoch 225/300\n",
            "\n",
            "Epoch 00225: val_loss did not improve from 0.40173\n",
            "620/620 - 0s - loss: 0.3747 - acc: 0.8177 - val_loss: 0.4755 - val_acc: 0.7500\n",
            "Epoch 226/300\n",
            "\n",
            "Epoch 00226: val_loss did not improve from 0.40173\n",
            "620/620 - 0s - loss: 0.3622 - acc: 0.8371 - val_loss: 0.5125 - val_acc: 0.7500\n",
            "Epoch 227/300\n",
            "\n",
            "Epoch 00227: val_loss did not improve from 0.40173\n",
            "620/620 - 0s - loss: 0.3702 - acc: 0.8403 - val_loss: 0.5224 - val_acc: 0.7600\n",
            "Epoch 228/300\n",
            "\n",
            "Epoch 00228: val_loss did not improve from 0.40173\n",
            "620/620 - 0s - loss: 0.3638 - acc: 0.8339 - val_loss: 0.4469 - val_acc: 0.7900\n",
            "Epoch 229/300\n",
            "\n",
            "Epoch 00229: val_loss did not improve from 0.40173\n",
            "620/620 - 0s - loss: 0.3581 - acc: 0.8468 - val_loss: 0.4638 - val_acc: 0.8000\n",
            "Epoch 230/300\n",
            "\n",
            "Epoch 00230: val_loss did not improve from 0.40173\n",
            "620/620 - 0s - loss: 0.3575 - acc: 0.8468 - val_loss: 0.4624 - val_acc: 0.7700\n",
            "Epoch 231/300\n",
            "\n",
            "Epoch 00231: val_loss did not improve from 0.40173\n",
            "620/620 - 0s - loss: 0.3910 - acc: 0.8403 - val_loss: 0.4386 - val_acc: 0.8000\n",
            "Epoch 232/300\n",
            "\n",
            "Epoch 00232: val_loss did not improve from 0.40173\n",
            "620/620 - 0s - loss: 0.3588 - acc: 0.8565 - val_loss: 0.4380 - val_acc: 0.8000\n",
            "Epoch 233/300\n",
            "\n",
            "Epoch 00233: val_loss did not improve from 0.40173\n",
            "620/620 - 0s - loss: 0.3512 - acc: 0.8500 - val_loss: 0.5369 - val_acc: 0.7500\n",
            "Epoch 234/300\n",
            "\n",
            "Epoch 00234: val_loss did not improve from 0.40173\n",
            "620/620 - 0s - loss: 0.3594 - acc: 0.8323 - val_loss: 0.6199 - val_acc: 0.7000\n",
            "Epoch 235/300\n",
            "\n",
            "Epoch 00235: val_loss did not improve from 0.40173\n",
            "620/620 - 0s - loss: 0.3564 - acc: 0.8548 - val_loss: 0.5439 - val_acc: 0.7300\n",
            "Epoch 236/300\n",
            "\n",
            "Epoch 00236: val_loss did not improve from 0.40173\n",
            "620/620 - 0s - loss: 0.3556 - acc: 0.8419 - val_loss: 0.4516 - val_acc: 0.8100\n",
            "Epoch 237/300\n",
            "\n",
            "Epoch 00237: val_loss did not improve from 0.40173\n",
            "620/620 - 0s - loss: 0.3706 - acc: 0.8323 - val_loss: 0.5206 - val_acc: 0.7600\n",
            "Epoch 238/300\n",
            "\n",
            "Epoch 00238: val_loss did not improve from 0.40173\n",
            "620/620 - 0s - loss: 0.3515 - acc: 0.8419 - val_loss: 0.5315 - val_acc: 0.7600\n",
            "Epoch 239/300\n",
            "\n",
            "Epoch 00239: val_loss did not improve from 0.40173\n",
            "620/620 - 0s - loss: 0.3462 - acc: 0.8581 - val_loss: 0.4055 - val_acc: 0.8200\n",
            "Epoch 240/300\n",
            "\n",
            "Epoch 00240: val_loss did not improve from 0.40173\n",
            "620/620 - 0s - loss: 0.3602 - acc: 0.8468 - val_loss: 0.4383 - val_acc: 0.8000\n",
            "Epoch 241/300\n",
            "\n",
            "Epoch 00241: val_loss did not improve from 0.40173\n",
            "620/620 - 0s - loss: 0.3688 - acc: 0.8274 - val_loss: 0.6349 - val_acc: 0.6800\n",
            "Epoch 242/300\n",
            "\n",
            "Epoch 00242: val_loss did not improve from 0.40173\n",
            "620/620 - 0s - loss: 0.3566 - acc: 0.8484 - val_loss: 0.5339 - val_acc: 0.7300\n",
            "Epoch 243/300\n",
            "\n",
            "Epoch 00243: val_loss did not improve from 0.40173\n",
            "620/620 - 0s - loss: 0.3525 - acc: 0.8452 - val_loss: 0.5142 - val_acc: 0.7200\n",
            "Epoch 244/300\n",
            "\n",
            "Epoch 00244: val_loss did not improve from 0.40173\n",
            "620/620 - 0s - loss: 0.3823 - acc: 0.8194 - val_loss: 0.5544 - val_acc: 0.7300\n",
            "Epoch 245/300\n",
            "\n",
            "Epoch 00245: val_loss did not improve from 0.40173\n",
            "620/620 - 0s - loss: 0.3894 - acc: 0.8145 - val_loss: 0.5415 - val_acc: 0.7400\n",
            "Epoch 246/300\n",
            "\n",
            "Epoch 00246: val_loss did not improve from 0.40173\n",
            "620/620 - 0s - loss: 0.3307 - acc: 0.8484 - val_loss: 0.5308 - val_acc: 0.7500\n",
            "Epoch 247/300\n",
            "\n",
            "Epoch 00247: val_loss did not improve from 0.40173\n",
            "620/620 - 0s - loss: 0.3408 - acc: 0.8581 - val_loss: 0.4548 - val_acc: 0.7900\n",
            "Epoch 248/300\n",
            "\n",
            "Epoch 00248: val_loss did not improve from 0.40173\n",
            "620/620 - 0s - loss: 0.3825 - acc: 0.8161 - val_loss: 0.4968 - val_acc: 0.7600\n",
            "Epoch 249/300\n",
            "\n",
            "Epoch 00249: val_loss did not improve from 0.40173\n",
            "620/620 - 0s - loss: 0.3585 - acc: 0.8419 - val_loss: 0.4913 - val_acc: 0.7800\n",
            "Epoch 250/300\n",
            "\n",
            "Epoch 00250: val_loss did not improve from 0.40173\n",
            "620/620 - 0s - loss: 0.3597 - acc: 0.8484 - val_loss: 0.6133 - val_acc: 0.6900\n",
            "Epoch 251/300\n",
            "\n",
            "Epoch 00251: val_loss did not improve from 0.40173\n",
            "620/620 - 0s - loss: 0.3732 - acc: 0.8403 - val_loss: 0.4478 - val_acc: 0.7900\n",
            "Epoch 252/300\n",
            "\n",
            "Epoch 00252: val_loss did not improve from 0.40173\n",
            "620/620 - 0s - loss: 0.3896 - acc: 0.8323 - val_loss: 0.5364 - val_acc: 0.7100\n",
            "Epoch 253/300\n",
            "\n",
            "Epoch 00253: val_loss did not improve from 0.40173\n",
            "620/620 - 0s - loss: 0.3584 - acc: 0.8516 - val_loss: 0.4595 - val_acc: 0.7700\n",
            "Epoch 254/300\n",
            "\n",
            "Epoch 00254: val_loss did not improve from 0.40173\n",
            "620/620 - 0s - loss: 0.3477 - acc: 0.8532 - val_loss: 0.5019 - val_acc: 0.7700\n",
            "Epoch 255/300\n",
            "\n",
            "Epoch 00255: val_loss did not improve from 0.40173\n",
            "620/620 - 0s - loss: 0.3967 - acc: 0.8226 - val_loss: 0.4362 - val_acc: 0.8100\n",
            "Epoch 256/300\n",
            "\n",
            "Epoch 00256: val_loss did not improve from 0.40173\n",
            "620/620 - 0s - loss: 0.3741 - acc: 0.8419 - val_loss: 0.4574 - val_acc: 0.7900\n",
            "Epoch 257/300\n",
            "\n",
            "Epoch 00257: val_loss did not improve from 0.40173\n",
            "620/620 - 0s - loss: 0.3587 - acc: 0.8371 - val_loss: 0.4361 - val_acc: 0.7900\n",
            "Epoch 258/300\n",
            "\n",
            "Epoch 00258: val_loss did not improve from 0.40173\n",
            "620/620 - 0s - loss: 0.3418 - acc: 0.8565 - val_loss: 0.4756 - val_acc: 0.7600\n",
            "Epoch 259/300\n",
            "\n",
            "Epoch 00259: val_loss did not improve from 0.40173\n",
            "620/620 - 0s - loss: 0.3682 - acc: 0.8565 - val_loss: 0.4263 - val_acc: 0.8200\n",
            "Epoch 260/300\n",
            "\n",
            "Epoch 00260: val_loss did not improve from 0.40173\n",
            "620/620 - 0s - loss: 0.3565 - acc: 0.8403 - val_loss: 0.5284 - val_acc: 0.7400\n",
            "Epoch 261/300\n",
            "\n",
            "Epoch 00261: val_loss did not improve from 0.40173\n",
            "620/620 - 0s - loss: 0.3769 - acc: 0.8274 - val_loss: 0.4064 - val_acc: 0.8200\n",
            "Epoch 262/300\n",
            "\n",
            "Epoch 00262: val_loss did not improve from 0.40173\n",
            "620/620 - 0s - loss: 0.3525 - acc: 0.8500 - val_loss: 0.4094 - val_acc: 0.8100\n",
            "Epoch 263/300\n",
            "\n",
            "Epoch 00263: val_loss did not improve from 0.40173\n",
            "620/620 - 0s - loss: 0.3534 - acc: 0.8323 - val_loss: 0.4397 - val_acc: 0.8200\n",
            "Epoch 264/300\n",
            "\n",
            "Epoch 00264: val_loss did not improve from 0.40173\n",
            "620/620 - 0s - loss: 0.3620 - acc: 0.8452 - val_loss: 0.4693 - val_acc: 0.7900\n",
            "Epoch 265/300\n",
            "\n",
            "Epoch 00265: val_loss did not improve from 0.40173\n",
            "620/620 - 0s - loss: 0.3669 - acc: 0.8484 - val_loss: 0.5157 - val_acc: 0.7700\n",
            "Epoch 266/300\n",
            "\n",
            "Epoch 00266: val_loss did not improve from 0.40173\n",
            "620/620 - 0s - loss: 0.3493 - acc: 0.8532 - val_loss: 0.4904 - val_acc: 0.7600\n",
            "Epoch 267/300\n",
            "\n",
            "Epoch 00267: val_loss did not improve from 0.40173\n",
            "620/620 - 0s - loss: 0.3484 - acc: 0.8435 - val_loss: 0.5037 - val_acc: 0.7600\n",
            "Epoch 268/300\n",
            "\n",
            "Epoch 00268: val_loss did not improve from 0.40173\n",
            "620/620 - 0s - loss: 0.3701 - acc: 0.8387 - val_loss: 0.4743 - val_acc: 0.7800\n",
            "Epoch 269/300\n",
            "\n",
            "Epoch 00269: val_loss did not improve from 0.40173\n",
            "620/620 - 0s - loss: 0.3592 - acc: 0.8258 - val_loss: 0.4521 - val_acc: 0.7700\n",
            "Epoch 270/300\n",
            "\n",
            "Epoch 00270: val_loss did not improve from 0.40173\n",
            "620/620 - 0s - loss: 0.3574 - acc: 0.8548 - val_loss: 0.4268 - val_acc: 0.8000\n",
            "Epoch 271/300\n",
            "\n",
            "Epoch 00271: val_loss did not improve from 0.40173\n",
            "620/620 - 0s - loss: 0.4048 - acc: 0.8145 - val_loss: 0.4803 - val_acc: 0.7800\n",
            "Epoch 272/300\n",
            "\n",
            "Epoch 00272: val_loss did not improve from 0.40173\n",
            "620/620 - 0s - loss: 0.3796 - acc: 0.8339 - val_loss: 0.5509 - val_acc: 0.7200\n",
            "Epoch 273/300\n",
            "\n",
            "Epoch 00273: val_loss did not improve from 0.40173\n",
            "620/620 - 0s - loss: 0.3501 - acc: 0.8452 - val_loss: 0.4218 - val_acc: 0.8200\n",
            "Epoch 274/300\n",
            "\n",
            "Epoch 00274: val_loss did not improve from 0.40173\n",
            "620/620 - 0s - loss: 0.3575 - acc: 0.8548 - val_loss: 0.4772 - val_acc: 0.7800\n",
            "Epoch 275/300\n",
            "\n",
            "Epoch 00275: val_loss did not improve from 0.40173\n",
            "620/620 - 0s - loss: 0.3536 - acc: 0.8484 - val_loss: 0.4655 - val_acc: 0.7900\n",
            "Epoch 276/300\n",
            "\n",
            "Epoch 00276: val_loss did not improve from 0.40173\n",
            "620/620 - 0s - loss: 0.3539 - acc: 0.8597 - val_loss: 0.4665 - val_acc: 0.7800\n",
            "Epoch 277/300\n",
            "\n",
            "Epoch 00277: val_loss did not improve from 0.40173\n",
            "620/620 - 0s - loss: 0.3323 - acc: 0.8581 - val_loss: 0.5549 - val_acc: 0.6900\n",
            "Epoch 278/300\n",
            "\n",
            "Epoch 00278: val_loss did not improve from 0.40173\n",
            "620/620 - 0s - loss: 0.3335 - acc: 0.8645 - val_loss: 0.5552 - val_acc: 0.7000\n",
            "Epoch 279/300\n",
            "\n",
            "Epoch 00279: val_loss did not improve from 0.40173\n",
            "620/620 - 0s - loss: 0.3608 - acc: 0.8548 - val_loss: 0.5713 - val_acc: 0.7100\n",
            "Epoch 280/300\n",
            "\n",
            "Epoch 00280: val_loss did not improve from 0.40173\n",
            "620/620 - 0s - loss: 0.3444 - acc: 0.8629 - val_loss: 0.4937 - val_acc: 0.7800\n",
            "Epoch 281/300\n",
            "\n",
            "Epoch 00281: val_loss did not improve from 0.40173\n",
            "620/620 - 0s - loss: 0.3689 - acc: 0.8371 - val_loss: 0.4335 - val_acc: 0.8000\n",
            "Epoch 282/300\n",
            "\n",
            "Epoch 00282: val_loss improved from 0.40173 to 0.39768, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.3776 - acc: 0.8419 - val_loss: 0.3977 - val_acc: 0.8400\n",
            "Epoch 283/300\n",
            "\n",
            "Epoch 00283: val_loss did not improve from 0.39768\n",
            "620/620 - 0s - loss: 0.3531 - acc: 0.8468 - val_loss: 0.4107 - val_acc: 0.8200\n",
            "Epoch 284/300\n",
            "\n",
            "Epoch 00284: val_loss improved from 0.39768 to 0.38662, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.3733 - acc: 0.8403 - val_loss: 0.3866 - val_acc: 0.7900\n",
            "Epoch 285/300\n",
            "\n",
            "Epoch 00285: val_loss did not improve from 0.38662\n",
            "620/620 - 0s - loss: 0.3526 - acc: 0.8435 - val_loss: 0.4851 - val_acc: 0.8000\n",
            "Epoch 286/300\n",
            "\n",
            "Epoch 00286: val_loss did not improve from 0.38662\n",
            "620/620 - 0s - loss: 0.3444 - acc: 0.8613 - val_loss: 0.5446 - val_acc: 0.7500\n",
            "Epoch 287/300\n",
            "\n",
            "Epoch 00287: val_loss did not improve from 0.38662\n",
            "620/620 - 0s - loss: 0.3596 - acc: 0.8355 - val_loss: 0.5998 - val_acc: 0.7000\n",
            "Epoch 288/300\n",
            "\n",
            "Epoch 00288: val_loss did not improve from 0.38662\n",
            "620/620 - 0s - loss: 0.3452 - acc: 0.8468 - val_loss: 0.6133 - val_acc: 0.6900\n",
            "Epoch 289/300\n",
            "\n",
            "Epoch 00289: val_loss did not improve from 0.38662\n",
            "620/620 - 0s - loss: 0.3899 - acc: 0.8306 - val_loss: 0.4263 - val_acc: 0.7800\n",
            "Epoch 290/300\n",
            "\n",
            "Epoch 00290: val_loss did not improve from 0.38662\n",
            "620/620 - 0s - loss: 0.3431 - acc: 0.8500 - val_loss: 0.5495 - val_acc: 0.7600\n",
            "Epoch 291/300\n",
            "\n",
            "Epoch 00291: val_loss did not improve from 0.38662\n",
            "620/620 - 0s - loss: 0.4112 - acc: 0.7952 - val_loss: 0.5323 - val_acc: 0.7200\n",
            "Epoch 292/300\n",
            "\n",
            "Epoch 00292: val_loss did not improve from 0.38662\n",
            "620/620 - 0s - loss: 0.3723 - acc: 0.8355 - val_loss: 0.5058 - val_acc: 0.7500\n",
            "Epoch 293/300\n",
            "\n",
            "Epoch 00293: val_loss did not improve from 0.38662\n",
            "620/620 - 0s - loss: 0.3400 - acc: 0.8565 - val_loss: 0.4469 - val_acc: 0.8400\n",
            "Epoch 294/300\n",
            "\n",
            "Epoch 00294: val_loss did not improve from 0.38662\n",
            "620/620 - 0s - loss: 0.3644 - acc: 0.8484 - val_loss: 0.4609 - val_acc: 0.8200\n",
            "Epoch 295/300\n",
            "\n",
            "Epoch 00295: val_loss did not improve from 0.38662\n",
            "620/620 - 0s - loss: 0.3416 - acc: 0.8597 - val_loss: 0.5238 - val_acc: 0.7700\n",
            "Epoch 296/300\n",
            "\n",
            "Epoch 00296: val_loss did not improve from 0.38662\n",
            "620/620 - 0s - loss: 0.3417 - acc: 0.8532 - val_loss: 0.4724 - val_acc: 0.7900\n",
            "Epoch 297/300\n",
            "\n",
            "Epoch 00297: val_loss did not improve from 0.38662\n",
            "620/620 - 0s - loss: 0.3568 - acc: 0.8452 - val_loss: 0.4311 - val_acc: 0.8400\n",
            "Epoch 298/300\n",
            "\n",
            "Epoch 00298: val_loss did not improve from 0.38662\n",
            "620/620 - 0s - loss: 0.3575 - acc: 0.8403 - val_loss: 0.4694 - val_acc: 0.8000\n",
            "Epoch 299/300\n",
            "\n",
            "Epoch 00299: val_loss did not improve from 0.38662\n",
            "620/620 - 0s - loss: 0.3443 - acc: 0.8581 - val_loss: 0.4363 - val_acc: 0.8200\n",
            "Epoch 300/300\n",
            "\n",
            "Epoch 00300: val_loss did not improve from 0.38662\n",
            "620/620 - 0s - loss: 0.3312 - acc: 0.8532 - val_loss: 0.6065 - val_acc: 0.6900\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "# Evaluate on test data\n",
            "40/40 [==============================] - 0s 3ms/sample - loss: 0.5414 - acc: 0.7250\n",
            "test loss, test acc: [0.5413805038901046, 0.725]\n",
            "1\n",
            "EEG_Deep/Data2A/parsed_P01T.mat\n",
            "(0, 12, 4096)\n",
            "(80, 12, 4096)\n",
            "(80, 1)\n",
            "2\n",
            "EEG_Deep/Data2A/parsed_P02T.mat\n",
            "(80, 12, 4096)\n",
            "(160, 12, 4096)\n",
            "(160, 1)\n",
            "3\n",
            "EEG_Deep/Data2A/parsed_P03T.mat\n",
            "(160, 12, 4096)\n",
            "(240, 12, 4096)\n",
            "(240, 1)\n",
            "4\n",
            "EEG_Deep/Data2A/parsed_P04T.mat\n",
            "(240, 12, 4096)\n",
            "(320, 12, 4096)\n",
            "(320, 1)\n",
            "5\n",
            "EEG_Deep/Data2A/parsed_P05T.mat\n",
            "(320, 12, 4096)\n",
            "(400, 12, 4096)\n",
            "(400, 1)\n",
            "6\n",
            "EEG_Deep/Data2A/parsed_P06T.mat\n",
            "(400, 12, 4096)\n",
            "(480, 12, 4096)\n",
            "(480, 1)\n",
            "7\n",
            "EEG_Deep/Data2A/parsed_P07T.mat\n",
            "(480, 12, 4096)\n",
            "(560, 12, 4096)\n",
            "(560, 1)\n",
            "8\n",
            "EEG_Deep/Data2A/parsed_P08T.mat\n",
            "(560, 12, 4096)\n",
            "(640, 12, 4096)\n",
            "(640, 1)\n",
            "9\n",
            "EEG_Deep/Data2A/parsed_P09T.mat\n",
            "(640, 12, 4096)\n",
            "(720, 12, 4096)\n",
            "(720, 1)\n",
            "10\n",
            "EEG_Deep/Data2A/parsed_P010E.mat\n",
            "(0, 12, 4096)\n",
            "(40, 12, 4096)\n",
            "(40, 1)\n",
            "Filtering of training data in progress\n",
            "(720, 12, 4096)\n",
            "Filtering of test data in progress\n",
            "(40, 12, 4096)\n",
            "[1. 2. 2. 1. 1. 2. 2. 1. 1. 2. 1. 2. 2. 1. 2. 1. 2. 1. 1. 2. 2. 2. 1. 1.\n",
            " 1. 2. 2. 1. 2. 2. 1. 1. 2. 2. 2. 1. 1. 1. 2. 2. 1. 1. 1. 2. 2. 1. 1. 1.\n",
            " 2. 1. 1. 1. 1. 1. 1. 2. 1. 2. 1. 1. 2. 2. 1. 1. 1. 1. 2. 2. 1. 2. 2. 1.\n",
            " 2. 2. 1. 1. 2. 2. 2. 1. 1. 2. 2. 1. 2. 1. 2. 1. 1. 1. 1. 1. 1. 2. 2. 2.\n",
            " 2. 1. 2. 2.]\n",
            "(620, 12, 1536)\n",
            "(620,)\n",
            "(100, 12, 1536)\n",
            "(100,)\n",
            "[[1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]]\n",
            "X_train shape: (620, 1, 12, 1536)\n",
            "620 train samples\n",
            "100 val samples\n",
            "(40, 12, 1536)\n",
            "(40, 1)\n",
            "X_train shape: (40, 1, 12, 1536)\n",
            "40 train samples\n",
            "Train on 620 samples, validate on 100 samples\n",
            "Epoch 1/300\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 1.19001, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.9536 - acc: 0.4774 - val_loss: 1.1900 - val_acc: 0.5200\n",
            "Epoch 2/300\n",
            "\n",
            "Epoch 00002: val_loss improved from 1.19001 to 1.04324, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.7721 - acc: 0.5403 - val_loss: 1.0432 - val_acc: 0.5000\n",
            "Epoch 3/300\n",
            "\n",
            "Epoch 00003: val_loss improved from 1.04324 to 0.93933, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.7270 - acc: 0.5984 - val_loss: 0.9393 - val_acc: 0.5300\n",
            "Epoch 4/300\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.93933 to 0.87342, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.6962 - acc: 0.6290 - val_loss: 0.8734 - val_acc: 0.5000\n",
            "Epoch 5/300\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.87342 to 0.81340, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.6656 - acc: 0.6484 - val_loss: 0.8134 - val_acc: 0.5000\n",
            "Epoch 6/300\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.81340 to 0.80339, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.6193 - acc: 0.7081 - val_loss: 0.8034 - val_acc: 0.4700\n",
            "Epoch 7/300\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.80339\n",
            "620/620 - 0s - loss: 0.5978 - acc: 0.7435 - val_loss: 0.8223 - val_acc: 0.5000\n",
            "Epoch 8/300\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.80339 to 0.78208, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.5850 - acc: 0.7274 - val_loss: 0.7821 - val_acc: 0.5200\n",
            "Epoch 9/300\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.78208\n",
            "620/620 - 0s - loss: 0.5670 - acc: 0.7290 - val_loss: 0.8393 - val_acc: 0.5100\n",
            "Epoch 10/300\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.78208\n",
            "620/620 - 0s - loss: 0.5644 - acc: 0.7177 - val_loss: 0.8493 - val_acc: 0.5100\n",
            "Epoch 11/300\n",
            "\n",
            "Epoch 00011: val_loss improved from 0.78208 to 0.76436, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.5428 - acc: 0.7403 - val_loss: 0.7644 - val_acc: 0.5900\n",
            "Epoch 12/300\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.76436\n",
            "620/620 - 0s - loss: 0.5333 - acc: 0.7548 - val_loss: 0.7658 - val_acc: 0.5500\n",
            "Epoch 13/300\n",
            "\n",
            "Epoch 00013: val_loss improved from 0.76436 to 0.76051, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.5580 - acc: 0.7210 - val_loss: 0.7605 - val_acc: 0.5600\n",
            "Epoch 14/300\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.76051\n",
            "620/620 - 0s - loss: 0.5593 - acc: 0.7097 - val_loss: 0.8098 - val_acc: 0.5500\n",
            "Epoch 15/300\n",
            "\n",
            "Epoch 00015: val_loss improved from 0.76051 to 0.74787, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.5538 - acc: 0.7145 - val_loss: 0.7479 - val_acc: 0.5700\n",
            "Epoch 16/300\n",
            "\n",
            "Epoch 00016: val_loss improved from 0.74787 to 0.74556, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.5300 - acc: 0.7419 - val_loss: 0.7456 - val_acc: 0.5900\n",
            "Epoch 17/300\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.74556\n",
            "620/620 - 0s - loss: 0.5022 - acc: 0.7532 - val_loss: 1.1934 - val_acc: 0.4900\n",
            "Epoch 18/300\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.74556\n",
            "620/620 - 0s - loss: 0.5211 - acc: 0.7387 - val_loss: 0.8041 - val_acc: 0.5700\n",
            "Epoch 19/300\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.74556\n",
            "620/620 - 0s - loss: 0.5288 - acc: 0.7290 - val_loss: 0.7462 - val_acc: 0.5900\n",
            "Epoch 20/300\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.74556\n",
            "620/620 - 1s - loss: 0.4984 - acc: 0.7677 - val_loss: 0.7514 - val_acc: 0.5900\n",
            "Epoch 21/300\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.74556\n",
            "620/620 - 0s - loss: 0.5010 - acc: 0.7500 - val_loss: 0.9058 - val_acc: 0.5400\n",
            "Epoch 22/300\n",
            "\n",
            "Epoch 00022: val_loss improved from 0.74556 to 0.72897, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.5113 - acc: 0.7452 - val_loss: 0.7290 - val_acc: 0.5700\n",
            "Epoch 23/300\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.72897\n",
            "620/620 - 1s - loss: 0.5054 - acc: 0.7532 - val_loss: 0.8210 - val_acc: 0.5700\n",
            "Epoch 24/300\n",
            "\n",
            "Epoch 00024: val_loss improved from 0.72897 to 0.67243, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.5016 - acc: 0.7710 - val_loss: 0.6724 - val_acc: 0.6800\n",
            "Epoch 25/300\n",
            "\n",
            "Epoch 00025: val_loss improved from 0.67243 to 0.65611, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.4850 - acc: 0.7839 - val_loss: 0.6561 - val_acc: 0.6600\n",
            "Epoch 26/300\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.65611\n",
            "620/620 - 0s - loss: 0.4813 - acc: 0.7742 - val_loss: 1.0015 - val_acc: 0.5400\n",
            "Epoch 27/300\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.65611\n",
            "620/620 - 0s - loss: 0.4815 - acc: 0.7823 - val_loss: 0.8596 - val_acc: 0.5700\n",
            "Epoch 28/300\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.65611\n",
            "620/620 - 0s - loss: 0.4995 - acc: 0.7645 - val_loss: 0.8220 - val_acc: 0.5800\n",
            "Epoch 29/300\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.65611\n",
            "620/620 - 0s - loss: 0.4740 - acc: 0.7629 - val_loss: 0.9136 - val_acc: 0.5600\n",
            "Epoch 30/300\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.65611\n",
            "620/620 - 0s - loss: 0.4817 - acc: 0.7661 - val_loss: 0.8781 - val_acc: 0.5700\n",
            "Epoch 31/300\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.65611\n",
            "620/620 - 0s - loss: 0.4464 - acc: 0.8016 - val_loss: 0.7855 - val_acc: 0.6100\n",
            "Epoch 32/300\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.65611\n",
            "620/620 - 0s - loss: 0.4727 - acc: 0.7726 - val_loss: 0.9498 - val_acc: 0.5700\n",
            "Epoch 33/300\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.65611\n",
            "620/620 - 0s - loss: 0.4894 - acc: 0.7758 - val_loss: 0.7913 - val_acc: 0.6100\n",
            "Epoch 34/300\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.65611\n",
            "620/620 - 0s - loss: 0.4800 - acc: 0.7548 - val_loss: 0.8410 - val_acc: 0.6200\n",
            "Epoch 35/300\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.65611\n",
            "620/620 - 1s - loss: 0.4342 - acc: 0.7952 - val_loss: 0.9565 - val_acc: 0.5800\n",
            "Epoch 36/300\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.65611\n",
            "620/620 - 0s - loss: 0.4603 - acc: 0.7823 - val_loss: 0.7842 - val_acc: 0.6000\n",
            "Epoch 37/300\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.65611\n",
            "620/620 - 0s - loss: 0.4819 - acc: 0.7710 - val_loss: 0.8124 - val_acc: 0.5900\n",
            "Epoch 38/300\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.65611\n",
            "620/620 - 1s - loss: 0.4481 - acc: 0.8016 - val_loss: 0.8240 - val_acc: 0.5900\n",
            "Epoch 39/300\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.65611\n",
            "620/620 - 1s - loss: 0.4431 - acc: 0.8016 - val_loss: 0.9351 - val_acc: 0.5700\n",
            "Epoch 40/300\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.65611\n",
            "620/620 - 0s - loss: 0.4483 - acc: 0.8097 - val_loss: 0.7131 - val_acc: 0.6500\n",
            "Epoch 41/300\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.65611\n",
            "620/620 - 0s - loss: 0.4638 - acc: 0.7790 - val_loss: 0.8801 - val_acc: 0.5800\n",
            "Epoch 42/300\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.65611\n",
            "620/620 - 0s - loss: 0.4493 - acc: 0.7887 - val_loss: 0.9339 - val_acc: 0.5900\n",
            "Epoch 43/300\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.65611\n",
            "620/620 - 1s - loss: 0.4557 - acc: 0.7855 - val_loss: 0.8688 - val_acc: 0.5700\n",
            "Epoch 44/300\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.65611\n",
            "620/620 - 1s - loss: 0.4816 - acc: 0.7710 - val_loss: 1.0163 - val_acc: 0.5300\n",
            "Epoch 45/300\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.65611\n",
            "620/620 - 1s - loss: 0.4450 - acc: 0.7968 - val_loss: 1.1052 - val_acc: 0.5500\n",
            "Epoch 46/300\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.65611\n",
            "620/620 - 0s - loss: 0.4230 - acc: 0.8210 - val_loss: 1.0699 - val_acc: 0.5700\n",
            "Epoch 47/300\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.65611\n",
            "620/620 - 0s - loss: 0.4211 - acc: 0.7871 - val_loss: 0.8898 - val_acc: 0.5900\n",
            "Epoch 48/300\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.65611\n",
            "620/620 - 0s - loss: 0.4301 - acc: 0.7919 - val_loss: 1.0258 - val_acc: 0.5700\n",
            "Epoch 49/300\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.65611\n",
            "620/620 - 0s - loss: 0.4505 - acc: 0.7887 - val_loss: 0.8873 - val_acc: 0.5800\n",
            "Epoch 50/300\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.65611\n",
            "620/620 - 0s - loss: 0.4354 - acc: 0.8032 - val_loss: 0.8353 - val_acc: 0.6100\n",
            "Epoch 51/300\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.65611\n",
            "620/620 - 0s - loss: 0.4076 - acc: 0.8145 - val_loss: 1.0399 - val_acc: 0.5900\n",
            "Epoch 52/300\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.65611\n",
            "620/620 - 0s - loss: 0.4178 - acc: 0.8242 - val_loss: 0.8270 - val_acc: 0.6300\n",
            "Epoch 53/300\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.65611\n",
            "620/620 - 0s - loss: 0.4473 - acc: 0.7903 - val_loss: 0.9752 - val_acc: 0.5600\n",
            "Epoch 54/300\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.65611\n",
            "620/620 - 0s - loss: 0.4361 - acc: 0.8000 - val_loss: 0.8900 - val_acc: 0.5800\n",
            "Epoch 55/300\n",
            "\n",
            "Epoch 00055: val_loss improved from 0.65611 to 0.63920, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.4348 - acc: 0.7903 - val_loss: 0.6392 - val_acc: 0.6700\n",
            "Epoch 56/300\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.4397 - acc: 0.7887 - val_loss: 0.7990 - val_acc: 0.6100\n",
            "Epoch 57/300\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.4441 - acc: 0.7742 - val_loss: 0.7065 - val_acc: 0.6800\n",
            "Epoch 58/300\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.4285 - acc: 0.7903 - val_loss: 0.8876 - val_acc: 0.5900\n",
            "Epoch 59/300\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.63920\n",
            "620/620 - 1s - loss: 0.4156 - acc: 0.8113 - val_loss: 0.7917 - val_acc: 0.5900\n",
            "Epoch 60/300\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.4419 - acc: 0.7774 - val_loss: 0.9125 - val_acc: 0.5700\n",
            "Epoch 61/300\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.4063 - acc: 0.8210 - val_loss: 1.1416 - val_acc: 0.5600\n",
            "Epoch 62/300\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.4273 - acc: 0.8081 - val_loss: 0.9170 - val_acc: 0.5700\n",
            "Epoch 63/300\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.4307 - acc: 0.8065 - val_loss: 1.0805 - val_acc: 0.5700\n",
            "Epoch 64/300\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.4395 - acc: 0.7952 - val_loss: 0.7923 - val_acc: 0.6200\n",
            "Epoch 65/300\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.4659 - acc: 0.7597 - val_loss: 0.9269 - val_acc: 0.5700\n",
            "Epoch 66/300\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.4352 - acc: 0.8081 - val_loss: 0.7126 - val_acc: 0.6500\n",
            "Epoch 67/300\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.4332 - acc: 0.8129 - val_loss: 1.3752 - val_acc: 0.5100\n",
            "Epoch 68/300\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3973 - acc: 0.8274 - val_loss: 1.1349 - val_acc: 0.5500\n",
            "Epoch 69/300\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.4084 - acc: 0.8242 - val_loss: 1.0397 - val_acc: 0.5700\n",
            "Epoch 70/300\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.4138 - acc: 0.8129 - val_loss: 0.8938 - val_acc: 0.5900\n",
            "Epoch 71/300\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.4114 - acc: 0.8242 - val_loss: 0.8131 - val_acc: 0.6300\n",
            "Epoch 72/300\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.4392 - acc: 0.8048 - val_loss: 0.9238 - val_acc: 0.5600\n",
            "Epoch 73/300\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.4259 - acc: 0.7935 - val_loss: 1.0160 - val_acc: 0.5700\n",
            "Epoch 74/300\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3966 - acc: 0.8226 - val_loss: 1.1010 - val_acc: 0.5500\n",
            "Epoch 75/300\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.4118 - acc: 0.8000 - val_loss: 0.9357 - val_acc: 0.5800\n",
            "Epoch 76/300\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.4182 - acc: 0.8065 - val_loss: 0.9046 - val_acc: 0.5700\n",
            "Epoch 77/300\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.4115 - acc: 0.7935 - val_loss: 0.9316 - val_acc: 0.5900\n",
            "Epoch 78/300\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.4072 - acc: 0.8129 - val_loss: 0.7928 - val_acc: 0.6100\n",
            "Epoch 79/300\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.4233 - acc: 0.8081 - val_loss: 1.1236 - val_acc: 0.5800\n",
            "Epoch 80/300\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.4149 - acc: 0.7984 - val_loss: 0.9129 - val_acc: 0.5700\n",
            "Epoch 81/300\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.4292 - acc: 0.7952 - val_loss: 0.9473 - val_acc: 0.5700\n",
            "Epoch 82/300\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.4208 - acc: 0.8032 - val_loss: 0.9907 - val_acc: 0.5800\n",
            "Epoch 83/300\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.4237 - acc: 0.8145 - val_loss: 0.7720 - val_acc: 0.6300\n",
            "Epoch 84/300\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.63920\n",
            "620/620 - 1s - loss: 0.4251 - acc: 0.7984 - val_loss: 1.0684 - val_acc: 0.5400\n",
            "Epoch 85/300\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.4306 - acc: 0.8081 - val_loss: 1.3321 - val_acc: 0.5300\n",
            "Epoch 86/300\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.4204 - acc: 0.7984 - val_loss: 0.9467 - val_acc: 0.5600\n",
            "Epoch 87/300\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.4079 - acc: 0.8097 - val_loss: 1.0187 - val_acc: 0.5500\n",
            "Epoch 88/300\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3903 - acc: 0.8290 - val_loss: 1.1661 - val_acc: 0.5600\n",
            "Epoch 89/300\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.4151 - acc: 0.8081 - val_loss: 0.9803 - val_acc: 0.5900\n",
            "Epoch 90/300\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.4009 - acc: 0.8113 - val_loss: 0.9615 - val_acc: 0.5700\n",
            "Epoch 91/300\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3951 - acc: 0.8194 - val_loss: 0.8824 - val_acc: 0.6100\n",
            "Epoch 92/300\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.4017 - acc: 0.8081 - val_loss: 0.9195 - val_acc: 0.5900\n",
            "Epoch 93/300\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3718 - acc: 0.8339 - val_loss: 1.0686 - val_acc: 0.5900\n",
            "Epoch 94/300\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.4033 - acc: 0.8065 - val_loss: 1.0140 - val_acc: 0.5800\n",
            "Epoch 95/300\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3852 - acc: 0.8339 - val_loss: 0.9518 - val_acc: 0.5800\n",
            "Epoch 96/300\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.63920\n",
            "620/620 - 1s - loss: 0.3737 - acc: 0.8290 - val_loss: 1.0661 - val_acc: 0.5700\n",
            "Epoch 97/300\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3992 - acc: 0.8242 - val_loss: 1.0261 - val_acc: 0.5800\n",
            "Epoch 98/300\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3937 - acc: 0.8177 - val_loss: 0.9223 - val_acc: 0.5800\n",
            "Epoch 99/300\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3841 - acc: 0.8274 - val_loss: 1.1307 - val_acc: 0.5600\n",
            "Epoch 100/300\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3983 - acc: 0.8145 - val_loss: 0.9556 - val_acc: 0.5800\n",
            "Epoch 101/300\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.4003 - acc: 0.8113 - val_loss: 0.7086 - val_acc: 0.6200\n",
            "Epoch 102/300\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3892 - acc: 0.8048 - val_loss: 1.2916 - val_acc: 0.5400\n",
            "Epoch 103/300\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3924 - acc: 0.8210 - val_loss: 1.2728 - val_acc: 0.5400\n",
            "Epoch 104/300\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.4213 - acc: 0.8048 - val_loss: 1.0220 - val_acc: 0.5600\n",
            "Epoch 105/300\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.4051 - acc: 0.8177 - val_loss: 0.9485 - val_acc: 0.6000\n",
            "Epoch 106/300\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3834 - acc: 0.8274 - val_loss: 0.9464 - val_acc: 0.5700\n",
            "Epoch 107/300\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3664 - acc: 0.8306 - val_loss: 0.9069 - val_acc: 0.5900\n",
            "Epoch 108/300\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3901 - acc: 0.8048 - val_loss: 1.0352 - val_acc: 0.5600\n",
            "Epoch 109/300\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.63920\n",
            "620/620 - 1s - loss: 0.3799 - acc: 0.8194 - val_loss: 0.9638 - val_acc: 0.5900\n",
            "Epoch 110/300\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3889 - acc: 0.8226 - val_loss: 1.0595 - val_acc: 0.5900\n",
            "Epoch 111/300\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3811 - acc: 0.8323 - val_loss: 1.0861 - val_acc: 0.5800\n",
            "Epoch 112/300\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3813 - acc: 0.8371 - val_loss: 0.8120 - val_acc: 0.6000\n",
            "Epoch 113/300\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3696 - acc: 0.8306 - val_loss: 1.1050 - val_acc: 0.5500\n",
            "Epoch 114/300\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3764 - acc: 0.8258 - val_loss: 1.1298 - val_acc: 0.5500\n",
            "Epoch 115/300\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3818 - acc: 0.8274 - val_loss: 0.9239 - val_acc: 0.6000\n",
            "Epoch 116/300\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3693 - acc: 0.8306 - val_loss: 0.7343 - val_acc: 0.6200\n",
            "Epoch 117/300\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.4340 - acc: 0.8032 - val_loss: 1.0792 - val_acc: 0.5600\n",
            "Epoch 118/300\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.4060 - acc: 0.8065 - val_loss: 1.0512 - val_acc: 0.5600\n",
            "Epoch 119/300\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3677 - acc: 0.8435 - val_loss: 1.3446 - val_acc: 0.5300\n",
            "Epoch 120/300\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3869 - acc: 0.8145 - val_loss: 0.8535 - val_acc: 0.6100\n",
            "Epoch 121/300\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.4131 - acc: 0.8032 - val_loss: 1.0634 - val_acc: 0.5500\n",
            "Epoch 122/300\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3757 - acc: 0.8274 - val_loss: 0.7753 - val_acc: 0.6300\n",
            "Epoch 123/300\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3945 - acc: 0.8145 - val_loss: 0.7699 - val_acc: 0.5900\n",
            "Epoch 124/300\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3934 - acc: 0.8258 - val_loss: 1.1072 - val_acc: 0.5800\n",
            "Epoch 125/300\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.4059 - acc: 0.8194 - val_loss: 1.0928 - val_acc: 0.5500\n",
            "Epoch 126/300\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3860 - acc: 0.8210 - val_loss: 0.7916 - val_acc: 0.6300\n",
            "Epoch 127/300\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.4369 - acc: 0.8032 - val_loss: 0.8633 - val_acc: 0.5900\n",
            "Epoch 128/300\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.4000 - acc: 0.8242 - val_loss: 1.0371 - val_acc: 0.5700\n",
            "Epoch 129/300\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3691 - acc: 0.8210 - val_loss: 0.8704 - val_acc: 0.6100\n",
            "Epoch 130/300\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3766 - acc: 0.8516 - val_loss: 1.1323 - val_acc: 0.5600\n",
            "Epoch 131/300\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3897 - acc: 0.8371 - val_loss: 0.9829 - val_acc: 0.5900\n",
            "Epoch 132/300\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3919 - acc: 0.8081 - val_loss: 0.7695 - val_acc: 0.6200\n",
            "Epoch 133/300\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3809 - acc: 0.8306 - val_loss: 1.1636 - val_acc: 0.5600\n",
            "Epoch 134/300\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.4096 - acc: 0.8194 - val_loss: 0.9533 - val_acc: 0.5900\n",
            "Epoch 135/300\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.4128 - acc: 0.8129 - val_loss: 0.7664 - val_acc: 0.6100\n",
            "Epoch 136/300\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3701 - acc: 0.8210 - val_loss: 0.9759 - val_acc: 0.5800\n",
            "Epoch 137/300\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3854 - acc: 0.8290 - val_loss: 1.1297 - val_acc: 0.5500\n",
            "Epoch 138/300\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3642 - acc: 0.8323 - val_loss: 0.8911 - val_acc: 0.6000\n",
            "Epoch 139/300\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3812 - acc: 0.8258 - val_loss: 0.9521 - val_acc: 0.5800\n",
            "Epoch 140/300\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3696 - acc: 0.8403 - val_loss: 1.0166 - val_acc: 0.5500\n",
            "Epoch 141/300\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3969 - acc: 0.8210 - val_loss: 0.8382 - val_acc: 0.6000\n",
            "Epoch 142/300\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3706 - acc: 0.8290 - val_loss: 1.0365 - val_acc: 0.5500\n",
            "Epoch 143/300\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3891 - acc: 0.8290 - val_loss: 0.9905 - val_acc: 0.5600\n",
            "Epoch 144/300\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3858 - acc: 0.8242 - val_loss: 1.0596 - val_acc: 0.5500\n",
            "Epoch 145/300\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.63920\n",
            "620/620 - 1s - loss: 0.4107 - acc: 0.8129 - val_loss: 1.0262 - val_acc: 0.5800\n",
            "Epoch 146/300\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3466 - acc: 0.8403 - val_loss: 1.0734 - val_acc: 0.5800\n",
            "Epoch 147/300\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3681 - acc: 0.8323 - val_loss: 0.8937 - val_acc: 0.6000\n",
            "Epoch 148/300\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3723 - acc: 0.8468 - val_loss: 1.1181 - val_acc: 0.5500\n",
            "Epoch 149/300\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3534 - acc: 0.8500 - val_loss: 1.2777 - val_acc: 0.5500\n",
            "Epoch 150/300\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3745 - acc: 0.8258 - val_loss: 0.9056 - val_acc: 0.5700\n",
            "Epoch 151/300\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.63920\n",
            "620/620 - 1s - loss: 0.3870 - acc: 0.8274 - val_loss: 1.0044 - val_acc: 0.5800\n",
            "Epoch 152/300\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3703 - acc: 0.8323 - val_loss: 1.2666 - val_acc: 0.5400\n",
            "Epoch 153/300\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3507 - acc: 0.8242 - val_loss: 0.9040 - val_acc: 0.5800\n",
            "Epoch 154/300\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3500 - acc: 0.8452 - val_loss: 0.9866 - val_acc: 0.5600\n",
            "Epoch 155/300\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3424 - acc: 0.8452 - val_loss: 1.0811 - val_acc: 0.5700\n",
            "Epoch 156/300\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3568 - acc: 0.8516 - val_loss: 1.2265 - val_acc: 0.5600\n",
            "Epoch 157/300\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.4012 - acc: 0.8161 - val_loss: 1.0264 - val_acc: 0.5500\n",
            "Epoch 158/300\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3486 - acc: 0.8403 - val_loss: 1.1562 - val_acc: 0.5600\n",
            "Epoch 159/300\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.4108 - acc: 0.8097 - val_loss: 1.0047 - val_acc: 0.5800\n",
            "Epoch 160/300\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3600 - acc: 0.8419 - val_loss: 0.9832 - val_acc: 0.5800\n",
            "Epoch 161/300\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3734 - acc: 0.8484 - val_loss: 1.0666 - val_acc: 0.5500\n",
            "Epoch 162/300\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3810 - acc: 0.8339 - val_loss: 1.2055 - val_acc: 0.5500\n",
            "Epoch 163/300\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3458 - acc: 0.8548 - val_loss: 0.9232 - val_acc: 0.6000\n",
            "Epoch 164/300\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3860 - acc: 0.8113 - val_loss: 1.0879 - val_acc: 0.5700\n",
            "Epoch 165/300\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3921 - acc: 0.8161 - val_loss: 0.8148 - val_acc: 0.6100\n",
            "Epoch 166/300\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3429 - acc: 0.8500 - val_loss: 1.0036 - val_acc: 0.5900\n",
            "Epoch 167/300\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3662 - acc: 0.8323 - val_loss: 1.0672 - val_acc: 0.5700\n",
            "Epoch 168/300\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3539 - acc: 0.8290 - val_loss: 1.0119 - val_acc: 0.5700\n",
            "Epoch 169/300\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3517 - acc: 0.8468 - val_loss: 1.0786 - val_acc: 0.5500\n",
            "Epoch 170/300\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3848 - acc: 0.8226 - val_loss: 0.7245 - val_acc: 0.6300\n",
            "Epoch 171/300\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3787 - acc: 0.8194 - val_loss: 0.8850 - val_acc: 0.6100\n",
            "Epoch 172/300\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.4099 - acc: 0.8129 - val_loss: 0.8978 - val_acc: 0.5900\n",
            "Epoch 173/300\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3785 - acc: 0.8274 - val_loss: 1.1978 - val_acc: 0.5400\n",
            "Epoch 174/300\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.4038 - acc: 0.8177 - val_loss: 1.2643 - val_acc: 0.5400\n",
            "Epoch 175/300\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3739 - acc: 0.8419 - val_loss: 1.3079 - val_acc: 0.5400\n",
            "Epoch 176/300\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3506 - acc: 0.8468 - val_loss: 1.1081 - val_acc: 0.5500\n",
            "Epoch 177/300\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3491 - acc: 0.8581 - val_loss: 1.2383 - val_acc: 0.5600\n",
            "Epoch 178/300\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3551 - acc: 0.8323 - val_loss: 1.0278 - val_acc: 0.5400\n",
            "Epoch 179/300\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3531 - acc: 0.8242 - val_loss: 0.8997 - val_acc: 0.5900\n",
            "Epoch 180/300\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3520 - acc: 0.8306 - val_loss: 1.1483 - val_acc: 0.5400\n",
            "Epoch 181/300\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3973 - acc: 0.8065 - val_loss: 0.8343 - val_acc: 0.6200\n",
            "Epoch 182/300\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.63920\n",
            "620/620 - 1s - loss: 0.3552 - acc: 0.8516 - val_loss: 1.2414 - val_acc: 0.5600\n",
            "Epoch 183/300\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3365 - acc: 0.8645 - val_loss: 1.1633 - val_acc: 0.5400\n",
            "Epoch 184/300\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3751 - acc: 0.8129 - val_loss: 0.8664 - val_acc: 0.6000\n",
            "Epoch 185/300\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3504 - acc: 0.8468 - val_loss: 1.1612 - val_acc: 0.5700\n",
            "Epoch 186/300\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3355 - acc: 0.8613 - val_loss: 1.2469 - val_acc: 0.5600\n",
            "Epoch 187/300\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3569 - acc: 0.8306 - val_loss: 1.1917 - val_acc: 0.5700\n",
            "Epoch 188/300\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3430 - acc: 0.8452 - val_loss: 1.3032 - val_acc: 0.5500\n",
            "Epoch 189/300\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3666 - acc: 0.8226 - val_loss: 0.8998 - val_acc: 0.5900\n",
            "Epoch 190/300\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3471 - acc: 0.8516 - val_loss: 0.9945 - val_acc: 0.5900\n",
            "Epoch 191/300\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3562 - acc: 0.8532 - val_loss: 1.0705 - val_acc: 0.5500\n",
            "Epoch 192/300\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3798 - acc: 0.8226 - val_loss: 1.0371 - val_acc: 0.5500\n",
            "Epoch 193/300\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3573 - acc: 0.8355 - val_loss: 1.0951 - val_acc: 0.5600\n",
            "Epoch 194/300\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3398 - acc: 0.8548 - val_loss: 0.9830 - val_acc: 0.6000\n",
            "Epoch 195/300\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3618 - acc: 0.8387 - val_loss: 0.7931 - val_acc: 0.5900\n",
            "Epoch 196/300\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3402 - acc: 0.8548 - val_loss: 1.0493 - val_acc: 0.5700\n",
            "Epoch 197/300\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3209 - acc: 0.8661 - val_loss: 0.9582 - val_acc: 0.5900\n",
            "Epoch 198/300\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3419 - acc: 0.8452 - val_loss: 0.9747 - val_acc: 0.6000\n",
            "Epoch 199/300\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3308 - acc: 0.8452 - val_loss: 0.9623 - val_acc: 0.5900\n",
            "Epoch 200/300\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3370 - acc: 0.8613 - val_loss: 1.0841 - val_acc: 0.5800\n",
            "Epoch 201/300\n",
            "\n",
            "Epoch 00201: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3493 - acc: 0.8435 - val_loss: 1.4755 - val_acc: 0.5300\n",
            "Epoch 202/300\n",
            "\n",
            "Epoch 00202: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3320 - acc: 0.8435 - val_loss: 1.1411 - val_acc: 0.5800\n",
            "Epoch 203/300\n",
            "\n",
            "Epoch 00203: val_loss did not improve from 0.63920\n",
            "620/620 - 1s - loss: 0.3446 - acc: 0.8435 - val_loss: 1.1220 - val_acc: 0.5700\n",
            "Epoch 204/300\n",
            "\n",
            "Epoch 00204: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3115 - acc: 0.8532 - val_loss: 1.0173 - val_acc: 0.5800\n",
            "Epoch 205/300\n",
            "\n",
            "Epoch 00205: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3552 - acc: 0.8387 - val_loss: 0.9082 - val_acc: 0.5900\n",
            "Epoch 206/300\n",
            "\n",
            "Epoch 00206: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3550 - acc: 0.8371 - val_loss: 1.3400 - val_acc: 0.5500\n",
            "Epoch 207/300\n",
            "\n",
            "Epoch 00207: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3780 - acc: 0.8258 - val_loss: 0.9885 - val_acc: 0.6000\n",
            "Epoch 208/300\n",
            "\n",
            "Epoch 00208: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3740 - acc: 0.8274 - val_loss: 1.1156 - val_acc: 0.5500\n",
            "Epoch 209/300\n",
            "\n",
            "Epoch 00209: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3734 - acc: 0.8468 - val_loss: 0.9960 - val_acc: 0.5900\n",
            "Epoch 210/300\n",
            "\n",
            "Epoch 00210: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3433 - acc: 0.8435 - val_loss: 1.0458 - val_acc: 0.5600\n",
            "Epoch 211/300\n",
            "\n",
            "Epoch 00211: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3605 - acc: 0.8274 - val_loss: 0.9378 - val_acc: 0.5800\n",
            "Epoch 212/300\n",
            "\n",
            "Epoch 00212: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3637 - acc: 0.8355 - val_loss: 1.2402 - val_acc: 0.5500\n",
            "Epoch 213/300\n",
            "\n",
            "Epoch 00213: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3646 - acc: 0.8210 - val_loss: 0.9715 - val_acc: 0.5700\n",
            "Epoch 214/300\n",
            "\n",
            "Epoch 00214: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3680 - acc: 0.8226 - val_loss: 1.2404 - val_acc: 0.5500\n",
            "Epoch 215/300\n",
            "\n",
            "Epoch 00215: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3471 - acc: 0.8435 - val_loss: 1.2413 - val_acc: 0.5500\n",
            "Epoch 216/300\n",
            "\n",
            "Epoch 00216: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3495 - acc: 0.8516 - val_loss: 1.3010 - val_acc: 0.5500\n",
            "Epoch 217/300\n",
            "\n",
            "Epoch 00217: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.4053 - acc: 0.8048 - val_loss: 0.9447 - val_acc: 0.5800\n",
            "Epoch 218/300\n",
            "\n",
            "Epoch 00218: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3736 - acc: 0.8306 - val_loss: 1.0336 - val_acc: 0.5800\n",
            "Epoch 219/300\n",
            "\n",
            "Epoch 00219: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3479 - acc: 0.8500 - val_loss: 0.7311 - val_acc: 0.6100\n",
            "Epoch 220/300\n",
            "\n",
            "Epoch 00220: val_loss did not improve from 0.63920\n",
            "620/620 - 1s - loss: 0.3663 - acc: 0.8484 - val_loss: 1.1489 - val_acc: 0.5500\n",
            "Epoch 221/300\n",
            "\n",
            "Epoch 00221: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3487 - acc: 0.8516 - val_loss: 1.0733 - val_acc: 0.5600\n",
            "Epoch 222/300\n",
            "\n",
            "Epoch 00222: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3345 - acc: 0.8516 - val_loss: 0.8390 - val_acc: 0.6200\n",
            "Epoch 223/300\n",
            "\n",
            "Epoch 00223: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3448 - acc: 0.8323 - val_loss: 1.2070 - val_acc: 0.5400\n",
            "Epoch 224/300\n",
            "\n",
            "Epoch 00224: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3519 - acc: 0.8355 - val_loss: 0.8008 - val_acc: 0.6200\n",
            "Epoch 225/300\n",
            "\n",
            "Epoch 00225: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3793 - acc: 0.8274 - val_loss: 0.9635 - val_acc: 0.5800\n",
            "Epoch 226/300\n",
            "\n",
            "Epoch 00226: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3736 - acc: 0.8435 - val_loss: 1.4558 - val_acc: 0.5400\n",
            "Epoch 227/300\n",
            "\n",
            "Epoch 00227: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3653 - acc: 0.8290 - val_loss: 1.1310 - val_acc: 0.5700\n",
            "Epoch 228/300\n",
            "\n",
            "Epoch 00228: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3528 - acc: 0.8435 - val_loss: 0.8611 - val_acc: 0.5900\n",
            "Epoch 229/300\n",
            "\n",
            "Epoch 00229: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3629 - acc: 0.8145 - val_loss: 1.0020 - val_acc: 0.5700\n",
            "Epoch 230/300\n",
            "\n",
            "Epoch 00230: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3372 - acc: 0.8597 - val_loss: 1.0580 - val_acc: 0.5700\n",
            "Epoch 231/300\n",
            "\n",
            "Epoch 00231: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3135 - acc: 0.8484 - val_loss: 0.9973 - val_acc: 0.5700\n",
            "Epoch 232/300\n",
            "\n",
            "Epoch 00232: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3579 - acc: 0.8226 - val_loss: 1.3523 - val_acc: 0.5400\n",
            "Epoch 233/300\n",
            "\n",
            "Epoch 00233: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3398 - acc: 0.8516 - val_loss: 1.2246 - val_acc: 0.5500\n",
            "Epoch 234/300\n",
            "\n",
            "Epoch 00234: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3509 - acc: 0.8371 - val_loss: 1.2731 - val_acc: 0.5400\n",
            "Epoch 235/300\n",
            "\n",
            "Epoch 00235: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3589 - acc: 0.8258 - val_loss: 1.0312 - val_acc: 0.5500\n",
            "Epoch 236/300\n",
            "\n",
            "Epoch 00236: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3448 - acc: 0.8581 - val_loss: 1.1851 - val_acc: 0.5700\n",
            "Epoch 237/300\n",
            "\n",
            "Epoch 00237: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3570 - acc: 0.8403 - val_loss: 1.3210 - val_acc: 0.5400\n",
            "Epoch 238/300\n",
            "\n",
            "Epoch 00238: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3423 - acc: 0.8581 - val_loss: 0.9850 - val_acc: 0.5800\n",
            "Epoch 239/300\n",
            "\n",
            "Epoch 00239: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3386 - acc: 0.8484 - val_loss: 0.9938 - val_acc: 0.5800\n",
            "Epoch 240/300\n",
            "\n",
            "Epoch 00240: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3185 - acc: 0.8565 - val_loss: 1.2601 - val_acc: 0.5500\n",
            "Epoch 241/300\n",
            "\n",
            "Epoch 00241: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3608 - acc: 0.8355 - val_loss: 1.0467 - val_acc: 0.5600\n",
            "Epoch 242/300\n",
            "\n",
            "Epoch 00242: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3677 - acc: 0.8306 - val_loss: 0.9016 - val_acc: 0.5900\n",
            "Epoch 243/300\n",
            "\n",
            "Epoch 00243: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3719 - acc: 0.8355 - val_loss: 1.1611 - val_acc: 0.5500\n",
            "Epoch 244/300\n",
            "\n",
            "Epoch 00244: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3176 - acc: 0.8726 - val_loss: 1.2268 - val_acc: 0.5500\n",
            "Epoch 245/300\n",
            "\n",
            "Epoch 00245: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3242 - acc: 0.8597 - val_loss: 1.2370 - val_acc: 0.5400\n",
            "Epoch 246/300\n",
            "\n",
            "Epoch 00246: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3043 - acc: 0.8790 - val_loss: 1.0580 - val_acc: 0.5900\n",
            "Epoch 247/300\n",
            "\n",
            "Epoch 00247: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3330 - acc: 0.8435 - val_loss: 1.0039 - val_acc: 0.5500\n",
            "Epoch 248/300\n",
            "\n",
            "Epoch 00248: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3203 - acc: 0.8581 - val_loss: 0.9656 - val_acc: 0.5900\n",
            "Epoch 249/300\n",
            "\n",
            "Epoch 00249: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3345 - acc: 0.8548 - val_loss: 1.0482 - val_acc: 0.5600\n",
            "Epoch 250/300\n",
            "\n",
            "Epoch 00250: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3703 - acc: 0.8274 - val_loss: 0.9916 - val_acc: 0.5800\n",
            "Epoch 251/300\n",
            "\n",
            "Epoch 00251: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3283 - acc: 0.8516 - val_loss: 1.0738 - val_acc: 0.5700\n",
            "Epoch 252/300\n",
            "\n",
            "Epoch 00252: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3558 - acc: 0.8355 - val_loss: 0.9754 - val_acc: 0.5700\n",
            "Epoch 253/300\n",
            "\n",
            "Epoch 00253: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3461 - acc: 0.8306 - val_loss: 0.9856 - val_acc: 0.5800\n",
            "Epoch 254/300\n",
            "\n",
            "Epoch 00254: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3525 - acc: 0.8226 - val_loss: 1.1184 - val_acc: 0.5700\n",
            "Epoch 255/300\n",
            "\n",
            "Epoch 00255: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3713 - acc: 0.8387 - val_loss: 0.9109 - val_acc: 0.5900\n",
            "Epoch 256/300\n",
            "\n",
            "Epoch 00256: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3534 - acc: 0.8435 - val_loss: 0.9294 - val_acc: 0.5900\n",
            "Epoch 257/300\n",
            "\n",
            "Epoch 00257: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3324 - acc: 0.8500 - val_loss: 1.1930 - val_acc: 0.5500\n",
            "Epoch 258/300\n",
            "\n",
            "Epoch 00258: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3212 - acc: 0.8435 - val_loss: 1.0298 - val_acc: 0.5800\n",
            "Epoch 259/300\n",
            "\n",
            "Epoch 00259: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3403 - acc: 0.8516 - val_loss: 1.1217 - val_acc: 0.5600\n",
            "Epoch 260/300\n",
            "\n",
            "Epoch 00260: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3626 - acc: 0.8323 - val_loss: 1.2394 - val_acc: 0.5400\n",
            "Epoch 261/300\n",
            "\n",
            "Epoch 00261: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3490 - acc: 0.8403 - val_loss: 1.1446 - val_acc: 0.5800\n",
            "Epoch 262/300\n",
            "\n",
            "Epoch 00262: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3473 - acc: 0.8565 - val_loss: 1.2529 - val_acc: 0.5600\n",
            "Epoch 263/300\n",
            "\n",
            "Epoch 00263: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3474 - acc: 0.8435 - val_loss: 1.2354 - val_acc: 0.5700\n",
            "Epoch 264/300\n",
            "\n",
            "Epoch 00264: val_loss did not improve from 0.63920\n",
            "620/620 - 1s - loss: 0.3245 - acc: 0.8565 - val_loss: 1.2773 - val_acc: 0.5500\n",
            "Epoch 265/300\n",
            "\n",
            "Epoch 00265: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3329 - acc: 0.8694 - val_loss: 1.1635 - val_acc: 0.5400\n",
            "Epoch 266/300\n",
            "\n",
            "Epoch 00266: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3470 - acc: 0.8645 - val_loss: 1.0370 - val_acc: 0.5700\n",
            "Epoch 267/300\n",
            "\n",
            "Epoch 00267: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3557 - acc: 0.8290 - val_loss: 0.9975 - val_acc: 0.5700\n",
            "Epoch 268/300\n",
            "\n",
            "Epoch 00268: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3484 - acc: 0.8339 - val_loss: 1.1693 - val_acc: 0.5600\n",
            "Epoch 269/300\n",
            "\n",
            "Epoch 00269: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3676 - acc: 0.8484 - val_loss: 0.7844 - val_acc: 0.5900\n",
            "Epoch 270/300\n",
            "\n",
            "Epoch 00270: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3416 - acc: 0.8435 - val_loss: 1.1202 - val_acc: 0.5600\n",
            "Epoch 271/300\n",
            "\n",
            "Epoch 00271: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3318 - acc: 0.8565 - val_loss: 1.1711 - val_acc: 0.5600\n",
            "Epoch 272/300\n",
            "\n",
            "Epoch 00272: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3397 - acc: 0.8468 - val_loss: 0.9690 - val_acc: 0.5700\n",
            "Epoch 273/300\n",
            "\n",
            "Epoch 00273: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3250 - acc: 0.8500 - val_loss: 1.0403 - val_acc: 0.5800\n",
            "Epoch 274/300\n",
            "\n",
            "Epoch 00274: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3590 - acc: 0.8548 - val_loss: 1.2166 - val_acc: 0.5600\n",
            "Epoch 275/300\n",
            "\n",
            "Epoch 00275: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3487 - acc: 0.8435 - val_loss: 1.0665 - val_acc: 0.5700\n",
            "Epoch 276/300\n",
            "\n",
            "Epoch 00276: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3380 - acc: 0.8565 - val_loss: 1.3849 - val_acc: 0.5400\n",
            "Epoch 277/300\n",
            "\n",
            "Epoch 00277: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3619 - acc: 0.8371 - val_loss: 1.1169 - val_acc: 0.5500\n",
            "Epoch 278/300\n",
            "\n",
            "Epoch 00278: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3749 - acc: 0.8274 - val_loss: 0.9799 - val_acc: 0.5800\n",
            "Epoch 279/300\n",
            "\n",
            "Epoch 00279: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3393 - acc: 0.8581 - val_loss: 1.0166 - val_acc: 0.5900\n",
            "Epoch 280/300\n",
            "\n",
            "Epoch 00280: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3276 - acc: 0.8548 - val_loss: 1.0338 - val_acc: 0.5700\n",
            "Epoch 281/300\n",
            "\n",
            "Epoch 00281: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3183 - acc: 0.8532 - val_loss: 1.0349 - val_acc: 0.5600\n",
            "Epoch 282/300\n",
            "\n",
            "Epoch 00282: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3183 - acc: 0.8694 - val_loss: 0.9069 - val_acc: 0.6100\n",
            "Epoch 283/300\n",
            "\n",
            "Epoch 00283: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3257 - acc: 0.8435 - val_loss: 1.1673 - val_acc: 0.5600\n",
            "Epoch 284/300\n",
            "\n",
            "Epoch 00284: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3367 - acc: 0.8500 - val_loss: 0.9119 - val_acc: 0.5900\n",
            "Epoch 285/300\n",
            "\n",
            "Epoch 00285: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3339 - acc: 0.8403 - val_loss: 0.9315 - val_acc: 0.5900\n",
            "Epoch 286/300\n",
            "\n",
            "Epoch 00286: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3557 - acc: 0.8516 - val_loss: 1.0287 - val_acc: 0.5600\n",
            "Epoch 287/300\n",
            "\n",
            "Epoch 00287: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3614 - acc: 0.8548 - val_loss: 1.0294 - val_acc: 0.5400\n",
            "Epoch 288/300\n",
            "\n",
            "Epoch 00288: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3087 - acc: 0.8677 - val_loss: 1.1564 - val_acc: 0.5700\n",
            "Epoch 289/300\n",
            "\n",
            "Epoch 00289: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.2989 - acc: 0.8758 - val_loss: 1.3776 - val_acc: 0.5400\n",
            "Epoch 290/300\n",
            "\n",
            "Epoch 00290: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3358 - acc: 0.8371 - val_loss: 0.9704 - val_acc: 0.5700\n",
            "Epoch 291/300\n",
            "\n",
            "Epoch 00291: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3064 - acc: 0.8661 - val_loss: 1.0305 - val_acc: 0.5700\n",
            "Epoch 292/300\n",
            "\n",
            "Epoch 00292: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3345 - acc: 0.8565 - val_loss: 0.8927 - val_acc: 0.5900\n",
            "Epoch 293/300\n",
            "\n",
            "Epoch 00293: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3408 - acc: 0.8403 - val_loss: 1.2934 - val_acc: 0.5300\n",
            "Epoch 294/300\n",
            "\n",
            "Epoch 00294: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3294 - acc: 0.8645 - val_loss: 1.1148 - val_acc: 0.5700\n",
            "Epoch 295/300\n",
            "\n",
            "Epoch 00295: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3913 - acc: 0.8000 - val_loss: 0.9638 - val_acc: 0.5700\n",
            "Epoch 296/300\n",
            "\n",
            "Epoch 00296: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3479 - acc: 0.8435 - val_loss: 0.9963 - val_acc: 0.5800\n",
            "Epoch 297/300\n",
            "\n",
            "Epoch 00297: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3395 - acc: 0.8452 - val_loss: 0.8857 - val_acc: 0.5800\n",
            "Epoch 298/300\n",
            "\n",
            "Epoch 00298: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3418 - acc: 0.8435 - val_loss: 1.0141 - val_acc: 0.5900\n",
            "Epoch 299/300\n",
            "\n",
            "Epoch 00299: val_loss did not improve from 0.63920\n",
            "620/620 - 0s - loss: 0.3800 - acc: 0.8226 - val_loss: 0.8633 - val_acc: 0.5700\n",
            "Epoch 300/300\n",
            "\n",
            "Epoch 00300: val_loss did not improve from 0.63920\n",
            "620/620 - 1s - loss: 0.3390 - acc: 0.8435 - val_loss: 1.3601 - val_acc: 0.5500\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "# Evaluate on test data\n",
            "40/40 [==============================] - 0s 3ms/sample - loss: 1.3366 - acc: 0.5000\n",
            "test loss, test acc: [1.3365937612019478, 0.5]\n",
            "1\n",
            "EEG_Deep/Data2A/parsed_P01T.mat\n",
            "(0, 12, 4096)\n",
            "(80, 12, 4096)\n",
            "(80, 1)\n",
            "2\n",
            "EEG_Deep/Data2A/parsed_P02T.mat\n",
            "(80, 12, 4096)\n",
            "(160, 12, 4096)\n",
            "(160, 1)\n",
            "3\n",
            "EEG_Deep/Data2A/parsed_P03T.mat\n",
            "(160, 12, 4096)\n",
            "(240, 12, 4096)\n",
            "(240, 1)\n",
            "4\n",
            "EEG_Deep/Data2A/parsed_P04T.mat\n",
            "(240, 12, 4096)\n",
            "(320, 12, 4096)\n",
            "(320, 1)\n",
            "5\n",
            "EEG_Deep/Data2A/parsed_P05T.mat\n",
            "(320, 12, 4096)\n",
            "(400, 12, 4096)\n",
            "(400, 1)\n",
            "6\n",
            "EEG_Deep/Data2A/parsed_P06T.mat\n",
            "(400, 12, 4096)\n",
            "(480, 12, 4096)\n",
            "(480, 1)\n",
            "8\n",
            "EEG_Deep/Data2A/parsed_P08T.mat\n",
            "(480, 12, 4096)\n",
            "(560, 12, 4096)\n",
            "(560, 1)\n",
            "9\n",
            "EEG_Deep/Data2A/parsed_P09T.mat\n",
            "(560, 12, 4096)\n",
            "(640, 12, 4096)\n",
            "(640, 1)\n",
            "10\n",
            "EEG_Deep/Data2A/parsed_P010T.mat\n",
            "(640, 12, 4096)\n",
            "(720, 12, 4096)\n",
            "(720, 1)\n",
            "7\n",
            "EEG_Deep/Data2A/parsed_P07E.mat\n",
            "(0, 12, 4096)\n",
            "(40, 12, 4096)\n",
            "(40, 1)\n",
            "Filtering of training data in progress\n",
            "(720, 12, 4096)\n",
            "Filtering of test data in progress\n",
            "(40, 12, 4096)\n",
            "[1. 2. 1. 2. 2. 2. 1. 2. 1. 2. 1. 2. 1. 1. 1. 2. 2. 1. 2. 1. 1. 2. 1. 2.\n",
            " 2. 2. 1. 1. 2. 2. 1. 1. 1. 1. 2. 1. 1. 1. 1. 1. 1. 2. 2. 2. 2. 2. 1. 2.\n",
            " 2. 1. 2. 1. 1. 2. 1. 2. 1. 2. 2. 2. 1. 1. 2. 1. 2. 1. 1. 2. 2. 2. 2. 1.\n",
            " 1. 1. 1. 1. 1. 1. 2. 1. 2. 1. 2. 1. 2. 2. 1. 1. 2. 2. 2. 1. 1. 2. 1. 1.\n",
            " 1. 1. 2. 2.]\n",
            "(620, 12, 1536)\n",
            "(620,)\n",
            "(100, 12, 1536)\n",
            "(100,)\n",
            "[[1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]]\n",
            "X_train shape: (620, 1, 12, 1536)\n",
            "620 train samples\n",
            "100 val samples\n",
            "(40, 12, 1536)\n",
            "(40, 1)\n",
            "X_train shape: (40, 1, 12, 1536)\n",
            "40 train samples\n",
            "Train on 620 samples, validate on 100 samples\n",
            "Epoch 1/300\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 1.16620, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.9770 - acc: 0.5242 - val_loss: 1.1662 - val_acc: 0.4600\n",
            "Epoch 2/300\n",
            "\n",
            "Epoch 00002: val_loss improved from 1.16620 to 1.04062, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.7757 - acc: 0.5710 - val_loss: 1.0406 - val_acc: 0.5000\n",
            "Epoch 3/300\n",
            "\n",
            "Epoch 00003: val_loss improved from 1.04062 to 0.93962, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.7323 - acc: 0.6242 - val_loss: 0.9396 - val_acc: 0.4700\n",
            "Epoch 4/300\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.93962 to 0.84745, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.7092 - acc: 0.6065 - val_loss: 0.8475 - val_acc: 0.6000\n",
            "Epoch 5/300\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.84745 to 0.77311, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.6836 - acc: 0.6500 - val_loss: 0.7731 - val_acc: 0.6300\n",
            "Epoch 6/300\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.77311 to 0.70748, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.6523 - acc: 0.6903 - val_loss: 0.7075 - val_acc: 0.6800\n",
            "Epoch 7/300\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.70748 to 0.65152, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.6006 - acc: 0.7161 - val_loss: 0.6515 - val_acc: 0.7500\n",
            "Epoch 8/300\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.65152 to 0.63912, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.6003 - acc: 0.7129 - val_loss: 0.6391 - val_acc: 0.7100\n",
            "Epoch 9/300\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.63912 to 0.59040, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.5796 - acc: 0.7097 - val_loss: 0.5904 - val_acc: 0.7800\n",
            "Epoch 10/300\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.59040 to 0.57588, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.5765 - acc: 0.7177 - val_loss: 0.5759 - val_acc: 0.7800\n",
            "Epoch 11/300\n",
            "\n",
            "Epoch 00011: val_loss improved from 0.57588 to 0.55274, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.5671 - acc: 0.7210 - val_loss: 0.5527 - val_acc: 0.7500\n",
            "Epoch 12/300\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.55274\n",
            "620/620 - 0s - loss: 0.5416 - acc: 0.7484 - val_loss: 0.5607 - val_acc: 0.7800\n",
            "Epoch 13/300\n",
            "\n",
            "Epoch 00013: val_loss improved from 0.55274 to 0.53100, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.5311 - acc: 0.7694 - val_loss: 0.5310 - val_acc: 0.7500\n",
            "Epoch 14/300\n",
            "\n",
            "Epoch 00014: val_loss improved from 0.53100 to 0.52332, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.5095 - acc: 0.7613 - val_loss: 0.5233 - val_acc: 0.7500\n",
            "Epoch 15/300\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.5415 - acc: 0.7339 - val_loss: 0.5363 - val_acc: 0.7400\n",
            "Epoch 16/300\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.5269 - acc: 0.7452 - val_loss: 0.6837 - val_acc: 0.6400\n",
            "Epoch 17/300\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.5200 - acc: 0.7516 - val_loss: 0.6255 - val_acc: 0.6600\n",
            "Epoch 18/300\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.5154 - acc: 0.7677 - val_loss: 0.5507 - val_acc: 0.7500\n",
            "Epoch 19/300\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.5123 - acc: 0.7532 - val_loss: 0.5788 - val_acc: 0.7200\n",
            "Epoch 20/300\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.4798 - acc: 0.7694 - val_loss: 0.6185 - val_acc: 0.7100\n",
            "Epoch 21/300\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.4806 - acc: 0.7871 - val_loss: 0.5579 - val_acc: 0.7400\n",
            "Epoch 22/300\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.5014 - acc: 0.7694 - val_loss: 0.5963 - val_acc: 0.7500\n",
            "Epoch 23/300\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.4986 - acc: 0.7597 - val_loss: 0.5376 - val_acc: 0.7800\n",
            "Epoch 24/300\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.5144 - acc: 0.7516 - val_loss: 0.6907 - val_acc: 0.6400\n",
            "Epoch 25/300\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.4760 - acc: 0.7919 - val_loss: 0.7434 - val_acc: 0.6100\n",
            "Epoch 26/300\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.4653 - acc: 0.7871 - val_loss: 0.6854 - val_acc: 0.6600\n",
            "Epoch 27/300\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.5159 - acc: 0.7613 - val_loss: 0.5390 - val_acc: 0.7700\n",
            "Epoch 28/300\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.4951 - acc: 0.7613 - val_loss: 0.5499 - val_acc: 0.7600\n",
            "Epoch 29/300\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.4936 - acc: 0.7565 - val_loss: 0.7687 - val_acc: 0.5800\n",
            "Epoch 30/300\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.4906 - acc: 0.7806 - val_loss: 0.7080 - val_acc: 0.6300\n",
            "Epoch 31/300\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.4697 - acc: 0.7758 - val_loss: 0.7518 - val_acc: 0.6200\n",
            "Epoch 32/300\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.4571 - acc: 0.7790 - val_loss: 0.6804 - val_acc: 0.6600\n",
            "Epoch 33/300\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.4720 - acc: 0.7823 - val_loss: 0.6368 - val_acc: 0.6600\n",
            "Epoch 34/300\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.4691 - acc: 0.7839 - val_loss: 0.5677 - val_acc: 0.7500\n",
            "Epoch 35/300\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.4869 - acc: 0.7710 - val_loss: 0.5435 - val_acc: 0.7700\n",
            "Epoch 36/300\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.4728 - acc: 0.7742 - val_loss: 0.7096 - val_acc: 0.6500\n",
            "Epoch 37/300\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.4428 - acc: 0.8032 - val_loss: 0.5387 - val_acc: 0.7900\n",
            "Epoch 38/300\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.4719 - acc: 0.7710 - val_loss: 0.5726 - val_acc: 0.7600\n",
            "Epoch 39/300\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.4639 - acc: 0.7710 - val_loss: 0.6603 - val_acc: 0.6800\n",
            "Epoch 40/300\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.4702 - acc: 0.7790 - val_loss: 0.6439 - val_acc: 0.6900\n",
            "Epoch 41/300\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.4465 - acc: 0.8065 - val_loss: 0.6970 - val_acc: 0.6600\n",
            "Epoch 42/300\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.4561 - acc: 0.7855 - val_loss: 0.5418 - val_acc: 0.7600\n",
            "Epoch 43/300\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.4785 - acc: 0.7758 - val_loss: 0.8043 - val_acc: 0.5900\n",
            "Epoch 44/300\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.4656 - acc: 0.7790 - val_loss: 0.6406 - val_acc: 0.6900\n",
            "Epoch 45/300\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.4303 - acc: 0.7952 - val_loss: 0.6724 - val_acc: 0.6500\n",
            "Epoch 46/300\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.4476 - acc: 0.7935 - val_loss: 0.6267 - val_acc: 0.7400\n",
            "Epoch 47/300\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.4399 - acc: 0.7968 - val_loss: 0.7829 - val_acc: 0.5900\n",
            "Epoch 48/300\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.4661 - acc: 0.7855 - val_loss: 0.5839 - val_acc: 0.7400\n",
            "Epoch 49/300\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.4329 - acc: 0.7968 - val_loss: 0.8635 - val_acc: 0.5800\n",
            "Epoch 50/300\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.4445 - acc: 0.7968 - val_loss: 0.6195 - val_acc: 0.7100\n",
            "Epoch 51/300\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.4278 - acc: 0.8097 - val_loss: 0.9692 - val_acc: 0.5400\n",
            "Epoch 52/300\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.4423 - acc: 0.8113 - val_loss: 0.6262 - val_acc: 0.6900\n",
            "Epoch 53/300\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.4348 - acc: 0.8016 - val_loss: 0.8396 - val_acc: 0.5900\n",
            "Epoch 54/300\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.4362 - acc: 0.7742 - val_loss: 0.8065 - val_acc: 0.6000\n",
            "Epoch 55/300\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.4352 - acc: 0.7968 - val_loss: 0.7528 - val_acc: 0.5900\n",
            "Epoch 56/300\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.4188 - acc: 0.8210 - val_loss: 0.6542 - val_acc: 0.6500\n",
            "Epoch 57/300\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.4086 - acc: 0.8161 - val_loss: 0.8432 - val_acc: 0.6100\n",
            "Epoch 58/300\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.4424 - acc: 0.7871 - val_loss: 0.7607 - val_acc: 0.6300\n",
            "Epoch 59/300\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.4223 - acc: 0.8145 - val_loss: 0.7379 - val_acc: 0.6300\n",
            "Epoch 60/300\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.4405 - acc: 0.7952 - val_loss: 0.6867 - val_acc: 0.6400\n",
            "Epoch 61/300\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.4231 - acc: 0.8048 - val_loss: 0.8152 - val_acc: 0.6000\n",
            "Epoch 62/300\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.4421 - acc: 0.7903 - val_loss: 0.6937 - val_acc: 0.6700\n",
            "Epoch 63/300\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.4314 - acc: 0.8032 - val_loss: 0.5748 - val_acc: 0.7700\n",
            "Epoch 64/300\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.4439 - acc: 0.8016 - val_loss: 0.9168 - val_acc: 0.5300\n",
            "Epoch 65/300\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.4189 - acc: 0.8065 - val_loss: 0.7522 - val_acc: 0.6300\n",
            "Epoch 66/300\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.4584 - acc: 0.7806 - val_loss: 0.7571 - val_acc: 0.6200\n",
            "Epoch 67/300\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.4214 - acc: 0.8113 - val_loss: 0.6040 - val_acc: 0.6900\n",
            "Epoch 68/300\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.4252 - acc: 0.8226 - val_loss: 0.8773 - val_acc: 0.5600\n",
            "Epoch 69/300\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.4163 - acc: 0.8081 - val_loss: 0.8415 - val_acc: 0.6000\n",
            "Epoch 70/300\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3995 - acc: 0.8113 - val_loss: 0.9102 - val_acc: 0.5500\n",
            "Epoch 71/300\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.4374 - acc: 0.7968 - val_loss: 0.6142 - val_acc: 0.7200\n",
            "Epoch 72/300\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.4050 - acc: 0.8129 - val_loss: 0.6128 - val_acc: 0.7200\n",
            "Epoch 73/300\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3985 - acc: 0.8048 - val_loss: 0.8188 - val_acc: 0.5900\n",
            "Epoch 74/300\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3982 - acc: 0.8210 - val_loss: 0.6045 - val_acc: 0.7400\n",
            "Epoch 75/300\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.4332 - acc: 0.7935 - val_loss: 0.9700 - val_acc: 0.5500\n",
            "Epoch 76/300\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.4184 - acc: 0.7984 - val_loss: 0.6675 - val_acc: 0.6900\n",
            "Epoch 77/300\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.52332\n",
            "620/620 - 1s - loss: 0.4406 - acc: 0.8194 - val_loss: 0.8448 - val_acc: 0.5700\n",
            "Epoch 78/300\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.4228 - acc: 0.8081 - val_loss: 0.7330 - val_acc: 0.6600\n",
            "Epoch 79/300\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.4068 - acc: 0.8081 - val_loss: 0.7319 - val_acc: 0.6300\n",
            "Epoch 80/300\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.4369 - acc: 0.7935 - val_loss: 0.8055 - val_acc: 0.6100\n",
            "Epoch 81/300\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.4254 - acc: 0.8016 - val_loss: 0.7726 - val_acc: 0.6300\n",
            "Epoch 82/300\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.4232 - acc: 0.8161 - val_loss: 0.8013 - val_acc: 0.5700\n",
            "Epoch 83/300\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.4170 - acc: 0.8145 - val_loss: 0.8337 - val_acc: 0.5700\n",
            "Epoch 84/300\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.4190 - acc: 0.8032 - val_loss: 0.6687 - val_acc: 0.7000\n",
            "Epoch 85/300\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.4365 - acc: 0.8226 - val_loss: 0.8415 - val_acc: 0.5700\n",
            "Epoch 86/300\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.4329 - acc: 0.7952 - val_loss: 0.8484 - val_acc: 0.5600\n",
            "Epoch 87/300\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3785 - acc: 0.8484 - val_loss: 0.8176 - val_acc: 0.6000\n",
            "Epoch 88/300\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.4116 - acc: 0.8177 - val_loss: 0.8275 - val_acc: 0.5800\n",
            "Epoch 89/300\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3923 - acc: 0.8306 - val_loss: 0.7322 - val_acc: 0.6500\n",
            "Epoch 90/300\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3935 - acc: 0.8258 - val_loss: 0.8140 - val_acc: 0.6200\n",
            "Epoch 91/300\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3603 - acc: 0.8435 - val_loss: 0.8017 - val_acc: 0.6200\n",
            "Epoch 92/300\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.4083 - acc: 0.8048 - val_loss: 1.1184 - val_acc: 0.5600\n",
            "Epoch 93/300\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.4226 - acc: 0.8032 - val_loss: 0.6360 - val_acc: 0.6900\n",
            "Epoch 94/300\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3900 - acc: 0.8242 - val_loss: 0.7099 - val_acc: 0.6600\n",
            "Epoch 95/300\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.4458 - acc: 0.7806 - val_loss: 0.7047 - val_acc: 0.6600\n",
            "Epoch 96/300\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3979 - acc: 0.8194 - val_loss: 0.8041 - val_acc: 0.6400\n",
            "Epoch 97/300\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3908 - acc: 0.8194 - val_loss: 0.7868 - val_acc: 0.6200\n",
            "Epoch 98/300\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.52332\n",
            "620/620 - 1s - loss: 0.4070 - acc: 0.8032 - val_loss: 0.7388 - val_acc: 0.6300\n",
            "Epoch 99/300\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.4310 - acc: 0.7984 - val_loss: 0.7606 - val_acc: 0.6200\n",
            "Epoch 100/300\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.4103 - acc: 0.8210 - val_loss: 0.6666 - val_acc: 0.7000\n",
            "Epoch 101/300\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.4196 - acc: 0.8081 - val_loss: 0.7523 - val_acc: 0.6200\n",
            "Epoch 102/300\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3999 - acc: 0.8097 - val_loss: 0.6220 - val_acc: 0.7400\n",
            "Epoch 103/300\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.4173 - acc: 0.8065 - val_loss: 0.7613 - val_acc: 0.6300\n",
            "Epoch 104/300\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3870 - acc: 0.8226 - val_loss: 0.7687 - val_acc: 0.6200\n",
            "Epoch 105/300\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3944 - acc: 0.8323 - val_loss: 0.7934 - val_acc: 0.6100\n",
            "Epoch 106/300\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.4259 - acc: 0.7968 - val_loss: 0.6299 - val_acc: 0.7300\n",
            "Epoch 107/300\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.4057 - acc: 0.8290 - val_loss: 0.8483 - val_acc: 0.6000\n",
            "Epoch 108/300\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.4113 - acc: 0.8323 - val_loss: 0.6197 - val_acc: 0.7100\n",
            "Epoch 109/300\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.4190 - acc: 0.7968 - val_loss: 0.7938 - val_acc: 0.6100\n",
            "Epoch 110/300\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3963 - acc: 0.8258 - val_loss: 0.6862 - val_acc: 0.6400\n",
            "Epoch 111/300\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.4037 - acc: 0.8065 - val_loss: 1.0439 - val_acc: 0.5500\n",
            "Epoch 112/300\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3809 - acc: 0.8419 - val_loss: 1.0061 - val_acc: 0.5400\n",
            "Epoch 113/300\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3876 - acc: 0.8194 - val_loss: 0.6042 - val_acc: 0.7400\n",
            "Epoch 114/300\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.4015 - acc: 0.8210 - val_loss: 0.8969 - val_acc: 0.5900\n",
            "Epoch 115/300\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3845 - acc: 0.8290 - val_loss: 0.6176 - val_acc: 0.7100\n",
            "Epoch 116/300\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3991 - acc: 0.8355 - val_loss: 0.6547 - val_acc: 0.6900\n",
            "Epoch 117/300\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.4216 - acc: 0.8048 - val_loss: 0.6577 - val_acc: 0.7000\n",
            "Epoch 118/300\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.4094 - acc: 0.8032 - val_loss: 0.8382 - val_acc: 0.6000\n",
            "Epoch 119/300\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3840 - acc: 0.8306 - val_loss: 0.8543 - val_acc: 0.6000\n",
            "Epoch 120/300\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3677 - acc: 0.8452 - val_loss: 0.6732 - val_acc: 0.7000\n",
            "Epoch 121/300\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.4186 - acc: 0.8129 - val_loss: 0.8317 - val_acc: 0.5900\n",
            "Epoch 122/300\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3981 - acc: 0.8242 - val_loss: 0.8856 - val_acc: 0.6000\n",
            "Epoch 123/300\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3880 - acc: 0.8274 - val_loss: 0.8745 - val_acc: 0.5900\n",
            "Epoch 124/300\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3704 - acc: 0.8323 - val_loss: 0.8601 - val_acc: 0.6000\n",
            "Epoch 125/300\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.4083 - acc: 0.8032 - val_loss: 0.9808 - val_acc: 0.5500\n",
            "Epoch 126/300\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3850 - acc: 0.8274 - val_loss: 0.7675 - val_acc: 0.6700\n",
            "Epoch 127/300\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3768 - acc: 0.8484 - val_loss: 0.8023 - val_acc: 0.6400\n",
            "Epoch 128/300\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3714 - acc: 0.8210 - val_loss: 0.6039 - val_acc: 0.7400\n",
            "Epoch 129/300\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3774 - acc: 0.8306 - val_loss: 0.8156 - val_acc: 0.6100\n",
            "Epoch 130/300\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3676 - acc: 0.8403 - val_loss: 0.7516 - val_acc: 0.6100\n",
            "Epoch 131/300\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3782 - acc: 0.8194 - val_loss: 0.7122 - val_acc: 0.6700\n",
            "Epoch 132/300\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3961 - acc: 0.8177 - val_loss: 0.7692 - val_acc: 0.6500\n",
            "Epoch 133/300\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3940 - acc: 0.8371 - val_loss: 0.6778 - val_acc: 0.6800\n",
            "Epoch 134/300\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3806 - acc: 0.8210 - val_loss: 0.9182 - val_acc: 0.5900\n",
            "Epoch 135/300\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3522 - acc: 0.8629 - val_loss: 0.8976 - val_acc: 0.5800\n",
            "Epoch 136/300\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3602 - acc: 0.8419 - val_loss: 0.9213 - val_acc: 0.5800\n",
            "Epoch 137/300\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.4031 - acc: 0.8000 - val_loss: 0.7766 - val_acc: 0.6500\n",
            "Epoch 138/300\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3589 - acc: 0.8484 - val_loss: 0.6187 - val_acc: 0.7500\n",
            "Epoch 139/300\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3502 - acc: 0.8565 - val_loss: 0.8650 - val_acc: 0.6200\n",
            "Epoch 140/300\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3680 - acc: 0.8452 - val_loss: 0.6351 - val_acc: 0.7500\n",
            "Epoch 141/300\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.4287 - acc: 0.7903 - val_loss: 0.5845 - val_acc: 0.7600\n",
            "Epoch 142/300\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.4283 - acc: 0.8000 - val_loss: 0.7829 - val_acc: 0.6200\n",
            "Epoch 143/300\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3660 - acc: 0.8419 - val_loss: 0.8350 - val_acc: 0.6200\n",
            "Epoch 144/300\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3840 - acc: 0.8274 - val_loss: 0.8589 - val_acc: 0.6000\n",
            "Epoch 145/300\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3870 - acc: 0.8274 - val_loss: 0.6683 - val_acc: 0.6900\n",
            "Epoch 146/300\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3698 - acc: 0.8290 - val_loss: 0.6828 - val_acc: 0.6900\n",
            "Epoch 147/300\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3768 - acc: 0.8258 - val_loss: 0.7678 - val_acc: 0.6200\n",
            "Epoch 148/300\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3943 - acc: 0.8274 - val_loss: 0.7196 - val_acc: 0.6300\n",
            "Epoch 149/300\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3835 - acc: 0.8210 - val_loss: 0.7325 - val_acc: 0.6700\n",
            "Epoch 150/300\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3888 - acc: 0.8242 - val_loss: 0.6630 - val_acc: 0.7000\n",
            "Epoch 151/300\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3752 - acc: 0.8387 - val_loss: 0.8710 - val_acc: 0.6100\n",
            "Epoch 152/300\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3934 - acc: 0.8177 - val_loss: 0.9072 - val_acc: 0.5800\n",
            "Epoch 153/300\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3679 - acc: 0.8226 - val_loss: 0.8352 - val_acc: 0.6000\n",
            "Epoch 154/300\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3849 - acc: 0.8129 - val_loss: 1.0577 - val_acc: 0.5500\n",
            "Epoch 155/300\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3782 - acc: 0.8274 - val_loss: 0.7441 - val_acc: 0.6600\n",
            "Epoch 156/300\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3780 - acc: 0.8290 - val_loss: 1.0368 - val_acc: 0.5400\n",
            "Epoch 157/300\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3887 - acc: 0.8306 - val_loss: 0.8260 - val_acc: 0.6100\n",
            "Epoch 158/300\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3615 - acc: 0.8419 - val_loss: 0.9059 - val_acc: 0.5600\n",
            "Epoch 159/300\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3530 - acc: 0.8419 - val_loss: 0.5891 - val_acc: 0.7600\n",
            "Epoch 160/300\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3791 - acc: 0.8258 - val_loss: 0.7281 - val_acc: 0.6700\n",
            "Epoch 161/300\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3877 - acc: 0.8371 - val_loss: 0.7429 - val_acc: 0.6900\n",
            "Epoch 162/300\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3684 - acc: 0.8516 - val_loss: 1.0145 - val_acc: 0.5700\n",
            "Epoch 163/300\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3724 - acc: 0.8306 - val_loss: 0.9237 - val_acc: 0.5700\n",
            "Epoch 164/300\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3661 - acc: 0.8371 - val_loss: 0.7309 - val_acc: 0.6800\n",
            "Epoch 165/300\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3526 - acc: 0.8565 - val_loss: 0.7747 - val_acc: 0.6700\n",
            "Epoch 166/300\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3745 - acc: 0.8435 - val_loss: 0.9188 - val_acc: 0.5800\n",
            "Epoch 167/300\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3693 - acc: 0.8468 - val_loss: 0.8042 - val_acc: 0.6400\n",
            "Epoch 168/300\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3700 - acc: 0.8435 - val_loss: 0.7150 - val_acc: 0.6600\n",
            "Epoch 169/300\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3471 - acc: 0.8629 - val_loss: 1.1115 - val_acc: 0.5300\n",
            "Epoch 170/300\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3889 - acc: 0.8097 - val_loss: 0.8625 - val_acc: 0.5900\n",
            "Epoch 171/300\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3693 - acc: 0.8371 - val_loss: 0.7409 - val_acc: 0.6700\n",
            "Epoch 172/300\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3647 - acc: 0.8355 - val_loss: 0.8829 - val_acc: 0.6000\n",
            "Epoch 173/300\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3760 - acc: 0.8371 - val_loss: 1.1034 - val_acc: 0.5500\n",
            "Epoch 174/300\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3579 - acc: 0.8355 - val_loss: 0.8182 - val_acc: 0.6300\n",
            "Epoch 175/300\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3570 - acc: 0.8452 - val_loss: 0.8349 - val_acc: 0.6200\n",
            "Epoch 176/300\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3749 - acc: 0.8339 - val_loss: 0.7033 - val_acc: 0.6700\n",
            "Epoch 177/300\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3522 - acc: 0.8516 - val_loss: 0.8649 - val_acc: 0.6100\n",
            "Epoch 178/300\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3779 - acc: 0.8194 - val_loss: 0.7899 - val_acc: 0.6400\n",
            "Epoch 179/300\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3742 - acc: 0.8290 - val_loss: 0.9231 - val_acc: 0.5800\n",
            "Epoch 180/300\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3860 - acc: 0.8242 - val_loss: 0.6949 - val_acc: 0.6600\n",
            "Epoch 181/300\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3746 - acc: 0.8387 - val_loss: 1.0603 - val_acc: 0.5600\n",
            "Epoch 182/300\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3650 - acc: 0.8355 - val_loss: 0.9237 - val_acc: 0.5800\n",
            "Epoch 183/300\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3847 - acc: 0.8306 - val_loss: 0.8445 - val_acc: 0.6000\n",
            "Epoch 184/300\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3731 - acc: 0.8129 - val_loss: 0.7796 - val_acc: 0.6300\n",
            "Epoch 185/300\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3450 - acc: 0.8565 - val_loss: 0.8922 - val_acc: 0.6100\n",
            "Epoch 186/300\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3589 - acc: 0.8403 - val_loss: 0.7833 - val_acc: 0.6400\n",
            "Epoch 187/300\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3567 - acc: 0.8403 - val_loss: 0.6759 - val_acc: 0.6700\n",
            "Epoch 188/300\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3723 - acc: 0.8371 - val_loss: 0.8639 - val_acc: 0.6000\n",
            "Epoch 189/300\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3448 - acc: 0.8435 - val_loss: 0.7448 - val_acc: 0.7000\n",
            "Epoch 190/300\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3883 - acc: 0.8177 - val_loss: 0.7576 - val_acc: 0.6900\n",
            "Epoch 191/300\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3735 - acc: 0.8226 - val_loss: 0.9121 - val_acc: 0.5900\n",
            "Epoch 192/300\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3403 - acc: 0.8565 - val_loss: 0.7290 - val_acc: 0.6900\n",
            "Epoch 193/300\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3648 - acc: 0.8323 - val_loss: 1.0871 - val_acc: 0.5600\n",
            "Epoch 194/300\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3688 - acc: 0.8339 - val_loss: 0.8245 - val_acc: 0.6200\n",
            "Epoch 195/300\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3483 - acc: 0.8339 - val_loss: 1.0271 - val_acc: 0.5800\n",
            "Epoch 196/300\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3561 - acc: 0.8419 - val_loss: 0.9428 - val_acc: 0.5800\n",
            "Epoch 197/300\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3616 - acc: 0.8387 - val_loss: 0.9281 - val_acc: 0.5900\n",
            "Epoch 198/300\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3591 - acc: 0.8242 - val_loss: 0.9087 - val_acc: 0.5700\n",
            "Epoch 199/300\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3852 - acc: 0.8194 - val_loss: 0.6794 - val_acc: 0.6900\n",
            "Epoch 200/300\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3565 - acc: 0.8452 - val_loss: 0.8756 - val_acc: 0.6200\n",
            "Epoch 201/300\n",
            "\n",
            "Epoch 00201: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3388 - acc: 0.8484 - val_loss: 0.6824 - val_acc: 0.6700\n",
            "Epoch 202/300\n",
            "\n",
            "Epoch 00202: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3662 - acc: 0.8290 - val_loss: 0.5813 - val_acc: 0.7300\n",
            "Epoch 203/300\n",
            "\n",
            "Epoch 00203: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3604 - acc: 0.8306 - val_loss: 0.8507 - val_acc: 0.6100\n",
            "Epoch 204/300\n",
            "\n",
            "Epoch 00204: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3525 - acc: 0.8468 - val_loss: 0.8897 - val_acc: 0.6000\n",
            "Epoch 205/300\n",
            "\n",
            "Epoch 00205: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3519 - acc: 0.8371 - val_loss: 0.8916 - val_acc: 0.6000\n",
            "Epoch 206/300\n",
            "\n",
            "Epoch 00206: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3426 - acc: 0.8468 - val_loss: 0.6813 - val_acc: 0.6800\n",
            "Epoch 207/300\n",
            "\n",
            "Epoch 00207: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3288 - acc: 0.8645 - val_loss: 0.8005 - val_acc: 0.6600\n",
            "Epoch 208/300\n",
            "\n",
            "Epoch 00208: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3618 - acc: 0.8403 - val_loss: 0.7704 - val_acc: 0.6800\n",
            "Epoch 209/300\n",
            "\n",
            "Epoch 00209: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3324 - acc: 0.8548 - val_loss: 1.0312 - val_acc: 0.5600\n",
            "Epoch 210/300\n",
            "\n",
            "Epoch 00210: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3551 - acc: 0.8452 - val_loss: 0.8374 - val_acc: 0.6100\n",
            "Epoch 211/300\n",
            "\n",
            "Epoch 00211: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3802 - acc: 0.8323 - val_loss: 0.9279 - val_acc: 0.5900\n",
            "Epoch 212/300\n",
            "\n",
            "Epoch 00212: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3572 - acc: 0.8371 - val_loss: 0.7772 - val_acc: 0.6300\n",
            "Epoch 213/300\n",
            "\n",
            "Epoch 00213: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3662 - acc: 0.8323 - val_loss: 0.9728 - val_acc: 0.5700\n",
            "Epoch 214/300\n",
            "\n",
            "Epoch 00214: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3587 - acc: 0.8435 - val_loss: 0.8341 - val_acc: 0.6400\n",
            "Epoch 215/300\n",
            "\n",
            "Epoch 00215: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3312 - acc: 0.8645 - val_loss: 0.8439 - val_acc: 0.6200\n",
            "Epoch 216/300\n",
            "\n",
            "Epoch 00216: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3577 - acc: 0.8484 - val_loss: 0.9089 - val_acc: 0.5700\n",
            "Epoch 217/300\n",
            "\n",
            "Epoch 00217: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3617 - acc: 0.8435 - val_loss: 0.7733 - val_acc: 0.6600\n",
            "Epoch 218/300\n",
            "\n",
            "Epoch 00218: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3487 - acc: 0.8532 - val_loss: 0.7858 - val_acc: 0.6400\n",
            "Epoch 219/300\n",
            "\n",
            "Epoch 00219: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3395 - acc: 0.8532 - val_loss: 0.9698 - val_acc: 0.5700\n",
            "Epoch 220/300\n",
            "\n",
            "Epoch 00220: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3290 - acc: 0.8629 - val_loss: 0.8490 - val_acc: 0.6500\n",
            "Epoch 221/300\n",
            "\n",
            "Epoch 00221: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3385 - acc: 0.8484 - val_loss: 0.8069 - val_acc: 0.6700\n",
            "Epoch 222/300\n",
            "\n",
            "Epoch 00222: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3479 - acc: 0.8645 - val_loss: 0.9088 - val_acc: 0.6100\n",
            "Epoch 223/300\n",
            "\n",
            "Epoch 00223: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3620 - acc: 0.8242 - val_loss: 0.8031 - val_acc: 0.6400\n",
            "Epoch 224/300\n",
            "\n",
            "Epoch 00224: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3523 - acc: 0.8387 - val_loss: 0.7838 - val_acc: 0.6200\n",
            "Epoch 225/300\n",
            "\n",
            "Epoch 00225: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3909 - acc: 0.8129 - val_loss: 0.7027 - val_acc: 0.6500\n",
            "Epoch 226/300\n",
            "\n",
            "Epoch 00226: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3507 - acc: 0.8677 - val_loss: 0.6125 - val_acc: 0.7400\n",
            "Epoch 227/300\n",
            "\n",
            "Epoch 00227: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3421 - acc: 0.8516 - val_loss: 0.7650 - val_acc: 0.6600\n",
            "Epoch 228/300\n",
            "\n",
            "Epoch 00228: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3551 - acc: 0.8435 - val_loss: 0.7683 - val_acc: 0.6400\n",
            "Epoch 229/300\n",
            "\n",
            "Epoch 00229: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3601 - acc: 0.8452 - val_loss: 1.2135 - val_acc: 0.5400\n",
            "Epoch 230/300\n",
            "\n",
            "Epoch 00230: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3238 - acc: 0.8565 - val_loss: 0.6240 - val_acc: 0.7300\n",
            "Epoch 231/300\n",
            "\n",
            "Epoch 00231: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3483 - acc: 0.8597 - val_loss: 0.6523 - val_acc: 0.6900\n",
            "Epoch 232/300\n",
            "\n",
            "Epoch 00232: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3623 - acc: 0.8371 - val_loss: 0.8519 - val_acc: 0.6200\n",
            "Epoch 233/300\n",
            "\n",
            "Epoch 00233: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3487 - acc: 0.8452 - val_loss: 0.7579 - val_acc: 0.6400\n",
            "Epoch 234/300\n",
            "\n",
            "Epoch 00234: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3610 - acc: 0.8403 - val_loss: 0.8988 - val_acc: 0.6100\n",
            "Epoch 235/300\n",
            "\n",
            "Epoch 00235: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3447 - acc: 0.8355 - val_loss: 0.9700 - val_acc: 0.5800\n",
            "Epoch 236/300\n",
            "\n",
            "Epoch 00236: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3563 - acc: 0.8371 - val_loss: 0.7946 - val_acc: 0.6700\n",
            "Epoch 237/300\n",
            "\n",
            "Epoch 00237: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3240 - acc: 0.8645 - val_loss: 0.8142 - val_acc: 0.6500\n",
            "Epoch 238/300\n",
            "\n",
            "Epoch 00238: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3672 - acc: 0.8339 - val_loss: 0.6941 - val_acc: 0.6900\n",
            "Epoch 239/300\n",
            "\n",
            "Epoch 00239: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3565 - acc: 0.8419 - val_loss: 0.7093 - val_acc: 0.6800\n",
            "Epoch 240/300\n",
            "\n",
            "Epoch 00240: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3391 - acc: 0.8387 - val_loss: 0.9769 - val_acc: 0.5900\n",
            "Epoch 241/300\n",
            "\n",
            "Epoch 00241: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3255 - acc: 0.8597 - val_loss: 0.8637 - val_acc: 0.6300\n",
            "Epoch 242/300\n",
            "\n",
            "Epoch 00242: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3317 - acc: 0.8597 - val_loss: 0.7988 - val_acc: 0.6600\n",
            "Epoch 243/300\n",
            "\n",
            "Epoch 00243: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3305 - acc: 0.8661 - val_loss: 0.7436 - val_acc: 0.6900\n",
            "Epoch 244/300\n",
            "\n",
            "Epoch 00244: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3520 - acc: 0.8419 - val_loss: 0.8969 - val_acc: 0.6000\n",
            "Epoch 245/300\n",
            "\n",
            "Epoch 00245: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3220 - acc: 0.8581 - val_loss: 1.0253 - val_acc: 0.5800\n",
            "Epoch 246/300\n",
            "\n",
            "Epoch 00246: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3541 - acc: 0.8419 - val_loss: 1.1194 - val_acc: 0.5500\n",
            "Epoch 247/300\n",
            "\n",
            "Epoch 00247: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3300 - acc: 0.8677 - val_loss: 0.6317 - val_acc: 0.7300\n",
            "Epoch 248/300\n",
            "\n",
            "Epoch 00248: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3504 - acc: 0.8629 - val_loss: 0.9125 - val_acc: 0.6200\n",
            "Epoch 249/300\n",
            "\n",
            "Epoch 00249: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3205 - acc: 0.8565 - val_loss: 0.8164 - val_acc: 0.6500\n",
            "Epoch 250/300\n",
            "\n",
            "Epoch 00250: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3305 - acc: 0.8613 - val_loss: 0.7310 - val_acc: 0.6600\n",
            "Epoch 251/300\n",
            "\n",
            "Epoch 00251: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3450 - acc: 0.8387 - val_loss: 0.8391 - val_acc: 0.6400\n",
            "Epoch 252/300\n",
            "\n",
            "Epoch 00252: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3606 - acc: 0.8323 - val_loss: 0.7925 - val_acc: 0.6600\n",
            "Epoch 253/300\n",
            "\n",
            "Epoch 00253: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3311 - acc: 0.8629 - val_loss: 0.9188 - val_acc: 0.6000\n",
            "Epoch 254/300\n",
            "\n",
            "Epoch 00254: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3674 - acc: 0.8468 - val_loss: 0.6717 - val_acc: 0.7200\n",
            "Epoch 255/300\n",
            "\n",
            "Epoch 00255: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3634 - acc: 0.8242 - val_loss: 0.8801 - val_acc: 0.6000\n",
            "Epoch 256/300\n",
            "\n",
            "Epoch 00256: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3410 - acc: 0.8452 - val_loss: 1.0393 - val_acc: 0.5600\n",
            "Epoch 257/300\n",
            "\n",
            "Epoch 00257: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3229 - acc: 0.8677 - val_loss: 0.9542 - val_acc: 0.6000\n",
            "Epoch 258/300\n",
            "\n",
            "Epoch 00258: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3323 - acc: 0.8613 - val_loss: 0.7884 - val_acc: 0.6400\n",
            "Epoch 259/300\n",
            "\n",
            "Epoch 00259: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3283 - acc: 0.8532 - val_loss: 0.8808 - val_acc: 0.6100\n",
            "Epoch 260/300\n",
            "\n",
            "Epoch 00260: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3419 - acc: 0.8694 - val_loss: 0.9477 - val_acc: 0.6200\n",
            "Epoch 261/300\n",
            "\n",
            "Epoch 00261: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3222 - acc: 0.8710 - val_loss: 0.7193 - val_acc: 0.6700\n",
            "Epoch 262/300\n",
            "\n",
            "Epoch 00262: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3311 - acc: 0.8500 - val_loss: 1.1001 - val_acc: 0.5700\n",
            "Epoch 263/300\n",
            "\n",
            "Epoch 00263: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3259 - acc: 0.8629 - val_loss: 0.8002 - val_acc: 0.6600\n",
            "Epoch 264/300\n",
            "\n",
            "Epoch 00264: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3130 - acc: 0.8613 - val_loss: 0.7610 - val_acc: 0.6600\n",
            "Epoch 265/300\n",
            "\n",
            "Epoch 00265: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3329 - acc: 0.8597 - val_loss: 0.7354 - val_acc: 0.6700\n",
            "Epoch 266/300\n",
            "\n",
            "Epoch 00266: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3430 - acc: 0.8565 - val_loss: 0.9651 - val_acc: 0.5800\n",
            "Epoch 267/300\n",
            "\n",
            "Epoch 00267: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3372 - acc: 0.8484 - val_loss: 0.7849 - val_acc: 0.6300\n",
            "Epoch 268/300\n",
            "\n",
            "Epoch 00268: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3368 - acc: 0.8532 - val_loss: 0.9261 - val_acc: 0.5800\n",
            "Epoch 269/300\n",
            "\n",
            "Epoch 00269: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3651 - acc: 0.8435 - val_loss: 0.7536 - val_acc: 0.6500\n",
            "Epoch 270/300\n",
            "\n",
            "Epoch 00270: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3577 - acc: 0.8484 - val_loss: 0.8342 - val_acc: 0.6200\n",
            "Epoch 271/300\n",
            "\n",
            "Epoch 00271: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3485 - acc: 0.8532 - val_loss: 0.7504 - val_acc: 0.6700\n",
            "Epoch 272/300\n",
            "\n",
            "Epoch 00272: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3577 - acc: 0.8403 - val_loss: 0.6584 - val_acc: 0.6900\n",
            "Epoch 273/300\n",
            "\n",
            "Epoch 00273: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3611 - acc: 0.8306 - val_loss: 1.0677 - val_acc: 0.5600\n",
            "Epoch 274/300\n",
            "\n",
            "Epoch 00274: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3441 - acc: 0.8452 - val_loss: 0.9072 - val_acc: 0.6200\n",
            "Epoch 275/300\n",
            "\n",
            "Epoch 00275: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3380 - acc: 0.8597 - val_loss: 0.9558 - val_acc: 0.5600\n",
            "Epoch 276/300\n",
            "\n",
            "Epoch 00276: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3432 - acc: 0.8387 - val_loss: 0.7999 - val_acc: 0.6400\n",
            "Epoch 277/300\n",
            "\n",
            "Epoch 00277: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3519 - acc: 0.8516 - val_loss: 0.7273 - val_acc: 0.6500\n",
            "Epoch 278/300\n",
            "\n",
            "Epoch 00278: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3095 - acc: 0.8694 - val_loss: 0.9510 - val_acc: 0.6000\n",
            "Epoch 279/300\n",
            "\n",
            "Epoch 00279: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3796 - acc: 0.8484 - val_loss: 0.9442 - val_acc: 0.6200\n",
            "Epoch 280/300\n",
            "\n",
            "Epoch 00280: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3114 - acc: 0.8823 - val_loss: 0.8779 - val_acc: 0.6400\n",
            "Epoch 281/300\n",
            "\n",
            "Epoch 00281: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3298 - acc: 0.8565 - val_loss: 0.7544 - val_acc: 0.6700\n",
            "Epoch 282/300\n",
            "\n",
            "Epoch 00282: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3161 - acc: 0.8774 - val_loss: 0.9271 - val_acc: 0.6000\n",
            "Epoch 283/300\n",
            "\n",
            "Epoch 00283: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3317 - acc: 0.8565 - val_loss: 0.6889 - val_acc: 0.7100\n",
            "Epoch 284/300\n",
            "\n",
            "Epoch 00284: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3144 - acc: 0.8629 - val_loss: 0.8496 - val_acc: 0.6300\n",
            "Epoch 285/300\n",
            "\n",
            "Epoch 00285: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3275 - acc: 0.8468 - val_loss: 0.7115 - val_acc: 0.6800\n",
            "Epoch 286/300\n",
            "\n",
            "Epoch 00286: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3205 - acc: 0.8581 - val_loss: 0.7552 - val_acc: 0.6600\n",
            "Epoch 287/300\n",
            "\n",
            "Epoch 00287: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3349 - acc: 0.8532 - val_loss: 0.7367 - val_acc: 0.6800\n",
            "Epoch 288/300\n",
            "\n",
            "Epoch 00288: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3343 - acc: 0.8694 - val_loss: 0.8087 - val_acc: 0.6500\n",
            "Epoch 289/300\n",
            "\n",
            "Epoch 00289: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3604 - acc: 0.8403 - val_loss: 0.7809 - val_acc: 0.6600\n",
            "Epoch 290/300\n",
            "\n",
            "Epoch 00290: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3282 - acc: 0.8661 - val_loss: 1.1838 - val_acc: 0.5400\n",
            "Epoch 291/300\n",
            "\n",
            "Epoch 00291: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3475 - acc: 0.8500 - val_loss: 0.7912 - val_acc: 0.6500\n",
            "Epoch 292/300\n",
            "\n",
            "Epoch 00292: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3184 - acc: 0.8790 - val_loss: 0.8586 - val_acc: 0.6200\n",
            "Epoch 293/300\n",
            "\n",
            "Epoch 00293: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3393 - acc: 0.8516 - val_loss: 0.9528 - val_acc: 0.5800\n",
            "Epoch 294/300\n",
            "\n",
            "Epoch 00294: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3385 - acc: 0.8468 - val_loss: 0.8296 - val_acc: 0.6300\n",
            "Epoch 295/300\n",
            "\n",
            "Epoch 00295: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3205 - acc: 0.8677 - val_loss: 1.0168 - val_acc: 0.5800\n",
            "Epoch 296/300\n",
            "\n",
            "Epoch 00296: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3157 - acc: 0.8677 - val_loss: 0.7987 - val_acc: 0.6400\n",
            "Epoch 297/300\n",
            "\n",
            "Epoch 00297: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3259 - acc: 0.8500 - val_loss: 0.7495 - val_acc: 0.6800\n",
            "Epoch 298/300\n",
            "\n",
            "Epoch 00298: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3483 - acc: 0.8435 - val_loss: 0.9058 - val_acc: 0.6200\n",
            "Epoch 299/300\n",
            "\n",
            "Epoch 00299: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3716 - acc: 0.8355 - val_loss: 0.6845 - val_acc: 0.6800\n",
            "Epoch 300/300\n",
            "\n",
            "Epoch 00300: val_loss did not improve from 0.52332\n",
            "620/620 - 0s - loss: 0.3362 - acc: 0.8629 - val_loss: 0.7349 - val_acc: 0.6600\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "# Evaluate on test data\n",
            "40/40 [==============================] - 0s 3ms/sample - loss: 1.0251 - acc: 0.5000\n",
            "test loss, test acc: [1.0250928244553505, 0.5]\n",
            "1\n",
            "EEG_Deep/Data2A/parsed_P01T.mat\n",
            "(0, 12, 4096)\n",
            "(80, 12, 4096)\n",
            "(80, 1)\n",
            "2\n",
            "EEG_Deep/Data2A/parsed_P02T.mat\n",
            "(80, 12, 4096)\n",
            "(160, 12, 4096)\n",
            "(160, 1)\n",
            "3\n",
            "EEG_Deep/Data2A/parsed_P03T.mat\n",
            "(160, 12, 4096)\n",
            "(240, 12, 4096)\n",
            "(240, 1)\n",
            "4\n",
            "EEG_Deep/Data2A/parsed_P04T.mat\n",
            "(240, 12, 4096)\n",
            "(320, 12, 4096)\n",
            "(320, 1)\n",
            "6\n",
            "EEG_Deep/Data2A/parsed_P06T.mat\n",
            "(320, 12, 4096)\n",
            "(400, 12, 4096)\n",
            "(400, 1)\n",
            "7\n",
            "EEG_Deep/Data2A/parsed_P07T.mat\n",
            "(400, 12, 4096)\n",
            "(480, 12, 4096)\n",
            "(480, 1)\n",
            "8\n",
            "EEG_Deep/Data2A/parsed_P08T.mat\n",
            "(480, 12, 4096)\n",
            "(560, 12, 4096)\n",
            "(560, 1)\n",
            "9\n",
            "EEG_Deep/Data2A/parsed_P09T.mat\n",
            "(560, 12, 4096)\n",
            "(640, 12, 4096)\n",
            "(640, 1)\n",
            "10\n",
            "EEG_Deep/Data2A/parsed_P010T.mat\n",
            "(640, 12, 4096)\n",
            "(720, 12, 4096)\n",
            "(720, 1)\n",
            "5\n",
            "EEG_Deep/Data2A/parsed_P05E.mat\n",
            "(0, 12, 4096)\n",
            "(40, 12, 4096)\n",
            "(40, 1)\n",
            "Filtering of training data in progress\n",
            "(720, 12, 4096)\n",
            "Filtering of test data in progress\n",
            "(40, 12, 4096)\n",
            "[2. 1. 2. 1. 1. 1. 1. 2. 1. 2. 1. 1. 1. 1. 2. 2. 1. 1. 1. 2. 2. 1. 1. 1.\n",
            " 2. 1. 2. 2. 1. 2. 1. 2. 2. 1. 2. 1. 1. 2. 2. 2. 2. 2. 1. 2. 1. 2. 2. 2.\n",
            " 1. 2. 1. 1. 1. 2. 2. 1. 1. 1. 2. 2. 1. 1. 2. 2. 2. 1. 2. 2. 2. 2. 1. 1.\n",
            " 2. 1. 1. 1. 1. 1. 2. 2. 1. 2. 2. 2. 2. 1. 2. 1. 1. 2. 2. 1. 1. 1. 2. 2.\n",
            " 1. 1. 1. 2.]\n",
            "(620, 12, 1536)\n",
            "(620,)\n",
            "(100, 12, 1536)\n",
            "(100,)\n",
            "[[0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]]\n",
            "X_train shape: (620, 1, 12, 1536)\n",
            "620 train samples\n",
            "100 val samples\n",
            "(40, 12, 1536)\n",
            "(40, 1)\n",
            "X_train shape: (40, 1, 12, 1536)\n",
            "40 train samples\n",
            "Train on 620 samples, validate on 100 samples\n",
            "Epoch 1/300\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 1.14965, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.9394 - acc: 0.5161 - val_loss: 1.1496 - val_acc: 0.5300\n",
            "Epoch 2/300\n",
            "\n",
            "Epoch 00002: val_loss improved from 1.14965 to 0.99403, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.7741 - acc: 0.5823 - val_loss: 0.9940 - val_acc: 0.6000\n",
            "Epoch 3/300\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.99403 to 0.89700, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.7249 - acc: 0.6306 - val_loss: 0.8970 - val_acc: 0.5200\n",
            "Epoch 4/300\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.89700 to 0.80960, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.6919 - acc: 0.6726 - val_loss: 0.8096 - val_acc: 0.5500\n",
            "Epoch 5/300\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.80960 to 0.73674, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.6569 - acc: 0.6726 - val_loss: 0.7367 - val_acc: 0.6600\n",
            "Epoch 6/300\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.73674 to 0.70556, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.6406 - acc: 0.6645 - val_loss: 0.7056 - val_acc: 0.6200\n",
            "Epoch 7/300\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.70556 to 0.64613, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.6333 - acc: 0.6774 - val_loss: 0.6461 - val_acc: 0.7000\n",
            "Epoch 8/300\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.64613 to 0.61607, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.6047 - acc: 0.6952 - val_loss: 0.6161 - val_acc: 0.7000\n",
            "Epoch 9/300\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.61607\n",
            "620/620 - 0s - loss: 0.5676 - acc: 0.7274 - val_loss: 0.6404 - val_acc: 0.6400\n",
            "Epoch 10/300\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.61607\n",
            "620/620 - 0s - loss: 0.5876 - acc: 0.7048 - val_loss: 0.6326 - val_acc: 0.6700\n",
            "Epoch 11/300\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.61607\n",
            "620/620 - 0s - loss: 0.5569 - acc: 0.7226 - val_loss: 0.6494 - val_acc: 0.6600\n",
            "Epoch 12/300\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.61607\n",
            "620/620 - 0s - loss: 0.5517 - acc: 0.7194 - val_loss: 0.6193 - val_acc: 0.6700\n",
            "Epoch 13/300\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.61607\n",
            "620/620 - 0s - loss: 0.5404 - acc: 0.7258 - val_loss: 0.6373 - val_acc: 0.6500\n",
            "Epoch 14/300\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.61607\n",
            "620/620 - 0s - loss: 0.5293 - acc: 0.7419 - val_loss: 0.6993 - val_acc: 0.6000\n",
            "Epoch 15/300\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.61607\n",
            "620/620 - 0s - loss: 0.5378 - acc: 0.7355 - val_loss: 0.7107 - val_acc: 0.6100\n",
            "Epoch 16/300\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.61607\n",
            "620/620 - 0s - loss: 0.5373 - acc: 0.7274 - val_loss: 0.7219 - val_acc: 0.6000\n",
            "Epoch 17/300\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.61607\n",
            "620/620 - 0s - loss: 0.5130 - acc: 0.7645 - val_loss: 0.8561 - val_acc: 0.5600\n",
            "Epoch 18/300\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.61607\n",
            "620/620 - 0s - loss: 0.5058 - acc: 0.7677 - val_loss: 0.7117 - val_acc: 0.6300\n",
            "Epoch 19/300\n",
            "\n",
            "Epoch 00019: val_loss improved from 0.61607 to 0.59361, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.5210 - acc: 0.7516 - val_loss: 0.5936 - val_acc: 0.6700\n",
            "Epoch 20/300\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.59361\n",
            "620/620 - 0s - loss: 0.5432 - acc: 0.7242 - val_loss: 0.6752 - val_acc: 0.6600\n",
            "Epoch 21/300\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.59361\n",
            "620/620 - 0s - loss: 0.4996 - acc: 0.7500 - val_loss: 0.8564 - val_acc: 0.5900\n",
            "Epoch 22/300\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.59361\n",
            "620/620 - 0s - loss: 0.4866 - acc: 0.7806 - val_loss: 0.7716 - val_acc: 0.6000\n",
            "Epoch 23/300\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.59361\n",
            "620/620 - 0s - loss: 0.5187 - acc: 0.7565 - val_loss: 0.6006 - val_acc: 0.6800\n",
            "Epoch 24/300\n",
            "\n",
            "Epoch 00024: val_loss improved from 0.59361 to 0.57650, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.5042 - acc: 0.7694 - val_loss: 0.5765 - val_acc: 0.6800\n",
            "Epoch 25/300\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.57650\n",
            "620/620 - 0s - loss: 0.5160 - acc: 0.7516 - val_loss: 0.6052 - val_acc: 0.6700\n",
            "Epoch 26/300\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.57650\n",
            "620/620 - 0s - loss: 0.5046 - acc: 0.7452 - val_loss: 0.6028 - val_acc: 0.6800\n",
            "Epoch 27/300\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.57650\n",
            "620/620 - 0s - loss: 0.4792 - acc: 0.7694 - val_loss: 0.8002 - val_acc: 0.5900\n",
            "Epoch 28/300\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.57650\n",
            "620/620 - 0s - loss: 0.5080 - acc: 0.7532 - val_loss: 0.6557 - val_acc: 0.6600\n",
            "Epoch 29/300\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.57650\n",
            "620/620 - 0s - loss: 0.5114 - acc: 0.7613 - val_loss: 0.6946 - val_acc: 0.6300\n",
            "Epoch 30/300\n",
            "\n",
            "Epoch 00030: val_loss improved from 0.57650 to 0.56646, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.5055 - acc: 0.7435 - val_loss: 0.5665 - val_acc: 0.6800\n",
            "Epoch 31/300\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.56646\n",
            "620/620 - 0s - loss: 0.4969 - acc: 0.7500 - val_loss: 0.7038 - val_acc: 0.6200\n",
            "Epoch 32/300\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.56646\n",
            "620/620 - 0s - loss: 0.5068 - acc: 0.7581 - val_loss: 0.6588 - val_acc: 0.6400\n",
            "Epoch 33/300\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.56646\n",
            "620/620 - 0s - loss: 0.5052 - acc: 0.7532 - val_loss: 0.7023 - val_acc: 0.6000\n",
            "Epoch 34/300\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.56646\n",
            "620/620 - 0s - loss: 0.4658 - acc: 0.7887 - val_loss: 1.0426 - val_acc: 0.5400\n",
            "Epoch 35/300\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.56646\n",
            "620/620 - 0s - loss: 0.5016 - acc: 0.7613 - val_loss: 0.9771 - val_acc: 0.5500\n",
            "Epoch 36/300\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.56646\n",
            "620/620 - 0s - loss: 0.4738 - acc: 0.7790 - val_loss: 0.8193 - val_acc: 0.5900\n",
            "Epoch 37/300\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.56646\n",
            "620/620 - 0s - loss: 0.4746 - acc: 0.7726 - val_loss: 0.6916 - val_acc: 0.6200\n",
            "Epoch 38/300\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.56646\n",
            "620/620 - 0s - loss: 0.5011 - acc: 0.7452 - val_loss: 0.6969 - val_acc: 0.6500\n",
            "Epoch 39/300\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.56646\n",
            "620/620 - 0s - loss: 0.4750 - acc: 0.7790 - val_loss: 0.7011 - val_acc: 0.6300\n",
            "Epoch 40/300\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.56646\n",
            "620/620 - 0s - loss: 0.4872 - acc: 0.7532 - val_loss: 0.6503 - val_acc: 0.6300\n",
            "Epoch 41/300\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.56646\n",
            "620/620 - 0s - loss: 0.5029 - acc: 0.7629 - val_loss: 0.7129 - val_acc: 0.6000\n",
            "Epoch 42/300\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.56646\n",
            "620/620 - 0s - loss: 0.4627 - acc: 0.7887 - val_loss: 0.7253 - val_acc: 0.6300\n",
            "Epoch 43/300\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.56646\n",
            "620/620 - 0s - loss: 0.4799 - acc: 0.7645 - val_loss: 0.5811 - val_acc: 0.6800\n",
            "Epoch 44/300\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.56646\n",
            "620/620 - 0s - loss: 0.4987 - acc: 0.7726 - val_loss: 0.6640 - val_acc: 0.6600\n",
            "Epoch 45/300\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.56646\n",
            "620/620 - 0s - loss: 0.4588 - acc: 0.7903 - val_loss: 0.7329 - val_acc: 0.6300\n",
            "Epoch 46/300\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.56646\n",
            "620/620 - 0s - loss: 0.4700 - acc: 0.7871 - val_loss: 0.9009 - val_acc: 0.5800\n",
            "Epoch 47/300\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.56646\n",
            "620/620 - 0s - loss: 0.4524 - acc: 0.7935 - val_loss: 0.7707 - val_acc: 0.6100\n",
            "Epoch 48/300\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.56646\n",
            "620/620 - 0s - loss: 0.5056 - acc: 0.7484 - val_loss: 0.6254 - val_acc: 0.6500\n",
            "Epoch 49/300\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.56646\n",
            "620/620 - 0s - loss: 0.4781 - acc: 0.7855 - val_loss: 0.7386 - val_acc: 0.6200\n",
            "Epoch 50/300\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.56646\n",
            "620/620 - 0s - loss: 0.4680 - acc: 0.7855 - val_loss: 0.6068 - val_acc: 0.6500\n",
            "Epoch 51/300\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.56646\n",
            "620/620 - 0s - loss: 0.4620 - acc: 0.7871 - val_loss: 0.6070 - val_acc: 0.6900\n",
            "Epoch 52/300\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.56646\n",
            "620/620 - 0s - loss: 0.4691 - acc: 0.7823 - val_loss: 0.6618 - val_acc: 0.6400\n",
            "Epoch 53/300\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.56646\n",
            "620/620 - 0s - loss: 0.4673 - acc: 0.7823 - val_loss: 0.7564 - val_acc: 0.6400\n",
            "Epoch 54/300\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.56646\n",
            "620/620 - 0s - loss: 0.4615 - acc: 0.7726 - val_loss: 0.6919 - val_acc: 0.6500\n",
            "Epoch 55/300\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.56646\n",
            "620/620 - 0s - loss: 0.4337 - acc: 0.7935 - val_loss: 0.6958 - val_acc: 0.6400\n",
            "Epoch 56/300\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.56646\n",
            "620/620 - 0s - loss: 0.4783 - acc: 0.7629 - val_loss: 0.6319 - val_acc: 0.6700\n",
            "Epoch 57/300\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.56646\n",
            "620/620 - 0s - loss: 0.4543 - acc: 0.7823 - val_loss: 0.7259 - val_acc: 0.6400\n",
            "Epoch 58/300\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.56646\n",
            "620/620 - 0s - loss: 0.4506 - acc: 0.7871 - val_loss: 0.9555 - val_acc: 0.5800\n",
            "Epoch 59/300\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.56646\n",
            "620/620 - 0s - loss: 0.4311 - acc: 0.8016 - val_loss: 0.6077 - val_acc: 0.6800\n",
            "Epoch 60/300\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.56646\n",
            "620/620 - 0s - loss: 0.4647 - acc: 0.7903 - val_loss: 0.5753 - val_acc: 0.6900\n",
            "Epoch 61/300\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.56646\n",
            "620/620 - 0s - loss: 0.4421 - acc: 0.7855 - val_loss: 0.6805 - val_acc: 0.6700\n",
            "Epoch 62/300\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.56646\n",
            "620/620 - 0s - loss: 0.4500 - acc: 0.7774 - val_loss: 0.7296 - val_acc: 0.6400\n",
            "Epoch 63/300\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.56646\n",
            "620/620 - 0s - loss: 0.4536 - acc: 0.7919 - val_loss: 0.7467 - val_acc: 0.6200\n",
            "Epoch 64/300\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.56646\n",
            "620/620 - 0s - loss: 0.4553 - acc: 0.7806 - val_loss: 0.7920 - val_acc: 0.6100\n",
            "Epoch 65/300\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.56646\n",
            "620/620 - 0s - loss: 0.4500 - acc: 0.7855 - val_loss: 0.6532 - val_acc: 0.6700\n",
            "Epoch 66/300\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.56646\n",
            "620/620 - 0s - loss: 0.4420 - acc: 0.7952 - val_loss: 0.6390 - val_acc: 0.6800\n",
            "Epoch 67/300\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.56646\n",
            "620/620 - 0s - loss: 0.4530 - acc: 0.7839 - val_loss: 0.6849 - val_acc: 0.6400\n",
            "Epoch 68/300\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.56646\n",
            "620/620 - 0s - loss: 0.4423 - acc: 0.7823 - val_loss: 0.6348 - val_acc: 0.6800\n",
            "Epoch 69/300\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.56646\n",
            "620/620 - 0s - loss: 0.4781 - acc: 0.7710 - val_loss: 0.7102 - val_acc: 0.6400\n",
            "Epoch 70/300\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.56646\n",
            "620/620 - 0s - loss: 0.4350 - acc: 0.8113 - val_loss: 0.6540 - val_acc: 0.6900\n",
            "Epoch 71/300\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.56646\n",
            "620/620 - 0s - loss: 0.5003 - acc: 0.7597 - val_loss: 0.8702 - val_acc: 0.6100\n",
            "Epoch 72/300\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.56646\n",
            "620/620 - 0s - loss: 0.4278 - acc: 0.8129 - val_loss: 0.7329 - val_acc: 0.6400\n",
            "Epoch 73/300\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.56646\n",
            "620/620 - 0s - loss: 0.4437 - acc: 0.7887 - val_loss: 0.7048 - val_acc: 0.6400\n",
            "Epoch 74/300\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.56646\n",
            "620/620 - 0s - loss: 0.4529 - acc: 0.7839 - val_loss: 0.5931 - val_acc: 0.6800\n",
            "Epoch 75/300\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.56646\n",
            "620/620 - 0s - loss: 0.4566 - acc: 0.7823 - val_loss: 0.7632 - val_acc: 0.6300\n",
            "Epoch 76/300\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.56646\n",
            "620/620 - 0s - loss: 0.4287 - acc: 0.8081 - val_loss: 0.7539 - val_acc: 0.6200\n",
            "Epoch 77/300\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.56646\n",
            "620/620 - 0s - loss: 0.4455 - acc: 0.7952 - val_loss: 0.7212 - val_acc: 0.6000\n",
            "Epoch 78/300\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.56646\n",
            "620/620 - 0s - loss: 0.4254 - acc: 0.8210 - val_loss: 0.7462 - val_acc: 0.6600\n",
            "Epoch 79/300\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.56646\n",
            "620/620 - 0s - loss: 0.4260 - acc: 0.7919 - val_loss: 0.7769 - val_acc: 0.6100\n",
            "Epoch 80/300\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.56646\n",
            "620/620 - 0s - loss: 0.4144 - acc: 0.8032 - val_loss: 0.6659 - val_acc: 0.6800\n",
            "Epoch 81/300\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.56646\n",
            "620/620 - 0s - loss: 0.4880 - acc: 0.7661 - val_loss: 0.6745 - val_acc: 0.6400\n",
            "Epoch 82/300\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.56646\n",
            "620/620 - 0s - loss: 0.4436 - acc: 0.7919 - val_loss: 0.9235 - val_acc: 0.5900\n",
            "Epoch 83/300\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.56646\n",
            "620/620 - 0s - loss: 0.4318 - acc: 0.7919 - val_loss: 0.7835 - val_acc: 0.6100\n",
            "Epoch 84/300\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.56646\n",
            "620/620 - 0s - loss: 0.4412 - acc: 0.7839 - val_loss: 0.7109 - val_acc: 0.6200\n",
            "Epoch 85/300\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.56646\n",
            "620/620 - 0s - loss: 0.4446 - acc: 0.8000 - val_loss: 0.8549 - val_acc: 0.5900\n",
            "Epoch 86/300\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.56646\n",
            "620/620 - 0s - loss: 0.4396 - acc: 0.7903 - val_loss: 0.7017 - val_acc: 0.6300\n",
            "Epoch 87/300\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.56646\n",
            "620/620 - 0s - loss: 0.4316 - acc: 0.7919 - val_loss: 0.8449 - val_acc: 0.6000\n",
            "Epoch 88/300\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.56646\n",
            "620/620 - 0s - loss: 0.4384 - acc: 0.7871 - val_loss: 0.6252 - val_acc: 0.6900\n",
            "Epoch 89/300\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.56646\n",
            "620/620 - 0s - loss: 0.4278 - acc: 0.8032 - val_loss: 0.8150 - val_acc: 0.6000\n",
            "Epoch 90/300\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.56646\n",
            "620/620 - 0s - loss: 0.4463 - acc: 0.8000 - val_loss: 0.8931 - val_acc: 0.5800\n",
            "Epoch 91/300\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.56646\n",
            "620/620 - 0s - loss: 0.4242 - acc: 0.7919 - val_loss: 0.8067 - val_acc: 0.5900\n",
            "Epoch 92/300\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.56646\n",
            "620/620 - 0s - loss: 0.4284 - acc: 0.8000 - val_loss: 0.8651 - val_acc: 0.5800\n",
            "Epoch 93/300\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.56646\n",
            "620/620 - 0s - loss: 0.4351 - acc: 0.8145 - val_loss: 0.7248 - val_acc: 0.6100\n",
            "Epoch 94/300\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.56646\n",
            "620/620 - 0s - loss: 0.4326 - acc: 0.8000 - val_loss: 0.6679 - val_acc: 0.6600\n",
            "Epoch 95/300\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.56646\n",
            "620/620 - 0s - loss: 0.4100 - acc: 0.8065 - val_loss: 0.5909 - val_acc: 0.6900\n",
            "Epoch 96/300\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.56646\n",
            "620/620 - 0s - loss: 0.4283 - acc: 0.8081 - val_loss: 0.8665 - val_acc: 0.6000\n",
            "Epoch 97/300\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.56646\n",
            "620/620 - 0s - loss: 0.4293 - acc: 0.8194 - val_loss: 0.9140 - val_acc: 0.5800\n",
            "Epoch 98/300\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.56646\n",
            "620/620 - 0s - loss: 0.4215 - acc: 0.7919 - val_loss: 0.6569 - val_acc: 0.6800\n",
            "Epoch 99/300\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.56646\n",
            "620/620 - 0s - loss: 0.4025 - acc: 0.8145 - val_loss: 0.6228 - val_acc: 0.7000\n",
            "Epoch 100/300\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.56646\n",
            "620/620 - 0s - loss: 0.4157 - acc: 0.7968 - val_loss: 0.7393 - val_acc: 0.6500\n",
            "Epoch 101/300\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.56646\n",
            "620/620 - 0s - loss: 0.4322 - acc: 0.8032 - val_loss: 0.6677 - val_acc: 0.6600\n",
            "Epoch 102/300\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.56646\n",
            "620/620 - 0s - loss: 0.4451 - acc: 0.7806 - val_loss: 0.6834 - val_acc: 0.6500\n",
            "Epoch 103/300\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.56646\n",
            "620/620 - 0s - loss: 0.4203 - acc: 0.7935 - val_loss: 0.6381 - val_acc: 0.6700\n",
            "Epoch 104/300\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.56646\n",
            "620/620 - 0s - loss: 0.4063 - acc: 0.8032 - val_loss: 0.6537 - val_acc: 0.6900\n",
            "Epoch 105/300\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 0.56646\n",
            "620/620 - 0s - loss: 0.4185 - acc: 0.8161 - val_loss: 0.6617 - val_acc: 0.6800\n",
            "Epoch 106/300\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 0.56646\n",
            "620/620 - 0s - loss: 0.4412 - acc: 0.7871 - val_loss: 0.7237 - val_acc: 0.6200\n",
            "Epoch 107/300\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.56646\n",
            "620/620 - 0s - loss: 0.4106 - acc: 0.8210 - val_loss: 0.7273 - val_acc: 0.6100\n",
            "Epoch 108/300\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 0.56646\n",
            "620/620 - 0s - loss: 0.4235 - acc: 0.8081 - val_loss: 0.6150 - val_acc: 0.7000\n",
            "Epoch 109/300\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.56646\n",
            "620/620 - 0s - loss: 0.4270 - acc: 0.7903 - val_loss: 0.6219 - val_acc: 0.7000\n",
            "Epoch 110/300\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.56646\n",
            "620/620 - 0s - loss: 0.4202 - acc: 0.7968 - val_loss: 0.7747 - val_acc: 0.6500\n",
            "Epoch 111/300\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 0.56646\n",
            "620/620 - 0s - loss: 0.4264 - acc: 0.8097 - val_loss: 0.6401 - val_acc: 0.6900\n",
            "Epoch 112/300\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 0.56646\n",
            "620/620 - 0s - loss: 0.4314 - acc: 0.8065 - val_loss: 0.6507 - val_acc: 0.6900\n",
            "Epoch 113/300\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.56646\n",
            "620/620 - 0s - loss: 0.4455 - acc: 0.7952 - val_loss: 0.9060 - val_acc: 0.5800\n",
            "Epoch 114/300\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.56646\n",
            "620/620 - 0s - loss: 0.4192 - acc: 0.8290 - val_loss: 0.7822 - val_acc: 0.6400\n",
            "Epoch 115/300\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.56646\n",
            "620/620 - 0s - loss: 0.4540 - acc: 0.7839 - val_loss: 0.8163 - val_acc: 0.5800\n",
            "Epoch 116/300\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.56646\n",
            "620/620 - 0s - loss: 0.4191 - acc: 0.8145 - val_loss: 0.8755 - val_acc: 0.6000\n",
            "Epoch 117/300\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.56646\n",
            "620/620 - 0s - loss: 0.4219 - acc: 0.8032 - val_loss: 1.0092 - val_acc: 0.5700\n",
            "Epoch 118/300\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.56646\n",
            "620/620 - 0s - loss: 0.4200 - acc: 0.8032 - val_loss: 1.0047 - val_acc: 0.5800\n",
            "Epoch 119/300\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 0.56646\n",
            "620/620 - 0s - loss: 0.4122 - acc: 0.8290 - val_loss: 0.7493 - val_acc: 0.6400\n",
            "Epoch 120/300\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.56646\n",
            "620/620 - 0s - loss: 0.4289 - acc: 0.7871 - val_loss: 0.7663 - val_acc: 0.6300\n",
            "Epoch 121/300\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.56646\n",
            "620/620 - 0s - loss: 0.3927 - acc: 0.8097 - val_loss: 0.8030 - val_acc: 0.6200\n",
            "Epoch 122/300\n",
            "\n",
            "Epoch 00122: val_loss improved from 0.56646 to 0.54702, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 0s - loss: 0.4265 - acc: 0.8016 - val_loss: 0.5470 - val_acc: 0.6900\n",
            "Epoch 123/300\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 0.54702\n",
            "620/620 - 0s - loss: 0.4164 - acc: 0.8081 - val_loss: 0.8717 - val_acc: 0.5900\n",
            "Epoch 124/300\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 0.54702\n",
            "620/620 - 0s - loss: 0.4030 - acc: 0.8242 - val_loss: 0.7983 - val_acc: 0.6300\n",
            "Epoch 125/300\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 0.54702\n",
            "620/620 - 0s - loss: 0.3996 - acc: 0.8097 - val_loss: 0.6532 - val_acc: 0.6700\n",
            "Epoch 126/300\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 0.54702\n",
            "620/620 - 0s - loss: 0.4326 - acc: 0.7968 - val_loss: 0.6651 - val_acc: 0.6500\n",
            "Epoch 127/300\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 0.54702\n",
            "620/620 - 0s - loss: 0.4075 - acc: 0.8258 - val_loss: 0.5708 - val_acc: 0.6800\n",
            "Epoch 128/300\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 0.54702\n",
            "620/620 - 0s - loss: 0.3999 - acc: 0.8226 - val_loss: 0.7766 - val_acc: 0.6100\n",
            "Epoch 129/300\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.54702\n",
            "620/620 - 0s - loss: 0.4167 - acc: 0.8194 - val_loss: 0.8005 - val_acc: 0.6100\n",
            "Epoch 130/300\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 0.54702\n",
            "620/620 - 0s - loss: 0.4209 - acc: 0.8145 - val_loss: 1.0414 - val_acc: 0.5800\n",
            "Epoch 131/300\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 0.54702\n",
            "620/620 - 0s - loss: 0.4418 - acc: 0.7887 - val_loss: 0.7365 - val_acc: 0.6300\n",
            "Epoch 132/300\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 0.54702\n",
            "620/620 - 0s - loss: 0.3975 - acc: 0.8387 - val_loss: 0.7174 - val_acc: 0.6500\n",
            "Epoch 133/300\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.54702\n",
            "620/620 - 0s - loss: 0.4045 - acc: 0.8032 - val_loss: 0.8343 - val_acc: 0.6000\n",
            "Epoch 134/300\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.54702\n",
            "620/620 - 0s - loss: 0.4195 - acc: 0.7952 - val_loss: 0.5975 - val_acc: 0.7000\n",
            "Epoch 135/300\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 0.54702\n",
            "620/620 - 0s - loss: 0.3792 - acc: 0.8226 - val_loss: 0.7794 - val_acc: 0.6300\n",
            "Epoch 136/300\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.54702\n",
            "620/620 - 0s - loss: 0.4016 - acc: 0.8081 - val_loss: 0.6390 - val_acc: 0.6900\n",
            "Epoch 137/300\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 0.54702\n",
            "620/620 - 0s - loss: 0.4063 - acc: 0.8145 - val_loss: 0.7250 - val_acc: 0.6500\n",
            "Epoch 138/300\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.54702\n",
            "620/620 - 0s - loss: 0.4324 - acc: 0.7839 - val_loss: 0.6197 - val_acc: 0.7000\n",
            "Epoch 139/300\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.54702\n",
            "620/620 - 0s - loss: 0.4005 - acc: 0.8081 - val_loss: 0.7954 - val_acc: 0.5900\n",
            "Epoch 140/300\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 0.54702\n",
            "620/620 - 0s - loss: 0.3897 - acc: 0.8403 - val_loss: 0.7754 - val_acc: 0.6200\n",
            "Epoch 141/300\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.54702\n",
            "620/620 - 0s - loss: 0.4186 - acc: 0.7855 - val_loss: 0.5750 - val_acc: 0.6900\n",
            "Epoch 142/300\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.54702\n",
            "620/620 - 0s - loss: 0.4120 - acc: 0.8129 - val_loss: 0.7180 - val_acc: 0.6300\n",
            "Epoch 143/300\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 0.54702\n",
            "620/620 - 0s - loss: 0.4027 - acc: 0.8194 - val_loss: 0.5634 - val_acc: 0.7200\n",
            "Epoch 144/300\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 0.54702\n",
            "620/620 - 0s - loss: 0.4295 - acc: 0.7887 - val_loss: 0.6436 - val_acc: 0.6900\n",
            "Epoch 145/300\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.54702\n",
            "620/620 - 0s - loss: 0.4027 - acc: 0.8210 - val_loss: 0.6699 - val_acc: 0.7000\n",
            "Epoch 146/300\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.54702\n",
            "620/620 - 0s - loss: 0.4258 - acc: 0.7935 - val_loss: 0.8339 - val_acc: 0.5800\n",
            "Epoch 147/300\n",
            "\n",
            "Epoch 00147: val_loss improved from 0.54702 to 0.49368, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.4011 - acc: 0.8161 - val_loss: 0.4937 - val_acc: 0.7700\n",
            "Epoch 148/300\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.4025 - acc: 0.7855 - val_loss: 0.6868 - val_acc: 0.6700\n",
            "Epoch 149/300\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.4124 - acc: 0.8065 - val_loss: 0.8030 - val_acc: 0.6300\n",
            "Epoch 150/300\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.4009 - acc: 0.8226 - val_loss: 0.8327 - val_acc: 0.6000\n",
            "Epoch 151/300\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.4020 - acc: 0.8258 - val_loss: 0.7055 - val_acc: 0.6500\n",
            "Epoch 152/300\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.3844 - acc: 0.8258 - val_loss: 0.8557 - val_acc: 0.6300\n",
            "Epoch 153/300\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.4080 - acc: 0.8161 - val_loss: 0.6375 - val_acc: 0.6800\n",
            "Epoch 154/300\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.4236 - acc: 0.8210 - val_loss: 0.6310 - val_acc: 0.6700\n",
            "Epoch 155/300\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.4122 - acc: 0.7984 - val_loss: 0.6911 - val_acc: 0.6400\n",
            "Epoch 156/300\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.4115 - acc: 0.8129 - val_loss: 0.8086 - val_acc: 0.5900\n",
            "Epoch 157/300\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.4030 - acc: 0.8048 - val_loss: 0.5824 - val_acc: 0.6900\n",
            "Epoch 158/300\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.3888 - acc: 0.8290 - val_loss: 0.8165 - val_acc: 0.5900\n",
            "Epoch 159/300\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.4009 - acc: 0.8097 - val_loss: 0.7228 - val_acc: 0.6200\n",
            "Epoch 160/300\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.3984 - acc: 0.8194 - val_loss: 0.7566 - val_acc: 0.6300\n",
            "Epoch 161/300\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.3866 - acc: 0.8226 - val_loss: 0.7350 - val_acc: 0.6600\n",
            "Epoch 162/300\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.4062 - acc: 0.8161 - val_loss: 0.8137 - val_acc: 0.6200\n",
            "Epoch 163/300\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.3848 - acc: 0.8258 - val_loss: 0.7172 - val_acc: 0.6500\n",
            "Epoch 164/300\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.3761 - acc: 0.8371 - val_loss: 0.6317 - val_acc: 0.6900\n",
            "Epoch 165/300\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.4484 - acc: 0.8000 - val_loss: 0.9430 - val_acc: 0.5800\n",
            "Epoch 166/300\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.3983 - acc: 0.8258 - val_loss: 0.6067 - val_acc: 0.6800\n",
            "Epoch 167/300\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.4074 - acc: 0.8113 - val_loss: 0.6968 - val_acc: 0.6700\n",
            "Epoch 168/300\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.3698 - acc: 0.8500 - val_loss: 0.7685 - val_acc: 0.6400\n",
            "Epoch 169/300\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.3917 - acc: 0.8306 - val_loss: 0.5731 - val_acc: 0.7000\n",
            "Epoch 170/300\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.4022 - acc: 0.7984 - val_loss: 0.6090 - val_acc: 0.7000\n",
            "Epoch 171/300\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.3962 - acc: 0.8194 - val_loss: 0.6992 - val_acc: 0.6700\n",
            "Epoch 172/300\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.3825 - acc: 0.8177 - val_loss: 0.5903 - val_acc: 0.7100\n",
            "Epoch 173/300\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.3778 - acc: 0.8274 - val_loss: 0.7249 - val_acc: 0.6600\n",
            "Epoch 174/300\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.3740 - acc: 0.8306 - val_loss: 0.6647 - val_acc: 0.6800\n",
            "Epoch 175/300\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.4161 - acc: 0.8113 - val_loss: 0.6772 - val_acc: 0.6800\n",
            "Epoch 176/300\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.4092 - acc: 0.8081 - val_loss: 0.7932 - val_acc: 0.6100\n",
            "Epoch 177/300\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.3975 - acc: 0.8161 - val_loss: 0.6020 - val_acc: 0.7200\n",
            "Epoch 178/300\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.4269 - acc: 0.7839 - val_loss: 0.5765 - val_acc: 0.7000\n",
            "Epoch 179/300\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.3925 - acc: 0.8177 - val_loss: 0.8624 - val_acc: 0.6200\n",
            "Epoch 180/300\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.3858 - acc: 0.8113 - val_loss: 0.6014 - val_acc: 0.6900\n",
            "Epoch 181/300\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.4266 - acc: 0.8032 - val_loss: 0.6154 - val_acc: 0.7100\n",
            "Epoch 182/300\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.4227 - acc: 0.8016 - val_loss: 0.7424 - val_acc: 0.6300\n",
            "Epoch 183/300\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.4035 - acc: 0.8065 - val_loss: 0.8748 - val_acc: 0.5800\n",
            "Epoch 184/300\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.3991 - acc: 0.8129 - val_loss: 0.7740 - val_acc: 0.6300\n",
            "Epoch 185/300\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.3760 - acc: 0.8161 - val_loss: 0.6368 - val_acc: 0.6900\n",
            "Epoch 186/300\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.3734 - acc: 0.8323 - val_loss: 0.7755 - val_acc: 0.6500\n",
            "Epoch 187/300\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.3992 - acc: 0.8177 - val_loss: 0.7541 - val_acc: 0.6200\n",
            "Epoch 188/300\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.3798 - acc: 0.8129 - val_loss: 0.8487 - val_acc: 0.5900\n",
            "Epoch 189/300\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.3777 - acc: 0.8290 - val_loss: 0.8052 - val_acc: 0.6200\n",
            "Epoch 190/300\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.3872 - acc: 0.8113 - val_loss: 0.6394 - val_acc: 0.6800\n",
            "Epoch 191/300\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.3887 - acc: 0.8306 - val_loss: 0.6139 - val_acc: 0.7100\n",
            "Epoch 192/300\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.3696 - acc: 0.8468 - val_loss: 0.9161 - val_acc: 0.5900\n",
            "Epoch 193/300\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.4317 - acc: 0.7968 - val_loss: 0.7514 - val_acc: 0.6200\n",
            "Epoch 194/300\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.3888 - acc: 0.8194 - val_loss: 0.6632 - val_acc: 0.6900\n",
            "Epoch 195/300\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.4207 - acc: 0.8032 - val_loss: 0.8138 - val_acc: 0.6200\n",
            "Epoch 196/300\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.3834 - acc: 0.8258 - val_loss: 0.6887 - val_acc: 0.6500\n",
            "Epoch 197/300\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.4003 - acc: 0.8194 - val_loss: 0.6787 - val_acc: 0.6600\n",
            "Epoch 198/300\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.3903 - acc: 0.8081 - val_loss: 0.7901 - val_acc: 0.6300\n",
            "Epoch 199/300\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.3720 - acc: 0.8290 - val_loss: 0.7460 - val_acc: 0.6400\n",
            "Epoch 200/300\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.3750 - acc: 0.8403 - val_loss: 0.7883 - val_acc: 0.6300\n",
            "Epoch 201/300\n",
            "\n",
            "Epoch 00201: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.3659 - acc: 0.8484 - val_loss: 1.1069 - val_acc: 0.5600\n",
            "Epoch 202/300\n",
            "\n",
            "Epoch 00202: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.3777 - acc: 0.8210 - val_loss: 0.6056 - val_acc: 0.6800\n",
            "Epoch 203/300\n",
            "\n",
            "Epoch 00203: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.3835 - acc: 0.8274 - val_loss: 0.7879 - val_acc: 0.6100\n",
            "Epoch 204/300\n",
            "\n",
            "Epoch 00204: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.3556 - acc: 0.8516 - val_loss: 0.7525 - val_acc: 0.6300\n",
            "Epoch 205/300\n",
            "\n",
            "Epoch 00205: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.4101 - acc: 0.8210 - val_loss: 0.5552 - val_acc: 0.6900\n",
            "Epoch 206/300\n",
            "\n",
            "Epoch 00206: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.3884 - acc: 0.8242 - val_loss: 0.7646 - val_acc: 0.6400\n",
            "Epoch 207/300\n",
            "\n",
            "Epoch 00207: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.3922 - acc: 0.8355 - val_loss: 0.9277 - val_acc: 0.5800\n",
            "Epoch 208/300\n",
            "\n",
            "Epoch 00208: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.3755 - acc: 0.8258 - val_loss: 0.7487 - val_acc: 0.6600\n",
            "Epoch 209/300\n",
            "\n",
            "Epoch 00209: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.3881 - acc: 0.8161 - val_loss: 0.8281 - val_acc: 0.6000\n",
            "Epoch 210/300\n",
            "\n",
            "Epoch 00210: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.3825 - acc: 0.8339 - val_loss: 0.6473 - val_acc: 0.6800\n",
            "Epoch 211/300\n",
            "\n",
            "Epoch 00211: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.4179 - acc: 0.8016 - val_loss: 0.6033 - val_acc: 0.7100\n",
            "Epoch 212/300\n",
            "\n",
            "Epoch 00212: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.4347 - acc: 0.7952 - val_loss: 0.6615 - val_acc: 0.7000\n",
            "Epoch 213/300\n",
            "\n",
            "Epoch 00213: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.4209 - acc: 0.7871 - val_loss: 0.6503 - val_acc: 0.6600\n",
            "Epoch 214/300\n",
            "\n",
            "Epoch 00214: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.3967 - acc: 0.8274 - val_loss: 0.6818 - val_acc: 0.6700\n",
            "Epoch 215/300\n",
            "\n",
            "Epoch 00215: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.3901 - acc: 0.8065 - val_loss: 0.6420 - val_acc: 0.6900\n",
            "Epoch 216/300\n",
            "\n",
            "Epoch 00216: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.3942 - acc: 0.8065 - val_loss: 0.7013 - val_acc: 0.6700\n",
            "Epoch 217/300\n",
            "\n",
            "Epoch 00217: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.3747 - acc: 0.8435 - val_loss: 0.5941 - val_acc: 0.7000\n",
            "Epoch 218/300\n",
            "\n",
            "Epoch 00218: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.3888 - acc: 0.8129 - val_loss: 0.8275 - val_acc: 0.6100\n",
            "Epoch 219/300\n",
            "\n",
            "Epoch 00219: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.3729 - acc: 0.8403 - val_loss: 0.6881 - val_acc: 0.6700\n",
            "Epoch 220/300\n",
            "\n",
            "Epoch 00220: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.4080 - acc: 0.8145 - val_loss: 0.8416 - val_acc: 0.5900\n",
            "Epoch 221/300\n",
            "\n",
            "Epoch 00221: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.4097 - acc: 0.8032 - val_loss: 0.8500 - val_acc: 0.5900\n",
            "Epoch 222/300\n",
            "\n",
            "Epoch 00222: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.3560 - acc: 0.8452 - val_loss: 0.8040 - val_acc: 0.6200\n",
            "Epoch 223/300\n",
            "\n",
            "Epoch 00223: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.4035 - acc: 0.8339 - val_loss: 0.7057 - val_acc: 0.6300\n",
            "Epoch 224/300\n",
            "\n",
            "Epoch 00224: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.3560 - acc: 0.8355 - val_loss: 0.7311 - val_acc: 0.6500\n",
            "Epoch 225/300\n",
            "\n",
            "Epoch 00225: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.3771 - acc: 0.8161 - val_loss: 0.8187 - val_acc: 0.6000\n",
            "Epoch 226/300\n",
            "\n",
            "Epoch 00226: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.3610 - acc: 0.8194 - val_loss: 0.8205 - val_acc: 0.5800\n",
            "Epoch 227/300\n",
            "\n",
            "Epoch 00227: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.4043 - acc: 0.8113 - val_loss: 0.5612 - val_acc: 0.7400\n",
            "Epoch 228/300\n",
            "\n",
            "Epoch 00228: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.3773 - acc: 0.8323 - val_loss: 1.0810 - val_acc: 0.5700\n",
            "Epoch 229/300\n",
            "\n",
            "Epoch 00229: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.3670 - acc: 0.8419 - val_loss: 0.7291 - val_acc: 0.6600\n",
            "Epoch 230/300\n",
            "\n",
            "Epoch 00230: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.3993 - acc: 0.8161 - val_loss: 0.6443 - val_acc: 0.6700\n",
            "Epoch 231/300\n",
            "\n",
            "Epoch 00231: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.3946 - acc: 0.8194 - val_loss: 0.6973 - val_acc: 0.6500\n",
            "Epoch 232/300\n",
            "\n",
            "Epoch 00232: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.3453 - acc: 0.8355 - val_loss: 0.7201 - val_acc: 0.6300\n",
            "Epoch 233/300\n",
            "\n",
            "Epoch 00233: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.3913 - acc: 0.8274 - val_loss: 0.6022 - val_acc: 0.7000\n",
            "Epoch 234/300\n",
            "\n",
            "Epoch 00234: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.3726 - acc: 0.8371 - val_loss: 0.6466 - val_acc: 0.6600\n",
            "Epoch 235/300\n",
            "\n",
            "Epoch 00235: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.3987 - acc: 0.8129 - val_loss: 0.7621 - val_acc: 0.6100\n",
            "Epoch 236/300\n",
            "\n",
            "Epoch 00236: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.3796 - acc: 0.8435 - val_loss: 0.7066 - val_acc: 0.6600\n",
            "Epoch 237/300\n",
            "\n",
            "Epoch 00237: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.4002 - acc: 0.8081 - val_loss: 0.7018 - val_acc: 0.6400\n",
            "Epoch 238/300\n",
            "\n",
            "Epoch 00238: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.3773 - acc: 0.8339 - val_loss: 0.6951 - val_acc: 0.6600\n",
            "Epoch 239/300\n",
            "\n",
            "Epoch 00239: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.3779 - acc: 0.8435 - val_loss: 0.8451 - val_acc: 0.6000\n",
            "Epoch 240/300\n",
            "\n",
            "Epoch 00240: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.3459 - acc: 0.8532 - val_loss: 0.8527 - val_acc: 0.6100\n",
            "Epoch 241/300\n",
            "\n",
            "Epoch 00241: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.3495 - acc: 0.8500 - val_loss: 0.8460 - val_acc: 0.6200\n",
            "Epoch 242/300\n",
            "\n",
            "Epoch 00242: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.3742 - acc: 0.8339 - val_loss: 0.6564 - val_acc: 0.6800\n",
            "Epoch 243/300\n",
            "\n",
            "Epoch 00243: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.3764 - acc: 0.8290 - val_loss: 0.8134 - val_acc: 0.6200\n",
            "Epoch 244/300\n",
            "\n",
            "Epoch 00244: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.3751 - acc: 0.8403 - val_loss: 0.6008 - val_acc: 0.7000\n",
            "Epoch 245/300\n",
            "\n",
            "Epoch 00245: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.3610 - acc: 0.8581 - val_loss: 0.6755 - val_acc: 0.6700\n",
            "Epoch 246/300\n",
            "\n",
            "Epoch 00246: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.4060 - acc: 0.8161 - val_loss: 0.6977 - val_acc: 0.6600\n",
            "Epoch 247/300\n",
            "\n",
            "Epoch 00247: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.3808 - acc: 0.8226 - val_loss: 0.7147 - val_acc: 0.6700\n",
            "Epoch 248/300\n",
            "\n",
            "Epoch 00248: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.3611 - acc: 0.8387 - val_loss: 0.7940 - val_acc: 0.6300\n",
            "Epoch 249/300\n",
            "\n",
            "Epoch 00249: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.3603 - acc: 0.8403 - val_loss: 0.7246 - val_acc: 0.6500\n",
            "Epoch 250/300\n",
            "\n",
            "Epoch 00250: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.3948 - acc: 0.8161 - val_loss: 0.6908 - val_acc: 0.6600\n",
            "Epoch 251/300\n",
            "\n",
            "Epoch 00251: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.3610 - acc: 0.8306 - val_loss: 0.8594 - val_acc: 0.5900\n",
            "Epoch 252/300\n",
            "\n",
            "Epoch 00252: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.3681 - acc: 0.8177 - val_loss: 0.7688 - val_acc: 0.6200\n",
            "Epoch 253/300\n",
            "\n",
            "Epoch 00253: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.3846 - acc: 0.8355 - val_loss: 0.8084 - val_acc: 0.6100\n",
            "Epoch 254/300\n",
            "\n",
            "Epoch 00254: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.3716 - acc: 0.8290 - val_loss: 0.6939 - val_acc: 0.6700\n",
            "Epoch 255/300\n",
            "\n",
            "Epoch 00255: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.3713 - acc: 0.8306 - val_loss: 0.7832 - val_acc: 0.6500\n",
            "Epoch 256/300\n",
            "\n",
            "Epoch 00256: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.3899 - acc: 0.8226 - val_loss: 0.7212 - val_acc: 0.6600\n",
            "Epoch 257/300\n",
            "\n",
            "Epoch 00257: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.3841 - acc: 0.8129 - val_loss: 0.6638 - val_acc: 0.6900\n",
            "Epoch 258/300\n",
            "\n",
            "Epoch 00258: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.3691 - acc: 0.8306 - val_loss: 0.6428 - val_acc: 0.6900\n",
            "Epoch 259/300\n",
            "\n",
            "Epoch 00259: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.3636 - acc: 0.8258 - val_loss: 0.7320 - val_acc: 0.6500\n",
            "Epoch 260/300\n",
            "\n",
            "Epoch 00260: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.3605 - acc: 0.8516 - val_loss: 0.6873 - val_acc: 0.6600\n",
            "Epoch 261/300\n",
            "\n",
            "Epoch 00261: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.3625 - acc: 0.8355 - val_loss: 0.6910 - val_acc: 0.6600\n",
            "Epoch 262/300\n",
            "\n",
            "Epoch 00262: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.3506 - acc: 0.8371 - val_loss: 0.6354 - val_acc: 0.6700\n",
            "Epoch 263/300\n",
            "\n",
            "Epoch 00263: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.3723 - acc: 0.8371 - val_loss: 0.7600 - val_acc: 0.6400\n",
            "Epoch 264/300\n",
            "\n",
            "Epoch 00264: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.3635 - acc: 0.8323 - val_loss: 0.9435 - val_acc: 0.5800\n",
            "Epoch 265/300\n",
            "\n",
            "Epoch 00265: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.3563 - acc: 0.8645 - val_loss: 0.6044 - val_acc: 0.7100\n",
            "Epoch 266/300\n",
            "\n",
            "Epoch 00266: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.3852 - acc: 0.8194 - val_loss: 0.7861 - val_acc: 0.6400\n",
            "Epoch 267/300\n",
            "\n",
            "Epoch 00267: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.3832 - acc: 0.8226 - val_loss: 0.8966 - val_acc: 0.6000\n",
            "Epoch 268/300\n",
            "\n",
            "Epoch 00268: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.3462 - acc: 0.8387 - val_loss: 0.7478 - val_acc: 0.6600\n",
            "Epoch 269/300\n",
            "\n",
            "Epoch 00269: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.3412 - acc: 0.8645 - val_loss: 0.7277 - val_acc: 0.6400\n",
            "Epoch 270/300\n",
            "\n",
            "Epoch 00270: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.3805 - acc: 0.8274 - val_loss: 0.6402 - val_acc: 0.7000\n",
            "Epoch 271/300\n",
            "\n",
            "Epoch 00271: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.3821 - acc: 0.8194 - val_loss: 0.9876 - val_acc: 0.5900\n",
            "Epoch 272/300\n",
            "\n",
            "Epoch 00272: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.3806 - acc: 0.8177 - val_loss: 0.6641 - val_acc: 0.6900\n",
            "Epoch 273/300\n",
            "\n",
            "Epoch 00273: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.4123 - acc: 0.8081 - val_loss: 0.7274 - val_acc: 0.6400\n",
            "Epoch 274/300\n",
            "\n",
            "Epoch 00274: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.3568 - acc: 0.8371 - val_loss: 0.6793 - val_acc: 0.6600\n",
            "Epoch 275/300\n",
            "\n",
            "Epoch 00275: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.3872 - acc: 0.8161 - val_loss: 0.7497 - val_acc: 0.6300\n",
            "Epoch 276/300\n",
            "\n",
            "Epoch 00276: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.3478 - acc: 0.8435 - val_loss: 0.8919 - val_acc: 0.5900\n",
            "Epoch 277/300\n",
            "\n",
            "Epoch 00277: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.3590 - acc: 0.8323 - val_loss: 0.6389 - val_acc: 0.6800\n",
            "Epoch 278/300\n",
            "\n",
            "Epoch 00278: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.3884 - acc: 0.8177 - val_loss: 0.7235 - val_acc: 0.6400\n",
            "Epoch 279/300\n",
            "\n",
            "Epoch 00279: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.3810 - acc: 0.8226 - val_loss: 0.6907 - val_acc: 0.6500\n",
            "Epoch 280/300\n",
            "\n",
            "Epoch 00280: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.3548 - acc: 0.8323 - val_loss: 0.5946 - val_acc: 0.7100\n",
            "Epoch 281/300\n",
            "\n",
            "Epoch 00281: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.3326 - acc: 0.8661 - val_loss: 0.8009 - val_acc: 0.6300\n",
            "Epoch 282/300\n",
            "\n",
            "Epoch 00282: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.3449 - acc: 0.8484 - val_loss: 0.7752 - val_acc: 0.6200\n",
            "Epoch 283/300\n",
            "\n",
            "Epoch 00283: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.3884 - acc: 0.8306 - val_loss: 0.6471 - val_acc: 0.6700\n",
            "Epoch 284/300\n",
            "\n",
            "Epoch 00284: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.3759 - acc: 0.8274 - val_loss: 0.7527 - val_acc: 0.6500\n",
            "Epoch 285/300\n",
            "\n",
            "Epoch 00285: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.3689 - acc: 0.8258 - val_loss: 0.6850 - val_acc: 0.6500\n",
            "Epoch 286/300\n",
            "\n",
            "Epoch 00286: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.3886 - acc: 0.8097 - val_loss: 0.7245 - val_acc: 0.6800\n",
            "Epoch 287/300\n",
            "\n",
            "Epoch 00287: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.3752 - acc: 0.8371 - val_loss: 0.8521 - val_acc: 0.6100\n",
            "Epoch 288/300\n",
            "\n",
            "Epoch 00288: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.3722 - acc: 0.8371 - val_loss: 0.7018 - val_acc: 0.6700\n",
            "Epoch 289/300\n",
            "\n",
            "Epoch 00289: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.3840 - acc: 0.8274 - val_loss: 0.7302 - val_acc: 0.6500\n",
            "Epoch 290/300\n",
            "\n",
            "Epoch 00290: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.3904 - acc: 0.8194 - val_loss: 0.6151 - val_acc: 0.6900\n",
            "Epoch 291/300\n",
            "\n",
            "Epoch 00291: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.3693 - acc: 0.8484 - val_loss: 0.7087 - val_acc: 0.6700\n",
            "Epoch 292/300\n",
            "\n",
            "Epoch 00292: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.3842 - acc: 0.8306 - val_loss: 0.6626 - val_acc: 0.6700\n",
            "Epoch 293/300\n",
            "\n",
            "Epoch 00293: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.3800 - acc: 0.8339 - val_loss: 0.8582 - val_acc: 0.6000\n",
            "Epoch 294/300\n",
            "\n",
            "Epoch 00294: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.3518 - acc: 0.8419 - val_loss: 0.8101 - val_acc: 0.6300\n",
            "Epoch 295/300\n",
            "\n",
            "Epoch 00295: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.3617 - acc: 0.8500 - val_loss: 0.6250 - val_acc: 0.6900\n",
            "Epoch 296/300\n",
            "\n",
            "Epoch 00296: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.3488 - acc: 0.8677 - val_loss: 0.5930 - val_acc: 0.6900\n",
            "Epoch 297/300\n",
            "\n",
            "Epoch 00297: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.3549 - acc: 0.8419 - val_loss: 0.8192 - val_acc: 0.6300\n",
            "Epoch 298/300\n",
            "\n",
            "Epoch 00298: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.3695 - acc: 0.8371 - val_loss: 0.5978 - val_acc: 0.7000\n",
            "Epoch 299/300\n",
            "\n",
            "Epoch 00299: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.3561 - acc: 0.8516 - val_loss: 0.9944 - val_acc: 0.6000\n",
            "Epoch 300/300\n",
            "\n",
            "Epoch 00300: val_loss did not improve from 0.49368\n",
            "620/620 - 0s - loss: 0.3766 - acc: 0.8274 - val_loss: 0.7907 - val_acc: 0.6400\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBAQEeBEEREBAQECAF/8f4GiCIEVyXF8AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "# Evaluate on test data\n",
            "40/40 [==============================] - 0s 2ms/sample - loss: 0.5279 - acc: 0.7750\n",
            "test loss, test acc: [0.5279170392546803, 0.775]\n",
            "2\n",
            "EEG_Deep/Data2A/parsed_P02T.mat\n",
            "(0, 12, 4096)\n",
            "(80, 12, 4096)\n",
            "(80, 1)\n",
            "3\n",
            "EEG_Deep/Data2A/parsed_P03T.mat\n",
            "(80, 12, 4096)\n",
            "(160, 12, 4096)\n",
            "(160, 1)\n",
            "4\n",
            "EEG_Deep/Data2A/parsed_P04T.mat\n",
            "(160, 12, 4096)\n",
            "(240, 12, 4096)\n",
            "(240, 1)\n",
            "5\n",
            "EEG_Deep/Data2A/parsed_P05T.mat\n",
            "(240, 12, 4096)\n",
            "(320, 12, 4096)\n",
            "(320, 1)\n",
            "6\n",
            "EEG_Deep/Data2A/parsed_P06T.mat\n",
            "(320, 12, 4096)\n",
            "(400, 12, 4096)\n",
            "(400, 1)\n",
            "7\n",
            "EEG_Deep/Data2A/parsed_P07T.mat\n",
            "(400, 12, 4096)\n",
            "(480, 12, 4096)\n",
            "(480, 1)\n",
            "8\n",
            "EEG_Deep/Data2A/parsed_P08T.mat\n",
            "(480, 12, 4096)\n",
            "(560, 12, 4096)\n",
            "(560, 1)\n",
            "9\n",
            "EEG_Deep/Data2A/parsed_P09T.mat\n",
            "(560, 12, 4096)\n",
            "(640, 12, 4096)\n",
            "(640, 1)\n",
            "10\n",
            "EEG_Deep/Data2A/parsed_P010T.mat\n",
            "(640, 12, 4096)\n",
            "(720, 12, 4096)\n",
            "(720, 1)\n",
            "1\n",
            "EEG_Deep/Data2A/parsed_P01E.mat\n",
            "(0, 12, 4096)\n",
            "(40, 12, 4096)\n",
            "(40, 1)\n",
            "Filtering of training data in progress\n",
            "(720, 12, 4096)\n",
            "Filtering of test data in progress\n",
            "(40, 12, 4096)\n",
            "[2. 2. 1. 1. 2. 2. 1. 1. 2. 2. 2. 1. 1. 2. 1. 1. 1. 1. 2. 2. 1. 2. 1. 1.\n",
            " 2. 1. 1. 2. 1. 1. 1. 1. 1. 1. 2. 1. 1. 1. 2. 1. 1. 1. 2. 1. 2. 1. 1. 1.\n",
            " 1. 2. 1. 2. 1. 1. 1. 2. 1. 2. 2. 2. 1. 2. 1. 1. 1. 1. 2. 2. 1. 1. 2. 1.\n",
            " 2. 2. 2. 1. 1. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 1. 1. 2. 2.\n",
            " 1. 1. 1. 2.]\n",
            "(620, 12, 1536)\n",
            "(620,)\n",
            "(100, 12, 1536)\n",
            "(100,)\n",
            "[[0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]]\n",
            "X_train shape: (620, 1, 12, 1536)\n",
            "620 train samples\n",
            "100 val samples\n",
            "(40, 12, 1536)\n",
            "(40, 1)\n",
            "X_train shape: (40, 1, 12, 1536)\n",
            "40 train samples\n",
            "Train on 620 samples, validate on 100 samples\n",
            "Epoch 1/300\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 1.17363, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.9647 - acc: 0.5048 - val_loss: 1.1736 - val_acc: 0.4700\n",
            "Epoch 2/300\n",
            "\n",
            "Epoch 00002: val_loss improved from 1.17363 to 1.03809, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.7668 - acc: 0.5839 - val_loss: 1.0381 - val_acc: 0.4700\n",
            "Epoch 3/300\n",
            "\n",
            "Epoch 00003: val_loss improved from 1.03809 to 0.94954, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.7231 - acc: 0.6242 - val_loss: 0.9495 - val_acc: 0.4000\n",
            "Epoch 4/300\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.94954 to 0.90603, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.6880 - acc: 0.6548 - val_loss: 0.9060 - val_acc: 0.3800\n",
            "Epoch 5/300\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.90603 to 0.84461, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.6510 - acc: 0.6887 - val_loss: 0.8446 - val_acc: 0.4000\n",
            "Epoch 6/300\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.84461 to 0.80061, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.6141 - acc: 0.6903 - val_loss: 0.8006 - val_acc: 0.5000\n",
            "Epoch 7/300\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.80061 to 0.77717, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.5832 - acc: 0.7242 - val_loss: 0.7772 - val_acc: 0.5200\n",
            "Epoch 8/300\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.77717 to 0.72297, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.5497 - acc: 0.7532 - val_loss: 0.7230 - val_acc: 0.5900\n",
            "Epoch 9/300\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.72297\n",
            "620/620 - 0s - loss: 0.5583 - acc: 0.7306 - val_loss: 0.9019 - val_acc: 0.4500\n",
            "Epoch 10/300\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.72297\n",
            "620/620 - 0s - loss: 0.5407 - acc: 0.7532 - val_loss: 0.8192 - val_acc: 0.5200\n",
            "Epoch 11/300\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.72297\n",
            "620/620 - 0s - loss: 0.5286 - acc: 0.7500 - val_loss: 0.8576 - val_acc: 0.5100\n",
            "Epoch 12/300\n",
            "\n",
            "Epoch 00012: val_loss improved from 0.72297 to 0.71652, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.5246 - acc: 0.7613 - val_loss: 0.7165 - val_acc: 0.6300\n",
            "Epoch 13/300\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.71652\n",
            "620/620 - 0s - loss: 0.5229 - acc: 0.7645 - val_loss: 0.7437 - val_acc: 0.5800\n",
            "Epoch 14/300\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.71652\n",
            "620/620 - 0s - loss: 0.5217 - acc: 0.7323 - val_loss: 0.7779 - val_acc: 0.5700\n",
            "Epoch 15/300\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.71652\n",
            "620/620 - 0s - loss: 0.5247 - acc: 0.7484 - val_loss: 0.7388 - val_acc: 0.6100\n",
            "Epoch 16/300\n",
            "\n",
            "Epoch 00016: val_loss improved from 0.71652 to 0.69876, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.4858 - acc: 0.7758 - val_loss: 0.6988 - val_acc: 0.6200\n",
            "Epoch 17/300\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.69876\n",
            "620/620 - 0s - loss: 0.5047 - acc: 0.7790 - val_loss: 0.7474 - val_acc: 0.6000\n",
            "Epoch 18/300\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.69876\n",
            "620/620 - 0s - loss: 0.4980 - acc: 0.7565 - val_loss: 0.7829 - val_acc: 0.5500\n",
            "Epoch 19/300\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.69876\n",
            "620/620 - 0s - loss: 0.5017 - acc: 0.7613 - val_loss: 0.7380 - val_acc: 0.6500\n",
            "Epoch 20/300\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.69876\n",
            "620/620 - 0s - loss: 0.4785 - acc: 0.7968 - val_loss: 0.8145 - val_acc: 0.5700\n",
            "Epoch 21/300\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.69876\n",
            "620/620 - 0s - loss: 0.5122 - acc: 0.7613 - val_loss: 0.9699 - val_acc: 0.5300\n",
            "Epoch 22/300\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.69876\n",
            "620/620 - 0s - loss: 0.5035 - acc: 0.7694 - val_loss: 0.8225 - val_acc: 0.6000\n",
            "Epoch 23/300\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.69876\n",
            "620/620 - 0s - loss: 0.4961 - acc: 0.7629 - val_loss: 0.7786 - val_acc: 0.5800\n",
            "Epoch 24/300\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.69876\n",
            "620/620 - 0s - loss: 0.4600 - acc: 0.8048 - val_loss: 0.7943 - val_acc: 0.6100\n",
            "Epoch 25/300\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.69876\n",
            "620/620 - 0s - loss: 0.4754 - acc: 0.7774 - val_loss: 0.7207 - val_acc: 0.6500\n",
            "Epoch 26/300\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.69876\n",
            "620/620 - 0s - loss: 0.4863 - acc: 0.7758 - val_loss: 0.7113 - val_acc: 0.6700\n",
            "Epoch 27/300\n",
            "\n",
            "Epoch 00027: val_loss improved from 0.69876 to 0.68895, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.4645 - acc: 0.7774 - val_loss: 0.6889 - val_acc: 0.7000\n",
            "Epoch 28/300\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.68895\n",
            "620/620 - 0s - loss: 0.4662 - acc: 0.7839 - val_loss: 0.7257 - val_acc: 0.6700\n",
            "Epoch 29/300\n",
            "\n",
            "Epoch 00029: val_loss improved from 0.68895 to 0.59476, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.4771 - acc: 0.7742 - val_loss: 0.5948 - val_acc: 0.7600\n",
            "Epoch 30/300\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.59476\n",
            "620/620 - 0s - loss: 0.4653 - acc: 0.7871 - val_loss: 0.9260 - val_acc: 0.5600\n",
            "Epoch 31/300\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.59476\n",
            "620/620 - 0s - loss: 0.4827 - acc: 0.7903 - val_loss: 0.7695 - val_acc: 0.6100\n",
            "Epoch 32/300\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.59476\n",
            "620/620 - 0s - loss: 0.4610 - acc: 0.7806 - val_loss: 0.6221 - val_acc: 0.7000\n",
            "Epoch 33/300\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.59476\n",
            "620/620 - 0s - loss: 0.4529 - acc: 0.8016 - val_loss: 0.6836 - val_acc: 0.6600\n",
            "Epoch 34/300\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.59476\n",
            "620/620 - 0s - loss: 0.4556 - acc: 0.7903 - val_loss: 0.6523 - val_acc: 0.6800\n",
            "Epoch 35/300\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.59476\n",
            "620/620 - 0s - loss: 0.4744 - acc: 0.7790 - val_loss: 0.7772 - val_acc: 0.6200\n",
            "Epoch 36/300\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.59476\n",
            "620/620 - 0s - loss: 0.4474 - acc: 0.7903 - val_loss: 0.8157 - val_acc: 0.5700\n",
            "Epoch 37/300\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.59476\n",
            "620/620 - 0s - loss: 0.4895 - acc: 0.7629 - val_loss: 0.7479 - val_acc: 0.6300\n",
            "Epoch 38/300\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.59476\n",
            "620/620 - 0s - loss: 0.4627 - acc: 0.7758 - val_loss: 0.8908 - val_acc: 0.5500\n",
            "Epoch 39/300\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.59476\n",
            "620/620 - 0s - loss: 0.4512 - acc: 0.7952 - val_loss: 0.7663 - val_acc: 0.6500\n",
            "Epoch 40/300\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.59476\n",
            "620/620 - 0s - loss: 0.4463 - acc: 0.8032 - val_loss: 0.8243 - val_acc: 0.6100\n",
            "Epoch 41/300\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.59476\n",
            "620/620 - 0s - loss: 0.4795 - acc: 0.7710 - val_loss: 0.7724 - val_acc: 0.6400\n",
            "Epoch 42/300\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.59476\n",
            "620/620 - 0s - loss: 0.4355 - acc: 0.8129 - val_loss: 1.0063 - val_acc: 0.5700\n",
            "Epoch 43/300\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.59476\n",
            "620/620 - 0s - loss: 0.4462 - acc: 0.7887 - val_loss: 0.8488 - val_acc: 0.5900\n",
            "Epoch 44/300\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.59476\n",
            "620/620 - 0s - loss: 0.4281 - acc: 0.7984 - val_loss: 0.6445 - val_acc: 0.7200\n",
            "Epoch 45/300\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.59476\n",
            "620/620 - 0s - loss: 0.4345 - acc: 0.8048 - val_loss: 0.8264 - val_acc: 0.6500\n",
            "Epoch 46/300\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.59476\n",
            "620/620 - 0s - loss: 0.4771 - acc: 0.7823 - val_loss: 0.7199 - val_acc: 0.6500\n",
            "Epoch 47/300\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.59476\n",
            "620/620 - 0s - loss: 0.4577 - acc: 0.8016 - val_loss: 0.7737 - val_acc: 0.6300\n",
            "Epoch 48/300\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.59476\n",
            "620/620 - 0s - loss: 0.4695 - acc: 0.7597 - val_loss: 0.8342 - val_acc: 0.6300\n",
            "Epoch 49/300\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.59476\n",
            "620/620 - 0s - loss: 0.4427 - acc: 0.8016 - val_loss: 0.7119 - val_acc: 0.6800\n",
            "Epoch 50/300\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.59476\n",
            "620/620 - 0s - loss: 0.4597 - acc: 0.8048 - val_loss: 0.7088 - val_acc: 0.6900\n",
            "Epoch 51/300\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.59476\n",
            "620/620 - 0s - loss: 0.4586 - acc: 0.7726 - val_loss: 0.8234 - val_acc: 0.5900\n",
            "Epoch 52/300\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.59476\n",
            "620/620 - 0s - loss: 0.4456 - acc: 0.7968 - val_loss: 0.6974 - val_acc: 0.6600\n",
            "Epoch 53/300\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.59476\n",
            "620/620 - 0s - loss: 0.4695 - acc: 0.7903 - val_loss: 0.6982 - val_acc: 0.6800\n",
            "Epoch 54/300\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.59476\n",
            "620/620 - 0s - loss: 0.4339 - acc: 0.7903 - val_loss: 0.7311 - val_acc: 0.6700\n",
            "Epoch 55/300\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.59476\n",
            "620/620 - 0s - loss: 0.4398 - acc: 0.8000 - val_loss: 0.6784 - val_acc: 0.6900\n",
            "Epoch 56/300\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.59476\n",
            "620/620 - 0s - loss: 0.4512 - acc: 0.7952 - val_loss: 0.7737 - val_acc: 0.6200\n",
            "Epoch 57/300\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.59476\n",
            "620/620 - 0s - loss: 0.4570 - acc: 0.7742 - val_loss: 0.6209 - val_acc: 0.7400\n",
            "Epoch 58/300\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.59476\n",
            "620/620 - 0s - loss: 0.4690 - acc: 0.7823 - val_loss: 0.8375 - val_acc: 0.6100\n",
            "Epoch 59/300\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.59476\n",
            "620/620 - 0s - loss: 0.4338 - acc: 0.7871 - val_loss: 0.7357 - val_acc: 0.6800\n",
            "Epoch 60/300\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.59476\n",
            "620/620 - 0s - loss: 0.4350 - acc: 0.8016 - val_loss: 0.8235 - val_acc: 0.6000\n",
            "Epoch 61/300\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.59476\n",
            "620/620 - 0s - loss: 0.4303 - acc: 0.8048 - val_loss: 0.7970 - val_acc: 0.6400\n",
            "Epoch 62/300\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.59476\n",
            "620/620 - 0s - loss: 0.4015 - acc: 0.8194 - val_loss: 0.7485 - val_acc: 0.6600\n",
            "Epoch 63/300\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.59476\n",
            "620/620 - 0s - loss: 0.4105 - acc: 0.8065 - val_loss: 0.8745 - val_acc: 0.5900\n",
            "Epoch 64/300\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.59476\n",
            "620/620 - 0s - loss: 0.4195 - acc: 0.7742 - val_loss: 0.7591 - val_acc: 0.6400\n",
            "Epoch 65/300\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.59476\n",
            "620/620 - 0s - loss: 0.4198 - acc: 0.8000 - val_loss: 0.8764 - val_acc: 0.6200\n",
            "Epoch 66/300\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.59476\n",
            "620/620 - 0s - loss: 0.4505 - acc: 0.7823 - val_loss: 0.6325 - val_acc: 0.7000\n",
            "Epoch 67/300\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.59476\n",
            "620/620 - 0s - loss: 0.4378 - acc: 0.7887 - val_loss: 0.9437 - val_acc: 0.5700\n",
            "Epoch 68/300\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.59476\n",
            "620/620 - 0s - loss: 0.4460 - acc: 0.7839 - val_loss: 1.0093 - val_acc: 0.5200\n",
            "Epoch 69/300\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.59476\n",
            "620/620 - 0s - loss: 0.3920 - acc: 0.8161 - val_loss: 0.7018 - val_acc: 0.7100\n",
            "Epoch 70/300\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.59476\n",
            "620/620 - 0s - loss: 0.4041 - acc: 0.8097 - val_loss: 0.8337 - val_acc: 0.6200\n",
            "Epoch 71/300\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.59476\n",
            "620/620 - 0s - loss: 0.4302 - acc: 0.7968 - val_loss: 0.8974 - val_acc: 0.6100\n",
            "Epoch 72/300\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.59476\n",
            "620/620 - 0s - loss: 0.4365 - acc: 0.7887 - val_loss: 0.7849 - val_acc: 0.6200\n",
            "Epoch 73/300\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.59476\n",
            "620/620 - 0s - loss: 0.4297 - acc: 0.8000 - val_loss: 0.6321 - val_acc: 0.7300\n",
            "Epoch 74/300\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.59476\n",
            "620/620 - 0s - loss: 0.4601 - acc: 0.7823 - val_loss: 0.8716 - val_acc: 0.5800\n",
            "Epoch 75/300\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.59476\n",
            "620/620 - 0s - loss: 0.4420 - acc: 0.7919 - val_loss: 0.8344 - val_acc: 0.6100\n",
            "Epoch 76/300\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.59476\n",
            "620/620 - 0s - loss: 0.3994 - acc: 0.8065 - val_loss: 0.6903 - val_acc: 0.7300\n",
            "Epoch 77/300\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.59476\n",
            "620/620 - 0s - loss: 0.4332 - acc: 0.7935 - val_loss: 0.7755 - val_acc: 0.6600\n",
            "Epoch 78/300\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.59476\n",
            "620/620 - 0s - loss: 0.3931 - acc: 0.8161 - val_loss: 0.8860 - val_acc: 0.5700\n",
            "Epoch 79/300\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.59476\n",
            "620/620 - 0s - loss: 0.4170 - acc: 0.8048 - val_loss: 0.8028 - val_acc: 0.5900\n",
            "Epoch 80/300\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.59476\n",
            "620/620 - 0s - loss: 0.4088 - acc: 0.8274 - val_loss: 0.8079 - val_acc: 0.6100\n",
            "Epoch 81/300\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.59476\n",
            "620/620 - 0s - loss: 0.4395 - acc: 0.7919 - val_loss: 0.7022 - val_acc: 0.6900\n",
            "Epoch 82/300\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.59476\n",
            "620/620 - 0s - loss: 0.4239 - acc: 0.8081 - val_loss: 0.7823 - val_acc: 0.6300\n",
            "Epoch 83/300\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.59476\n",
            "620/620 - 0s - loss: 0.4175 - acc: 0.8129 - val_loss: 0.7084 - val_acc: 0.6500\n",
            "Epoch 84/300\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.59476\n",
            "620/620 - 0s - loss: 0.3909 - acc: 0.8097 - val_loss: 0.6844 - val_acc: 0.7100\n",
            "Epoch 85/300\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.59476\n",
            "620/620 - 0s - loss: 0.4089 - acc: 0.8161 - val_loss: 0.6953 - val_acc: 0.6900\n",
            "Epoch 86/300\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.59476\n",
            "620/620 - 0s - loss: 0.3716 - acc: 0.8468 - val_loss: 0.7909 - val_acc: 0.6600\n",
            "Epoch 87/300\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.59476\n",
            "620/620 - 0s - loss: 0.4104 - acc: 0.8065 - val_loss: 0.7265 - val_acc: 0.6600\n",
            "Epoch 88/300\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.59476\n",
            "620/620 - 0s - loss: 0.4075 - acc: 0.8048 - val_loss: 1.2362 - val_acc: 0.4900\n",
            "Epoch 89/300\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.59476\n",
            "620/620 - 0s - loss: 0.4354 - acc: 0.8032 - val_loss: 0.8683 - val_acc: 0.6200\n",
            "Epoch 90/300\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.59476\n",
            "620/620 - 0s - loss: 0.3896 - acc: 0.8226 - val_loss: 0.8422 - val_acc: 0.6500\n",
            "Epoch 91/300\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.59476\n",
            "620/620 - 0s - loss: 0.4278 - acc: 0.7855 - val_loss: 0.7400 - val_acc: 0.6400\n",
            "Epoch 92/300\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.59476\n",
            "620/620 - 0s - loss: 0.3924 - acc: 0.8306 - val_loss: 0.8817 - val_acc: 0.6100\n",
            "Epoch 93/300\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.59476\n",
            "620/620 - 0s - loss: 0.4147 - acc: 0.8016 - val_loss: 0.7294 - val_acc: 0.6600\n",
            "Epoch 94/300\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.59476\n",
            "620/620 - 0s - loss: 0.3895 - acc: 0.8226 - val_loss: 0.9071 - val_acc: 0.5500\n",
            "Epoch 95/300\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.59476\n",
            "620/620 - 0s - loss: 0.4307 - acc: 0.8113 - val_loss: 0.7218 - val_acc: 0.6500\n",
            "Epoch 96/300\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.59476\n",
            "620/620 - 0s - loss: 0.3643 - acc: 0.8339 - val_loss: 0.8060 - val_acc: 0.6200\n",
            "Epoch 97/300\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.59476\n",
            "620/620 - 0s - loss: 0.4072 - acc: 0.8048 - val_loss: 0.7425 - val_acc: 0.6400\n",
            "Epoch 98/300\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.59476\n",
            "620/620 - 0s - loss: 0.3805 - acc: 0.8387 - val_loss: 0.9277 - val_acc: 0.5600\n",
            "Epoch 99/300\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.59476\n",
            "620/620 - 0s - loss: 0.3920 - acc: 0.8226 - val_loss: 0.8386 - val_acc: 0.6400\n",
            "Epoch 100/300\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.59476\n",
            "620/620 - 0s - loss: 0.3599 - acc: 0.8306 - val_loss: 0.7160 - val_acc: 0.6800\n",
            "Epoch 101/300\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.59476\n",
            "620/620 - 0s - loss: 0.3884 - acc: 0.8242 - val_loss: 0.8375 - val_acc: 0.6300\n",
            "Epoch 102/300\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.59476\n",
            "620/620 - 0s - loss: 0.3688 - acc: 0.8500 - val_loss: 0.8242 - val_acc: 0.6000\n",
            "Epoch 103/300\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.59476\n",
            "620/620 - 0s - loss: 0.3906 - acc: 0.8290 - val_loss: 0.7500 - val_acc: 0.6100\n",
            "Epoch 104/300\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.59476\n",
            "620/620 - 0s - loss: 0.3911 - acc: 0.8387 - val_loss: 0.9026 - val_acc: 0.5900\n",
            "Epoch 105/300\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 0.59476\n",
            "620/620 - 0s - loss: 0.3873 - acc: 0.8306 - val_loss: 0.6233 - val_acc: 0.7200\n",
            "Epoch 106/300\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 0.59476\n",
            "620/620 - 0s - loss: 0.4113 - acc: 0.8065 - val_loss: 0.7700 - val_acc: 0.6300\n",
            "Epoch 107/300\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.59476\n",
            "620/620 - 0s - loss: 0.3887 - acc: 0.8500 - val_loss: 0.7838 - val_acc: 0.6100\n",
            "Epoch 108/300\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 0.59476\n",
            "620/620 - 0s - loss: 0.3925 - acc: 0.8306 - val_loss: 0.7805 - val_acc: 0.6500\n",
            "Epoch 109/300\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.59476\n",
            "620/620 - 0s - loss: 0.4038 - acc: 0.8274 - val_loss: 0.8556 - val_acc: 0.5900\n",
            "Epoch 110/300\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.59476\n",
            "620/620 - 0s - loss: 0.3767 - acc: 0.8177 - val_loss: 0.9881 - val_acc: 0.5500\n",
            "Epoch 111/300\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 0.59476\n",
            "620/620 - 0s - loss: 0.3929 - acc: 0.8210 - val_loss: 0.7237 - val_acc: 0.7000\n",
            "Epoch 112/300\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 0.59476\n",
            "620/620 - 0s - loss: 0.4593 - acc: 0.7742 - val_loss: 0.9815 - val_acc: 0.5100\n",
            "Epoch 113/300\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.59476\n",
            "620/620 - 0s - loss: 0.3908 - acc: 0.8226 - val_loss: 0.8502 - val_acc: 0.6200\n",
            "Epoch 114/300\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.59476\n",
            "620/620 - 0s - loss: 0.4059 - acc: 0.8161 - val_loss: 0.6199 - val_acc: 0.7300\n",
            "Epoch 115/300\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.59476\n",
            "620/620 - 0s - loss: 0.3975 - acc: 0.8194 - val_loss: 0.7465 - val_acc: 0.6600\n",
            "Epoch 116/300\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.59476\n",
            "620/620 - 0s - loss: 0.3938 - acc: 0.8387 - val_loss: 0.7678 - val_acc: 0.6800\n",
            "Epoch 117/300\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.59476\n",
            "620/620 - 0s - loss: 0.3942 - acc: 0.8177 - val_loss: 0.7868 - val_acc: 0.6400\n",
            "Epoch 118/300\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.59476\n",
            "620/620 - 0s - loss: 0.3965 - acc: 0.8210 - val_loss: 0.8039 - val_acc: 0.6600\n",
            "Epoch 119/300\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 0.59476\n",
            "620/620 - 0s - loss: 0.4054 - acc: 0.8065 - val_loss: 1.0457 - val_acc: 0.5100\n",
            "Epoch 120/300\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.59476\n",
            "620/620 - 0s - loss: 0.4049 - acc: 0.8048 - val_loss: 0.8611 - val_acc: 0.6300\n",
            "Epoch 121/300\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.59476\n",
            "620/620 - 0s - loss: 0.4078 - acc: 0.8016 - val_loss: 0.7358 - val_acc: 0.6800\n",
            "Epoch 122/300\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.59476\n",
            "620/620 - 0s - loss: 0.3920 - acc: 0.8194 - val_loss: 0.9676 - val_acc: 0.6000\n",
            "Epoch 123/300\n",
            "\n",
            "Epoch 00123: val_loss improved from 0.59476 to 0.58209, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.3942 - acc: 0.8323 - val_loss: 0.5821 - val_acc: 0.7500\n",
            "Epoch 124/300\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.4183 - acc: 0.8065 - val_loss: 0.9275 - val_acc: 0.5400\n",
            "Epoch 125/300\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3876 - acc: 0.8226 - val_loss: 0.8091 - val_acc: 0.6000\n",
            "Epoch 126/300\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3687 - acc: 0.8306 - val_loss: 0.9394 - val_acc: 0.5800\n",
            "Epoch 127/300\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3499 - acc: 0.8452 - val_loss: 0.7917 - val_acc: 0.6400\n",
            "Epoch 128/300\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.4042 - acc: 0.8097 - val_loss: 0.8606 - val_acc: 0.6500\n",
            "Epoch 129/300\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3897 - acc: 0.8323 - val_loss: 0.9968 - val_acc: 0.5500\n",
            "Epoch 130/300\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3979 - acc: 0.8194 - val_loss: 0.8472 - val_acc: 0.6300\n",
            "Epoch 131/300\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3899 - acc: 0.8081 - val_loss: 0.7341 - val_acc: 0.6600\n",
            "Epoch 132/300\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3929 - acc: 0.8323 - val_loss: 0.8539 - val_acc: 0.6100\n",
            "Epoch 133/300\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3961 - acc: 0.8194 - val_loss: 1.0862 - val_acc: 0.5200\n",
            "Epoch 134/300\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3978 - acc: 0.8194 - val_loss: 0.9414 - val_acc: 0.5900\n",
            "Epoch 135/300\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3697 - acc: 0.8274 - val_loss: 0.8994 - val_acc: 0.6300\n",
            "Epoch 136/300\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3857 - acc: 0.8323 - val_loss: 0.8185 - val_acc: 0.6100\n",
            "Epoch 137/300\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3760 - acc: 0.8274 - val_loss: 0.8222 - val_acc: 0.6400\n",
            "Epoch 138/300\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3699 - acc: 0.8403 - val_loss: 0.8232 - val_acc: 0.6200\n",
            "Epoch 139/300\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3552 - acc: 0.8435 - val_loss: 0.6471 - val_acc: 0.7400\n",
            "Epoch 140/300\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3764 - acc: 0.8339 - val_loss: 0.7692 - val_acc: 0.6400\n",
            "Epoch 141/300\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3703 - acc: 0.8339 - val_loss: 0.8961 - val_acc: 0.5800\n",
            "Epoch 142/300\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3886 - acc: 0.8210 - val_loss: 0.7556 - val_acc: 0.6700\n",
            "Epoch 143/300\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3783 - acc: 0.8339 - val_loss: 0.7737 - val_acc: 0.6400\n",
            "Epoch 144/300\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3727 - acc: 0.8290 - val_loss: 0.6604 - val_acc: 0.6600\n",
            "Epoch 145/300\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3912 - acc: 0.8290 - val_loss: 0.7000 - val_acc: 0.6800\n",
            "Epoch 146/300\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3686 - acc: 0.8371 - val_loss: 0.7616 - val_acc: 0.6500\n",
            "Epoch 147/300\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3901 - acc: 0.8258 - val_loss: 0.8668 - val_acc: 0.5600\n",
            "Epoch 148/300\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3805 - acc: 0.8242 - val_loss: 0.6961 - val_acc: 0.6900\n",
            "Epoch 149/300\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3770 - acc: 0.8419 - val_loss: 0.7690 - val_acc: 0.6500\n",
            "Epoch 150/300\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3517 - acc: 0.8565 - val_loss: 0.8283 - val_acc: 0.6700\n",
            "Epoch 151/300\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3831 - acc: 0.8306 - val_loss: 0.8122 - val_acc: 0.6400\n",
            "Epoch 152/300\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3609 - acc: 0.8355 - val_loss: 0.7018 - val_acc: 0.6800\n",
            "Epoch 153/300\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3566 - acc: 0.8403 - val_loss: 0.8232 - val_acc: 0.6000\n",
            "Epoch 154/300\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3908 - acc: 0.8161 - val_loss: 0.9754 - val_acc: 0.5500\n",
            "Epoch 155/300\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3979 - acc: 0.8371 - val_loss: 0.8032 - val_acc: 0.6300\n",
            "Epoch 156/300\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3530 - acc: 0.8435 - val_loss: 0.9020 - val_acc: 0.6100\n",
            "Epoch 157/300\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3772 - acc: 0.8226 - val_loss: 0.8063 - val_acc: 0.6000\n",
            "Epoch 158/300\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3861 - acc: 0.8161 - val_loss: 1.1105 - val_acc: 0.4800\n",
            "Epoch 159/300\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3588 - acc: 0.8306 - val_loss: 0.9040 - val_acc: 0.5700\n",
            "Epoch 160/300\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3610 - acc: 0.8387 - val_loss: 0.7243 - val_acc: 0.6900\n",
            "Epoch 161/300\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3479 - acc: 0.8548 - val_loss: 0.8751 - val_acc: 0.6400\n",
            "Epoch 162/300\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3912 - acc: 0.8210 - val_loss: 1.0407 - val_acc: 0.5600\n",
            "Epoch 163/300\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3657 - acc: 0.8371 - val_loss: 0.6784 - val_acc: 0.7100\n",
            "Epoch 164/300\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3715 - acc: 0.8258 - val_loss: 0.7709 - val_acc: 0.6300\n",
            "Epoch 165/300\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3501 - acc: 0.8694 - val_loss: 0.9132 - val_acc: 0.6300\n",
            "Epoch 166/300\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3899 - acc: 0.8161 - val_loss: 0.8978 - val_acc: 0.6400\n",
            "Epoch 167/300\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3450 - acc: 0.8468 - val_loss: 0.6496 - val_acc: 0.7100\n",
            "Epoch 168/300\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3957 - acc: 0.8145 - val_loss: 0.8177 - val_acc: 0.6400\n",
            "Epoch 169/300\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3418 - acc: 0.8419 - val_loss: 0.9383 - val_acc: 0.6200\n",
            "Epoch 170/300\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3673 - acc: 0.8435 - val_loss: 0.8122 - val_acc: 0.5900\n",
            "Epoch 171/300\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3652 - acc: 0.8387 - val_loss: 0.7251 - val_acc: 0.6800\n",
            "Epoch 172/300\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3523 - acc: 0.8371 - val_loss: 0.6779 - val_acc: 0.6900\n",
            "Epoch 173/300\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3657 - acc: 0.8500 - val_loss: 0.7252 - val_acc: 0.7000\n",
            "Epoch 174/300\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3840 - acc: 0.8419 - val_loss: 0.8104 - val_acc: 0.6600\n",
            "Epoch 175/300\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3662 - acc: 0.8532 - val_loss: 0.8299 - val_acc: 0.6200\n",
            "Epoch 176/300\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3817 - acc: 0.8339 - val_loss: 0.7860 - val_acc: 0.6200\n",
            "Epoch 177/300\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3644 - acc: 0.8419 - val_loss: 0.8438 - val_acc: 0.6300\n",
            "Epoch 178/300\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3657 - acc: 0.8355 - val_loss: 0.9854 - val_acc: 0.5700\n",
            "Epoch 179/300\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3796 - acc: 0.8323 - val_loss: 0.7278 - val_acc: 0.6600\n",
            "Epoch 180/300\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3534 - acc: 0.8452 - val_loss: 0.8005 - val_acc: 0.6800\n",
            "Epoch 181/300\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3418 - acc: 0.8629 - val_loss: 0.8050 - val_acc: 0.6500\n",
            "Epoch 182/300\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3633 - acc: 0.8419 - val_loss: 1.1005 - val_acc: 0.5400\n",
            "Epoch 183/300\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.4048 - acc: 0.8371 - val_loss: 0.6786 - val_acc: 0.6700\n",
            "Epoch 184/300\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3619 - acc: 0.8484 - val_loss: 0.9481 - val_acc: 0.5600\n",
            "Epoch 185/300\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3445 - acc: 0.8500 - val_loss: 0.7666 - val_acc: 0.6500\n",
            "Epoch 186/300\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3622 - acc: 0.8452 - val_loss: 1.3515 - val_acc: 0.4500\n",
            "Epoch 187/300\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3533 - acc: 0.8468 - val_loss: 0.8494 - val_acc: 0.6200\n",
            "Epoch 188/300\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3530 - acc: 0.8629 - val_loss: 0.7518 - val_acc: 0.6600\n",
            "Epoch 189/300\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3842 - acc: 0.8306 - val_loss: 0.8533 - val_acc: 0.6000\n",
            "Epoch 190/300\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3832 - acc: 0.8306 - val_loss: 0.8573 - val_acc: 0.6200\n",
            "Epoch 191/300\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3840 - acc: 0.8242 - val_loss: 0.8261 - val_acc: 0.6100\n",
            "Epoch 192/300\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3688 - acc: 0.8274 - val_loss: 0.8863 - val_acc: 0.5800\n",
            "Epoch 193/300\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3494 - acc: 0.8387 - val_loss: 0.7686 - val_acc: 0.6800\n",
            "Epoch 194/300\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3676 - acc: 0.8435 - val_loss: 0.9347 - val_acc: 0.5700\n",
            "Epoch 195/300\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3713 - acc: 0.8339 - val_loss: 0.8952 - val_acc: 0.6000\n",
            "Epoch 196/300\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3299 - acc: 0.8645 - val_loss: 0.8748 - val_acc: 0.6000\n",
            "Epoch 197/300\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3359 - acc: 0.8565 - val_loss: 1.0540 - val_acc: 0.5400\n",
            "Epoch 198/300\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3713 - acc: 0.8177 - val_loss: 0.7961 - val_acc: 0.6600\n",
            "Epoch 199/300\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3492 - acc: 0.8484 - val_loss: 0.7940 - val_acc: 0.6800\n",
            "Epoch 200/300\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3494 - acc: 0.8355 - val_loss: 0.8973 - val_acc: 0.6200\n",
            "Epoch 201/300\n",
            "\n",
            "Epoch 00201: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3650 - acc: 0.8516 - val_loss: 0.7931 - val_acc: 0.6600\n",
            "Epoch 202/300\n",
            "\n",
            "Epoch 00202: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3785 - acc: 0.8145 - val_loss: 0.8520 - val_acc: 0.6200\n",
            "Epoch 203/300\n",
            "\n",
            "Epoch 00203: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3634 - acc: 0.8452 - val_loss: 0.8814 - val_acc: 0.6000\n",
            "Epoch 204/300\n",
            "\n",
            "Epoch 00204: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3646 - acc: 0.8290 - val_loss: 0.8703 - val_acc: 0.6500\n",
            "Epoch 205/300\n",
            "\n",
            "Epoch 00205: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3585 - acc: 0.8371 - val_loss: 0.9324 - val_acc: 0.5700\n",
            "Epoch 206/300\n",
            "\n",
            "Epoch 00206: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3646 - acc: 0.8371 - val_loss: 0.9201 - val_acc: 0.5600\n",
            "Epoch 207/300\n",
            "\n",
            "Epoch 00207: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3519 - acc: 0.8355 - val_loss: 0.8584 - val_acc: 0.6200\n",
            "Epoch 208/300\n",
            "\n",
            "Epoch 00208: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3649 - acc: 0.8371 - val_loss: 0.7229 - val_acc: 0.6800\n",
            "Epoch 209/300\n",
            "\n",
            "Epoch 00209: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3763 - acc: 0.8306 - val_loss: 0.8634 - val_acc: 0.6200\n",
            "Epoch 210/300\n",
            "\n",
            "Epoch 00210: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3763 - acc: 0.8339 - val_loss: 0.7276 - val_acc: 0.6700\n",
            "Epoch 211/300\n",
            "\n",
            "Epoch 00211: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3569 - acc: 0.8387 - val_loss: 0.8202 - val_acc: 0.6300\n",
            "Epoch 212/300\n",
            "\n",
            "Epoch 00212: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3751 - acc: 0.8194 - val_loss: 0.8501 - val_acc: 0.6000\n",
            "Epoch 213/300\n",
            "\n",
            "Epoch 00213: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3220 - acc: 0.8726 - val_loss: 0.7439 - val_acc: 0.6800\n",
            "Epoch 214/300\n",
            "\n",
            "Epoch 00214: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3511 - acc: 0.8581 - val_loss: 0.8052 - val_acc: 0.6400\n",
            "Epoch 215/300\n",
            "\n",
            "Epoch 00215: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3635 - acc: 0.8226 - val_loss: 0.8860 - val_acc: 0.6100\n",
            "Epoch 216/300\n",
            "\n",
            "Epoch 00216: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3177 - acc: 0.8677 - val_loss: 0.8999 - val_acc: 0.6300\n",
            "Epoch 217/300\n",
            "\n",
            "Epoch 00217: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3621 - acc: 0.8419 - val_loss: 0.8701 - val_acc: 0.6400\n",
            "Epoch 218/300\n",
            "\n",
            "Epoch 00218: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3515 - acc: 0.8484 - val_loss: 0.7764 - val_acc: 0.6500\n",
            "Epoch 219/300\n",
            "\n",
            "Epoch 00219: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3642 - acc: 0.8258 - val_loss: 0.8889 - val_acc: 0.6400\n",
            "Epoch 220/300\n",
            "\n",
            "Epoch 00220: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3537 - acc: 0.8484 - val_loss: 0.8784 - val_acc: 0.6400\n",
            "Epoch 221/300\n",
            "\n",
            "Epoch 00221: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3715 - acc: 0.8355 - val_loss: 0.6442 - val_acc: 0.7000\n",
            "Epoch 222/300\n",
            "\n",
            "Epoch 00222: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3397 - acc: 0.8613 - val_loss: 0.8539 - val_acc: 0.6500\n",
            "Epoch 223/300\n",
            "\n",
            "Epoch 00223: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3718 - acc: 0.8306 - val_loss: 0.8687 - val_acc: 0.6000\n",
            "Epoch 224/300\n",
            "\n",
            "Epoch 00224: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3437 - acc: 0.8516 - val_loss: 0.8513 - val_acc: 0.6200\n",
            "Epoch 225/300\n",
            "\n",
            "Epoch 00225: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3355 - acc: 0.8290 - val_loss: 0.8800 - val_acc: 0.6200\n",
            "Epoch 226/300\n",
            "\n",
            "Epoch 00226: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3229 - acc: 0.8677 - val_loss: 0.9245 - val_acc: 0.6000\n",
            "Epoch 227/300\n",
            "\n",
            "Epoch 00227: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3562 - acc: 0.8371 - val_loss: 0.7594 - val_acc: 0.6800\n",
            "Epoch 228/300\n",
            "\n",
            "Epoch 00228: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3437 - acc: 0.8387 - val_loss: 0.8075 - val_acc: 0.6200\n",
            "Epoch 229/300\n",
            "\n",
            "Epoch 00229: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3454 - acc: 0.8355 - val_loss: 0.8383 - val_acc: 0.6000\n",
            "Epoch 230/300\n",
            "\n",
            "Epoch 00230: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3716 - acc: 0.8371 - val_loss: 0.8509 - val_acc: 0.6100\n",
            "Epoch 231/300\n",
            "\n",
            "Epoch 00231: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3591 - acc: 0.8242 - val_loss: 0.9259 - val_acc: 0.5600\n",
            "Epoch 232/300\n",
            "\n",
            "Epoch 00232: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3584 - acc: 0.8435 - val_loss: 0.6630 - val_acc: 0.7200\n",
            "Epoch 233/300\n",
            "\n",
            "Epoch 00233: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3476 - acc: 0.8565 - val_loss: 0.8909 - val_acc: 0.6100\n",
            "Epoch 234/300\n",
            "\n",
            "Epoch 00234: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3479 - acc: 0.8387 - val_loss: 0.8350 - val_acc: 0.6400\n",
            "Epoch 235/300\n",
            "\n",
            "Epoch 00235: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3624 - acc: 0.8258 - val_loss: 0.7436 - val_acc: 0.6400\n",
            "Epoch 236/300\n",
            "\n",
            "Epoch 00236: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3634 - acc: 0.8435 - val_loss: 0.8205 - val_acc: 0.6400\n",
            "Epoch 237/300\n",
            "\n",
            "Epoch 00237: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3561 - acc: 0.8274 - val_loss: 0.8836 - val_acc: 0.5900\n",
            "Epoch 238/300\n",
            "\n",
            "Epoch 00238: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3457 - acc: 0.8532 - val_loss: 0.8087 - val_acc: 0.6200\n",
            "Epoch 239/300\n",
            "\n",
            "Epoch 00239: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3354 - acc: 0.8694 - val_loss: 0.8583 - val_acc: 0.6300\n",
            "Epoch 240/300\n",
            "\n",
            "Epoch 00240: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3712 - acc: 0.8306 - val_loss: 0.7929 - val_acc: 0.6300\n",
            "Epoch 241/300\n",
            "\n",
            "Epoch 00241: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3593 - acc: 0.8387 - val_loss: 0.7554 - val_acc: 0.6600\n",
            "Epoch 242/300\n",
            "\n",
            "Epoch 00242: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3457 - acc: 0.8710 - val_loss: 0.9356 - val_acc: 0.6500\n",
            "Epoch 243/300\n",
            "\n",
            "Epoch 00243: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3531 - acc: 0.8419 - val_loss: 1.1242 - val_acc: 0.5600\n",
            "Epoch 244/300\n",
            "\n",
            "Epoch 00244: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3314 - acc: 0.8613 - val_loss: 0.8051 - val_acc: 0.6400\n",
            "Epoch 245/300\n",
            "\n",
            "Epoch 00245: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3464 - acc: 0.8371 - val_loss: 0.9422 - val_acc: 0.6000\n",
            "Epoch 246/300\n",
            "\n",
            "Epoch 00246: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3372 - acc: 0.8516 - val_loss: 0.7442 - val_acc: 0.6700\n",
            "Epoch 247/300\n",
            "\n",
            "Epoch 00247: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3518 - acc: 0.8355 - val_loss: 0.8797 - val_acc: 0.5900\n",
            "Epoch 248/300\n",
            "\n",
            "Epoch 00248: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3716 - acc: 0.8355 - val_loss: 0.8399 - val_acc: 0.5900\n",
            "Epoch 249/300\n",
            "\n",
            "Epoch 00249: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3477 - acc: 0.8435 - val_loss: 1.0345 - val_acc: 0.5700\n",
            "Epoch 250/300\n",
            "\n",
            "Epoch 00250: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3682 - acc: 0.8258 - val_loss: 0.7681 - val_acc: 0.6800\n",
            "Epoch 251/300\n",
            "\n",
            "Epoch 00251: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3468 - acc: 0.8500 - val_loss: 0.7226 - val_acc: 0.7100\n",
            "Epoch 252/300\n",
            "\n",
            "Epoch 00252: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3347 - acc: 0.8661 - val_loss: 0.6983 - val_acc: 0.6700\n",
            "Epoch 253/300\n",
            "\n",
            "Epoch 00253: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3495 - acc: 0.8613 - val_loss: 0.7244 - val_acc: 0.6900\n",
            "Epoch 254/300\n",
            "\n",
            "Epoch 00254: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3570 - acc: 0.8532 - val_loss: 0.6698 - val_acc: 0.7200\n",
            "Epoch 255/300\n",
            "\n",
            "Epoch 00255: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3235 - acc: 0.8548 - val_loss: 0.7845 - val_acc: 0.6600\n",
            "Epoch 256/300\n",
            "\n",
            "Epoch 00256: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3442 - acc: 0.8468 - val_loss: 0.7836 - val_acc: 0.6300\n",
            "Epoch 257/300\n",
            "\n",
            "Epoch 00257: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3414 - acc: 0.8468 - val_loss: 0.8573 - val_acc: 0.6600\n",
            "Epoch 258/300\n",
            "\n",
            "Epoch 00258: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3146 - acc: 0.8677 - val_loss: 0.8740 - val_acc: 0.6400\n",
            "Epoch 259/300\n",
            "\n",
            "Epoch 00259: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3421 - acc: 0.8468 - val_loss: 0.7542 - val_acc: 0.6700\n",
            "Epoch 260/300\n",
            "\n",
            "Epoch 00260: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3242 - acc: 0.8435 - val_loss: 0.9073 - val_acc: 0.6100\n",
            "Epoch 261/300\n",
            "\n",
            "Epoch 00261: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3403 - acc: 0.8371 - val_loss: 0.9151 - val_acc: 0.6000\n",
            "Epoch 262/300\n",
            "\n",
            "Epoch 00262: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3154 - acc: 0.8645 - val_loss: 0.8296 - val_acc: 0.6900\n",
            "Epoch 263/300\n",
            "\n",
            "Epoch 00263: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3199 - acc: 0.8435 - val_loss: 0.7843 - val_acc: 0.6800\n",
            "Epoch 264/300\n",
            "\n",
            "Epoch 00264: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3621 - acc: 0.8452 - val_loss: 0.8594 - val_acc: 0.6300\n",
            "Epoch 265/300\n",
            "\n",
            "Epoch 00265: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3362 - acc: 0.8532 - val_loss: 0.8910 - val_acc: 0.6200\n",
            "Epoch 266/300\n",
            "\n",
            "Epoch 00266: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3511 - acc: 0.8290 - val_loss: 0.7617 - val_acc: 0.6700\n",
            "Epoch 267/300\n",
            "\n",
            "Epoch 00267: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3599 - acc: 0.8403 - val_loss: 0.8965 - val_acc: 0.6500\n",
            "Epoch 268/300\n",
            "\n",
            "Epoch 00268: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3441 - acc: 0.8387 - val_loss: 0.8954 - val_acc: 0.6400\n",
            "Epoch 269/300\n",
            "\n",
            "Epoch 00269: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3661 - acc: 0.8355 - val_loss: 0.8313 - val_acc: 0.6300\n",
            "Epoch 270/300\n",
            "\n",
            "Epoch 00270: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3347 - acc: 0.8548 - val_loss: 1.0619 - val_acc: 0.5600\n",
            "Epoch 271/300\n",
            "\n",
            "Epoch 00271: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3418 - acc: 0.8500 - val_loss: 0.8386 - val_acc: 0.6600\n",
            "Epoch 272/300\n",
            "\n",
            "Epoch 00272: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3120 - acc: 0.8645 - val_loss: 0.9319 - val_acc: 0.6400\n",
            "Epoch 273/300\n",
            "\n",
            "Epoch 00273: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3506 - acc: 0.8435 - val_loss: 0.9675 - val_acc: 0.5900\n",
            "Epoch 274/300\n",
            "\n",
            "Epoch 00274: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3663 - acc: 0.8355 - val_loss: 0.8606 - val_acc: 0.6100\n",
            "Epoch 275/300\n",
            "\n",
            "Epoch 00275: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3512 - acc: 0.8226 - val_loss: 0.9322 - val_acc: 0.6000\n",
            "Epoch 276/300\n",
            "\n",
            "Epoch 00276: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3408 - acc: 0.8435 - val_loss: 0.8421 - val_acc: 0.6800\n",
            "Epoch 277/300\n",
            "\n",
            "Epoch 00277: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3459 - acc: 0.8597 - val_loss: 0.6757 - val_acc: 0.6800\n",
            "Epoch 278/300\n",
            "\n",
            "Epoch 00278: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3335 - acc: 0.8548 - val_loss: 0.8318 - val_acc: 0.6200\n",
            "Epoch 279/300\n",
            "\n",
            "Epoch 00279: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.2996 - acc: 0.8806 - val_loss: 0.8786 - val_acc: 0.6300\n",
            "Epoch 280/300\n",
            "\n",
            "Epoch 00280: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3182 - acc: 0.8661 - val_loss: 0.7680 - val_acc: 0.6600\n",
            "Epoch 281/300\n",
            "\n",
            "Epoch 00281: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3266 - acc: 0.8581 - val_loss: 1.1180 - val_acc: 0.5200\n",
            "Epoch 282/300\n",
            "\n",
            "Epoch 00282: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3611 - acc: 0.8355 - val_loss: 0.7427 - val_acc: 0.7000\n",
            "Epoch 283/300\n",
            "\n",
            "Epoch 00283: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3536 - acc: 0.8548 - val_loss: 0.7230 - val_acc: 0.6800\n",
            "Epoch 284/300\n",
            "\n",
            "Epoch 00284: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3541 - acc: 0.8468 - val_loss: 0.8114 - val_acc: 0.6200\n",
            "Epoch 285/300\n",
            "\n",
            "Epoch 00285: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3190 - acc: 0.8452 - val_loss: 0.7731 - val_acc: 0.6300\n",
            "Epoch 286/300\n",
            "\n",
            "Epoch 00286: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3202 - acc: 0.8661 - val_loss: 0.9870 - val_acc: 0.6100\n",
            "Epoch 287/300\n",
            "\n",
            "Epoch 00287: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3468 - acc: 0.8516 - val_loss: 0.8469 - val_acc: 0.6100\n",
            "Epoch 288/300\n",
            "\n",
            "Epoch 00288: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3372 - acc: 0.8403 - val_loss: 0.6139 - val_acc: 0.7100\n",
            "Epoch 289/300\n",
            "\n",
            "Epoch 00289: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3814 - acc: 0.8226 - val_loss: 0.8197 - val_acc: 0.6300\n",
            "Epoch 290/300\n",
            "\n",
            "Epoch 00290: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3362 - acc: 0.8774 - val_loss: 0.8195 - val_acc: 0.6400\n",
            "Epoch 291/300\n",
            "\n",
            "Epoch 00291: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3659 - acc: 0.8274 - val_loss: 0.9018 - val_acc: 0.6700\n",
            "Epoch 292/300\n",
            "\n",
            "Epoch 00292: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3379 - acc: 0.8532 - val_loss: 0.9353 - val_acc: 0.5500\n",
            "Epoch 293/300\n",
            "\n",
            "Epoch 00293: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3399 - acc: 0.8613 - val_loss: 0.7733 - val_acc: 0.6800\n",
            "Epoch 294/300\n",
            "\n",
            "Epoch 00294: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3256 - acc: 0.8742 - val_loss: 0.9130 - val_acc: 0.6400\n",
            "Epoch 295/300\n",
            "\n",
            "Epoch 00295: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3577 - acc: 0.8419 - val_loss: 0.9326 - val_acc: 0.6100\n",
            "Epoch 296/300\n",
            "\n",
            "Epoch 00296: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3516 - acc: 0.8468 - val_loss: 0.7900 - val_acc: 0.6400\n",
            "Epoch 297/300\n",
            "\n",
            "Epoch 00297: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3283 - acc: 0.8629 - val_loss: 0.8511 - val_acc: 0.5800\n",
            "Epoch 298/300\n",
            "\n",
            "Epoch 00298: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3396 - acc: 0.8581 - val_loss: 0.8213 - val_acc: 0.6300\n",
            "Epoch 299/300\n",
            "\n",
            "Epoch 00299: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3724 - acc: 0.8226 - val_loss: 0.7050 - val_acc: 0.6700\n",
            "Epoch 300/300\n",
            "\n",
            "Epoch 00300: val_loss did not improve from 0.58209\n",
            "620/620 - 0s - loss: 0.3360 - acc: 0.8484 - val_loss: 0.9803 - val_acc: 0.5600\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "# Evaluate on test data\n",
            "40/40 [==============================] - 0s 2ms/sample - loss: 0.6749 - acc: 0.6750\n",
            "test loss, test acc: [0.6749049691716209, 0.675]\n",
            "1\n",
            "EEG_Deep/Data2A/parsed_P01T.mat\n",
            "(0, 12, 4096)\n",
            "(80, 12, 4096)\n",
            "(80, 1)\n",
            "2\n",
            "EEG_Deep/Data2A/parsed_P02T.mat\n",
            "(80, 12, 4096)\n",
            "(160, 12, 4096)\n",
            "(160, 1)\n",
            "3\n",
            "EEG_Deep/Data2A/parsed_P03T.mat\n",
            "(160, 12, 4096)\n",
            "(240, 12, 4096)\n",
            "(240, 1)\n",
            "5\n",
            "EEG_Deep/Data2A/parsed_P05T.mat\n",
            "(240, 12, 4096)\n",
            "(320, 12, 4096)\n",
            "(320, 1)\n",
            "6\n",
            "EEG_Deep/Data2A/parsed_P06T.mat\n",
            "(320, 12, 4096)\n",
            "(400, 12, 4096)\n",
            "(400, 1)\n",
            "7\n",
            "EEG_Deep/Data2A/parsed_P07T.mat\n",
            "(400, 12, 4096)\n",
            "(480, 12, 4096)\n",
            "(480, 1)\n",
            "8\n",
            "EEG_Deep/Data2A/parsed_P08T.mat\n",
            "(480, 12, 4096)\n",
            "(560, 12, 4096)\n",
            "(560, 1)\n",
            "9\n",
            "EEG_Deep/Data2A/parsed_P09T.mat\n",
            "(560, 12, 4096)\n",
            "(640, 12, 4096)\n",
            "(640, 1)\n",
            "10\n",
            "EEG_Deep/Data2A/parsed_P010T.mat\n",
            "(640, 12, 4096)\n",
            "(720, 12, 4096)\n",
            "(720, 1)\n",
            "4\n",
            "EEG_Deep/Data2A/parsed_P04E.mat\n",
            "(0, 12, 4096)\n",
            "(40, 12, 4096)\n",
            "(40, 1)\n",
            "Filtering of training data in progress\n",
            "(720, 12, 4096)\n",
            "Filtering of test data in progress\n",
            "(40, 12, 4096)\n",
            "[1. 2. 2. 1. 2. 2. 2. 2. 1. 2. 1. 2. 2. 2. 2. 2. 2. 2. 1. 2. 1. 1. 1. 2.\n",
            " 2. 2. 1. 1. 1. 1. 1. 1. 1. 2. 1. 2. 1. 1. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 2. 1. 2. 1. 2. 1. 1. 2. 1. 1. 1. 2. 1. 2. 2. 2. 1. 2. 1. 1. 1. 2. 2. 2.\n",
            " 1. 1. 2. 1. 2. 1. 2. 1. 1. 2. 2. 2. 2. 1. 1. 2. 2. 1. 2. 2. 1. 1. 2. 2.\n",
            " 1. 2. 2. 2.]\n",
            "(620, 12, 1536)\n",
            "(620,)\n",
            "(100, 12, 1536)\n",
            "(100,)\n",
            "[[1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]]\n",
            "X_train shape: (620, 1, 12, 1536)\n",
            "620 train samples\n",
            "100 val samples\n",
            "(40, 12, 1536)\n",
            "(40, 1)\n",
            "X_train shape: (40, 1, 12, 1536)\n",
            "40 train samples\n",
            "Train on 620 samples, validate on 100 samples\n",
            "Epoch 1/300\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 1.23599, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.9531 - acc: 0.5145 - val_loss: 1.2360 - val_acc: 0.5100\n",
            "Epoch 2/300\n",
            "\n",
            "Epoch 00002: val_loss improved from 1.23599 to 1.09393, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.7619 - acc: 0.6065 - val_loss: 1.0939 - val_acc: 0.6000\n",
            "Epoch 3/300\n",
            "\n",
            "Epoch 00003: val_loss improved from 1.09393 to 0.94578, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.7021 - acc: 0.6565 - val_loss: 0.9458 - val_acc: 0.5900\n",
            "Epoch 4/300\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.94578 to 0.83979, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.6377 - acc: 0.6968 - val_loss: 0.8398 - val_acc: 0.6100\n",
            "Epoch 5/300\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.83979 to 0.76649, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.6086 - acc: 0.7339 - val_loss: 0.7665 - val_acc: 0.6000\n",
            "Epoch 6/300\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.76649 to 0.70001, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.6115 - acc: 0.7016 - val_loss: 0.7000 - val_acc: 0.6500\n",
            "Epoch 7/300\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.70001 to 0.67022, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.5615 - acc: 0.7468 - val_loss: 0.6702 - val_acc: 0.6600\n",
            "Epoch 8/300\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.67022 to 0.63679, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.5843 - acc: 0.7097 - val_loss: 0.6368 - val_acc: 0.6100\n",
            "Epoch 9/300\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.63679 to 0.63191, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.5867 - acc: 0.7113 - val_loss: 0.6319 - val_acc: 0.6000\n",
            "Epoch 10/300\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.63191 to 0.62073, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.5590 - acc: 0.7242 - val_loss: 0.6207 - val_acc: 0.7100\n",
            "Epoch 11/300\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.62073\n",
            "620/620 - 0s - loss: 0.5547 - acc: 0.7258 - val_loss: 0.7562 - val_acc: 0.5900\n",
            "Epoch 12/300\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.62073\n",
            "620/620 - 0s - loss: 0.5546 - acc: 0.7161 - val_loss: 0.6581 - val_acc: 0.6400\n",
            "Epoch 13/300\n",
            "\n",
            "Epoch 00013: val_loss improved from 0.62073 to 0.60685, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.5225 - acc: 0.7565 - val_loss: 0.6068 - val_acc: 0.6900\n",
            "Epoch 14/300\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.60685\n",
            "620/620 - 0s - loss: 0.5331 - acc: 0.7435 - val_loss: 0.6232 - val_acc: 0.5800\n",
            "Epoch 15/300\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.60685\n",
            "620/620 - 0s - loss: 0.5482 - acc: 0.7371 - val_loss: 0.6513 - val_acc: 0.6500\n",
            "Epoch 16/300\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.60685\n",
            "620/620 - 0s - loss: 0.5331 - acc: 0.7371 - val_loss: 0.7545 - val_acc: 0.6100\n",
            "Epoch 17/300\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.60685\n",
            "620/620 - 0s - loss: 0.5175 - acc: 0.7435 - val_loss: 0.7975 - val_acc: 0.5800\n",
            "Epoch 18/300\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.60685\n",
            "620/620 - 0s - loss: 0.5368 - acc: 0.7339 - val_loss: 0.6460 - val_acc: 0.6700\n",
            "Epoch 19/300\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.60685\n",
            "620/620 - 0s - loss: 0.5210 - acc: 0.7484 - val_loss: 0.6192 - val_acc: 0.6400\n",
            "Epoch 20/300\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.60685\n",
            "620/620 - 0s - loss: 0.5278 - acc: 0.7339 - val_loss: 0.6359 - val_acc: 0.6300\n",
            "Epoch 21/300\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.60685\n",
            "620/620 - 0s - loss: 0.5356 - acc: 0.7419 - val_loss: 0.6607 - val_acc: 0.6400\n",
            "Epoch 22/300\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.60685\n",
            "620/620 - 0s - loss: 0.5345 - acc: 0.7210 - val_loss: 0.7403 - val_acc: 0.6200\n",
            "Epoch 23/300\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.60685\n",
            "620/620 - 0s - loss: 0.5171 - acc: 0.7419 - val_loss: 0.6920 - val_acc: 0.6300\n",
            "Epoch 24/300\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.60685\n",
            "620/620 - 0s - loss: 0.4995 - acc: 0.7726 - val_loss: 0.6111 - val_acc: 0.6400\n",
            "Epoch 25/300\n",
            "\n",
            "Epoch 00025: val_loss improved from 0.60685 to 0.58961, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.5172 - acc: 0.7645 - val_loss: 0.5896 - val_acc: 0.6100\n",
            "Epoch 26/300\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.58961\n",
            "620/620 - 0s - loss: 0.5278 - acc: 0.7306 - val_loss: 0.5989 - val_acc: 0.6100\n",
            "Epoch 27/300\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.58961\n",
            "620/620 - 0s - loss: 0.5105 - acc: 0.7435 - val_loss: 0.7242 - val_acc: 0.6300\n",
            "Epoch 28/300\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.58961\n",
            "620/620 - 0s - loss: 0.5317 - acc: 0.7387 - val_loss: 0.7070 - val_acc: 0.6200\n",
            "Epoch 29/300\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.58961\n",
            "620/620 - 0s - loss: 0.5064 - acc: 0.7548 - val_loss: 0.6515 - val_acc: 0.6100\n",
            "Epoch 30/300\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.58961\n",
            "620/620 - 0s - loss: 0.5034 - acc: 0.7613 - val_loss: 0.6081 - val_acc: 0.6400\n",
            "Epoch 31/300\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.58961\n",
            "620/620 - 0s - loss: 0.5187 - acc: 0.7565 - val_loss: 0.6205 - val_acc: 0.6400\n",
            "Epoch 32/300\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.58961\n",
            "620/620 - 0s - loss: 0.4769 - acc: 0.7710 - val_loss: 0.6798 - val_acc: 0.6200\n",
            "Epoch 33/300\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.58961\n",
            "620/620 - 0s - loss: 0.4697 - acc: 0.7806 - val_loss: 0.6634 - val_acc: 0.6300\n",
            "Epoch 34/300\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.58961\n",
            "620/620 - 0s - loss: 0.4783 - acc: 0.7661 - val_loss: 0.6784 - val_acc: 0.6100\n",
            "Epoch 35/300\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.58961\n",
            "620/620 - 0s - loss: 0.4992 - acc: 0.7726 - val_loss: 0.7396 - val_acc: 0.6000\n",
            "Epoch 36/300\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.58961\n",
            "620/620 - 0s - loss: 0.4682 - acc: 0.7919 - val_loss: 0.6596 - val_acc: 0.5900\n",
            "Epoch 37/300\n",
            "\n",
            "Epoch 00037: val_loss improved from 0.58961 to 0.57546, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.4872 - acc: 0.7887 - val_loss: 0.5755 - val_acc: 0.6500\n",
            "Epoch 38/300\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.57546\n",
            "620/620 - 0s - loss: 0.4615 - acc: 0.7806 - val_loss: 0.5898 - val_acc: 0.6400\n",
            "Epoch 39/300\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.57546\n",
            "620/620 - 0s - loss: 0.5112 - acc: 0.7710 - val_loss: 0.6327 - val_acc: 0.6100\n",
            "Epoch 40/300\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.57546\n",
            "620/620 - 0s - loss: 0.4799 - acc: 0.7774 - val_loss: 0.6102 - val_acc: 0.5900\n",
            "Epoch 41/300\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.57546\n",
            "620/620 - 0s - loss: 0.4531 - acc: 0.7919 - val_loss: 0.6197 - val_acc: 0.6200\n",
            "Epoch 42/300\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.57546\n",
            "620/620 - 0s - loss: 0.4916 - acc: 0.7581 - val_loss: 0.6994 - val_acc: 0.6200\n",
            "Epoch 43/300\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.57546\n",
            "620/620 - 0s - loss: 0.4884 - acc: 0.7758 - val_loss: 0.6069 - val_acc: 0.6200\n",
            "Epoch 44/300\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.57546\n",
            "620/620 - 0s - loss: 0.4761 - acc: 0.7677 - val_loss: 0.6885 - val_acc: 0.6200\n",
            "Epoch 45/300\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.57546\n",
            "620/620 - 0s - loss: 0.4694 - acc: 0.7790 - val_loss: 0.5834 - val_acc: 0.6100\n",
            "Epoch 46/300\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.57546\n",
            "620/620 - 0s - loss: 0.4574 - acc: 0.8000 - val_loss: 0.6386 - val_acc: 0.6100\n",
            "Epoch 47/300\n",
            "\n",
            "Epoch 00047: val_loss improved from 0.57546 to 0.57418, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.5156 - acc: 0.7403 - val_loss: 0.5742 - val_acc: 0.6200\n",
            "Epoch 48/300\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.57418\n",
            "620/620 - 0s - loss: 0.4483 - acc: 0.7935 - val_loss: 0.6306 - val_acc: 0.6200\n",
            "Epoch 49/300\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.57418\n",
            "620/620 - 0s - loss: 0.4623 - acc: 0.7919 - val_loss: 0.9204 - val_acc: 0.5700\n",
            "Epoch 50/300\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.57418\n",
            "620/620 - 0s - loss: 0.5007 - acc: 0.7548 - val_loss: 0.7165 - val_acc: 0.6000\n",
            "Epoch 51/300\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.57418\n",
            "620/620 - 0s - loss: 0.4760 - acc: 0.7710 - val_loss: 0.6489 - val_acc: 0.6100\n",
            "Epoch 52/300\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.57418\n",
            "620/620 - 0s - loss: 0.4549 - acc: 0.7871 - val_loss: 0.8509 - val_acc: 0.5700\n",
            "Epoch 53/300\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.57418\n",
            "620/620 - 0s - loss: 0.4652 - acc: 0.7919 - val_loss: 0.6483 - val_acc: 0.6000\n",
            "Epoch 54/300\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.57418\n",
            "620/620 - 0s - loss: 0.4679 - acc: 0.7710 - val_loss: 0.6395 - val_acc: 0.6000\n",
            "Epoch 55/300\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.57418\n",
            "620/620 - 0s - loss: 0.4426 - acc: 0.8000 - val_loss: 0.6873 - val_acc: 0.6100\n",
            "Epoch 56/300\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.57418\n",
            "620/620 - 0s - loss: 0.4783 - acc: 0.7839 - val_loss: 0.7279 - val_acc: 0.5800\n",
            "Epoch 57/300\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.57418\n",
            "620/620 - 0s - loss: 0.4729 - acc: 0.7935 - val_loss: 0.6370 - val_acc: 0.6300\n",
            "Epoch 58/300\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.57418\n",
            "620/620 - 0s - loss: 0.4708 - acc: 0.7726 - val_loss: 0.6122 - val_acc: 0.6700\n",
            "Epoch 59/300\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.57418\n",
            "620/620 - 0s - loss: 0.4645 - acc: 0.7726 - val_loss: 0.6723 - val_acc: 0.5800\n",
            "Epoch 60/300\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.57418\n",
            "620/620 - 0s - loss: 0.4544 - acc: 0.7855 - val_loss: 0.6096 - val_acc: 0.6300\n",
            "Epoch 61/300\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.57418\n",
            "620/620 - 0s - loss: 0.4673 - acc: 0.7839 - val_loss: 0.6117 - val_acc: 0.6400\n",
            "Epoch 62/300\n",
            "\n",
            "Epoch 00062: val_loss improved from 0.57418 to 0.56570, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.4787 - acc: 0.7710 - val_loss: 0.5657 - val_acc: 0.6800\n",
            "Epoch 63/300\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.56570\n",
            "620/620 - 0s - loss: 0.4539 - acc: 0.7919 - val_loss: 0.5777 - val_acc: 0.6400\n",
            "Epoch 64/300\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.56570\n",
            "620/620 - 0s - loss: 0.4346 - acc: 0.8065 - val_loss: 0.6010 - val_acc: 0.6400\n",
            "Epoch 65/300\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.56570\n",
            "620/620 - 0s - loss: 0.4586 - acc: 0.7774 - val_loss: 0.6411 - val_acc: 0.5900\n",
            "Epoch 66/300\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.56570\n",
            "620/620 - 0s - loss: 0.4621 - acc: 0.7806 - val_loss: 0.7917 - val_acc: 0.5900\n",
            "Epoch 67/300\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.56570\n",
            "620/620 - 0s - loss: 0.4623 - acc: 0.7871 - val_loss: 0.6808 - val_acc: 0.5800\n",
            "Epoch 68/300\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.56570\n",
            "620/620 - 0s - loss: 0.4492 - acc: 0.8000 - val_loss: 0.8678 - val_acc: 0.5700\n",
            "Epoch 69/300\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.56570\n",
            "620/620 - 0s - loss: 0.4936 - acc: 0.7710 - val_loss: 0.6474 - val_acc: 0.6000\n",
            "Epoch 70/300\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.56570\n",
            "620/620 - 0s - loss: 0.4961 - acc: 0.7613 - val_loss: 0.5816 - val_acc: 0.7000\n",
            "Epoch 71/300\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.56570\n",
            "620/620 - 0s - loss: 0.4551 - acc: 0.8048 - val_loss: 0.8519 - val_acc: 0.5700\n",
            "Epoch 72/300\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.56570\n",
            "620/620 - 0s - loss: 0.4430 - acc: 0.7855 - val_loss: 0.6974 - val_acc: 0.5900\n",
            "Epoch 73/300\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.56570\n",
            "620/620 - 0s - loss: 0.4697 - acc: 0.7839 - val_loss: 0.6112 - val_acc: 0.6300\n",
            "Epoch 74/300\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.56570\n",
            "620/620 - 0s - loss: 0.4593 - acc: 0.7758 - val_loss: 0.6305 - val_acc: 0.6400\n",
            "Epoch 75/300\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.56570\n",
            "620/620 - 0s - loss: 0.4376 - acc: 0.7855 - val_loss: 0.6391 - val_acc: 0.6400\n",
            "Epoch 76/300\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.56570\n",
            "620/620 - 0s - loss: 0.4464 - acc: 0.7806 - val_loss: 0.7388 - val_acc: 0.6200\n",
            "Epoch 77/300\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.56570\n",
            "620/620 - 0s - loss: 0.4345 - acc: 0.7855 - val_loss: 0.8526 - val_acc: 0.5900\n",
            "Epoch 78/300\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.56570\n",
            "620/620 - 0s - loss: 0.4465 - acc: 0.7935 - val_loss: 0.6969 - val_acc: 0.6400\n",
            "Epoch 79/300\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.56570\n",
            "620/620 - 0s - loss: 0.4773 - acc: 0.7839 - val_loss: 0.6357 - val_acc: 0.6100\n",
            "Epoch 80/300\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.56570\n",
            "620/620 - 0s - loss: 0.4569 - acc: 0.7758 - val_loss: 0.6395 - val_acc: 0.6400\n",
            "Epoch 81/300\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.56570\n",
            "620/620 - 0s - loss: 0.4419 - acc: 0.8016 - val_loss: 0.5714 - val_acc: 0.6700\n",
            "Epoch 82/300\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.56570\n",
            "620/620 - 0s - loss: 0.4360 - acc: 0.7935 - val_loss: 0.7723 - val_acc: 0.6000\n",
            "Epoch 83/300\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.56570\n",
            "620/620 - 1s - loss: 0.4642 - acc: 0.7903 - val_loss: 0.7331 - val_acc: 0.6100\n",
            "Epoch 84/300\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.56570\n",
            "620/620 - 0s - loss: 0.4731 - acc: 0.7645 - val_loss: 0.6874 - val_acc: 0.5900\n",
            "Epoch 85/300\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.56570\n",
            "620/620 - 0s - loss: 0.4364 - acc: 0.7855 - val_loss: 0.7385 - val_acc: 0.6100\n",
            "Epoch 86/300\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.56570\n",
            "620/620 - 0s - loss: 0.4470 - acc: 0.7968 - val_loss: 0.6333 - val_acc: 0.6600\n",
            "Epoch 87/300\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.56570\n",
            "620/620 - 1s - loss: 0.4514 - acc: 0.7726 - val_loss: 0.6458 - val_acc: 0.6300\n",
            "Epoch 88/300\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.56570\n",
            "620/620 - 0s - loss: 0.4492 - acc: 0.7823 - val_loss: 0.6303 - val_acc: 0.6500\n",
            "Epoch 89/300\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.56570\n",
            "620/620 - 0s - loss: 0.4320 - acc: 0.8145 - val_loss: 0.6614 - val_acc: 0.6500\n",
            "Epoch 90/300\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.56570\n",
            "620/620 - 0s - loss: 0.4478 - acc: 0.7919 - val_loss: 0.6948 - val_acc: 0.6200\n",
            "Epoch 91/300\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.56570\n",
            "620/620 - 0s - loss: 0.4468 - acc: 0.7887 - val_loss: 0.6228 - val_acc: 0.6400\n",
            "Epoch 92/300\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.56570\n",
            "620/620 - 0s - loss: 0.4224 - acc: 0.7935 - val_loss: 0.8232 - val_acc: 0.5700\n",
            "Epoch 93/300\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.56570\n",
            "620/620 - 0s - loss: 0.4350 - acc: 0.7952 - val_loss: 0.5871 - val_acc: 0.6400\n",
            "Epoch 94/300\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.56570\n",
            "620/620 - 0s - loss: 0.4414 - acc: 0.7984 - val_loss: 0.6611 - val_acc: 0.6200\n",
            "Epoch 95/300\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.56570\n",
            "620/620 - 0s - loss: 0.4365 - acc: 0.7984 - val_loss: 0.6622 - val_acc: 0.6400\n",
            "Epoch 96/300\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.56570\n",
            "620/620 - 0s - loss: 0.4168 - acc: 0.8048 - val_loss: 0.6331 - val_acc: 0.6500\n",
            "Epoch 97/300\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.56570\n",
            "620/620 - 0s - loss: 0.4330 - acc: 0.8016 - val_loss: 0.6694 - val_acc: 0.6200\n",
            "Epoch 98/300\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.56570\n",
            "620/620 - 0s - loss: 0.4452 - acc: 0.7871 - val_loss: 0.6437 - val_acc: 0.6300\n",
            "Epoch 99/300\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.56570\n",
            "620/620 - 0s - loss: 0.4629 - acc: 0.7726 - val_loss: 0.7893 - val_acc: 0.5700\n",
            "Epoch 100/300\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.56570\n",
            "620/620 - 0s - loss: 0.4306 - acc: 0.7968 - val_loss: 0.6583 - val_acc: 0.6400\n",
            "Epoch 101/300\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.56570\n",
            "620/620 - 0s - loss: 0.4390 - acc: 0.7935 - val_loss: 0.5753 - val_acc: 0.6700\n",
            "Epoch 102/300\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.56570\n",
            "620/620 - 0s - loss: 0.4379 - acc: 0.7726 - val_loss: 0.7899 - val_acc: 0.6000\n",
            "Epoch 103/300\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.56570\n",
            "620/620 - 0s - loss: 0.4321 - acc: 0.7871 - val_loss: 0.6021 - val_acc: 0.6700\n",
            "Epoch 104/300\n",
            "\n",
            "Epoch 00104: val_loss improved from 0.56570 to 0.56235, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.4279 - acc: 0.7903 - val_loss: 0.5624 - val_acc: 0.6800\n",
            "Epoch 105/300\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.4311 - acc: 0.7871 - val_loss: 0.5779 - val_acc: 0.6400\n",
            "Epoch 106/300\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.4239 - acc: 0.8016 - val_loss: 0.9577 - val_acc: 0.5600\n",
            "Epoch 107/300\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.4522 - acc: 0.7774 - val_loss: 0.6464 - val_acc: 0.6600\n",
            "Epoch 108/300\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.4318 - acc: 0.7984 - val_loss: 0.6489 - val_acc: 0.6700\n",
            "Epoch 109/300\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.4434 - acc: 0.8016 - val_loss: 0.8891 - val_acc: 0.5700\n",
            "Epoch 110/300\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.4528 - acc: 0.7581 - val_loss: 0.6852 - val_acc: 0.6300\n",
            "Epoch 111/300\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.4204 - acc: 0.8129 - val_loss: 0.6160 - val_acc: 0.6600\n",
            "Epoch 112/300\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.4141 - acc: 0.7887 - val_loss: 0.6407 - val_acc: 0.6700\n",
            "Epoch 113/300\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.4224 - acc: 0.8194 - val_loss: 0.7031 - val_acc: 0.6300\n",
            "Epoch 114/300\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.3972 - acc: 0.8226 - val_loss: 0.7849 - val_acc: 0.6000\n",
            "Epoch 115/300\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.4360 - acc: 0.7871 - val_loss: 0.6981 - val_acc: 0.6400\n",
            "Epoch 116/300\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.4215 - acc: 0.8065 - val_loss: 0.6390 - val_acc: 0.6700\n",
            "Epoch 117/300\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.4246 - acc: 0.7903 - val_loss: 0.6291 - val_acc: 0.6700\n",
            "Epoch 118/300\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.4137 - acc: 0.8177 - val_loss: 0.6819 - val_acc: 0.6500\n",
            "Epoch 119/300\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.4179 - acc: 0.8145 - val_loss: 0.6902 - val_acc: 0.6400\n",
            "Epoch 120/300\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.4068 - acc: 0.8129 - val_loss: 0.6165 - val_acc: 0.6900\n",
            "Epoch 121/300\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.4211 - acc: 0.8048 - val_loss: 0.6992 - val_acc: 0.6300\n",
            "Epoch 122/300\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.4026 - acc: 0.8161 - val_loss: 0.6278 - val_acc: 0.6800\n",
            "Epoch 123/300\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.4251 - acc: 0.7919 - val_loss: 0.7934 - val_acc: 0.5900\n",
            "Epoch 124/300\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.4306 - acc: 0.7871 - val_loss: 0.5930 - val_acc: 0.7000\n",
            "Epoch 125/300\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.4023 - acc: 0.8145 - val_loss: 0.6803 - val_acc: 0.6600\n",
            "Epoch 126/300\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.4058 - acc: 0.8145 - val_loss: 0.6002 - val_acc: 0.6500\n",
            "Epoch 127/300\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.4350 - acc: 0.7984 - val_loss: 0.6489 - val_acc: 0.6700\n",
            "Epoch 128/300\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.4306 - acc: 0.7984 - val_loss: 0.6206 - val_acc: 0.6600\n",
            "Epoch 129/300\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.4078 - acc: 0.8258 - val_loss: 0.6050 - val_acc: 0.7000\n",
            "Epoch 130/300\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.4144 - acc: 0.7984 - val_loss: 0.6169 - val_acc: 0.6900\n",
            "Epoch 131/300\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.4024 - acc: 0.7984 - val_loss: 0.6644 - val_acc: 0.6600\n",
            "Epoch 132/300\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.4106 - acc: 0.8016 - val_loss: 0.6773 - val_acc: 0.6400\n",
            "Epoch 133/300\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.4209 - acc: 0.8081 - val_loss: 0.8088 - val_acc: 0.6100\n",
            "Epoch 134/300\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.4043 - acc: 0.8016 - val_loss: 0.6647 - val_acc: 0.6400\n",
            "Epoch 135/300\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.4218 - acc: 0.8016 - val_loss: 0.6265 - val_acc: 0.6700\n",
            "Epoch 136/300\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.4077 - acc: 0.8161 - val_loss: 0.6979 - val_acc: 0.6400\n",
            "Epoch 137/300\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.4147 - acc: 0.8145 - val_loss: 0.6996 - val_acc: 0.6400\n",
            "Epoch 138/300\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.4032 - acc: 0.8145 - val_loss: 0.6419 - val_acc: 0.6500\n",
            "Epoch 139/300\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.3812 - acc: 0.8355 - val_loss: 0.5790 - val_acc: 0.6500\n",
            "Epoch 140/300\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.4280 - acc: 0.7919 - val_loss: 0.6620 - val_acc: 0.6400\n",
            "Epoch 141/300\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.4054 - acc: 0.8194 - val_loss: 0.7003 - val_acc: 0.6100\n",
            "Epoch 142/300\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.4457 - acc: 0.7952 - val_loss: 0.5836 - val_acc: 0.7000\n",
            "Epoch 143/300\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.3820 - acc: 0.8419 - val_loss: 0.6836 - val_acc: 0.6300\n",
            "Epoch 144/300\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.4281 - acc: 0.8081 - val_loss: 0.6543 - val_acc: 0.6600\n",
            "Epoch 145/300\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.4124 - acc: 0.8113 - val_loss: 0.6926 - val_acc: 0.6200\n",
            "Epoch 146/300\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.4030 - acc: 0.8032 - val_loss: 0.8549 - val_acc: 0.6000\n",
            "Epoch 147/300\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.4076 - acc: 0.8226 - val_loss: 0.6139 - val_acc: 0.6300\n",
            "Epoch 148/300\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.4179 - acc: 0.8129 - val_loss: 0.6882 - val_acc: 0.6400\n",
            "Epoch 149/300\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.4091 - acc: 0.8210 - val_loss: 0.6527 - val_acc: 0.6400\n",
            "Epoch 150/300\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.4242 - acc: 0.8097 - val_loss: 0.6263 - val_acc: 0.6300\n",
            "Epoch 151/300\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.3934 - acc: 0.8290 - val_loss: 0.6412 - val_acc: 0.6600\n",
            "Epoch 152/300\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.4005 - acc: 0.8097 - val_loss: 0.7399 - val_acc: 0.6300\n",
            "Epoch 153/300\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.4128 - acc: 0.8210 - val_loss: 0.6677 - val_acc: 0.6400\n",
            "Epoch 154/300\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.4124 - acc: 0.8032 - val_loss: 0.6741 - val_acc: 0.6600\n",
            "Epoch 155/300\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.4323 - acc: 0.8016 - val_loss: 0.7208 - val_acc: 0.6100\n",
            "Epoch 156/300\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.3932 - acc: 0.8226 - val_loss: 0.6927 - val_acc: 0.6200\n",
            "Epoch 157/300\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.3990 - acc: 0.8081 - val_loss: 0.6900 - val_acc: 0.6300\n",
            "Epoch 158/300\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.3919 - acc: 0.8274 - val_loss: 0.6932 - val_acc: 0.6500\n",
            "Epoch 159/300\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.4079 - acc: 0.8081 - val_loss: 0.7470 - val_acc: 0.6100\n",
            "Epoch 160/300\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.4082 - acc: 0.7952 - val_loss: 0.7019 - val_acc: 0.6300\n",
            "Epoch 161/300\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.4191 - acc: 0.8065 - val_loss: 0.7648 - val_acc: 0.6100\n",
            "Epoch 162/300\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.4134 - acc: 0.8177 - val_loss: 0.7546 - val_acc: 0.6300\n",
            "Epoch 163/300\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.3844 - acc: 0.8274 - val_loss: 0.6039 - val_acc: 0.6700\n",
            "Epoch 164/300\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.4108 - acc: 0.8145 - val_loss: 0.6174 - val_acc: 0.6500\n",
            "Epoch 165/300\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.3952 - acc: 0.8242 - val_loss: 0.6639 - val_acc: 0.6600\n",
            "Epoch 166/300\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.4091 - acc: 0.8210 - val_loss: 0.5696 - val_acc: 0.7000\n",
            "Epoch 167/300\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.4368 - acc: 0.7887 - val_loss: 0.7179 - val_acc: 0.5800\n",
            "Epoch 168/300\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.4071 - acc: 0.8097 - val_loss: 0.6989 - val_acc: 0.6300\n",
            "Epoch 169/300\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.4049 - acc: 0.8177 - val_loss: 0.6029 - val_acc: 0.6800\n",
            "Epoch 170/300\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.4242 - acc: 0.7952 - val_loss: 0.6544 - val_acc: 0.6400\n",
            "Epoch 171/300\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.3734 - acc: 0.8274 - val_loss: 0.7252 - val_acc: 0.6100\n",
            "Epoch 172/300\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.4049 - acc: 0.8097 - val_loss: 0.6330 - val_acc: 0.6500\n",
            "Epoch 173/300\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.3975 - acc: 0.8016 - val_loss: 0.6561 - val_acc: 0.6400\n",
            "Epoch 174/300\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.4275 - acc: 0.8016 - val_loss: 0.6322 - val_acc: 0.6600\n",
            "Epoch 175/300\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.4079 - acc: 0.8177 - val_loss: 0.5844 - val_acc: 0.6800\n",
            "Epoch 176/300\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.3843 - acc: 0.8097 - val_loss: 0.6269 - val_acc: 0.6700\n",
            "Epoch 177/300\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.3727 - acc: 0.8484 - val_loss: 0.8125 - val_acc: 0.6000\n",
            "Epoch 178/300\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.4020 - acc: 0.8048 - val_loss: 0.6347 - val_acc: 0.6600\n",
            "Epoch 179/300\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.56235\n",
            "620/620 - 1s - loss: 0.3845 - acc: 0.8177 - val_loss: 0.6460 - val_acc: 0.6500\n",
            "Epoch 180/300\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.4030 - acc: 0.8065 - val_loss: 0.6670 - val_acc: 0.6600\n",
            "Epoch 181/300\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.3955 - acc: 0.8371 - val_loss: 0.6278 - val_acc: 0.6800\n",
            "Epoch 182/300\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.3977 - acc: 0.8274 - val_loss: 0.6625 - val_acc: 0.6400\n",
            "Epoch 183/300\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.3995 - acc: 0.8113 - val_loss: 0.7154 - val_acc: 0.6400\n",
            "Epoch 184/300\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.3838 - acc: 0.8290 - val_loss: 0.6824 - val_acc: 0.6500\n",
            "Epoch 185/300\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.3691 - acc: 0.8419 - val_loss: 0.7914 - val_acc: 0.5800\n",
            "Epoch 186/300\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.4063 - acc: 0.8065 - val_loss: 0.9083 - val_acc: 0.5600\n",
            "Epoch 187/300\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.4069 - acc: 0.8161 - val_loss: 0.7378 - val_acc: 0.6000\n",
            "Epoch 188/300\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.3724 - acc: 0.8355 - val_loss: 0.6801 - val_acc: 0.6600\n",
            "Epoch 189/300\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.3942 - acc: 0.8177 - val_loss: 0.6117 - val_acc: 0.6500\n",
            "Epoch 190/300\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.3939 - acc: 0.8242 - val_loss: 0.6406 - val_acc: 0.6600\n",
            "Epoch 191/300\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.3972 - acc: 0.8194 - val_loss: 0.6222 - val_acc: 0.6500\n",
            "Epoch 192/300\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.3970 - acc: 0.8355 - val_loss: 0.6410 - val_acc: 0.6800\n",
            "Epoch 193/300\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.3849 - acc: 0.8258 - val_loss: 0.6992 - val_acc: 0.6400\n",
            "Epoch 194/300\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.3667 - acc: 0.8435 - val_loss: 0.6628 - val_acc: 0.6700\n",
            "Epoch 195/300\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.3719 - acc: 0.8145 - val_loss: 0.6650 - val_acc: 0.6400\n",
            "Epoch 196/300\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.4029 - acc: 0.8048 - val_loss: 0.7603 - val_acc: 0.6000\n",
            "Epoch 197/300\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.3782 - acc: 0.8210 - val_loss: 0.7034 - val_acc: 0.6300\n",
            "Epoch 198/300\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.3902 - acc: 0.8177 - val_loss: 0.6963 - val_acc: 0.6300\n",
            "Epoch 199/300\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.3567 - acc: 0.8371 - val_loss: 0.6986 - val_acc: 0.6400\n",
            "Epoch 200/300\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.3560 - acc: 0.8452 - val_loss: 0.9060 - val_acc: 0.5700\n",
            "Epoch 201/300\n",
            "\n",
            "Epoch 00201: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.3542 - acc: 0.8435 - val_loss: 0.6709 - val_acc: 0.6400\n",
            "Epoch 202/300\n",
            "\n",
            "Epoch 00202: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.3887 - acc: 0.8177 - val_loss: 0.6901 - val_acc: 0.6500\n",
            "Epoch 203/300\n",
            "\n",
            "Epoch 00203: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.3543 - acc: 0.8516 - val_loss: 0.8193 - val_acc: 0.5900\n",
            "Epoch 204/300\n",
            "\n",
            "Epoch 00204: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.3877 - acc: 0.8161 - val_loss: 0.7520 - val_acc: 0.6100\n",
            "Epoch 205/300\n",
            "\n",
            "Epoch 00205: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.3815 - acc: 0.8242 - val_loss: 0.7054 - val_acc: 0.6200\n",
            "Epoch 206/300\n",
            "\n",
            "Epoch 00206: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.4071 - acc: 0.7903 - val_loss: 0.7311 - val_acc: 0.6000\n",
            "Epoch 207/300\n",
            "\n",
            "Epoch 00207: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.3917 - acc: 0.8403 - val_loss: 0.7542 - val_acc: 0.6200\n",
            "Epoch 208/300\n",
            "\n",
            "Epoch 00208: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.3908 - acc: 0.8194 - val_loss: 0.7622 - val_acc: 0.6000\n",
            "Epoch 209/300\n",
            "\n",
            "Epoch 00209: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.3888 - acc: 0.8177 - val_loss: 0.7746 - val_acc: 0.6100\n",
            "Epoch 210/300\n",
            "\n",
            "Epoch 00210: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.3540 - acc: 0.8516 - val_loss: 0.6381 - val_acc: 0.6400\n",
            "Epoch 211/300\n",
            "\n",
            "Epoch 00211: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.3882 - acc: 0.8161 - val_loss: 0.8375 - val_acc: 0.5700\n",
            "Epoch 212/300\n",
            "\n",
            "Epoch 00212: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.3732 - acc: 0.8290 - val_loss: 0.6912 - val_acc: 0.6500\n",
            "Epoch 213/300\n",
            "\n",
            "Epoch 00213: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.3780 - acc: 0.8371 - val_loss: 0.6706 - val_acc: 0.6800\n",
            "Epoch 214/300\n",
            "\n",
            "Epoch 00214: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.3539 - acc: 0.8419 - val_loss: 0.6487 - val_acc: 0.6500\n",
            "Epoch 215/300\n",
            "\n",
            "Epoch 00215: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.3945 - acc: 0.8048 - val_loss: 0.6731 - val_acc: 0.6200\n",
            "Epoch 216/300\n",
            "\n",
            "Epoch 00216: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.4148 - acc: 0.8258 - val_loss: 0.6275 - val_acc: 0.6700\n",
            "Epoch 217/300\n",
            "\n",
            "Epoch 00217: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.4196 - acc: 0.7935 - val_loss: 0.6034 - val_acc: 0.7000\n",
            "Epoch 218/300\n",
            "\n",
            "Epoch 00218: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.3823 - acc: 0.8226 - val_loss: 0.7074 - val_acc: 0.6400\n",
            "Epoch 219/300\n",
            "\n",
            "Epoch 00219: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.4119 - acc: 0.8097 - val_loss: 0.7100 - val_acc: 0.6400\n",
            "Epoch 220/300\n",
            "\n",
            "Epoch 00220: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.3720 - acc: 0.8548 - val_loss: 0.5956 - val_acc: 0.6800\n",
            "Epoch 221/300\n",
            "\n",
            "Epoch 00221: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.3664 - acc: 0.8468 - val_loss: 0.6353 - val_acc: 0.6700\n",
            "Epoch 222/300\n",
            "\n",
            "Epoch 00222: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.3951 - acc: 0.8290 - val_loss: 0.7412 - val_acc: 0.6400\n",
            "Epoch 223/300\n",
            "\n",
            "Epoch 00223: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.3600 - acc: 0.8290 - val_loss: 0.6625 - val_acc: 0.6400\n",
            "Epoch 224/300\n",
            "\n",
            "Epoch 00224: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.3556 - acc: 0.8516 - val_loss: 0.7452 - val_acc: 0.6500\n",
            "Epoch 225/300\n",
            "\n",
            "Epoch 00225: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.3915 - acc: 0.8258 - val_loss: 0.7400 - val_acc: 0.6100\n",
            "Epoch 226/300\n",
            "\n",
            "Epoch 00226: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.3852 - acc: 0.8274 - val_loss: 0.6296 - val_acc: 0.6600\n",
            "Epoch 227/300\n",
            "\n",
            "Epoch 00227: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.3883 - acc: 0.8226 - val_loss: 0.6974 - val_acc: 0.7000\n",
            "Epoch 228/300\n",
            "\n",
            "Epoch 00228: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.3560 - acc: 0.8597 - val_loss: 0.8880 - val_acc: 0.5700\n",
            "Epoch 229/300\n",
            "\n",
            "Epoch 00229: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.3995 - acc: 0.8177 - val_loss: 0.6743 - val_acc: 0.6600\n",
            "Epoch 230/300\n",
            "\n",
            "Epoch 00230: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.3783 - acc: 0.8258 - val_loss: 0.6554 - val_acc: 0.6800\n",
            "Epoch 231/300\n",
            "\n",
            "Epoch 00231: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.3672 - acc: 0.8548 - val_loss: 0.6907 - val_acc: 0.6600\n",
            "Epoch 232/300\n",
            "\n",
            "Epoch 00232: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.3502 - acc: 0.8597 - val_loss: 0.6239 - val_acc: 0.6900\n",
            "Epoch 233/300\n",
            "\n",
            "Epoch 00233: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.3880 - acc: 0.8306 - val_loss: 0.7342 - val_acc: 0.6100\n",
            "Epoch 234/300\n",
            "\n",
            "Epoch 00234: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.3651 - acc: 0.8484 - val_loss: 0.6882 - val_acc: 0.6600\n",
            "Epoch 235/300\n",
            "\n",
            "Epoch 00235: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.3777 - acc: 0.8371 - val_loss: 0.7012 - val_acc: 0.6400\n",
            "Epoch 236/300\n",
            "\n",
            "Epoch 00236: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.3960 - acc: 0.8177 - val_loss: 0.6405 - val_acc: 0.6600\n",
            "Epoch 237/300\n",
            "\n",
            "Epoch 00237: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.3684 - acc: 0.8339 - val_loss: 0.8618 - val_acc: 0.5700\n",
            "Epoch 238/300\n",
            "\n",
            "Epoch 00238: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.3771 - acc: 0.8306 - val_loss: 0.7398 - val_acc: 0.6100\n",
            "Epoch 239/300\n",
            "\n",
            "Epoch 00239: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.3503 - acc: 0.8484 - val_loss: 0.6676 - val_acc: 0.6500\n",
            "Epoch 240/300\n",
            "\n",
            "Epoch 00240: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.3839 - acc: 0.8306 - val_loss: 0.5778 - val_acc: 0.6800\n",
            "Epoch 241/300\n",
            "\n",
            "Epoch 00241: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.3965 - acc: 0.8194 - val_loss: 0.7672 - val_acc: 0.6100\n",
            "Epoch 242/300\n",
            "\n",
            "Epoch 00242: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.3627 - acc: 0.8403 - val_loss: 0.7283 - val_acc: 0.6200\n",
            "Epoch 243/300\n",
            "\n",
            "Epoch 00243: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.4110 - acc: 0.8016 - val_loss: 0.8549 - val_acc: 0.5700\n",
            "Epoch 244/300\n",
            "\n",
            "Epoch 00244: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.3776 - acc: 0.8323 - val_loss: 0.8899 - val_acc: 0.5600\n",
            "Epoch 245/300\n",
            "\n",
            "Epoch 00245: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.3557 - acc: 0.8468 - val_loss: 0.6835 - val_acc: 0.6700\n",
            "Epoch 246/300\n",
            "\n",
            "Epoch 00246: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.3734 - acc: 0.8194 - val_loss: 0.6797 - val_acc: 0.6900\n",
            "Epoch 247/300\n",
            "\n",
            "Epoch 00247: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.3765 - acc: 0.8290 - val_loss: 0.7982 - val_acc: 0.6000\n",
            "Epoch 248/300\n",
            "\n",
            "Epoch 00248: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.3682 - acc: 0.8339 - val_loss: 0.6835 - val_acc: 0.6700\n",
            "Epoch 249/300\n",
            "\n",
            "Epoch 00249: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.3938 - acc: 0.8145 - val_loss: 0.7854 - val_acc: 0.5800\n",
            "Epoch 250/300\n",
            "\n",
            "Epoch 00250: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.3790 - acc: 0.8113 - val_loss: 0.6070 - val_acc: 0.7000\n",
            "Epoch 251/300\n",
            "\n",
            "Epoch 00251: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.3907 - acc: 0.8177 - val_loss: 0.6848 - val_acc: 0.6700\n",
            "Epoch 252/300\n",
            "\n",
            "Epoch 00252: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.3964 - acc: 0.8161 - val_loss: 0.5831 - val_acc: 0.6900\n",
            "Epoch 253/300\n",
            "\n",
            "Epoch 00253: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.3848 - acc: 0.8355 - val_loss: 0.6396 - val_acc: 0.6700\n",
            "Epoch 254/300\n",
            "\n",
            "Epoch 00254: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.3594 - acc: 0.8484 - val_loss: 0.7543 - val_acc: 0.6100\n",
            "Epoch 255/300\n",
            "\n",
            "Epoch 00255: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.3738 - acc: 0.8242 - val_loss: 0.7090 - val_acc: 0.6500\n",
            "Epoch 256/300\n",
            "\n",
            "Epoch 00256: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.3993 - acc: 0.8081 - val_loss: 0.7183 - val_acc: 0.6400\n",
            "Epoch 257/300\n",
            "\n",
            "Epoch 00257: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.3373 - acc: 0.8581 - val_loss: 0.7990 - val_acc: 0.6000\n",
            "Epoch 258/300\n",
            "\n",
            "Epoch 00258: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.3855 - acc: 0.8484 - val_loss: 0.6464 - val_acc: 0.6700\n",
            "Epoch 259/300\n",
            "\n",
            "Epoch 00259: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.3782 - acc: 0.8355 - val_loss: 0.6371 - val_acc: 0.6600\n",
            "Epoch 260/300\n",
            "\n",
            "Epoch 00260: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.3994 - acc: 0.8226 - val_loss: 0.6472 - val_acc: 0.6700\n",
            "Epoch 261/300\n",
            "\n",
            "Epoch 00261: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.3716 - acc: 0.8387 - val_loss: 0.6910 - val_acc: 0.6700\n",
            "Epoch 262/300\n",
            "\n",
            "Epoch 00262: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.3763 - acc: 0.8403 - val_loss: 0.6759 - val_acc: 0.6600\n",
            "Epoch 263/300\n",
            "\n",
            "Epoch 00263: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.3532 - acc: 0.8387 - val_loss: 0.7624 - val_acc: 0.6000\n",
            "Epoch 264/300\n",
            "\n",
            "Epoch 00264: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.3440 - acc: 0.8581 - val_loss: 0.6990 - val_acc: 0.6500\n",
            "Epoch 265/300\n",
            "\n",
            "Epoch 00265: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.3876 - acc: 0.8258 - val_loss: 0.5971 - val_acc: 0.6800\n",
            "Epoch 266/300\n",
            "\n",
            "Epoch 00266: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.3744 - acc: 0.8242 - val_loss: 0.6379 - val_acc: 0.6700\n",
            "Epoch 267/300\n",
            "\n",
            "Epoch 00267: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.3674 - acc: 0.8403 - val_loss: 0.6045 - val_acc: 0.6700\n",
            "Epoch 268/300\n",
            "\n",
            "Epoch 00268: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.3847 - acc: 0.8145 - val_loss: 0.6614 - val_acc: 0.6400\n",
            "Epoch 269/300\n",
            "\n",
            "Epoch 00269: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.4000 - acc: 0.7984 - val_loss: 0.7229 - val_acc: 0.6300\n",
            "Epoch 270/300\n",
            "\n",
            "Epoch 00270: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.3592 - acc: 0.8355 - val_loss: 0.6352 - val_acc: 0.6700\n",
            "Epoch 271/300\n",
            "\n",
            "Epoch 00271: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.3681 - acc: 0.8290 - val_loss: 0.6442 - val_acc: 0.6600\n",
            "Epoch 272/300\n",
            "\n",
            "Epoch 00272: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.3744 - acc: 0.8435 - val_loss: 0.7499 - val_acc: 0.6200\n",
            "Epoch 273/300\n",
            "\n",
            "Epoch 00273: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.3820 - acc: 0.8419 - val_loss: 0.6506 - val_acc: 0.6900\n",
            "Epoch 274/300\n",
            "\n",
            "Epoch 00274: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.3681 - acc: 0.8419 - val_loss: 0.6533 - val_acc: 0.6900\n",
            "Epoch 275/300\n",
            "\n",
            "Epoch 00275: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.3583 - acc: 0.8226 - val_loss: 0.6986 - val_acc: 0.6600\n",
            "Epoch 276/300\n",
            "\n",
            "Epoch 00276: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.3452 - acc: 0.8532 - val_loss: 0.7516 - val_acc: 0.6400\n",
            "Epoch 277/300\n",
            "\n",
            "Epoch 00277: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.3728 - acc: 0.8500 - val_loss: 0.7630 - val_acc: 0.6100\n",
            "Epoch 278/300\n",
            "\n",
            "Epoch 00278: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.3623 - acc: 0.8387 - val_loss: 0.7453 - val_acc: 0.6400\n",
            "Epoch 279/300\n",
            "\n",
            "Epoch 00279: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.3709 - acc: 0.8339 - val_loss: 0.6850 - val_acc: 0.6400\n",
            "Epoch 280/300\n",
            "\n",
            "Epoch 00280: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.4075 - acc: 0.8065 - val_loss: 0.7647 - val_acc: 0.6300\n",
            "Epoch 281/300\n",
            "\n",
            "Epoch 00281: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.3857 - acc: 0.8242 - val_loss: 0.6267 - val_acc: 0.6400\n",
            "Epoch 282/300\n",
            "\n",
            "Epoch 00282: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.3848 - acc: 0.8161 - val_loss: 0.6971 - val_acc: 0.6700\n",
            "Epoch 283/300\n",
            "\n",
            "Epoch 00283: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.3626 - acc: 0.8548 - val_loss: 0.7266 - val_acc: 0.6000\n",
            "Epoch 284/300\n",
            "\n",
            "Epoch 00284: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.3601 - acc: 0.8306 - val_loss: 0.7869 - val_acc: 0.6200\n",
            "Epoch 285/300\n",
            "\n",
            "Epoch 00285: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.3817 - acc: 0.8403 - val_loss: 0.7573 - val_acc: 0.5800\n",
            "Epoch 286/300\n",
            "\n",
            "Epoch 00286: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.3690 - acc: 0.8355 - val_loss: 0.7365 - val_acc: 0.6400\n",
            "Epoch 287/300\n",
            "\n",
            "Epoch 00287: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.3963 - acc: 0.8258 - val_loss: 0.6251 - val_acc: 0.6800\n",
            "Epoch 288/300\n",
            "\n",
            "Epoch 00288: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.3353 - acc: 0.8581 - val_loss: 0.7195 - val_acc: 0.6600\n",
            "Epoch 289/300\n",
            "\n",
            "Epoch 00289: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.3341 - acc: 0.8710 - val_loss: 0.6248 - val_acc: 0.6500\n",
            "Epoch 290/300\n",
            "\n",
            "Epoch 00290: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.3496 - acc: 0.8468 - val_loss: 0.7377 - val_acc: 0.6600\n",
            "Epoch 291/300\n",
            "\n",
            "Epoch 00291: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.3685 - acc: 0.8435 - val_loss: 0.7246 - val_acc: 0.6300\n",
            "Epoch 292/300\n",
            "\n",
            "Epoch 00292: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.3764 - acc: 0.8210 - val_loss: 0.6582 - val_acc: 0.6500\n",
            "Epoch 293/300\n",
            "\n",
            "Epoch 00293: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.3862 - acc: 0.8129 - val_loss: 0.6222 - val_acc: 0.7000\n",
            "Epoch 294/300\n",
            "\n",
            "Epoch 00294: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.3743 - acc: 0.8339 - val_loss: 0.6743 - val_acc: 0.6600\n",
            "Epoch 295/300\n",
            "\n",
            "Epoch 00295: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.3786 - acc: 0.8226 - val_loss: 0.7093 - val_acc: 0.6200\n",
            "Epoch 296/300\n",
            "\n",
            "Epoch 00296: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.3596 - acc: 0.8355 - val_loss: 0.7160 - val_acc: 0.6400\n",
            "Epoch 297/300\n",
            "\n",
            "Epoch 00297: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.3754 - acc: 0.8290 - val_loss: 0.8000 - val_acc: 0.6000\n",
            "Epoch 298/300\n",
            "\n",
            "Epoch 00298: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.3911 - acc: 0.8355 - val_loss: 0.7305 - val_acc: 0.6100\n",
            "Epoch 299/300\n",
            "\n",
            "Epoch 00299: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.3717 - acc: 0.8355 - val_loss: 0.6404 - val_acc: 0.6800\n",
            "Epoch 300/300\n",
            "\n",
            "Epoch 00300: val_loss did not improve from 0.56235\n",
            "620/620 - 0s - loss: 0.3719 - acc: 0.8161 - val_loss: 0.7379 - val_acc: 0.6300\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAQEBAYn4/0/iilZWlKw9AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "# Evaluate on test data\n",
            "40/40 [==============================] - 0s 2ms/sample - loss: 0.5788 - acc: 0.7250\n",
            "test loss, test acc: [0.5788059629034251, 0.725]\n",
            "1\n",
            "EEG_Deep/Data2A/parsed_P01T.mat\n",
            "(0, 12, 4096)\n",
            "(80, 12, 4096)\n",
            "(80, 1)\n",
            "3\n",
            "EEG_Deep/Data2A/parsed_P03T.mat\n",
            "(80, 12, 4096)\n",
            "(160, 12, 4096)\n",
            "(160, 1)\n",
            "4\n",
            "EEG_Deep/Data2A/parsed_P04T.mat\n",
            "(160, 12, 4096)\n",
            "(240, 12, 4096)\n",
            "(240, 1)\n",
            "5\n",
            "EEG_Deep/Data2A/parsed_P05T.mat\n",
            "(240, 12, 4096)\n",
            "(320, 12, 4096)\n",
            "(320, 1)\n",
            "6\n",
            "EEG_Deep/Data2A/parsed_P06T.mat\n",
            "(320, 12, 4096)\n",
            "(400, 12, 4096)\n",
            "(400, 1)\n",
            "7\n",
            "EEG_Deep/Data2A/parsed_P07T.mat\n",
            "(400, 12, 4096)\n",
            "(480, 12, 4096)\n",
            "(480, 1)\n",
            "8\n",
            "EEG_Deep/Data2A/parsed_P08T.mat\n",
            "(480, 12, 4096)\n",
            "(560, 12, 4096)\n",
            "(560, 1)\n",
            "9\n",
            "EEG_Deep/Data2A/parsed_P09T.mat\n",
            "(560, 12, 4096)\n",
            "(640, 12, 4096)\n",
            "(640, 1)\n",
            "10\n",
            "EEG_Deep/Data2A/parsed_P010T.mat\n",
            "(640, 12, 4096)\n",
            "(720, 12, 4096)\n",
            "(720, 1)\n",
            "2\n",
            "EEG_Deep/Data2A/parsed_P02E.mat\n",
            "(0, 12, 4096)\n",
            "(40, 12, 4096)\n",
            "(40, 1)\n",
            "Filtering of training data in progress\n",
            "(720, 12, 4096)\n",
            "Filtering of test data in progress\n",
            "(40, 12, 4096)\n",
            "[1. 1. 2. 2. 2. 1. 2. 1. 1. 1. 2. 1. 1. 2. 2. 1. 2. 2. 1. 1. 2. 2. 1. 2.\n",
            " 2. 1. 1. 2. 2. 2. 2. 1. 1. 1. 1. 2. 1. 1. 1. 2. 1. 1. 1. 1. 2. 2. 1. 1.\n",
            " 2. 1. 1. 2. 1. 2. 1. 1. 1. 1. 1. 1. 2. 2. 2. 1. 2. 1. 2. 2. 1. 2. 1. 1.\n",
            " 1. 1. 2. 2. 2. 1. 2. 1. 1. 2. 2. 1. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 2.\n",
            " 1. 2. 1. 2.]\n",
            "(620, 12, 1536)\n",
            "(620,)\n",
            "(100, 12, 1536)\n",
            "(100,)\n",
            "[[1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]]\n",
            "X_train shape: (620, 1, 12, 1536)\n",
            "620 train samples\n",
            "100 val samples\n",
            "(40, 12, 1536)\n",
            "(40, 1)\n",
            "X_train shape: (40, 1, 12, 1536)\n",
            "40 train samples\n",
            "Train on 620 samples, validate on 100 samples\n",
            "Epoch 1/300\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 1.21475, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.9485 - acc: 0.4871 - val_loss: 1.2148 - val_acc: 0.4900\n",
            "Epoch 2/300\n",
            "\n",
            "Epoch 00002: val_loss improved from 1.21475 to 1.05331, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.7671 - acc: 0.5516 - val_loss: 1.0533 - val_acc: 0.5400\n",
            "Epoch 3/300\n",
            "\n",
            "Epoch 00003: val_loss improved from 1.05331 to 0.95241, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.7257 - acc: 0.6290 - val_loss: 0.9524 - val_acc: 0.5900\n",
            "Epoch 4/300\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.95241 to 0.88009, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.6941 - acc: 0.6339 - val_loss: 0.8801 - val_acc: 0.6800\n",
            "Epoch 5/300\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.88009 to 0.85125, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.6595 - acc: 0.6516 - val_loss: 0.8512 - val_acc: 0.5800\n",
            "Epoch 6/300\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.85125 to 0.80374, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 0s - loss: 0.6257 - acc: 0.6871 - val_loss: 0.8037 - val_acc: 0.5500\n",
            "Epoch 7/300\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.80374 to 0.78257, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.6144 - acc: 0.6952 - val_loss: 0.7826 - val_acc: 0.5300\n",
            "Epoch 8/300\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.78257\n",
            "620/620 - 0s - loss: 0.5986 - acc: 0.7000 - val_loss: 0.7883 - val_acc: 0.5200\n",
            "Epoch 9/300\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.78257 to 0.70824, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.5737 - acc: 0.7435 - val_loss: 0.7082 - val_acc: 0.6900\n",
            "Epoch 10/300\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.70824 to 0.64652, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.5692 - acc: 0.7274 - val_loss: 0.6465 - val_acc: 0.7000\n",
            "Epoch 11/300\n",
            "\n",
            "Epoch 00011: val_loss improved from 0.64652 to 0.62770, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.5788 - acc: 0.7113 - val_loss: 0.6277 - val_acc: 0.7300\n",
            "Epoch 12/300\n",
            "\n",
            "Epoch 00012: val_loss improved from 0.62770 to 0.61951, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.5473 - acc: 0.7516 - val_loss: 0.6195 - val_acc: 0.6800\n",
            "Epoch 13/300\n",
            "\n",
            "Epoch 00013: val_loss improved from 0.61951 to 0.60168, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.5735 - acc: 0.7000 - val_loss: 0.6017 - val_acc: 0.7100\n",
            "Epoch 14/300\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.60168\n",
            "620/620 - 0s - loss: 0.5439 - acc: 0.7177 - val_loss: 0.6478 - val_acc: 0.6600\n",
            "Epoch 15/300\n",
            "\n",
            "Epoch 00015: val_loss improved from 0.60168 to 0.54628, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.5593 - acc: 0.7113 - val_loss: 0.5463 - val_acc: 0.7300\n",
            "Epoch 16/300\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.54628\n",
            "620/620 - 0s - loss: 0.5537 - acc: 0.7290 - val_loss: 0.6058 - val_acc: 0.7200\n",
            "Epoch 17/300\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.54628\n",
            "620/620 - 0s - loss: 0.5424 - acc: 0.7403 - val_loss: 0.5628 - val_acc: 0.7300\n",
            "Epoch 18/300\n",
            "\n",
            "Epoch 00018: val_loss improved from 0.54628 to 0.54312, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.5505 - acc: 0.7339 - val_loss: 0.5431 - val_acc: 0.7600\n",
            "Epoch 19/300\n",
            "\n",
            "Epoch 00019: val_loss improved from 0.54312 to 0.53649, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.5487 - acc: 0.7145 - val_loss: 0.5365 - val_acc: 0.7500\n",
            "Epoch 20/300\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.53649\n",
            "620/620 - 0s - loss: 0.5284 - acc: 0.7468 - val_loss: 0.5547 - val_acc: 0.7100\n",
            "Epoch 21/300\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.53649\n",
            "620/620 - 0s - loss: 0.5310 - acc: 0.7274 - val_loss: 0.7139 - val_acc: 0.7100\n",
            "Epoch 22/300\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.53649\n",
            "620/620 - 0s - loss: 0.5348 - acc: 0.7323 - val_loss: 0.6106 - val_acc: 0.6700\n",
            "Epoch 23/300\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.53649\n",
            "620/620 - 0s - loss: 0.5219 - acc: 0.7403 - val_loss: 0.6094 - val_acc: 0.6900\n",
            "Epoch 24/300\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.53649\n",
            "620/620 - 0s - loss: 0.5107 - acc: 0.7597 - val_loss: 0.5704 - val_acc: 0.7400\n",
            "Epoch 25/300\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.53649\n",
            "620/620 - 0s - loss: 0.5171 - acc: 0.7452 - val_loss: 0.5399 - val_acc: 0.7300\n",
            "Epoch 26/300\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.53649\n",
            "620/620 - 0s - loss: 0.5235 - acc: 0.7516 - val_loss: 0.5727 - val_acc: 0.7100\n",
            "Epoch 27/300\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.53649\n",
            "620/620 - 0s - loss: 0.5143 - acc: 0.7435 - val_loss: 0.6480 - val_acc: 0.5900\n",
            "Epoch 28/300\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.53649\n",
            "620/620 - 0s - loss: 0.5114 - acc: 0.7645 - val_loss: 0.5428 - val_acc: 0.7200\n",
            "Epoch 29/300\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.53649\n",
            "620/620 - 0s - loss: 0.5101 - acc: 0.7516 - val_loss: 0.6570 - val_acc: 0.6900\n",
            "Epoch 30/300\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.53649\n",
            "620/620 - 0s - loss: 0.4750 - acc: 0.7758 - val_loss: 0.5940 - val_acc: 0.7400\n",
            "Epoch 31/300\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.53649\n",
            "620/620 - 0s - loss: 0.4830 - acc: 0.7613 - val_loss: 0.5485 - val_acc: 0.7300\n",
            "Epoch 32/300\n",
            "\n",
            "Epoch 00032: val_loss improved from 0.53649 to 0.50492, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.4929 - acc: 0.7629 - val_loss: 0.5049 - val_acc: 0.7600\n",
            "Epoch 33/300\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.50492\n",
            "620/620 - 0s - loss: 0.4989 - acc: 0.7500 - val_loss: 0.5174 - val_acc: 0.7500\n",
            "Epoch 34/300\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.50492\n",
            "620/620 - 0s - loss: 0.4818 - acc: 0.7597 - val_loss: 0.6305 - val_acc: 0.6200\n",
            "Epoch 35/300\n",
            "\n",
            "Epoch 00035: val_loss improved from 0.50492 to 0.50357, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.4815 - acc: 0.7774 - val_loss: 0.5036 - val_acc: 0.7600\n",
            "Epoch 36/300\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4903 - acc: 0.7468 - val_loss: 0.6051 - val_acc: 0.7000\n",
            "Epoch 37/300\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4962 - acc: 0.7694 - val_loss: 0.5326 - val_acc: 0.7300\n",
            "Epoch 38/300\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4840 - acc: 0.7774 - val_loss: 0.5934 - val_acc: 0.7200\n",
            "Epoch 39/300\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4985 - acc: 0.7516 - val_loss: 0.5923 - val_acc: 0.7300\n",
            "Epoch 40/300\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4840 - acc: 0.7532 - val_loss: 0.7089 - val_acc: 0.6300\n",
            "Epoch 41/300\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4795 - acc: 0.7565 - val_loss: 0.8137 - val_acc: 0.5500\n",
            "Epoch 42/300\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4846 - acc: 0.7710 - val_loss: 0.6258 - val_acc: 0.7000\n",
            "Epoch 43/300\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4845 - acc: 0.7774 - val_loss: 0.6604 - val_acc: 0.6800\n",
            "Epoch 44/300\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4906 - acc: 0.7645 - val_loss: 0.5735 - val_acc: 0.7400\n",
            "Epoch 45/300\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4586 - acc: 0.8081 - val_loss: 0.7633 - val_acc: 0.6300\n",
            "Epoch 46/300\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4825 - acc: 0.7532 - val_loss: 0.6928 - val_acc: 0.6100\n",
            "Epoch 47/300\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4652 - acc: 0.7871 - val_loss: 0.5300 - val_acc: 0.7400\n",
            "Epoch 48/300\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4733 - acc: 0.7871 - val_loss: 0.7749 - val_acc: 0.6000\n",
            "Epoch 49/300\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4729 - acc: 0.7710 - val_loss: 0.5957 - val_acc: 0.7100\n",
            "Epoch 50/300\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4756 - acc: 0.7903 - val_loss: 0.6281 - val_acc: 0.7000\n",
            "Epoch 51/300\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4849 - acc: 0.7677 - val_loss: 0.6849 - val_acc: 0.6500\n",
            "Epoch 52/300\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4491 - acc: 0.7806 - val_loss: 0.6620 - val_acc: 0.6400\n",
            "Epoch 53/300\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4738 - acc: 0.7790 - val_loss: 0.6259 - val_acc: 0.7000\n",
            "Epoch 54/300\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4699 - acc: 0.7855 - val_loss: 0.5801 - val_acc: 0.7100\n",
            "Epoch 55/300\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4965 - acc: 0.7694 - val_loss: 0.7540 - val_acc: 0.6000\n",
            "Epoch 56/300\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4662 - acc: 0.7710 - val_loss: 0.6936 - val_acc: 0.6500\n",
            "Epoch 57/300\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4545 - acc: 0.7935 - val_loss: 0.6957 - val_acc: 0.6000\n",
            "Epoch 58/300\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4733 - acc: 0.7629 - val_loss: 0.5745 - val_acc: 0.7100\n",
            "Epoch 59/300\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4480 - acc: 0.7935 - val_loss: 0.5726 - val_acc: 0.7200\n",
            "Epoch 60/300\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4632 - acc: 0.7742 - val_loss: 0.8029 - val_acc: 0.6000\n",
            "Epoch 61/300\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4521 - acc: 0.7823 - val_loss: 0.6017 - val_acc: 0.7300\n",
            "Epoch 62/300\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4408 - acc: 0.7919 - val_loss: 0.5312 - val_acc: 0.7500\n",
            "Epoch 63/300\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4667 - acc: 0.7742 - val_loss: 0.5740 - val_acc: 0.7200\n",
            "Epoch 64/300\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4688 - acc: 0.7839 - val_loss: 0.5629 - val_acc: 0.7400\n",
            "Epoch 65/300\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4613 - acc: 0.7790 - val_loss: 0.6252 - val_acc: 0.7000\n",
            "Epoch 66/300\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4668 - acc: 0.7839 - val_loss: 0.6432 - val_acc: 0.6700\n",
            "Epoch 67/300\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4498 - acc: 0.8048 - val_loss: 0.6358 - val_acc: 0.7300\n",
            "Epoch 68/300\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4457 - acc: 0.7968 - val_loss: 0.5622 - val_acc: 0.7500\n",
            "Epoch 69/300\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4613 - acc: 0.8032 - val_loss: 0.7384 - val_acc: 0.6100\n",
            "Epoch 70/300\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4482 - acc: 0.7758 - val_loss: 0.6622 - val_acc: 0.6900\n",
            "Epoch 71/300\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4480 - acc: 0.7855 - val_loss: 0.9739 - val_acc: 0.5200\n",
            "Epoch 72/300\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4578 - acc: 0.7871 - val_loss: 0.6540 - val_acc: 0.6700\n",
            "Epoch 73/300\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4578 - acc: 0.7774 - val_loss: 0.7796 - val_acc: 0.5900\n",
            "Epoch 74/300\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4241 - acc: 0.8129 - val_loss: 0.5773 - val_acc: 0.7300\n",
            "Epoch 75/300\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4233 - acc: 0.8194 - val_loss: 0.9105 - val_acc: 0.5700\n",
            "Epoch 76/300\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4490 - acc: 0.7806 - val_loss: 0.5688 - val_acc: 0.7200\n",
            "Epoch 77/300\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4896 - acc: 0.7516 - val_loss: 0.9521 - val_acc: 0.5500\n",
            "Epoch 78/300\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4548 - acc: 0.7726 - val_loss: 0.6163 - val_acc: 0.7200\n",
            "Epoch 79/300\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4313 - acc: 0.8016 - val_loss: 0.5297 - val_acc: 0.7100\n",
            "Epoch 80/300\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4528 - acc: 0.7871 - val_loss: 0.7172 - val_acc: 0.6500\n",
            "Epoch 81/300\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4289 - acc: 0.7935 - val_loss: 0.6422 - val_acc: 0.7200\n",
            "Epoch 82/300\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4661 - acc: 0.7790 - val_loss: 0.7159 - val_acc: 0.6300\n",
            "Epoch 83/300\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4426 - acc: 0.7758 - val_loss: 0.5750 - val_acc: 0.7300\n",
            "Epoch 84/300\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4551 - acc: 0.7887 - val_loss: 0.5410 - val_acc: 0.7100\n",
            "Epoch 85/300\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4419 - acc: 0.8113 - val_loss: 0.8092 - val_acc: 0.6500\n",
            "Epoch 86/300\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4503 - acc: 0.7887 - val_loss: 0.6637 - val_acc: 0.6400\n",
            "Epoch 87/300\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4325 - acc: 0.7855 - val_loss: 0.8089 - val_acc: 0.6200\n",
            "Epoch 88/300\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4680 - acc: 0.7726 - val_loss: 0.6302 - val_acc: 0.6800\n",
            "Epoch 89/300\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4438 - acc: 0.7919 - val_loss: 0.6099 - val_acc: 0.6900\n",
            "Epoch 90/300\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4296 - acc: 0.7984 - val_loss: 0.6917 - val_acc: 0.6500\n",
            "Epoch 91/300\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4247 - acc: 0.7952 - val_loss: 0.8570 - val_acc: 0.5700\n",
            "Epoch 92/300\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4164 - acc: 0.8032 - val_loss: 1.0177 - val_acc: 0.5400\n",
            "Epoch 93/300\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4748 - acc: 0.7710 - val_loss: 0.5979 - val_acc: 0.7300\n",
            "Epoch 94/300\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4658 - acc: 0.7919 - val_loss: 0.7514 - val_acc: 0.6000\n",
            "Epoch 95/300\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4499 - acc: 0.8048 - val_loss: 0.5880 - val_acc: 0.7200\n",
            "Epoch 96/300\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4181 - acc: 0.8129 - val_loss: 0.7575 - val_acc: 0.5900\n",
            "Epoch 97/300\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4419 - acc: 0.7984 - val_loss: 0.8805 - val_acc: 0.5700\n",
            "Epoch 98/300\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4231 - acc: 0.8097 - val_loss: 0.5977 - val_acc: 0.7000\n",
            "Epoch 99/300\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4333 - acc: 0.7952 - val_loss: 1.0136 - val_acc: 0.5200\n",
            "Epoch 100/300\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4493 - acc: 0.7903 - val_loss: 0.5583 - val_acc: 0.7100\n",
            "Epoch 101/300\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4649 - acc: 0.7758 - val_loss: 0.7735 - val_acc: 0.5900\n",
            "Epoch 102/300\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4338 - acc: 0.7935 - val_loss: 0.8240 - val_acc: 0.5700\n",
            "Epoch 103/300\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4213 - acc: 0.8032 - val_loss: 0.5661 - val_acc: 0.7400\n",
            "Epoch 104/300\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4355 - acc: 0.7968 - val_loss: 0.6783 - val_acc: 0.6800\n",
            "Epoch 105/300\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4553 - acc: 0.8065 - val_loss: 0.6189 - val_acc: 0.6900\n",
            "Epoch 106/300\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.3975 - acc: 0.8129 - val_loss: 0.6891 - val_acc: 0.7200\n",
            "Epoch 107/300\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4047 - acc: 0.8177 - val_loss: 0.6267 - val_acc: 0.7100\n",
            "Epoch 108/300\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4248 - acc: 0.8048 - val_loss: 0.7146 - val_acc: 0.6800\n",
            "Epoch 109/300\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4407 - acc: 0.8048 - val_loss: 0.6680 - val_acc: 0.6700\n",
            "Epoch 110/300\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.3956 - acc: 0.8081 - val_loss: 0.5941 - val_acc: 0.7300\n",
            "Epoch 111/300\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.3889 - acc: 0.8274 - val_loss: 0.7572 - val_acc: 0.6500\n",
            "Epoch 112/300\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4061 - acc: 0.8177 - val_loss: 0.7305 - val_acc: 0.6400\n",
            "Epoch 113/300\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4426 - acc: 0.7823 - val_loss: 0.6863 - val_acc: 0.6600\n",
            "Epoch 114/300\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.50357\n",
            "620/620 - 1s - loss: 0.4406 - acc: 0.8016 - val_loss: 0.6724 - val_acc: 0.6700\n",
            "Epoch 115/300\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4217 - acc: 0.8016 - val_loss: 0.5870 - val_acc: 0.7200\n",
            "Epoch 116/300\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4172 - acc: 0.8129 - val_loss: 1.0096 - val_acc: 0.5400\n",
            "Epoch 117/300\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4293 - acc: 0.8226 - val_loss: 0.6209 - val_acc: 0.7100\n",
            "Epoch 118/300\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4294 - acc: 0.8000 - val_loss: 0.8283 - val_acc: 0.6000\n",
            "Epoch 119/300\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4195 - acc: 0.7935 - val_loss: 0.6313 - val_acc: 0.6900\n",
            "Epoch 120/300\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4064 - acc: 0.8113 - val_loss: 0.9080 - val_acc: 0.5300\n",
            "Epoch 121/300\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4051 - acc: 0.8065 - val_loss: 0.8729 - val_acc: 0.5800\n",
            "Epoch 122/300\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4437 - acc: 0.7935 - val_loss: 0.5582 - val_acc: 0.7400\n",
            "Epoch 123/300\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4451 - acc: 0.7790 - val_loss: 0.6131 - val_acc: 0.6700\n",
            "Epoch 124/300\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4308 - acc: 0.8016 - val_loss: 0.7910 - val_acc: 0.5900\n",
            "Epoch 125/300\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.3919 - acc: 0.8274 - val_loss: 0.5610 - val_acc: 0.7300\n",
            "Epoch 126/300\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4179 - acc: 0.8000 - val_loss: 0.6628 - val_acc: 0.6300\n",
            "Epoch 127/300\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4337 - acc: 0.7903 - val_loss: 0.6524 - val_acc: 0.6300\n",
            "Epoch 128/300\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4143 - acc: 0.8258 - val_loss: 0.9416 - val_acc: 0.5600\n",
            "Epoch 129/300\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4216 - acc: 0.8016 - val_loss: 0.7015 - val_acc: 0.6600\n",
            "Epoch 130/300\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4369 - acc: 0.7984 - val_loss: 0.8674 - val_acc: 0.5800\n",
            "Epoch 131/300\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4342 - acc: 0.7887 - val_loss: 0.5881 - val_acc: 0.7100\n",
            "Epoch 132/300\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4371 - acc: 0.7935 - val_loss: 0.6792 - val_acc: 0.7100\n",
            "Epoch 133/300\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4123 - acc: 0.8065 - val_loss: 0.6055 - val_acc: 0.7300\n",
            "Epoch 134/300\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4105 - acc: 0.8194 - val_loss: 0.6683 - val_acc: 0.6700\n",
            "Epoch 135/300\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4317 - acc: 0.7919 - val_loss: 0.5729 - val_acc: 0.7300\n",
            "Epoch 136/300\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4145 - acc: 0.8145 - val_loss: 0.6678 - val_acc: 0.6400\n",
            "Epoch 137/300\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4175 - acc: 0.8145 - val_loss: 0.6397 - val_acc: 0.7100\n",
            "Epoch 138/300\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4248 - acc: 0.8016 - val_loss: 0.7032 - val_acc: 0.6400\n",
            "Epoch 139/300\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.3943 - acc: 0.8194 - val_loss: 0.8007 - val_acc: 0.6500\n",
            "Epoch 140/300\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4287 - acc: 0.8065 - val_loss: 0.6895 - val_acc: 0.6800\n",
            "Epoch 141/300\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4144 - acc: 0.8097 - val_loss: 0.7470 - val_acc: 0.6500\n",
            "Epoch 142/300\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4065 - acc: 0.8129 - val_loss: 0.6495 - val_acc: 0.6900\n",
            "Epoch 143/300\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4125 - acc: 0.8145 - val_loss: 0.6140 - val_acc: 0.7200\n",
            "Epoch 144/300\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4071 - acc: 0.7968 - val_loss: 0.7020 - val_acc: 0.6700\n",
            "Epoch 145/300\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.3818 - acc: 0.8274 - val_loss: 0.5716 - val_acc: 0.7500\n",
            "Epoch 146/300\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.3925 - acc: 0.8242 - val_loss: 0.7212 - val_acc: 0.6900\n",
            "Epoch 147/300\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4029 - acc: 0.8016 - val_loss: 0.6999 - val_acc: 0.6400\n",
            "Epoch 148/300\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.3956 - acc: 0.8323 - val_loss: 0.8509 - val_acc: 0.5900\n",
            "Epoch 149/300\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4201 - acc: 0.8000 - val_loss: 0.9384 - val_acc: 0.5600\n",
            "Epoch 150/300\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4115 - acc: 0.7968 - val_loss: 0.6717 - val_acc: 0.6300\n",
            "Epoch 151/300\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4110 - acc: 0.8081 - val_loss: 0.7347 - val_acc: 0.6300\n",
            "Epoch 152/300\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4096 - acc: 0.8226 - val_loss: 0.6845 - val_acc: 0.6500\n",
            "Epoch 153/300\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4124 - acc: 0.8097 - val_loss: 0.6054 - val_acc: 0.7200\n",
            "Epoch 154/300\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4125 - acc: 0.7919 - val_loss: 0.7133 - val_acc: 0.6200\n",
            "Epoch 155/300\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4149 - acc: 0.8048 - val_loss: 0.8011 - val_acc: 0.6000\n",
            "Epoch 156/300\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.3924 - acc: 0.8161 - val_loss: 0.7420 - val_acc: 0.6400\n",
            "Epoch 157/300\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4034 - acc: 0.8194 - val_loss: 0.7090 - val_acc: 0.6300\n",
            "Epoch 158/300\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4112 - acc: 0.8113 - val_loss: 0.7383 - val_acc: 0.6200\n",
            "Epoch 159/300\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4142 - acc: 0.8065 - val_loss: 0.9704 - val_acc: 0.5500\n",
            "Epoch 160/300\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4017 - acc: 0.8161 - val_loss: 1.0243 - val_acc: 0.5100\n",
            "Epoch 161/300\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4218 - acc: 0.7984 - val_loss: 0.8408 - val_acc: 0.5900\n",
            "Epoch 162/300\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4065 - acc: 0.8306 - val_loss: 0.5689 - val_acc: 0.6900\n",
            "Epoch 163/300\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4118 - acc: 0.8081 - val_loss: 0.6697 - val_acc: 0.6500\n",
            "Epoch 164/300\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4108 - acc: 0.8000 - val_loss: 0.7505 - val_acc: 0.6100\n",
            "Epoch 165/300\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4212 - acc: 0.7903 - val_loss: 0.5679 - val_acc: 0.7200\n",
            "Epoch 166/300\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4104 - acc: 0.7919 - val_loss: 0.5608 - val_acc: 0.7300\n",
            "Epoch 167/300\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4338 - acc: 0.7935 - val_loss: 0.6319 - val_acc: 0.6500\n",
            "Epoch 168/300\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.3901 - acc: 0.8145 - val_loss: 0.6521 - val_acc: 0.6600\n",
            "Epoch 169/300\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4243 - acc: 0.7952 - val_loss: 0.6277 - val_acc: 0.6600\n",
            "Epoch 170/300\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.50357\n",
            "620/620 - 1s - loss: 0.4191 - acc: 0.8226 - val_loss: 0.5666 - val_acc: 0.7400\n",
            "Epoch 171/300\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4039 - acc: 0.8177 - val_loss: 0.6200 - val_acc: 0.7000\n",
            "Epoch 172/300\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.3866 - acc: 0.8339 - val_loss: 0.7015 - val_acc: 0.6000\n",
            "Epoch 173/300\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.3832 - acc: 0.8129 - val_loss: 0.8447 - val_acc: 0.5600\n",
            "Epoch 174/300\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.3930 - acc: 0.8290 - val_loss: 0.7254 - val_acc: 0.6300\n",
            "Epoch 175/300\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.3732 - acc: 0.8323 - val_loss: 0.7202 - val_acc: 0.6200\n",
            "Epoch 176/300\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.3902 - acc: 0.8161 - val_loss: 0.5857 - val_acc: 0.7400\n",
            "Epoch 177/300\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.3875 - acc: 0.8323 - val_loss: 0.7058 - val_acc: 0.6400\n",
            "Epoch 178/300\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4015 - acc: 0.8048 - val_loss: 0.6424 - val_acc: 0.6600\n",
            "Epoch 179/300\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4210 - acc: 0.7968 - val_loss: 0.7028 - val_acc: 0.6900\n",
            "Epoch 180/300\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4020 - acc: 0.8113 - val_loss: 0.6505 - val_acc: 0.7200\n",
            "Epoch 181/300\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.3943 - acc: 0.8048 - val_loss: 0.5956 - val_acc: 0.7300\n",
            "Epoch 182/300\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.3930 - acc: 0.8355 - val_loss: 0.7711 - val_acc: 0.5900\n",
            "Epoch 183/300\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4017 - acc: 0.8145 - val_loss: 0.6251 - val_acc: 0.7000\n",
            "Epoch 184/300\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.3899 - acc: 0.8210 - val_loss: 0.5917 - val_acc: 0.7300\n",
            "Epoch 185/300\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4357 - acc: 0.8000 - val_loss: 0.6159 - val_acc: 0.7200\n",
            "Epoch 186/300\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4260 - acc: 0.7952 - val_loss: 0.6834 - val_acc: 0.6500\n",
            "Epoch 187/300\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4057 - acc: 0.8306 - val_loss: 0.6111 - val_acc: 0.7100\n",
            "Epoch 188/300\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.3997 - acc: 0.8258 - val_loss: 0.6835 - val_acc: 0.6400\n",
            "Epoch 189/300\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.3796 - acc: 0.8177 - val_loss: 0.7874 - val_acc: 0.6300\n",
            "Epoch 190/300\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.3832 - acc: 0.8177 - val_loss: 0.7688 - val_acc: 0.6000\n",
            "Epoch 191/300\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4109 - acc: 0.8113 - val_loss: 0.7918 - val_acc: 0.6100\n",
            "Epoch 192/300\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.3973 - acc: 0.8339 - val_loss: 0.8742 - val_acc: 0.5600\n",
            "Epoch 193/300\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4246 - acc: 0.8032 - val_loss: 0.7073 - val_acc: 0.6500\n",
            "Epoch 194/300\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.3962 - acc: 0.8242 - val_loss: 0.6172 - val_acc: 0.6800\n",
            "Epoch 195/300\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4259 - acc: 0.7935 - val_loss: 0.7807 - val_acc: 0.5700\n",
            "Epoch 196/300\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4024 - acc: 0.8161 - val_loss: 0.7759 - val_acc: 0.5900\n",
            "Epoch 197/300\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.3832 - acc: 0.8306 - val_loss: 0.7873 - val_acc: 0.6000\n",
            "Epoch 198/300\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4050 - acc: 0.7935 - val_loss: 0.7072 - val_acc: 0.6600\n",
            "Epoch 199/300\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.3937 - acc: 0.8177 - val_loss: 0.6478 - val_acc: 0.7000\n",
            "Epoch 200/300\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.3877 - acc: 0.8210 - val_loss: 0.5439 - val_acc: 0.7500\n",
            "Epoch 201/300\n",
            "\n",
            "Epoch 00201: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4246 - acc: 0.8000 - val_loss: 0.8466 - val_acc: 0.5700\n",
            "Epoch 202/300\n",
            "\n",
            "Epoch 00202: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4412 - acc: 0.7742 - val_loss: 0.6331 - val_acc: 0.7100\n",
            "Epoch 203/300\n",
            "\n",
            "Epoch 00203: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.3980 - acc: 0.8242 - val_loss: 0.7097 - val_acc: 0.6300\n",
            "Epoch 204/300\n",
            "\n",
            "Epoch 00204: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4246 - acc: 0.7871 - val_loss: 0.8504 - val_acc: 0.5900\n",
            "Epoch 205/300\n",
            "\n",
            "Epoch 00205: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4033 - acc: 0.8081 - val_loss: 0.7795 - val_acc: 0.5800\n",
            "Epoch 206/300\n",
            "\n",
            "Epoch 00206: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.3795 - acc: 0.8339 - val_loss: 0.7616 - val_acc: 0.6000\n",
            "Epoch 207/300\n",
            "\n",
            "Epoch 00207: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.3722 - acc: 0.8016 - val_loss: 0.7560 - val_acc: 0.6900\n",
            "Epoch 208/300\n",
            "\n",
            "Epoch 00208: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.3865 - acc: 0.8306 - val_loss: 0.8918 - val_acc: 0.5600\n",
            "Epoch 209/300\n",
            "\n",
            "Epoch 00209: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.3784 - acc: 0.8242 - val_loss: 0.8251 - val_acc: 0.6100\n",
            "Epoch 210/300\n",
            "\n",
            "Epoch 00210: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4015 - acc: 0.8145 - val_loss: 0.6539 - val_acc: 0.6600\n",
            "Epoch 211/300\n",
            "\n",
            "Epoch 00211: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4422 - acc: 0.8129 - val_loss: 0.6281 - val_acc: 0.6900\n",
            "Epoch 212/300\n",
            "\n",
            "Epoch 00212: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.3926 - acc: 0.8339 - val_loss: 0.6640 - val_acc: 0.6600\n",
            "Epoch 213/300\n",
            "\n",
            "Epoch 00213: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4053 - acc: 0.8129 - val_loss: 0.6299 - val_acc: 0.7300\n",
            "Epoch 214/300\n",
            "\n",
            "Epoch 00214: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.3950 - acc: 0.8290 - val_loss: 0.7308 - val_acc: 0.6600\n",
            "Epoch 215/300\n",
            "\n",
            "Epoch 00215: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4061 - acc: 0.8081 - val_loss: 0.8660 - val_acc: 0.6000\n",
            "Epoch 216/300\n",
            "\n",
            "Epoch 00216: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.3819 - acc: 0.8306 - val_loss: 0.7978 - val_acc: 0.6600\n",
            "Epoch 217/300\n",
            "\n",
            "Epoch 00217: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.3700 - acc: 0.8177 - val_loss: 0.6451 - val_acc: 0.6500\n",
            "Epoch 218/300\n",
            "\n",
            "Epoch 00218: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.3856 - acc: 0.8210 - val_loss: 0.6300 - val_acc: 0.7100\n",
            "Epoch 219/300\n",
            "\n",
            "Epoch 00219: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.3616 - acc: 0.8419 - val_loss: 0.5787 - val_acc: 0.7500\n",
            "Epoch 220/300\n",
            "\n",
            "Epoch 00220: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.3831 - acc: 0.8274 - val_loss: 0.6962 - val_acc: 0.6700\n",
            "Epoch 221/300\n",
            "\n",
            "Epoch 00221: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.3917 - acc: 0.8177 - val_loss: 0.7050 - val_acc: 0.6600\n",
            "Epoch 222/300\n",
            "\n",
            "Epoch 00222: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.3739 - acc: 0.8258 - val_loss: 0.6961 - val_acc: 0.6400\n",
            "Epoch 223/300\n",
            "\n",
            "Epoch 00223: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.3975 - acc: 0.8161 - val_loss: 0.6129 - val_acc: 0.7000\n",
            "Epoch 224/300\n",
            "\n",
            "Epoch 00224: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.3820 - acc: 0.8242 - val_loss: 0.5766 - val_acc: 0.7500\n",
            "Epoch 225/300\n",
            "\n",
            "Epoch 00225: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.3767 - acc: 0.8339 - val_loss: 0.6303 - val_acc: 0.7200\n",
            "Epoch 226/300\n",
            "\n",
            "Epoch 00226: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.3646 - acc: 0.8468 - val_loss: 0.8358 - val_acc: 0.5600\n",
            "Epoch 227/300\n",
            "\n",
            "Epoch 00227: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.3842 - acc: 0.8323 - val_loss: 0.6986 - val_acc: 0.6400\n",
            "Epoch 228/300\n",
            "\n",
            "Epoch 00228: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.3939 - acc: 0.8226 - val_loss: 0.5818 - val_acc: 0.7600\n",
            "Epoch 229/300\n",
            "\n",
            "Epoch 00229: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4060 - acc: 0.8145 - val_loss: 0.7910 - val_acc: 0.6100\n",
            "Epoch 230/300\n",
            "\n",
            "Epoch 00230: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.3911 - acc: 0.8242 - val_loss: 0.6873 - val_acc: 0.6800\n",
            "Epoch 231/300\n",
            "\n",
            "Epoch 00231: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.3928 - acc: 0.8210 - val_loss: 0.7591 - val_acc: 0.6100\n",
            "Epoch 232/300\n",
            "\n",
            "Epoch 00232: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.3952 - acc: 0.8242 - val_loss: 0.6836 - val_acc: 0.7000\n",
            "Epoch 233/300\n",
            "\n",
            "Epoch 00233: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4098 - acc: 0.8145 - val_loss: 0.8801 - val_acc: 0.6000\n",
            "Epoch 234/300\n",
            "\n",
            "Epoch 00234: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4100 - acc: 0.8129 - val_loss: 0.6109 - val_acc: 0.7100\n",
            "Epoch 235/300\n",
            "\n",
            "Epoch 00235: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4285 - acc: 0.7935 - val_loss: 0.6185 - val_acc: 0.7000\n",
            "Epoch 236/300\n",
            "\n",
            "Epoch 00236: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.3871 - acc: 0.8274 - val_loss: 0.6197 - val_acc: 0.6900\n",
            "Epoch 237/300\n",
            "\n",
            "Epoch 00237: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.3751 - acc: 0.8274 - val_loss: 0.7043 - val_acc: 0.6300\n",
            "Epoch 238/300\n",
            "\n",
            "Epoch 00238: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.3919 - acc: 0.8306 - val_loss: 0.6593 - val_acc: 0.7100\n",
            "Epoch 239/300\n",
            "\n",
            "Epoch 00239: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.3907 - acc: 0.8113 - val_loss: 0.5603 - val_acc: 0.7500\n",
            "Epoch 240/300\n",
            "\n",
            "Epoch 00240: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.3901 - acc: 0.8242 - val_loss: 0.5543 - val_acc: 0.7600\n",
            "Epoch 241/300\n",
            "\n",
            "Epoch 00241: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.3863 - acc: 0.8177 - val_loss: 0.7312 - val_acc: 0.6400\n",
            "Epoch 242/300\n",
            "\n",
            "Epoch 00242: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.3909 - acc: 0.8145 - val_loss: 0.8564 - val_acc: 0.5700\n",
            "Epoch 243/300\n",
            "\n",
            "Epoch 00243: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4082 - acc: 0.8032 - val_loss: 0.6398 - val_acc: 0.7000\n",
            "Epoch 244/300\n",
            "\n",
            "Epoch 00244: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.3856 - acc: 0.8323 - val_loss: 0.7614 - val_acc: 0.6000\n",
            "Epoch 245/300\n",
            "\n",
            "Epoch 00245: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4155 - acc: 0.8161 - val_loss: 0.6791 - val_acc: 0.6700\n",
            "Epoch 246/300\n",
            "\n",
            "Epoch 00246: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.3815 - acc: 0.8226 - val_loss: 0.6767 - val_acc: 0.6600\n",
            "Epoch 247/300\n",
            "\n",
            "Epoch 00247: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.3633 - acc: 0.8516 - val_loss: 0.6811 - val_acc: 0.6600\n",
            "Epoch 248/300\n",
            "\n",
            "Epoch 00248: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.3774 - acc: 0.8323 - val_loss: 0.5776 - val_acc: 0.7500\n",
            "Epoch 249/300\n",
            "\n",
            "Epoch 00249: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.3711 - acc: 0.8258 - val_loss: 0.8862 - val_acc: 0.5800\n",
            "Epoch 250/300\n",
            "\n",
            "Epoch 00250: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.3753 - acc: 0.8290 - val_loss: 0.7035 - val_acc: 0.6400\n",
            "Epoch 251/300\n",
            "\n",
            "Epoch 00251: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4002 - acc: 0.8210 - val_loss: 0.6610 - val_acc: 0.6700\n",
            "Epoch 252/300\n",
            "\n",
            "Epoch 00252: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.3910 - acc: 0.8290 - val_loss: 0.6322 - val_acc: 0.6800\n",
            "Epoch 253/300\n",
            "\n",
            "Epoch 00253: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.3797 - acc: 0.8161 - val_loss: 0.6210 - val_acc: 0.7200\n",
            "Epoch 254/300\n",
            "\n",
            "Epoch 00254: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4050 - acc: 0.8226 - val_loss: 0.7271 - val_acc: 0.6400\n",
            "Epoch 255/300\n",
            "\n",
            "Epoch 00255: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.3844 - acc: 0.8290 - val_loss: 0.6521 - val_acc: 0.6700\n",
            "Epoch 256/300\n",
            "\n",
            "Epoch 00256: val_loss did not improve from 0.50357\n",
            "620/620 - 1s - loss: 0.3652 - acc: 0.8532 - val_loss: 0.7510 - val_acc: 0.6100\n",
            "Epoch 257/300\n",
            "\n",
            "Epoch 00257: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.3651 - acc: 0.8468 - val_loss: 0.7098 - val_acc: 0.6600\n",
            "Epoch 258/300\n",
            "\n",
            "Epoch 00258: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.3703 - acc: 0.8323 - val_loss: 0.5895 - val_acc: 0.7300\n",
            "Epoch 259/300\n",
            "\n",
            "Epoch 00259: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.3753 - acc: 0.8371 - val_loss: 0.7126 - val_acc: 0.6300\n",
            "Epoch 260/300\n",
            "\n",
            "Epoch 00260: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.3875 - acc: 0.8274 - val_loss: 0.6900 - val_acc: 0.6500\n",
            "Epoch 261/300\n",
            "\n",
            "Epoch 00261: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.3972 - acc: 0.8371 - val_loss: 0.5859 - val_acc: 0.7500\n",
            "Epoch 262/300\n",
            "\n",
            "Epoch 00262: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.3625 - acc: 0.8516 - val_loss: 0.5637 - val_acc: 0.7500\n",
            "Epoch 263/300\n",
            "\n",
            "Epoch 00263: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.3904 - acc: 0.8242 - val_loss: 0.6096 - val_acc: 0.6800\n",
            "Epoch 264/300\n",
            "\n",
            "Epoch 00264: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4153 - acc: 0.8145 - val_loss: 0.5847 - val_acc: 0.7500\n",
            "Epoch 265/300\n",
            "\n",
            "Epoch 00265: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.3857 - acc: 0.8435 - val_loss: 0.8694 - val_acc: 0.5900\n",
            "Epoch 266/300\n",
            "\n",
            "Epoch 00266: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.3629 - acc: 0.8435 - val_loss: 0.7163 - val_acc: 0.6600\n",
            "Epoch 267/300\n",
            "\n",
            "Epoch 00267: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.3494 - acc: 0.8484 - val_loss: 0.6676 - val_acc: 0.6600\n",
            "Epoch 268/300\n",
            "\n",
            "Epoch 00268: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.3919 - acc: 0.8226 - val_loss: 0.5377 - val_acc: 0.7900\n",
            "Epoch 269/300\n",
            "\n",
            "Epoch 00269: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.3962 - acc: 0.8242 - val_loss: 0.8527 - val_acc: 0.5800\n",
            "Epoch 270/300\n",
            "\n",
            "Epoch 00270: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.3622 - acc: 0.8500 - val_loss: 0.8183 - val_acc: 0.6000\n",
            "Epoch 271/300\n",
            "\n",
            "Epoch 00271: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.3693 - acc: 0.8387 - val_loss: 0.7143 - val_acc: 0.6500\n",
            "Epoch 272/300\n",
            "\n",
            "Epoch 00272: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.3741 - acc: 0.8419 - val_loss: 0.8100 - val_acc: 0.6100\n",
            "Epoch 273/300\n",
            "\n",
            "Epoch 00273: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.3608 - acc: 0.8306 - val_loss: 0.6634 - val_acc: 0.6700\n",
            "Epoch 274/300\n",
            "\n",
            "Epoch 00274: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.3534 - acc: 0.8532 - val_loss: 0.8803 - val_acc: 0.5800\n",
            "Epoch 275/300\n",
            "\n",
            "Epoch 00275: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.3728 - acc: 0.8323 - val_loss: 0.6142 - val_acc: 0.6800\n",
            "Epoch 276/300\n",
            "\n",
            "Epoch 00276: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.3851 - acc: 0.8226 - val_loss: 0.6817 - val_acc: 0.6800\n",
            "Epoch 277/300\n",
            "\n",
            "Epoch 00277: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.3574 - acc: 0.8435 - val_loss: 0.5859 - val_acc: 0.7000\n",
            "Epoch 278/300\n",
            "\n",
            "Epoch 00278: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.3575 - acc: 0.8452 - val_loss: 0.5870 - val_acc: 0.7400\n",
            "Epoch 279/300\n",
            "\n",
            "Epoch 00279: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.3795 - acc: 0.8371 - val_loss: 0.5573 - val_acc: 0.7500\n",
            "Epoch 280/300\n",
            "\n",
            "Epoch 00280: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.3648 - acc: 0.8371 - val_loss: 0.9112 - val_acc: 0.5400\n",
            "Epoch 281/300\n",
            "\n",
            "Epoch 00281: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.3562 - acc: 0.8532 - val_loss: 0.6887 - val_acc: 0.6700\n",
            "Epoch 282/300\n",
            "\n",
            "Epoch 00282: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.3901 - acc: 0.8161 - val_loss: 0.7386 - val_acc: 0.7000\n",
            "Epoch 283/300\n",
            "\n",
            "Epoch 00283: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4038 - acc: 0.8194 - val_loss: 0.6654 - val_acc: 0.6700\n",
            "Epoch 284/300\n",
            "\n",
            "Epoch 00284: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.3780 - acc: 0.8339 - val_loss: 0.6814 - val_acc: 0.7000\n",
            "Epoch 285/300\n",
            "\n",
            "Epoch 00285: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.3631 - acc: 0.8339 - val_loss: 0.9097 - val_acc: 0.5900\n",
            "Epoch 286/300\n",
            "\n",
            "Epoch 00286: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.3761 - acc: 0.8548 - val_loss: 0.8222 - val_acc: 0.6500\n",
            "Epoch 287/300\n",
            "\n",
            "Epoch 00287: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.3857 - acc: 0.8323 - val_loss: 0.8112 - val_acc: 0.6100\n",
            "Epoch 288/300\n",
            "\n",
            "Epoch 00288: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.3892 - acc: 0.8339 - val_loss: 0.5578 - val_acc: 0.7300\n",
            "Epoch 289/300\n",
            "\n",
            "Epoch 00289: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.3848 - acc: 0.8226 - val_loss: 0.5607 - val_acc: 0.7300\n",
            "Epoch 290/300\n",
            "\n",
            "Epoch 00290: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.4075 - acc: 0.8129 - val_loss: 0.5848 - val_acc: 0.7300\n",
            "Epoch 291/300\n",
            "\n",
            "Epoch 00291: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.3728 - acc: 0.8210 - val_loss: 0.6635 - val_acc: 0.6800\n",
            "Epoch 292/300\n",
            "\n",
            "Epoch 00292: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.3689 - acc: 0.8452 - val_loss: 0.7004 - val_acc: 0.6800\n",
            "Epoch 293/300\n",
            "\n",
            "Epoch 00293: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.3842 - acc: 0.8097 - val_loss: 0.8590 - val_acc: 0.5900\n",
            "Epoch 294/300\n",
            "\n",
            "Epoch 00294: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.3578 - acc: 0.8323 - val_loss: 0.6007 - val_acc: 0.7600\n",
            "Epoch 295/300\n",
            "\n",
            "Epoch 00295: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.3811 - acc: 0.8258 - val_loss: 0.6829 - val_acc: 0.6700\n",
            "Epoch 296/300\n",
            "\n",
            "Epoch 00296: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.3755 - acc: 0.8435 - val_loss: 0.7505 - val_acc: 0.6200\n",
            "Epoch 297/300\n",
            "\n",
            "Epoch 00297: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.3881 - acc: 0.8435 - val_loss: 0.5445 - val_acc: 0.7000\n",
            "Epoch 298/300\n",
            "\n",
            "Epoch 00298: val_loss did not improve from 0.50357\n",
            "620/620 - 1s - loss: 0.3582 - acc: 0.8419 - val_loss: 0.7338 - val_acc: 0.6700\n",
            "Epoch 299/300\n",
            "\n",
            "Epoch 00299: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.3663 - acc: 0.8484 - val_loss: 0.9152 - val_acc: 0.5900\n",
            "Epoch 300/300\n",
            "\n",
            "Epoch 00300: val_loss did not improve from 0.50357\n",
            "620/620 - 0s - loss: 0.3886 - acc: 0.8226 - val_loss: 0.7335 - val_acc: 0.6700\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngAAngoEICAgICHDi/wMVS4CUYzIASgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "# Evaluate on test data\n",
            "40/40 [==============================] - 0s 2ms/sample - loss: 0.6418 - acc: 0.7750\n",
            "test loss, test acc: [0.6418359039351345, 0.775]\n",
            "1\n",
            "EEG_Deep/Data2A/parsed_P01T.mat\n",
            "(0, 12, 4096)\n",
            "(80, 12, 4096)\n",
            "(80, 1)\n",
            "2\n",
            "EEG_Deep/Data2A/parsed_P02T.mat\n",
            "(80, 12, 4096)\n",
            "(160, 12, 4096)\n",
            "(160, 1)\n",
            "3\n",
            "EEG_Deep/Data2A/parsed_P03T.mat\n",
            "(160, 12, 4096)\n",
            "(240, 12, 4096)\n",
            "(240, 1)\n",
            "4\n",
            "EEG_Deep/Data2A/parsed_P04T.mat\n",
            "(240, 12, 4096)\n",
            "(320, 12, 4096)\n",
            "(320, 1)\n",
            "5\n",
            "EEG_Deep/Data2A/parsed_P05T.mat\n",
            "(320, 12, 4096)\n",
            "(400, 12, 4096)\n",
            "(400, 1)\n",
            "6\n",
            "EEG_Deep/Data2A/parsed_P06T.mat\n",
            "(400, 12, 4096)\n",
            "(480, 12, 4096)\n",
            "(480, 1)\n",
            "7\n",
            "EEG_Deep/Data2A/parsed_P07T.mat\n",
            "(480, 12, 4096)\n",
            "(560, 12, 4096)\n",
            "(560, 1)\n",
            "9\n",
            "EEG_Deep/Data2A/parsed_P09T.mat\n",
            "(560, 12, 4096)\n",
            "(640, 12, 4096)\n",
            "(640, 1)\n",
            "10\n",
            "EEG_Deep/Data2A/parsed_P010T.mat\n",
            "(640, 12, 4096)\n",
            "(720, 12, 4096)\n",
            "(720, 1)\n",
            "8\n",
            "EEG_Deep/Data2A/parsed_P08E.mat\n",
            "(0, 12, 4096)\n",
            "(40, 12, 4096)\n",
            "(40, 1)\n",
            "Filtering of training data in progress\n",
            "(720, 12, 4096)\n",
            "Filtering of test data in progress\n",
            "(40, 12, 4096)\n",
            "[2. 2. 1. 1. 1. 1. 2. 2. 1. 1. 1. 1. 2. 1. 2. 1. 2. 2. 2. 1. 2. 2. 2. 1.\n",
            " 1. 1. 2. 2. 2. 1. 1. 1. 1. 2. 1. 1. 2. 2. 1. 2. 1. 1. 1. 2. 1. 1. 2. 2.\n",
            " 1. 2. 1. 1. 2. 1. 2. 2. 1. 2. 1. 2. 1. 2. 2. 1. 2. 2. 1. 1. 1. 1. 2. 2.\n",
            " 2. 1. 1. 1. 1. 2. 2. 2. 1. 1. 2. 1. 2. 1. 1. 2. 2. 1. 2. 2. 2. 1. 2. 1.\n",
            " 1. 2. 2. 2.]\n",
            "(620, 12, 1536)\n",
            "(620,)\n",
            "(100, 12, 1536)\n",
            "(100,)\n",
            "[[0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]]\n",
            "X_train shape: (620, 1, 12, 1536)\n",
            "620 train samples\n",
            "100 val samples\n",
            "(40, 12, 1536)\n",
            "(40, 1)\n",
            "X_train shape: (40, 1, 12, 1536)\n",
            "40 train samples\n",
            "Train on 620 samples, validate on 100 samples\n",
            "Epoch 1/300\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 1.14203, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.9341 - acc: 0.4887 - val_loss: 1.1420 - val_acc: 0.5300\n",
            "Epoch 2/300\n",
            "\n",
            "Epoch 00002: val_loss improved from 1.14203 to 1.02097, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.7539 - acc: 0.5710 - val_loss: 1.0210 - val_acc: 0.5000\n",
            "Epoch 3/300\n",
            "\n",
            "Epoch 00003: val_loss improved from 1.02097 to 0.92447, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.7049 - acc: 0.6468 - val_loss: 0.9245 - val_acc: 0.5600\n",
            "Epoch 4/300\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.92447\n",
            "620/620 - 0s - loss: 0.6455 - acc: 0.6952 - val_loss: 0.9883 - val_acc: 0.5400\n",
            "Epoch 5/300\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.92447 to 0.83520, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.6198 - acc: 0.7129 - val_loss: 0.8352 - val_acc: 0.6800\n",
            "Epoch 6/300\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.83520 to 0.72720, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.5951 - acc: 0.7226 - val_loss: 0.7272 - val_acc: 0.6200\n",
            "Epoch 7/300\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.72720 to 0.65137, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 0s - loss: 0.5455 - acc: 0.7435 - val_loss: 0.6514 - val_acc: 0.6800\n",
            "Epoch 8/300\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.65137 to 0.63814, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.5451 - acc: 0.7532 - val_loss: 0.6381 - val_acc: 0.7200\n",
            "Epoch 9/300\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.63814 to 0.54718, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.4986 - acc: 0.7726 - val_loss: 0.5472 - val_acc: 0.7700\n",
            "Epoch 10/300\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.54718 to 0.53944, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.5131 - acc: 0.7500 - val_loss: 0.5394 - val_acc: 0.8000\n",
            "Epoch 11/300\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.53944\n",
            "620/620 - 0s - loss: 0.4975 - acc: 0.7774 - val_loss: 0.5425 - val_acc: 0.7600\n",
            "Epoch 12/300\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.53944\n",
            "620/620 - 0s - loss: 0.4983 - acc: 0.7629 - val_loss: 0.5497 - val_acc: 0.7600\n",
            "Epoch 13/300\n",
            "\n",
            "Epoch 00013: val_loss improved from 0.53944 to 0.51096, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.4916 - acc: 0.7548 - val_loss: 0.5110 - val_acc: 0.7600\n",
            "Epoch 14/300\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.51096\n",
            "620/620 - 0s - loss: 0.4763 - acc: 0.7677 - val_loss: 0.5653 - val_acc: 0.6800\n",
            "Epoch 15/300\n",
            "\n",
            "Epoch 00015: val_loss improved from 0.51096 to 0.50719, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.4749 - acc: 0.7839 - val_loss: 0.5072 - val_acc: 0.7200\n",
            "Epoch 16/300\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.50719\n",
            "620/620 - 0s - loss: 0.5215 - acc: 0.7435 - val_loss: 0.5546 - val_acc: 0.6900\n",
            "Epoch 17/300\n",
            "\n",
            "Epoch 00017: val_loss improved from 0.50719 to 0.49454, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.4574 - acc: 0.7871 - val_loss: 0.4945 - val_acc: 0.6900\n",
            "Epoch 18/300\n",
            "\n",
            "Epoch 00018: val_loss improved from 0.49454 to 0.46735, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.4560 - acc: 0.8016 - val_loss: 0.4673 - val_acc: 0.7400\n",
            "Epoch 19/300\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.46735\n",
            "620/620 - 0s - loss: 0.4880 - acc: 0.7645 - val_loss: 0.4802 - val_acc: 0.7300\n",
            "Epoch 20/300\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.46735\n",
            "620/620 - 0s - loss: 0.4521 - acc: 0.7919 - val_loss: 0.5204 - val_acc: 0.7400\n",
            "Epoch 21/300\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.46735\n",
            "620/620 - 0s - loss: 0.4746 - acc: 0.7855 - val_loss: 0.4818 - val_acc: 0.7400\n",
            "Epoch 22/300\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.46735\n",
            "620/620 - 0s - loss: 0.4685 - acc: 0.7887 - val_loss: 0.5192 - val_acc: 0.7300\n",
            "Epoch 23/300\n",
            "\n",
            "Epoch 00023: val_loss improved from 0.46735 to 0.46430, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.4704 - acc: 0.7758 - val_loss: 0.4643 - val_acc: 0.7700\n",
            "Epoch 24/300\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.46430\n",
            "620/620 - 0s - loss: 0.4676 - acc: 0.7919 - val_loss: 0.5854 - val_acc: 0.7000\n",
            "Epoch 25/300\n",
            "\n",
            "Epoch 00025: val_loss improved from 0.46430 to 0.45839, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.4685 - acc: 0.7903 - val_loss: 0.4584 - val_acc: 0.7600\n",
            "Epoch 26/300\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.45839\n",
            "620/620 - 0s - loss: 0.4642 - acc: 0.7968 - val_loss: 0.4765 - val_acc: 0.7200\n",
            "Epoch 27/300\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.45839\n",
            "620/620 - 0s - loss: 0.4701 - acc: 0.7823 - val_loss: 0.4603 - val_acc: 0.7100\n",
            "Epoch 28/300\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.45839\n",
            "620/620 - 0s - loss: 0.4630 - acc: 0.7855 - val_loss: 0.5063 - val_acc: 0.6900\n",
            "Epoch 29/300\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.45839\n",
            "620/620 - 0s - loss: 0.4505 - acc: 0.7806 - val_loss: 0.4766 - val_acc: 0.7500\n",
            "Epoch 30/300\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.45839\n",
            "620/620 - 0s - loss: 0.4632 - acc: 0.8000 - val_loss: 0.5192 - val_acc: 0.7000\n",
            "Epoch 31/300\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.45839\n",
            "620/620 - 0s - loss: 0.4568 - acc: 0.7984 - val_loss: 0.4750 - val_acc: 0.7200\n",
            "Epoch 32/300\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.45839\n",
            "620/620 - 0s - loss: 0.4781 - acc: 0.7629 - val_loss: 0.5097 - val_acc: 0.7000\n",
            "Epoch 33/300\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.45839\n",
            "620/620 - 0s - loss: 0.4485 - acc: 0.7968 - val_loss: 0.4788 - val_acc: 0.7400\n",
            "Epoch 34/300\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.45839\n",
            "620/620 - 0s - loss: 0.4805 - acc: 0.7694 - val_loss: 0.5029 - val_acc: 0.7200\n",
            "Epoch 35/300\n",
            "\n",
            "Epoch 00035: val_loss improved from 0.45839 to 0.45723, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.4551 - acc: 0.7887 - val_loss: 0.4572 - val_acc: 0.7500\n",
            "Epoch 36/300\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.45723\n",
            "620/620 - 0s - loss: 0.4521 - acc: 0.7855 - val_loss: 0.4995 - val_acc: 0.7500\n",
            "Epoch 37/300\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.45723\n",
            "620/620 - 0s - loss: 0.4427 - acc: 0.8000 - val_loss: 0.4859 - val_acc: 0.7300\n",
            "Epoch 38/300\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.45723\n",
            "620/620 - 0s - loss: 0.4505 - acc: 0.7935 - val_loss: 0.4866 - val_acc: 0.7600\n",
            "Epoch 39/300\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.45723\n",
            "620/620 - 0s - loss: 0.4543 - acc: 0.7726 - val_loss: 0.4728 - val_acc: 0.7400\n",
            "Epoch 40/300\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.45723\n",
            "620/620 - 0s - loss: 0.4669 - acc: 0.7710 - val_loss: 0.5276 - val_acc: 0.7100\n",
            "Epoch 41/300\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.45723\n",
            "620/620 - 0s - loss: 0.4367 - acc: 0.7968 - val_loss: 0.4698 - val_acc: 0.7200\n",
            "Epoch 42/300\n",
            "\n",
            "Epoch 00042: val_loss improved from 0.45723 to 0.44500, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.4243 - acc: 0.7952 - val_loss: 0.4450 - val_acc: 0.7400\n",
            "Epoch 43/300\n",
            "\n",
            "Epoch 00043: val_loss improved from 0.44500 to 0.44109, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.4350 - acc: 0.8065 - val_loss: 0.4411 - val_acc: 0.7300\n",
            "Epoch 44/300\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.44109\n",
            "620/620 - 0s - loss: 0.4229 - acc: 0.8016 - val_loss: 0.4591 - val_acc: 0.7600\n",
            "Epoch 45/300\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.44109\n",
            "620/620 - 0s - loss: 0.4422 - acc: 0.7774 - val_loss: 0.5017 - val_acc: 0.7500\n",
            "Epoch 46/300\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.44109\n",
            "620/620 - 0s - loss: 0.4295 - acc: 0.8016 - val_loss: 0.4675 - val_acc: 0.7500\n",
            "Epoch 47/300\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.44109\n",
            "620/620 - 0s - loss: 0.4634 - acc: 0.7952 - val_loss: 0.4957 - val_acc: 0.7300\n",
            "Epoch 48/300\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.44109\n",
            "620/620 - 0s - loss: 0.4175 - acc: 0.8081 - val_loss: 0.4807 - val_acc: 0.7500\n",
            "Epoch 49/300\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.44109\n",
            "620/620 - 0s - loss: 0.4126 - acc: 0.8113 - val_loss: 0.5376 - val_acc: 0.7600\n",
            "Epoch 50/300\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.44109\n",
            "620/620 - 0s - loss: 0.4442 - acc: 0.7952 - val_loss: 0.5949 - val_acc: 0.7500\n",
            "Epoch 51/300\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.44109\n",
            "620/620 - 0s - loss: 0.4566 - acc: 0.7790 - val_loss: 0.4434 - val_acc: 0.7800\n",
            "Epoch 52/300\n",
            "\n",
            "Epoch 00052: val_loss improved from 0.44109 to 0.44079, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.4625 - acc: 0.7758 - val_loss: 0.4408 - val_acc: 0.7800\n",
            "Epoch 53/300\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.44079\n",
            "620/620 - 0s - loss: 0.4149 - acc: 0.8161 - val_loss: 0.4747 - val_acc: 0.7600\n",
            "Epoch 54/300\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.44079\n",
            "620/620 - 0s - loss: 0.4018 - acc: 0.8113 - val_loss: 0.4936 - val_acc: 0.7700\n",
            "Epoch 55/300\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.44079\n",
            "620/620 - 0s - loss: 0.4272 - acc: 0.8097 - val_loss: 0.4498 - val_acc: 0.7700\n",
            "Epoch 56/300\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.44079\n",
            "620/620 - 0s - loss: 0.4292 - acc: 0.8081 - val_loss: 0.4506 - val_acc: 0.7800\n",
            "Epoch 57/300\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.44079\n",
            "620/620 - 0s - loss: 0.3985 - acc: 0.8355 - val_loss: 0.4442 - val_acc: 0.7600\n",
            "Epoch 58/300\n",
            "\n",
            "Epoch 00058: val_loss improved from 0.44079 to 0.41541, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.4548 - acc: 0.7758 - val_loss: 0.4154 - val_acc: 0.7700\n",
            "Epoch 59/300\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.41541\n",
            "620/620 - 0s - loss: 0.4074 - acc: 0.8145 - val_loss: 0.4743 - val_acc: 0.7500\n",
            "Epoch 60/300\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.41541\n",
            "620/620 - 0s - loss: 0.4457 - acc: 0.7677 - val_loss: 0.4973 - val_acc: 0.7700\n",
            "Epoch 61/300\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.41541\n",
            "620/620 - 0s - loss: 0.4089 - acc: 0.8097 - val_loss: 0.4673 - val_acc: 0.7800\n",
            "Epoch 62/300\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.41541\n",
            "620/620 - 0s - loss: 0.4223 - acc: 0.8065 - val_loss: 0.5158 - val_acc: 0.7500\n",
            "Epoch 63/300\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.41541\n",
            "620/620 - 0s - loss: 0.4073 - acc: 0.8081 - val_loss: 0.4404 - val_acc: 0.7700\n",
            "Epoch 64/300\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.41541\n",
            "620/620 - 0s - loss: 0.4303 - acc: 0.7968 - val_loss: 0.4519 - val_acc: 0.7800\n",
            "Epoch 65/300\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.41541\n",
            "620/620 - 0s - loss: 0.4100 - acc: 0.8097 - val_loss: 0.4983 - val_acc: 0.7500\n",
            "Epoch 66/300\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.41541\n",
            "620/620 - 0s - loss: 0.4019 - acc: 0.8274 - val_loss: 0.4241 - val_acc: 0.8000\n",
            "Epoch 67/300\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.41541\n",
            "620/620 - 0s - loss: 0.4243 - acc: 0.8065 - val_loss: 0.5237 - val_acc: 0.7200\n",
            "Epoch 68/300\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.41541\n",
            "620/620 - 0s - loss: 0.4210 - acc: 0.7952 - val_loss: 0.5069 - val_acc: 0.7500\n",
            "Epoch 69/300\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.41541\n",
            "620/620 - 0s - loss: 0.4402 - acc: 0.8032 - val_loss: 0.4499 - val_acc: 0.7600\n",
            "Epoch 70/300\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.41541\n",
            "620/620 - 0s - loss: 0.4094 - acc: 0.8129 - val_loss: 0.4421 - val_acc: 0.7500\n",
            "Epoch 71/300\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.41541\n",
            "620/620 - 0s - loss: 0.4170 - acc: 0.8258 - val_loss: 0.4854 - val_acc: 0.7500\n",
            "Epoch 72/300\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.41541\n",
            "620/620 - 0s - loss: 0.4536 - acc: 0.7871 - val_loss: 0.4530 - val_acc: 0.7400\n",
            "Epoch 73/300\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.41541\n",
            "620/620 - 0s - loss: 0.4224 - acc: 0.7935 - val_loss: 0.4822 - val_acc: 0.7400\n",
            "Epoch 74/300\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.41541\n",
            "620/620 - 0s - loss: 0.4027 - acc: 0.8306 - val_loss: 0.4596 - val_acc: 0.7600\n",
            "Epoch 75/300\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.41541\n",
            "620/620 - 0s - loss: 0.3846 - acc: 0.8258 - val_loss: 0.4348 - val_acc: 0.7800\n",
            "Epoch 76/300\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.41541\n",
            "620/620 - 0s - loss: 0.3985 - acc: 0.8226 - val_loss: 0.4599 - val_acc: 0.7500\n",
            "Epoch 77/300\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.41541\n",
            "620/620 - 0s - loss: 0.4041 - acc: 0.8113 - val_loss: 0.4408 - val_acc: 0.7900\n",
            "Epoch 78/300\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.41541\n",
            "620/620 - 0s - loss: 0.4101 - acc: 0.8226 - val_loss: 0.4557 - val_acc: 0.7700\n",
            "Epoch 79/300\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.41541\n",
            "620/620 - 0s - loss: 0.4050 - acc: 0.8161 - val_loss: 0.4846 - val_acc: 0.7600\n",
            "Epoch 80/300\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.41541\n",
            "620/620 - 0s - loss: 0.4386 - acc: 0.8065 - val_loss: 0.4770 - val_acc: 0.7800\n",
            "Epoch 81/300\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.41541\n",
            "620/620 - 0s - loss: 0.4045 - acc: 0.8194 - val_loss: 0.4501 - val_acc: 0.7300\n",
            "Epoch 82/300\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.41541\n",
            "620/620 - 0s - loss: 0.4055 - acc: 0.8290 - val_loss: 0.5557 - val_acc: 0.7800\n",
            "Epoch 83/300\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.41541\n",
            "620/620 - 0s - loss: 0.4100 - acc: 0.8032 - val_loss: 0.4999 - val_acc: 0.7400\n",
            "Epoch 84/300\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.41541\n",
            "620/620 - 0s - loss: 0.4167 - acc: 0.7952 - val_loss: 0.4227 - val_acc: 0.8000\n",
            "Epoch 85/300\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.41541\n",
            "620/620 - 0s - loss: 0.3825 - acc: 0.8371 - val_loss: 0.4255 - val_acc: 0.7700\n",
            "Epoch 86/300\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.41541\n",
            "620/620 - 0s - loss: 0.3979 - acc: 0.8274 - val_loss: 0.4650 - val_acc: 0.7700\n",
            "Epoch 87/300\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.41541\n",
            "620/620 - 0s - loss: 0.4013 - acc: 0.8371 - val_loss: 0.4379 - val_acc: 0.7800\n",
            "Epoch 88/300\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.41541\n",
            "620/620 - 0s - loss: 0.3946 - acc: 0.8129 - val_loss: 0.4376 - val_acc: 0.8000\n",
            "Epoch 89/300\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.41541\n",
            "620/620 - 0s - loss: 0.4265 - acc: 0.8274 - val_loss: 0.5856 - val_acc: 0.7400\n",
            "Epoch 90/300\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.41541\n",
            "620/620 - 0s - loss: 0.4086 - acc: 0.8097 - val_loss: 0.4875 - val_acc: 0.7700\n",
            "Epoch 91/300\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.41541\n",
            "620/620 - 0s - loss: 0.3940 - acc: 0.8210 - val_loss: 0.6373 - val_acc: 0.6900\n",
            "Epoch 92/300\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.41541\n",
            "620/620 - 0s - loss: 0.3827 - acc: 0.8290 - val_loss: 0.4297 - val_acc: 0.7900\n",
            "Epoch 93/300\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.41541\n",
            "620/620 - 0s - loss: 0.4024 - acc: 0.8290 - val_loss: 0.4534 - val_acc: 0.7900\n",
            "Epoch 94/300\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.41541\n",
            "620/620 - 0s - loss: 0.4148 - acc: 0.8194 - val_loss: 0.4498 - val_acc: 0.7600\n",
            "Epoch 95/300\n",
            "\n",
            "Epoch 00095: val_loss improved from 0.41541 to 0.40965, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.3905 - acc: 0.8435 - val_loss: 0.4096 - val_acc: 0.7900\n",
            "Epoch 96/300\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.40965\n",
            "620/620 - 0s - loss: 0.4007 - acc: 0.8065 - val_loss: 0.4413 - val_acc: 0.8000\n",
            "Epoch 97/300\n",
            "\n",
            "Epoch 00097: val_loss improved from 0.40965 to 0.40672, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.4355 - acc: 0.7935 - val_loss: 0.4067 - val_acc: 0.8000\n",
            "Epoch 98/300\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.40672\n",
            "620/620 - 0s - loss: 0.4085 - acc: 0.8274 - val_loss: 0.4887 - val_acc: 0.7600\n",
            "Epoch 99/300\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.40672\n",
            "620/620 - 0s - loss: 0.3888 - acc: 0.8242 - val_loss: 0.5243 - val_acc: 0.7700\n",
            "Epoch 100/300\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.40672\n",
            "620/620 - 0s - loss: 0.3888 - acc: 0.8290 - val_loss: 0.4388 - val_acc: 0.7500\n",
            "Epoch 101/300\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.40672\n",
            "620/620 - 0s - loss: 0.3787 - acc: 0.8161 - val_loss: 0.4942 - val_acc: 0.7600\n",
            "Epoch 102/300\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.40672\n",
            "620/620 - 0s - loss: 0.3983 - acc: 0.8274 - val_loss: 0.4344 - val_acc: 0.8000\n",
            "Epoch 103/300\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.40672\n",
            "620/620 - 0s - loss: 0.3951 - acc: 0.8161 - val_loss: 0.5109 - val_acc: 0.7500\n",
            "Epoch 104/300\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.40672\n",
            "620/620 - 0s - loss: 0.3979 - acc: 0.8274 - val_loss: 0.4758 - val_acc: 0.7800\n",
            "Epoch 105/300\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 0.40672\n",
            "620/620 - 0s - loss: 0.3878 - acc: 0.8290 - val_loss: 0.4131 - val_acc: 0.7900\n",
            "Epoch 106/300\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 0.40672\n",
            "620/620 - 0s - loss: 0.3921 - acc: 0.8194 - val_loss: 0.4109 - val_acc: 0.8000\n",
            "Epoch 107/300\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.40672\n",
            "620/620 - 0s - loss: 0.3786 - acc: 0.8226 - val_loss: 0.4580 - val_acc: 0.8000\n",
            "Epoch 108/300\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 0.40672\n",
            "620/620 - 0s - loss: 0.3881 - acc: 0.8387 - val_loss: 0.4934 - val_acc: 0.7800\n",
            "Epoch 109/300\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.40672\n",
            "620/620 - 1s - loss: 0.3849 - acc: 0.8210 - val_loss: 0.4823 - val_acc: 0.7600\n",
            "Epoch 110/300\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.40672\n",
            "620/620 - 0s - loss: 0.4105 - acc: 0.8097 - val_loss: 0.4501 - val_acc: 0.7700\n",
            "Epoch 111/300\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 0.40672\n",
            "620/620 - 0s - loss: 0.3898 - acc: 0.8242 - val_loss: 0.4321 - val_acc: 0.7800\n",
            "Epoch 112/300\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 0.40672\n",
            "620/620 - 0s - loss: 0.3817 - acc: 0.8274 - val_loss: 0.4975 - val_acc: 0.7600\n",
            "Epoch 113/300\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.40672\n",
            "620/620 - 0s - loss: 0.4174 - acc: 0.8081 - val_loss: 0.5324 - val_acc: 0.7400\n",
            "Epoch 114/300\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.40672\n",
            "620/620 - 0s - loss: 0.3970 - acc: 0.8097 - val_loss: 0.4709 - val_acc: 0.8100\n",
            "Epoch 115/300\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.40672\n",
            "620/620 - 0s - loss: 0.3904 - acc: 0.8258 - val_loss: 0.5321 - val_acc: 0.7500\n",
            "Epoch 116/300\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.40672\n",
            "620/620 - 0s - loss: 0.3879 - acc: 0.8097 - val_loss: 0.5243 - val_acc: 0.7600\n",
            "Epoch 117/300\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.40672\n",
            "620/620 - 0s - loss: 0.3975 - acc: 0.8339 - val_loss: 0.5064 - val_acc: 0.7600\n",
            "Epoch 118/300\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.40672\n",
            "620/620 - 0s - loss: 0.3576 - acc: 0.8468 - val_loss: 0.5000 - val_acc: 0.7600\n",
            "Epoch 119/300\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 0.40672\n",
            "620/620 - 0s - loss: 0.3830 - acc: 0.8258 - val_loss: 0.4875 - val_acc: 0.7700\n",
            "Epoch 120/300\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.40672\n",
            "620/620 - 0s - loss: 0.3959 - acc: 0.8242 - val_loss: 0.4156 - val_acc: 0.7300\n",
            "Epoch 121/300\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.40672\n",
            "620/620 - 0s - loss: 0.3818 - acc: 0.8355 - val_loss: 0.4304 - val_acc: 0.7900\n",
            "Epoch 122/300\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.40672\n",
            "620/620 - 0s - loss: 0.3832 - acc: 0.8194 - val_loss: 0.4677 - val_acc: 0.7700\n",
            "Epoch 123/300\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 0.40672\n",
            "620/620 - 0s - loss: 0.3831 - acc: 0.8081 - val_loss: 0.4999 - val_acc: 0.7600\n",
            "Epoch 124/300\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 0.40672\n",
            "620/620 - 0s - loss: 0.4072 - acc: 0.7968 - val_loss: 0.5026 - val_acc: 0.7400\n",
            "Epoch 125/300\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 0.40672\n",
            "620/620 - 0s - loss: 0.3934 - acc: 0.8355 - val_loss: 0.4426 - val_acc: 0.7700\n",
            "Epoch 126/300\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 0.40672\n",
            "620/620 - 0s - loss: 0.3874 - acc: 0.8210 - val_loss: 0.4616 - val_acc: 0.7300\n",
            "Epoch 127/300\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 0.40672\n",
            "620/620 - 0s - loss: 0.4064 - acc: 0.8000 - val_loss: 0.4639 - val_acc: 0.7700\n",
            "Epoch 128/300\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 0.40672\n",
            "620/620 - 0s - loss: 0.4205 - acc: 0.7887 - val_loss: 0.4826 - val_acc: 0.7800\n",
            "Epoch 129/300\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.40672\n",
            "620/620 - 0s - loss: 0.3551 - acc: 0.8323 - val_loss: 0.4161 - val_acc: 0.8100\n",
            "Epoch 130/300\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 0.40672\n",
            "620/620 - 0s - loss: 0.3541 - acc: 0.8387 - val_loss: 0.4320 - val_acc: 0.7700\n",
            "Epoch 131/300\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 0.40672\n",
            "620/620 - 0s - loss: 0.3630 - acc: 0.8435 - val_loss: 0.4784 - val_acc: 0.7800\n",
            "Epoch 132/300\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 0.40672\n",
            "620/620 - 0s - loss: 0.3777 - acc: 0.8306 - val_loss: 0.4860 - val_acc: 0.7700\n",
            "Epoch 133/300\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.40672\n",
            "620/620 - 0s - loss: 0.3757 - acc: 0.8161 - val_loss: 0.4562 - val_acc: 0.7200\n",
            "Epoch 134/300\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.40672\n",
            "620/620 - 0s - loss: 0.3924 - acc: 0.8242 - val_loss: 0.4264 - val_acc: 0.7500\n",
            "Epoch 135/300\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 0.40672\n",
            "620/620 - 0s - loss: 0.4003 - acc: 0.8226 - val_loss: 0.5149 - val_acc: 0.7500\n",
            "Epoch 136/300\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.40672\n",
            "620/620 - 0s - loss: 0.3708 - acc: 0.8403 - val_loss: 0.5583 - val_acc: 0.7700\n",
            "Epoch 137/300\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 0.40672\n",
            "620/620 - 0s - loss: 0.3820 - acc: 0.8452 - val_loss: 0.4986 - val_acc: 0.7400\n",
            "Epoch 138/300\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.40672\n",
            "620/620 - 0s - loss: 0.3697 - acc: 0.8468 - val_loss: 0.5214 - val_acc: 0.7700\n",
            "Epoch 139/300\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.40672\n",
            "620/620 - 0s - loss: 0.3940 - acc: 0.8161 - val_loss: 0.4907 - val_acc: 0.7900\n",
            "Epoch 140/300\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 0.40672\n",
            "620/620 - 0s - loss: 0.3925 - acc: 0.8371 - val_loss: 0.4970 - val_acc: 0.7500\n",
            "Epoch 141/300\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.40672\n",
            "620/620 - 1s - loss: 0.3751 - acc: 0.8290 - val_loss: 0.5183 - val_acc: 0.7600\n",
            "Epoch 142/300\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.40672\n",
            "620/620 - 0s - loss: 0.4000 - acc: 0.8323 - val_loss: 0.4730 - val_acc: 0.7300\n",
            "Epoch 143/300\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 0.40672\n",
            "620/620 - 0s - loss: 0.4034 - acc: 0.8129 - val_loss: 0.4739 - val_acc: 0.7800\n",
            "Epoch 144/300\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 0.40672\n",
            "620/620 - 0s - loss: 0.3563 - acc: 0.8452 - val_loss: 0.4686 - val_acc: 0.7600\n",
            "Epoch 145/300\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.40672\n",
            "620/620 - 0s - loss: 0.3667 - acc: 0.8306 - val_loss: 0.4317 - val_acc: 0.8000\n",
            "Epoch 146/300\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.40672\n",
            "620/620 - 0s - loss: 0.3552 - acc: 0.8323 - val_loss: 0.4630 - val_acc: 0.7600\n",
            "Epoch 147/300\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.40672\n",
            "620/620 - 0s - loss: 0.4038 - acc: 0.8145 - val_loss: 0.4947 - val_acc: 0.7700\n",
            "Epoch 148/300\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 0.40672\n",
            "620/620 - 0s - loss: 0.3805 - acc: 0.8290 - val_loss: 0.4847 - val_acc: 0.7700\n",
            "Epoch 149/300\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.40672\n",
            "620/620 - 0s - loss: 0.4017 - acc: 0.8000 - val_loss: 0.4881 - val_acc: 0.7600\n",
            "Epoch 150/300\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.40672\n",
            "620/620 - 0s - loss: 0.3915 - acc: 0.8081 - val_loss: 0.4537 - val_acc: 0.7700\n",
            "Epoch 151/300\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.40672\n",
            "620/620 - 0s - loss: 0.3921 - acc: 0.8306 - val_loss: 0.5284 - val_acc: 0.7800\n",
            "Epoch 152/300\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.40672\n",
            "620/620 - 0s - loss: 0.3938 - acc: 0.8145 - val_loss: 0.4763 - val_acc: 0.7600\n",
            "Epoch 153/300\n",
            "\n",
            "Epoch 00153: val_loss improved from 0.40672 to 0.40226, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.3673 - acc: 0.8242 - val_loss: 0.4023 - val_acc: 0.7700\n",
            "Epoch 154/300\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.40226\n",
            "620/620 - 0s - loss: 0.3713 - acc: 0.8274 - val_loss: 0.4350 - val_acc: 0.8000\n",
            "Epoch 155/300\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.40226\n",
            "620/620 - 0s - loss: 0.3745 - acc: 0.8210 - val_loss: 0.4532 - val_acc: 0.7600\n",
            "Epoch 156/300\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.40226\n",
            "620/620 - 0s - loss: 0.3822 - acc: 0.8323 - val_loss: 0.4185 - val_acc: 0.7600\n",
            "Epoch 157/300\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.40226\n",
            "620/620 - 0s - loss: 0.3727 - acc: 0.8323 - val_loss: 0.5673 - val_acc: 0.7600\n",
            "Epoch 158/300\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.40226\n",
            "620/620 - 0s - loss: 0.3969 - acc: 0.8129 - val_loss: 0.4166 - val_acc: 0.7900\n",
            "Epoch 159/300\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.40226\n",
            "620/620 - 0s - loss: 0.3839 - acc: 0.8306 - val_loss: 0.4391 - val_acc: 0.7800\n",
            "Epoch 160/300\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.40226\n",
            "620/620 - 0s - loss: 0.3952 - acc: 0.8242 - val_loss: 0.4302 - val_acc: 0.7600\n",
            "Epoch 161/300\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.40226\n",
            "620/620 - 0s - loss: 0.3647 - acc: 0.8339 - val_loss: 0.4947 - val_acc: 0.7800\n",
            "Epoch 162/300\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.40226\n",
            "620/620 - 0s - loss: 0.3869 - acc: 0.8290 - val_loss: 0.4623 - val_acc: 0.7900\n",
            "Epoch 163/300\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.40226\n",
            "620/620 - 0s - loss: 0.3849 - acc: 0.8371 - val_loss: 0.4673 - val_acc: 0.7900\n",
            "Epoch 164/300\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.40226\n",
            "620/620 - 0s - loss: 0.3535 - acc: 0.8387 - val_loss: 0.5580 - val_acc: 0.7800\n",
            "Epoch 165/300\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.40226\n",
            "620/620 - 0s - loss: 0.3979 - acc: 0.8226 - val_loss: 0.4471 - val_acc: 0.7600\n",
            "Epoch 166/300\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 0.40226\n",
            "620/620 - 0s - loss: 0.3679 - acc: 0.8403 - val_loss: 0.4066 - val_acc: 0.7700\n",
            "Epoch 167/300\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.40226\n",
            "620/620 - 0s - loss: 0.3697 - acc: 0.8387 - val_loss: 0.4663 - val_acc: 0.7400\n",
            "Epoch 168/300\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.40226\n",
            "620/620 - 0s - loss: 0.3994 - acc: 0.8129 - val_loss: 0.6046 - val_acc: 0.7500\n",
            "Epoch 169/300\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 0.40226\n",
            "620/620 - 0s - loss: 0.3526 - acc: 0.8452 - val_loss: 0.6612 - val_acc: 0.7400\n",
            "Epoch 170/300\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.40226\n",
            "620/620 - 0s - loss: 0.3525 - acc: 0.8500 - val_loss: 0.4545 - val_acc: 0.7800\n",
            "Epoch 171/300\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.40226\n",
            "620/620 - 0s - loss: 0.3771 - acc: 0.8339 - val_loss: 0.4267 - val_acc: 0.7700\n",
            "Epoch 172/300\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.40226\n",
            "620/620 - 0s - loss: 0.3623 - acc: 0.8355 - val_loss: 0.5078 - val_acc: 0.7800\n",
            "Epoch 173/300\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.40226\n",
            "620/620 - 0s - loss: 0.3389 - acc: 0.8645 - val_loss: 0.4727 - val_acc: 0.7800\n",
            "Epoch 174/300\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.40226\n",
            "620/620 - 0s - loss: 0.4002 - acc: 0.8081 - val_loss: 0.5517 - val_acc: 0.7800\n",
            "Epoch 175/300\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.40226\n",
            "620/620 - 0s - loss: 0.3793 - acc: 0.8258 - val_loss: 0.4482 - val_acc: 0.7700\n",
            "Epoch 176/300\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 0.40226\n",
            "620/620 - 0s - loss: 0.3828 - acc: 0.8161 - val_loss: 0.4176 - val_acc: 0.7600\n",
            "Epoch 177/300\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.40226\n",
            "620/620 - 0s - loss: 0.4085 - acc: 0.8129 - val_loss: 0.4584 - val_acc: 0.7500\n",
            "Epoch 178/300\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.40226\n",
            "620/620 - 0s - loss: 0.3663 - acc: 0.8323 - val_loss: 0.5232 - val_acc: 0.7500\n",
            "Epoch 179/300\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.40226\n",
            "620/620 - 0s - loss: 0.3700 - acc: 0.8339 - val_loss: 0.4534 - val_acc: 0.7900\n",
            "Epoch 180/300\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.40226\n",
            "620/620 - 0s - loss: 0.3656 - acc: 0.8452 - val_loss: 0.4421 - val_acc: 0.7200\n",
            "Epoch 181/300\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.40226\n",
            "620/620 - 0s - loss: 0.3996 - acc: 0.8403 - val_loss: 0.4730 - val_acc: 0.7100\n",
            "Epoch 182/300\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.40226\n",
            "620/620 - 0s - loss: 0.3521 - acc: 0.8500 - val_loss: 0.4553 - val_acc: 0.7500\n",
            "Epoch 183/300\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.40226\n",
            "620/620 - 0s - loss: 0.3720 - acc: 0.8419 - val_loss: 0.4781 - val_acc: 0.7700\n",
            "Epoch 184/300\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 0.40226\n",
            "620/620 - 0s - loss: 0.3605 - acc: 0.8387 - val_loss: 0.5065 - val_acc: 0.7600\n",
            "Epoch 185/300\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.40226\n",
            "620/620 - 0s - loss: 0.3503 - acc: 0.8468 - val_loss: 0.4551 - val_acc: 0.7500\n",
            "Epoch 186/300\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.40226\n",
            "620/620 - 0s - loss: 0.3697 - acc: 0.8226 - val_loss: 0.4547 - val_acc: 0.7400\n",
            "Epoch 187/300\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 0.40226\n",
            "620/620 - 0s - loss: 0.3650 - acc: 0.8403 - val_loss: 0.4626 - val_acc: 0.7700\n",
            "Epoch 188/300\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.40226\n",
            "620/620 - 0s - loss: 0.3783 - acc: 0.8403 - val_loss: 0.4920 - val_acc: 0.7600\n",
            "Epoch 189/300\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.40226\n",
            "620/620 - 0s - loss: 0.3524 - acc: 0.8468 - val_loss: 0.5131 - val_acc: 0.7500\n",
            "Epoch 190/300\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.40226\n",
            "620/620 - 0s - loss: 0.3430 - acc: 0.8532 - val_loss: 0.4609 - val_acc: 0.7700\n",
            "Epoch 191/300\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.40226\n",
            "620/620 - 0s - loss: 0.3739 - acc: 0.8403 - val_loss: 0.4974 - val_acc: 0.7900\n",
            "Epoch 192/300\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 0.40226\n",
            "620/620 - 0s - loss: 0.3693 - acc: 0.8500 - val_loss: 0.5083 - val_acc: 0.7800\n",
            "Epoch 193/300\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.40226\n",
            "620/620 - 0s - loss: 0.3863 - acc: 0.8129 - val_loss: 0.4954 - val_acc: 0.7900\n",
            "Epoch 194/300\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.40226\n",
            "620/620 - 0s - loss: 0.3611 - acc: 0.8387 - val_loss: 0.5506 - val_acc: 0.8000\n",
            "Epoch 195/300\n",
            "\n",
            "Epoch 00195: val_loss improved from 0.40226 to 0.39822, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.3818 - acc: 0.8387 - val_loss: 0.3982 - val_acc: 0.7900\n",
            "Epoch 196/300\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.39822\n",
            "620/620 - 0s - loss: 0.3742 - acc: 0.8226 - val_loss: 0.4496 - val_acc: 0.7600\n",
            "Epoch 197/300\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.39822\n",
            "620/620 - 0s - loss: 0.3705 - acc: 0.8290 - val_loss: 0.4544 - val_acc: 0.7600\n",
            "Epoch 198/300\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.39822\n",
            "620/620 - 0s - loss: 0.3623 - acc: 0.8403 - val_loss: 0.4349 - val_acc: 0.7700\n",
            "Epoch 199/300\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.39822\n",
            "620/620 - 0s - loss: 0.3940 - acc: 0.8161 - val_loss: 0.4359 - val_acc: 0.7800\n",
            "Epoch 200/300\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.39822\n",
            "620/620 - 0s - loss: 0.3767 - acc: 0.8258 - val_loss: 0.4523 - val_acc: 0.7800\n",
            "Epoch 201/300\n",
            "\n",
            "Epoch 00201: val_loss did not improve from 0.39822\n",
            "620/620 - 0s - loss: 0.3480 - acc: 0.8435 - val_loss: 0.5029 - val_acc: 0.7200\n",
            "Epoch 202/300\n",
            "\n",
            "Epoch 00202: val_loss did not improve from 0.39822\n",
            "620/620 - 0s - loss: 0.3643 - acc: 0.8258 - val_loss: 0.6252 - val_acc: 0.7300\n",
            "Epoch 203/300\n",
            "\n",
            "Epoch 00203: val_loss did not improve from 0.39822\n",
            "620/620 - 0s - loss: 0.3609 - acc: 0.8371 - val_loss: 0.6729 - val_acc: 0.7300\n",
            "Epoch 204/300\n",
            "\n",
            "Epoch 00204: val_loss did not improve from 0.39822\n",
            "620/620 - 0s - loss: 0.3398 - acc: 0.8565 - val_loss: 0.4698 - val_acc: 0.7300\n",
            "Epoch 205/300\n",
            "\n",
            "Epoch 00205: val_loss did not improve from 0.39822\n",
            "620/620 - 0s - loss: 0.3483 - acc: 0.8371 - val_loss: 0.4716 - val_acc: 0.7900\n",
            "Epoch 206/300\n",
            "\n",
            "Epoch 00206: val_loss did not improve from 0.39822\n",
            "620/620 - 0s - loss: 0.3659 - acc: 0.8387 - val_loss: 0.5113 - val_acc: 0.7700\n",
            "Epoch 207/300\n",
            "\n",
            "Epoch 00207: val_loss did not improve from 0.39822\n",
            "620/620 - 0s - loss: 0.3582 - acc: 0.8435 - val_loss: 0.5037 - val_acc: 0.7600\n",
            "Epoch 208/300\n",
            "\n",
            "Epoch 00208: val_loss did not improve from 0.39822\n",
            "620/620 - 0s - loss: 0.3863 - acc: 0.8242 - val_loss: 0.5572 - val_acc: 0.7400\n",
            "Epoch 209/300\n",
            "\n",
            "Epoch 00209: val_loss did not improve from 0.39822\n",
            "620/620 - 0s - loss: 0.3650 - acc: 0.8339 - val_loss: 0.5400 - val_acc: 0.8000\n",
            "Epoch 210/300\n",
            "\n",
            "Epoch 00210: val_loss did not improve from 0.39822\n",
            "620/620 - 0s - loss: 0.3567 - acc: 0.8403 - val_loss: 0.4721 - val_acc: 0.7400\n",
            "Epoch 211/300\n",
            "\n",
            "Epoch 00211: val_loss did not improve from 0.39822\n",
            "620/620 - 0s - loss: 0.3529 - acc: 0.8516 - val_loss: 0.5465 - val_acc: 0.7600\n",
            "Epoch 212/300\n",
            "\n",
            "Epoch 00212: val_loss did not improve from 0.39822\n",
            "620/620 - 0s - loss: 0.3438 - acc: 0.8484 - val_loss: 0.5188 - val_acc: 0.7500\n",
            "Epoch 213/300\n",
            "\n",
            "Epoch 00213: val_loss did not improve from 0.39822\n",
            "620/620 - 0s - loss: 0.3645 - acc: 0.8371 - val_loss: 0.4664 - val_acc: 0.7600\n",
            "Epoch 214/300\n",
            "\n",
            "Epoch 00214: val_loss did not improve from 0.39822\n",
            "620/620 - 0s - loss: 0.3484 - acc: 0.8500 - val_loss: 0.4345 - val_acc: 0.7800\n",
            "Epoch 215/300\n",
            "\n",
            "Epoch 00215: val_loss did not improve from 0.39822\n",
            "620/620 - 0s - loss: 0.3654 - acc: 0.8532 - val_loss: 0.4830 - val_acc: 0.7600\n",
            "Epoch 216/300\n",
            "\n",
            "Epoch 00216: val_loss did not improve from 0.39822\n",
            "620/620 - 0s - loss: 0.3712 - acc: 0.8484 - val_loss: 0.5332 - val_acc: 0.7600\n",
            "Epoch 217/300\n",
            "\n",
            "Epoch 00217: val_loss did not improve from 0.39822\n",
            "620/620 - 0s - loss: 0.3675 - acc: 0.8419 - val_loss: 0.5250 - val_acc: 0.7600\n",
            "Epoch 218/300\n",
            "\n",
            "Epoch 00218: val_loss did not improve from 0.39822\n",
            "620/620 - 0s - loss: 0.3650 - acc: 0.8306 - val_loss: 0.4336 - val_acc: 0.7800\n",
            "Epoch 219/300\n",
            "\n",
            "Epoch 00219: val_loss did not improve from 0.39822\n",
            "620/620 - 0s - loss: 0.3433 - acc: 0.8532 - val_loss: 0.5122 - val_acc: 0.7900\n",
            "Epoch 220/300\n",
            "\n",
            "Epoch 00220: val_loss did not improve from 0.39822\n",
            "620/620 - 0s - loss: 0.3715 - acc: 0.8323 - val_loss: 0.4943 - val_acc: 0.7800\n",
            "Epoch 221/300\n",
            "\n",
            "Epoch 00221: val_loss did not improve from 0.39822\n",
            "620/620 - 0s - loss: 0.3821 - acc: 0.8290 - val_loss: 0.5730 - val_acc: 0.7400\n",
            "Epoch 222/300\n",
            "\n",
            "Epoch 00222: val_loss did not improve from 0.39822\n",
            "620/620 - 0s - loss: 0.4061 - acc: 0.8065 - val_loss: 0.4771 - val_acc: 0.7200\n",
            "Epoch 223/300\n",
            "\n",
            "Epoch 00223: val_loss did not improve from 0.39822\n",
            "620/620 - 0s - loss: 0.3525 - acc: 0.8274 - val_loss: 0.5557 - val_acc: 0.7800\n",
            "Epoch 224/300\n",
            "\n",
            "Epoch 00224: val_loss did not improve from 0.39822\n",
            "620/620 - 0s - loss: 0.3447 - acc: 0.8516 - val_loss: 0.4508 - val_acc: 0.7900\n",
            "Epoch 225/300\n",
            "\n",
            "Epoch 00225: val_loss did not improve from 0.39822\n",
            "620/620 - 0s - loss: 0.3475 - acc: 0.8532 - val_loss: 0.4918 - val_acc: 0.7800\n",
            "Epoch 226/300\n",
            "\n",
            "Epoch 00226: val_loss did not improve from 0.39822\n",
            "620/620 - 0s - loss: 0.3604 - acc: 0.8306 - val_loss: 0.5612 - val_acc: 0.7800\n",
            "Epoch 227/300\n",
            "\n",
            "Epoch 00227: val_loss did not improve from 0.39822\n",
            "620/620 - 0s - loss: 0.3620 - acc: 0.8468 - val_loss: 0.6230 - val_acc: 0.7300\n",
            "Epoch 228/300\n",
            "\n",
            "Epoch 00228: val_loss did not improve from 0.39822\n",
            "620/620 - 0s - loss: 0.3719 - acc: 0.8274 - val_loss: 0.4825 - val_acc: 0.7800\n",
            "Epoch 229/300\n",
            "\n",
            "Epoch 00229: val_loss did not improve from 0.39822\n",
            "620/620 - 0s - loss: 0.3675 - acc: 0.8323 - val_loss: 0.6488 - val_acc: 0.7100\n",
            "Epoch 230/300\n",
            "\n",
            "Epoch 00230: val_loss did not improve from 0.39822\n",
            "620/620 - 0s - loss: 0.3613 - acc: 0.8258 - val_loss: 0.5228 - val_acc: 0.7600\n",
            "Epoch 231/300\n",
            "\n",
            "Epoch 00231: val_loss did not improve from 0.39822\n",
            "620/620 - 0s - loss: 0.3431 - acc: 0.8565 - val_loss: 0.5688 - val_acc: 0.7400\n",
            "Epoch 232/300\n",
            "\n",
            "Epoch 00232: val_loss did not improve from 0.39822\n",
            "620/620 - 0s - loss: 0.3550 - acc: 0.8355 - val_loss: 0.4570 - val_acc: 0.7600\n",
            "Epoch 233/300\n",
            "\n",
            "Epoch 00233: val_loss did not improve from 0.39822\n",
            "620/620 - 0s - loss: 0.3525 - acc: 0.8371 - val_loss: 0.4554 - val_acc: 0.7800\n",
            "Epoch 234/300\n",
            "\n",
            "Epoch 00234: val_loss did not improve from 0.39822\n",
            "620/620 - 0s - loss: 0.3849 - acc: 0.8290 - val_loss: 0.4421 - val_acc: 0.7600\n",
            "Epoch 235/300\n",
            "\n",
            "Epoch 00235: val_loss did not improve from 0.39822\n",
            "620/620 - 0s - loss: 0.3626 - acc: 0.8452 - val_loss: 0.5644 - val_acc: 0.7700\n",
            "Epoch 236/300\n",
            "\n",
            "Epoch 00236: val_loss did not improve from 0.39822\n",
            "620/620 - 0s - loss: 0.3580 - acc: 0.8435 - val_loss: 0.5067 - val_acc: 0.7700\n",
            "Epoch 237/300\n",
            "\n",
            "Epoch 00237: val_loss did not improve from 0.39822\n",
            "620/620 - 0s - loss: 0.3603 - acc: 0.8290 - val_loss: 0.4657 - val_acc: 0.7400\n",
            "Epoch 238/300\n",
            "\n",
            "Epoch 00238: val_loss did not improve from 0.39822\n",
            "620/620 - 0s - loss: 0.3339 - acc: 0.8548 - val_loss: 0.5080 - val_acc: 0.7500\n",
            "Epoch 239/300\n",
            "\n",
            "Epoch 00239: val_loss did not improve from 0.39822\n",
            "620/620 - 0s - loss: 0.3428 - acc: 0.8532 - val_loss: 0.5345 - val_acc: 0.7500\n",
            "Epoch 240/300\n",
            "\n",
            "Epoch 00240: val_loss did not improve from 0.39822\n",
            "620/620 - 0s - loss: 0.3461 - acc: 0.8468 - val_loss: 0.5060 - val_acc: 0.7500\n",
            "Epoch 241/300\n",
            "\n",
            "Epoch 00241: val_loss did not improve from 0.39822\n",
            "620/620 - 0s - loss: 0.3616 - acc: 0.8226 - val_loss: 0.6026 - val_acc: 0.7200\n",
            "Epoch 242/300\n",
            "\n",
            "Epoch 00242: val_loss did not improve from 0.39822\n",
            "620/620 - 0s - loss: 0.3706 - acc: 0.8274 - val_loss: 0.6170 - val_acc: 0.7900\n",
            "Epoch 243/300\n",
            "\n",
            "Epoch 00243: val_loss did not improve from 0.39822\n",
            "620/620 - 0s - loss: 0.3807 - acc: 0.8242 - val_loss: 0.4996 - val_acc: 0.7700\n",
            "Epoch 244/300\n",
            "\n",
            "Epoch 00244: val_loss did not improve from 0.39822\n",
            "620/620 - 0s - loss: 0.3958 - acc: 0.8032 - val_loss: 0.4558 - val_acc: 0.7700\n",
            "Epoch 245/300\n",
            "\n",
            "Epoch 00245: val_loss did not improve from 0.39822\n",
            "620/620 - 0s - loss: 0.3583 - acc: 0.8403 - val_loss: 0.4831 - val_acc: 0.7900\n",
            "Epoch 246/300\n",
            "\n",
            "Epoch 00246: val_loss did not improve from 0.39822\n",
            "620/620 - 0s - loss: 0.3534 - acc: 0.8581 - val_loss: 0.5755 - val_acc: 0.7700\n",
            "Epoch 247/300\n",
            "\n",
            "Epoch 00247: val_loss did not improve from 0.39822\n",
            "620/620 - 0s - loss: 0.3602 - acc: 0.8452 - val_loss: 0.5102 - val_acc: 0.7300\n",
            "Epoch 248/300\n",
            "\n",
            "Epoch 00248: val_loss did not improve from 0.39822\n",
            "620/620 - 0s - loss: 0.3984 - acc: 0.8048 - val_loss: 0.5516 - val_acc: 0.7600\n",
            "Epoch 249/300\n",
            "\n",
            "Epoch 00249: val_loss did not improve from 0.39822\n",
            "620/620 - 0s - loss: 0.3343 - acc: 0.8500 - val_loss: 0.5175 - val_acc: 0.7900\n",
            "Epoch 250/300\n",
            "\n",
            "Epoch 00250: val_loss did not improve from 0.39822\n",
            "620/620 - 0s - loss: 0.3398 - acc: 0.8548 - val_loss: 0.5608 - val_acc: 0.7400\n",
            "Epoch 251/300\n",
            "\n",
            "Epoch 00251: val_loss did not improve from 0.39822\n",
            "620/620 - 0s - loss: 0.3523 - acc: 0.8484 - val_loss: 0.5892 - val_acc: 0.7700\n",
            "Epoch 252/300\n",
            "\n",
            "Epoch 00252: val_loss did not improve from 0.39822\n",
            "620/620 - 0s - loss: 0.3524 - acc: 0.8323 - val_loss: 0.5318 - val_acc: 0.8000\n",
            "Epoch 253/300\n",
            "\n",
            "Epoch 00253: val_loss did not improve from 0.39822\n",
            "620/620 - 0s - loss: 0.3502 - acc: 0.8387 - val_loss: 0.5176 - val_acc: 0.7900\n",
            "Epoch 254/300\n",
            "\n",
            "Epoch 00254: val_loss did not improve from 0.39822\n",
            "620/620 - 0s - loss: 0.3556 - acc: 0.8387 - val_loss: 0.5174 - val_acc: 0.7500\n",
            "Epoch 255/300\n",
            "\n",
            "Epoch 00255: val_loss did not improve from 0.39822\n",
            "620/620 - 0s - loss: 0.3321 - acc: 0.8677 - val_loss: 0.4966 - val_acc: 0.7600\n",
            "Epoch 256/300\n",
            "\n",
            "Epoch 00256: val_loss did not improve from 0.39822\n",
            "620/620 - 0s - loss: 0.3735 - acc: 0.8161 - val_loss: 0.5539 - val_acc: 0.7600\n",
            "Epoch 257/300\n",
            "\n",
            "Epoch 00257: val_loss did not improve from 0.39822\n",
            "620/620 - 0s - loss: 0.3447 - acc: 0.8565 - val_loss: 0.5613 - val_acc: 0.7700\n",
            "Epoch 258/300\n",
            "\n",
            "Epoch 00258: val_loss did not improve from 0.39822\n",
            "620/620 - 0s - loss: 0.3647 - acc: 0.8226 - val_loss: 0.5930 - val_acc: 0.7600\n",
            "Epoch 259/300\n",
            "\n",
            "Epoch 00259: val_loss did not improve from 0.39822\n",
            "620/620 - 0s - loss: 0.3649 - acc: 0.8468 - val_loss: 0.5410 - val_acc: 0.7700\n",
            "Epoch 260/300\n",
            "\n",
            "Epoch 00260: val_loss did not improve from 0.39822\n",
            "620/620 - 0s - loss: 0.3630 - acc: 0.8323 - val_loss: 0.4781 - val_acc: 0.7800\n",
            "Epoch 261/300\n",
            "\n",
            "Epoch 00261: val_loss did not improve from 0.39822\n",
            "620/620 - 0s - loss: 0.3276 - acc: 0.8484 - val_loss: 0.4390 - val_acc: 0.7700\n",
            "Epoch 262/300\n",
            "\n",
            "Epoch 00262: val_loss did not improve from 0.39822\n",
            "620/620 - 0s - loss: 0.3850 - acc: 0.8226 - val_loss: 0.4679 - val_acc: 0.7600\n",
            "Epoch 263/300\n",
            "\n",
            "Epoch 00263: val_loss did not improve from 0.39822\n",
            "620/620 - 0s - loss: 0.3778 - acc: 0.8323 - val_loss: 0.4991 - val_acc: 0.7500\n",
            "Epoch 264/300\n",
            "\n",
            "Epoch 00264: val_loss did not improve from 0.39822\n",
            "620/620 - 0s - loss: 0.3481 - acc: 0.8323 - val_loss: 0.5492 - val_acc: 0.7100\n",
            "Epoch 265/300\n",
            "\n",
            "Epoch 00265: val_loss did not improve from 0.39822\n",
            "620/620 - 0s - loss: 0.3475 - acc: 0.8500 - val_loss: 0.5118 - val_acc: 0.7500\n",
            "Epoch 266/300\n",
            "\n",
            "Epoch 00266: val_loss did not improve from 0.39822\n",
            "620/620 - 0s - loss: 0.3453 - acc: 0.8500 - val_loss: 0.4866 - val_acc: 0.7500\n",
            "Epoch 267/300\n",
            "\n",
            "Epoch 00267: val_loss did not improve from 0.39822\n",
            "620/620 - 0s - loss: 0.3391 - acc: 0.8468 - val_loss: 0.5189 - val_acc: 0.7700\n",
            "Epoch 268/300\n",
            "\n",
            "Epoch 00268: val_loss did not improve from 0.39822\n",
            "620/620 - 0s - loss: 0.3853 - acc: 0.8323 - val_loss: 0.4611 - val_acc: 0.7700\n",
            "Epoch 269/300\n",
            "\n",
            "Epoch 00269: val_loss did not improve from 0.39822\n",
            "620/620 - 0s - loss: 0.3307 - acc: 0.8532 - val_loss: 0.5108 - val_acc: 0.7600\n",
            "Epoch 270/300\n",
            "\n",
            "Epoch 00270: val_loss did not improve from 0.39822\n",
            "620/620 - 0s - loss: 0.3597 - acc: 0.8403 - val_loss: 0.4575 - val_acc: 0.7900\n",
            "Epoch 271/300\n",
            "\n",
            "Epoch 00271: val_loss did not improve from 0.39822\n",
            "620/620 - 0s - loss: 0.3444 - acc: 0.8532 - val_loss: 0.4928 - val_acc: 0.7700\n",
            "Epoch 272/300\n",
            "\n",
            "Epoch 00272: val_loss did not improve from 0.39822\n",
            "620/620 - 0s - loss: 0.3838 - acc: 0.8032 - val_loss: 0.4659 - val_acc: 0.7800\n",
            "Epoch 273/300\n",
            "\n",
            "Epoch 00273: val_loss did not improve from 0.39822\n",
            "620/620 - 0s - loss: 0.3369 - acc: 0.8661 - val_loss: 0.5432 - val_acc: 0.7700\n",
            "Epoch 274/300\n",
            "\n",
            "Epoch 00274: val_loss did not improve from 0.39822\n",
            "620/620 - 0s - loss: 0.3752 - acc: 0.8242 - val_loss: 0.5876 - val_acc: 0.7500\n",
            "Epoch 275/300\n",
            "\n",
            "Epoch 00275: val_loss did not improve from 0.39822\n",
            "620/620 - 0s - loss: 0.3585 - acc: 0.8290 - val_loss: 0.6388 - val_acc: 0.7500\n",
            "Epoch 276/300\n",
            "\n",
            "Epoch 00276: val_loss did not improve from 0.39822\n",
            "620/620 - 0s - loss: 0.3718 - acc: 0.8371 - val_loss: 0.5952 - val_acc: 0.7600\n",
            "Epoch 277/300\n",
            "\n",
            "Epoch 00277: val_loss did not improve from 0.39822\n",
            "620/620 - 0s - loss: 0.3598 - acc: 0.8435 - val_loss: 0.5078 - val_acc: 0.7300\n",
            "Epoch 278/300\n",
            "\n",
            "Epoch 00278: val_loss did not improve from 0.39822\n",
            "620/620 - 0s - loss: 0.3475 - acc: 0.8452 - val_loss: 0.5135 - val_acc: 0.7600\n",
            "Epoch 279/300\n",
            "\n",
            "Epoch 00279: val_loss did not improve from 0.39822\n",
            "620/620 - 0s - loss: 0.3322 - acc: 0.8581 - val_loss: 0.5497 - val_acc: 0.7500\n",
            "Epoch 280/300\n",
            "\n",
            "Epoch 00280: val_loss did not improve from 0.39822\n",
            "620/620 - 0s - loss: 0.3355 - acc: 0.8532 - val_loss: 0.5273 - val_acc: 0.7700\n",
            "Epoch 281/300\n",
            "\n",
            "Epoch 00281: val_loss did not improve from 0.39822\n",
            "620/620 - 0s - loss: 0.3322 - acc: 0.8516 - val_loss: 0.5598 - val_acc: 0.7800\n",
            "Epoch 282/300\n",
            "\n",
            "Epoch 00282: val_loss did not improve from 0.39822\n",
            "620/620 - 0s - loss: 0.3489 - acc: 0.8516 - val_loss: 0.5725 - val_acc: 0.7600\n",
            "Epoch 283/300\n",
            "\n",
            "Epoch 00283: val_loss did not improve from 0.39822\n",
            "620/620 - 0s - loss: 0.3479 - acc: 0.8516 - val_loss: 0.4816 - val_acc: 0.7500\n",
            "Epoch 284/300\n",
            "\n",
            "Epoch 00284: val_loss did not improve from 0.39822\n",
            "620/620 - 0s - loss: 0.3733 - acc: 0.8355 - val_loss: 0.4986 - val_acc: 0.7800\n",
            "Epoch 285/300\n",
            "\n",
            "Epoch 00285: val_loss did not improve from 0.39822\n",
            "620/620 - 0s - loss: 0.3810 - acc: 0.8242 - val_loss: 0.4817 - val_acc: 0.7700\n",
            "Epoch 286/300\n",
            "\n",
            "Epoch 00286: val_loss did not improve from 0.39822\n",
            "620/620 - 0s - loss: 0.3487 - acc: 0.8403 - val_loss: 0.5517 - val_acc: 0.7700\n",
            "Epoch 287/300\n",
            "\n",
            "Epoch 00287: val_loss did not improve from 0.39822\n",
            "620/620 - 0s - loss: 0.3496 - acc: 0.8403 - val_loss: 0.4739 - val_acc: 0.7900\n",
            "Epoch 288/300\n",
            "\n",
            "Epoch 00288: val_loss did not improve from 0.39822\n",
            "620/620 - 0s - loss: 0.3334 - acc: 0.8516 - val_loss: 0.5194 - val_acc: 0.7600\n",
            "Epoch 289/300\n",
            "\n",
            "Epoch 00289: val_loss did not improve from 0.39822\n",
            "620/620 - 0s - loss: 0.3276 - acc: 0.8661 - val_loss: 0.5521 - val_acc: 0.7600\n",
            "Epoch 290/300\n",
            "\n",
            "Epoch 00290: val_loss did not improve from 0.39822\n",
            "620/620 - 0s - loss: 0.3902 - acc: 0.8194 - val_loss: 0.5597 - val_acc: 0.7500\n",
            "Epoch 291/300\n",
            "\n",
            "Epoch 00291: val_loss did not improve from 0.39822\n",
            "620/620 - 0s - loss: 0.3510 - acc: 0.8484 - val_loss: 0.5042 - val_acc: 0.7900\n",
            "Epoch 292/300\n",
            "\n",
            "Epoch 00292: val_loss did not improve from 0.39822\n",
            "620/620 - 0s - loss: 0.3577 - acc: 0.8387 - val_loss: 0.5073 - val_acc: 0.7700\n",
            "Epoch 293/300\n",
            "\n",
            "Epoch 00293: val_loss did not improve from 0.39822\n",
            "620/620 - 0s - loss: 0.3564 - acc: 0.8387 - val_loss: 0.5624 - val_acc: 0.7300\n",
            "Epoch 294/300\n",
            "\n",
            "Epoch 00294: val_loss did not improve from 0.39822\n",
            "620/620 - 0s - loss: 0.3219 - acc: 0.8581 - val_loss: 0.5055 - val_acc: 0.7400\n",
            "Epoch 295/300\n",
            "\n",
            "Epoch 00295: val_loss did not improve from 0.39822\n",
            "620/620 - 0s - loss: 0.3211 - acc: 0.8597 - val_loss: 0.5017 - val_acc: 0.7700\n",
            "Epoch 296/300\n",
            "\n",
            "Epoch 00296: val_loss did not improve from 0.39822\n",
            "620/620 - 0s - loss: 0.3944 - acc: 0.8129 - val_loss: 0.5135 - val_acc: 0.7800\n",
            "Epoch 297/300\n",
            "\n",
            "Epoch 00297: val_loss did not improve from 0.39822\n",
            "620/620 - 0s - loss: 0.3613 - acc: 0.8371 - val_loss: 0.5619 - val_acc: 0.7800\n",
            "Epoch 298/300\n",
            "\n",
            "Epoch 00298: val_loss did not improve from 0.39822\n",
            "620/620 - 0s - loss: 0.3512 - acc: 0.8355 - val_loss: 0.5752 - val_acc: 0.7400\n",
            "Epoch 299/300\n",
            "\n",
            "Epoch 00299: val_loss did not improve from 0.39822\n",
            "620/620 - 0s - loss: 0.3124 - acc: 0.8839 - val_loss: 0.5087 - val_acc: 0.7800\n",
            "Epoch 300/300\n",
            "\n",
            "Epoch 00300: val_loss did not improve from 0.39822\n",
            "620/620 - 0s - loss: 0.3624 - acc: 0.8226 - val_loss: 0.5524 - val_acc: 0.7400\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nP3AAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "# Evaluate on test data\n",
            "40/40 [==============================] - 0s 3ms/sample - loss: 0.5257 - acc: 0.7000\n",
            "test loss, test acc: [0.5257109666950782, 0.7]\n",
            "1\n",
            "EEG_Deep/Data2A/parsed_P01T.mat\n",
            "(0, 12, 4096)\n",
            "(80, 12, 4096)\n",
            "(80, 1)\n",
            "2\n",
            "EEG_Deep/Data2A/parsed_P02T.mat\n",
            "(80, 12, 4096)\n",
            "(160, 12, 4096)\n",
            "(160, 1)\n",
            "3\n",
            "EEG_Deep/Data2A/parsed_P03T.mat\n",
            "(160, 12, 4096)\n",
            "(240, 12, 4096)\n",
            "(240, 1)\n",
            "4\n",
            "EEG_Deep/Data2A/parsed_P04T.mat\n",
            "(240, 12, 4096)\n",
            "(320, 12, 4096)\n",
            "(320, 1)\n",
            "5\n",
            "EEG_Deep/Data2A/parsed_P05T.mat\n",
            "(320, 12, 4096)\n",
            "(400, 12, 4096)\n",
            "(400, 1)\n",
            "6\n",
            "EEG_Deep/Data2A/parsed_P06T.mat\n",
            "(400, 12, 4096)\n",
            "(480, 12, 4096)\n",
            "(480, 1)\n",
            "7\n",
            "EEG_Deep/Data2A/parsed_P07T.mat\n",
            "(480, 12, 4096)\n",
            "(560, 12, 4096)\n",
            "(560, 1)\n",
            "8\n",
            "EEG_Deep/Data2A/parsed_P08T.mat\n",
            "(560, 12, 4096)\n",
            "(640, 12, 4096)\n",
            "(640, 1)\n",
            "10\n",
            "EEG_Deep/Data2A/parsed_P010T.mat\n",
            "(640, 12, 4096)\n",
            "(720, 12, 4096)\n",
            "(720, 1)\n",
            "9\n",
            "EEG_Deep/Data2A/parsed_P09E.mat\n",
            "(0, 12, 4096)\n",
            "(40, 12, 4096)\n",
            "(40, 1)\n",
            "Filtering of training data in progress\n",
            "(720, 12, 4096)\n",
            "Filtering of test data in progress\n",
            "(40, 12, 4096)\n",
            "[2. 1. 1. 2. 2. 2. 2. 1. 1. 2. 1. 2. 2. 1. 1. 2. 2. 2. 2. 2. 2. 1. 1. 2.\n",
            " 2. 2. 1. 1. 2. 1. 1. 1. 2. 2. 1. 1. 2. 1. 1. 1. 2. 1. 1. 2. 1. 2. 1. 1.\n",
            " 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 1. 1. 1. 2. 1. 2. 2. 2. 1. 2. 1. 2. 1.\n",
            " 2. 1. 2. 1. 1. 1. 1. 1. 2. 2. 2. 2. 1. 1. 2. 1. 1. 2. 2. 1. 2. 1. 2. 1.\n",
            " 2. 1. 2. 2.]\n",
            "(620, 12, 1536)\n",
            "(620,)\n",
            "(100, 12, 1536)\n",
            "(100,)\n",
            "[[0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]]\n",
            "X_train shape: (620, 1, 12, 1536)\n",
            "620 train samples\n",
            "100 val samples\n",
            "(40, 12, 1536)\n",
            "(40, 1)\n",
            "X_train shape: (40, 1, 12, 1536)\n",
            "40 train samples\n",
            "Train on 620 samples, validate on 100 samples\n",
            "Epoch 1/300\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 1.20674, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.9582 - acc: 0.5161 - val_loss: 1.2067 - val_acc: 0.5900\n",
            "Epoch 2/300\n",
            "\n",
            "Epoch 00002: val_loss improved from 1.20674 to 1.04650, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.7550 - acc: 0.6065 - val_loss: 1.0465 - val_acc: 0.5600\n",
            "Epoch 3/300\n",
            "\n",
            "Epoch 00003: val_loss improved from 1.04650 to 0.92475, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.7005 - acc: 0.6790 - val_loss: 0.9248 - val_acc: 0.6300\n",
            "Epoch 4/300\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.92475 to 0.88459, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.6502 - acc: 0.6806 - val_loss: 0.8846 - val_acc: 0.5500\n",
            "Epoch 5/300\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.88459 to 0.83415, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.6118 - acc: 0.7145 - val_loss: 0.8341 - val_acc: 0.5600\n",
            "Epoch 6/300\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.83415 to 0.73547, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.5935 - acc: 0.7161 - val_loss: 0.7355 - val_acc: 0.6400\n",
            "Epoch 7/300\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.73547 to 0.65986, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.5701 - acc: 0.7226 - val_loss: 0.6599 - val_acc: 0.6600\n",
            "Epoch 8/300\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.65986\n",
            "620/620 - 0s - loss: 0.5493 - acc: 0.7387 - val_loss: 0.6703 - val_acc: 0.6600\n",
            "Epoch 9/300\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.65986 to 0.60258, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.5228 - acc: 0.7565 - val_loss: 0.6026 - val_acc: 0.6800\n",
            "Epoch 10/300\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.60258 to 0.58415, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.5159 - acc: 0.7323 - val_loss: 0.5841 - val_acc: 0.7200\n",
            "Epoch 11/300\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.58415\n",
            "620/620 - 0s - loss: 0.5236 - acc: 0.7500 - val_loss: 0.6367 - val_acc: 0.6700\n",
            "Epoch 12/300\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.58415\n",
            "620/620 - 0s - loss: 0.5154 - acc: 0.7532 - val_loss: 0.6757 - val_acc: 0.6400\n",
            "Epoch 13/300\n",
            "\n",
            "Epoch 00013: val_loss improved from 0.58415 to 0.58021, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.5066 - acc: 0.7597 - val_loss: 0.5802 - val_acc: 0.7200\n",
            "Epoch 14/300\n",
            "\n",
            "Epoch 00014: val_loss improved from 0.58021 to 0.57844, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.5167 - acc: 0.7565 - val_loss: 0.5784 - val_acc: 0.7000\n",
            "Epoch 15/300\n",
            "\n",
            "Epoch 00015: val_loss improved from 0.57844 to 0.57437, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.4931 - acc: 0.7645 - val_loss: 0.5744 - val_acc: 0.7000\n",
            "Epoch 16/300\n",
            "\n",
            "Epoch 00016: val_loss improved from 0.57437 to 0.52338, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.4804 - acc: 0.7823 - val_loss: 0.5234 - val_acc: 0.7400\n",
            "Epoch 17/300\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.52338\n",
            "620/620 - 0s - loss: 0.4947 - acc: 0.7790 - val_loss: 0.6345 - val_acc: 0.6800\n",
            "Epoch 18/300\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.52338\n",
            "620/620 - 0s - loss: 0.4829 - acc: 0.7532 - val_loss: 0.5432 - val_acc: 0.7400\n",
            "Epoch 19/300\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.52338\n",
            "620/620 - 0s - loss: 0.4730 - acc: 0.7855 - val_loss: 0.5615 - val_acc: 0.7000\n",
            "Epoch 20/300\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.52338\n",
            "620/620 - 0s - loss: 0.4849 - acc: 0.7629 - val_loss: 0.5315 - val_acc: 0.7100\n",
            "Epoch 21/300\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.52338\n",
            "620/620 - 0s - loss: 0.4841 - acc: 0.7613 - val_loss: 0.5813 - val_acc: 0.6900\n",
            "Epoch 22/300\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.52338\n",
            "620/620 - 0s - loss: 0.4645 - acc: 0.8065 - val_loss: 0.5513 - val_acc: 0.7100\n",
            "Epoch 23/300\n",
            "\n",
            "Epoch 00023: val_loss improved from 0.52338 to 0.49979, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.4589 - acc: 0.7790 - val_loss: 0.4998 - val_acc: 0.7900\n",
            "Epoch 24/300\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.49979\n",
            "620/620 - 0s - loss: 0.4653 - acc: 0.8113 - val_loss: 0.5282 - val_acc: 0.7000\n",
            "Epoch 25/300\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.49979\n",
            "620/620 - 0s - loss: 0.4418 - acc: 0.7823 - val_loss: 0.6269 - val_acc: 0.6700\n",
            "Epoch 26/300\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.49979\n",
            "620/620 - 0s - loss: 0.4343 - acc: 0.8145 - val_loss: 0.5231 - val_acc: 0.7300\n",
            "Epoch 27/300\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.49979\n",
            "620/620 - 0s - loss: 0.4651 - acc: 0.7823 - val_loss: 0.5995 - val_acc: 0.6900\n",
            "Epoch 28/300\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.49979\n",
            "620/620 - 0s - loss: 0.4586 - acc: 0.7839 - val_loss: 0.5249 - val_acc: 0.7300\n",
            "Epoch 29/300\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.49979\n",
            "620/620 - 0s - loss: 0.4534 - acc: 0.7984 - val_loss: 0.5582 - val_acc: 0.7300\n",
            "Epoch 30/300\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.49979\n",
            "620/620 - 0s - loss: 0.4702 - acc: 0.7726 - val_loss: 0.5934 - val_acc: 0.7000\n",
            "Epoch 31/300\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.49979\n",
            "620/620 - 0s - loss: 0.4665 - acc: 0.7694 - val_loss: 0.5233 - val_acc: 0.7300\n",
            "Epoch 32/300\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.49979\n",
            "620/620 - 0s - loss: 0.4288 - acc: 0.7968 - val_loss: 0.6496 - val_acc: 0.6600\n",
            "Epoch 33/300\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.49979\n",
            "620/620 - 0s - loss: 0.4516 - acc: 0.7935 - val_loss: 0.5064 - val_acc: 0.7400\n",
            "Epoch 34/300\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.49979\n",
            "620/620 - 0s - loss: 0.4215 - acc: 0.8177 - val_loss: 0.5509 - val_acc: 0.7400\n",
            "Epoch 35/300\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.49979\n",
            "620/620 - 0s - loss: 0.4227 - acc: 0.8032 - val_loss: 0.5880 - val_acc: 0.7100\n",
            "Epoch 36/300\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.49979\n",
            "620/620 - 0s - loss: 0.4407 - acc: 0.7806 - val_loss: 0.5319 - val_acc: 0.7300\n",
            "Epoch 37/300\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.49979\n",
            "620/620 - 0s - loss: 0.4260 - acc: 0.7887 - val_loss: 0.5125 - val_acc: 0.7400\n",
            "Epoch 38/300\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.49979\n",
            "620/620 - 0s - loss: 0.4592 - acc: 0.7694 - val_loss: 0.5465 - val_acc: 0.7400\n",
            "Epoch 39/300\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.49979\n",
            "620/620 - 0s - loss: 0.4205 - acc: 0.7919 - val_loss: 0.5122 - val_acc: 0.7700\n",
            "Epoch 40/300\n",
            "\n",
            "Epoch 00040: val_loss improved from 0.49979 to 0.47878, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.4409 - acc: 0.8032 - val_loss: 0.4788 - val_acc: 0.7600\n",
            "Epoch 41/300\n",
            "\n",
            "Epoch 00041: val_loss improved from 0.47878 to 0.47660, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.4069 - acc: 0.8129 - val_loss: 0.4766 - val_acc: 0.8100\n",
            "Epoch 42/300\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.47660\n",
            "620/620 - 0s - loss: 0.4263 - acc: 0.8065 - val_loss: 0.5018 - val_acc: 0.8000\n",
            "Epoch 43/300\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.47660\n",
            "620/620 - 0s - loss: 0.4259 - acc: 0.8032 - val_loss: 0.6027 - val_acc: 0.7000\n",
            "Epoch 44/300\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.47660\n",
            "620/620 - 0s - loss: 0.4337 - acc: 0.8032 - val_loss: 0.5548 - val_acc: 0.7100\n",
            "Epoch 45/300\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.47660\n",
            "620/620 - 0s - loss: 0.4462 - acc: 0.7855 - val_loss: 0.5272 - val_acc: 0.7400\n",
            "Epoch 46/300\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.47660\n",
            "620/620 - 0s - loss: 0.4369 - acc: 0.8000 - val_loss: 0.5139 - val_acc: 0.7400\n",
            "Epoch 47/300\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.47660\n",
            "620/620 - 0s - loss: 0.4444 - acc: 0.7758 - val_loss: 0.6570 - val_acc: 0.7000\n",
            "Epoch 48/300\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.47660\n",
            "620/620 - 0s - loss: 0.4421 - acc: 0.8032 - val_loss: 0.5192 - val_acc: 0.7200\n",
            "Epoch 49/300\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.47660\n",
            "620/620 - 0s - loss: 0.4187 - acc: 0.8065 - val_loss: 0.5572 - val_acc: 0.7200\n",
            "Epoch 50/300\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.47660\n",
            "620/620 - 0s - loss: 0.3918 - acc: 0.8419 - val_loss: 0.5030 - val_acc: 0.7300\n",
            "Epoch 51/300\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.47660\n",
            "620/620 - 0s - loss: 0.4137 - acc: 0.8016 - val_loss: 0.6504 - val_acc: 0.6900\n",
            "Epoch 52/300\n",
            "\n",
            "Epoch 00052: val_loss improved from 0.47660 to 0.47064, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.4181 - acc: 0.8000 - val_loss: 0.4706 - val_acc: 0.7700\n",
            "Epoch 53/300\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.4031 - acc: 0.7984 - val_loss: 0.5152 - val_acc: 0.7500\n",
            "Epoch 54/300\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.4563 - acc: 0.7774 - val_loss: 0.5274 - val_acc: 0.7300\n",
            "Epoch 55/300\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.4252 - acc: 0.8000 - val_loss: 0.6739 - val_acc: 0.6600\n",
            "Epoch 56/300\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.47064\n",
            "620/620 - 1s - loss: 0.3887 - acc: 0.8339 - val_loss: 0.5827 - val_acc: 0.7000\n",
            "Epoch 57/300\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3889 - acc: 0.8210 - val_loss: 0.5749 - val_acc: 0.7200\n",
            "Epoch 58/300\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.4083 - acc: 0.8194 - val_loss: 0.5528 - val_acc: 0.7100\n",
            "Epoch 59/300\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3868 - acc: 0.8242 - val_loss: 0.5640 - val_acc: 0.7200\n",
            "Epoch 60/300\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3974 - acc: 0.8306 - val_loss: 0.4788 - val_acc: 0.7800\n",
            "Epoch 61/300\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.4096 - acc: 0.8194 - val_loss: 0.6857 - val_acc: 0.6800\n",
            "Epoch 62/300\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3877 - acc: 0.8177 - val_loss: 0.5870 - val_acc: 0.6900\n",
            "Epoch 63/300\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3789 - acc: 0.8274 - val_loss: 0.6252 - val_acc: 0.7100\n",
            "Epoch 64/300\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3785 - acc: 0.8371 - val_loss: 0.5245 - val_acc: 0.7000\n",
            "Epoch 65/300\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3923 - acc: 0.8129 - val_loss: 0.6636 - val_acc: 0.7000\n",
            "Epoch 66/300\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3905 - acc: 0.8339 - val_loss: 0.6546 - val_acc: 0.7100\n",
            "Epoch 67/300\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.4133 - acc: 0.8113 - val_loss: 0.5457 - val_acc: 0.7100\n",
            "Epoch 68/300\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3895 - acc: 0.8242 - val_loss: 0.5881 - val_acc: 0.7100\n",
            "Epoch 69/300\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3809 - acc: 0.8290 - val_loss: 0.5250 - val_acc: 0.7500\n",
            "Epoch 70/300\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3894 - acc: 0.8242 - val_loss: 0.6953 - val_acc: 0.6700\n",
            "Epoch 71/300\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.4307 - acc: 0.8194 - val_loss: 0.5383 - val_acc: 0.6900\n",
            "Epoch 72/300\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3886 - acc: 0.8226 - val_loss: 0.5496 - val_acc: 0.7700\n",
            "Epoch 73/300\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.4003 - acc: 0.8129 - val_loss: 0.6127 - val_acc: 0.7000\n",
            "Epoch 74/300\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.4137 - acc: 0.8145 - val_loss: 0.7877 - val_acc: 0.6500\n",
            "Epoch 75/300\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.4182 - acc: 0.8065 - val_loss: 0.5844 - val_acc: 0.6800\n",
            "Epoch 76/300\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3834 - acc: 0.8306 - val_loss: 0.6867 - val_acc: 0.6500\n",
            "Epoch 77/300\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3931 - acc: 0.8161 - val_loss: 0.5623 - val_acc: 0.6900\n",
            "Epoch 78/300\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.4345 - acc: 0.8145 - val_loss: 0.6364 - val_acc: 0.6800\n",
            "Epoch 79/300\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3769 - acc: 0.8387 - val_loss: 0.5888 - val_acc: 0.7000\n",
            "Epoch 80/300\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3896 - acc: 0.8129 - val_loss: 0.5127 - val_acc: 0.7000\n",
            "Epoch 81/300\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3594 - acc: 0.8323 - val_loss: 0.7597 - val_acc: 0.6700\n",
            "Epoch 82/300\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3929 - acc: 0.8339 - val_loss: 0.5132 - val_acc: 0.7200\n",
            "Epoch 83/300\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3940 - acc: 0.8177 - val_loss: 0.5579 - val_acc: 0.7000\n",
            "Epoch 84/300\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.47064\n",
            "620/620 - 1s - loss: 0.3859 - acc: 0.8242 - val_loss: 0.6211 - val_acc: 0.7000\n",
            "Epoch 85/300\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3779 - acc: 0.8387 - val_loss: 0.5284 - val_acc: 0.7100\n",
            "Epoch 86/300\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3949 - acc: 0.8065 - val_loss: 0.5281 - val_acc: 0.7100\n",
            "Epoch 87/300\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3855 - acc: 0.8323 - val_loss: 0.5802 - val_acc: 0.7100\n",
            "Epoch 88/300\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3961 - acc: 0.8161 - val_loss: 0.5754 - val_acc: 0.7100\n",
            "Epoch 89/300\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3680 - acc: 0.8355 - val_loss: 0.5889 - val_acc: 0.6900\n",
            "Epoch 90/300\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3965 - acc: 0.8210 - val_loss: 0.5669 - val_acc: 0.6900\n",
            "Epoch 91/300\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3774 - acc: 0.8226 - val_loss: 0.5641 - val_acc: 0.7100\n",
            "Epoch 92/300\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3787 - acc: 0.8161 - val_loss: 0.5406 - val_acc: 0.6900\n",
            "Epoch 93/300\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3926 - acc: 0.8274 - val_loss: 0.7135 - val_acc: 0.6700\n",
            "Epoch 94/300\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3780 - acc: 0.8210 - val_loss: 0.6871 - val_acc: 0.6700\n",
            "Epoch 95/300\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3716 - acc: 0.8468 - val_loss: 0.5984 - val_acc: 0.7000\n",
            "Epoch 96/300\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3887 - acc: 0.8065 - val_loss: 0.5575 - val_acc: 0.6800\n",
            "Epoch 97/300\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3619 - acc: 0.8452 - val_loss: 0.6044 - val_acc: 0.6800\n",
            "Epoch 98/300\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3462 - acc: 0.8452 - val_loss: 0.5970 - val_acc: 0.6900\n",
            "Epoch 99/300\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3650 - acc: 0.8306 - val_loss: 0.5331 - val_acc: 0.7000\n",
            "Epoch 100/300\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3540 - acc: 0.8484 - val_loss: 0.5718 - val_acc: 0.7000\n",
            "Epoch 101/300\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3818 - acc: 0.8452 - val_loss: 0.5891 - val_acc: 0.6700\n",
            "Epoch 102/300\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3853 - acc: 0.8274 - val_loss: 0.5119 - val_acc: 0.6700\n",
            "Epoch 103/300\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3574 - acc: 0.8323 - val_loss: 0.5430 - val_acc: 0.6800\n",
            "Epoch 104/300\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3544 - acc: 0.8403 - val_loss: 0.5950 - val_acc: 0.6800\n",
            "Epoch 105/300\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3652 - acc: 0.8355 - val_loss: 0.5373 - val_acc: 0.6700\n",
            "Epoch 106/300\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.4122 - acc: 0.8097 - val_loss: 0.6532 - val_acc: 0.7000\n",
            "Epoch 107/300\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3679 - acc: 0.8419 - val_loss: 0.5748 - val_acc: 0.6900\n",
            "Epoch 108/300\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3794 - acc: 0.8210 - val_loss: 0.6005 - val_acc: 0.6900\n",
            "Epoch 109/300\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3752 - acc: 0.8210 - val_loss: 0.7390 - val_acc: 0.6400\n",
            "Epoch 110/300\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3575 - acc: 0.8403 - val_loss: 0.5107 - val_acc: 0.7100\n",
            "Epoch 111/300\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3604 - acc: 0.8339 - val_loss: 0.5286 - val_acc: 0.7200\n",
            "Epoch 112/300\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3697 - acc: 0.8274 - val_loss: 0.8143 - val_acc: 0.6600\n",
            "Epoch 113/300\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3996 - acc: 0.8113 - val_loss: 0.5203 - val_acc: 0.7200\n",
            "Epoch 114/300\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3803 - acc: 0.8226 - val_loss: 0.6479 - val_acc: 0.6700\n",
            "Epoch 115/300\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3748 - acc: 0.8306 - val_loss: 0.5665 - val_acc: 0.7100\n",
            "Epoch 116/300\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3635 - acc: 0.8210 - val_loss: 0.6455 - val_acc: 0.6900\n",
            "Epoch 117/300\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3587 - acc: 0.8226 - val_loss: 0.6638 - val_acc: 0.6900\n",
            "Epoch 118/300\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3794 - acc: 0.8371 - val_loss: 0.5414 - val_acc: 0.7200\n",
            "Epoch 119/300\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3570 - acc: 0.8242 - val_loss: 0.5804 - val_acc: 0.6900\n",
            "Epoch 120/300\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3453 - acc: 0.8484 - val_loss: 0.5701 - val_acc: 0.7200\n",
            "Epoch 121/300\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3742 - acc: 0.8210 - val_loss: 0.5351 - val_acc: 0.6900\n",
            "Epoch 122/300\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3550 - acc: 0.8371 - val_loss: 0.6470 - val_acc: 0.6700\n",
            "Epoch 123/300\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3736 - acc: 0.8161 - val_loss: 0.5490 - val_acc: 0.6700\n",
            "Epoch 124/300\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3695 - acc: 0.8306 - val_loss: 0.5427 - val_acc: 0.7200\n",
            "Epoch 125/300\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3738 - acc: 0.8323 - val_loss: 0.5333 - val_acc: 0.6900\n",
            "Epoch 126/300\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3953 - acc: 0.8081 - val_loss: 0.6731 - val_acc: 0.6600\n",
            "Epoch 127/300\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3735 - acc: 0.8290 - val_loss: 0.5809 - val_acc: 0.6800\n",
            "Epoch 128/300\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3884 - acc: 0.8113 - val_loss: 0.6159 - val_acc: 0.6800\n",
            "Epoch 129/300\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3597 - acc: 0.8419 - val_loss: 0.6160 - val_acc: 0.6800\n",
            "Epoch 130/300\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3751 - acc: 0.8129 - val_loss: 0.6302 - val_acc: 0.7000\n",
            "Epoch 131/300\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3599 - acc: 0.8435 - val_loss: 0.5686 - val_acc: 0.7000\n",
            "Epoch 132/300\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3429 - acc: 0.8452 - val_loss: 0.5233 - val_acc: 0.7200\n",
            "Epoch 133/300\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3562 - acc: 0.8435 - val_loss: 0.5465 - val_acc: 0.6700\n",
            "Epoch 134/300\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3411 - acc: 0.8532 - val_loss: 0.5978 - val_acc: 0.7000\n",
            "Epoch 135/300\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3624 - acc: 0.8290 - val_loss: 0.6133 - val_acc: 0.7100\n",
            "Epoch 136/300\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3952 - acc: 0.8226 - val_loss: 0.6138 - val_acc: 0.7000\n",
            "Epoch 137/300\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3516 - acc: 0.8452 - val_loss: 0.6895 - val_acc: 0.6900\n",
            "Epoch 138/300\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3641 - acc: 0.8452 - val_loss: 0.6591 - val_acc: 0.6800\n",
            "Epoch 139/300\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3475 - acc: 0.8419 - val_loss: 0.6440 - val_acc: 0.6700\n",
            "Epoch 140/300\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3614 - acc: 0.8452 - val_loss: 0.6509 - val_acc: 0.6900\n",
            "Epoch 141/300\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.47064\n",
            "620/620 - 1s - loss: 0.3699 - acc: 0.8371 - val_loss: 0.5313 - val_acc: 0.7600\n",
            "Epoch 142/300\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3369 - acc: 0.8419 - val_loss: 0.6056 - val_acc: 0.6700\n",
            "Epoch 143/300\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3572 - acc: 0.8355 - val_loss: 0.5763 - val_acc: 0.6900\n",
            "Epoch 144/300\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3673 - acc: 0.8387 - val_loss: 0.6608 - val_acc: 0.6700\n",
            "Epoch 145/300\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3663 - acc: 0.8403 - val_loss: 0.5814 - val_acc: 0.6900\n",
            "Epoch 146/300\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3399 - acc: 0.8581 - val_loss: 0.5301 - val_acc: 0.7100\n",
            "Epoch 147/300\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3354 - acc: 0.8548 - val_loss: 0.6469 - val_acc: 0.6800\n",
            "Epoch 148/300\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3527 - acc: 0.8371 - val_loss: 0.5754 - val_acc: 0.7100\n",
            "Epoch 149/300\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3712 - acc: 0.8435 - val_loss: 0.6207 - val_acc: 0.6800\n",
            "Epoch 150/300\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3363 - acc: 0.8306 - val_loss: 0.6673 - val_acc: 0.6800\n",
            "Epoch 151/300\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3424 - acc: 0.8452 - val_loss: 0.5641 - val_acc: 0.7100\n",
            "Epoch 152/300\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3579 - acc: 0.8435 - val_loss: 0.6796 - val_acc: 0.6800\n",
            "Epoch 153/300\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3578 - acc: 0.8565 - val_loss: 0.5181 - val_acc: 0.7500\n",
            "Epoch 154/300\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3209 - acc: 0.8790 - val_loss: 0.5522 - val_acc: 0.7000\n",
            "Epoch 155/300\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3564 - acc: 0.8500 - val_loss: 0.5574 - val_acc: 0.7100\n",
            "Epoch 156/300\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3572 - acc: 0.8403 - val_loss: 0.6623 - val_acc: 0.6800\n",
            "Epoch 157/300\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3643 - acc: 0.8371 - val_loss: 0.4814 - val_acc: 0.7600\n",
            "Epoch 158/300\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3439 - acc: 0.8548 - val_loss: 0.6281 - val_acc: 0.6800\n",
            "Epoch 159/300\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3389 - acc: 0.8661 - val_loss: 0.5929 - val_acc: 0.6900\n",
            "Epoch 160/300\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3497 - acc: 0.8452 - val_loss: 0.5974 - val_acc: 0.6800\n",
            "Epoch 161/300\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3460 - acc: 0.8435 - val_loss: 0.7187 - val_acc: 0.7000\n",
            "Epoch 162/300\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3471 - acc: 0.8500 - val_loss: 0.6203 - val_acc: 0.6800\n",
            "Epoch 163/300\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3286 - acc: 0.8661 - val_loss: 0.5440 - val_acc: 0.7000\n",
            "Epoch 164/300\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3431 - acc: 0.8532 - val_loss: 0.6117 - val_acc: 0.6700\n",
            "Epoch 165/300\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3599 - acc: 0.8371 - val_loss: 0.5831 - val_acc: 0.7000\n",
            "Epoch 166/300\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3887 - acc: 0.8161 - val_loss: 0.6415 - val_acc: 0.7200\n",
            "Epoch 167/300\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3318 - acc: 0.8484 - val_loss: 0.5363 - val_acc: 0.7100\n",
            "Epoch 168/300\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3512 - acc: 0.8258 - val_loss: 0.5870 - val_acc: 0.6900\n",
            "Epoch 169/300\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3467 - acc: 0.8419 - val_loss: 0.6769 - val_acc: 0.6800\n",
            "Epoch 170/300\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3415 - acc: 0.8355 - val_loss: 0.5693 - val_acc: 0.6900\n",
            "Epoch 171/300\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3277 - acc: 0.8565 - val_loss: 0.6056 - val_acc: 0.6700\n",
            "Epoch 172/300\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3501 - acc: 0.8435 - val_loss: 0.5821 - val_acc: 0.6600\n",
            "Epoch 173/300\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3950 - acc: 0.8242 - val_loss: 0.6286 - val_acc: 0.6600\n",
            "Epoch 174/300\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3495 - acc: 0.8403 - val_loss: 0.5749 - val_acc: 0.6800\n",
            "Epoch 175/300\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3552 - acc: 0.8419 - val_loss: 0.5644 - val_acc: 0.6900\n",
            "Epoch 176/300\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3559 - acc: 0.8532 - val_loss: 0.6775 - val_acc: 0.6900\n",
            "Epoch 177/300\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3348 - acc: 0.8694 - val_loss: 0.5770 - val_acc: 0.7000\n",
            "Epoch 178/300\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3314 - acc: 0.8597 - val_loss: 0.6106 - val_acc: 0.7000\n",
            "Epoch 179/300\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3368 - acc: 0.8677 - val_loss: 0.5726 - val_acc: 0.7200\n",
            "Epoch 180/300\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3385 - acc: 0.8468 - val_loss: 0.6698 - val_acc: 0.6700\n",
            "Epoch 181/300\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3252 - acc: 0.8629 - val_loss: 0.6463 - val_acc: 0.7000\n",
            "Epoch 182/300\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3364 - acc: 0.8548 - val_loss: 0.8165 - val_acc: 0.6700\n",
            "Epoch 183/300\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3218 - acc: 0.8565 - val_loss: 0.6190 - val_acc: 0.6900\n",
            "Epoch 184/300\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3282 - acc: 0.8613 - val_loss: 0.6167 - val_acc: 0.6900\n",
            "Epoch 185/300\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3423 - acc: 0.8516 - val_loss: 0.5464 - val_acc: 0.7200\n",
            "Epoch 186/300\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3362 - acc: 0.8468 - val_loss: 0.6114 - val_acc: 0.6800\n",
            "Epoch 187/300\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3236 - acc: 0.8645 - val_loss: 0.5378 - val_acc: 0.7100\n",
            "Epoch 188/300\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3043 - acc: 0.8677 - val_loss: 0.5435 - val_acc: 0.7300\n",
            "Epoch 189/300\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3706 - acc: 0.8258 - val_loss: 0.5530 - val_acc: 0.7100\n",
            "Epoch 190/300\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3329 - acc: 0.8645 - val_loss: 0.5955 - val_acc: 0.6700\n",
            "Epoch 191/300\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3518 - acc: 0.8452 - val_loss: 0.5238 - val_acc: 0.7300\n",
            "Epoch 192/300\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3513 - acc: 0.8306 - val_loss: 0.6130 - val_acc: 0.6800\n",
            "Epoch 193/300\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3407 - acc: 0.8435 - val_loss: 0.5493 - val_acc: 0.7100\n",
            "Epoch 194/300\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3449 - acc: 0.8435 - val_loss: 0.5855 - val_acc: 0.6900\n",
            "Epoch 195/300\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3425 - acc: 0.8484 - val_loss: 0.7715 - val_acc: 0.6600\n",
            "Epoch 196/300\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3587 - acc: 0.8403 - val_loss: 0.7188 - val_acc: 0.6700\n",
            "Epoch 197/300\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3313 - acc: 0.8581 - val_loss: 0.6690 - val_acc: 0.6800\n",
            "Epoch 198/300\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3593 - acc: 0.8532 - val_loss: 0.5833 - val_acc: 0.6600\n",
            "Epoch 199/300\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3545 - acc: 0.8371 - val_loss: 0.5566 - val_acc: 0.7100\n",
            "Epoch 200/300\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3362 - acc: 0.8468 - val_loss: 0.6170 - val_acc: 0.7000\n",
            "Epoch 201/300\n",
            "\n",
            "Epoch 00201: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3257 - acc: 0.8565 - val_loss: 0.6214 - val_acc: 0.6800\n",
            "Epoch 202/300\n",
            "\n",
            "Epoch 00202: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3586 - acc: 0.8274 - val_loss: 0.6488 - val_acc: 0.6900\n",
            "Epoch 203/300\n",
            "\n",
            "Epoch 00203: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3344 - acc: 0.8629 - val_loss: 0.6904 - val_acc: 0.6900\n",
            "Epoch 204/300\n",
            "\n",
            "Epoch 00204: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3059 - acc: 0.8613 - val_loss: 0.5837 - val_acc: 0.6900\n",
            "Epoch 205/300\n",
            "\n",
            "Epoch 00205: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3457 - acc: 0.8419 - val_loss: 0.5578 - val_acc: 0.7300\n",
            "Epoch 206/300\n",
            "\n",
            "Epoch 00206: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3452 - acc: 0.8500 - val_loss: 0.6426 - val_acc: 0.6900\n",
            "Epoch 207/300\n",
            "\n",
            "Epoch 00207: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3320 - acc: 0.8516 - val_loss: 0.6946 - val_acc: 0.7000\n",
            "Epoch 208/300\n",
            "\n",
            "Epoch 00208: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3568 - acc: 0.8371 - val_loss: 0.5814 - val_acc: 0.7100\n",
            "Epoch 209/300\n",
            "\n",
            "Epoch 00209: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3384 - acc: 0.8387 - val_loss: 0.6040 - val_acc: 0.6800\n",
            "Epoch 210/300\n",
            "\n",
            "Epoch 00210: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3346 - acc: 0.8565 - val_loss: 0.5161 - val_acc: 0.7200\n",
            "Epoch 211/300\n",
            "\n",
            "Epoch 00211: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3252 - acc: 0.8597 - val_loss: 0.6417 - val_acc: 0.7000\n",
            "Epoch 212/300\n",
            "\n",
            "Epoch 00212: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3424 - acc: 0.8387 - val_loss: 0.5913 - val_acc: 0.6800\n",
            "Epoch 213/300\n",
            "\n",
            "Epoch 00213: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3178 - acc: 0.8516 - val_loss: 0.8034 - val_acc: 0.6800\n",
            "Epoch 214/300\n",
            "\n",
            "Epoch 00214: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3393 - acc: 0.8468 - val_loss: 0.5676 - val_acc: 0.7100\n",
            "Epoch 215/300\n",
            "\n",
            "Epoch 00215: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3575 - acc: 0.8403 - val_loss: 0.5497 - val_acc: 0.7000\n",
            "Epoch 216/300\n",
            "\n",
            "Epoch 00216: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3318 - acc: 0.8694 - val_loss: 0.5920 - val_acc: 0.6900\n",
            "Epoch 217/300\n",
            "\n",
            "Epoch 00217: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3338 - acc: 0.8581 - val_loss: 0.5742 - val_acc: 0.6800\n",
            "Epoch 218/300\n",
            "\n",
            "Epoch 00218: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3317 - acc: 0.8484 - val_loss: 0.6838 - val_acc: 0.6900\n",
            "Epoch 219/300\n",
            "\n",
            "Epoch 00219: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3521 - acc: 0.8387 - val_loss: 0.5711 - val_acc: 0.7000\n",
            "Epoch 220/300\n",
            "\n",
            "Epoch 00220: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3278 - acc: 0.8629 - val_loss: 0.6326 - val_acc: 0.6900\n",
            "Epoch 221/300\n",
            "\n",
            "Epoch 00221: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3516 - acc: 0.8484 - val_loss: 0.5013 - val_acc: 0.7200\n",
            "Epoch 222/300\n",
            "\n",
            "Epoch 00222: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3459 - acc: 0.8419 - val_loss: 0.6816 - val_acc: 0.6900\n",
            "Epoch 223/300\n",
            "\n",
            "Epoch 00223: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.2994 - acc: 0.8726 - val_loss: 0.6443 - val_acc: 0.6800\n",
            "Epoch 224/300\n",
            "\n",
            "Epoch 00224: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3505 - acc: 0.8355 - val_loss: 0.7010 - val_acc: 0.6700\n",
            "Epoch 225/300\n",
            "\n",
            "Epoch 00225: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3043 - acc: 0.8613 - val_loss: 0.6701 - val_acc: 0.6700\n",
            "Epoch 226/300\n",
            "\n",
            "Epoch 00226: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3180 - acc: 0.8565 - val_loss: 0.5644 - val_acc: 0.7000\n",
            "Epoch 227/300\n",
            "\n",
            "Epoch 00227: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3273 - acc: 0.8677 - val_loss: 0.6027 - val_acc: 0.7000\n",
            "Epoch 228/300\n",
            "\n",
            "Epoch 00228: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3147 - acc: 0.8597 - val_loss: 0.6175 - val_acc: 0.7100\n",
            "Epoch 229/300\n",
            "\n",
            "Epoch 00229: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3234 - acc: 0.8468 - val_loss: 0.6015 - val_acc: 0.6900\n",
            "Epoch 230/300\n",
            "\n",
            "Epoch 00230: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3301 - acc: 0.8355 - val_loss: 0.6798 - val_acc: 0.6800\n",
            "Epoch 231/300\n",
            "\n",
            "Epoch 00231: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3441 - acc: 0.8581 - val_loss: 0.5815 - val_acc: 0.7000\n",
            "Epoch 232/300\n",
            "\n",
            "Epoch 00232: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3259 - acc: 0.8613 - val_loss: 0.5501 - val_acc: 0.6900\n",
            "Epoch 233/300\n",
            "\n",
            "Epoch 00233: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3164 - acc: 0.8581 - val_loss: 0.5684 - val_acc: 0.7100\n",
            "Epoch 234/300\n",
            "\n",
            "Epoch 00234: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3293 - acc: 0.8532 - val_loss: 0.6013 - val_acc: 0.7000\n",
            "Epoch 235/300\n",
            "\n",
            "Epoch 00235: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3240 - acc: 0.8677 - val_loss: 0.6487 - val_acc: 0.6800\n",
            "Epoch 236/300\n",
            "\n",
            "Epoch 00236: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3056 - acc: 0.8758 - val_loss: 0.6462 - val_acc: 0.6900\n",
            "Epoch 237/300\n",
            "\n",
            "Epoch 00237: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.2910 - acc: 0.8597 - val_loss: 0.6849 - val_acc: 0.7100\n",
            "Epoch 238/300\n",
            "\n",
            "Epoch 00238: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3431 - acc: 0.8435 - val_loss: 0.5774 - val_acc: 0.7100\n",
            "Epoch 239/300\n",
            "\n",
            "Epoch 00239: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3441 - acc: 0.8532 - val_loss: 0.8610 - val_acc: 0.6500\n",
            "Epoch 240/300\n",
            "\n",
            "Epoch 00240: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3554 - acc: 0.8226 - val_loss: 0.5652 - val_acc: 0.7200\n",
            "Epoch 241/300\n",
            "\n",
            "Epoch 00241: val_loss did not improve from 0.47064\n",
            "620/620 - 0s - loss: 0.3548 - acc: 0.8532 - val_loss: 0.5824 - val_acc: 0.7100\n",
            "Epoch 242/300\n",
            "\n",
            "Epoch 00242: val_loss improved from 0.47064 to 0.46986, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.3450 - acc: 0.8500 - val_loss: 0.4699 - val_acc: 0.7500\n",
            "Epoch 243/300\n",
            "\n",
            "Epoch 00243: val_loss did not improve from 0.46986\n",
            "620/620 - 0s - loss: 0.3208 - acc: 0.8581 - val_loss: 0.6521 - val_acc: 0.7100\n",
            "Epoch 244/300\n",
            "\n",
            "Epoch 00244: val_loss did not improve from 0.46986\n",
            "620/620 - 0s - loss: 0.3187 - acc: 0.8597 - val_loss: 0.6830 - val_acc: 0.7000\n",
            "Epoch 245/300\n",
            "\n",
            "Epoch 00245: val_loss did not improve from 0.46986\n",
            "620/620 - 0s - loss: 0.3220 - acc: 0.8548 - val_loss: 0.6201 - val_acc: 0.7000\n",
            "Epoch 246/300\n",
            "\n",
            "Epoch 00246: val_loss did not improve from 0.46986\n",
            "620/620 - 0s - loss: 0.3077 - acc: 0.8677 - val_loss: 0.5879 - val_acc: 0.7200\n",
            "Epoch 247/300\n",
            "\n",
            "Epoch 00247: val_loss did not improve from 0.46986\n",
            "620/620 - 0s - loss: 0.3426 - acc: 0.8452 - val_loss: 0.6133 - val_acc: 0.7000\n",
            "Epoch 248/300\n",
            "\n",
            "Epoch 00248: val_loss did not improve from 0.46986\n",
            "620/620 - 0s - loss: 0.3426 - acc: 0.8581 - val_loss: 0.6257 - val_acc: 0.7200\n",
            "Epoch 249/300\n",
            "\n",
            "Epoch 00249: val_loss did not improve from 0.46986\n",
            "620/620 - 0s - loss: 0.3394 - acc: 0.8468 - val_loss: 0.7064 - val_acc: 0.6900\n",
            "Epoch 250/300\n",
            "\n",
            "Epoch 00250: val_loss did not improve from 0.46986\n",
            "620/620 - 0s - loss: 0.3402 - acc: 0.8548 - val_loss: 0.6655 - val_acc: 0.6900\n",
            "Epoch 251/300\n",
            "\n",
            "Epoch 00251: val_loss did not improve from 0.46986\n",
            "620/620 - 0s - loss: 0.3140 - acc: 0.8677 - val_loss: 0.6989 - val_acc: 0.6800\n",
            "Epoch 252/300\n",
            "\n",
            "Epoch 00252: val_loss did not improve from 0.46986\n",
            "620/620 - 0s - loss: 0.3466 - acc: 0.8565 - val_loss: 0.6221 - val_acc: 0.7000\n",
            "Epoch 253/300\n",
            "\n",
            "Epoch 00253: val_loss did not improve from 0.46986\n",
            "620/620 - 0s - loss: 0.3010 - acc: 0.8758 - val_loss: 0.5304 - val_acc: 0.7300\n",
            "Epoch 254/300\n",
            "\n",
            "Epoch 00254: val_loss did not improve from 0.46986\n",
            "620/620 - 0s - loss: 0.3321 - acc: 0.8516 - val_loss: 0.6522 - val_acc: 0.7000\n",
            "Epoch 255/300\n",
            "\n",
            "Epoch 00255: val_loss did not improve from 0.46986\n",
            "620/620 - 0s - loss: 0.3135 - acc: 0.8661 - val_loss: 0.6725 - val_acc: 0.7100\n",
            "Epoch 256/300\n",
            "\n",
            "Epoch 00256: val_loss did not improve from 0.46986\n",
            "620/620 - 0s - loss: 0.3245 - acc: 0.8581 - val_loss: 0.5648 - val_acc: 0.7000\n",
            "Epoch 257/300\n",
            "\n",
            "Epoch 00257: val_loss did not improve from 0.46986\n",
            "620/620 - 0s - loss: 0.2955 - acc: 0.8790 - val_loss: 0.5882 - val_acc: 0.6800\n",
            "Epoch 258/300\n",
            "\n",
            "Epoch 00258: val_loss did not improve from 0.46986\n",
            "620/620 - 0s - loss: 0.3641 - acc: 0.8274 - val_loss: 0.6886 - val_acc: 0.6900\n",
            "Epoch 259/300\n",
            "\n",
            "Epoch 00259: val_loss did not improve from 0.46986\n",
            "620/620 - 0s - loss: 0.3068 - acc: 0.8774 - val_loss: 0.7941 - val_acc: 0.6900\n",
            "Epoch 260/300\n",
            "\n",
            "Epoch 00260: val_loss did not improve from 0.46986\n",
            "620/620 - 0s - loss: 0.3020 - acc: 0.8710 - val_loss: 0.6462 - val_acc: 0.7100\n",
            "Epoch 261/300\n",
            "\n",
            "Epoch 00261: val_loss did not improve from 0.46986\n",
            "620/620 - 0s - loss: 0.3127 - acc: 0.8694 - val_loss: 0.4938 - val_acc: 0.7200\n",
            "Epoch 262/300\n",
            "\n",
            "Epoch 00262: val_loss did not improve from 0.46986\n",
            "620/620 - 0s - loss: 0.3178 - acc: 0.8565 - val_loss: 0.5996 - val_acc: 0.6700\n",
            "Epoch 263/300\n",
            "\n",
            "Epoch 00263: val_loss did not improve from 0.46986\n",
            "620/620 - 0s - loss: 0.3322 - acc: 0.8677 - val_loss: 0.6312 - val_acc: 0.6900\n",
            "Epoch 264/300\n",
            "\n",
            "Epoch 00264: val_loss did not improve from 0.46986\n",
            "620/620 - 0s - loss: 0.3503 - acc: 0.8468 - val_loss: 0.6005 - val_acc: 0.6700\n",
            "Epoch 265/300\n",
            "\n",
            "Epoch 00265: val_loss did not improve from 0.46986\n",
            "620/620 - 0s - loss: 0.3506 - acc: 0.8452 - val_loss: 0.5627 - val_acc: 0.6900\n",
            "Epoch 266/300\n",
            "\n",
            "Epoch 00266: val_loss did not improve from 0.46986\n",
            "620/620 - 0s - loss: 0.3409 - acc: 0.8468 - val_loss: 0.6235 - val_acc: 0.6900\n",
            "Epoch 267/300\n",
            "\n",
            "Epoch 00267: val_loss did not improve from 0.46986\n",
            "620/620 - 0s - loss: 0.3156 - acc: 0.8629 - val_loss: 0.5473 - val_acc: 0.7000\n",
            "Epoch 268/300\n",
            "\n",
            "Epoch 00268: val_loss did not improve from 0.46986\n",
            "620/620 - 0s - loss: 0.3380 - acc: 0.8468 - val_loss: 0.5600 - val_acc: 0.6800\n",
            "Epoch 269/300\n",
            "\n",
            "Epoch 00269: val_loss did not improve from 0.46986\n",
            "620/620 - 0s - loss: 0.3035 - acc: 0.8823 - val_loss: 0.5954 - val_acc: 0.7000\n",
            "Epoch 270/300\n",
            "\n",
            "Epoch 00270: val_loss did not improve from 0.46986\n",
            "620/620 - 0s - loss: 0.3360 - acc: 0.8484 - val_loss: 0.5509 - val_acc: 0.7100\n",
            "Epoch 271/300\n",
            "\n",
            "Epoch 00271: val_loss did not improve from 0.46986\n",
            "620/620 - 0s - loss: 0.3436 - acc: 0.8371 - val_loss: 0.5698 - val_acc: 0.7200\n",
            "Epoch 272/300\n",
            "\n",
            "Epoch 00272: val_loss did not improve from 0.46986\n",
            "620/620 - 0s - loss: 0.3091 - acc: 0.8645 - val_loss: 0.8113 - val_acc: 0.6700\n",
            "Epoch 273/300\n",
            "\n",
            "Epoch 00273: val_loss did not improve from 0.46986\n",
            "620/620 - 0s - loss: 0.2998 - acc: 0.8710 - val_loss: 0.7838 - val_acc: 0.6800\n",
            "Epoch 274/300\n",
            "\n",
            "Epoch 00274: val_loss did not improve from 0.46986\n",
            "620/620 - 0s - loss: 0.3056 - acc: 0.8500 - val_loss: 0.6380 - val_acc: 0.7200\n",
            "Epoch 275/300\n",
            "\n",
            "Epoch 00275: val_loss did not improve from 0.46986\n",
            "620/620 - 0s - loss: 0.3345 - acc: 0.8452 - val_loss: 0.7242 - val_acc: 0.7100\n",
            "Epoch 276/300\n",
            "\n",
            "Epoch 00276: val_loss did not improve from 0.46986\n",
            "620/620 - 0s - loss: 0.3367 - acc: 0.8613 - val_loss: 0.6746 - val_acc: 0.6900\n",
            "Epoch 277/300\n",
            "\n",
            "Epoch 00277: val_loss did not improve from 0.46986\n",
            "620/620 - 0s - loss: 0.3166 - acc: 0.8661 - val_loss: 0.6077 - val_acc: 0.7000\n",
            "Epoch 278/300\n",
            "\n",
            "Epoch 00278: val_loss did not improve from 0.46986\n",
            "620/620 - 0s - loss: 0.3464 - acc: 0.8468 - val_loss: 0.5952 - val_acc: 0.6900\n",
            "Epoch 279/300\n",
            "\n",
            "Epoch 00279: val_loss did not improve from 0.46986\n",
            "620/620 - 0s - loss: 0.2915 - acc: 0.8919 - val_loss: 0.6596 - val_acc: 0.6900\n",
            "Epoch 280/300\n",
            "\n",
            "Epoch 00280: val_loss did not improve from 0.46986\n",
            "620/620 - 0s - loss: 0.3333 - acc: 0.8500 - val_loss: 0.6285 - val_acc: 0.7100\n",
            "Epoch 281/300\n",
            "\n",
            "Epoch 00281: val_loss did not improve from 0.46986\n",
            "620/620 - 0s - loss: 0.3257 - acc: 0.8565 - val_loss: 0.5544 - val_acc: 0.6900\n",
            "Epoch 282/300\n",
            "\n",
            "Epoch 00282: val_loss did not improve from 0.46986\n",
            "620/620 - 0s - loss: 0.3080 - acc: 0.8710 - val_loss: 0.5427 - val_acc: 0.7000\n",
            "Epoch 283/300\n",
            "\n",
            "Epoch 00283: val_loss did not improve from 0.46986\n",
            "620/620 - 0s - loss: 0.3227 - acc: 0.8452 - val_loss: 0.5267 - val_acc: 0.7400\n",
            "Epoch 284/300\n",
            "\n",
            "Epoch 00284: val_loss did not improve from 0.46986\n",
            "620/620 - 0s - loss: 0.3152 - acc: 0.8581 - val_loss: 0.6287 - val_acc: 0.7000\n",
            "Epoch 285/300\n",
            "\n",
            "Epoch 00285: val_loss did not improve from 0.46986\n",
            "620/620 - 0s - loss: 0.3153 - acc: 0.8629 - val_loss: 0.6010 - val_acc: 0.6900\n",
            "Epoch 286/300\n",
            "\n",
            "Epoch 00286: val_loss did not improve from 0.46986\n",
            "620/620 - 0s - loss: 0.3042 - acc: 0.8565 - val_loss: 0.6161 - val_acc: 0.6900\n",
            "Epoch 287/300\n",
            "\n",
            "Epoch 00287: val_loss did not improve from 0.46986\n",
            "620/620 - 0s - loss: 0.2972 - acc: 0.8597 - val_loss: 0.5640 - val_acc: 0.7000\n",
            "Epoch 288/300\n",
            "\n",
            "Epoch 00288: val_loss did not improve from 0.46986\n",
            "620/620 - 0s - loss: 0.2887 - acc: 0.8790 - val_loss: 0.7102 - val_acc: 0.7000\n",
            "Epoch 289/300\n",
            "\n",
            "Epoch 00289: val_loss did not improve from 0.46986\n",
            "620/620 - 0s - loss: 0.3354 - acc: 0.8371 - val_loss: 0.6928 - val_acc: 0.6900\n",
            "Epoch 290/300\n",
            "\n",
            "Epoch 00290: val_loss did not improve from 0.46986\n",
            "620/620 - 0s - loss: 0.3101 - acc: 0.8742 - val_loss: 0.6845 - val_acc: 0.7100\n",
            "Epoch 291/300\n",
            "\n",
            "Epoch 00291: val_loss did not improve from 0.46986\n",
            "620/620 - 0s - loss: 0.3125 - acc: 0.8581 - val_loss: 0.6142 - val_acc: 0.7000\n",
            "Epoch 292/300\n",
            "\n",
            "Epoch 00292: val_loss did not improve from 0.46986\n",
            "620/620 - 0s - loss: 0.3117 - acc: 0.8677 - val_loss: 0.5598 - val_acc: 0.7200\n",
            "Epoch 293/300\n",
            "\n",
            "Epoch 00293: val_loss did not improve from 0.46986\n",
            "620/620 - 0s - loss: 0.3058 - acc: 0.8629 - val_loss: 0.6133 - val_acc: 0.7000\n",
            "Epoch 294/300\n",
            "\n",
            "Epoch 00294: val_loss did not improve from 0.46986\n",
            "620/620 - 0s - loss: 0.2935 - acc: 0.8774 - val_loss: 0.7086 - val_acc: 0.6800\n",
            "Epoch 295/300\n",
            "\n",
            "Epoch 00295: val_loss did not improve from 0.46986\n",
            "620/620 - 0s - loss: 0.2963 - acc: 0.8710 - val_loss: 0.5659 - val_acc: 0.7200\n",
            "Epoch 296/300\n",
            "\n",
            "Epoch 00296: val_loss did not improve from 0.46986\n",
            "620/620 - 0s - loss: 0.3317 - acc: 0.8435 - val_loss: 0.5673 - val_acc: 0.7100\n",
            "Epoch 297/300\n",
            "\n",
            "Epoch 00297: val_loss did not improve from 0.46986\n",
            "620/620 - 0s - loss: 0.3214 - acc: 0.8613 - val_loss: 0.7124 - val_acc: 0.6900\n",
            "Epoch 298/300\n",
            "\n",
            "Epoch 00298: val_loss did not improve from 0.46986\n",
            "620/620 - 0s - loss: 0.3181 - acc: 0.8694 - val_loss: 0.6440 - val_acc: 0.7000\n",
            "Epoch 299/300\n",
            "\n",
            "Epoch 00299: val_loss did not improve from 0.46986\n",
            "620/620 - 0s - loss: 0.3248 - acc: 0.8565 - val_loss: 0.6045 - val_acc: 0.7000\n",
            "Epoch 300/300\n",
            "\n",
            "Epoch 00300: val_loss did not improve from 0.46986\n",
            "620/620 - 0s - loss: 0.3226 - acc: 0.8532 - val_loss: 0.7020 - val_acc: 0.6800\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUUL5/58PpAdVW2MsAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "# Evaluate on test data\n",
            "40/40 [==============================] - 0s 3ms/sample - loss: 0.5988 - acc: 0.6750\n",
            "test loss, test acc: [0.5987671745941043, 0.675]\n",
            "1\n",
            "EEG_Deep/Data2A/parsed_P01T.mat\n",
            "(0, 12, 4096)\n",
            "(80, 12, 4096)\n",
            "(80, 1)\n",
            "2\n",
            "EEG_Deep/Data2A/parsed_P02T.mat\n",
            "(80, 12, 4096)\n",
            "(160, 12, 4096)\n",
            "(160, 1)\n",
            "3\n",
            "EEG_Deep/Data2A/parsed_P03T.mat\n",
            "(160, 12, 4096)\n",
            "(240, 12, 4096)\n",
            "(240, 1)\n",
            "4\n",
            "EEG_Deep/Data2A/parsed_P04T.mat\n",
            "(240, 12, 4096)\n",
            "(320, 12, 4096)\n",
            "(320, 1)\n",
            "5\n",
            "EEG_Deep/Data2A/parsed_P05T.mat\n",
            "(320, 12, 4096)\n",
            "(400, 12, 4096)\n",
            "(400, 1)\n",
            "7\n",
            "EEG_Deep/Data2A/parsed_P07T.mat\n",
            "(400, 12, 4096)\n",
            "(480, 12, 4096)\n",
            "(480, 1)\n",
            "8\n",
            "EEG_Deep/Data2A/parsed_P08T.mat\n",
            "(480, 12, 4096)\n",
            "(560, 12, 4096)\n",
            "(560, 1)\n",
            "9\n",
            "EEG_Deep/Data2A/parsed_P09T.mat\n",
            "(560, 12, 4096)\n",
            "(640, 12, 4096)\n",
            "(640, 1)\n",
            "10\n",
            "EEG_Deep/Data2A/parsed_P010T.mat\n",
            "(640, 12, 4096)\n",
            "(720, 12, 4096)\n",
            "(720, 1)\n",
            "6\n",
            "EEG_Deep/Data2A/parsed_P06E.mat\n",
            "(0, 12, 4096)\n",
            "(40, 12, 4096)\n",
            "(40, 1)\n",
            "Filtering of training data in progress\n",
            "(720, 12, 4096)\n",
            "Filtering of test data in progress\n",
            "(40, 12, 4096)\n",
            "[1. 1. 1. 1. 1. 1. 2. 2. 1. 1. 1. 2. 1. 2. 1. 1. 2. 2. 2. 1. 2. 1. 2. 1.\n",
            " 1. 2. 1. 1. 2. 2. 1. 1. 1. 1. 2. 2. 1. 2. 1. 1. 1. 2. 1. 2. 2. 2. 1. 1.\n",
            " 2. 2. 1. 2. 1. 1. 1. 2. 1. 1. 1. 2. 1. 2. 2. 1. 1. 2. 2. 1. 2. 2. 2. 2.\n",
            " 1. 1. 1. 2. 2. 2. 1. 2. 2. 1. 1. 1. 2. 2. 2. 1. 2. 1. 1. 1. 2. 1. 1. 1.\n",
            " 2. 1. 1. 1.]\n",
            "(620, 12, 1536)\n",
            "(620,)\n",
            "(100, 12, 1536)\n",
            "(100,)\n",
            "[[1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]]\n",
            "X_train shape: (620, 1, 12, 1536)\n",
            "620 train samples\n",
            "100 val samples\n",
            "(40, 12, 1536)\n",
            "(40, 1)\n",
            "X_train shape: (40, 1, 12, 1536)\n",
            "40 train samples\n",
            "Train on 620 samples, validate on 100 samples\n",
            "Epoch 1/300\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 1.16598, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.9825 - acc: 0.4887 - val_loss: 1.1660 - val_acc: 0.5300\n",
            "Epoch 2/300\n",
            "\n",
            "Epoch 00002: val_loss improved from 1.16598 to 1.03255, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 0s - loss: 0.7777 - acc: 0.6032 - val_loss: 1.0326 - val_acc: 0.4600\n",
            "Epoch 3/300\n",
            "\n",
            "Epoch 00003: val_loss improved from 1.03255 to 0.92878, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.7335 - acc: 0.6129 - val_loss: 0.9288 - val_acc: 0.5000\n",
            "Epoch 4/300\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.92878 to 0.84911, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.7115 - acc: 0.6048 - val_loss: 0.8491 - val_acc: 0.5800\n",
            "Epoch 5/300\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.84911 to 0.81409, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.6972 - acc: 0.6129 - val_loss: 0.8141 - val_acc: 0.5200\n",
            "Epoch 6/300\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.81409 to 0.80413, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.6747 - acc: 0.6532 - val_loss: 0.8041 - val_acc: 0.4400\n",
            "Epoch 7/300\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.80413 to 0.78672, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.6441 - acc: 0.6984 - val_loss: 0.7867 - val_acc: 0.4600\n",
            "Epoch 8/300\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.78672\n",
            "620/620 - 0s - loss: 0.6366 - acc: 0.6742 - val_loss: 0.8727 - val_acc: 0.4400\n",
            "Epoch 9/300\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.78672\n",
            "620/620 - 0s - loss: 0.6039 - acc: 0.7081 - val_loss: 0.9202 - val_acc: 0.4600\n",
            "Epoch 10/300\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.78672 to 0.76942, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.5793 - acc: 0.7290 - val_loss: 0.7694 - val_acc: 0.4900\n",
            "Epoch 11/300\n",
            "\n",
            "Epoch 00011: val_loss improved from 0.76942 to 0.76902, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.5978 - acc: 0.7145 - val_loss: 0.7690 - val_acc: 0.5200\n",
            "Epoch 12/300\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.76902\n",
            "620/620 - 0s - loss: 0.5702 - acc: 0.7177 - val_loss: 0.9757 - val_acc: 0.4900\n",
            "Epoch 13/300\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.76902\n",
            "620/620 - 0s - loss: 0.5521 - acc: 0.7468 - val_loss: 0.8787 - val_acc: 0.4800\n",
            "Epoch 14/300\n",
            "\n",
            "Epoch 00014: val_loss improved from 0.76902 to 0.66211, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.5627 - acc: 0.7387 - val_loss: 0.6621 - val_acc: 0.6400\n",
            "Epoch 15/300\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.66211\n",
            "620/620 - 0s - loss: 0.5627 - acc: 0.7145 - val_loss: 0.6653 - val_acc: 0.6400\n",
            "Epoch 16/300\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.66211\n",
            "620/620 - 0s - loss: 0.5409 - acc: 0.7694 - val_loss: 0.7327 - val_acc: 0.5600\n",
            "Epoch 17/300\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.66211\n",
            "620/620 - 0s - loss: 0.5509 - acc: 0.7419 - val_loss: 0.7923 - val_acc: 0.5600\n",
            "Epoch 18/300\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.66211\n",
            "620/620 - 0s - loss: 0.5201 - acc: 0.7516 - val_loss: 0.7995 - val_acc: 0.5500\n",
            "Epoch 19/300\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.66211\n",
            "620/620 - 0s - loss: 0.5163 - acc: 0.7435 - val_loss: 0.8785 - val_acc: 0.5200\n",
            "Epoch 20/300\n",
            "\n",
            "Epoch 00020: val_loss improved from 0.66211 to 0.64437, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.5313 - acc: 0.7548 - val_loss: 0.6444 - val_acc: 0.6300\n",
            "Epoch 21/300\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.64437\n",
            "620/620 - 0s - loss: 0.5502 - acc: 0.7242 - val_loss: 0.7964 - val_acc: 0.5400\n",
            "Epoch 22/300\n",
            "\n",
            "Epoch 00022: val_loss improved from 0.64437 to 0.62030, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.5358 - acc: 0.7403 - val_loss: 0.6203 - val_acc: 0.6400\n",
            "Epoch 23/300\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.62030\n",
            "620/620 - 0s - loss: 0.5335 - acc: 0.7306 - val_loss: 0.6814 - val_acc: 0.6300\n",
            "Epoch 24/300\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.62030\n",
            "620/620 - 0s - loss: 0.4927 - acc: 0.7726 - val_loss: 0.7083 - val_acc: 0.6100\n",
            "Epoch 25/300\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.62030\n",
            "620/620 - 0s - loss: 0.5142 - acc: 0.7403 - val_loss: 0.8830 - val_acc: 0.5300\n",
            "Epoch 26/300\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.62030\n",
            "620/620 - 0s - loss: 0.4909 - acc: 0.7613 - val_loss: 0.6819 - val_acc: 0.6100\n",
            "Epoch 27/300\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.62030\n",
            "620/620 - 0s - loss: 0.5160 - acc: 0.7613 - val_loss: 0.7551 - val_acc: 0.5700\n",
            "Epoch 28/300\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.62030\n",
            "620/620 - 0s - loss: 0.5052 - acc: 0.7548 - val_loss: 0.8623 - val_acc: 0.5500\n",
            "Epoch 29/300\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.62030\n",
            "620/620 - 0s - loss: 0.4947 - acc: 0.7516 - val_loss: 0.6830 - val_acc: 0.6000\n",
            "Epoch 30/300\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.62030\n",
            "620/620 - 0s - loss: 0.5163 - acc: 0.7500 - val_loss: 0.6743 - val_acc: 0.6300\n",
            "Epoch 31/300\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.62030\n",
            "620/620 - 0s - loss: 0.4908 - acc: 0.7661 - val_loss: 0.7614 - val_acc: 0.5600\n",
            "Epoch 32/300\n",
            "\n",
            "Epoch 00032: val_loss improved from 0.62030 to 0.58758, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.5082 - acc: 0.7468 - val_loss: 0.5876 - val_acc: 0.6300\n",
            "Epoch 33/300\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.58758\n",
            "620/620 - 0s - loss: 0.4976 - acc: 0.7500 - val_loss: 0.6742 - val_acc: 0.6300\n",
            "Epoch 34/300\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.58758\n",
            "620/620 - 0s - loss: 0.4618 - acc: 0.7806 - val_loss: 0.6659 - val_acc: 0.6200\n",
            "Epoch 35/300\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.58758\n",
            "620/620 - 0s - loss: 0.5014 - acc: 0.7435 - val_loss: 0.7723 - val_acc: 0.5800\n",
            "Epoch 36/300\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.58758\n",
            "620/620 - 0s - loss: 0.4852 - acc: 0.7710 - val_loss: 0.6526 - val_acc: 0.5900\n",
            "Epoch 37/300\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.58758\n",
            "620/620 - 0s - loss: 0.4723 - acc: 0.7758 - val_loss: 0.8530 - val_acc: 0.5500\n",
            "Epoch 38/300\n",
            "\n",
            "Epoch 00038: val_loss improved from 0.58758 to 0.53939, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.4871 - acc: 0.7790 - val_loss: 0.5394 - val_acc: 0.7200\n",
            "Epoch 39/300\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.4883 - acc: 0.7581 - val_loss: 0.6065 - val_acc: 0.6200\n",
            "Epoch 40/300\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.5039 - acc: 0.7532 - val_loss: 0.6170 - val_acc: 0.6400\n",
            "Epoch 41/300\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.4714 - acc: 0.7581 - val_loss: 0.6225 - val_acc: 0.6200\n",
            "Epoch 42/300\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.4746 - acc: 0.7742 - val_loss: 0.6938 - val_acc: 0.6100\n",
            "Epoch 43/300\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.4993 - acc: 0.7661 - val_loss: 0.5983 - val_acc: 0.6100\n",
            "Epoch 44/300\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.5001 - acc: 0.7500 - val_loss: 0.7414 - val_acc: 0.5800\n",
            "Epoch 45/300\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.4926 - acc: 0.7661 - val_loss: 0.6501 - val_acc: 0.6300\n",
            "Epoch 46/300\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.4621 - acc: 0.7968 - val_loss: 0.5630 - val_acc: 0.6600\n",
            "Epoch 47/300\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.4857 - acc: 0.7742 - val_loss: 0.5920 - val_acc: 0.6600\n",
            "Epoch 48/300\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.4748 - acc: 0.7790 - val_loss: 0.7696 - val_acc: 0.5700\n",
            "Epoch 49/300\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.4601 - acc: 0.7790 - val_loss: 0.7116 - val_acc: 0.6100\n",
            "Epoch 50/300\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.4728 - acc: 0.7726 - val_loss: 0.7737 - val_acc: 0.5700\n",
            "Epoch 51/300\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.4689 - acc: 0.7871 - val_loss: 0.7079 - val_acc: 0.5900\n",
            "Epoch 52/300\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.5117 - acc: 0.7516 - val_loss: 0.6160 - val_acc: 0.6300\n",
            "Epoch 53/300\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.4640 - acc: 0.7774 - val_loss: 0.8423 - val_acc: 0.5800\n",
            "Epoch 54/300\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.4598 - acc: 0.7855 - val_loss: 0.6494 - val_acc: 0.6100\n",
            "Epoch 55/300\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.5136 - acc: 0.7710 - val_loss: 0.7020 - val_acc: 0.6000\n",
            "Epoch 56/300\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.4783 - acc: 0.7806 - val_loss: 0.6235 - val_acc: 0.6400\n",
            "Epoch 57/300\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.4735 - acc: 0.7726 - val_loss: 0.7087 - val_acc: 0.6400\n",
            "Epoch 58/300\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.4551 - acc: 0.7903 - val_loss: 0.7260 - val_acc: 0.6200\n",
            "Epoch 59/300\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.4571 - acc: 0.8016 - val_loss: 0.6785 - val_acc: 0.6100\n",
            "Epoch 60/300\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.4753 - acc: 0.7855 - val_loss: 0.6762 - val_acc: 0.6100\n",
            "Epoch 61/300\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.4491 - acc: 0.7903 - val_loss: 0.8276 - val_acc: 0.5800\n",
            "Epoch 62/300\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.4540 - acc: 0.7823 - val_loss: 0.7543 - val_acc: 0.6200\n",
            "Epoch 63/300\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.4608 - acc: 0.7855 - val_loss: 0.7118 - val_acc: 0.6100\n",
            "Epoch 64/300\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.4791 - acc: 0.7710 - val_loss: 0.6080 - val_acc: 0.6500\n",
            "Epoch 65/300\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.4527 - acc: 0.7823 - val_loss: 0.6407 - val_acc: 0.6300\n",
            "Epoch 66/300\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.4550 - acc: 0.7839 - val_loss: 0.5447 - val_acc: 0.6900\n",
            "Epoch 67/300\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.4727 - acc: 0.7742 - val_loss: 0.8134 - val_acc: 0.5900\n",
            "Epoch 68/300\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.4467 - acc: 0.7871 - val_loss: 0.7680 - val_acc: 0.5900\n",
            "Epoch 69/300\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.4410 - acc: 0.8065 - val_loss: 0.7142 - val_acc: 0.6100\n",
            "Epoch 70/300\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.4448 - acc: 0.8032 - val_loss: 0.7417 - val_acc: 0.6100\n",
            "Epoch 71/300\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.4538 - acc: 0.8129 - val_loss: 0.7598 - val_acc: 0.6300\n",
            "Epoch 72/300\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.4701 - acc: 0.7774 - val_loss: 0.8038 - val_acc: 0.5800\n",
            "Epoch 73/300\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.4397 - acc: 0.7806 - val_loss: 0.8169 - val_acc: 0.5900\n",
            "Epoch 74/300\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.4455 - acc: 0.7952 - val_loss: 0.7008 - val_acc: 0.6100\n",
            "Epoch 75/300\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.4446 - acc: 0.7968 - val_loss: 0.9598 - val_acc: 0.5500\n",
            "Epoch 76/300\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.4277 - acc: 0.8129 - val_loss: 0.7356 - val_acc: 0.5900\n",
            "Epoch 77/300\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.4395 - acc: 0.7952 - val_loss: 0.7368 - val_acc: 0.6100\n",
            "Epoch 78/300\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.4597 - acc: 0.7726 - val_loss: 0.8263 - val_acc: 0.5700\n",
            "Epoch 79/300\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.4473 - acc: 0.8048 - val_loss: 0.6693 - val_acc: 0.6300\n",
            "Epoch 80/300\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.4172 - acc: 0.8113 - val_loss: 0.7142 - val_acc: 0.6100\n",
            "Epoch 81/300\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.4271 - acc: 0.8032 - val_loss: 0.8119 - val_acc: 0.5900\n",
            "Epoch 82/300\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.4821 - acc: 0.7661 - val_loss: 0.7744 - val_acc: 0.5900\n",
            "Epoch 83/300\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.4540 - acc: 0.8000 - val_loss: 0.6297 - val_acc: 0.6000\n",
            "Epoch 84/300\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.4376 - acc: 0.7871 - val_loss: 0.7066 - val_acc: 0.6100\n",
            "Epoch 85/300\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.4314 - acc: 0.7952 - val_loss: 0.7459 - val_acc: 0.6300\n",
            "Epoch 86/300\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.4538 - acc: 0.7871 - val_loss: 0.9301 - val_acc: 0.5700\n",
            "Epoch 87/300\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.4791 - acc: 0.7645 - val_loss: 0.8077 - val_acc: 0.6000\n",
            "Epoch 88/300\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.4521 - acc: 0.8081 - val_loss: 0.6674 - val_acc: 0.6300\n",
            "Epoch 89/300\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.4581 - acc: 0.7790 - val_loss: 0.7323 - val_acc: 0.6200\n",
            "Epoch 90/300\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.4209 - acc: 0.7984 - val_loss: 0.7400 - val_acc: 0.6100\n",
            "Epoch 91/300\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.4630 - acc: 0.7645 - val_loss: 0.6345 - val_acc: 0.6500\n",
            "Epoch 92/300\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.4489 - acc: 0.7903 - val_loss: 0.8464 - val_acc: 0.5900\n",
            "Epoch 93/300\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.4201 - acc: 0.8065 - val_loss: 0.6255 - val_acc: 0.6100\n",
            "Epoch 94/300\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.4332 - acc: 0.8016 - val_loss: 0.6580 - val_acc: 0.6200\n",
            "Epoch 95/300\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.4314 - acc: 0.8097 - val_loss: 0.7110 - val_acc: 0.6000\n",
            "Epoch 96/300\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.4216 - acc: 0.8016 - val_loss: 1.0391 - val_acc: 0.5400\n",
            "Epoch 97/300\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.4366 - acc: 0.7952 - val_loss: 0.7403 - val_acc: 0.6000\n",
            "Epoch 98/300\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.4273 - acc: 0.8000 - val_loss: 0.5824 - val_acc: 0.6500\n",
            "Epoch 99/300\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.4337 - acc: 0.7871 - val_loss: 0.6483 - val_acc: 0.6200\n",
            "Epoch 100/300\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.4199 - acc: 0.8290 - val_loss: 0.7147 - val_acc: 0.6200\n",
            "Epoch 101/300\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.4001 - acc: 0.8306 - val_loss: 0.7508 - val_acc: 0.6200\n",
            "Epoch 102/300\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.4177 - acc: 0.8145 - val_loss: 0.6709 - val_acc: 0.6200\n",
            "Epoch 103/300\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.4314 - acc: 0.7984 - val_loss: 0.9615 - val_acc: 0.5800\n",
            "Epoch 104/300\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.4276 - acc: 0.7968 - val_loss: 0.7241 - val_acc: 0.6000\n",
            "Epoch 105/300\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.4053 - acc: 0.8226 - val_loss: 0.9491 - val_acc: 0.5800\n",
            "Epoch 106/300\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.4331 - acc: 0.8161 - val_loss: 0.7702 - val_acc: 0.6100\n",
            "Epoch 107/300\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.4103 - acc: 0.8387 - val_loss: 0.8285 - val_acc: 0.5900\n",
            "Epoch 108/300\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.4772 - acc: 0.7758 - val_loss: 0.7854 - val_acc: 0.6000\n",
            "Epoch 109/300\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.4245 - acc: 0.8000 - val_loss: 0.6488 - val_acc: 0.6200\n",
            "Epoch 110/300\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.4558 - acc: 0.7823 - val_loss: 0.7972 - val_acc: 0.5600\n",
            "Epoch 111/300\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.4300 - acc: 0.7919 - val_loss: 0.8599 - val_acc: 0.5700\n",
            "Epoch 112/300\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.4166 - acc: 0.8016 - val_loss: 0.6570 - val_acc: 0.6100\n",
            "Epoch 113/300\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.4155 - acc: 0.7952 - val_loss: 0.7572 - val_acc: 0.5800\n",
            "Epoch 114/300\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.4123 - acc: 0.8145 - val_loss: 0.7478 - val_acc: 0.5900\n",
            "Epoch 115/300\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.4342 - acc: 0.8065 - val_loss: 0.7897 - val_acc: 0.6000\n",
            "Epoch 116/300\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.4423 - acc: 0.7758 - val_loss: 0.9827 - val_acc: 0.5500\n",
            "Epoch 117/300\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.4477 - acc: 0.7935 - val_loss: 0.5989 - val_acc: 0.6200\n",
            "Epoch 118/300\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.4275 - acc: 0.7871 - val_loss: 0.5984 - val_acc: 0.6400\n",
            "Epoch 119/300\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.4233 - acc: 0.8000 - val_loss: 0.8378 - val_acc: 0.5900\n",
            "Epoch 120/300\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.4380 - acc: 0.8048 - val_loss: 0.9538 - val_acc: 0.5500\n",
            "Epoch 121/300\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.4176 - acc: 0.8274 - val_loss: 0.7623 - val_acc: 0.6100\n",
            "Epoch 122/300\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.3825 - acc: 0.8323 - val_loss: 0.7953 - val_acc: 0.6100\n",
            "Epoch 123/300\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.3856 - acc: 0.8435 - val_loss: 0.7441 - val_acc: 0.6300\n",
            "Epoch 124/300\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.4071 - acc: 0.8290 - val_loss: 0.5959 - val_acc: 0.6400\n",
            "Epoch 125/300\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.4162 - acc: 0.8048 - val_loss: 0.7224 - val_acc: 0.6200\n",
            "Epoch 126/300\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.3917 - acc: 0.8258 - val_loss: 0.6803 - val_acc: 0.6200\n",
            "Epoch 127/300\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.4064 - acc: 0.8210 - val_loss: 0.8626 - val_acc: 0.5700\n",
            "Epoch 128/300\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.4093 - acc: 0.8097 - val_loss: 0.9228 - val_acc: 0.5700\n",
            "Epoch 129/300\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.4155 - acc: 0.8097 - val_loss: 0.9864 - val_acc: 0.5700\n",
            "Epoch 130/300\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.3927 - acc: 0.8290 - val_loss: 0.6728 - val_acc: 0.6100\n",
            "Epoch 131/300\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.4210 - acc: 0.7984 - val_loss: 0.7273 - val_acc: 0.6100\n",
            "Epoch 132/300\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.4337 - acc: 0.7919 - val_loss: 0.6421 - val_acc: 0.6300\n",
            "Epoch 133/300\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.4057 - acc: 0.8226 - val_loss: 0.7937 - val_acc: 0.6100\n",
            "Epoch 134/300\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.3918 - acc: 0.8065 - val_loss: 0.7886 - val_acc: 0.6100\n",
            "Epoch 135/300\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.4006 - acc: 0.8226 - val_loss: 0.8851 - val_acc: 0.5800\n",
            "Epoch 136/300\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.4038 - acc: 0.8145 - val_loss: 0.8004 - val_acc: 0.6000\n",
            "Epoch 137/300\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.3770 - acc: 0.8403 - val_loss: 0.6798 - val_acc: 0.6300\n",
            "Epoch 138/300\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.4191 - acc: 0.8081 - val_loss: 0.7482 - val_acc: 0.6100\n",
            "Epoch 139/300\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.4141 - acc: 0.7919 - val_loss: 1.1564 - val_acc: 0.5500\n",
            "Epoch 140/300\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.3999 - acc: 0.8339 - val_loss: 0.7356 - val_acc: 0.5900\n",
            "Epoch 141/300\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.4320 - acc: 0.7903 - val_loss: 0.8083 - val_acc: 0.6000\n",
            "Epoch 142/300\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.3955 - acc: 0.8129 - val_loss: 0.6156 - val_acc: 0.6000\n",
            "Epoch 143/300\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.4113 - acc: 0.8065 - val_loss: 0.9718 - val_acc: 0.5700\n",
            "Epoch 144/300\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.4081 - acc: 0.8129 - val_loss: 0.7890 - val_acc: 0.5900\n",
            "Epoch 145/300\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.4001 - acc: 0.8145 - val_loss: 0.8725 - val_acc: 0.5900\n",
            "Epoch 146/300\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.4018 - acc: 0.8177 - val_loss: 0.8887 - val_acc: 0.5900\n",
            "Epoch 147/300\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.3928 - acc: 0.8258 - val_loss: 0.8570 - val_acc: 0.5900\n",
            "Epoch 148/300\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.4175 - acc: 0.8081 - val_loss: 0.8521 - val_acc: 0.5900\n",
            "Epoch 149/300\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.4041 - acc: 0.8081 - val_loss: 0.7762 - val_acc: 0.6000\n",
            "Epoch 150/300\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.3922 - acc: 0.8323 - val_loss: 0.8998 - val_acc: 0.5600\n",
            "Epoch 151/300\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.4224 - acc: 0.8194 - val_loss: 0.8161 - val_acc: 0.5800\n",
            "Epoch 152/300\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.3909 - acc: 0.8129 - val_loss: 0.8175 - val_acc: 0.6100\n",
            "Epoch 153/300\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.4077 - acc: 0.8145 - val_loss: 0.7612 - val_acc: 0.6000\n",
            "Epoch 154/300\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.3808 - acc: 0.8339 - val_loss: 0.7882 - val_acc: 0.5900\n",
            "Epoch 155/300\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.4062 - acc: 0.8210 - val_loss: 0.7792 - val_acc: 0.6000\n",
            "Epoch 156/300\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.3857 - acc: 0.8290 - val_loss: 0.6858 - val_acc: 0.6200\n",
            "Epoch 157/300\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.3979 - acc: 0.8161 - val_loss: 0.9052 - val_acc: 0.5700\n",
            "Epoch 158/300\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.3784 - acc: 0.8371 - val_loss: 0.9683 - val_acc: 0.5700\n",
            "Epoch 159/300\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.4262 - acc: 0.8048 - val_loss: 0.7023 - val_acc: 0.6100\n",
            "Epoch 160/300\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.4326 - acc: 0.7806 - val_loss: 0.6321 - val_acc: 0.6300\n",
            "Epoch 161/300\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.4251 - acc: 0.8113 - val_loss: 0.8440 - val_acc: 0.5700\n",
            "Epoch 162/300\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.4017 - acc: 0.8161 - val_loss: 0.9174 - val_acc: 0.5800\n",
            "Epoch 163/300\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.3967 - acc: 0.8081 - val_loss: 0.7156 - val_acc: 0.6500\n",
            "Epoch 164/300\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.4012 - acc: 0.8258 - val_loss: 0.6633 - val_acc: 0.6500\n",
            "Epoch 165/300\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.4034 - acc: 0.8258 - val_loss: 0.8335 - val_acc: 0.6100\n",
            "Epoch 166/300\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.3841 - acc: 0.8323 - val_loss: 0.9740 - val_acc: 0.5600\n",
            "Epoch 167/300\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.4183 - acc: 0.8145 - val_loss: 0.8948 - val_acc: 0.5800\n",
            "Epoch 168/300\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.4321 - acc: 0.8081 - val_loss: 0.7056 - val_acc: 0.6200\n",
            "Epoch 169/300\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.4030 - acc: 0.8161 - val_loss: 0.8758 - val_acc: 0.5700\n",
            "Epoch 170/300\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.3941 - acc: 0.8258 - val_loss: 0.9472 - val_acc: 0.5800\n",
            "Epoch 171/300\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.4322 - acc: 0.7887 - val_loss: 0.9227 - val_acc: 0.5900\n",
            "Epoch 172/300\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.4125 - acc: 0.8323 - val_loss: 0.8623 - val_acc: 0.5800\n",
            "Epoch 173/300\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.3690 - acc: 0.8371 - val_loss: 0.8537 - val_acc: 0.5800\n",
            "Epoch 174/300\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.3971 - acc: 0.8129 - val_loss: 0.6955 - val_acc: 0.6100\n",
            "Epoch 175/300\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.3920 - acc: 0.8161 - val_loss: 0.7859 - val_acc: 0.5700\n",
            "Epoch 176/300\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.4036 - acc: 0.8097 - val_loss: 0.9083 - val_acc: 0.5700\n",
            "Epoch 177/300\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.3813 - acc: 0.8323 - val_loss: 0.7605 - val_acc: 0.6100\n",
            "Epoch 178/300\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.3816 - acc: 0.8435 - val_loss: 0.6526 - val_acc: 0.6500\n",
            "Epoch 179/300\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.4090 - acc: 0.8274 - val_loss: 0.7016 - val_acc: 0.6200\n",
            "Epoch 180/300\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.4129 - acc: 0.8048 - val_loss: 0.8819 - val_acc: 0.5800\n",
            "Epoch 181/300\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.3866 - acc: 0.8371 - val_loss: 0.7787 - val_acc: 0.5700\n",
            "Epoch 182/300\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.4031 - acc: 0.8226 - val_loss: 0.8026 - val_acc: 0.5700\n",
            "Epoch 183/300\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.3814 - acc: 0.8339 - val_loss: 0.7664 - val_acc: 0.5800\n",
            "Epoch 184/300\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.4204 - acc: 0.8048 - val_loss: 0.7622 - val_acc: 0.5900\n",
            "Epoch 185/300\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.3795 - acc: 0.8290 - val_loss: 0.8626 - val_acc: 0.5700\n",
            "Epoch 186/300\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.4239 - acc: 0.8000 - val_loss: 0.8295 - val_acc: 0.6100\n",
            "Epoch 187/300\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.4235 - acc: 0.8210 - val_loss: 0.9050 - val_acc: 0.5800\n",
            "Epoch 188/300\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.4088 - acc: 0.8306 - val_loss: 0.7948 - val_acc: 0.5900\n",
            "Epoch 189/300\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.3950 - acc: 0.8419 - val_loss: 0.8454 - val_acc: 0.5800\n",
            "Epoch 190/300\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.4210 - acc: 0.7887 - val_loss: 0.6854 - val_acc: 0.6100\n",
            "Epoch 191/300\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.4235 - acc: 0.8032 - val_loss: 0.8144 - val_acc: 0.5800\n",
            "Epoch 192/300\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.3770 - acc: 0.8226 - val_loss: 0.8127 - val_acc: 0.6000\n",
            "Epoch 193/300\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.3710 - acc: 0.8290 - val_loss: 0.6376 - val_acc: 0.6800\n",
            "Epoch 194/300\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.3879 - acc: 0.8274 - val_loss: 0.7668 - val_acc: 0.6000\n",
            "Epoch 195/300\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.3851 - acc: 0.8258 - val_loss: 0.7309 - val_acc: 0.5900\n",
            "Epoch 196/300\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.3877 - acc: 0.8113 - val_loss: 0.8108 - val_acc: 0.5900\n",
            "Epoch 197/300\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.3771 - acc: 0.8387 - val_loss: 0.7775 - val_acc: 0.5900\n",
            "Epoch 198/300\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.3809 - acc: 0.8226 - val_loss: 0.8996 - val_acc: 0.5900\n",
            "Epoch 199/300\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.3730 - acc: 0.8290 - val_loss: 0.7209 - val_acc: 0.5700\n",
            "Epoch 200/300\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.3889 - acc: 0.8161 - val_loss: 0.7282 - val_acc: 0.6000\n",
            "Epoch 201/300\n",
            "\n",
            "Epoch 00201: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.4001 - acc: 0.8355 - val_loss: 0.9003 - val_acc: 0.5600\n",
            "Epoch 202/300\n",
            "\n",
            "Epoch 00202: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.4065 - acc: 0.8016 - val_loss: 0.7403 - val_acc: 0.5800\n",
            "Epoch 203/300\n",
            "\n",
            "Epoch 00203: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.4182 - acc: 0.8194 - val_loss: 0.7110 - val_acc: 0.6300\n",
            "Epoch 204/300\n",
            "\n",
            "Epoch 00204: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.3823 - acc: 0.8258 - val_loss: 0.9502 - val_acc: 0.5800\n",
            "Epoch 205/300\n",
            "\n",
            "Epoch 00205: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.3824 - acc: 0.8339 - val_loss: 0.9534 - val_acc: 0.5800\n",
            "Epoch 206/300\n",
            "\n",
            "Epoch 00206: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.3649 - acc: 0.8355 - val_loss: 0.7920 - val_acc: 0.5800\n",
            "Epoch 207/300\n",
            "\n",
            "Epoch 00207: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.3539 - acc: 0.8565 - val_loss: 0.7691 - val_acc: 0.6000\n",
            "Epoch 208/300\n",
            "\n",
            "Epoch 00208: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.3645 - acc: 0.8500 - val_loss: 0.8644 - val_acc: 0.5800\n",
            "Epoch 209/300\n",
            "\n",
            "Epoch 00209: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.3973 - acc: 0.8065 - val_loss: 0.8113 - val_acc: 0.5600\n",
            "Epoch 210/300\n",
            "\n",
            "Epoch 00210: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.3853 - acc: 0.8258 - val_loss: 0.8955 - val_acc: 0.5800\n",
            "Epoch 211/300\n",
            "\n",
            "Epoch 00211: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.4040 - acc: 0.8242 - val_loss: 0.9241 - val_acc: 0.5700\n",
            "Epoch 212/300\n",
            "\n",
            "Epoch 00212: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.3705 - acc: 0.8452 - val_loss: 0.8139 - val_acc: 0.5800\n",
            "Epoch 213/300\n",
            "\n",
            "Epoch 00213: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.3984 - acc: 0.8145 - val_loss: 0.7832 - val_acc: 0.6000\n",
            "Epoch 214/300\n",
            "\n",
            "Epoch 00214: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.4127 - acc: 0.8194 - val_loss: 0.8767 - val_acc: 0.5700\n",
            "Epoch 215/300\n",
            "\n",
            "Epoch 00215: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.3878 - acc: 0.8306 - val_loss: 0.5840 - val_acc: 0.6500\n",
            "Epoch 216/300\n",
            "\n",
            "Epoch 00216: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.3909 - acc: 0.8161 - val_loss: 0.7032 - val_acc: 0.5800\n",
            "Epoch 217/300\n",
            "\n",
            "Epoch 00217: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.3792 - acc: 0.8403 - val_loss: 0.7710 - val_acc: 0.5800\n",
            "Epoch 218/300\n",
            "\n",
            "Epoch 00218: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.3767 - acc: 0.8226 - val_loss: 0.8122 - val_acc: 0.5800\n",
            "Epoch 219/300\n",
            "\n",
            "Epoch 00219: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.3774 - acc: 0.8419 - val_loss: 0.8222 - val_acc: 0.5800\n",
            "Epoch 220/300\n",
            "\n",
            "Epoch 00220: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.3844 - acc: 0.8226 - val_loss: 0.8682 - val_acc: 0.5600\n",
            "Epoch 221/300\n",
            "\n",
            "Epoch 00221: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.3958 - acc: 0.8290 - val_loss: 0.8119 - val_acc: 0.5700\n",
            "Epoch 222/300\n",
            "\n",
            "Epoch 00222: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.3836 - acc: 0.8274 - val_loss: 0.7709 - val_acc: 0.5800\n",
            "Epoch 223/300\n",
            "\n",
            "Epoch 00223: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.3709 - acc: 0.8306 - val_loss: 0.7534 - val_acc: 0.6100\n",
            "Epoch 224/300\n",
            "\n",
            "Epoch 00224: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.4022 - acc: 0.8048 - val_loss: 0.9356 - val_acc: 0.5600\n",
            "Epoch 225/300\n",
            "\n",
            "Epoch 00225: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.3670 - acc: 0.8371 - val_loss: 0.9297 - val_acc: 0.5700\n",
            "Epoch 226/300\n",
            "\n",
            "Epoch 00226: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.3760 - acc: 0.8113 - val_loss: 0.9450 - val_acc: 0.5700\n",
            "Epoch 227/300\n",
            "\n",
            "Epoch 00227: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.4156 - acc: 0.8000 - val_loss: 0.9460 - val_acc: 0.5500\n",
            "Epoch 228/300\n",
            "\n",
            "Epoch 00228: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.3773 - acc: 0.8226 - val_loss: 0.8968 - val_acc: 0.5900\n",
            "Epoch 229/300\n",
            "\n",
            "Epoch 00229: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.3662 - acc: 0.8468 - val_loss: 0.9605 - val_acc: 0.5700\n",
            "Epoch 230/300\n",
            "\n",
            "Epoch 00230: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.3720 - acc: 0.8194 - val_loss: 0.8864 - val_acc: 0.5800\n",
            "Epoch 231/300\n",
            "\n",
            "Epoch 00231: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.3784 - acc: 0.8226 - val_loss: 0.8660 - val_acc: 0.5500\n",
            "Epoch 232/300\n",
            "\n",
            "Epoch 00232: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.4009 - acc: 0.8081 - val_loss: 0.8623 - val_acc: 0.5900\n",
            "Epoch 233/300\n",
            "\n",
            "Epoch 00233: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.3741 - acc: 0.8226 - val_loss: 0.7378 - val_acc: 0.5800\n",
            "Epoch 234/300\n",
            "\n",
            "Epoch 00234: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.3908 - acc: 0.8468 - val_loss: 0.6345 - val_acc: 0.6200\n",
            "Epoch 235/300\n",
            "\n",
            "Epoch 00235: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.3733 - acc: 0.8323 - val_loss: 0.6978 - val_acc: 0.6000\n",
            "Epoch 236/300\n",
            "\n",
            "Epoch 00236: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.3868 - acc: 0.8226 - val_loss: 0.8324 - val_acc: 0.5900\n",
            "Epoch 237/300\n",
            "\n",
            "Epoch 00237: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.3957 - acc: 0.8129 - val_loss: 1.0677 - val_acc: 0.5600\n",
            "Epoch 238/300\n",
            "\n",
            "Epoch 00238: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.3693 - acc: 0.8242 - val_loss: 0.7195 - val_acc: 0.5800\n",
            "Epoch 239/300\n",
            "\n",
            "Epoch 00239: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.3915 - acc: 0.8177 - val_loss: 0.9158 - val_acc: 0.5600\n",
            "Epoch 240/300\n",
            "\n",
            "Epoch 00240: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.3763 - acc: 0.8355 - val_loss: 0.7923 - val_acc: 0.5900\n",
            "Epoch 241/300\n",
            "\n",
            "Epoch 00241: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.3725 - acc: 0.8339 - val_loss: 0.7888 - val_acc: 0.5700\n",
            "Epoch 242/300\n",
            "\n",
            "Epoch 00242: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.3679 - acc: 0.8452 - val_loss: 0.5921 - val_acc: 0.6600\n",
            "Epoch 243/300\n",
            "\n",
            "Epoch 00243: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.3780 - acc: 0.8274 - val_loss: 0.6860 - val_acc: 0.5900\n",
            "Epoch 244/300\n",
            "\n",
            "Epoch 00244: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.4031 - acc: 0.8081 - val_loss: 0.7149 - val_acc: 0.6100\n",
            "Epoch 245/300\n",
            "\n",
            "Epoch 00245: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.3718 - acc: 0.8500 - val_loss: 0.9453 - val_acc: 0.5700\n",
            "Epoch 246/300\n",
            "\n",
            "Epoch 00246: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.3636 - acc: 0.8403 - val_loss: 0.7963 - val_acc: 0.6000\n",
            "Epoch 247/300\n",
            "\n",
            "Epoch 00247: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.3516 - acc: 0.8419 - val_loss: 0.7028 - val_acc: 0.6200\n",
            "Epoch 248/300\n",
            "\n",
            "Epoch 00248: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.3660 - acc: 0.8226 - val_loss: 0.6159 - val_acc: 0.6400\n",
            "Epoch 249/300\n",
            "\n",
            "Epoch 00249: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.3621 - acc: 0.8452 - val_loss: 0.8710 - val_acc: 0.5900\n",
            "Epoch 250/300\n",
            "\n",
            "Epoch 00250: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.3948 - acc: 0.8129 - val_loss: 0.9432 - val_acc: 0.5900\n",
            "Epoch 251/300\n",
            "\n",
            "Epoch 00251: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.3575 - acc: 0.8323 - val_loss: 0.7847 - val_acc: 0.5900\n",
            "Epoch 252/300\n",
            "\n",
            "Epoch 00252: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.3686 - acc: 0.8468 - val_loss: 0.7985 - val_acc: 0.6000\n",
            "Epoch 253/300\n",
            "\n",
            "Epoch 00253: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.3647 - acc: 0.8258 - val_loss: 0.8784 - val_acc: 0.5700\n",
            "Epoch 254/300\n",
            "\n",
            "Epoch 00254: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.3775 - acc: 0.8242 - val_loss: 0.7915 - val_acc: 0.5800\n",
            "Epoch 255/300\n",
            "\n",
            "Epoch 00255: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.3861 - acc: 0.8177 - val_loss: 0.8341 - val_acc: 0.5600\n",
            "Epoch 256/300\n",
            "\n",
            "Epoch 00256: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.3475 - acc: 0.8516 - val_loss: 0.7304 - val_acc: 0.5900\n",
            "Epoch 257/300\n",
            "\n",
            "Epoch 00257: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.4060 - acc: 0.8113 - val_loss: 0.7883 - val_acc: 0.5800\n",
            "Epoch 258/300\n",
            "\n",
            "Epoch 00258: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.3847 - acc: 0.8274 - val_loss: 0.8106 - val_acc: 0.6000\n",
            "Epoch 259/300\n",
            "\n",
            "Epoch 00259: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.3550 - acc: 0.8532 - val_loss: 0.8074 - val_acc: 0.6000\n",
            "Epoch 260/300\n",
            "\n",
            "Epoch 00260: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.4130 - acc: 0.8129 - val_loss: 0.6647 - val_acc: 0.6200\n",
            "Epoch 261/300\n",
            "\n",
            "Epoch 00261: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.3542 - acc: 0.8468 - val_loss: 1.0255 - val_acc: 0.5500\n",
            "Epoch 262/300\n",
            "\n",
            "Epoch 00262: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.3920 - acc: 0.8129 - val_loss: 0.9087 - val_acc: 0.5600\n",
            "Epoch 263/300\n",
            "\n",
            "Epoch 00263: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.3817 - acc: 0.8419 - val_loss: 0.8800 - val_acc: 0.5600\n",
            "Epoch 264/300\n",
            "\n",
            "Epoch 00264: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.3772 - acc: 0.8226 - val_loss: 0.8994 - val_acc: 0.5800\n",
            "Epoch 265/300\n",
            "\n",
            "Epoch 00265: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.3557 - acc: 0.8274 - val_loss: 0.9186 - val_acc: 0.5800\n",
            "Epoch 266/300\n",
            "\n",
            "Epoch 00266: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.3529 - acc: 0.8419 - val_loss: 0.9408 - val_acc: 0.5700\n",
            "Epoch 267/300\n",
            "\n",
            "Epoch 00267: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.3921 - acc: 0.8177 - val_loss: 0.9465 - val_acc: 0.5700\n",
            "Epoch 268/300\n",
            "\n",
            "Epoch 00268: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.3917 - acc: 0.8258 - val_loss: 0.8275 - val_acc: 0.5600\n",
            "Epoch 269/300\n",
            "\n",
            "Epoch 00269: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.3695 - acc: 0.8258 - val_loss: 0.9000 - val_acc: 0.5700\n",
            "Epoch 270/300\n",
            "\n",
            "Epoch 00270: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.3725 - acc: 0.8355 - val_loss: 0.7408 - val_acc: 0.5900\n",
            "Epoch 271/300\n",
            "\n",
            "Epoch 00271: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.3775 - acc: 0.8290 - val_loss: 0.7590 - val_acc: 0.5700\n",
            "Epoch 272/300\n",
            "\n",
            "Epoch 00272: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.3780 - acc: 0.8371 - val_loss: 0.8018 - val_acc: 0.5800\n",
            "Epoch 273/300\n",
            "\n",
            "Epoch 00273: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.3577 - acc: 0.8387 - val_loss: 0.7667 - val_acc: 0.6100\n",
            "Epoch 274/300\n",
            "\n",
            "Epoch 00274: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.3747 - acc: 0.8274 - val_loss: 0.8287 - val_acc: 0.5700\n",
            "Epoch 275/300\n",
            "\n",
            "Epoch 00275: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.3681 - acc: 0.8435 - val_loss: 1.0194 - val_acc: 0.5500\n",
            "Epoch 276/300\n",
            "\n",
            "Epoch 00276: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.3602 - acc: 0.8677 - val_loss: 0.8778 - val_acc: 0.5600\n",
            "Epoch 277/300\n",
            "\n",
            "Epoch 00277: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.3208 - acc: 0.8758 - val_loss: 0.9185 - val_acc: 0.5500\n",
            "Epoch 278/300\n",
            "\n",
            "Epoch 00278: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.3862 - acc: 0.8387 - val_loss: 0.7647 - val_acc: 0.5800\n",
            "Epoch 279/300\n",
            "\n",
            "Epoch 00279: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.3703 - acc: 0.8210 - val_loss: 0.8104 - val_acc: 0.5800\n",
            "Epoch 280/300\n",
            "\n",
            "Epoch 00280: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.3783 - acc: 0.8145 - val_loss: 0.8277 - val_acc: 0.5800\n",
            "Epoch 281/300\n",
            "\n",
            "Epoch 00281: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.3771 - acc: 0.8323 - val_loss: 0.6288 - val_acc: 0.6300\n",
            "Epoch 282/300\n",
            "\n",
            "Epoch 00282: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.4166 - acc: 0.8016 - val_loss: 0.7550 - val_acc: 0.5600\n",
            "Epoch 283/300\n",
            "\n",
            "Epoch 00283: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.3369 - acc: 0.8581 - val_loss: 0.7992 - val_acc: 0.6000\n",
            "Epoch 284/300\n",
            "\n",
            "Epoch 00284: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.3942 - acc: 0.8194 - val_loss: 0.8657 - val_acc: 0.5600\n",
            "Epoch 285/300\n",
            "\n",
            "Epoch 00285: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.3824 - acc: 0.8145 - val_loss: 0.7517 - val_acc: 0.6100\n",
            "Epoch 286/300\n",
            "\n",
            "Epoch 00286: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.3701 - acc: 0.8516 - val_loss: 0.8958 - val_acc: 0.5700\n",
            "Epoch 287/300\n",
            "\n",
            "Epoch 00287: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.3761 - acc: 0.8452 - val_loss: 0.9341 - val_acc: 0.5800\n",
            "Epoch 288/300\n",
            "\n",
            "Epoch 00288: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.3493 - acc: 0.8452 - val_loss: 0.8751 - val_acc: 0.5600\n",
            "Epoch 289/300\n",
            "\n",
            "Epoch 00289: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.3687 - acc: 0.8290 - val_loss: 0.7602 - val_acc: 0.5900\n",
            "Epoch 290/300\n",
            "\n",
            "Epoch 00290: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.3742 - acc: 0.8452 - val_loss: 0.7207 - val_acc: 0.6000\n",
            "Epoch 291/300\n",
            "\n",
            "Epoch 00291: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.3470 - acc: 0.8597 - val_loss: 0.8584 - val_acc: 0.5700\n",
            "Epoch 292/300\n",
            "\n",
            "Epoch 00292: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.3556 - acc: 0.8581 - val_loss: 0.8818 - val_acc: 0.5400\n",
            "Epoch 293/300\n",
            "\n",
            "Epoch 00293: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.3567 - acc: 0.8452 - val_loss: 0.9486 - val_acc: 0.5600\n",
            "Epoch 294/300\n",
            "\n",
            "Epoch 00294: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.3580 - acc: 0.8435 - val_loss: 0.8021 - val_acc: 0.6000\n",
            "Epoch 295/300\n",
            "\n",
            "Epoch 00295: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.3593 - acc: 0.8435 - val_loss: 0.8044 - val_acc: 0.6000\n",
            "Epoch 296/300\n",
            "\n",
            "Epoch 00296: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.3774 - acc: 0.8274 - val_loss: 0.7615 - val_acc: 0.5900\n",
            "Epoch 297/300\n",
            "\n",
            "Epoch 00297: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.3573 - acc: 0.8355 - val_loss: 0.5898 - val_acc: 0.7100\n",
            "Epoch 298/300\n",
            "\n",
            "Epoch 00298: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.3620 - acc: 0.8274 - val_loss: 0.8531 - val_acc: 0.6000\n",
            "Epoch 299/300\n",
            "\n",
            "Epoch 00299: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.3486 - acc: 0.8484 - val_loss: 0.8309 - val_acc: 0.6000\n",
            "Epoch 300/300\n",
            "\n",
            "Epoch 00300: val_loss did not improve from 0.53939\n",
            "620/620 - 0s - loss: 0.3736 - acc: 0.8242 - val_loss: 0.7920 - val_acc: 0.5800\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAgAElEQVR4nOydd5hdVbm433X6md4zSSbJpIcUCCH0\ndulFxQoCdkD057WABdGLioBXvYrYEARFmgqIXcEoLZRQEkhCAumTSWaSmUwvp7f9+2Pvtc8+bebM\nZM7MJFnv88wz5+y69j57f9/6yvqW0DQNhUKhUBy52Ca6AQqFQqGYWJQiUCgUiiMcpQgUCoXiCEcp\nAoVCoTjCUYpAoVAojnCUIlAoFIojHKUIFEcEQohGIYQmhHDkse3HhRAvjke7FIrJgFIEikmHEKJZ\nCBERQtSkLV9vCPPGiWmZQnF4ohSBYrKyG7hCfhFCLAOKJq45k4N8LBqFYqQoRaCYrDwEfNTy/WPA\ng9YNhBDlQogHhRCdQog9QoibhBA2Y51dCPFDIUSXEKIJeEeWfX8thGgTQuwTQtwmhLDn0zAhxB+E\nEO1CiH4hxPNCiCWWdV4hxO1Ge/qFEC8KIbzGutOEEGuEEH1CiBYhxMeN5c8JIa6xHCPFNWVYQf8t\nhNgB7DCW/cQ4xoAQ4nUhxOmW7e1CiK8LIXYJIQaN9TOEEHcKIW5Pu5a/CSGuz+e6FYcvShEoJiuv\nAGVCiKMMAX058HDaNj8DyoE5wJnoiuMTxrpPAu8EjgVWAh9I2/d+IAbMM7Y5H7iG/HgSmA/UAW8A\nv7Ws+yFwHHAKUAXcACSEELOM/X4G1ALLgQ15ng/gPcCJwGLj+1rjGFXA74A/CCE8xrovoltTFwNl\nwFVAAHgAuMKiLGuAc439FUcymqapP/U3qf6AZnQBdRPwXeBC4D+AA9CARsAORIDFlv0+BTxnfH4G\n+LRl3fnGvg5gChAGvJb1VwDPGp8/DryYZ1srjOOWo3esgsAxWbb7GvDnHMd4DrjG8j3l/Mbxzx6m\nHb3yvMA24N05ttsCnGd8/izwxET/3upv4v+Uv1ExmXkIeB6YTZpbCKgBnMAey7I9wHTj8zSgJW2d\nZJaxb5sQQi6zpW2fFcM6+Q5wKXrPPmFpjxvwALuy7Dojx/J8SWmbEOLLwNXo16mh9/xlcH2ocz0A\nfBhdsX4Y+MlBtElxmKBcQ4pJi6Zpe9CDxhcDf0pb3QVE0YW6ZCawz/jchi4QreskLegWQY2maRXG\nX5mmaUsYniuBd6NbLOXo1gmAMNoUAuZm2a8lx3IAP6mB8Pos25hlgo14wA3AZUClpmkVQL/RhuHO\n9TDwbiHEMcBRwF9ybKc4glCKQDHZuRrdLeK3LtQ0LQ48BnxHCFFq+OC/SDKO8BjweSFEgxCiErjR\nsm8b8G/gdiFEmRDCJoSYK4Q4M4/2lKIrkW504f2/luMmgPuAHwkhphlB25OFEG70OMK5QojLhBAO\nIUS1EGK5sesG4H1CiCIhxDzjmodrQwzoBBxCiG+iWwSSXwG3CiHmC52jhRDVRhtb0eMLDwF/1DQt\nmMc1Kw5zlCJQTGo0Tduladq6HKs/h96bbgJeRA963mesuxdYBWxED+imWxQfBVzA2+j+9ceBqXk0\n6UF0N9M+Y99X0tZ/GdiELmx7gO8DNk3T9qJbNl8ylm8AjjH2uQM93nEA3XXzW4ZmFfAvYLvRlhCp\nrqMfoSvCfwMDwK8Br2X9A8AydGWgUCA0TU1Mo1AcSQghzkC3nGZpSgAoUBaBQnFEIYRwAl8AfqWU\ngEKiFIFCcYQghDgK6EN3gf14gpujmEQo15BCoVAc4SiLQKFQKI5wDrkBZTU1NVpjY+NEN0OhUCgO\nKV5//fUuTdNqs6075BRBY2Mj69blyiZUKBQKRTaEEHtyrVOuIYVCoTjCUYpAoVAojnCUIlAoFIoj\nnEMuRpCNaDRKa2sroVBoopsybng8HhoaGnA6nRPdFIVCcYhzWCiC1tZWSktLaWxsxFJW+LBF0zS6\nu7tpbW1l9uzZE90chUJxiHNYuIZCoRDV1dVHhBIAEEJQXV19RFlACoWicBwWigA4YpSA5Ei7XoVC\nUTgOG0WgUCgUE0lbf5B/bW6b6GaMCqUIxoDu7m6WL1/O8uXLqa+vZ/r06eb3SCSS1zE+8YlPsG3b\ntgK3VKFQFIrz73ieTz/8BrF4YviNJxmHRbB4oqmurmbDhg0A3HzzzZSUlPDlL385ZRs5SbTNll33\n/uY3vyl4OxUKRWHQNI3BUAwAXzhGRZFrgls0MpRFUEB27tzJ4sWL+dCHPsSSJUtoa2vj2muvZeXK\nlSxZsoRbbrnF3Pa0005jw4YNxGIxKioquPHGGznmmGM4+eST6ejomMCrUCgUw7Gzw2d+lgrhUOKw\nswi+/fe3eHv/wJgec/G0Mr71rnzmNc9k69atPPjgg6xcuRKA733ve1RVVRGLxTjrrLP4wAc+wOLF\ni1P26e/v58wzz+R73/seX/ziF7nvvvu48cYbsx1eoVBMAp7f0WV+7g9GmTGBbRkNyiIoMHPnzjWV\nAMDvf/97VqxYwYoVK9iyZQtvv/12xj5er5eLLroIgOOOO47m5ubxaq5CoRgFzV1+87OyCCYBo+25\nF4ri4mLz844dO/jJT37Ca6+9RkVFBR/+8IezjgVwuZL+RbvdTix26D1YCsWRROdg2Pw8EIpOYEtG\nh7IIxpGBgQFKS0spKyujra2NVatWTXSTFArFGNDpC9NQ6QVGZxH0+PPLLiwUShGMIytWrGDx4sUs\nWrSIj370o5x66qkT3SSFQjEGdA6GmVtbAsBAMNUi8IVj/OftAzn33dTaz4pb/8PV968lOkGpp4ed\na2iiufnmm83P8+bNM9NKQR8N/NBDD2Xd78UXXzQ/9/X1mZ8vv/xyLr/88rFvqEIxCh5b14IALl15\nqIVDC0uXL8zZi+pYvb0zwyL4x8b93PinTbz69XOYUubJ2PfNffr7/vTWDp7d2sH5S+rHpc1WlEWg\nUCjy5obH3+Qrj79JJHboDZrKRedgmP7g6P36/nCMQCROfbmHYpc9I0YgFUNfIPs5dncmA83b2gdH\n3Y6DQSkChUIxYl7Y0TnRTTgontvWwY4DutD95IPruPUfmdl7+SIDxbUlbko9TgbTFEEwGgdgMBRl\nMBTl4Vf2EE9o5vrmbj8Lp5Qyo8rL1gNKESgUiklMyBBoAP94c3LV1InFE3z+9+vZvK8/r+2/+sc3\nuXt1EwB7uv2094++km+nz1AEpW7KvA4GgqmuoaQiiPHgy3u46S+beWxdi7m+qcvP7JpiFk4pY7uy\nCBQKxWTGKiybLHnzY0EsnuDN1r6UZZqm8cbe3vzaNhDibxv3c9kvX866fseBQdP9o2kavf4oA6Eo\nsXiC3kCUwXCM1t4AHQMjVwhdg0lFUOpxMhhOswgiuiIYCEVx2XWR+8QmXZHG4glaegI01hSzsL6E\npi4/4Vg8Zf/+YJRjb/k3z28vnBWmFIFCociLdkNIlnud+MPJXu/a5h7+tbn9oI79z01tXPLzl9jf\nFzSXvbyrm/f9Yg2v78muDOIJjTuf3UmPP2IK20AkKUQffmUPzV1+4gmN9/5iDXc9twuAUDRBJJ5g\nMBSlJ6CnbfpCUa57ZAPf+OvmEbc9xSLwZLEIIkmLQFoHrzR1851/vk1zt59oXGNOTTEL68uIJzQ2\ntaZaNXu6/fQGomzen5+1MxqUIlAclrT2Brj07jV0DB6ek/e09OjX1+ULD78xEIjE0DRt+A2HQFoE\n8+pKUhTBz5/ZyXef3JJ1n3hCS3Ep5aK1V1cAbRarQy7LFUDd0NLHD1Zt459v7mfQ0p4efwRfOMZN\nf9nM46+3sq83iC8cY2+PbsVIy8AXjtE1GDE/tw+EODAQJhJLmMHwaDwxbGC8czCMTUBlkWuYGEHM\nXFdT4ubeF3bzxzf2AdBYU8xJs6so9zr50K9e5ezbn2Nnx2DKPZFtLQRKEYwBY1GGGuC+++6jvf3g\nelYKnWe3dbK2uZfXdveMeN/XdvfQnyPDY7LwzNYO1jb35uwtWzkwEGLxN1fxwJrmgzqntAjm1hbj\nswje9v5QRkZMx0CIjS193PN8E+f+aPWwSki6ZKwDq2RPe1enL+s+Mh7Q2hfEZ0nZXHHrf/jlar33\n3xeMmPtLRWYqglCMbn/Y/NwfjDIQjHLdo+u5/jE97furf3yTax9aN2TbW3uDTC33YrcJPUYQyh4j\n8IWjDARj1JW6eexTJwPw3Dbd3TO7ppi6Mg9PfOF03reigaZOPy836c/uAePedOap9EeDUgRjgCxD\nvWHDBj796U9z/fXXm9+t5SKGQymCJBta+vj1i7tHvf9mw7ze1TEyX3YwEufKe1/h4Vf3jPrc48Em\nQwju7Q4Mu61UFtbCaOn86oWmYf3x7f0hSt0O6ko9+MNJC6N9IMRAKJqSCfOzZ3bysd+8xub9/bT2\nBjOEYzpSyPX4k8JOZuM05VAE8h7IHr+Vf7+lD+DqC0RNRXBgQD+e1SLo9umKxx+JM2gog50dPrNw\n5Rt7etlxIPP8mqbx1w37uO/F3TR3+5lVXQRgWgRWxReyWgThKKUeB1PLPThsgi1tA5S4HdSU6HJi\neoWX77xnKR6njb3d+rMrLYLOAlq3ShEUmAceeIATTjiB5cuX85nPfIZEIkEsFuMjH/kIy5YtY+nS\npfz0pz/l0UcfZcOGDXzwgx8csSVxOPKeO186qJQ+KSSaurILkVx0+cLEElreLpd8+fWLu/nSYxtH\nte8zWw9w7o9Wpwg72Rtu7vbT2htg6bdWsbU9e9VdWSJ5dk1x1vWapvH9f23lt6/sNZf1BSIZLp32\n/hD15R5KPA4Smt7TDUbi9AejaBopufht/UH6AlG2tA2Y3628tLOL9/7iJdPt0mEI6e6sFkGmMu8Y\nDJm+9H19SUXwwg1nMb+uhB2GW6U/GDX3PzAQIpHQzHYOhGIZv/NAKEqPP8L+viDhWJyW3iCdg2FT\nsAcjcXr9Ee59oYkvPLKBW/7xNjs7fMyq1u9tdbGLaFxLuRfWGMFAMEaZ14nDbmNGla48ZtcUp0w9\na7MJZlYV0dwdYH9f0LRkrPWMxprDb2TxkzdC+6axPWb9MrjoeyPebfPmzfz5z39mzZo1OBwOrr32\nWh555BHmzp1LV1cXmzbp7ezr66OiooKf/exn/PznP2f58uVj2/5DGE3TRjw/cygaZ7uRj53LrZAL\nKYika2j19k4WTClharl3RMd5e/8AkXiC5TMqAD1vXQrFkaBpGlfdr7smmjp9HN1QQSgaZ4ch3Pf2\nBHh2awe+cIxfv7Cbk+dW846jp+J22M1jvGUEGR227PdxIBQjGtdMHzrA5fe8wklzqrn5Er2I4zNb\nD/Dizi5OmlNNsVsXG75wDH84qSz6AhGqivWebbI3rx9zf1+QRfVl5rbrmntZv7ePAwMhZlQVJS0C\nn0URGMdo7Q0QisbxOPVr2tsd4IwfPGtut6836Roq9TiYVuE1709/MEo4pn+OJTS6/MnBY5FYgv19\nqb3saFyjy2jD+r19xBMacTQGgjHKi5x862+bWbOrmyJX8v4OhmKmRdBQWWS0OWhOTpM+jkAun1lV\nxO4uP41ZFPSs6mJebermtO8/gzS0unwqRnBI8tRTT7F27VpWrlzJ8uXLWb16Nbt27WLevHls27aN\nz3/+86xatYry8vKJbuqkJTyKEaxb2weJJTSmlnto6vSPKEjabQikvqBu3l/74Druz8O3/tAre/jj\n663m99v++TZf/kPSApB+9FxtWdvcw81/eytjvdVdI90Yb7cNEE9olHocNHf7TcH8h9db+eJjG7n/\npdT2bt6nK6BgjqCt9Ms3G26meEJjZ4eP1l79u6ZpfP1Pm5le4eVb71pMiVsXgv5wPCWltNcSJ0gX\nWvssAlfTNPqC+vpufwRN00yLwBoj6BoM47LbSGi65SPZbhl0deaCWjoGw/Qa2T/Fbl0RSPoCUZo6\n/aaCau8PpfTWrcovnRctrrROn97+l3Z209obZPsBHx8/pdFc32goghlV+rlbegJsbR/gpr9sMjOZ\nBkIxBkMxSj2OlH2yWWqN1UUMhGJYvG2GUhs+8D4aDj+LYBQ990KhaRpXXXUVt956a8a6N998kyef\nfJI777yTP/7xj9xzzz0T0MKJpa0/yGd/t54pZW5+8aHjsm4TjibMnmC+PLp2Ly6HjcuPn8kdT22n\nfSCU0aPfcWCQObUl2NN6yVLY9gUihGMJwrFERhGxbDywphmn3cb7j2sA9B7hPotZ3z4QIpbQ8Efi\nlLgzX7tL79bz3687d37KNIdvWSZZkr1mGRc4Y0EtT25qy6hcGY0n2H5gkHm1JfQFo+wzUjKtqZXN\nXX5qS90Uux2mX75zMMz2A4MI9N6zFJjbD/hoHwhx/XnzmVFVZFo2/nCM9oGky6cvEOFnT+/gteae\nDDdGc5efXZ0+/rJ+H396Yx8rGysBXfH6I3FTSaW4hgbDnDC7ihd3drGptd+0KKSCWnfTuTy3rZPV\n2zvZfmAQj9OG025jWnmyns+BgRDhWIILl9Tzr7faMxTB7iHGQ7ywM6kIOgbDFLkc5r0EuGzlDP6+\ncT/d/ggzq3RhbrUItrQN8PAre01LzBeKMRCKUuZ1ApjupNk1RRnnnlmdqhyKXXb8kThdvgjTK0Zm\nneaDsggKyLnnnstjjz1GV5f+QHV3d7N37146OzvRNI1LL72UW265hTfeeAOA0tJSBgcnZmThRPD9\nJ7fy+p5entjUntITtk7+nasXm4uOwRB/fH0fHziuwRQ26S97S0+A83/8fNaKkFLY9gejpt/ZFx6+\nDZ2DYXZ1+IjFEyQSGm39QeIJjUt/uYbL73nZrDeTraZNt8VPLVMmJda4gBSuMovkhMYqEhoZM/K1\n9AS54MfP8/TWjpSsIumr7hwMc/4dz3PRT15g/d5eU/mBPgH7dY9uSGmrHMh0xoJaAEqMHu1gKEZ7\nf7LtfYEo/9lygBd2dBFJq6L56xd3c87tq/nZMzvZ1xc0f5NuXyRlEJdUaqFonMFwjJPm6CmV1uto\n6Q3iddqpLnYxrUIX+tvaB00Fa7UIpEW5YlaFee+sin1Pd8DsoaezsaUP6ZXs8kVYZ7RhybQyGiq9\nHDW1lKXTdWteuobKvU5KPQ5aegPmbxkzuvWD4SgDFotgybQyhIAl0zI9Ao1m8FnfVr4dXQWKExx+\nFsEkYtmyZXzrW9/i3HPPJZFI4HQ6ufvuu7Hb7Vx99dWm//v73/8+AJ/4xCe45ppr8Hq9vPbaayPK\nODoU6bA81N3+CDUlbvOzJJ8cdCuvNvUQiSe44viZeF16P2fHAR93r27ilkuW0FhTTHO3H00jZfCS\n2Q5DKPYHo6bf2R/OnvHyf//aSmN1Me8+dpopNJu7/ZR5nUTj+qvb0pN6jr5Aao/u6S0HuPnvb5nf\nW3oCLJ1ezoaWPn65ehczq4pw2AQep91UBHrv1M6i+lKAjIFGG1v7zOvb3x/EaRfMrikmENGv44lN\nbUTiCULROB+4+2XOXlSXsr+0QuQ1rd7eyfy6ZJxECtx7X2jima3J+bQ7fWG2tmV2ZCqLnCluI4Bd\nhg+/0xc2n4P6Mo+pCOS11pV6WDmrkrXNeiplNJ6gtTdAQ6UXIQQNFbrA3NMTYJYRfJ2Wpce8bHoF\ndpvglaYebBYrMJbQmFdXwvq9fRn7ABw/q4rXmnu44z/bGQxFKXLZefRTJxONJRBC8I6jp+KwCdM9\nBzCjsojW3mDGeIJuX4RILEGZR7cITpxTzStfy16RdNn0chbVl/K1i4/im3/dzDWnzeYbf32rYAFj\npQjGGGsZaoArr7ySK6+8MmO79evXZyy77LLLuOyyywrVtEmHtbe7p9tPTYmbLl84ZSh9aIQ+Udlb\nnlHlRaC/8E9sauPV3T1cdf9anvnyf5kKoM/wK7++p5cl08p4u23AdDv0BawWQaYi8IVj/MIYqXra\n/Bpz+db2QdM9kI308Qmr3mqnazDCh0+aycOv7DV7kR+4aw2xhMa5R02h2O2guthlWisHBkLUlbpN\nAZJu8cgAbY8/wrrmXpZNL8dht5muob9t3M+i+lIe/dTJnP3D53LWyu8PRun2hXm5qZtPnj7HXC6F\nnlQC9WUeOgZDvLa7J8USKPPoOfUyM+iuD62gusTNZb98Gb/Rlm5fxBRui6aW8uKOLtbs6jLdgbWl\nbo5rrDStmyvufYVILMFZC3XrpL7cgxCgaUlLRVoJFUVOc3xDQ6WXj5/SaKYk2wSm/33ptPKciuC6\n8+bzsfteY3eXn4ZKL589a56uCPU+C5etnMFlaSW5Gyq97O7yZzw38v6XWSyQbEpAb7uLf113BgCr\nv3IW+/uCuiIo0FgC5RpSTBi+UIwl03S/b3OXLoCveWAdX3n8TXObUHRkweLOwTAuh41yr5MyrwOX\n3cY2I7jY1OXXe8lG4LI3oAu6D9y9hnufb+IDd63h34ZQjCU0U6n4suTAr7H4j629tO3tg6aicTsy\nX6++NNdQS0+Qo6aWctt7lpkuBXl+0IW+nmfuTrEI6so81JXp0iga1/A67aYvWgrj9n49xXJlYxVF\nLjuhaBxfOMbre3q5cGk95V4nC6boVoU1C0YSiib48/p9xBMa714+zVxujXEcN6uSh685gYoiV0pw\nFeCsRXXMrinmIyc3mt+rS1Kt3G5/0iJYWF9KLKFx5b2vmgOtakvdrJxVBcBdz+00lYpMvXQ5bEwp\n1YVpsUtv19RyL9XFLk6aXQ2AELrC+MY7F3PBkikApvUJsHR6MpvJuqzc6+TkOdWmdffJ0+fw8VNn\nZ2ybTkOlng3UbnF5yWA1YMYIRkJNiZtvX7KElbMqR7xvPihFoJgwBsMxFtWXYRO6aQ+YpYElI3UN\nyd6yEAIhBDUlrpRRr6u3d5qCujcQodMXRtPgqS0HUjI0ADMw6I9kKoLnLWWY5QtvE3pGjzz+tWfM\n4ZrTZpuFxiAzRtDaFzCFmnQpWOvZt/WHKHbbqS11mznvnYNh6krdFLkclBpC+diZFbx58/mcafjx\nQR+UF4knWDKtDK/TTiASN9s2x5hNa06tHpSsKnax9dYLefzTJ6e07/ev7WV+XYnphoJURXDavBrm\n1ZVS4XUSiScocTuoK9WF7DffuZh/XXc6N1ywkC23XIjHaaem2J1y/C5fWFfedhvTLAH9VUbtotk1\nxRzdUI7LbuNpixvKKsinG1NESn+6y2HjpRvP5prTdaE9pdSD0/gN3rN8OpDqljxqqu6rrylxIQQ4\n7YI/f+ZUXvufc1JSl8+w3NuhOGqqrtCsCWDynljbORJcDhsfO6WR+VNKh994FBw2iuBg66gcahwO\n1+sPx6gqdjKtwsseIz1wbl1JyjYjVQQdg+EUc7vWeAFrSlyUehxs2tfPfmNwU38wSq9fF7pvWsoX\nSxfyPsNNkx4j6AtEUoqsvWXse9bCOl7c2cXODh9FLjtfPG8BN71zMXPrSvA4bca++vn29wV5aWcX\nbX0hc67bhkovrb0B3rAERrt8YUrcDmpL3eztDvCPN/fT3h+izugF1xpWQUWRkyKXI6XnKcdQTKvw\n4nXpikAqt+mG+0ROr1hd7MLjtLNoamrveFenn+NnV6UIRKv1IOMdcvV5i6cwf0oJTrugqtiF22HH\nZhN4jX3KvA6c9uSxun0ROgZD1Ja6eefRU/n82fMA2HZgkKnlHordDjxOO0unl6UI1qmWzCDZBquC\n8jjtVBr3YmpFctuzjHjIsunJAO2s6mJKXA7divQ4qShy4bTbzLEY1cZxZAB3OM5fnDnDmFVByxjB\nZOKwUAQej4fu7u7DQjjmg6ZpdHd34/Fk9y8eCsQTGoFInGK3g8bqYjOHXeaDS7K5hnZ2+LjtH2+n\nlDSQSItAIhXB1HIvS6eVs3lfv8U1FKHfyGe3PjrysNJfb/X1+sMxvvjYRvqDUb520SIA3jD8y1ed\nNptQNMEja1uYVuE1hefp82s4aU41LrvNzJ//yuMb+dCvXiWW0JhRKXPQi2jpCfJKU2p9pGJDEcQS\nGp/93XqC0ThTDAUg3SLlXl1YVRQlhYx0L9WXeShy2QlGkxaBDKhaLQLQhenKWZUcZ3FBzEiLeViV\ngjyOHL37qTPncPKcao5uqMg6EFAIQbVhFbjsNrqMGEFNqZvqEjdfPH+hmYEjlRTAykbdPXTtGXP4\ny3+fyvtXNJjrpEVQnJaWW2G4YKzBY4/TzjNfOpMHrzrBXCYzfcq9Tsq9TqqKUt1XT153Oi/ccFbe\nAxvLLb+BdNcta0gqnpJRWASFZvK1aBQ0NDTQ2tpKZ+ehPWvSSPB4PDQ0NAy/YR7s7vJzwR3P87fP\nnWrmavvCMd73i5f4znuXcbzxErb2BphSljSzDwYpXEvcDmZWF/GkUZ+92xfh6tNmc+WJMznn9tVZ\nB9B8++9v8cKOLs5dPIWT5lSnrOsYDHPavGTwVroQppR5mFNbzP0vNRM3pH6vP5qSzVJf5uG0+TWs\nnFXJjX/aZAaOQ9EEH7hrDafNr2H19k42tPTx7UuWcMb8Wr775FbW7+01/ckuh41ILME7j55qHvfr\nFx8FwPHfeYr+QJRNrf28tLPbXC+Dy0unlxGMxvntq3uYU1Ns1vwvcTsyxh7I+ECdxSIAMoSY3KbI\n5SAQidHWF8JuE6ZFIYVtlcVl8/j/O4X1e3t57y/WAMlBUtmQgdmfXL6cbe2DLKovY1F9GZ89e37O\nfapLXLQPhGisKWJnh48DA04zpx5gTk0xe7oDppICWDmrknuApdPLzdHaEtMiSBOw5V4nNkFG3v2c\n2lSrE3S/fWWRi3hCo8iVdq9LR97h+tNnTuHhl/fw1v4Bth0YxOu0s+q6M/jxU9tprM5e6mMiOSwU\ngdPpZPbs4YM4iuw8/MoeIvEE/9rcbiqCbe2DbD/g45Vd3RzfWEXHYIizb1/NF89bwKfPnDvic6xt\n7sFhExw7U+9pSkVQ6nHQWF1EbyBKe3+IQCROTYkbr5E1ks01JIOCa3Z1pyiCoFE4rC6La2hquYel\n08vNQKrLbqMvEEmxQGbXFPPDS48xB4FZc/rX7ek188i/+c7FfPTkRiKxBELoBcvm1Xmx2QS/u+ZE\nBkMx0wVhpcKrZ7H88Y1WvEx3F58AACAASURBVE67OUZCCtpzj5qC22FjMBTjE6c08tNndurX63bw\nnmOnY7cJ7nx2J239ISoMC0C6wcqN3m9lcaoiqDbcMx6nnVBUT72sL/OYA+mmV3ipLHIysyq1119u\nCWgOlQUlU0rfbfje86HaUM7z6krYfsDH7i6/2eMHXTk9u60zxSI4a1Edt757iRnstWLGCNKUpcNu\n484rV3BMmuJI2ddQEt++ZAnFbgfhWByH7eA7OitmVrJiZiVX3PMKAF6nnYX1pdz14ewDJyeaw8I1\npBg9iYRm1mWptgTgpM9eZrH88802IrEET+VINQS44z/beXpL5npN07j+0Q3cYMkGkn73YrfD7A3K\nQUPVJS4zfTAYyVQEMnibPmOTnHsgm2uovtzDibOrqChy4nXaOW1+Df5I3CxtANBojPCUvetuf2Zt\nF4dN8N5jdaHnctioNwRxrXHvVjZWZVUC8rj9wSidg+EUv7UUpqUep5nTf97iejPIXOLW3RYfPmkW\nt196DKUehzmQSV6rdINIF4+MSUhFIf36u7v8Kf51m03w5BfO4FNnJtNDIV0R5LYIvFmyjYZDVto8\n0cjqica1lN8sPZAN4LTb+MjJjSk1lMz2VWR3DQFctGxq1nEFAGv/51xWXa+naJ44p5ql08s5blbV\nkIpjpMhnyTOK+zSeFNQiEEJcCPwEsAO/0jTte2nrZwIPABXGNjdqmvZEIdukSNLU6eO8O543fe3p\nIy4h2Sv+64b9AKxv6dOHyWcJeD3wcjNnLqjlnKOSvbaP3fcaLT3JUZb7+4JMq/CaI21L3A7qDcEk\n6+rUlrhNQRbKUmtI+rk3tvYxGIpSarRFZoKkBIstrqEpZR42fPN8AB56uZlntnawp9tvunNkzReP\n027mwKdzxoLalF73NafPYfX2Tt6/YvgecbnXxb6+IMKYxOSRa5fxalMPLkua6X+fNY/6cg9LppVR\n5nXQ5YukuIVOmVfDppsvML9L60cKnErDNTS/rpRN+/rNeysVwc4OX8rvA5jbWJEpjnIE71gi3XWn\nzqs2xwBY3S/nLq5j076ZZtrocMyuKeYTpzZmDIwbjtpS9/AbHSSyXEi29NzJRMEUgRDCDtwJnAe0\nAmuFEH/TNM1aW/gm4DFN0+4SQiwGngAaC9UmRSqb9vWnBFytaYvSIpDpjBta+jh9fg0v7Ohizc4u\nLlw6NeVYmqbhC8Uy6t6sTuu1v7Cjkw8ePzPFNSTdEikWgSO7a0jTNNr6Q1QVu+jx64FGqQg2tuhB\nW2sPdk5tCULAwrS0O/mCNncHmFNTzA0XLkxxT8yqLjZLWUu+dtEiLl6Wet1Xnzabq0/Lzy1ZUeQ0\ny0dPr/Bw0pzqjBjH0unlZm+/1OOkyxfJ2tOVzK/Tr0/WuplZXYTdJji+sSpFEUhXmz8Sz9lDtuK0\n2yh22VOC3lb+c/0ZZjB6pMyrLaHU46ChsojZNcU0dfpThHJdqYfvvm9Z3sdz2G18611LRtWWQlNZ\nlFSok5lCuoZOAHZqmtakaVoEeAR4d9o2GiDz1cqB/QVszxHDhpY+rrp/7bBT7MkBU9JPap1rVWbx\n7O8LmvVNZAB0S9sgkZheU0cSjiWIJbQUn3tfmv+9vszDc9s6icYT5rpit4Mil557vsEQ5NUlbmw2\ngctuIxiNm9exZmcXl979MoFInMVGmqMM9kbjCX7zUjPHN1amBAMX1pfyxk3npWRtQLLnvLvLT0WR\nk7MXTUmxcmTmilUGvnv5dDPnfzTUlLjp9ofpC0TMLJ+hkCNQZbXPbBw1tYw3bjqPxcbAvOkVXtb9\nz7lcvExPYZSuK6sLZyhXj5WKIlfObedPKeWoqZkDsfLh/cc18OJXz8bjtJtpnHXj0DufCORzdiQr\ngulAi+V7q7HMys3Ah4UQrejWwOcK2J4jhr9v3M8zWzuGnZRlf58+49RLN57NnNriFItgb08At8NG\nLKGZZX9rStyUuB0MhKIsuOlJvvm35ETf0uff64+yrX2Q9v6Q6V66eFk9X7/4KM45qo5nt3Vw1g+f\n4wuP6IXNpNvDmkkhXRFup417n2/izB88SyKh8fgbrSmFvwAz/fO5bZ3s6wvyqTMyA9npAVRITbOs\nzJJpI9sjfe9ywNHBUFvqJhrXaB8ImT3FoZCWzlAWAWReX2Wxy3SPyUC01TVh9b0PxZcvWMCnRpEY\nMBx2mzBjEMtnVCBE6riAw4l3HD2VL5+/YFzcUAfDRAeLrwDu1zStAbgYeEgIkdEmIcS1Qoh1Qoh1\nR1KKKMCaXV08bqlznw/SpbFnmGkM9/cFzaBlmcdpxghaegL0+COcMLsq5Xgy31paEg9bZrSSE5T0\nBiJ88sF1/GDVNrOG/BfOWcB5i6dwyTHTjMyVZDZOqVsXCEdbeuwyUOxx2klo+ujaLl84pQKl7AHL\nAWGb9vVjE6l1f4bC6hevyKIIZhoWgXR/VBe7cRxk2qwUBpqWXTmlU+bVFcBwiiAbM6qKeOTak3jn\n0XppCK8zeYx5WdIns/HeYxsyXFdjzRUnzOTRa09OyfQ6nJhW4eWzZ88f8eRK400hFcE+wFqNqcFY\nZuVq4DEATdNeBjxAxpusado9mqat1DRtZW1tfsO8Dxd+81IzP1i1Ne/tEwnNLEu8p3vo+Xr39wdN\nf3GZV1cE/cEo77nzJYpcdq48YSaQnNikokgfeWkV5B+77zUef72VwbAukAOROC29Abp8YbNuvowB\nHN9YldHzKzbcHl+5cCH/77/mcu0ZyewVqzn9xt6+FMUmLYLegB4n2NY+QGN1cd5zF9SUuDl2ZoV5\nXelIi0CWMR4L10WtJSsr2znTkUoyPS0yX06aU22O+bBaBJOpd+px2s0Oh2LiKGTW0FpgvhBiNroC\nuBxIL8O5FzgHuF8IcRS6IjiyuvzD0B+I0uWLEE9oGZOoZGN3d7Lq4XAWQVtfiKMbdGFY7nXS0hNg\n875+uv0Rfv2xlZxqDMza1j5obKOXaWiyVLtcvb0TfzjGDRcuMpfJ+WubuwNMKXOb/mmbTfB/Hzia\nV5q6ufNZvXKn7GW7HXa+ajkGJFMgAf66Qe9D3HnlCnr8YebUlGATcPfqJm775xbcDtuIs0bOWVTH\n+r19WWu8y3ICMj4hR/IeDLWlSSugIp8YwUFYBOlYFcFk750qxp+CWQSapsWAzwKrgC3o2UFvCSFu\nEUJcYmz2JeCTQoiNwO+Bj2tHSp2IPOkN6Eqg259f+VmZlVLqdvD0lg5ueHxj1kFZoWicbn/EnM2p\nzONgIBg169MsnV5uplHKomp6RU9nRmbQ+pa+jMnJB4JR9vb4U0aMApw+v5Zrs/jxs2Ht3T+9pQO3\nw8Z5i6fwkZMbsdkEFUUusxBbOJYwK2nmi5xNLD0TCJK95q9csBCnXYxqdGnGMUuSxxjLGEE+jHSW\nN8WRRUHHERhjAp5IW/ZNy+e3gVML2YZDHZkV0zEQNoVRIBKjLxDNmga4q9OPTcDpC2p4YlM7j61r\n5aJlUzlrYWpvOb3mTJnXyUAoyq4OX0oFyboyDwMhH8UuOy6HLaNyosdpIxRNmOWbJf3BKJF4ImvZ\n3PI8y/BayzhH4glOmF2VknNfkaaUrBUy82FquZfm770j6zohhLnOH46llK0YLbIsdiSeyBqXSEcq\nC2kZHAzSIlgxc+wGSykOHyY6WKwYAk3TzDRLOWoW4Ip7X+WU7z0D6IFd6TYBzBICHssIzL9t2M9v\nXtqdku7Z0pumCDz6rFpv7R9gTm2x6T6QLhEpvNMHkn3s5EaKXHaes5QIBl0RdPnCKeWCreQzwCZ9\n4vp0pWL1s9sEZv79WHPDhYs4ZQwUgRDCtDQqi4dXhpcsn87Przx2TKyR6hI3P7/yWO77+PEHfSzF\n4cdhUWvocMUfiZtZK9ZSCHLgVDyh8ZOnd/D4662cc9QUStwOWnuCNFQVcenKGbyxt5cSj4M/r9/H\nn9fv49iZlXT7wjy2roX5dXrveb5R9ln2Ote39HHJMclJSKQQKjd6sFaL4KkvnsmMKi9Pb+1gZ0dq\nqmosoRFLaNTkCEy+9j/nEo8P7QWUo4+ddkE0rplzEEtk2uf7jp3O9ectOKgc//GipkQfXZxPjKDc\n6zSzfsaCsTyW4vBCKYJxZneXnzKPI6WuTy56LW6PjiwBzcFQlBeMCVLe2tdPsdtBa2+Ak+ZWc/Lc\nap77yln86oUmM+vnD+ta+O2resrnml3d1JS4zXbInn48oTHXkmduVreUFoElr35OTTE2m6Cu1J2h\nCCS5yhOkV9PMhlQERzdUsH5vLytmplsE+rEbqooOCSUAeuzB7bCNqkaPQlEolCIYZ65+YC0nNFbx\nvfcfDejZMFXFLk6fn5kWa51Z6+8b99PaG0ipyLi2uZcDhqXwtT9tMrN5rNUiP3TiLOrLPXz+9+v5\n4xvJ8QiDoRjHzEv6i61+e+uIUWkRSDeMtAjKPE5zEnA5eMnj1CfzGAxFzZr+uVxD+eAzUlK/euEi\nEpqW4VeXPvR8R8pOBhZMKTXnQ1AoJgtKEYwzB/pD5tyzr+/p4QuPbMBpF+z4zsUZ28pJTAB2dPjM\nKqGSJ4wa/kKQktJpFYxel513Hj2NH/17O01dfubUFjO9wssLO7pYaAmuSgHvddpTAstT0urdS8vB\nqjhkYLnE7eTCpVPwhWL8xShSlz5H7UiQk9IsnFKaMtmHRLYpfeKUycz15y3g8+fkrtWvUEwEKlg8\njkRiCfyROAOhKImExo1/3ASkCtWmTp9Z0sE6aQrA9ecuSPn+ZmsfFUVOZqW5RbIJRll/Z9n0cnOi\nGWshtiXTyvnc2fNY/ZX/Mnv6YIkReFNjBNY215qKwM5t71nGNacnB4UdjEUgB3zlypqpL/ciRLJ8\n9KGA025TqZyKSYdSBGOApmnc8Z/tZg5+LmQG0GAoxlNbDrCjw0ddqZvBUMycZvN//ryZTz/0esr2\n7zpmGi6Hjc8a87lK9vYEqClxp7iLILurRPr9l00v56yFdbgcNo6zBF9dDhtfOn9hxlD/jKwhb6ZF\nIF1DMt/duq7qIEoY3/+JE1h13Rk5B0C965ip/Pkzp5r1/BUKxehQiiBP5KxV2egPRvnJ0zv455tt\nQx5D9vAHglHuX9NMQ6WXq0+bTTiWoN+o89PU5aOpy8/e7oBZR+dHlx3DllsuxG4T/OJDKzjXqCcf\njWtUF7vMCd9/8aEVvOuYaVnHF0hlsXR6Ocsaytl6y4UZCiQb0yu8XHLMNE43aviUZbEIpGvIVARF\nydIIB9P7Lfc6U9xX6bgd9oxpCxUKxchRMYI82Nsd4MwfPsvvrjmJk+dmFuGS2S39wWjGOiuyhz8Q\nirGr08eZC2rNwG77QAi3w24Gf1fv6KQvGKHU7UiZI/jiZVM5fX4Ny27+N6C7Xj568iwW1Zdy8bKp\nWUfJgl4FMRCJmW4hWx7lKkAvAfHTK441v8vRrmVZLAKZCVTicmATBxcfUCgU44dSBHnQMRhC06C5\n259VEcjaPn2B7IrgR//ZznGzKs1pF33hGOFYnKpiN/Xlem+6rT+EICmcn9/eqU9RmCVIWmwI2oSm\nC9uGyqIh55UFvbf+8VMPfl7nrMHislSLwGYTlHqcBxUfUCgU44dyDeVBwBDg6TV2JLIWf38wc300\nnuDOZ3fypzdaUyZqicY1qoqd1Bv+7QP9IbNa6JyaYt7eP8D+vmDKtIsSKWjh4IKxo8HjtPGpM+eY\nE58AFLn0OXXLLUHd6mKXqSAUCsXkRlkEeRA0irZ1+7IrgkGLRfDgy83s6Q5wxQkzmFdXyv6+IPGE\nxv6+YEYWUFWxm7pSN0LoFoF0MZ2xoJYHX24mHItzxoLsZbfLvA76g9Fxd78IIfjaRUdlLL/7w8eZ\nk6AA/ODSY/IqtaxQKCYepQjyIGhaBNkrgPoMAb6/L8g3//oWoAeX7/zQCsuUj6EUiwD0XrPTbqO6\n2M2BgRBdNkFFkZOjG8pJaNDli6TM3GWl3OukhSDVxZOj153uMjsuS7E5hUIxOVGKIA9Mi2AY19B+\nI7OozOPgqS0H8IVjprunfSCUsb9MrZxW4WFHh494QmNWdbE5Xy6Q8tlKmekaUgFZhUJxcKgYQR4E\n02IE8YTGj5/aTodRp18GiyWfOnMu4ViCf21uNyeHiSc0dhwYTJkMXSqCi5dN5fU9vWxo6eNdR09N\nqeGfXs9fUjZBMQKFQnH4oRRBHkiLoMcfoccf4YUdnfz4qR387xNbgGT6qOTiZVNZMKWEH6zaalYK\nBXhr/wD1luCv9O9feeJMSt16wPWKE2ZSXeyi2ChKlj5qWCJH26oUTYVCcbAo11AeSIugrT/ECd95\nyszFdxs1/9MtgqnlHn502XLec+dLHBgIM7XcQ1t/iFhCY2ZVEW39ITxOG0WuZAG3Oz64HJstmYI5\nq7qY1t5AzoDrlDIPZR5HXlU8FQqFYiiURZAHMn0U9Dr7Lzd1A1BkTLzutyiCiiInHqedpdPL+dXH\nVpq9fImcPCU9yHvu4imcvWiK+f3EOVWcOKc6Z3mFa8+Yw5//+1Q1/6xCoTholCLIQUtPgIU3Pcnm\nff2mayidfiMddNCiCKyun/9aWMfGb52fUm3y82fPp8hlH7YGz7fetYR7P7oy5/pSjzOvEhEKhUIx\nHMqvkIOXd3UTjiX48VM7KHFnr5fTa6SD+kIxcy7a+vLs0wre85HjmFldRHmRk1KPg8qDKMamUCgU\nY4myCHJheFw27etLsQiObiinsboIr9NuDhDzhWNMNyp+1mcZCQxw/pJ6FtXrE76cMreGk+ZUFbDx\nCoVCkT/KIsiBnCbywECY9oEwi6eWcezMCj5/znymlHn43O/X82pTNytve4ouX5jT59ewp9vP9CyV\nP9O544PLC918hUKhyBulCHLQYxkFvLGlj5PnVPOd9y4zl1V4nSnzCNeWunngqhNYZgSDFQqF4lBB\nKYIc9KaNAi5Km2y8Mi2ts8TtyDrvsEKhUEx2VIwgB72BKI2W8g6eNEWQPpF6KEdmkUKhUEx2lCLI\nQa8/wrQKrzlgq8iZrghSLYLdlsnjFQqF4lBCKQILTZ0+LvrJC3QMhugJRKgscpkTs3szXEO6RbB8\nRgUXLJnC1y/OLM2sUCgUhwIqRmBh3Z5etrQN8MaePnr9ESqLndSWuNnd5c9QBNIimF9Xwg8uPWYi\nmqtQKBRjgrIILBwwykjvODBIXzBKVZHLHAHsdWa3CHKViVYoFIpDBaUILLQbZaXXt/ShaVBZ7KLE\nY8QI0iyChkovH1w5gwuXZp8sXqFQKA4VlGvIQrthEaxr7gH0+QJksNiWVtzNYbfx/Q8cffAnffEO\n8JTDyqsO/lgKhUIxCpRFYEFaBAPG/AIVRUlF4A8XKD10859gyz8Kc2yFQqHIA6UILEiLQHJMQ7k5\nP4AvHM22y8GjJSCefQpMhUKhGA+UIjAIx+J0+yOcuUAfHXz7pcdQUeQyJ2U/aU71ULuPnkQM4gVS\nMgqFQpEHKkZg0DGg1w26eFk9P7vyWHNO4OUzKnjr2xeYlsGYk4hDQikChUKRhY4tULMQbIXtsyuL\nwEDGB+rLvaYSkBRMCQBoceUaUigUmfTvg1+cDDtWFfxUShEYdBqVROtK3cNsOcYo15BCochGqB/Q\nINhb8FMVVBEIIS4UQmwTQuwUQtyYZf0dQogNxt92IURfIdszFH3GJDO5JosvGAkVLFYoFFmQLuNx\nkA8F83kIIezAncB5QCuwVgjxN03T3pbbaJp2vWX7zwHHFqo9w9Ef1G96uXe8FUEM4sowUygUacSN\nudDHwWMwrAQSQnxOCFE5imOfAOzUNK1J07QI8Ajw7iG2vwL4/SjOMyb0B6M47SKjlETBUTEChUKR\njUQs9X8ByacrOgW9N/+Y4eoRw+6hMx1osXxvNZZlIISYBcwGnsmx/lohxDohxLrOzs48T58f6/f2\ncsJ3nmJPt59yr5P8L2+MSChFoFAosmC6hiaBRaBp2k3AfODXwMeBHUKI/xVCzB3DdlwOPK5pWtbh\nu5qm3aNp2kpN01bW1o7tLGDb2gfpGAyzoaWPsvF2C4GhCFSwWKFQpBEfvxhBXs5pTdM0oN34iwGV\nwONCiP8bYrd9wAzL9wZjWTYuZ4LcQr6wbna19Ycy0kbHBU2NI1AoFFmYTK4hIcQXhBCvA/8HvAQs\n0zTt/wHHAe8fYte1wHwhxGwhhAtd2P8ty/EXoSuWl0fR/oPGWkNo3APFMDGuof5c+lihUEwaEpMo\nWAxUAe/TNO0CTdP+oGlaFEDTtATwzlw7aZoWAz4LrAK2AI9pmvaWEOIWIcQllk0vBx4xrI5xxx9J\natuJUQQxvd5QYpzmPG55De5YDN27xud8CoVidEgFMA4eg3zSR58EeuQXIUQZcJSmaa9qmrZlqB01\nTXsCeCJt2TfTvt+cd2sLgD88wYpAhkXiEbB5C3++gf36f38nVI9lmEehUIwpkylYDNwF+Czffcay\nw4IJVQSaplsDMH4B42jQOJ/KVFIoJjWTaRwBIKxuG8MldNgUq/NNZIzA6g4aN0UQMM6nFIFCMalJ\njJ9rKB9F0CSE+LwQwmn8fQFoKnTDxouJtQisimCcBHPMmHNBpawe2az+ARx4e/jtFBOHGSyeBFlD\nwKeBU9BTP1uBE4FrC9mo8SRgCRaP+zgCa1rYeCkC0yJQiuCIJRaGZ2+Dt/860S1RDMVkChZrmtaB\nntlzWOKbSIvA6hoah1xhAKLSIlCuoSOWmF5pl3h4YtuhGBrTIpgEReeEEB7gamAJ4JHLNU07LGZb\n94fjFLvs+CNxqktc43vyCbEIZLBYWQRHLFIRxJQimNTEJ1fW0ENAPXABsBp9hPBgIRs1nvgjMd5z\n7HTu+chxzK8rGd+Ty4whGH/XkBrNfOQSV4rgkGAyjSwG5mma9g3Ar2naA8A70OMEhzyapuEPx6go\ncnL+kvqJKTgnGa8eeky5ho54lGvo0GCSjSyWregTQiwFyoG6wjVp/AhFEyS0Ak9FORQprqHxTh9V\nFsERi+wExFRnYFIzjkXn8pGA9xjzEdyEXiuoBPhGQVs1TshAcclEKYKJSB9VwWKFsggODcxxBIV3\nDQ0pAYUQNmBA07Re4HlgTsFbNI7I1NFi10RZBBM5oExZBEcsKlh8aDBZXEPGKOIbCt6KCUJaBMXu\ncZ6VTJLIwyLY/QL85TOw/uGxOacaUKY4XILFiQQ8/0Pwd090SwqDHEg2SUYWPyWE+LIQYoYQokr+\nFbxl44AsQT1hMQKrayjXj/3q3bDht/Dsd8fmnKrWkELGBg71Z6B3NzxzK2x/cqJbUhjGsehcPhLw\ng8b//7Ys0zgM3ER+0yKYxMHigFH4dax6BarWkOJwsQhMF1doYttRKMZxHEE+I4tnF7wVE4Sci2DC\ngsX5uIaCPUOvHykyWDxeI5kVk4/DJVhsZj8d4teRi8T4uYbyGVn80WzLNU17cOybM75Ii6DINUEx\ngnyyhgKG/3MsegWapiyCyYSm6b+vqwScnuG3HyvMnvQh/gzId+JwtQgmWdG54y1/pwM3A5cMtcOh\nwmBIv8GlEzFXMQyfNaRpEOzVPx9sr6d1HfzvdAj1GecrsBB46H3wn29mX/f3L8BjHyvs+Q8FXv45\n/GAu3HnCyPc98BbcVg+9zSPfN15Al8pz34ffXDz2x82GvI7oYaoIJlnRuc9ZvwshKoBHCtaicUQq\ngsnhGsryY4cH9F6BqwQiPl0xjHb0c+c2iPqHPt9Y0rEFHDl6uZ3bwd9R2PMfCkgh3rdH7/XZR/Ac\ndm2HWBC6dkJl48jOW8hgcfcOvW3jgekaOkwVQWL8BpTlYxGk4wcOi7iBLxyj2GXHbhvn0hKS4YrO\nyUBxyZTc2+RLeCD1e6EVQXgg6YZKJ+pPXtuRjFWApf8+wxEytg+O4j4WMlgcC4+fzz52mMcI4uPn\nGsonRvB39Cwh0BXHYuCxQjZqvPCFYpR4JnCyNW0Yi0C+5KVToWeXrggc7tGdK5xWJ7CQvYxEXLdg\ncvXUIn7dRZVIgG00fZHDBKsACw9C0QiysuXvORqFagaLC/AMxCPj10M/7C2CyTWO4IfA7cbfd4Ez\nNE27saCtGicGw9Gh3UKxMDz3PXjmtkxBKnn9fvB3ja4BVtfQ7tWw95XU9QEjPlAqLYIhHoi+vfDv\nb8Crv8y+fjwtAnmvclkEkYBeeVXGKw6W0ACs/ZXuOrMSC8Mrd03eoGiKRTDCgr6mIhjFYKpCjiyO\nhXQBnUgMv+3BMlTW0MZHoX9f4dtQSCbZ5PV7gVc1TVutadpLQLcQorGgrRonBkOxoQPFbRvhue/C\n8z+A5pcy1/s69MDng+8eXQOsiqD5Bfj3Tanr5UteUq//H+rF3fgorPkpPHkD+Doz14fSFEEhexmm\nIhjCIoBkIPxg+cd18M8v6QFxK1v/Cf+6UVeyk5EUi2CErqHwGLiGEtGxF9hm/GEc3DWmIgimLg/2\nwZ+vhY2/K3wbCol0CWnxgivWfBTBHwBrK+LGskMeXzhG6VCuIasmzmZ+yvUHNo+uAVbXEGT2YEzX\nkKEIhjLlrb3rgSw9oQzX0HgogmDmOk1LBq1H05vNRqcRnEwPpLe8pv/vbx2b84w10SDYjcmQRmwR\nGIpgVK4hy3M01gLbzOTJ8tuPNbksAvl7h32Fb0MhsXbWCuweykcRODRNM58c4/M4T+VVGHyh2NCu\nIWswN1tv3CqYE/HM9cORvo+vPTUwFOgBBJTUZZ4vHasgyaoI0l1DBXSXyHNlcw3FI8n7OlYBY6kE\n05V1y6v6/2z3YzIQC0Ox8dumW2zDIX/vg7EIZBvGkvEM4MZyxAgG9uv/I34OaaydtQK7h/JRBJ1C\nCHPcgBDi3cAoneKTC194BIogW8/J+uN0bh36ZJqWGf1PH92rJWCwLfk92APeimQa5pCKYAA85frn\nbL7R8QwWy3Nls6KsL+dohFg2goYiiFgUTzQI7W/qnyerrzgWgpJa/fNos4YO2iIY4+egkGMUMs6V\nQ+kMGBZBrhjVoYJV4VuvXQAAIABJREFUPkwCi+DTwNeFEHuFEHuBrwKfKmirxolhYwTWqSRT/Lk+\n+MlyaH4+uax1bfZj3FYPT98C978Tbq3Wg8/m8bNYEdbea6AHvJXJTCHrS/vGQ/DrCyxtGoSqObqr\nYSCLK8SqCOyuVKXUtRNuLoeWHNcQi8DPj4dt/8pcd2tt8poeeh+8eAeE+vXv0UBmANeqCMbKIogY\n1xaxuALa3jReJJHbIggPwo+Pht2W3zERh1+cApv/NDZtS+efX4IfLdE/x8JQLBXBKIPFo4mzWIX0\ncD33x69OfWbzPfZ4WAS5soak4o9MsGtox1P6ezPaezGOE1cNqwg0TduladpJ6GmjizVNO0XTtJ0F\nbdU4kEhoukUwVIwgV56/v0OvfNj2ZnJZthGeibgeyHrhdmjfpC/b+s/U9elYhVbEB+4ysBvKytqT\n279ed31IQRsyLIKyaUnT2EpoAJZdCu+9BxZelHo9W/+h/3/7L5n7ge566doOe19OXR7x68d5zqiM\numcNvP3XpJDSEpkPsLWXNhYWgVXRWI8tf48pS3Mrgu6d+mAu6UICXbB2vJWZwTVWrP1VUlHHgrqi\nF/bRB4tHE2exWrfDWQStr8G+N/I/di53TSHIaRFIRTDBFkH7Rv29kR2jkTKZXENCiP8VQlRomubT\nNM0nhKgUQtxW0FaNA7LgXOmQriGLoLY+bLI3be3dZnM/WB+AhCWwLINY2RSB9TiRALiKkwFF60sb\nHgC05AsXHgR3KZRNz+0aKqqGYz4IdnfqsaSP3VuRuR8kBWy6QJXnsbt0V0wsqCs8X0fmvuY1WXpp\nY2ERWHvE1t9DCtsZx+vtTLdMINn+/jQrDMYnrhAL624/d+noLYJoYOQlFqwdiuEEdiQwMhfLRLiG\n0gPTpiKY4BiBVESjvReTLFh8kaZpZkqKMVvZOBUTKRxyUpohs4ZyBYvlj2IVatl64VZBFw3CtGP1\nXvJ+o4eV7hpylWZaBK5iXXBDmiKQ7hB/8ru7TFcE6a6hREJXHO4y/bvdmdrDkMLUVZp5DZAUNOkK\nRp7HW2Uplx1LdbWkvwSRMbYIUu5XmmL2VkL1fF1BZXOhyH2tx5BtKnSmkWYocYdH/11GEyx2Fuuf\nR3ofRxIsjvhH5mKZDBaBfE6jE60IjPOP2jUUB5vhDSjw6OJ8FIFdCGEOZxVCeIFRDm+dPJh1hvJ2\nDWUxp+UPXT4ju1/efEEFoEHjafpX6YpIDxaXTUsVStEAOIuSrqGUXvxAahukoC+fDgNtybzjsA/W\nP6if320IersztYchg63p+djWdkBmL1kqv6LqVGHUYnGrRAPQ+jq0b05tr8Oju9ae/6H+1/Rc9nMP\ntsPOp7KvgzQLymoR7IOyBv1+gF7gLb3nKIV9IS2Czm3J2ItVCcZCuoBwesBTllTs/m7YNsxEK3Lk\nduWs1DbnS77BYunaHImLZSzLV8Rj+viYXDn02ZSOpo3OImh9HTqGSfgYKVIRjTaVNh4FV5H+eRJY\nBL8FnhZCXC2EuAb4D/BAQVs1DuRVcC4lWGx9edJcQ5WNulBMf2DlC2ozylyX1EPFrOQDJ11DlbNh\n9pn6cbp2JPeP+PWCc9mCxVaLIJFIuoYqZukPTV+zvv75H+iD3kAXOGAEiy3Hkn7mXC+OfJDTr1EK\nUG9FqjCyKrhoEP50Daz6uvHdOMesU5IzTD1zK/z9uuznvue/4OH3Z3ftQGrxOqsLY2CfrlinLNW/\nv3A7NL+Yuq9UZNksAn/n2AizO0+AX5+bej7QhatpEZQmff7rH4TfXzG0EJO/fcVMo80jDBjHQuDw\nGp+HuEZ5P/MVqIl48rcfC4ug6Tl9YNjeNdnXZ7MIAj3Jc49Egf3qbPjFiaNqZk4O2iKI6h1BKHjh\nuXyCxd8HbgOOAhYCq4BZBW3VODBi11B8CNdQZaP+QwXSsmqlULEZ53C4daEpX2R5/KtWwcf+Bg0r\n9TRUMx3Sr/cIZIwglh4jMLaJ+jF7/A0r9eWyF2q9BtMicKW6hoYLrklLIRHVBaS5n9GjTsST1zp1\neeq+fS3Q05TsfcuX410/hZs69b+VV+UuNyHTaXO9CPIlszlSXRj9+3RroHou/D9DkKQHZOV1h/qS\n7bIqtGzuvoPBajXK63W4UxVBsBfQhh4MJZ+f8obU7/kSjySfhaEEjHwe8nWxxEbgcsoHqeByuemy\nZQ3J37R0Wv4KLFcn42A52BhBPAZOb/JzAcm34tcB9MJzlwJnA1sK1qICE4sn2NPtx5fPXAQ5g8VS\nEVgsAsh0J8ietjBus9Oru2+sWTWQVBQzjLr0+4xSCVEZLM7iGjIHbfmTbiJPGdQt1q2IVmNUrYwL\nWD/bHMlr0DTLAJwcwsdq2lqFmdwvFkxe64ILU/eV5R0GjICtfDlcxeBw6X8eQzkO9ULmMq/lS+at\nSh47EtAVU5nhFpLXnS4Y+vcl4y/SurG6uMY6YGxVLFLZyxiBfCbCWVJh05G/vby+kSqCWDipCIYS\n2LIN+QrUlNjDGFgE8jqHUwRaPPk8y9+sZr7+buQj5Meq5lU68v4djEUgLbeJcg0JIRYIIb4lhNgK\n/Ay95pDQNO0sTdN+XtBWFZDfvbaXc25fzZut+o9f4R1KERha2O4eOlhcZVTlTg+mmgFUQ6GYL/1A\n6nJZgXP6cbrSaHlNf7DjET0gmB4s1rRU15D87C7V3VANK5NxCGsv2J3mGpIzZMmXNmfZaGuPyyLM\nzKBcMFkgb4ExtsFbqf9vMhRBLKTfD3nPXMWWdpXq93oo4ZFLEci2FVUnBZZsoxSU8lxWiycRh8H9\nMM2wYKSCs6ZjHuxAtPRevfV4IeN+pWcNSaU+VKaO3NZUBCMMNMetimCoe26ZzS6f9EWrxToWk8XI\n68qlkK0dI3kdUmnULtSfqXxcKoUacCjvX67Y23AkrBbBxMUItqL3/t+padppmqb9DL3O0CHNSzu7\niCU0frOmmZlVRdSVDTFFoFQErqK0YLHxo8gXvdJQBAP74IFLksE+c75hY990N4A8vrQI3KVQt0T3\n6//pWuPclvTRtb/WB/hEg8l933go6duUgr7hBD2N82crU1M55XnsLkDTffR3nZJcH/HrBev+t0FX\nRpEA3Hs2ND2b3Ea+NNagXDSoX6urBKYeo7dDFsrreCu570Cr/nLYHMlrktcNSSG4/rfwyIdSzeFY\nUB+g85uL0yy1kJ6H7ynXe4AdW+A+QxmVpysC4/cK9cNPj9XvobTCHv0o7HpGV1bS9z4ai+Cpm5PF\nA637x2NpsQirRVCavPb0bDBJ25tw9+n6dqZraAhFEB6EX56hF04EXUj/6jw9oysWScaLhnQN+bN/\nzkU+FsH2f+uDK/MpoiavM5egtiqeu06Bva/q99jmTFrp+bR7tJbf+t/CY1ln8k09t+xEtm3Uf5N8\nLThrsHgCYwTvA9qAZ4UQ9wohzkFPfzlk0TSN1/foPbFILMEZC2qG2cF4WJ3FacFi6VYxBFKxcZxA\nT2o56fRsDkdahojcX1jmTL7gO/p5txujeF1FuvsE4MAm2PHv1AdpuyXDRCqC4z6uvwjdO3TBWDIF\nLvw+TF+hr5czYbWuA98B3Uc/ZZn+4O56Rh+p+8pdum9/3+t6ZVSJ7Dn3NOkCyOYwLIIe3T1js8P7\n7oH/+mpyn9qjjH336+dwFqcWiJOlMeR1Nb+gD7zrb0luEw3qVs6el1Lvqwy4uor1Y3ds0WM1R10C\nM04yrtel32PZQ+vbqw8kq10Ep3wBzrpJv+Z9r+t+6fIZujAZaU8bYOfTuoCwKkrQlZTV7ST937Jz\nEA/rz1UuRdD6ml4yo29PcnxKca1+bdlST3v36IJHFt7ztevH2Puqfs/kszKka8jShnzGEuQTI9jz\nkv775pOSKu9FrliNVTj27dVdqgP7oWyq3imB/BSBtCKkGyZfdj+vK7ZcpMcI9r2h/yZ9e4c/diIO\naMlgcXqG4RiTUxFomvYXTdMuBxYBzwLXAXVCiLuEEOfnc3AhxIVCiG1CiJ1CiKxzGAghLhNCvC2E\neEsIUdC6sXu6A3T5Imam0JkL6obeIZdFkO6vk/ncUnCkBP4syJc+NKALCtM1ZFEEc86ElVcnXzxX\nSWrvOTyYe6Si7FmXT4d33K5/7m3WXQgnfTopfOXxenbr/8+6Sa95E7EIq6KqpCCTVkVJffKllAJm\n9plJi6DIcActvAhmWDIwZK+7vzUZAM/W7pTRspoumCXRYPbRtLGwfl9dRclMHIDzbkkqUCGM6T4N\noSAF50Xf16/7zK/ov2GgJ1nWw1k0OvdGsFe/F927MlNbw4PJeFHQ4hqSL3vEn5oEYEW63qwWgbs0\n92C09GdQKs9AtxEsLoBFEMvDIpDPVz7Hk79TttRsyGx7aEC/52XTk1ZgPgpMPueyQ5IvwR7dUs3l\ntjFjBDKLKe35Gwp5TLPO2MSXmPBrmvY7TdPeBTQA69HrDQ2JEMIO3AlchF6e4gohxOK0beYDXwNO\n1TRtCbqyKRjSGvjaxYs4vrGSU+ZWD72DFNROb/b0UYnDldozyzV7lMOrv7ha3HDvSEWQlrlUZGmX\nsyg5qAQATfdtZ0MKVNB756D3dKUbQCIVQe9u/UErqkr2qGWbvVXJnpJ8kKvnJYVby6u6MJm2PBks\nlueEpG8T9G1sDv2Fi/j/f3vnHi9nVd7739ozs+eys2cnO9edvQO5EAgQbmkIoNwMogIieMCKWhVL\nVVA8xVaOqEeqPdXTemw/lRZrgWLxeIEDhZparHKUo1Q0EBEIAYEIhCTkRm472feZWeePZz2z1rvm\nfeeys2dPdt7n+/nMZ2beeed913tbz3quK+gfAGyn5JdWZmEDGEEQUnGzrBGYjp4f/JQnbNpzweQ7\n/3zlZtJ+h/bQ+Uhlxle0jIXU5rVestsg7ZenHXUFgdtpRVVu5e2OHHDan68iCLyJa/h9aE/9zmK3\nDRNlGuJrW8+5desphUW0+YJg5AAJDVcQ1KN58OCmUYese03CKPsIRoLf6zEN8SA0dfjkEZTRWu/V\nWt+mtb6gjtVXAdiotX7JlK6+G4A/g8uHAdxqspWhtW7qjOa7B+iCXH5qL+699g3oqDVpfflidFTX\nCNpS5NAd8QWBVwcmmQ52emXTkHcZ3CkL2zvImewKgyhV2e3wXWHidniAjULa8zLF2itFxzg2YNuc\nSAX3k8xQuCJ3blseJ6d0Kmeqpu4IttvtiLsWUDhf/2s2Sc6F27frBeC1J21Hv8UXBI6gGB2kOPPC\nMJ3XVI7az6P4lOf7YUEHOB2pMwLMzaD9Du6hc5fK2s7s5UeAdXdSol4YO5+jczk2bB/2LY9VmoaG\n+21JcfYRpBxB4Dr+/Q6Mz8nIAXMeFP3PDT5w8QcjLHgGdtF9V00QbHuKjtVtA5+7118kbcdn32aq\nf8XUEgR1mYac4wq754ujCFirh/fTel29lQECW9aFT9i05de23Y1G9/CxhJ3/UtGeg6G9ZEbiY3bX\nL5XIvMTRTaUSsP4+4AmTqsUDqnV3NnV+hWZOGNsLwDHyYotZ5nIsgGOVUr9QSv1KKeXFHhJKqY8o\npdYppdbt2hVyMetkrEgnO5mo09XBI/b2XPDG9kciiRRpBX7khx+WxlFDAK1bKpDt2p9QJesJAiBo\nHopynrFdFAh2ymlPI2ChwmYj3s/oAPkMAOrQ3I4slTVZy69RJ7ZjA5l/uFM/sC3Ybre9+V6aXOfg\njgiNwHRKD30e+N5V9gFzi/oVhoITtj92G80M1/8atY3bz9FFvrBJ5ZwR2f7gfgFq+95XSMjnZtr1\nx4aB71wJ/OCTVFk1jK+fCdxyqpdd/Riw+yX7nTv5ahrB6EEnY9zXCLjT2W/LiSgVDD11YfMht4n/\nv888kpku0lD93BcA+N57gZ/9pVfW23z+/sdtgqLLQ5+nyqpMTdNQnRoBm2t2h9S5LIwGr+G+TfRs\nds635lq+J+64gO4tn3+5xpaQbzQDmI8lzNTjalC/+TZw16W2EKIrCB79GvDddwEv/Ii+b3+a2sQJ\nmJ09dJ1e/jmwvnnzgbV65vAkgKUAzgfwHgC3K6UqKp8ZLWSl1nrl7Nmzx72zghEEqXonTOcReyoX\nbRpSCbLxJzPBWHAeEbgdEvsIALoZSsWgf4DxNQLA2ruBSpvpqo8CN+8Nbqt9mu3wfUHAnfTB7TYp\niW3sbA4aGw7GbyezpD3ooomK0kDf6XbEoovBdrvCrauXtj82RK+U55Tjh70wTAKFBagu2ogs1zQ0\nuMc65A/uND6CDvr/6EETleSFBbs+glDTULftbPLz6XqODdHomDu1gRCF1R1lcmc7dzlpCVvXUUgw\n4JiGWCNwnMV8jwzstvecb4pxNYLhfqv9uVFoLhUagXnf8zt7jPNPqyyfrjXdFwOvez4CMxrdu4k6\nXB+/+m7U6LqsEdRhahrpB45+Iz1jrnbIFD1BsNe0KzfTMbcN2BH/7heD/y+VKCDhjGuB8z9r8hHq\ndMoWC1bYhgli9/gObKd3FsLu+ht/YtppBKNfN6pjFnDji3RPu8ETE0wzBcFWAAuc731mmcsWAGu0\n1mNa65cBvAASDE2hUCqhTQFtbfVqBKaefTITbRriTjXRHnQW843gjpI5aggwDuNiMGKIyYaYWFzT\nEGsEXMs+121zERilbMccZRoCHI1gGo26+WYbGwyq46ks1e4BgA33A1DGNOR06q45yiWdJ0EyNmgE\nQYRpKIzF55n2eM5i7hgGdwdH1YOvh0d/uD6CYRPt5LY9222jxPJ91lnM+5mxMLymj9tB8UN87FtR\nrgy7+E20bKSf/DWhGoHR5A5ut9vys3kHndHnSL89Z5mIgnX+nMb8fz7Grj5y4m97OjgSZk115ECw\nDaODNBg6uCNYy4rxTTdhGoHWtj31ZCuzBjXvpKC/iCmOBAc5PEDKzbABCaMD9r/dS4L/H9hJxzrz\nGHsv1JsI52r7YYIg4AMxZh8WCO714sGHn1DIJFJ0rTvnN3WCpWYKgscBLFVKLVJKtQO4CsAab51/\nBWkDUErNApmKXkKTGCtqJBMNHHKpSB1Gsj08fBSwnWoy7TiLHUHgjpIDGsEBu32fgEbA5h4nQ5JN\nNhyrH9UBs0CpcBa7gmA+vXPnzD6CsaFw0xBAIaxzjqeRvN+ZhqEUrTc2TMIm6dnvE6no0L1FriAw\nD8nWJxzn597KUbWvcQAkKFxnXbozqLW457Cr1zqLN68lITD7+PAqn5y41zHHdrbHXIiy7XqJEQQc\neZXtpmseEASOeY2piBpyfQQHbAdY01nsRAu55OeTICiNkV+GcTWP0QHrvxodMO3TptSIox0VRoP5\nKu3TwjUCFjJhxxfGiAl0WHAGRZD5o/XiGJB2zKEs5LLdTvjooBUEvgmWO9Z8r70n6/UTuOczTCML\n84Gw2bVcQnzYXnPfzHSUye9hIe0XpJxganhLx4/WuqCUuh5UmygB4E6t9Qal1J8DWKe1XmN+e4tS\n6llQstqNWusJmtG8kkKxhFS92gBAN21bwtTvD0koA2ynmmgPSvUwQZDKWkFw/4dp+7693P8PdxKu\nIOrfarQLY1LhLN6o7VRoBI6ZyTUNufS/FhwdpbJWewBsSKjbgeci2gGYEfYQaUFhHXW6M5iBmTeO\n6YXn0PeCIwgCE+ToSo3AdxQDZDMOlOz2zgmfK9VGApadxVvWURsSKUrSe+A6YJNTvG7A2NiLo/Zh\nnnE0lfoY2gvMOo6W8QOf7qRzNuwklLE56ICjEYwOUr2oh/8CePe3rV+DncW5WXZ7IyYcWSnq+O95\nv71fR/qBb15C82CUUdb2DAD3vM+cPwUsu8T+b3SQ9jOwk0bw7qi/fyv5fQATxeYMVDJd4fZ2V5C6\ngmD7euChm4Grvkvnfc9LwL0fMvkOncC8k4HH/pGSE3tOoc7y7vfSOZ0eUvaMo+AA4P992fojXI3u\n3//UOr27eq2vpFYW8G++Te094XK7LFQQhPhA+DrzfcxTqQKVEUhL30zF9vie6OptbIKgBmmaIAAA\nrfWDAB70lt3sfNYA/sS8mk6hNF6NIBNeYgKwnWoybdXdUsFeWHekmWh3EnlMJxumEaTztFyXnJGK\n0ykP7we6jrI3ey5iJM4Cwu/0FqwCTv8j+ny0GXm4jmbAqtkdsynShMNM3/Q5sgevMpnP1TSCd91l\nj59H2NUEgTvKPO9GOv8dswAoY6owD5c/jwNn5wI0MvVNT4AXNdQfjBhy2z5tHiXcJbM0qjuwjaYA\nHT1I13TD/WRi6DnZ/nfnc9Q58DXPdgMXftHWigLsA53upOPn6TW5sqy7DkD7W38vRUa5ZbhHTB4B\n+07SeVueI5Wl9V1BBVR+75xHgm3abOCCP6NZtADgtw8CT33P7OcAtSE3k67/qBc8sH+r9X/4Jot0\nPnxkPRghCDb9khIZX3+Rzuuv/xnY9qTd1kxj0tm/hQTBrt/aJMf5pwHHvJmWPXMfLct203ld/Xky\nvag26vR3OSXSHr/Dfs73UblwoLZG8Oz3yT+16Fy7LNQ0VK16rBEcbmIZ54nwts78OA04+TnL9wLP\n/cAK/AmmqYLgcGOsWEKq3oghgDqctoQxDdXSCLwpGngEyB1hMmOiPLxOOcxHoBR14mPD9qIXvRuU\nHbBAtEmmrBF4nV6myyacMW7n2bfKjiDzvdQR8O/n/bfo//kC6URn1MQj7FKEIPDNV4vPt2UCUrnK\n0L8ZC62D0hUEA7uozoyPn0dQoREYocnmr1TWjvbTneYeMKPFFR+gBD3mP/+WOq79W0jzSGWApRfS\nb1oDUPZ+yOSDx5/M2MEAr5M0QpP9D+5c0Wwacp3FvDyVjZ47GyDz1cDOoGZ3jjMG++YlVmiw4G3v\nsELUDR5whYJvssjka2sErg2dtZ3+rSQI2BcFkCDg+7ts5nK2k2ynQcPDX6bvbUl7Ts79lF3v4f9J\n82RU5ACZAQ4L5Fo+gv1bqSPf6zjMq0UNtSUrs4L9ZL9pcx2TXL+9h853cnDzvdQHDO62lQwmkFZH\nDU0qhaJGst6IIcCGdybSpAWwg8zVCNocH4ELj+7KgsD87kezhEUNAcbOGWI2YtykmUiNIMI0FEZ5\nX8o8yOZBZdNRmLnFXx4lkADrLC6OhI/YuY0dcyq3lcpa+yoLXHbCAkHfy9hghLN4minjYKI9/HPC\n++NO0g03TXcG29PVG/wvd8p7N1VeC2Xi/csagSsIFN0PqSx95nU655EQ4nmuudwI56oMO85i1jC5\nM3LnX/YHJ3wt/faHHdfYIJ2n9g6bo9H/mnH8Z6KFArcpVCOImFa0XFOII9ac39Kd9pxyZ+kKFD5G\nPg+5meEjZt7G8L6gozffS+uHad5hsKbMA6VEOiJqaNC2x8f333QvCeYk+IMiwF6bJs2cFytBMFYq\nUQ5BYTQYox6F6ywGbP6AO6pwTUMuHAHCHYjvIGXCTEOAsXOGdJhM3omVrqURhN1YUe2Yuzw4YnU7\nxjB43bZUdYHjj4J90nnaBmchu9tyBQEXhFviCgInP8PfV3mZaf/YQHBEzeR8QeC0MZMPdvDsYHfb\nDlBYZZi/JpULFwSpLHVCvrDo7KGOplSgz9xxTT+KOtPCkN0nH8dIP2mQ7n093Q3ag3Vg5iMEgb/8\nwA6jEeTIdPLaE7RO3iQH9m8j84ZbzhsgjXNoD/DM/fRiZzSbztpSEYJgMyV4uR1rImVDoQf30O+u\no5afP75fop4FXr55bTAhjq9lWSMwAmzvJmq7u+7IQRsyuv1palPn3GB7Rw4Y89Gj0e0Z2ke+p6E9\npkDj7KCTPuw54mvTJIdxrARBoaiRSrSRnfe282tP8ec6iwFrnnETylxnsUtZI6ghCPysYmbW0nBH\nGG+nq4/MI53zozWHWcfS+hyyWA1+IM68Ljiidk0lYfC6ue7qtkv3/2FCpXsRMPdEcgzOXBrcVipr\no1J6V9DD49po3YztqLa6maZhD1vHHDKhzVte2cZ0Z3Bk55ou+HeAOsWwEWB7RzCJjc+Z65dJ5azv\nY9pcGwHzRpO8pRLAnGW2I+Dj5Y7m4A4yT5XG7G+zjqX3Mz9G76e8h46RZ23z8TWFg9upvfk+ctBv\neZzMbvleascPPknJZ3tesvvi7QzsAu77EL3++e22CF9bioSbKwhYm/nF12imMDdUtKvPhkK//DP6\nff199ncepLFAjNKOefnd7wV+eKNdzueCnys2aa35BLX93qvtuq6zfOdv6Tqlu4LO4sduo4qkT3yL\nBmrTQvKe9r5MCW6v/pIGDu6c366258La3ERPlmSIlY+gUCoh2WYiK3SRRlpRNw7gaATeaKEU4iOo\nMA2F+AiYz26jaIZH/y7aNHTxV8Mn1cjPpwcv30vF3VZ8ILoDPvZtwKdeqK+Y1qylwKc30QxqrnmB\nRyJR4Z3c6VYzC7nrAeFmpgv+jHwviVTQbg3QuTtokoFO/zCdm4zJTSiYBLVMvYJgIBiHX/49B3zy\nGds5u9cr3WX/35ay+Rvl382+S4Xw0bY/9wK3j+dC4HUGQJ0LX6/uJZTstPQt9Puvvm6FBbd/3nKT\ncLXOnoPjLgKevofi4z+9ibZ3/k1AeycFCfiBAYzfdtZILvkba5KYsRD4wQ1UdmNoLzl493YAy6+g\n6rgAsPpm4NT30f27/l7gka+a0fw6cvYWhj0fgWda2fcqaT8ffYTuR4DuL9Z22GQGOEmTrBHUiKAD\n7PzZVz9ISZFAZfgo+5+2r7cJfG4iZ2mMrt/gnmD7D2yn++Gah+jZ//dPhreH29FzMrVtaI8z5WyI\nBp+bReeD5z6ZYGKlEYwWTNQQj+xrpZSXncWeIAgzDUVqBJ6PAKBOh8P/oqoKJtPhHSY/rPn51LZ0\nxEMNkIBopKIiP3TuaLicZxAlCMy61QQqEBQkYRpBImWm5UxVajipHMrhidnptsMrZ12n6cXXoJog\nGNobrL7pksnbxDx3G66dOt9TmbznCpUw+7vrf2mfZjVKDsF118nPtx31gjPoGs5cQn4Dt83uOZi3\nnBzLmx+jaKKV9aabAAAWlklEQVSeU+w62en2PmhrCx6jT5gQy/fSPTZnGb1SGWrjgW1GOGgyObnH\nkkiS5jBnmW3L3lfItMRlSdw4ez/8cv8W4yR2igzkuq0QdKPGWLNI19AI3IEK/7+rz2oUro+gVKKR\nd88pCFTB5egoDvBYcEZlZvfgHnLmzllG2oA/gHI1Rl2kduVmkgZYLh8SohG0tZHQqMffNw5iJQgK\nJRM1xDH5tcoMu85iwD7AjWgEfDP7piG+oPUU33LhG6mrr/p6hwILoGQ2GP4ZRiJF5q2okVh5m9nw\nz3W1x+uUGfah8Lnl38K0F7cmEhAuCKrts+xMDjnvrjYS1pnyvtOd9ECzo7EvTBA40WALTg9uJzDt\nqHMeFpxBtvNXf0kdsj9FZ73wPZVzolLCBFu+11TQdUb1btnxwDbN/1/4D+pkF5xuy5kwvkbgFsVj\nojp4Djn2TWU+Yf/3kz0BGuwN7KJn/Ph3AFDWVMVmubmmiPKCVZWZ3UN7vGoCXr/gD3JyM4NRUSP9\njV+3CSBegqCoyTRU1ghqFL7yncVljSAsj8DrKIf3mzAw7qy8G4JH6o0Wukp30j5zEx9CVsbtuKp1\nroDJGs7V1ggCNZeqzAoX+t8oQcBmHC9yJFQjMOtyMa9aIyt3G5k83QPt08I7Rndb1UxDrlYC2Dh8\nwJ6frj67nt+5uvtxP/etspVjF6yyHXqjo8fsDLrOHLYLhB+PPwjJziAzVBiBsiSmrTyn9u2rge9e\nFe6r8zvDqA5+1InsAqLvQ98c1pYKLitrBE5G/ZwTKIN+7T8AP/0SaSodc0jraksBPaeGawQBAePd\n6/7znut2oqL2hgcyTAKx8hGMFUtkGirUaRqKdBaHhI+WTURpU4Buv5ldzPzX75zqmTPW5YP/RpEb\nc5eT2t1IGGyjuCPszh7gjTeQ3TmK828Kjm7DcDWKqAikyP+yQ3pW8L+pCI0gTBDMPQE46ffpoe1d\nASw6p8Y+eT/KRme96bPW1OHSXsM0dOr76F7iLOmrHyRzQ6A8gjFX5OcDx15EkSU8sxuz6BzghMvo\neOc4U3ssvZBs9KUisOxS6pjPuBZYUk+1eAelgAtupkiY+/7QHE+IBuRGTZ11PfmXlALe/0Bl8bmO\n2fSM7N5IQqGr155bd+Kh4y6hKKe136DvtTSCk68ibfts40/q7CHH+rK3Rx/bm79IkV3r7qwMM3U1\nAhYE+fnAOX9KOQq/+jolFs4+jmb0O+osuqdzs6jzZ//W4B6a+c7fbm4WZS+v/u/AjmdJEL72GxJw\nZY1gd7RpqMnEShAUShqZlCMIaqWT65JNKAPqcxYn03Sz7zQx2DyS9jWCRi/2onNtpMzRZzX230bh\njjSTp4flwi9WX/8Nn6h/m0C0mSkKPodsM2cqTEPVNIIO4IrbG9ins00Wumd9PHzdRBLl+RzCRtDL\nLqYXs/CN9HLh+QnyvcDsY8PPeVcf8PvfqlyenQ5ceWdw2UV/Fd7WWpz1MTvKTqTDo6DcYzzzOiss\nlqyuXLetjfwq+161pq6y491JtpqzjITQhn+10UouvkbQOZdmoXP3434P4+wbgFd+YQSBtz236Bz7\nArr6yCFcKgIPfIRCRs/+Ewpd5vDlrl7QZFHbSZANeRoBb5fLWHQvpqlk7/tDEgSuRrDvVdpWCwRB\nzExDJUooq9dZXCrYEhNAddOQm09QdrB2UCehEiE+gslX/+qGb96JvCFrOYurwQlGvs3cj/BhlbrR\nuWfDcM1j9ZDupGs6XrWecwWa6fupl1SW7lmetMiHp/Lkuky1YPNQ2dRlHP8nvtOu4zt7o/I8WDDV\nilKLgjU2//8JTyNwhaDrCPfNdXxs/VvJ9zh6MNxHwOu5viDAhI8a/xprU+IjaC5jRe05i+sQBK6z\n+OWf0+igFBI1VM4cTgcnewGoozpUjWAyKQuCCbwhD8VZvGMDvfsPYSOmoUZJedusRSZfmWjWCEMm\nz+BQtjFRKJNdHiWUlKJ2cl2mWvAxsfmQ6/qc4ExY6CeERWkEHO5ZyycVRWeP+b8X3NDWRs/yvldp\n5O8KwRkLbchwnzcYYcGy+3e2iJy77XLeD+fjeIIg1w1kppNQLQsC0QiaCuURNKIRGGcx33SPfBV4\nbo2nEZgHoSwQ2u0DxNm6nfMqR04tcAjVTXk03CRB0OiInQvjzT8tuNwNHwWqm4Yahc9Bvdepqy+8\nxlG9sOmIO6pWU+t4Zi+r/3hnL6PObt5J9J1H2Ee9wY66yxoBF0r0wp7ZgX38pQBU0KHdCMk05WeE\n/T+ZAZ78DhXuc53fSpFZdu5yoMMzlbGQ+9FnKUkMCGoEnT203Z5Tgzkoc5bRcUw/moRQdgZNdwqI\ns7jZFIralJio00fAM4jNXAJc9yhwx4XApke9zGIvasjVCDgd/ZofV4aNtR/GGoE/wp4IDkUjuOh/\nAed/pvJ/rsYFTKxG0Og5uPLO8AKC9XLpLcCbv1BZi6pVvP/71X05l3/dZj/X4g2fAFa83/raVn+e\nnNkdM+lZGdxdWyOYewJww3pKNFt0Ls2DPV6u+XG4eZKnpj3tD4AL/0fwt0u/FiwFz2S66Fl26xe5\nfpUT30kDmWnzaJIlFiSLzgNueNqWTMl22yqwnZOvFcZKIxgrlajERL1RQ5xQBlD5g77fo6zb0pg1\nF/mmoWS7HSVwmGDHrMrOqR6VulU0OhpuZJtA4x11st3WvndhQeCbcSZSI6hXEGRnHNr5ijrGVtEx\ns3Lw4pLpqp07wqQywWNLpOwzwoOmcokI1hBCzjt3mtOPOrRSzB2zwut4sS/q+MsqTU/pzkptgPEj\nxdz/thlfS1tb0NSmlD0e/g8HoUQVBWwisRIENo+gAR+BWxSubxWlhQ/32xvJrzWUzNgL7k9eP1Vo\n1D5eD4mU8be0R5fVaBTfRzChzmIvEkloDtzp+XkArTSd9q1sbH0/Umw8jmz+T3tnY9UAJohYCYLy\nVJWN5BG46v6CM0hL2LfJRqz4ZagTjkZQK2HtcKXR0XAj252I0TrTSEJZoySbEDklVJL3BEEjpdOb\nRaOO6GoaQaP7bIE2AMTNR8AlJhp1FjNznEQR7izDwkfbOyjR5jgndjyMt/1lZQGzw4HpR1OlSrfm\n/0SQyhyaHd1nyWrg1D+wNtWF51DS2KE4bZm2NrqGUQlKwsSw7BKKuOHwysXnm2u4rNq/msNFX6lu\nDoti+ZU0kj/+7ZQHMZ6BCJvZWhQ1Fi9BwBPTNOosZlyVj28YP6GMfQdv/VLtBp15Xe11WkGyHXjn\nNyZ+uxyfPlHMOga4/Fb7Pd/TWNJYLeq5hsKhMfu44DXs6p3Ya9gIZ3x0fP9bfB69ABvh1ij+fBiT\nTMxMQ1x0bhzOYoA6fx75VwgCLtTmVSEVLMnsxJqGBOFIYTKKSVYhVoKAJq8/BGexUo5ThwWBbxpq\nsHxCnEiJIBCEUMrVbVtjGoqNINBao1jyTEONOouBYIp7W9La9pJeOKlQScfs5lZNFYSpCmsCUVVc\nm0xsfARjRapvknI1gpo+glLlnMJsy5s2G7j2P+2Fi5q7WLBcdmvtdQQhjvSuoBnIOPt6komNICiU\nKAuSwkdN6ee6TEOe0sQaQKKdapUzohHUprOOuZMFIa70nNyyXcfGNMQaQVLB8RHUmAtAFys1Ap5x\nrM0rBZBwylALgiBMIWIjCNTzD+LvU19DFiN2Yc0ZygqVgoCTlvzs2HKJCXEWC4IwtYiPINj7Mt6e\nWItsycyPqtrG5yzmbNbCSHB5Kgec9C47C5UgCMIUIT4+AkW2+1zJaAGZLioKp3V0AaswZzGnvvsT\nbre1AVfcMYEtFgRBmBxioxEU20gQZIqmA88YW3+1OYPDnMUsCEYPVK4vCIIwBYmNICiwICgdpAXs\n9A0zD3Hd8TBnMQuC4f4mtFIQBGHyiZEgIGdupmAEQSZCEGxfD3x5PhXCCnMW88xG3Yub11hBEIRJ\nJDY+gjFFgiBdNJNPcL3zouf03fU8TRCxeyPNwOQ7i3tXAB/6IdDbYM1yQRCEw5T4CAIT999eNBoB\nm3j86J/BPfQ+sIvefY0AGH+FQUEQhMOQ+JiGQBpB+5hx8nI+gC8IhnxBEJtTJAhCTIlNLzemjEZQ\nME5eFgRFb0Jq1ggOVtEIBEEQjiBiIwhGjI8gNcbO4loawU56F0EgCMIRTmwEAWsEybJpyPgIfGdx\nWSMwgmAiZ9QSBEE4DGmqIFBKvU0p9bxSaqNS6qaQ369WSu1SSj1pXn/UrLaMgPIIKgRBwTcN7ab3\nso9ABIEgCEc2TbN7KKUSAG4FcCGALQAeV0qt0Vo/6616j9b6+ma1gxkFaQSJUV8QeJnFFc5iEQSC\nIBzZNFMjWAVgo9b6Ja31KIC7AVzWxP1VZaQsCGo5i/fSe7XwUUEQhCOIZgqCXgCbne9bzDKfK5RS\nTyul7lNKLQjbkFLqI0qpdUqpdbt27RpXY0a0EQQ84ucpJ11ncWHU1BByitCJIBAE4Qin1c7ifwOw\nUGt9MoCHANwVtpLW+jat9Uqt9crZs2ePa0eFEjCsU1ClUZpUJtNFP7gawZDRBroceSTOYkEQjnCa\nKQi2AnBH+H1mWRmt9W6tNQ/J7wDwe81qTKFUKpuHkO60U0q6GgFrCzOX2GXiIxAE4QinmYLgcQBL\nlVKLlFLtAK4CsMZdQSnV43x9B4DnmtWYsaIuRw4h3WlnFHOdxRwxxBPSAyIIBEE44mmaAVxrXVBK\nXQ/gRwASAO7UWm9QSv05gHVa6zUA/qtS6h0ACgD2ALi6We0pFEsY1iky/2fyViNwTUOcQzD3RLtM\nfASCIBzhNLWX01o/COBBb9nNzufPAPhMM9vAnDi/C5lcBzC8iyKGlCJhEGYaWnSuXSY+AkEQjnBa\n7SyeNM5eOgtzppuQUQ4dTWbCNYLOHprTGBCNQBCEI57YCAIA1PEDNpksTCNIZoH2nJ1vQJcmt42C\nIAiTTMwEgXEQsyBIpoOCYHAvkOumz0tW0/vw/slrnyAIQguIl90jlaV3rjyaaA8WnRvaA2SNIDj3\nU8C0OcDy/zK5bRQEQZhk4iUIamoEu4HcDPqcSAGnXzO57RMEQWgBMTMNsY/A1Qg8ZzFrBIIgCDEh\nZoKANQInash3FudEEAiCEC9iJgiMjyDMNFQqUa0hLkYnCIIQE2ImCIxGEOYsHtlPoaJiGhIEIWbE\nTBB4eQTJtJ2hjJPJxDQkCELMiLcgcDUCFgSiEQiCEDPiJQhSLAjMXATJNPD6C8A/vQXYt4mWiUYg\nCELMiFcewfIrSCvoMA5h9hlsXgvMXEqfp81tTdsEQRBaRLw0gvx8YNWH7fdE2n7evJYKzXXOm/x2\nCYIgtJB4CQKfpCMIdr9I2kAi1br2CIIgtIB4CwKenIbJ97amHYIgCC0k3oLA1QgAoEsEgSAI8SPe\ngsCfdCbf15p2CIIgtJB4C4LhfcHv+fmtaYcgCEILibcgGNhN76wJiGlIEIQYEm9BwCWoe0+jdzEN\nCYIQQ+KVUObz1i8B048CzrgWmLEI6Dml1S0SBEGYdJTWutVtaIiVK1fqdevWtboZgiAIUwql1K+1\n1ivDfou3aUgQBEEQQSAIghB3RBAIgiDEHBEEgiAIMUcEgSAIQswRQSAIghBzRBAIgiDEHBEEgiAI\nMWfKJZQppXYB2DTOv88C8PoENqeVyLEcnsixHJ7IsQBHa61nh/0w5QTBoaCUWheVWTfVkGM5PJFj\nOTyRY6mOmIYEQRBijggCQRCEmBM3QXBbqxswgcixHJ7IsRyeyLFUIVY+AkEQBKGSuGkEgiAIgocI\nAkEQhJgTG0GglHqbUup5pdRGpdRNrW5PoyilXlFKrVdKPamUWmeWdSulHlJKvWjeZ7S6nWEope5U\nSu1USj3jLAttuyJuMdfpaaXUita1vJKIY/mCUmqruTZPKqUudn77jDmW55VSb21NqytRSi1QSj2s\nlHpWKbVBKfXHZvmUuy5VjmUqXpeMUuoxpdRT5li+aJYvUkqtNW2+RynVbpanzfeN5veF49qx1vqI\nfwFIAPgdgMUA2gE8BeCEVrerwWN4BcAsb9lXANxkPt8E4K9a3c6Itp8LYAWAZ2q1HcDFAH4IQAE4\nE8DaVre/jmP5AoBPhax7grnX0gAWmXsw0epjMG3rAbDCfO4E8IJp75S7LlWOZSpeFwVgmvmcArDW\nnO//A+Aqs/wbAK4znz8G4Bvm81UA7hnPfuOiEawCsFFr/ZLWehTA3QAua3GbJoLLANxlPt8F4PIW\ntiUSrfXPAezxFke1/TIA39LErwBMV0r1TE5LaxNxLFFcBuBurfWI1vplABtB92LL0Vpv01o/YT4f\nAPAcgF5MwetS5ViiOJyvi9ZaHzRfU+alAawGcJ9Z7l8Xvl73AbhAKaUa3W9cBEEvgM3O9y2ofqMc\njmgAP1ZK/Vop9RGzbK7Wepv5vB3A3NY0bVxEtX2qXqvrjcnkTsdENyWOxZgTTgONPqf0dfGOBZiC\n10UplVBKPQlgJ4CHQBrLPq11wazitrd8LOb3/QBmNrrPuAiCI4GztdYrAFwE4ONKqXPdHzXphlMy\nFngqt93wDwCWADgVwDYAf93a5tSPUmoagH8BcIPWut/9bapdl5BjmZLXRWtd1FqfCqAPpKksa/Y+\n4yIItgJY4HzvM8umDFrrreZ9J4AHQDfIDlbPzfvO1rWwYaLaPuWuldZ6h3l4SwBuhzUzHNbHopRK\ngTrO72it7zeLp+R1CTuWqXpdGK31PgAPAzgLZIpLmp/c9paPxfzeBWB3o/uKiyB4HMBS43lvBzlV\n1rS4TXWjlOpQSnXyZwBvAfAM6Bg+aFb7IIDvt6aF4yKq7WsAfMBEqZwJYL9jqjgs8Wzl7wRdG4CO\n5SoT2bEIwFIAj012+8IwduR/AvCc1vpvnJ+m3HWJOpYpel1mK6Wmm89ZABeCfB4PA7jSrOZfF75e\nVwL4qdHkGqPVXvLJeoGiHl4A2ds+1+r2NNj2xaAoh6cAbOD2g2yBPwHwIoD/C6C71W2NaP/3QKr5\nGMi+eU1U20FRE7ea67QewMpWt7+OY/nfpq1Pmwezx1n/c+ZYngdwUavb77TrbJDZ52kAT5rXxVPx\nulQ5lql4XU4G8BvT5mcA3GyWLwYJq40A7gWQNssz5vtG8/vi8exXSkwIgiDEnLiYhgRBEIQIRBAI\ngiDEHBEEgiAIMUcEgSAIQswRQSAIghBzRBAIgodSquhUrHxSTWC1WqXUQrdyqSAcDiRrryIIsWNI\nU4q/IMQC0QgEoU4UzQnxFUXzQjymlDrGLF+olPqpKW72E6XUUWb5XKXUA6a2/FNKqTeYTSWUUreb\nevM/NhmkgtAyRBAIQiVZzzT0bue3/VrrkwD8PYC/Ncv+DsBdWuuTAXwHwC1m+S0Afqa1PgU0h8EG\ns3wpgFu11icC2AfgiiYfjyBURTKLBcFDKXVQaz0tZPkrAFZrrV8yRc62a61nKqVeB5UvGDPLt2mt\nZymldgHo01qPONtYCOAhrfVS8/3TAFJa679o/pEJQjiiEQhCY+iIz40w4nwuQnx1QosRQSAIjfFu\n5/2X5vOjoIq2APA+AI+Yzz8BcB1Qnmyka7IaKQiNICMRQagka2aIYv5Da80hpDOUUk+DRvXvMcs+\nAeCbSqkbAewC8CGz/I8B3KaUugY08r8OVLlUEA4rxEcgCHVifAQrtdavt7otgjCRiGlIEAQh5ohG\nIAiCEHNEIxAEQYg5IggEQRBijggCQRCEmCOCQBAEIeaIIBAEQYg5/x/kevMeQ4xyxAAAAABJRU5E\nrkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "# Evaluate on test data\n",
            "40/40 [==============================] - 0s 2ms/sample - loss: 1.0262 - acc: 0.5000\n",
            "test loss, test acc: [1.0261869238689543, 0.5]\n",
            "[[0.67490497]\n",
            " [0.6418359 ]\n",
            " [0.5413805 ]\n",
            " [0.57880596]\n",
            " [0.52791704]\n",
            " [1.02618692]\n",
            " [1.02509282]\n",
            " [0.52571097]\n",
            " [0.59876717]\n",
            " [1.33659376]]\n",
            "[[0.67500001]\n",
            " [0.77499998]\n",
            " [0.72500002]\n",
            " [0.72500002]\n",
            " [0.77499998]\n",
            " [0.5       ]\n",
            " [0.5       ]\n",
            " [0.69999999]\n",
            " [0.67500001]\n",
            " [0.5       ]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kd6FTGs7vBI3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "df_accl_all = pd.DataFrame({'Class1vs2': acc_all[:, 0]})\n",
        "df_accl_all.to_csv (r'EEG_Deep/df_accl_Cross_Patient_8_24_2560:4096.csv', index = None, header=True) #Don't forget to add '.csv' at the end of the path\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}