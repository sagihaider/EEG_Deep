{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "main_Single_Trial_Classification_PatientData_CrossSubject",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sagihaider/EEG_Deep/blob/master/main_Single_Trial_Classification_PatientData_CrossSubject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X110PE9MjrlE",
        "colab_type": "code",
        "outputId": "a1c73de0-ff4e-4f19-c07b-891a2894c946",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        }
      },
      "source": [
        "!git clone https://github.com/sagihaider/EEG_Deep.git"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'EEG_Deep'...\n",
            "remote: Enumerating objects: 19, done.\u001b[K\n",
            "remote: Counting objects: 100% (19/19), done.\u001b[K\n",
            "remote: Compressing objects: 100% (18/18), done.\u001b[K\n",
            "remote: Total 172 (delta 8), reused 4 (delta 1), pack-reused 153\u001b[K\n",
            "Receiving objects: 100% (172/172), 856.62 MiB | 47.18 MiB/s, done.\n",
            "Resolving deltas: 100% (61/61), done.\n",
            "Checking out files: 100% (57/57), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ioLzMoiS60Qf",
        "colab_type": "code",
        "outputId": "04dc663d-dd01-4e66-cb66-3eeeee7652fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 65
        }
      },
      "source": [
        "import scipy.io as spio\n",
        "import numpy as np\n",
        "from importlib.machinery import SourceFileLoader\n",
        "\n",
        "# EEGNet-specific imports\n",
        "from EEG_Deep.EEGModels import EEGNet, ShallowConvNet, DeepConvNet\n",
        "from tensorflow.keras import utils as np_utils\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras import backend as K\n",
        "K.set_image_data_format('channels_first')\n",
        "\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# tools for plotting confusion matrices\n",
        "from matplotlib import pyplot as plt\n",
        "from scipy.signal import butter, lfilter"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mo80Jg_Pn5lP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Band-pass Filter\n",
        "def butter_bandpass_filter(data, lowcut, highcut, fs, order=4):\n",
        "    nyq = 0.5 * fs\n",
        "    low = lowcut / nyq\n",
        "    high = highcut / nyq\n",
        "    b, a = butter(order, [low, high], btype='band')\n",
        "    y = lfilter(b, a, data)\n",
        "    return y\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZjtnE_6RSpum",
        "colab_type": "code",
        "outputId": "854028d0-579b-4808-b7fc-568dffd3ffe1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from numpy import zeros\n",
        "K.clear_session()\n",
        "cols = 1\n",
        "rows = 10\n",
        "acc_all = zeros([rows, cols])\n",
        "loss_all = zeros([rows, cols])\n",
        "X_tr_c12 = np.empty([80, 12, 4096])\n",
        "X_ts_c12 = np.empty([80, 12, 4096])\n",
        "result=[]\n",
        "\n",
        "from itertools import combinations \n",
        "comb = combinations([1, 2], 2) \n",
        "  # Print the obtained combinations \n",
        "bincomb=[]\n",
        "for i in list(comb): \n",
        "    bincomb.append(i)\n",
        "\n",
        "for x in range(1,11):\n",
        "  fName = 'EEG_Deep/Data2A/parsed_P0' + str(x) + 'T.mat'  # Load Data\n",
        "  print(fName)\n",
        "  mat = spio.loadmat(fName)\n",
        "  r_X_tr = mat['RawEEGData']\n",
        "  r_y_tr = mat['Labels']\n",
        "\n",
        "  ### Filter Data ###\n",
        "  for t in range(r_X_tr.shape[0]):\n",
        "    tril = r_X_tr[t,:,:]\n",
        "    # tril = tril.transpose()\n",
        "    tril_filtered = butter_bandpass_filter(tril, \n",
        "                                              lowcut=8, \n",
        "                                              highcut=24, \n",
        "                                              fs=512,\n",
        "                                              order=4)\n",
        "    # tril_filtered = tril_filtered.transpose()\n",
        "    X_tr_c12[t,:,:] = tril_filtered\n",
        "\n",
        "  print(\"Filtering of Training Data Finished\")\n",
        "  ## Test Data Load \n",
        "\n",
        "  fName = 'EEG_Deep/Data2A/parsed_P0' + str(x) + 'E.mat'  # Load Data\n",
        "  print(fName)\n",
        "  mat = spio.loadmat(fName)\n",
        "  r_X_ts = mat['RawEEGData']\n",
        "  r_y_ts = mat['Labels']\n",
        "  for t in range(r_X_ts.shape[0]):\n",
        "    tril = r_X_ts[t,:,:]\n",
        "    # tril = tril.transpose()\n",
        "    tril_filtered = butter_bandpass_filter(tril, \n",
        "                                              lowcut=8, \n",
        "                                              highcut=24, \n",
        "                                              fs=512,\n",
        "                                              order=4)\n",
        "    # tril_filtered = tril_filtered.transpose()\n",
        "    X_ts_c12[t,:,:] = tril_filtered\n",
        "  \n",
        "  print(\"Filtering of Testing Data Finished\")    \n",
        "\n",
        "  for k, com in enumerate(bincomb):\n",
        "      print(com)\n",
        "      # # Find labels to specific class\n",
        "      print(\"Finding labels in training data\")\n",
        "      class1indx = list(np.where(r_y_tr == com[0]))\n",
        "      class2indx = list(np.where(r_y_tr == com[1]))\n",
        "      c1=list(class1indx[0])\n",
        "      c2=list(class2indx[0])\n",
        "      y_tr_c12 = c1 + c2\n",
        "      y_tr_c12.sort()\n",
        "      # print(y_tr_c12)\n",
        "      x_tr_12 = X_tr_c12[y_tr_c12,:,:]\n",
        "      y_tr_12 = r_y_tr[y_tr_c12]\n",
        "      # print(np.shape(x_tr_12))\n",
        "      # print(np.shape(y_tr_12))\n",
        "      # # Find labels to specific class\n",
        "      print(\"Finding labels in testing data\")\n",
        "      class1indx = list(np.where(r_y_ts == com[0]))\n",
        "      class2indx = list(np.where(r_y_ts == com[1]))\n",
        "      c1=list(class1indx[0])\n",
        "      c2=list(class2indx[0])\n",
        "      y_ts_c12 = c1 + c2\n",
        "      y_ts_c12.sort()\n",
        "      # print(y_ts_c12)\n",
        "      x_ts_12 = X_ts_c12[y_ts_c12,:,:]\n",
        "      y_ts_12 = r_y_ts[y_ts_c12]\n",
        "      # print(np.shape(x_ts_12))\n",
        "      # print(np.shape(y_ts_12))\n",
        "      del class1indx, class2indx, c1, c2\n",
        "      # split data of each subject in training and validation\n",
        "      X_train = x_tr_12[0:60,:,2560:4096]\n",
        "      Y_train = y_tr_12[0:60].ravel()\n",
        "      X_val   = x_tr_12[60:,:,2560:4096]\n",
        "      Y_val   = y_tr_12[60:].ravel()\n",
        "      print(Y_val)\n",
        "      print(np.shape(X_train))\n",
        "      print(np.shape(Y_train))\n",
        "      print(np.shape(X_val))\n",
        "      print(np.shape(Y_val))\n",
        "  \n",
        "      # convert labels to one-hot encodings.\n",
        "      Y_train      = np_utils.to_categorical(Y_train-1, num_classes=4)\n",
        "      Y_val       = np_utils.to_categorical(Y_val-1, num_classes=4)\n",
        "      print(Y_val)\n",
        "\n",
        "      kernels, chans, samples = 1, 12, 1536\n",
        "      # convert data to NCHW (trials, kernels, channels, samples) format. Data \n",
        "      # contains 22 channels and 500 time-points. Set the number of kernels to 1.\n",
        "      X_train = X_train.reshape(X_train.shape[0], kernels, chans, samples)\n",
        "      X_val   = X_val.reshape(X_val.shape[0], kernels, chans, samples)\n",
        "   \n",
        "      print('X_train shape:', X_train.shape)\n",
        "      print(X_train.shape[0], 'train samples')\n",
        "      print(X_val.shape[0], 'val samples')\n",
        "\n",
        "      X_test      = x_ts_12[:,:,2560:4096]\n",
        "      Y_test      = y_ts_12[:]\n",
        "      print(np.shape(X_test))\n",
        "      print(np.shape(Y_test))\n",
        "\n",
        "      #convert labels to one-hot encodings.\n",
        "      Y_test      = np_utils.to_categorical(Y_test-1, num_classes=4)\n",
        "\n",
        "      # convert data to NCHW (trials, kernels, channels, samples) format. Data \n",
        "      # contains 22 channels and 500 time-points. Set the number of kernels to 1.\n",
        "      X_test      = X_test.reshape(X_test.shape[0], kernels, chans, samples)\n",
        "\n",
        "      print('X_train shape:', X_test.shape)\n",
        "      print(X_test.shape[0], 'train samples')\n",
        "\n",
        "      # configure the EEGNet-8,2,16 model with kernel length of 32 samples (other \n",
        "      # model configurations may do better, but this is a good starting point)\n",
        "      model = EEGNet(nb_classes = 4, Chans = 12, Samples = 1536,\n",
        "                     dropoutRate = 0.5, kernLength = 25, F1 = 8, \n",
        "                     D = 2, F2 = 16, norm_rate = 0.25, dropoutType = 'Dropout')\n",
        "      \n",
        "      # compile the model and set the optimizers\n",
        "      model.compile(loss='categorical_crossentropy', optimizer='adam', \n",
        "                    metrics = ['accuracy'])\n",
        "\n",
        "      # count number of parameters in the model\n",
        "      numParams    = model.count_params() \n",
        "\n",
        "      # set a valid path for your system to record model checkpoints\n",
        "      checkpointer = ModelCheckpoint(filepath='/tmp/checkpoint.h5', verbose=1,\n",
        "                                     save_best_only=True)\n",
        "  \n",
        "      # the syntax is {class_1:weight_1, class_2:weight_2,...}. Here just setting\n",
        "      # the weights all to be 1\n",
        "      class_weights = {0:1, 1:1, 2:1, 3:1}\n",
        "\n",
        "      history = model.fit(X_train, Y_train, batch_size = 16, epochs = 300, \n",
        "                      verbose = 2, validation_data=(X_val, Y_val),\n",
        "                      callbacks=[checkpointer], class_weight = class_weights)\n",
        "  \n",
        "      # Plot training & validation accuracy values\n",
        "      plt.plot(history.history['acc'])\n",
        "      plt.plot(history.history['val_acc'])\n",
        "      plt.title('Model accuracy')\n",
        "      plt.ylabel('Accuracy')\n",
        "      plt.xlabel('Epoch')\n",
        "      plt.legend(['Train', 'Test'], loc='upper left')\n",
        "      plt.show()\n",
        "      figName = 'Accuracy_A0' + str(x) + '.png'  \n",
        "      plt.savefig(figName)\n",
        "\n",
        "      print('\\n# Evaluate on test data')\n",
        "      results = model.evaluate(X_test, Y_test, batch_size=1)\n",
        "      print('test loss, test acc:', results)\n",
        "\n",
        "      loss_all[x - 1, k-1] = results[0]\n",
        "      acc_all[x - 1, k-1] = results[1]\n",
        "\n",
        "      from keras import backend as K \n",
        "      # Do some code, e.g. train and save model\n",
        "      K.clear_session()\n",
        "\n",
        "\n",
        "print(loss_all)\n",
        "print(acc_all)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "EEG_Deep/Data2A/parsed_P01T.mat\n",
            "Filtering of Training Data Finished\n",
            "EEG_Deep/Data2A/parsed_P01E.mat\n",
            "Filtering of Testing Data Finished\n",
            "(1, 2)\n",
            "Finding labels in training data\n",
            "Finding labels in testing data\n",
            "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
            "(60, 12, 1536)\n",
            "(60,)\n",
            "(20, 12, 1536)\n",
            "(20,)\n",
            "[[1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]]\n",
            "X_train shape: (60, 1, 12, 1536)\n",
            "60 train samples\n",
            "20 val samples\n",
            "(40, 12, 1536)\n",
            "(40, 1)\n",
            "X_train shape: (40, 1, 12, 1536)\n",
            "40 train samples\n",
            "Train on 60 samples, validate on 20 samples\n",
            "Epoch 1/300\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 1.36385, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 1s - loss: 1.3715 - acc: 0.4000 - val_loss: 1.3638 - val_acc: 0.3000\n",
            "Epoch 2/300\n",
            "\n",
            "Epoch 00002: val_loss improved from 1.36385 to 1.35542, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 1.0403 - acc: 0.6833 - val_loss: 1.3554 - val_acc: 0.0500\n",
            "Epoch 3/300\n",
            "\n",
            "Epoch 00003: val_loss improved from 1.35542 to 1.34632, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.8803 - acc: 0.6667 - val_loss: 1.3463 - val_acc: 0.0000e+00\n",
            "Epoch 4/300\n",
            "\n",
            "Epoch 00004: val_loss improved from 1.34632 to 1.34271, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.8049 - acc: 0.6667 - val_loss: 1.3427 - val_acc: 0.0000e+00\n",
            "Epoch 5/300\n",
            "\n",
            "Epoch 00005: val_loss improved from 1.34271 to 1.33907, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.7641 - acc: 0.6667 - val_loss: 1.3391 - val_acc: 0.0000e+00\n",
            "Epoch 6/300\n",
            "\n",
            "Epoch 00006: val_loss improved from 1.33907 to 1.33041, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.7320 - acc: 0.6667 - val_loss: 1.3304 - val_acc: 0.0000e+00\n",
            "Epoch 7/300\n",
            "\n",
            "Epoch 00007: val_loss improved from 1.33041 to 1.32334, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.7240 - acc: 0.6667 - val_loss: 1.3233 - val_acc: 0.0000e+00\n",
            "Epoch 8/300\n",
            "\n",
            "Epoch 00008: val_loss improved from 1.32334 to 1.31613, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.7009 - acc: 0.6667 - val_loss: 1.3161 - val_acc: 0.0000e+00\n",
            "Epoch 9/300\n",
            "\n",
            "Epoch 00009: val_loss improved from 1.31613 to 1.30712, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6794 - acc: 0.6667 - val_loss: 1.3071 - val_acc: 0.0000e+00\n",
            "Epoch 10/300\n",
            "\n",
            "Epoch 00010: val_loss improved from 1.30712 to 1.29503, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6548 - acc: 0.6833 - val_loss: 1.2950 - val_acc: 0.0000e+00\n",
            "Epoch 11/300\n",
            "\n",
            "Epoch 00011: val_loss improved from 1.29503 to 1.28973, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6466 - acc: 0.6667 - val_loss: 1.2897 - val_acc: 0.0000e+00\n",
            "Epoch 12/300\n",
            "\n",
            "Epoch 00012: val_loss improved from 1.28973 to 1.28776, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6306 - acc: 0.7000 - val_loss: 1.2878 - val_acc: 0.0000e+00\n",
            "Epoch 13/300\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.6402 - acc: 0.6667 - val_loss: 1.2896 - val_acc: 0.0000e+00\n",
            "Epoch 14/300\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.5955 - acc: 0.7000 - val_loss: 1.3039 - val_acc: 0.0000e+00\n",
            "Epoch 15/300\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.5888 - acc: 0.7000 - val_loss: 1.3158 - val_acc: 0.0000e+00\n",
            "Epoch 16/300\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.5968 - acc: 0.6833 - val_loss: 1.3325 - val_acc: 0.0000e+00\n",
            "Epoch 17/300\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.5708 - acc: 0.6833 - val_loss: 1.3467 - val_acc: 0.0000e+00\n",
            "Epoch 18/300\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.5591 - acc: 0.6833 - val_loss: 1.3679 - val_acc: 0.0000e+00\n",
            "Epoch 19/300\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.5563 - acc: 0.7167 - val_loss: 1.3948 - val_acc: 0.0000e+00\n",
            "Epoch 20/300\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.5177 - acc: 0.8000 - val_loss: 1.4306 - val_acc: 0.0000e+00\n",
            "Epoch 21/300\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.5173 - acc: 0.7833 - val_loss: 1.4570 - val_acc: 0.0000e+00\n",
            "Epoch 22/300\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.5355 - acc: 0.8167 - val_loss: 1.4686 - val_acc: 0.0000e+00\n",
            "Epoch 23/300\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.4861 - acc: 0.8667 - val_loss: 1.4826 - val_acc: 0.0000e+00\n",
            "Epoch 24/300\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.5074 - acc: 0.8667 - val_loss: 1.4883 - val_acc: 0.0000e+00\n",
            "Epoch 25/300\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.4901 - acc: 0.8333 - val_loss: 1.5045 - val_acc: 0.0500\n",
            "Epoch 26/300\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.4426 - acc: 0.8833 - val_loss: 1.5568 - val_acc: 0.1000\n",
            "Epoch 27/300\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.5002 - acc: 0.8167 - val_loss: 1.5996 - val_acc: 0.1000\n",
            "Epoch 28/300\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.4351 - acc: 0.9333 - val_loss: 1.6017 - val_acc: 0.2000\n",
            "Epoch 29/300\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.4446 - acc: 0.8500 - val_loss: 1.5988 - val_acc: 0.3000\n",
            "Epoch 30/300\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.4204 - acc: 0.9500 - val_loss: 1.6550 - val_acc: 0.3000\n",
            "Epoch 31/300\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.4038 - acc: 0.9500 - val_loss: 1.7729 - val_acc: 0.3000\n",
            "Epoch 32/300\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.4052 - acc: 0.9000 - val_loss: 1.8693 - val_acc: 0.3000\n",
            "Epoch 33/300\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.4221 - acc: 0.9167 - val_loss: 1.9882 - val_acc: 0.2500\n",
            "Epoch 34/300\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.4023 - acc: 0.9167 - val_loss: 2.0242 - val_acc: 0.3000\n",
            "Epoch 35/300\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.4131 - acc: 0.8833 - val_loss: 1.9997 - val_acc: 0.3000\n",
            "Epoch 36/300\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.3713 - acc: 0.9333 - val_loss: 1.9998 - val_acc: 0.3500\n",
            "Epoch 37/300\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.3680 - acc: 0.9500 - val_loss: 2.0473 - val_acc: 0.4000\n",
            "Epoch 38/300\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.4218 - acc: 0.8333 - val_loss: 2.1509 - val_acc: 0.3500\n",
            "Epoch 39/300\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.3721 - acc: 0.8667 - val_loss: 2.2679 - val_acc: 0.3500\n",
            "Epoch 40/300\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.3407 - acc: 0.9500 - val_loss: 2.4089 - val_acc: 0.3000\n",
            "Epoch 41/300\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.3692 - acc: 0.9167 - val_loss: 2.4798 - val_acc: 0.3500\n",
            "Epoch 42/300\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.3624 - acc: 0.9333 - val_loss: 2.4998 - val_acc: 0.3500\n",
            "Epoch 43/300\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.3718 - acc: 0.8833 - val_loss: 2.5104 - val_acc: 0.3500\n",
            "Epoch 44/300\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.3137 - acc: 0.9167 - val_loss: 2.5643 - val_acc: 0.3500\n",
            "Epoch 45/300\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.3251 - acc: 0.9667 - val_loss: 2.6020 - val_acc: 0.3500\n",
            "Epoch 46/300\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.3774 - acc: 0.9000 - val_loss: 2.6163 - val_acc: 0.4000\n",
            "Epoch 47/300\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.3077 - acc: 0.9167 - val_loss: 2.6198 - val_acc: 0.4000\n",
            "Epoch 48/300\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.3855 - acc: 0.8667 - val_loss: 2.5417 - val_acc: 0.4000\n",
            "Epoch 49/300\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.3227 - acc: 0.9500 - val_loss: 2.4496 - val_acc: 0.4000\n",
            "Epoch 50/300\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.3080 - acc: 0.9333 - val_loss: 2.3865 - val_acc: 0.4500\n",
            "Epoch 51/300\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.3214 - acc: 0.9333 - val_loss: 2.4323 - val_acc: 0.4500\n",
            "Epoch 52/300\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.2464 - acc: 1.0000 - val_loss: 2.4702 - val_acc: 0.5000\n",
            "Epoch 53/300\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.3042 - acc: 0.9333 - val_loss: 2.4937 - val_acc: 0.5000\n",
            "Epoch 54/300\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.3289 - acc: 0.9167 - val_loss: 2.4954 - val_acc: 0.5500\n",
            "Epoch 55/300\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.3019 - acc: 0.9667 - val_loss: 2.5087 - val_acc: 0.5500\n",
            "Epoch 56/300\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.2772 - acc: 0.9500 - val_loss: 2.5577 - val_acc: 0.5500\n",
            "Epoch 57/300\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.2873 - acc: 0.9667 - val_loss: 2.5774 - val_acc: 0.5500\n",
            "Epoch 58/300\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.3061 - acc: 0.9333 - val_loss: 2.6077 - val_acc: 0.5500\n",
            "Epoch 59/300\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.2925 - acc: 0.9500 - val_loss: 2.5708 - val_acc: 0.5500\n",
            "Epoch 60/300\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.3431 - acc: 0.8833 - val_loss: 2.6014 - val_acc: 0.5500\n",
            "Epoch 61/300\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.2745 - acc: 0.9000 - val_loss: 2.5860 - val_acc: 0.5500\n",
            "Epoch 62/300\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.2432 - acc: 0.9833 - val_loss: 2.5830 - val_acc: 0.5500\n",
            "Epoch 63/300\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.2436 - acc: 0.9500 - val_loss: 2.5768 - val_acc: 0.5500\n",
            "Epoch 64/300\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.2856 - acc: 0.9667 - val_loss: 2.5912 - val_acc: 0.5500\n",
            "Epoch 65/300\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.3123 - acc: 0.9000 - val_loss: 2.5159 - val_acc: 0.5500\n",
            "Epoch 66/300\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.2571 - acc: 0.9333 - val_loss: 2.5016 - val_acc: 0.5500\n",
            "Epoch 67/300\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.2812 - acc: 0.9333 - val_loss: 2.4353 - val_acc: 0.5500\n",
            "Epoch 68/300\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.2719 - acc: 0.9500 - val_loss: 2.3866 - val_acc: 0.6000\n",
            "Epoch 69/300\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.2802 - acc: 0.9333 - val_loss: 2.3828 - val_acc: 0.6000\n",
            "Epoch 70/300\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.3004 - acc: 0.9500 - val_loss: 2.4134 - val_acc: 0.6000\n",
            "Epoch 71/300\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.2788 - acc: 0.9167 - val_loss: 2.4643 - val_acc: 0.6000\n",
            "Epoch 72/300\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1880 - acc: 1.0000 - val_loss: 2.3578 - val_acc: 0.6500\n",
            "Epoch 73/300\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.2656 - acc: 0.9167 - val_loss: 2.3041 - val_acc: 0.6500\n",
            "Epoch 74/300\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.2851 - acc: 0.9000 - val_loss: 2.2674 - val_acc: 0.6500\n",
            "Epoch 75/300\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.2467 - acc: 1.0000 - val_loss: 2.3265 - val_acc: 0.6500\n",
            "Epoch 76/300\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.2287 - acc: 0.9833 - val_loss: 2.4327 - val_acc: 0.6500\n",
            "Epoch 77/300\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.2541 - acc: 0.9667 - val_loss: 2.5400 - val_acc: 0.6000\n",
            "Epoch 78/300\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.2464 - acc: 0.9500 - val_loss: 2.5183 - val_acc: 0.6500\n",
            "Epoch 79/300\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.2655 - acc: 0.9500 - val_loss: 2.4551 - val_acc: 0.6500\n",
            "Epoch 80/300\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.2529 - acc: 0.9333 - val_loss: 2.4687 - val_acc: 0.6500\n",
            "Epoch 81/300\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.2210 - acc: 0.9833 - val_loss: 2.3995 - val_acc: 0.6500\n",
            "Epoch 82/300\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.2102 - acc: 1.0000 - val_loss: 2.3530 - val_acc: 0.6500\n",
            "Epoch 83/300\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.2445 - acc: 0.9500 - val_loss: 2.2291 - val_acc: 0.6500\n",
            "Epoch 84/300\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.2519 - acc: 0.9333 - val_loss: 2.1236 - val_acc: 0.7500\n",
            "Epoch 85/300\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.2191 - acc: 0.9833 - val_loss: 2.0497 - val_acc: 0.7500\n",
            "Epoch 86/300\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.2189 - acc: 0.9833 - val_loss: 2.0375 - val_acc: 0.7500\n",
            "Epoch 87/300\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.2633 - acc: 0.9833 - val_loss: 2.0688 - val_acc: 0.7500\n",
            "Epoch 88/300\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.2207 - acc: 0.9667 - val_loss: 2.0617 - val_acc: 0.7000\n",
            "Epoch 89/300\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.2146 - acc: 0.9833 - val_loss: 2.0608 - val_acc: 0.7000\n",
            "Epoch 90/300\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.2410 - acc: 0.9167 - val_loss: 2.0504 - val_acc: 0.7000\n",
            "Epoch 91/300\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1917 - acc: 0.9667 - val_loss: 2.0176 - val_acc: 0.7500\n",
            "Epoch 92/300\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1888 - acc: 0.9833 - val_loss: 2.0398 - val_acc: 0.7500\n",
            "Epoch 93/300\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.2478 - acc: 0.9500 - val_loss: 2.0471 - val_acc: 0.7500\n",
            "Epoch 94/300\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1834 - acc: 0.9833 - val_loss: 2.0494 - val_acc: 0.7500\n",
            "Epoch 95/300\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.2216 - acc: 0.9833 - val_loss: 2.0882 - val_acc: 0.7000\n",
            "Epoch 96/300\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.2528 - acc: 0.9333 - val_loss: 2.1346 - val_acc: 0.7000\n",
            "Epoch 97/300\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1883 - acc: 0.9833 - val_loss: 2.2413 - val_acc: 0.6500\n",
            "Epoch 98/300\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.2203 - acc: 0.9667 - val_loss: 2.2830 - val_acc: 0.6500\n",
            "Epoch 99/300\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.2317 - acc: 0.9333 - val_loss: 2.3492 - val_acc: 0.6000\n",
            "Epoch 100/300\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.2144 - acc: 0.9833 - val_loss: 2.4170 - val_acc: 0.6000\n",
            "Epoch 101/300\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1944 - acc: 0.9833 - val_loss: 2.4761 - val_acc: 0.6000\n",
            "Epoch 102/300\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.2100 - acc: 0.9500 - val_loss: 2.5479 - val_acc: 0.6000\n",
            "Epoch 103/300\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.2027 - acc: 0.9667 - val_loss: 2.5670 - val_acc: 0.6000\n",
            "Epoch 104/300\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1984 - acc: 1.0000 - val_loss: 2.5781 - val_acc: 0.6000\n",
            "Epoch 105/300\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1620 - acc: 1.0000 - val_loss: 2.5413 - val_acc: 0.6000\n",
            "Epoch 106/300\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.2376 - acc: 0.9500 - val_loss: 2.4709 - val_acc: 0.6000\n",
            "Epoch 107/300\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1811 - acc: 1.0000 - val_loss: 2.3783 - val_acc: 0.6000\n",
            "Epoch 108/300\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1691 - acc: 0.9833 - val_loss: 2.2944 - val_acc: 0.6000\n",
            "Epoch 109/300\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.2351 - acc: 0.9500 - val_loss: 2.2223 - val_acc: 0.6000\n",
            "Epoch 110/300\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1684 - acc: 0.9833 - val_loss: 2.1727 - val_acc: 0.6500\n",
            "Epoch 111/300\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1770 - acc: 0.9667 - val_loss: 2.1634 - val_acc: 0.6500\n",
            "Epoch 112/300\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1491 - acc: 1.0000 - val_loss: 2.2274 - val_acc: 0.6000\n",
            "Epoch 113/300\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1446 - acc: 0.9667 - val_loss: 2.2432 - val_acc: 0.6000\n",
            "Epoch 114/300\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.2340 - acc: 0.9333 - val_loss: 2.2488 - val_acc: 0.6000\n",
            "Epoch 115/300\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1799 - acc: 1.0000 - val_loss: 2.1274 - val_acc: 0.6500\n",
            "Epoch 116/300\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1754 - acc: 0.9833 - val_loss: 2.1032 - val_acc: 0.7000\n",
            "Epoch 117/300\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1796 - acc: 1.0000 - val_loss: 2.1192 - val_acc: 0.6500\n",
            "Epoch 118/300\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1474 - acc: 1.0000 - val_loss: 2.1671 - val_acc: 0.6500\n",
            "Epoch 119/300\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1778 - acc: 0.9833 - val_loss: 2.2676 - val_acc: 0.6000\n",
            "Epoch 120/300\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1743 - acc: 0.9833 - val_loss: 2.4045 - val_acc: 0.6000\n",
            "Epoch 121/300\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1601 - acc: 1.0000 - val_loss: 2.4873 - val_acc: 0.6000\n",
            "Epoch 122/300\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1797 - acc: 0.9833 - val_loss: 2.5281 - val_acc: 0.6000\n",
            "Epoch 123/300\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1702 - acc: 0.9833 - val_loss: 2.5160 - val_acc: 0.6000\n",
            "Epoch 124/300\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1786 - acc: 0.9500 - val_loss: 2.4385 - val_acc: 0.6000\n",
            "Epoch 125/300\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1864 - acc: 0.9667 - val_loss: 2.3643 - val_acc: 0.6000\n",
            "Epoch 126/300\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1771 - acc: 0.9667 - val_loss: 2.2527 - val_acc: 0.6500\n",
            "Epoch 127/300\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1798 - acc: 0.9667 - val_loss: 2.1759 - val_acc: 0.7000\n",
            "Epoch 128/300\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1418 - acc: 0.9833 - val_loss: 2.1343 - val_acc: 0.7000\n",
            "Epoch 129/300\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1888 - acc: 0.9833 - val_loss: 2.0767 - val_acc: 0.7000\n",
            "Epoch 130/300\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1713 - acc: 0.9833 - val_loss: 2.0516 - val_acc: 0.7000\n",
            "Epoch 131/300\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1608 - acc: 0.9833 - val_loss: 1.9933 - val_acc: 0.7500\n",
            "Epoch 132/300\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1659 - acc: 0.9667 - val_loss: 1.9692 - val_acc: 0.7500\n",
            "Epoch 133/300\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1495 - acc: 0.9833 - val_loss: 1.9497 - val_acc: 0.7500\n",
            "Epoch 134/300\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.2070 - acc: 0.9333 - val_loss: 1.8987 - val_acc: 0.7500\n",
            "Epoch 135/300\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1616 - acc: 0.9667 - val_loss: 1.9199 - val_acc: 0.7500\n",
            "Epoch 136/300\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1762 - acc: 0.9667 - val_loss: 1.9490 - val_acc: 0.7500\n",
            "Epoch 137/300\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1371 - acc: 1.0000 - val_loss: 1.9666 - val_acc: 0.7500\n",
            "Epoch 138/300\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1440 - acc: 0.9667 - val_loss: 2.0358 - val_acc: 0.7500\n",
            "Epoch 139/300\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1696 - acc: 0.9500 - val_loss: 2.1003 - val_acc: 0.7000\n",
            "Epoch 140/300\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1635 - acc: 0.9833 - val_loss: 2.2316 - val_acc: 0.6000\n",
            "Epoch 141/300\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1385 - acc: 1.0000 - val_loss: 2.3475 - val_acc: 0.6000\n",
            "Epoch 142/300\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1774 - acc: 0.9833 - val_loss: 2.3418 - val_acc: 0.6000\n",
            "Epoch 143/300\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1890 - acc: 0.9833 - val_loss: 2.3490 - val_acc: 0.6000\n",
            "Epoch 144/300\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1640 - acc: 0.9667 - val_loss: 2.4228 - val_acc: 0.6000\n",
            "Epoch 145/300\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1532 - acc: 1.0000 - val_loss: 2.3772 - val_acc: 0.6000\n",
            "Epoch 146/300\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1553 - acc: 1.0000 - val_loss: 2.2662 - val_acc: 0.6000\n",
            "Epoch 147/300\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1704 - acc: 0.9667 - val_loss: 2.0955 - val_acc: 0.7500\n",
            "Epoch 148/300\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1952 - acc: 0.9667 - val_loss: 2.0313 - val_acc: 0.7500\n",
            "Epoch 149/300\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1279 - acc: 1.0000 - val_loss: 2.0520 - val_acc: 0.6500\n",
            "Epoch 150/300\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1552 - acc: 0.9833 - val_loss: 2.0946 - val_acc: 0.6500\n",
            "Epoch 151/300\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1962 - acc: 0.9500 - val_loss: 2.0802 - val_acc: 0.6500\n",
            "Epoch 152/300\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1635 - acc: 0.9500 - val_loss: 1.9717 - val_acc: 0.7500\n",
            "Epoch 153/300\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1613 - acc: 0.9833 - val_loss: 1.9315 - val_acc: 0.7500\n",
            "Epoch 154/300\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1623 - acc: 0.9667 - val_loss: 2.0533 - val_acc: 0.6500\n",
            "Epoch 155/300\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1999 - acc: 0.9500 - val_loss: 2.1449 - val_acc: 0.6500\n",
            "Epoch 156/300\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1509 - acc: 1.0000 - val_loss: 2.2014 - val_acc: 0.6000\n",
            "Epoch 157/300\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1488 - acc: 0.9833 - val_loss: 2.1879 - val_acc: 0.6000\n",
            "Epoch 158/300\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1618 - acc: 0.9667 - val_loss: 2.1898 - val_acc: 0.6000\n",
            "Epoch 159/300\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1480 - acc: 0.9833 - val_loss: 2.0865 - val_acc: 0.6500\n",
            "Epoch 160/300\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1305 - acc: 1.0000 - val_loss: 2.0187 - val_acc: 0.7500\n",
            "Epoch 161/300\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1591 - acc: 0.9833 - val_loss: 2.1485 - val_acc: 0.6000\n",
            "Epoch 162/300\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1673 - acc: 0.9833 - val_loss: 2.1752 - val_acc: 0.6000\n",
            "Epoch 163/300\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1650 - acc: 0.9667 - val_loss: 2.2243 - val_acc: 0.6000\n",
            "Epoch 164/300\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1503 - acc: 0.9833 - val_loss: 2.2928 - val_acc: 0.6000\n",
            "Epoch 165/300\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1114 - acc: 1.0000 - val_loss: 2.2485 - val_acc: 0.6000\n",
            "Epoch 166/300\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1551 - acc: 0.9667 - val_loss: 2.1949 - val_acc: 0.6500\n",
            "Epoch 167/300\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1193 - acc: 1.0000 - val_loss: 2.1968 - val_acc: 0.7000\n",
            "Epoch 168/300\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1292 - acc: 1.0000 - val_loss: 2.1745 - val_acc: 0.7000\n",
            "Epoch 169/300\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1671 - acc: 0.9500 - val_loss: 2.2214 - val_acc: 0.6500\n",
            "Epoch 170/300\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1328 - acc: 0.9667 - val_loss: 2.3094 - val_acc: 0.6000\n",
            "Epoch 171/300\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1340 - acc: 0.9833 - val_loss: 2.3017 - val_acc: 0.6000\n",
            "Epoch 172/300\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1246 - acc: 0.9833 - val_loss: 2.2971 - val_acc: 0.6000\n",
            "Epoch 173/300\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1050 - acc: 1.0000 - val_loss: 2.2668 - val_acc: 0.6000\n",
            "Epoch 174/300\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1568 - acc: 0.9833 - val_loss: 2.2636 - val_acc: 0.6500\n",
            "Epoch 175/300\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1168 - acc: 1.0000 - val_loss: 2.3210 - val_acc: 0.6000\n",
            "Epoch 176/300\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.0967 - acc: 1.0000 - val_loss: 2.4698 - val_acc: 0.6000\n",
            "Epoch 177/300\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1206 - acc: 1.0000 - val_loss: 2.5437 - val_acc: 0.6000\n",
            "Epoch 178/300\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1303 - acc: 1.0000 - val_loss: 2.6308 - val_acc: 0.6000\n",
            "Epoch 179/300\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1189 - acc: 0.9833 - val_loss: 2.5876 - val_acc: 0.6000\n",
            "Epoch 180/300\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1365 - acc: 0.9833 - val_loss: 2.4520 - val_acc: 0.6000\n",
            "Epoch 181/300\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1521 - acc: 0.9667 - val_loss: 2.2962 - val_acc: 0.6000\n",
            "Epoch 182/300\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1479 - acc: 0.9833 - val_loss: 2.1744 - val_acc: 0.6000\n",
            "Epoch 183/300\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1185 - acc: 0.9833 - val_loss: 2.1192 - val_acc: 0.6000\n",
            "Epoch 184/300\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1249 - acc: 0.9833 - val_loss: 2.0772 - val_acc: 0.6000\n",
            "Epoch 185/300\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1450 - acc: 0.9667 - val_loss: 2.1459 - val_acc: 0.6000\n",
            "Epoch 186/300\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1419 - acc: 0.9667 - val_loss: 2.3460 - val_acc: 0.6000\n",
            "Epoch 187/300\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1301 - acc: 1.0000 - val_loss: 2.6055 - val_acc: 0.6000\n",
            "Epoch 188/300\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1305 - acc: 0.9833 - val_loss: 2.6653 - val_acc: 0.6000\n",
            "Epoch 189/300\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1212 - acc: 0.9833 - val_loss: 2.6777 - val_acc: 0.6000\n",
            "Epoch 190/300\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1225 - acc: 1.0000 - val_loss: 2.6301 - val_acc: 0.6000\n",
            "Epoch 191/300\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1475 - acc: 1.0000 - val_loss: 2.7409 - val_acc: 0.6000\n",
            "Epoch 192/300\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.0923 - acc: 1.0000 - val_loss: 2.8002 - val_acc: 0.6000\n",
            "Epoch 193/300\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1182 - acc: 0.9833 - val_loss: 2.8229 - val_acc: 0.5500\n",
            "Epoch 194/300\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1197 - acc: 1.0000 - val_loss: 2.7913 - val_acc: 0.6000\n",
            "Epoch 195/300\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1485 - acc: 0.9667 - val_loss: 2.7298 - val_acc: 0.6000\n",
            "Epoch 196/300\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1115 - acc: 1.0000 - val_loss: 2.7080 - val_acc: 0.6000\n",
            "Epoch 197/300\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1229 - acc: 1.0000 - val_loss: 2.5838 - val_acc: 0.6000\n",
            "Epoch 198/300\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1304 - acc: 0.9833 - val_loss: 2.4058 - val_acc: 0.6000\n",
            "Epoch 199/300\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1349 - acc: 0.9833 - val_loss: 2.3680 - val_acc: 0.6000\n",
            "Epoch 200/300\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1605 - acc: 0.9667 - val_loss: 2.4065 - val_acc: 0.6000\n",
            "Epoch 201/300\n",
            "\n",
            "Epoch 00201: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1117 - acc: 1.0000 - val_loss: 2.4287 - val_acc: 0.6000\n",
            "Epoch 202/300\n",
            "\n",
            "Epoch 00202: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1255 - acc: 0.9833 - val_loss: 2.4226 - val_acc: 0.6000\n",
            "Epoch 203/300\n",
            "\n",
            "Epoch 00203: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1397 - acc: 0.9833 - val_loss: 2.3294 - val_acc: 0.6000\n",
            "Epoch 204/300\n",
            "\n",
            "Epoch 00204: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.0779 - acc: 1.0000 - val_loss: 2.2528 - val_acc: 0.6000\n",
            "Epoch 205/300\n",
            "\n",
            "Epoch 00205: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1345 - acc: 0.9833 - val_loss: 2.2095 - val_acc: 0.6000\n",
            "Epoch 206/300\n",
            "\n",
            "Epoch 00206: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1163 - acc: 0.9833 - val_loss: 2.2136 - val_acc: 0.6000\n",
            "Epoch 207/300\n",
            "\n",
            "Epoch 00207: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1232 - acc: 1.0000 - val_loss: 2.2951 - val_acc: 0.6000\n",
            "Epoch 208/300\n",
            "\n",
            "Epoch 00208: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1265 - acc: 0.9833 - val_loss: 2.3897 - val_acc: 0.6000\n",
            "Epoch 209/300\n",
            "\n",
            "Epoch 00209: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.0784 - acc: 1.0000 - val_loss: 2.3822 - val_acc: 0.6000\n",
            "Epoch 210/300\n",
            "\n",
            "Epoch 00210: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1256 - acc: 0.9833 - val_loss: 2.3315 - val_acc: 0.6500\n",
            "Epoch 211/300\n",
            "\n",
            "Epoch 00211: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1742 - acc: 0.9333 - val_loss: 2.3047 - val_acc: 0.6500\n",
            "Epoch 212/300\n",
            "\n",
            "Epoch 00212: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1092 - acc: 0.9833 - val_loss: 2.4245 - val_acc: 0.6000\n",
            "Epoch 213/300\n",
            "\n",
            "Epoch 00213: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1081 - acc: 0.9833 - val_loss: 2.3869 - val_acc: 0.6000\n",
            "Epoch 214/300\n",
            "\n",
            "Epoch 00214: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1176 - acc: 0.9833 - val_loss: 2.3349 - val_acc: 0.6000\n",
            "Epoch 215/300\n",
            "\n",
            "Epoch 00215: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1320 - acc: 0.9833 - val_loss: 2.1827 - val_acc: 0.6000\n",
            "Epoch 216/300\n",
            "\n",
            "Epoch 00216: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.0920 - acc: 1.0000 - val_loss: 2.1854 - val_acc: 0.6000\n",
            "Epoch 217/300\n",
            "\n",
            "Epoch 00217: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1364 - acc: 0.9667 - val_loss: 2.1814 - val_acc: 0.6000\n",
            "Epoch 218/300\n",
            "\n",
            "Epoch 00218: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1020 - acc: 0.9833 - val_loss: 2.1788 - val_acc: 0.6000\n",
            "Epoch 219/300\n",
            "\n",
            "Epoch 00219: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1127 - acc: 1.0000 - val_loss: 2.3333 - val_acc: 0.6000\n",
            "Epoch 220/300\n",
            "\n",
            "Epoch 00220: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1158 - acc: 0.9667 - val_loss: 2.4445 - val_acc: 0.6000\n",
            "Epoch 221/300\n",
            "\n",
            "Epoch 00221: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1293 - acc: 0.9833 - val_loss: 2.6065 - val_acc: 0.6000\n",
            "Epoch 222/300\n",
            "\n",
            "Epoch 00222: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1156 - acc: 0.9833 - val_loss: 2.6700 - val_acc: 0.6000\n",
            "Epoch 223/300\n",
            "\n",
            "Epoch 00223: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1172 - acc: 0.9833 - val_loss: 2.7151 - val_acc: 0.6000\n",
            "Epoch 224/300\n",
            "\n",
            "Epoch 00224: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1114 - acc: 0.9833 - val_loss: 2.8415 - val_acc: 0.6000\n",
            "Epoch 225/300\n",
            "\n",
            "Epoch 00225: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1310 - acc: 0.9833 - val_loss: 2.9339 - val_acc: 0.6000\n",
            "Epoch 226/300\n",
            "\n",
            "Epoch 00226: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1635 - acc: 0.9500 - val_loss: 2.8841 - val_acc: 0.5500\n",
            "Epoch 227/300\n",
            "\n",
            "Epoch 00227: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1000 - acc: 1.0000 - val_loss: 2.7403 - val_acc: 0.6000\n",
            "Epoch 228/300\n",
            "\n",
            "Epoch 00228: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1270 - acc: 0.9667 - val_loss: 2.6155 - val_acc: 0.6000\n",
            "Epoch 229/300\n",
            "\n",
            "Epoch 00229: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1429 - acc: 0.9500 - val_loss: 2.5997 - val_acc: 0.6000\n",
            "Epoch 230/300\n",
            "\n",
            "Epoch 00230: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1116 - acc: 0.9833 - val_loss: 2.5828 - val_acc: 0.5500\n",
            "Epoch 231/300\n",
            "\n",
            "Epoch 00231: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1057 - acc: 1.0000 - val_loss: 2.5828 - val_acc: 0.5000\n",
            "Epoch 232/300\n",
            "\n",
            "Epoch 00232: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1375 - acc: 0.9833 - val_loss: 2.5595 - val_acc: 0.5000\n",
            "Epoch 233/300\n",
            "\n",
            "Epoch 00233: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1327 - acc: 0.9833 - val_loss: 2.4919 - val_acc: 0.5000\n",
            "Epoch 234/300\n",
            "\n",
            "Epoch 00234: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1318 - acc: 0.9833 - val_loss: 2.5365 - val_acc: 0.4500\n",
            "Epoch 235/300\n",
            "\n",
            "Epoch 00235: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1173 - acc: 1.0000 - val_loss: 2.4889 - val_acc: 0.4500\n",
            "Epoch 236/300\n",
            "\n",
            "Epoch 00236: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1139 - acc: 1.0000 - val_loss: 2.4280 - val_acc: 0.5500\n",
            "Epoch 237/300\n",
            "\n",
            "Epoch 00237: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1072 - acc: 0.9833 - val_loss: 2.5359 - val_acc: 0.5500\n",
            "Epoch 238/300\n",
            "\n",
            "Epoch 00238: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.0902 - acc: 0.9833 - val_loss: 2.7087 - val_acc: 0.4000\n",
            "Epoch 239/300\n",
            "\n",
            "Epoch 00239: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1003 - acc: 0.9833 - val_loss: 2.6966 - val_acc: 0.4500\n",
            "Epoch 240/300\n",
            "\n",
            "Epoch 00240: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1004 - acc: 1.0000 - val_loss: 2.7186 - val_acc: 0.5000\n",
            "Epoch 241/300\n",
            "\n",
            "Epoch 00241: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1227 - acc: 1.0000 - val_loss: 2.8187 - val_acc: 0.5000\n",
            "Epoch 242/300\n",
            "\n",
            "Epoch 00242: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1051 - acc: 0.9833 - val_loss: 2.9797 - val_acc: 0.5000\n",
            "Epoch 243/300\n",
            "\n",
            "Epoch 00243: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1205 - acc: 0.9833 - val_loss: 3.1793 - val_acc: 0.4500\n",
            "Epoch 244/300\n",
            "\n",
            "Epoch 00244: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1230 - acc: 0.9833 - val_loss: 3.3369 - val_acc: 0.4000\n",
            "Epoch 245/300\n",
            "\n",
            "Epoch 00245: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1140 - acc: 1.0000 - val_loss: 3.4039 - val_acc: 0.4000\n",
            "Epoch 246/300\n",
            "\n",
            "Epoch 00246: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1145 - acc: 0.9833 - val_loss: 3.2729 - val_acc: 0.4000\n",
            "Epoch 247/300\n",
            "\n",
            "Epoch 00247: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1334 - acc: 0.9833 - val_loss: 3.0664 - val_acc: 0.4500\n",
            "Epoch 248/300\n",
            "\n",
            "Epoch 00248: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1046 - acc: 0.9833 - val_loss: 2.8274 - val_acc: 0.5000\n",
            "Epoch 249/300\n",
            "\n",
            "Epoch 00249: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1295 - acc: 0.9667 - val_loss: 2.7064 - val_acc: 0.5000\n",
            "Epoch 250/300\n",
            "\n",
            "Epoch 00250: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1069 - acc: 1.0000 - val_loss: 2.5751 - val_acc: 0.5500\n",
            "Epoch 251/300\n",
            "\n",
            "Epoch 00251: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1033 - acc: 1.0000 - val_loss: 2.4912 - val_acc: 0.6000\n",
            "Epoch 252/300\n",
            "\n",
            "Epoch 00252: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.0963 - acc: 1.0000 - val_loss: 2.4837 - val_acc: 0.6000\n",
            "Epoch 253/300\n",
            "\n",
            "Epoch 00253: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1157 - acc: 0.9833 - val_loss: 2.5063 - val_acc: 0.6000\n",
            "Epoch 254/300\n",
            "\n",
            "Epoch 00254: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1314 - acc: 0.9833 - val_loss: 2.4631 - val_acc: 0.6000\n",
            "Epoch 255/300\n",
            "\n",
            "Epoch 00255: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1115 - acc: 0.9667 - val_loss: 2.3983 - val_acc: 0.6000\n",
            "Epoch 256/300\n",
            "\n",
            "Epoch 00256: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.0944 - acc: 1.0000 - val_loss: 2.4240 - val_acc: 0.6000\n",
            "Epoch 257/300\n",
            "\n",
            "Epoch 00257: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1396 - acc: 0.9500 - val_loss: 2.4276 - val_acc: 0.6000\n",
            "Epoch 258/300\n",
            "\n",
            "Epoch 00258: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1013 - acc: 1.0000 - val_loss: 2.5661 - val_acc: 0.6000\n",
            "Epoch 259/300\n",
            "\n",
            "Epoch 00259: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.0841 - acc: 1.0000 - val_loss: 2.7136 - val_acc: 0.5500\n",
            "Epoch 260/300\n",
            "\n",
            "Epoch 00260: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.0839 - acc: 0.9833 - val_loss: 2.6449 - val_acc: 0.5500\n",
            "Epoch 261/300\n",
            "\n",
            "Epoch 00261: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.0912 - acc: 1.0000 - val_loss: 2.4674 - val_acc: 0.6000\n",
            "Epoch 262/300\n",
            "\n",
            "Epoch 00262: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1019 - acc: 0.9833 - val_loss: 2.2884 - val_acc: 0.6000\n",
            "Epoch 263/300\n",
            "\n",
            "Epoch 00263: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.0788 - acc: 1.0000 - val_loss: 2.2504 - val_acc: 0.7000\n",
            "Epoch 264/300\n",
            "\n",
            "Epoch 00264: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.0923 - acc: 1.0000 - val_loss: 2.2129 - val_acc: 0.7000\n",
            "Epoch 265/300\n",
            "\n",
            "Epoch 00265: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.0923 - acc: 0.9667 - val_loss: 2.2704 - val_acc: 0.7000\n",
            "Epoch 266/300\n",
            "\n",
            "Epoch 00266: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.0638 - acc: 1.0000 - val_loss: 2.3079 - val_acc: 0.6500\n",
            "Epoch 267/300\n",
            "\n",
            "Epoch 00267: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.0902 - acc: 1.0000 - val_loss: 2.3233 - val_acc: 0.6500\n",
            "Epoch 268/300\n",
            "\n",
            "Epoch 00268: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.0956 - acc: 0.9833 - val_loss: 2.4348 - val_acc: 0.6000\n",
            "Epoch 269/300\n",
            "\n",
            "Epoch 00269: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.0855 - acc: 0.9833 - val_loss: 2.5169 - val_acc: 0.6000\n",
            "Epoch 270/300\n",
            "\n",
            "Epoch 00270: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1010 - acc: 0.9667 - val_loss: 2.5569 - val_acc: 0.5500\n",
            "Epoch 271/300\n",
            "\n",
            "Epoch 00271: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1062 - acc: 0.9833 - val_loss: 2.7462 - val_acc: 0.5500\n",
            "Epoch 272/300\n",
            "\n",
            "Epoch 00272: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.0949 - acc: 0.9833 - val_loss: 2.8051 - val_acc: 0.5500\n",
            "Epoch 273/300\n",
            "\n",
            "Epoch 00273: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1177 - acc: 1.0000 - val_loss: 2.8472 - val_acc: 0.5500\n",
            "Epoch 274/300\n",
            "\n",
            "Epoch 00274: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.0914 - acc: 1.0000 - val_loss: 2.7234 - val_acc: 0.5500\n",
            "Epoch 275/300\n",
            "\n",
            "Epoch 00275: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.0985 - acc: 1.0000 - val_loss: 2.7924 - val_acc: 0.6000\n",
            "Epoch 276/300\n",
            "\n",
            "Epoch 00276: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.0800 - acc: 1.0000 - val_loss: 2.7734 - val_acc: 0.6000\n",
            "Epoch 277/300\n",
            "\n",
            "Epoch 00277: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.0935 - acc: 1.0000 - val_loss: 2.8013 - val_acc: 0.6000\n",
            "Epoch 278/300\n",
            "\n",
            "Epoch 00278: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1013 - acc: 0.9833 - val_loss: 2.8716 - val_acc: 0.6000\n",
            "Epoch 279/300\n",
            "\n",
            "Epoch 00279: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.0945 - acc: 0.9667 - val_loss: 2.8489 - val_acc: 0.6000\n",
            "Epoch 280/300\n",
            "\n",
            "Epoch 00280: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1052 - acc: 1.0000 - val_loss: 2.7533 - val_acc: 0.6000\n",
            "Epoch 281/300\n",
            "\n",
            "Epoch 00281: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.0862 - acc: 0.9833 - val_loss: 2.7560 - val_acc: 0.6000\n",
            "Epoch 282/300\n",
            "\n",
            "Epoch 00282: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1120 - acc: 0.9833 - val_loss: 2.7578 - val_acc: 0.6000\n",
            "Epoch 283/300\n",
            "\n",
            "Epoch 00283: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.0903 - acc: 0.9833 - val_loss: 2.6282 - val_acc: 0.6000\n",
            "Epoch 284/300\n",
            "\n",
            "Epoch 00284: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.0831 - acc: 0.9833 - val_loss: 2.6126 - val_acc: 0.6000\n",
            "Epoch 285/300\n",
            "\n",
            "Epoch 00285: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.0896 - acc: 0.9833 - val_loss: 2.7446 - val_acc: 0.6000\n",
            "Epoch 286/300\n",
            "\n",
            "Epoch 00286: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.0848 - acc: 1.0000 - val_loss: 2.9992 - val_acc: 0.5000\n",
            "Epoch 287/300\n",
            "\n",
            "Epoch 00287: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.0919 - acc: 1.0000 - val_loss: 3.1149 - val_acc: 0.4500\n",
            "Epoch 288/300\n",
            "\n",
            "Epoch 00288: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.0896 - acc: 1.0000 - val_loss: 3.1015 - val_acc: 0.5000\n",
            "Epoch 289/300\n",
            "\n",
            "Epoch 00289: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.0979 - acc: 1.0000 - val_loss: 3.0032 - val_acc: 0.5000\n",
            "Epoch 290/300\n",
            "\n",
            "Epoch 00290: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.0824 - acc: 1.0000 - val_loss: 2.9379 - val_acc: 0.6000\n",
            "Epoch 291/300\n",
            "\n",
            "Epoch 00291: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.0919 - acc: 1.0000 - val_loss: 2.8369 - val_acc: 0.6000\n",
            "Epoch 292/300\n",
            "\n",
            "Epoch 00292: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1162 - acc: 0.9667 - val_loss: 2.9210 - val_acc: 0.5500\n",
            "Epoch 293/300\n",
            "\n",
            "Epoch 00293: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1184 - acc: 0.9833 - val_loss: 2.8686 - val_acc: 0.5500\n",
            "Epoch 294/300\n",
            "\n",
            "Epoch 00294: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1122 - acc: 1.0000 - val_loss: 2.7741 - val_acc: 0.5500\n",
            "Epoch 295/300\n",
            "\n",
            "Epoch 00295: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.0749 - acc: 1.0000 - val_loss: 2.6967 - val_acc: 0.5500\n",
            "Epoch 296/300\n",
            "\n",
            "Epoch 00296: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.0799 - acc: 0.9833 - val_loss: 2.6089 - val_acc: 0.6000\n",
            "Epoch 297/300\n",
            "\n",
            "Epoch 00297: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.0851 - acc: 1.0000 - val_loss: 2.5678 - val_acc: 0.6000\n",
            "Epoch 298/300\n",
            "\n",
            "Epoch 00298: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.0827 - acc: 1.0000 - val_loss: 2.5139 - val_acc: 0.6000\n",
            "Epoch 299/300\n",
            "\n",
            "Epoch 00299: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.0819 - acc: 0.9833 - val_loss: 2.5623 - val_acc: 0.5500\n",
            "Epoch 300/300\n",
            "\n",
            "Epoch 00300: val_loss did not improve from 1.28776\n",
            "60/60 - 0s - loss: 0.1273 - acc: 1.0000 - val_loss: 2.6038 - val_acc: 0.5500\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\noewVKxj1AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "# Evaluate on test data\n",
            "40/40 [==============================] - 0s 2ms/sample - loss: 0.5534 - acc: 0.6750\n",
            "test loss, test acc: [0.5534028882525035, 0.675]\n",
            "EEG_Deep/Data2A/parsed_P02T.mat\n",
            "Filtering of Training Data Finished\n",
            "EEG_Deep/Data2A/parsed_P02E.mat\n",
            "Filtering of Testing Data Finished\n",
            "(1, 2)\n",
            "Finding labels in training data\n",
            "Finding labels in testing data\n",
            "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
            "(60, 12, 1536)\n",
            "(60,)\n",
            "(20, 12, 1536)\n",
            "(20,)\n",
            "[[1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]]\n",
            "X_train shape: (60, 1, 12, 1536)\n",
            "60 train samples\n",
            "20 val samples\n",
            "(40, 12, 1536)\n",
            "(40, 1)\n",
            "X_train shape: (40, 1, 12, 1536)\n",
            "40 train samples\n",
            "Train on 60 samples, validate on 20 samples\n",
            "Epoch 1/300\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 1.36858, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 1s - loss: 1.2970 - acc: 0.4500 - val_loss: 1.3686 - val_acc: 0.5500\n",
            "Epoch 2/300\n",
            "\n",
            "Epoch 00002: val_loss improved from 1.36858 to 1.35495, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 1.0562 - acc: 0.6667 - val_loss: 1.3549 - val_acc: 0.2000\n",
            "Epoch 3/300\n",
            "\n",
            "Epoch 00003: val_loss improved from 1.35495 to 1.34147, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.9037 - acc: 0.6667 - val_loss: 1.3415 - val_acc: 0.0000e+00\n",
            "Epoch 4/300\n",
            "\n",
            "Epoch 00004: val_loss improved from 1.34147 to 1.32765, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.8192 - acc: 0.6667 - val_loss: 1.3276 - val_acc: 0.0000e+00\n",
            "Epoch 5/300\n",
            "\n",
            "Epoch 00005: val_loss improved from 1.32765 to 1.31209, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.7767 - acc: 0.6667 - val_loss: 1.3121 - val_acc: 0.0000e+00\n",
            "Epoch 6/300\n",
            "\n",
            "Epoch 00006: val_loss improved from 1.31209 to 1.29499, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.7139 - acc: 0.6667 - val_loss: 1.2950 - val_acc: 0.0000e+00\n",
            "Epoch 7/300\n",
            "\n",
            "Epoch 00007: val_loss improved from 1.29499 to 1.28064, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.7197 - acc: 0.6667 - val_loss: 1.2806 - val_acc: 0.0000e+00\n",
            "Epoch 8/300\n",
            "\n",
            "Epoch 00008: val_loss improved from 1.28064 to 1.26727, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6923 - acc: 0.6667 - val_loss: 1.2673 - val_acc: 0.0000e+00\n",
            "Epoch 9/300\n",
            "\n",
            "Epoch 00009: val_loss improved from 1.26727 to 1.25151, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6800 - acc: 0.6667 - val_loss: 1.2515 - val_acc: 0.0000e+00\n",
            "Epoch 10/300\n",
            "\n",
            "Epoch 00010: val_loss improved from 1.25151 to 1.23431, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6527 - acc: 0.6667 - val_loss: 1.2343 - val_acc: 0.0000e+00\n",
            "Epoch 11/300\n",
            "\n",
            "Epoch 00011: val_loss improved from 1.23431 to 1.22217, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6344 - acc: 0.6833 - val_loss: 1.2222 - val_acc: 0.0000e+00\n",
            "Epoch 12/300\n",
            "\n",
            "Epoch 00012: val_loss improved from 1.22217 to 1.21343, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6231 - acc: 0.6667 - val_loss: 1.2134 - val_acc: 0.0000e+00\n",
            "Epoch 13/300\n",
            "\n",
            "Epoch 00013: val_loss improved from 1.21343 to 1.20624, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6223 - acc: 0.7000 - val_loss: 1.2062 - val_acc: 0.0000e+00\n",
            "Epoch 14/300\n",
            "\n",
            "Epoch 00014: val_loss improved from 1.20624 to 1.19636, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6188 - acc: 0.7500 - val_loss: 1.1964 - val_acc: 0.0000e+00\n",
            "Epoch 15/300\n",
            "\n",
            "Epoch 00015: val_loss improved from 1.19636 to 1.19092, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6010 - acc: 0.7167 - val_loss: 1.1909 - val_acc: 0.0000e+00\n",
            "Epoch 16/300\n",
            "\n",
            "Epoch 00016: val_loss improved from 1.19092 to 1.18633, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5561 - acc: 0.7667 - val_loss: 1.1863 - val_acc: 0.0000e+00\n",
            "Epoch 17/300\n",
            "\n",
            "Epoch 00017: val_loss improved from 1.18633 to 1.18192, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5719 - acc: 0.7333 - val_loss: 1.1819 - val_acc: 0.0000e+00\n",
            "Epoch 18/300\n",
            "\n",
            "Epoch 00018: val_loss improved from 1.18192 to 1.17203, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5535 - acc: 0.7667 - val_loss: 1.1720 - val_acc: 0.0000e+00\n",
            "Epoch 19/300\n",
            "\n",
            "Epoch 00019: val_loss improved from 1.17203 to 1.16005, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5515 - acc: 0.8167 - val_loss: 1.1601 - val_acc: 0.0000e+00\n",
            "Epoch 20/300\n",
            "\n",
            "Epoch 00020: val_loss improved from 1.16005 to 1.14973, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5328 - acc: 0.7500 - val_loss: 1.1497 - val_acc: 0.0000e+00\n",
            "Epoch 21/300\n",
            "\n",
            "Epoch 00021: val_loss improved from 1.14973 to 1.14062, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5175 - acc: 0.8167 - val_loss: 1.1406 - val_acc: 0.0000e+00\n",
            "Epoch 22/300\n",
            "\n",
            "Epoch 00022: val_loss improved from 1.14062 to 1.13779, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5294 - acc: 0.8167 - val_loss: 1.1378 - val_acc: 0.0500\n",
            "Epoch 23/300\n",
            "\n",
            "Epoch 00023: val_loss improved from 1.13779 to 1.13192, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4975 - acc: 0.8667 - val_loss: 1.1319 - val_acc: 0.0500\n",
            "Epoch 24/300\n",
            "\n",
            "Epoch 00024: val_loss improved from 1.13192 to 1.12433, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5051 - acc: 0.8333 - val_loss: 1.1243 - val_acc: 0.0500\n",
            "Epoch 25/300\n",
            "\n",
            "Epoch 00025: val_loss improved from 1.12433 to 1.12025, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4842 - acc: 0.8500 - val_loss: 1.1202 - val_acc: 0.0500\n",
            "Epoch 26/300\n",
            "\n",
            "Epoch 00026: val_loss improved from 1.12025 to 1.11514, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4834 - acc: 0.8667 - val_loss: 1.1151 - val_acc: 0.0500\n",
            "Epoch 27/300\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 1.11514\n",
            "60/60 - 0s - loss: 0.4759 - acc: 0.8500 - val_loss: 1.1157 - val_acc: 0.0500\n",
            "Epoch 28/300\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 1.11514\n",
            "60/60 - 0s - loss: 0.4555 - acc: 0.9000 - val_loss: 1.1187 - val_acc: 0.0500\n",
            "Epoch 29/300\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 1.11514\n",
            "60/60 - 0s - loss: 0.4534 - acc: 0.8833 - val_loss: 1.1208 - val_acc: 0.0500\n",
            "Epoch 30/300\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 1.11514\n",
            "60/60 - 0s - loss: 0.4633 - acc: 0.9333 - val_loss: 1.1248 - val_acc: 0.0500\n",
            "Epoch 31/300\n",
            "\n",
            "Epoch 00031: val_loss improved from 1.11514 to 1.10810, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4928 - acc: 0.8667 - val_loss: 1.1081 - val_acc: 0.0500\n",
            "Epoch 32/300\n",
            "\n",
            "Epoch 00032: val_loss improved from 1.10810 to 1.10793, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4383 - acc: 0.8833 - val_loss: 1.1079 - val_acc: 0.0500\n",
            "Epoch 33/300\n",
            "\n",
            "Epoch 00033: val_loss improved from 1.10793 to 1.10648, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4328 - acc: 0.9000 - val_loss: 1.1065 - val_acc: 0.0500\n",
            "Epoch 34/300\n",
            "\n",
            "Epoch 00034: val_loss improved from 1.10648 to 1.09772, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4317 - acc: 0.8833 - val_loss: 1.0977 - val_acc: 0.0500\n",
            "Epoch 35/300\n",
            "\n",
            "Epoch 00035: val_loss improved from 1.09772 to 1.09066, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4307 - acc: 0.9333 - val_loss: 1.0907 - val_acc: 0.1000\n",
            "Epoch 36/300\n",
            "\n",
            "Epoch 00036: val_loss improved from 1.09066 to 1.08788, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4232 - acc: 0.8833 - val_loss: 1.0879 - val_acc: 0.1000\n",
            "Epoch 37/300\n",
            "\n",
            "Epoch 00037: val_loss improved from 1.08788 to 1.07176, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4449 - acc: 0.9000 - val_loss: 1.0718 - val_acc: 0.1500\n",
            "Epoch 38/300\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 1.07176\n",
            "60/60 - 0s - loss: 0.4207 - acc: 0.9000 - val_loss: 1.0745 - val_acc: 0.1500\n",
            "Epoch 39/300\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 1.07176\n",
            "60/60 - 0s - loss: 0.4235 - acc: 0.8833 - val_loss: 1.0800 - val_acc: 0.1500\n",
            "Epoch 40/300\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 1.07176\n",
            "60/60 - 0s - loss: 0.3979 - acc: 0.9333 - val_loss: 1.0975 - val_acc: 0.1500\n",
            "Epoch 41/300\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 1.07176\n",
            "60/60 - 0s - loss: 0.4152 - acc: 0.9000 - val_loss: 1.0796 - val_acc: 0.1500\n",
            "Epoch 42/300\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 1.07176\n",
            "60/60 - 0s - loss: 0.3724 - acc: 0.9500 - val_loss: 1.0856 - val_acc: 0.1500\n",
            "Epoch 43/300\n",
            "\n",
            "Epoch 00043: val_loss improved from 1.07176 to 1.07061, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4092 - acc: 0.9000 - val_loss: 1.0706 - val_acc: 0.1500\n",
            "Epoch 44/300\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 1.07061\n",
            "60/60 - 0s - loss: 0.3934 - acc: 0.9000 - val_loss: 1.0768 - val_acc: 0.1500\n",
            "Epoch 45/300\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 1.07061\n",
            "60/60 - 0s - loss: 0.3868 - acc: 0.9667 - val_loss: 1.1092 - val_acc: 0.1500\n",
            "Epoch 46/300\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 1.07061\n",
            "60/60 - 0s - loss: 0.3778 - acc: 0.9333 - val_loss: 1.1026 - val_acc: 0.1500\n",
            "Epoch 47/300\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 1.07061\n",
            "60/60 - 0s - loss: 0.3952 - acc: 0.9333 - val_loss: 1.0740 - val_acc: 0.1500\n",
            "Epoch 48/300\n",
            "\n",
            "Epoch 00048: val_loss improved from 1.07061 to 1.06836, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4183 - acc: 0.9000 - val_loss: 1.0684 - val_acc: 0.1500\n",
            "Epoch 49/300\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 1.06836\n",
            "60/60 - 0s - loss: 0.3846 - acc: 0.9167 - val_loss: 1.0788 - val_acc: 0.1500\n",
            "Epoch 50/300\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 1.06836\n",
            "60/60 - 0s - loss: 0.3699 - acc: 0.9500 - val_loss: 1.0930 - val_acc: 0.1500\n",
            "Epoch 51/300\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 1.06836\n",
            "60/60 - 0s - loss: 0.3659 - acc: 0.9333 - val_loss: 1.1141 - val_acc: 0.1500\n",
            "Epoch 52/300\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 1.06836\n",
            "60/60 - 0s - loss: 0.3612 - acc: 0.9500 - val_loss: 1.0986 - val_acc: 0.1500\n",
            "Epoch 53/300\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 1.06836\n",
            "60/60 - 0s - loss: 0.3740 - acc: 0.9333 - val_loss: 1.1251 - val_acc: 0.1500\n",
            "Epoch 54/300\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 1.06836\n",
            "60/60 - 0s - loss: 0.3557 - acc: 0.9333 - val_loss: 1.1318 - val_acc: 0.1500\n",
            "Epoch 55/300\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 1.06836\n",
            "60/60 - 0s - loss: 0.3473 - acc: 0.9333 - val_loss: 1.1555 - val_acc: 0.1500\n",
            "Epoch 56/300\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 1.06836\n",
            "60/60 - 0s - loss: 0.3456 - acc: 0.9500 - val_loss: 1.1677 - val_acc: 0.1500\n",
            "Epoch 57/300\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 1.06836\n",
            "60/60 - 0s - loss: 0.3442 - acc: 0.9667 - val_loss: 1.1672 - val_acc: 0.1500\n",
            "Epoch 58/300\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 1.06836\n",
            "60/60 - 0s - loss: 0.3442 - acc: 0.9833 - val_loss: 1.1109 - val_acc: 0.1500\n",
            "Epoch 59/300\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 1.06836\n",
            "60/60 - 0s - loss: 0.3830 - acc: 0.9333 - val_loss: 1.0818 - val_acc: 0.1500\n",
            "Epoch 60/300\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 1.06836\n",
            "60/60 - 0s - loss: 0.3458 - acc: 0.9167 - val_loss: 1.0979 - val_acc: 0.1500\n",
            "Epoch 61/300\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 1.06836\n",
            "60/60 - 0s - loss: 0.3430 - acc: 0.9500 - val_loss: 1.1423 - val_acc: 0.1500\n",
            "Epoch 62/300\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 1.06836\n",
            "60/60 - 0s - loss: 0.3391 - acc: 0.9500 - val_loss: 1.1504 - val_acc: 0.1500\n",
            "Epoch 63/300\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 1.06836\n",
            "60/60 - 0s - loss: 0.3518 - acc: 0.9333 - val_loss: 1.1185 - val_acc: 0.2000\n",
            "Epoch 64/300\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 1.06836\n",
            "60/60 - 0s - loss: 0.3706 - acc: 0.9333 - val_loss: 1.0918 - val_acc: 0.2000\n",
            "Epoch 65/300\n",
            "\n",
            "Epoch 00065: val_loss improved from 1.06836 to 1.06272, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3550 - acc: 0.9667 - val_loss: 1.0627 - val_acc: 0.2000\n",
            "Epoch 66/300\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 1.06272\n",
            "60/60 - 0s - loss: 0.3214 - acc: 0.9500 - val_loss: 1.0763 - val_acc: 0.2000\n",
            "Epoch 67/300\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 1.06272\n",
            "60/60 - 0s - loss: 0.3326 - acc: 0.9667 - val_loss: 1.0809 - val_acc: 0.1500\n",
            "Epoch 68/300\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 1.06272\n",
            "60/60 - 0s - loss: 0.3132 - acc: 0.9833 - val_loss: 1.1396 - val_acc: 0.1500\n",
            "Epoch 69/300\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 1.06272\n",
            "60/60 - 0s - loss: 0.3238 - acc: 0.9667 - val_loss: 1.1852 - val_acc: 0.1500\n",
            "Epoch 70/300\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 1.06272\n",
            "60/60 - 0s - loss: 0.3168 - acc: 0.9667 - val_loss: 1.1454 - val_acc: 0.1500\n",
            "Epoch 71/300\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 1.06272\n",
            "60/60 - 0s - loss: 0.3248 - acc: 0.9667 - val_loss: 1.1093 - val_acc: 0.1500\n",
            "Epoch 72/300\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 1.06272\n",
            "60/60 - 0s - loss: 0.3144 - acc: 0.9333 - val_loss: 1.0818 - val_acc: 0.1500\n",
            "Epoch 73/300\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 1.06272\n",
            "60/60 - 0s - loss: 0.3048 - acc: 0.9667 - val_loss: 1.0861 - val_acc: 0.1500\n",
            "Epoch 74/300\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 1.06272\n",
            "60/60 - 0s - loss: 0.3191 - acc: 0.9833 - val_loss: 1.0841 - val_acc: 0.1500\n",
            "Epoch 75/300\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 1.06272\n",
            "60/60 - 0s - loss: 0.2930 - acc: 0.9667 - val_loss: 1.0947 - val_acc: 0.1500\n",
            "Epoch 76/300\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 1.06272\n",
            "60/60 - 0s - loss: 0.3132 - acc: 0.9667 - val_loss: 1.1033 - val_acc: 0.1500\n",
            "Epoch 77/300\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 1.06272\n",
            "60/60 - 0s - loss: 0.2813 - acc: 0.9833 - val_loss: 1.1223 - val_acc: 0.1500\n",
            "Epoch 78/300\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 1.06272\n",
            "60/60 - 0s - loss: 0.3010 - acc: 0.9667 - val_loss: 1.1899 - val_acc: 0.1500\n",
            "Epoch 79/300\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 1.06272\n",
            "60/60 - 0s - loss: 0.3013 - acc: 0.9667 - val_loss: 1.2157 - val_acc: 0.1500\n",
            "Epoch 80/300\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 1.06272\n",
            "60/60 - 0s - loss: 0.2871 - acc: 0.9833 - val_loss: 1.1686 - val_acc: 0.1500\n",
            "Epoch 81/300\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 1.06272\n",
            "60/60 - 0s - loss: 0.3024 - acc: 0.9667 - val_loss: 1.0960 - val_acc: 0.2000\n",
            "Epoch 82/300\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 1.06272\n",
            "60/60 - 0s - loss: 0.2941 - acc: 0.9667 - val_loss: 1.0670 - val_acc: 0.2000\n",
            "Epoch 83/300\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 1.06272\n",
            "60/60 - 0s - loss: 0.2943 - acc: 0.9833 - val_loss: 1.0724 - val_acc: 0.2000\n",
            "Epoch 84/300\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 1.06272\n",
            "60/60 - 0s - loss: 0.2829 - acc: 0.9667 - val_loss: 1.0913 - val_acc: 0.2000\n",
            "Epoch 85/300\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 1.06272\n",
            "60/60 - 0s - loss: 0.3054 - acc: 0.9500 - val_loss: 1.1563 - val_acc: 0.2000\n",
            "Epoch 86/300\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 1.06272\n",
            "60/60 - 0s - loss: 0.2706 - acc: 0.9500 - val_loss: 1.1621 - val_acc: 0.2000\n",
            "Epoch 87/300\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 1.06272\n",
            "60/60 - 0s - loss: 0.2803 - acc: 0.9833 - val_loss: 1.1488 - val_acc: 0.2000\n",
            "Epoch 88/300\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 1.06272\n",
            "60/60 - 0s - loss: 0.2950 - acc: 0.9333 - val_loss: 1.1363 - val_acc: 0.2000\n",
            "Epoch 89/300\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 1.06272\n",
            "60/60 - 0s - loss: 0.2685 - acc: 0.9833 - val_loss: 1.0642 - val_acc: 0.2500\n",
            "Epoch 90/300\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 1.06272\n",
            "60/60 - 0s - loss: 0.2714 - acc: 0.9833 - val_loss: 1.0995 - val_acc: 0.2000\n",
            "Epoch 91/300\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 1.06272\n",
            "60/60 - 0s - loss: 0.2773 - acc: 0.9500 - val_loss: 1.1354 - val_acc: 0.2000\n",
            "Epoch 92/300\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 1.06272\n",
            "60/60 - 0s - loss: 0.2676 - acc: 0.9667 - val_loss: 1.1278 - val_acc: 0.2000\n",
            "Epoch 93/300\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 1.06272\n",
            "60/60 - 0s - loss: 0.2900 - acc: 0.9667 - val_loss: 1.0895 - val_acc: 0.2500\n",
            "Epoch 94/300\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 1.06272\n",
            "60/60 - 0s - loss: 0.2487 - acc: 1.0000 - val_loss: 1.1127 - val_acc: 0.2500\n",
            "Epoch 95/300\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 1.06272\n",
            "60/60 - 0s - loss: 0.2749 - acc: 1.0000 - val_loss: 1.1315 - val_acc: 0.2000\n",
            "Epoch 96/300\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 1.06272\n",
            "60/60 - 0s - loss: 0.2472 - acc: 0.9667 - val_loss: 1.1496 - val_acc: 0.1500\n",
            "Epoch 97/300\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 1.06272\n",
            "60/60 - 0s - loss: 0.2466 - acc: 0.9833 - val_loss: 1.1510 - val_acc: 0.2000\n",
            "Epoch 98/300\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 1.06272\n",
            "60/60 - 0s - loss: 0.2706 - acc: 0.9500 - val_loss: 1.1593 - val_acc: 0.2000\n",
            "Epoch 99/300\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 1.06272\n",
            "60/60 - 0s - loss: 0.2693 - acc: 0.9667 - val_loss: 1.0883 - val_acc: 0.2500\n",
            "Epoch 100/300\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 1.06272\n",
            "60/60 - 0s - loss: 0.2555 - acc: 0.9833 - val_loss: 1.0678 - val_acc: 0.3000\n",
            "Epoch 101/300\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 1.06272\n",
            "60/60 - 0s - loss: 0.2347 - acc: 0.9667 - val_loss: 1.1442 - val_acc: 0.2000\n",
            "Epoch 102/300\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 1.06272\n",
            "60/60 - 0s - loss: 0.2500 - acc: 0.9667 - val_loss: 1.1568 - val_acc: 0.2000\n",
            "Epoch 103/300\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 1.06272\n",
            "60/60 - 0s - loss: 0.2325 - acc: 0.9833 - val_loss: 1.2009 - val_acc: 0.1500\n",
            "Epoch 104/300\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 1.06272\n",
            "60/60 - 0s - loss: 0.2275 - acc: 0.9833 - val_loss: 1.1257 - val_acc: 0.2000\n",
            "Epoch 105/300\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 1.06272\n",
            "60/60 - 0s - loss: 0.2348 - acc: 0.9833 - val_loss: 1.1099 - val_acc: 0.3000\n",
            "Epoch 106/300\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 1.06272\n",
            "60/60 - 0s - loss: 0.2290 - acc: 0.9667 - val_loss: 1.1592 - val_acc: 0.2000\n",
            "Epoch 107/300\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 1.06272\n",
            "60/60 - 0s - loss: 0.2201 - acc: 1.0000 - val_loss: 1.2698 - val_acc: 0.2000\n",
            "Epoch 108/300\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 1.06272\n",
            "60/60 - 0s - loss: 0.2489 - acc: 0.9500 - val_loss: 1.3590 - val_acc: 0.1500\n",
            "Epoch 109/300\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 1.06272\n",
            "60/60 - 0s - loss: 0.2337 - acc: 0.9333 - val_loss: 1.5029 - val_acc: 0.0500\n",
            "Epoch 110/300\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 1.06272\n",
            "60/60 - 0s - loss: 0.2004 - acc: 0.9833 - val_loss: 1.4753 - val_acc: 0.0500\n",
            "Epoch 111/300\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 1.06272\n",
            "60/60 - 0s - loss: 0.2153 - acc: 0.9833 - val_loss: 1.4003 - val_acc: 0.1500\n",
            "Epoch 112/300\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 1.06272\n",
            "60/60 - 0s - loss: 0.2031 - acc: 1.0000 - val_loss: 1.3876 - val_acc: 0.2000\n",
            "Epoch 113/300\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 1.06272\n",
            "60/60 - 0s - loss: 0.1861 - acc: 1.0000 - val_loss: 1.3775 - val_acc: 0.2000\n",
            "Epoch 114/300\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 1.06272\n",
            "60/60 - 0s - loss: 0.1771 - acc: 1.0000 - val_loss: 1.4393 - val_acc: 0.2000\n",
            "Epoch 115/300\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 1.06272\n",
            "60/60 - 0s - loss: 0.2089 - acc: 0.9667 - val_loss: 1.5526 - val_acc: 0.2000\n",
            "Epoch 116/300\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 1.06272\n",
            "60/60 - 0s - loss: 0.2062 - acc: 0.9667 - val_loss: 1.6610 - val_acc: 0.2000\n",
            "Epoch 117/300\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 1.06272\n",
            "60/60 - 0s - loss: 0.2191 - acc: 0.9500 - val_loss: 1.6965 - val_acc: 0.2000\n",
            "Epoch 118/300\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 1.06272\n",
            "60/60 - 0s - loss: 0.1707 - acc: 0.9833 - val_loss: 1.5942 - val_acc: 0.2500\n",
            "Epoch 119/300\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 1.06272\n",
            "60/60 - 0s - loss: 0.1970 - acc: 0.9667 - val_loss: 1.4765 - val_acc: 0.2500\n",
            "Epoch 120/300\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 1.06272\n",
            "60/60 - 0s - loss: 0.2098 - acc: 0.9833 - val_loss: 1.4328 - val_acc: 0.2500\n",
            "Epoch 121/300\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 1.06272\n",
            "60/60 - 0s - loss: 0.1854 - acc: 0.9833 - val_loss: 1.4077 - val_acc: 0.3000\n",
            "Epoch 122/300\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 1.06272\n",
            "60/60 - 0s - loss: 0.1957 - acc: 0.9500 - val_loss: 1.4844 - val_acc: 0.3000\n",
            "Epoch 123/300\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 1.06272\n",
            "60/60 - 0s - loss: 0.1991 - acc: 0.9667 - val_loss: 1.4482 - val_acc: 0.3000\n",
            "Epoch 124/300\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 1.06272\n",
            "60/60 - 0s - loss: 0.1700 - acc: 0.9667 - val_loss: 1.4789 - val_acc: 0.3000\n",
            "Epoch 125/300\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 1.06272\n",
            "60/60 - 0s - loss: 0.1693 - acc: 0.9667 - val_loss: 1.5403 - val_acc: 0.2500\n",
            "Epoch 126/300\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 1.06272\n",
            "60/60 - 0s - loss: 0.1481 - acc: 0.9667 - val_loss: 1.4806 - val_acc: 0.3000\n",
            "Epoch 127/300\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 1.06272\n",
            "60/60 - 0s - loss: 0.1620 - acc: 1.0000 - val_loss: 1.4924 - val_acc: 0.3000\n",
            "Epoch 128/300\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 1.06272\n",
            "60/60 - 0s - loss: 0.1913 - acc: 0.9667 - val_loss: 1.5329 - val_acc: 0.3000\n",
            "Epoch 129/300\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 1.06272\n",
            "60/60 - 0s - loss: 0.1627 - acc: 0.9667 - val_loss: 1.6624 - val_acc: 0.2000\n",
            "Epoch 130/300\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 1.06272\n",
            "60/60 - 0s - loss: 0.1449 - acc: 0.9667 - val_loss: 1.7057 - val_acc: 0.2000\n",
            "Epoch 131/300\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 1.06272\n",
            "60/60 - 0s - loss: 0.1556 - acc: 0.9667 - val_loss: 1.7438 - val_acc: 0.2000\n",
            "Epoch 132/300\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 1.06272\n",
            "60/60 - 0s - loss: 0.1534 - acc: 0.9833 - val_loss: 1.5977 - val_acc: 0.3000\n",
            "Epoch 133/300\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 1.06272\n",
            "60/60 - 0s - loss: 0.1523 - acc: 1.0000 - val_loss: 1.5555 - val_acc: 0.3000\n",
            "Epoch 134/300\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 1.06272\n",
            "60/60 - 0s - loss: 0.1655 - acc: 0.9833 - val_loss: 1.5009 - val_acc: 0.3000\n",
            "Epoch 135/300\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 1.06272\n",
            "60/60 - 0s - loss: 0.1520 - acc: 0.9833 - val_loss: 1.4045 - val_acc: 0.4000\n",
            "Epoch 136/300\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 1.06272\n",
            "60/60 - 0s - loss: 0.1517 - acc: 0.9833 - val_loss: 1.4111 - val_acc: 0.4000\n",
            "Epoch 137/300\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 1.06272\n",
            "60/60 - 0s - loss: 0.1774 - acc: 0.9667 - val_loss: 1.4637 - val_acc: 0.4000\n",
            "Epoch 138/300\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 1.06272\n",
            "60/60 - 0s - loss: 0.1650 - acc: 0.9833 - val_loss: 1.4249 - val_acc: 0.4000\n",
            "Epoch 139/300\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 1.06272\n",
            "60/60 - 0s - loss: 0.1524 - acc: 0.9667 - val_loss: 1.3778 - val_acc: 0.4000\n",
            "Epoch 140/300\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 1.06272\n",
            "60/60 - 0s - loss: 0.1266 - acc: 0.9833 - val_loss: 1.3481 - val_acc: 0.4000\n",
            "Epoch 141/300\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 1.06272\n",
            "60/60 - 0s - loss: 0.1564 - acc: 0.9667 - val_loss: 1.3990 - val_acc: 0.4000\n",
            "Epoch 142/300\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 1.06272\n",
            "60/60 - 0s - loss: 0.1269 - acc: 1.0000 - val_loss: 1.4203 - val_acc: 0.4000\n",
            "Epoch 143/300\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 1.06272\n",
            "60/60 - 0s - loss: 0.1325 - acc: 1.0000 - val_loss: 1.4482 - val_acc: 0.4000\n",
            "Epoch 144/300\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 1.06272\n",
            "60/60 - 0s - loss: 0.1474 - acc: 0.9667 - val_loss: 1.3803 - val_acc: 0.4500\n",
            "Epoch 145/300\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 1.06272\n",
            "60/60 - 0s - loss: 0.1313 - acc: 0.9667 - val_loss: 1.3044 - val_acc: 0.4500\n",
            "Epoch 146/300\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 1.06272\n",
            "60/60 - 0s - loss: 0.1683 - acc: 0.9500 - val_loss: 1.3296 - val_acc: 0.4500\n",
            "Epoch 147/300\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 1.06272\n",
            "60/60 - 0s - loss: 0.1278 - acc: 0.9667 - val_loss: 1.3416 - val_acc: 0.4500\n",
            "Epoch 148/300\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 1.06272\n",
            "60/60 - 0s - loss: 0.1186 - acc: 0.9833 - val_loss: 1.3053 - val_acc: 0.4500\n",
            "Epoch 149/300\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 1.06272\n",
            "60/60 - 0s - loss: 0.1291 - acc: 0.9667 - val_loss: 1.1643 - val_acc: 0.4500\n",
            "Epoch 150/300\n",
            "\n",
            "Epoch 00150: val_loss improved from 1.06272 to 1.00937, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1558 - acc: 0.9667 - val_loss: 1.0094 - val_acc: 0.4500\n",
            "Epoch 151/300\n",
            "\n",
            "Epoch 00151: val_loss improved from 1.00937 to 0.98812, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1182 - acc: 1.0000 - val_loss: 0.9881 - val_acc: 0.5000\n",
            "Epoch 152/300\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.98812\n",
            "60/60 - 0s - loss: 0.1191 - acc: 1.0000 - val_loss: 1.0888 - val_acc: 0.4500\n",
            "Epoch 153/300\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 0.98812\n",
            "60/60 - 0s - loss: 0.1328 - acc: 1.0000 - val_loss: 1.1026 - val_acc: 0.5000\n",
            "Epoch 154/300\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.98812\n",
            "60/60 - 0s - loss: 0.1246 - acc: 0.9833 - val_loss: 1.1481 - val_acc: 0.4500\n",
            "Epoch 155/300\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.98812\n",
            "60/60 - 0s - loss: 0.1081 - acc: 0.9833 - val_loss: 1.1383 - val_acc: 0.5000\n",
            "Epoch 156/300\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.98812\n",
            "60/60 - 0s - loss: 0.1379 - acc: 0.9833 - val_loss: 1.0964 - val_acc: 0.5000\n",
            "Epoch 157/300\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.98812\n",
            "60/60 - 0s - loss: 0.1327 - acc: 0.9833 - val_loss: 0.9884 - val_acc: 0.5000\n",
            "Epoch 158/300\n",
            "\n",
            "Epoch 00158: val_loss improved from 0.98812 to 0.93732, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1544 - acc: 0.9833 - val_loss: 0.9373 - val_acc: 0.5000\n",
            "Epoch 159/300\n",
            "\n",
            "Epoch 00159: val_loss improved from 0.93732 to 0.93422, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1470 - acc: 0.9667 - val_loss: 0.9342 - val_acc: 0.5000\n",
            "Epoch 160/300\n",
            "\n",
            "Epoch 00160: val_loss improved from 0.93422 to 0.92112, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1278 - acc: 0.9667 - val_loss: 0.9211 - val_acc: 0.5000\n",
            "Epoch 161/300\n",
            "\n",
            "Epoch 00161: val_loss improved from 0.92112 to 0.88234, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1260 - acc: 0.9833 - val_loss: 0.8823 - val_acc: 0.5000\n",
            "Epoch 162/300\n",
            "\n",
            "Epoch 00162: val_loss improved from 0.88234 to 0.87123, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1340 - acc: 1.0000 - val_loss: 0.8712 - val_acc: 0.5000\n",
            "Epoch 163/300\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.87123\n",
            "60/60 - 0s - loss: 0.1178 - acc: 0.9833 - val_loss: 0.8897 - val_acc: 0.5000\n",
            "Epoch 164/300\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.87123\n",
            "60/60 - 0s - loss: 0.1094 - acc: 0.9833 - val_loss: 0.9404 - val_acc: 0.5000\n",
            "Epoch 165/300\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.87123\n",
            "60/60 - 0s - loss: 0.1779 - acc: 0.9333 - val_loss: 0.9544 - val_acc: 0.5000\n",
            "Epoch 166/300\n",
            "\n",
            "Epoch 00166: val_loss improved from 0.87123 to 0.86695, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1056 - acc: 1.0000 - val_loss: 0.8669 - val_acc: 0.5000\n",
            "Epoch 167/300\n",
            "\n",
            "Epoch 00167: val_loss improved from 0.86695 to 0.84832, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1069 - acc: 0.9833 - val_loss: 0.8483 - val_acc: 0.5000\n",
            "Epoch 168/300\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.84832\n",
            "60/60 - 0s - loss: 0.1486 - acc: 0.9833 - val_loss: 0.9423 - val_acc: 0.5000\n",
            "Epoch 169/300\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 0.84832\n",
            "60/60 - 0s - loss: 0.1432 - acc: 0.9667 - val_loss: 1.0792 - val_acc: 0.5000\n",
            "Epoch 170/300\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.84832\n",
            "60/60 - 0s - loss: 0.1629 - acc: 0.9667 - val_loss: 1.0684 - val_acc: 0.5000\n",
            "Epoch 171/300\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.84832\n",
            "60/60 - 0s - loss: 0.1112 - acc: 0.9833 - val_loss: 1.1252 - val_acc: 0.5000\n",
            "Epoch 172/300\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.84832\n",
            "60/60 - 0s - loss: 0.1393 - acc: 0.9833 - val_loss: 1.0221 - val_acc: 0.5000\n",
            "Epoch 173/300\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.84832\n",
            "60/60 - 0s - loss: 0.1268 - acc: 1.0000 - val_loss: 0.9715 - val_acc: 0.5000\n",
            "Epoch 174/300\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.84832\n",
            "60/60 - 0s - loss: 0.1499 - acc: 0.9667 - val_loss: 0.9282 - val_acc: 0.5000\n",
            "Epoch 175/300\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.84832\n",
            "60/60 - 0s - loss: 0.1147 - acc: 0.9833 - val_loss: 0.9613 - val_acc: 0.5000\n",
            "Epoch 176/300\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 0.84832\n",
            "60/60 - 0s - loss: 0.1268 - acc: 0.9833 - val_loss: 0.9389 - val_acc: 0.5000\n",
            "Epoch 177/300\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.84832\n",
            "60/60 - 0s - loss: 0.1306 - acc: 0.9833 - val_loss: 0.8750 - val_acc: 0.5000\n",
            "Epoch 178/300\n",
            "\n",
            "Epoch 00178: val_loss improved from 0.84832 to 0.77035, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.0864 - acc: 1.0000 - val_loss: 0.7703 - val_acc: 0.6500\n",
            "Epoch 179/300\n",
            "\n",
            "Epoch 00179: val_loss improved from 0.77035 to 0.72766, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1120 - acc: 1.0000 - val_loss: 0.7277 - val_acc: 0.6500\n",
            "Epoch 180/300\n",
            "\n",
            "Epoch 00180: val_loss improved from 0.72766 to 0.71914, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1138 - acc: 1.0000 - val_loss: 0.7191 - val_acc: 0.6500\n",
            "Epoch 181/300\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.71914\n",
            "60/60 - 0s - loss: 0.1310 - acc: 0.9667 - val_loss: 0.7828 - val_acc: 0.5500\n",
            "Epoch 182/300\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.71914\n",
            "60/60 - 0s - loss: 0.1130 - acc: 0.9833 - val_loss: 0.8307 - val_acc: 0.5500\n",
            "Epoch 183/300\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.71914\n",
            "60/60 - 0s - loss: 0.1182 - acc: 0.9833 - val_loss: 0.7907 - val_acc: 0.6500\n",
            "Epoch 184/300\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 0.71914\n",
            "60/60 - 0s - loss: 0.1097 - acc: 1.0000 - val_loss: 0.7761 - val_acc: 0.6500\n",
            "Epoch 185/300\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.71914\n",
            "60/60 - 0s - loss: 0.1256 - acc: 0.9667 - val_loss: 0.7793 - val_acc: 0.6000\n",
            "Epoch 186/300\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.71914\n",
            "60/60 - 0s - loss: 0.1294 - acc: 0.9667 - val_loss: 0.8156 - val_acc: 0.5000\n",
            "Epoch 187/300\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 0.71914\n",
            "60/60 - 0s - loss: 0.1048 - acc: 0.9833 - val_loss: 0.8508 - val_acc: 0.5000\n",
            "Epoch 188/300\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.71914\n",
            "60/60 - 0s - loss: 0.1466 - acc: 0.9667 - val_loss: 0.8568 - val_acc: 0.5000\n",
            "Epoch 189/300\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.71914\n",
            "60/60 - 0s - loss: 0.0873 - acc: 1.0000 - val_loss: 0.7821 - val_acc: 0.5500\n",
            "Epoch 190/300\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.71914\n",
            "60/60 - 0s - loss: 0.1186 - acc: 0.9833 - val_loss: 0.8409 - val_acc: 0.5500\n",
            "Epoch 191/300\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.71914\n",
            "60/60 - 0s - loss: 0.1386 - acc: 0.9500 - val_loss: 0.8854 - val_acc: 0.5000\n",
            "Epoch 192/300\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 0.71914\n",
            "60/60 - 0s - loss: 0.0880 - acc: 1.0000 - val_loss: 0.8369 - val_acc: 0.5000\n",
            "Epoch 193/300\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.71914\n",
            "60/60 - 0s - loss: 0.1203 - acc: 0.9833 - val_loss: 0.8746 - val_acc: 0.5000\n",
            "Epoch 194/300\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.71914\n",
            "60/60 - 0s - loss: 0.1016 - acc: 0.9833 - val_loss: 0.8293 - val_acc: 0.5000\n",
            "Epoch 195/300\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.71914\n",
            "60/60 - 0s - loss: 0.1371 - acc: 0.9833 - val_loss: 0.8744 - val_acc: 0.5000\n",
            "Epoch 196/300\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.71914\n",
            "60/60 - 0s - loss: 0.1104 - acc: 1.0000 - val_loss: 0.8827 - val_acc: 0.5000\n",
            "Epoch 197/300\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.71914\n",
            "60/60 - 0s - loss: 0.1385 - acc: 0.9667 - val_loss: 0.8553 - val_acc: 0.5000\n",
            "Epoch 198/300\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.71914\n",
            "60/60 - 0s - loss: 0.0874 - acc: 1.0000 - val_loss: 0.8965 - val_acc: 0.5000\n",
            "Epoch 199/300\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.71914\n",
            "60/60 - 0s - loss: 0.1236 - acc: 1.0000 - val_loss: 0.9233 - val_acc: 0.5000\n",
            "Epoch 200/300\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.71914\n",
            "60/60 - 0s - loss: 0.1011 - acc: 0.9833 - val_loss: 0.8665 - val_acc: 0.5000\n",
            "Epoch 201/300\n",
            "\n",
            "Epoch 00201: val_loss did not improve from 0.71914\n",
            "60/60 - 0s - loss: 0.0873 - acc: 1.0000 - val_loss: 0.8382 - val_acc: 0.5000\n",
            "Epoch 202/300\n",
            "\n",
            "Epoch 00202: val_loss did not improve from 0.71914\n",
            "60/60 - 0s - loss: 0.0928 - acc: 1.0000 - val_loss: 0.8210 - val_acc: 0.5000\n",
            "Epoch 203/300\n",
            "\n",
            "Epoch 00203: val_loss did not improve from 0.71914\n",
            "60/60 - 0s - loss: 0.1222 - acc: 0.9833 - val_loss: 0.8656 - val_acc: 0.5000\n",
            "Epoch 204/300\n",
            "\n",
            "Epoch 00204: val_loss did not improve from 0.71914\n",
            "60/60 - 0s - loss: 0.1002 - acc: 1.0000 - val_loss: 0.8306 - val_acc: 0.5000\n",
            "Epoch 205/300\n",
            "\n",
            "Epoch 00205: val_loss did not improve from 0.71914\n",
            "60/60 - 0s - loss: 0.1067 - acc: 0.9833 - val_loss: 0.8129 - val_acc: 0.5000\n",
            "Epoch 206/300\n",
            "\n",
            "Epoch 00206: val_loss did not improve from 0.71914\n",
            "60/60 - 0s - loss: 0.0892 - acc: 1.0000 - val_loss: 0.8264 - val_acc: 0.5000\n",
            "Epoch 207/300\n",
            "\n",
            "Epoch 00207: val_loss did not improve from 0.71914\n",
            "60/60 - 0s - loss: 0.1192 - acc: 1.0000 - val_loss: 0.8069 - val_acc: 0.5000\n",
            "Epoch 208/300\n",
            "\n",
            "Epoch 00208: val_loss did not improve from 0.71914\n",
            "60/60 - 0s - loss: 0.0910 - acc: 1.0000 - val_loss: 0.7535 - val_acc: 0.5500\n",
            "Epoch 209/300\n",
            "\n",
            "Epoch 00209: val_loss did not improve from 0.71914\n",
            "60/60 - 0s - loss: 0.0902 - acc: 0.9833 - val_loss: 0.7619 - val_acc: 0.5500\n",
            "Epoch 210/300\n",
            "\n",
            "Epoch 00210: val_loss did not improve from 0.71914\n",
            "60/60 - 0s - loss: 0.0768 - acc: 1.0000 - val_loss: 0.7489 - val_acc: 0.5500\n",
            "Epoch 211/300\n",
            "\n",
            "Epoch 00211: val_loss did not improve from 0.71914\n",
            "60/60 - 0s - loss: 0.1049 - acc: 0.9667 - val_loss: 0.7505 - val_acc: 0.5500\n",
            "Epoch 212/300\n",
            "\n",
            "Epoch 00212: val_loss did not improve from 0.71914\n",
            "60/60 - 0s - loss: 0.0857 - acc: 0.9833 - val_loss: 0.7383 - val_acc: 0.5500\n",
            "Epoch 213/300\n",
            "\n",
            "Epoch 00213: val_loss did not improve from 0.71914\n",
            "60/60 - 0s - loss: 0.1197 - acc: 0.9667 - val_loss: 0.7477 - val_acc: 0.5500\n",
            "Epoch 214/300\n",
            "\n",
            "Epoch 00214: val_loss improved from 0.71914 to 0.62856, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.0936 - acc: 0.9833 - val_loss: 0.6286 - val_acc: 0.6000\n",
            "Epoch 215/300\n",
            "\n",
            "Epoch 00215: val_loss did not improve from 0.62856\n",
            "60/60 - 0s - loss: 0.1145 - acc: 1.0000 - val_loss: 0.6392 - val_acc: 0.6500\n",
            "Epoch 216/300\n",
            "\n",
            "Epoch 00216: val_loss did not improve from 0.62856\n",
            "60/60 - 0s - loss: 0.0689 - acc: 1.0000 - val_loss: 0.6436 - val_acc: 0.6500\n",
            "Epoch 217/300\n",
            "\n",
            "Epoch 00217: val_loss did not improve from 0.62856\n",
            "60/60 - 0s - loss: 0.1171 - acc: 0.9833 - val_loss: 0.7682 - val_acc: 0.6000\n",
            "Epoch 218/300\n",
            "\n",
            "Epoch 00218: val_loss did not improve from 0.62856\n",
            "60/60 - 0s - loss: 0.0663 - acc: 1.0000 - val_loss: 0.9551 - val_acc: 0.5000\n",
            "Epoch 219/300\n",
            "\n",
            "Epoch 00219: val_loss did not improve from 0.62856\n",
            "60/60 - 0s - loss: 0.1040 - acc: 0.9667 - val_loss: 0.9618 - val_acc: 0.5000\n",
            "Epoch 220/300\n",
            "\n",
            "Epoch 00220: val_loss did not improve from 0.62856\n",
            "60/60 - 0s - loss: 0.0779 - acc: 1.0000 - val_loss: 0.8134 - val_acc: 0.5000\n",
            "Epoch 221/300\n",
            "\n",
            "Epoch 00221: val_loss did not improve from 0.62856\n",
            "60/60 - 0s - loss: 0.0637 - acc: 1.0000 - val_loss: 0.7783 - val_acc: 0.5500\n",
            "Epoch 222/300\n",
            "\n",
            "Epoch 00222: val_loss did not improve from 0.62856\n",
            "60/60 - 0s - loss: 0.0776 - acc: 1.0000 - val_loss: 0.8392 - val_acc: 0.5000\n",
            "Epoch 223/300\n",
            "\n",
            "Epoch 00223: val_loss did not improve from 0.62856\n",
            "60/60 - 0s - loss: 0.0941 - acc: 0.9833 - val_loss: 0.9456 - val_acc: 0.5000\n",
            "Epoch 224/300\n",
            "\n",
            "Epoch 00224: val_loss did not improve from 0.62856\n",
            "60/60 - 0s - loss: 0.1124 - acc: 0.9833 - val_loss: 0.9122 - val_acc: 0.5000\n",
            "Epoch 225/300\n",
            "\n",
            "Epoch 00225: val_loss did not improve from 0.62856\n",
            "60/60 - 0s - loss: 0.0980 - acc: 1.0000 - val_loss: 0.7585 - val_acc: 0.5500\n",
            "Epoch 226/300\n",
            "\n",
            "Epoch 00226: val_loss did not improve from 0.62856\n",
            "60/60 - 0s - loss: 0.0779 - acc: 1.0000 - val_loss: 0.6810 - val_acc: 0.5500\n",
            "Epoch 227/300\n",
            "\n",
            "Epoch 00227: val_loss improved from 0.62856 to 0.60391, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1272 - acc: 0.9833 - val_loss: 0.6039 - val_acc: 0.6500\n",
            "Epoch 228/300\n",
            "\n",
            "Epoch 00228: val_loss did not improve from 0.60391\n",
            "60/60 - 0s - loss: 0.0638 - acc: 1.0000 - val_loss: 0.6286 - val_acc: 0.6500\n",
            "Epoch 229/300\n",
            "\n",
            "Epoch 00229: val_loss did not improve from 0.60391\n",
            "60/60 - 0s - loss: 0.0744 - acc: 1.0000 - val_loss: 0.6552 - val_acc: 0.6500\n",
            "Epoch 230/300\n",
            "\n",
            "Epoch 00230: val_loss did not improve from 0.60391\n",
            "60/60 - 0s - loss: 0.0722 - acc: 1.0000 - val_loss: 0.6609 - val_acc: 0.6500\n",
            "Epoch 231/300\n",
            "\n",
            "Epoch 00231: val_loss did not improve from 0.60391\n",
            "60/60 - 0s - loss: 0.1016 - acc: 0.9833 - val_loss: 0.6657 - val_acc: 0.6500\n",
            "Epoch 232/300\n",
            "\n",
            "Epoch 00232: val_loss improved from 0.60391 to 0.58147, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.0857 - acc: 0.9833 - val_loss: 0.5815 - val_acc: 0.7000\n",
            "Epoch 233/300\n",
            "\n",
            "Epoch 00233: val_loss improved from 0.58147 to 0.57202, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1140 - acc: 0.9667 - val_loss: 0.5720 - val_acc: 0.6500\n",
            "Epoch 234/300\n",
            "\n",
            "Epoch 00234: val_loss did not improve from 0.57202\n",
            "60/60 - 0s - loss: 0.0724 - acc: 1.0000 - val_loss: 0.6030 - val_acc: 0.6500\n",
            "Epoch 235/300\n",
            "\n",
            "Epoch 00235: val_loss did not improve from 0.57202\n",
            "60/60 - 0s - loss: 0.0973 - acc: 0.9833 - val_loss: 0.6276 - val_acc: 0.6500\n",
            "Epoch 236/300\n",
            "\n",
            "Epoch 00236: val_loss did not improve from 0.57202\n",
            "60/60 - 0s - loss: 0.0997 - acc: 0.9833 - val_loss: 0.6257 - val_acc: 0.6500\n",
            "Epoch 237/300\n",
            "\n",
            "Epoch 00237: val_loss did not improve from 0.57202\n",
            "60/60 - 0s - loss: 0.0877 - acc: 0.9833 - val_loss: 0.6559 - val_acc: 0.6500\n",
            "Epoch 238/300\n",
            "\n",
            "Epoch 00238: val_loss did not improve from 0.57202\n",
            "60/60 - 0s - loss: 0.0637 - acc: 1.0000 - val_loss: 0.6896 - val_acc: 0.6500\n",
            "Epoch 239/300\n",
            "\n",
            "Epoch 00239: val_loss did not improve from 0.57202\n",
            "60/60 - 0s - loss: 0.1038 - acc: 0.9667 - val_loss: 0.6257 - val_acc: 0.7000\n",
            "Epoch 240/300\n",
            "\n",
            "Epoch 00240: val_loss did not improve from 0.57202\n",
            "60/60 - 0s - loss: 0.0695 - acc: 1.0000 - val_loss: 0.5971 - val_acc: 0.7500\n",
            "Epoch 241/300\n",
            "\n",
            "Epoch 00241: val_loss improved from 0.57202 to 0.56042, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.0920 - acc: 1.0000 - val_loss: 0.5604 - val_acc: 0.7500\n",
            "Epoch 242/300\n",
            "\n",
            "Epoch 00242: val_loss did not improve from 0.56042\n",
            "60/60 - 0s - loss: 0.0940 - acc: 1.0000 - val_loss: 0.5707 - val_acc: 0.7500\n",
            "Epoch 243/300\n",
            "\n",
            "Epoch 00243: val_loss did not improve from 0.56042\n",
            "60/60 - 0s - loss: 0.0857 - acc: 1.0000 - val_loss: 0.5775 - val_acc: 0.7500\n",
            "Epoch 244/300\n",
            "\n",
            "Epoch 00244: val_loss did not improve from 0.56042\n",
            "60/60 - 0s - loss: 0.1065 - acc: 0.9667 - val_loss: 0.6157 - val_acc: 0.7000\n",
            "Epoch 245/300\n",
            "\n",
            "Epoch 00245: val_loss did not improve from 0.56042\n",
            "60/60 - 0s - loss: 0.0677 - acc: 1.0000 - val_loss: 0.5793 - val_acc: 0.7000\n",
            "Epoch 246/300\n",
            "\n",
            "Epoch 00246: val_loss did not improve from 0.56042\n",
            "60/60 - 0s - loss: 0.0833 - acc: 0.9833 - val_loss: 0.5684 - val_acc: 0.7000\n",
            "Epoch 247/300\n",
            "\n",
            "Epoch 00247: val_loss did not improve from 0.56042\n",
            "60/60 - 0s - loss: 0.0847 - acc: 0.9833 - val_loss: 0.6787 - val_acc: 0.6000\n",
            "Epoch 248/300\n",
            "\n",
            "Epoch 00248: val_loss did not improve from 0.56042\n",
            "60/60 - 0s - loss: 0.0967 - acc: 0.9833 - val_loss: 0.7154 - val_acc: 0.6000\n",
            "Epoch 249/300\n",
            "\n",
            "Epoch 00249: val_loss did not improve from 0.56042\n",
            "60/60 - 0s - loss: 0.0793 - acc: 0.9833 - val_loss: 0.7226 - val_acc: 0.5500\n",
            "Epoch 250/300\n",
            "\n",
            "Epoch 00250: val_loss did not improve from 0.56042\n",
            "60/60 - 0s - loss: 0.0662 - acc: 1.0000 - val_loss: 0.7146 - val_acc: 0.5500\n",
            "Epoch 251/300\n",
            "\n",
            "Epoch 00251: val_loss did not improve from 0.56042\n",
            "60/60 - 0s - loss: 0.1042 - acc: 0.9667 - val_loss: 0.6200 - val_acc: 0.7000\n",
            "Epoch 252/300\n",
            "\n",
            "Epoch 00252: val_loss improved from 0.56042 to 0.53761, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1002 - acc: 0.9833 - val_loss: 0.5376 - val_acc: 0.7000\n",
            "Epoch 253/300\n",
            "\n",
            "Epoch 00253: val_loss improved from 0.53761 to 0.48501, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.0790 - acc: 1.0000 - val_loss: 0.4850 - val_acc: 0.7500\n",
            "Epoch 254/300\n",
            "\n",
            "Epoch 00254: val_loss did not improve from 0.48501\n",
            "60/60 - 0s - loss: 0.0786 - acc: 1.0000 - val_loss: 0.5507 - val_acc: 0.7000\n",
            "Epoch 255/300\n",
            "\n",
            "Epoch 00255: val_loss did not improve from 0.48501\n",
            "60/60 - 0s - loss: 0.0694 - acc: 1.0000 - val_loss: 0.6233 - val_acc: 0.7000\n",
            "Epoch 256/300\n",
            "\n",
            "Epoch 00256: val_loss did not improve from 0.48501\n",
            "60/60 - 0s - loss: 0.0602 - acc: 1.0000 - val_loss: 0.6849 - val_acc: 0.6500\n",
            "Epoch 257/300\n",
            "\n",
            "Epoch 00257: val_loss did not improve from 0.48501\n",
            "60/60 - 0s - loss: 0.0807 - acc: 1.0000 - val_loss: 0.6598 - val_acc: 0.6500\n",
            "Epoch 258/300\n",
            "\n",
            "Epoch 00258: val_loss did not improve from 0.48501\n",
            "60/60 - 0s - loss: 0.0875 - acc: 0.9667 - val_loss: 0.6611 - val_acc: 0.7000\n",
            "Epoch 259/300\n",
            "\n",
            "Epoch 00259: val_loss did not improve from 0.48501\n",
            "60/60 - 0s - loss: 0.0657 - acc: 0.9833 - val_loss: 0.6294 - val_acc: 0.7000\n",
            "Epoch 260/300\n",
            "\n",
            "Epoch 00260: val_loss did not improve from 0.48501\n",
            "60/60 - 0s - loss: 0.0497 - acc: 1.0000 - val_loss: 0.6031 - val_acc: 0.7000\n",
            "Epoch 261/300\n",
            "\n",
            "Epoch 00261: val_loss did not improve from 0.48501\n",
            "60/60 - 0s - loss: 0.0574 - acc: 1.0000 - val_loss: 0.5984 - val_acc: 0.7000\n",
            "Epoch 262/300\n",
            "\n",
            "Epoch 00262: val_loss did not improve from 0.48501\n",
            "60/60 - 0s - loss: 0.0785 - acc: 1.0000 - val_loss: 0.5736 - val_acc: 0.7000\n",
            "Epoch 263/300\n",
            "\n",
            "Epoch 00263: val_loss did not improve from 0.48501\n",
            "60/60 - 0s - loss: 0.0666 - acc: 0.9833 - val_loss: 0.5603 - val_acc: 0.7000\n",
            "Epoch 264/300\n",
            "\n",
            "Epoch 00264: val_loss did not improve from 0.48501\n",
            "60/60 - 0s - loss: 0.0963 - acc: 0.9833 - val_loss: 0.5177 - val_acc: 0.7500\n",
            "Epoch 265/300\n",
            "\n",
            "Epoch 00265: val_loss did not improve from 0.48501\n",
            "60/60 - 0s - loss: 0.0786 - acc: 1.0000 - val_loss: 0.5507 - val_acc: 0.7000\n",
            "Epoch 266/300\n",
            "\n",
            "Epoch 00266: val_loss did not improve from 0.48501\n",
            "60/60 - 0s - loss: 0.0898 - acc: 1.0000 - val_loss: 0.6240 - val_acc: 0.6500\n",
            "Epoch 267/300\n",
            "\n",
            "Epoch 00267: val_loss did not improve from 0.48501\n",
            "60/60 - 0s - loss: 0.0714 - acc: 1.0000 - val_loss: 0.5694 - val_acc: 0.7000\n",
            "Epoch 268/300\n",
            "\n",
            "Epoch 00268: val_loss did not improve from 0.48501\n",
            "60/60 - 0s - loss: 0.0867 - acc: 1.0000 - val_loss: 0.5134 - val_acc: 0.7500\n",
            "Epoch 269/300\n",
            "\n",
            "Epoch 00269: val_loss did not improve from 0.48501\n",
            "60/60 - 0s - loss: 0.0804 - acc: 1.0000 - val_loss: 0.5017 - val_acc: 0.7500\n",
            "Epoch 270/300\n",
            "\n",
            "Epoch 00270: val_loss did not improve from 0.48501\n",
            "60/60 - 0s - loss: 0.0858 - acc: 0.9833 - val_loss: 0.5690 - val_acc: 0.6500\n",
            "Epoch 271/300\n",
            "\n",
            "Epoch 00271: val_loss did not improve from 0.48501\n",
            "60/60 - 0s - loss: 0.0750 - acc: 0.9833 - val_loss: 0.6075 - val_acc: 0.6500\n",
            "Epoch 272/300\n",
            "\n",
            "Epoch 00272: val_loss did not improve from 0.48501\n",
            "60/60 - 0s - loss: 0.0803 - acc: 0.9833 - val_loss: 0.6633 - val_acc: 0.6500\n",
            "Epoch 273/300\n",
            "\n",
            "Epoch 00273: val_loss did not improve from 0.48501\n",
            "60/60 - 0s - loss: 0.0976 - acc: 1.0000 - val_loss: 0.6923 - val_acc: 0.6500\n",
            "Epoch 274/300\n",
            "\n",
            "Epoch 00274: val_loss did not improve from 0.48501\n",
            "60/60 - 0s - loss: 0.0766 - acc: 0.9833 - val_loss: 0.7349 - val_acc: 0.6500\n",
            "Epoch 275/300\n",
            "\n",
            "Epoch 00275: val_loss did not improve from 0.48501\n",
            "60/60 - 0s - loss: 0.1039 - acc: 0.9833 - val_loss: 0.7899 - val_acc: 0.6500\n",
            "Epoch 276/300\n",
            "\n",
            "Epoch 00276: val_loss did not improve from 0.48501\n",
            "60/60 - 0s - loss: 0.0785 - acc: 1.0000 - val_loss: 0.6981 - val_acc: 0.6500\n",
            "Epoch 277/300\n",
            "\n",
            "Epoch 00277: val_loss did not improve from 0.48501\n",
            "60/60 - 0s - loss: 0.0830 - acc: 1.0000 - val_loss: 0.5798 - val_acc: 0.7500\n",
            "Epoch 278/300\n",
            "\n",
            "Epoch 00278: val_loss did not improve from 0.48501\n",
            "60/60 - 0s - loss: 0.0838 - acc: 1.0000 - val_loss: 0.5823 - val_acc: 0.8000\n",
            "Epoch 279/300\n",
            "\n",
            "Epoch 00279: val_loss did not improve from 0.48501\n",
            "60/60 - 0s - loss: 0.0779 - acc: 1.0000 - val_loss: 0.6330 - val_acc: 0.7000\n",
            "Epoch 280/300\n",
            "\n",
            "Epoch 00280: val_loss did not improve from 0.48501\n",
            "60/60 - 0s - loss: 0.0743 - acc: 1.0000 - val_loss: 0.6079 - val_acc: 0.7500\n",
            "Epoch 281/300\n",
            "\n",
            "Epoch 00281: val_loss did not improve from 0.48501\n",
            "60/60 - 0s - loss: 0.0891 - acc: 1.0000 - val_loss: 0.5753 - val_acc: 0.7500\n",
            "Epoch 282/300\n",
            "\n",
            "Epoch 00282: val_loss did not improve from 0.48501\n",
            "60/60 - 0s - loss: 0.0671 - acc: 1.0000 - val_loss: 0.6146 - val_acc: 0.7000\n",
            "Epoch 283/300\n",
            "\n",
            "Epoch 00283: val_loss did not improve from 0.48501\n",
            "60/60 - 0s - loss: 0.0556 - acc: 1.0000 - val_loss: 0.6594 - val_acc: 0.7000\n",
            "Epoch 284/300\n",
            "\n",
            "Epoch 00284: val_loss did not improve from 0.48501\n",
            "60/60 - 0s - loss: 0.0967 - acc: 1.0000 - val_loss: 0.5705 - val_acc: 0.7000\n",
            "Epoch 285/300\n",
            "\n",
            "Epoch 00285: val_loss did not improve from 0.48501\n",
            "60/60 - 0s - loss: 0.0622 - acc: 1.0000 - val_loss: 0.5822 - val_acc: 0.7000\n",
            "Epoch 286/300\n",
            "\n",
            "Epoch 00286: val_loss did not improve from 0.48501\n",
            "60/60 - 0s - loss: 0.0943 - acc: 1.0000 - val_loss: 0.5437 - val_acc: 0.7500\n",
            "Epoch 287/300\n",
            "\n",
            "Epoch 00287: val_loss did not improve from 0.48501\n",
            "60/60 - 0s - loss: 0.0920 - acc: 0.9833 - val_loss: 0.5253 - val_acc: 0.7500\n",
            "Epoch 288/300\n",
            "\n",
            "Epoch 00288: val_loss did not improve from 0.48501\n",
            "60/60 - 0s - loss: 0.0573 - acc: 1.0000 - val_loss: 0.6207 - val_acc: 0.6500\n",
            "Epoch 289/300\n",
            "\n",
            "Epoch 00289: val_loss did not improve from 0.48501\n",
            "60/60 - 0s - loss: 0.0717 - acc: 1.0000 - val_loss: 0.6454 - val_acc: 0.6000\n",
            "Epoch 290/300\n",
            "\n",
            "Epoch 00290: val_loss did not improve from 0.48501\n",
            "60/60 - 0s - loss: 0.0588 - acc: 1.0000 - val_loss: 0.5983 - val_acc: 0.7500\n",
            "Epoch 291/300\n",
            "\n",
            "Epoch 00291: val_loss did not improve from 0.48501\n",
            "60/60 - 0s - loss: 0.0514 - acc: 1.0000 - val_loss: 0.5781 - val_acc: 0.7500\n",
            "Epoch 292/300\n",
            "\n",
            "Epoch 00292: val_loss did not improve from 0.48501\n",
            "60/60 - 0s - loss: 0.0790 - acc: 1.0000 - val_loss: 0.5944 - val_acc: 0.7500\n",
            "Epoch 293/300\n",
            "\n",
            "Epoch 00293: val_loss did not improve from 0.48501\n",
            "60/60 - 0s - loss: 0.0601 - acc: 1.0000 - val_loss: 0.6742 - val_acc: 0.6500\n",
            "Epoch 294/300\n",
            "\n",
            "Epoch 00294: val_loss did not improve from 0.48501\n",
            "60/60 - 0s - loss: 0.0657 - acc: 1.0000 - val_loss: 0.6848 - val_acc: 0.7000\n",
            "Epoch 295/300\n",
            "\n",
            "Epoch 00295: val_loss did not improve from 0.48501\n",
            "60/60 - 0s - loss: 0.0615 - acc: 1.0000 - val_loss: 0.5862 - val_acc: 0.7500\n",
            "Epoch 296/300\n",
            "\n",
            "Epoch 00296: val_loss did not improve from 0.48501\n",
            "60/60 - 0s - loss: 0.1047 - acc: 0.9833 - val_loss: 0.5610 - val_acc: 0.7500\n",
            "Epoch 297/300\n",
            "\n",
            "Epoch 00297: val_loss did not improve from 0.48501\n",
            "60/60 - 0s - loss: 0.0466 - acc: 1.0000 - val_loss: 0.5565 - val_acc: 0.7500\n",
            "Epoch 298/300\n",
            "\n",
            "Epoch 00298: val_loss did not improve from 0.48501\n",
            "60/60 - 0s - loss: 0.0552 - acc: 1.0000 - val_loss: 0.6372 - val_acc: 0.7000\n",
            "Epoch 299/300\n",
            "\n",
            "Epoch 00299: val_loss did not improve from 0.48501\n",
            "60/60 - 0s - loss: 0.1084 - acc: 0.9500 - val_loss: 0.6905 - val_acc: 0.6000\n",
            "Epoch 300/300\n",
            "\n",
            "Epoch 00300: val_loss did not improve from 0.48501\n",
            "60/60 - 0s - loss: 0.0831 - acc: 1.0000 - val_loss: 0.5893 - val_acc: 0.7000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "# Evaluate on test data\n",
            "40/40 [==============================] - 0s 2ms/sample - loss: 0.4836 - acc: 0.8750\n",
            "test loss, test acc: [0.48363449776079503, 0.875]\n",
            "EEG_Deep/Data2A/parsed_P03T.mat\n",
            "Filtering of Training Data Finished\n",
            "EEG_Deep/Data2A/parsed_P03E.mat\n",
            "Filtering of Testing Data Finished\n",
            "(1, 2)\n",
            "Finding labels in training data\n",
            "Finding labels in testing data\n",
            "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
            "(60, 12, 1536)\n",
            "(60,)\n",
            "(20, 12, 1536)\n",
            "(20,)\n",
            "[[1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]]\n",
            "X_train shape: (60, 1, 12, 1536)\n",
            "60 train samples\n",
            "20 val samples\n",
            "(40, 12, 1536)\n",
            "(40, 1)\n",
            "X_train shape: (40, 1, 12, 1536)\n",
            "40 train samples\n",
            "Train on 60 samples, validate on 20 samples\n",
            "Epoch 1/300\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 1.37734, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 1s - loss: 1.3417 - acc: 0.5167 - val_loss: 1.3773 - val_acc: 0.6000\n",
            "Epoch 2/300\n",
            "\n",
            "Epoch 00002: val_loss improved from 1.37734 to 1.36961, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 1.1388 - acc: 0.7500 - val_loss: 1.3696 - val_acc: 0.5500\n",
            "Epoch 3/300\n",
            "\n",
            "Epoch 00003: val_loss improved from 1.36961 to 1.35923, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 1.0303 - acc: 0.6833 - val_loss: 1.3592 - val_acc: 0.2000\n",
            "Epoch 4/300\n",
            "\n",
            "Epoch 00004: val_loss improved from 1.35923 to 1.34897, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.9144 - acc: 0.7500 - val_loss: 1.3490 - val_acc: 0.1000\n",
            "Epoch 5/300\n",
            "\n",
            "Epoch 00005: val_loss improved from 1.34897 to 1.33674, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.8313 - acc: 0.7833 - val_loss: 1.3367 - val_acc: 0.0500\n",
            "Epoch 6/300\n",
            "\n",
            "Epoch 00006: val_loss improved from 1.33674 to 1.32820, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.7624 - acc: 0.7833 - val_loss: 1.3282 - val_acc: 0.0500\n",
            "Epoch 7/300\n",
            "\n",
            "Epoch 00007: val_loss improved from 1.32820 to 1.32048, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.7232 - acc: 0.7833 - val_loss: 1.3205 - val_acc: 0.0000e+00\n",
            "Epoch 8/300\n",
            "\n",
            "Epoch 00008: val_loss improved from 1.32048 to 1.31996, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6776 - acc: 0.7500 - val_loss: 1.3200 - val_acc: 0.0000e+00\n",
            "Epoch 9/300\n",
            "\n",
            "Epoch 00009: val_loss improved from 1.31996 to 1.31894, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6467 - acc: 0.7833 - val_loss: 1.3189 - val_acc: 0.0000e+00\n",
            "Epoch 10/300\n",
            "\n",
            "Epoch 00010: val_loss improved from 1.31894 to 1.31620, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6227 - acc: 0.8000 - val_loss: 1.3162 - val_acc: 0.0000e+00\n",
            "Epoch 11/300\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 1.31620\n",
            "60/60 - 0s - loss: 0.6407 - acc: 0.7833 - val_loss: 1.3162 - val_acc: 0.0000e+00\n",
            "Epoch 12/300\n",
            "\n",
            "Epoch 00012: val_loss improved from 1.31620 to 1.31349, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6068 - acc: 0.8000 - val_loss: 1.3135 - val_acc: 0.0000e+00\n",
            "Epoch 13/300\n",
            "\n",
            "Epoch 00013: val_loss improved from 1.31349 to 1.31225, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5947 - acc: 0.8333 - val_loss: 1.3123 - val_acc: 0.0000e+00\n",
            "Epoch 14/300\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 1.31225\n",
            "60/60 - 0s - loss: 0.5798 - acc: 0.7833 - val_loss: 1.3158 - val_acc: 0.0000e+00\n",
            "Epoch 15/300\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 1.31225\n",
            "60/60 - 0s - loss: 0.5995 - acc: 0.7500 - val_loss: 1.3225 - val_acc: 0.0000e+00\n",
            "Epoch 16/300\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 1.31225\n",
            "60/60 - 0s - loss: 0.5564 - acc: 0.7833 - val_loss: 1.3297 - val_acc: 0.0000e+00\n",
            "Epoch 17/300\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 1.31225\n",
            "60/60 - 0s - loss: 0.5867 - acc: 0.7667 - val_loss: 1.3248 - val_acc: 0.0000e+00\n",
            "Epoch 18/300\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 1.31225\n",
            "60/60 - 0s - loss: 0.5368 - acc: 0.8000 - val_loss: 1.3214 - val_acc: 0.0000e+00\n",
            "Epoch 19/300\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 1.31225\n",
            "60/60 - 0s - loss: 0.5333 - acc: 0.8333 - val_loss: 1.3166 - val_acc: 0.0000e+00\n",
            "Epoch 20/300\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 1.31225\n",
            "60/60 - 0s - loss: 0.5463 - acc: 0.8000 - val_loss: 1.3154 - val_acc: 0.0000e+00\n",
            "Epoch 21/300\n",
            "\n",
            "Epoch 00021: val_loss improved from 1.31225 to 1.31049, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5153 - acc: 0.8000 - val_loss: 1.3105 - val_acc: 0.0000e+00\n",
            "Epoch 22/300\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.5210 - acc: 0.8333 - val_loss: 1.3126 - val_acc: 0.0000e+00\n",
            "Epoch 23/300\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.5344 - acc: 0.7833 - val_loss: 1.3193 - val_acc: 0.0000e+00\n",
            "Epoch 24/300\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.5034 - acc: 0.8000 - val_loss: 1.3346 - val_acc: 0.0000e+00\n",
            "Epoch 25/300\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.5289 - acc: 0.7833 - val_loss: 1.3294 - val_acc: 0.0000e+00\n",
            "Epoch 26/300\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.5095 - acc: 0.8333 - val_loss: 1.3474 - val_acc: 0.0000e+00\n",
            "Epoch 27/300\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.4905 - acc: 0.8167 - val_loss: 1.3615 - val_acc: 0.0000e+00\n",
            "Epoch 28/300\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.5092 - acc: 0.8000 - val_loss: 1.3921 - val_acc: 0.0000e+00\n",
            "Epoch 29/300\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.4588 - acc: 0.8333 - val_loss: 1.4295 - val_acc: 0.0000e+00\n",
            "Epoch 30/300\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.4586 - acc: 0.8333 - val_loss: 1.4848 - val_acc: 0.0000e+00\n",
            "Epoch 31/300\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.4421 - acc: 0.8500 - val_loss: 1.5302 - val_acc: 0.0000e+00\n",
            "Epoch 32/300\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.4400 - acc: 0.8500 - val_loss: 1.5650 - val_acc: 0.0000e+00\n",
            "Epoch 33/300\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.4456 - acc: 0.8333 - val_loss: 1.5722 - val_acc: 0.0000e+00\n",
            "Epoch 34/300\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.4563 - acc: 0.8167 - val_loss: 1.5557 - val_acc: 0.0000e+00\n",
            "Epoch 35/300\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.4750 - acc: 0.8333 - val_loss: 1.5722 - val_acc: 0.0000e+00\n",
            "Epoch 36/300\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.4430 - acc: 0.8333 - val_loss: 1.6107 - val_acc: 0.0000e+00\n",
            "Epoch 37/300\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.4698 - acc: 0.8500 - val_loss: 1.6603 - val_acc: 0.0000e+00\n",
            "Epoch 38/300\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.4520 - acc: 0.8167 - val_loss: 1.6640 - val_acc: 0.0000e+00\n",
            "Epoch 39/300\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.4027 - acc: 0.8667 - val_loss: 1.7247 - val_acc: 0.0000e+00\n",
            "Epoch 40/300\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.3977 - acc: 0.8333 - val_loss: 1.7489 - val_acc: 0.0000e+00\n",
            "Epoch 41/300\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.3882 - acc: 0.8667 - val_loss: 1.8196 - val_acc: 0.0000e+00\n",
            "Epoch 42/300\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.4343 - acc: 0.8333 - val_loss: 1.8879 - val_acc: 0.0000e+00\n",
            "Epoch 43/300\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.3815 - acc: 0.8667 - val_loss: 1.9518 - val_acc: 0.0000e+00\n",
            "Epoch 44/300\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.3629 - acc: 0.8667 - val_loss: 1.9912 - val_acc: 0.0000e+00\n",
            "Epoch 45/300\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.3759 - acc: 0.8333 - val_loss: 2.0905 - val_acc: 0.0000e+00\n",
            "Epoch 46/300\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.3763 - acc: 0.8833 - val_loss: 2.1760 - val_acc: 0.0000e+00\n",
            "Epoch 47/300\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.3645 - acc: 0.8667 - val_loss: 2.2700 - val_acc: 0.0000e+00\n",
            "Epoch 48/300\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.3530 - acc: 0.8667 - val_loss: 2.4730 - val_acc: 0.0000e+00\n",
            "Epoch 49/300\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.3510 - acc: 0.8833 - val_loss: 2.6225 - val_acc: 0.0000e+00\n",
            "Epoch 50/300\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.3515 - acc: 0.8833 - val_loss: 2.6829 - val_acc: 0.0000e+00\n",
            "Epoch 51/300\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.3843 - acc: 0.8833 - val_loss: 2.7918 - val_acc: 0.0000e+00\n",
            "Epoch 52/300\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.3033 - acc: 0.9000 - val_loss: 2.7751 - val_acc: 0.0000e+00\n",
            "Epoch 53/300\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.3227 - acc: 0.9000 - val_loss: 2.8691 - val_acc: 0.0000e+00\n",
            "Epoch 54/300\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.3155 - acc: 0.9000 - val_loss: 2.8951 - val_acc: 0.0000e+00\n",
            "Epoch 55/300\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.3265 - acc: 0.8833 - val_loss: 3.0112 - val_acc: 0.0000e+00\n",
            "Epoch 56/300\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.2951 - acc: 0.9500 - val_loss: 3.1286 - val_acc: 0.0000e+00\n",
            "Epoch 57/300\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.3580 - acc: 0.8667 - val_loss: 3.2192 - val_acc: 0.0000e+00\n",
            "Epoch 58/300\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.3394 - acc: 0.9000 - val_loss: 3.2431 - val_acc: 0.0000e+00\n",
            "Epoch 59/300\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.3417 - acc: 0.8833 - val_loss: 3.3089 - val_acc: 0.0000e+00\n",
            "Epoch 60/300\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.2975 - acc: 0.9167 - val_loss: 3.4226 - val_acc: 0.0000e+00\n",
            "Epoch 61/300\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.2889 - acc: 0.9000 - val_loss: 3.5159 - val_acc: 0.0000e+00\n",
            "Epoch 62/300\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.3124 - acc: 0.9000 - val_loss: 3.6012 - val_acc: 0.0000e+00\n",
            "Epoch 63/300\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.2719 - acc: 0.9000 - val_loss: 3.7147 - val_acc: 0.0000e+00\n",
            "Epoch 64/300\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.2895 - acc: 0.8833 - val_loss: 3.7165 - val_acc: 0.0000e+00\n",
            "Epoch 65/300\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.2722 - acc: 0.9333 - val_loss: 3.8694 - val_acc: 0.0000e+00\n",
            "Epoch 66/300\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.2789 - acc: 0.8833 - val_loss: 3.9205 - val_acc: 0.0000e+00\n",
            "Epoch 67/300\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.2813 - acc: 0.9333 - val_loss: 4.0446 - val_acc: 0.0000e+00\n",
            "Epoch 68/300\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.3288 - acc: 0.9167 - val_loss: 4.1698 - val_acc: 0.0000e+00\n",
            "Epoch 69/300\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.2704 - acc: 0.9167 - val_loss: 4.3607 - val_acc: 0.0000e+00\n",
            "Epoch 70/300\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.2522 - acc: 0.9167 - val_loss: 4.4076 - val_acc: 0.0000e+00\n",
            "Epoch 71/300\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.2349 - acc: 0.9500 - val_loss: 4.4804 - val_acc: 0.0000e+00\n",
            "Epoch 72/300\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.2975 - acc: 0.9000 - val_loss: 4.4889 - val_acc: 0.0000e+00\n",
            "Epoch 73/300\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.2599 - acc: 0.9667 - val_loss: 4.5285 - val_acc: 0.0000e+00\n",
            "Epoch 74/300\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.2537 - acc: 0.9000 - val_loss: 4.5211 - val_acc: 0.0000e+00\n",
            "Epoch 75/300\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.2899 - acc: 0.9333 - val_loss: 4.6619 - val_acc: 0.0000e+00\n",
            "Epoch 76/300\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.2741 - acc: 0.8833 - val_loss: 4.6370 - val_acc: 0.0000e+00\n",
            "Epoch 77/300\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.2058 - acc: 0.9500 - val_loss: 4.5656 - val_acc: 0.0000e+00\n",
            "Epoch 78/300\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.2185 - acc: 0.9500 - val_loss: 4.6040 - val_acc: 0.0000e+00\n",
            "Epoch 79/300\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.2388 - acc: 0.9167 - val_loss: 4.5482 - val_acc: 0.0000e+00\n",
            "Epoch 80/300\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.2739 - acc: 0.8833 - val_loss: 4.4724 - val_acc: 0.0000e+00\n",
            "Epoch 81/300\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.2595 - acc: 0.9000 - val_loss: 4.3499 - val_acc: 0.0000e+00\n",
            "Epoch 82/300\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.2410 - acc: 0.9500 - val_loss: 4.4834 - val_acc: 0.0000e+00\n",
            "Epoch 83/300\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.2641 - acc: 0.9167 - val_loss: 4.4565 - val_acc: 0.0000e+00\n",
            "Epoch 84/300\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.2105 - acc: 0.9333 - val_loss: 4.4740 - val_acc: 0.0000e+00\n",
            "Epoch 85/300\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.2285 - acc: 0.9500 - val_loss: 4.5661 - val_acc: 0.0000e+00\n",
            "Epoch 86/300\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.2525 - acc: 0.9167 - val_loss: 4.6680 - val_acc: 0.0000e+00\n",
            "Epoch 87/300\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.2187 - acc: 0.9667 - val_loss: 4.7129 - val_acc: 0.0000e+00\n",
            "Epoch 88/300\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.2137 - acc: 0.9167 - val_loss: 4.6718 - val_acc: 0.0000e+00\n",
            "Epoch 89/300\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.2797 - acc: 0.9000 - val_loss: 4.5425 - val_acc: 0.0000e+00\n",
            "Epoch 90/300\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.2238 - acc: 0.9333 - val_loss: 4.4427 - val_acc: 0.0000e+00\n",
            "Epoch 91/300\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.2257 - acc: 0.9333 - val_loss: 4.5036 - val_acc: 0.0000e+00\n",
            "Epoch 92/300\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.2270 - acc: 0.9500 - val_loss: 4.4784 - val_acc: 0.0000e+00\n",
            "Epoch 93/300\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.2295 - acc: 0.9500 - val_loss: 4.4972 - val_acc: 0.0000e+00\n",
            "Epoch 94/300\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.2178 - acc: 0.9500 - val_loss: 4.6219 - val_acc: 0.0000e+00\n",
            "Epoch 95/300\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.2690 - acc: 0.8833 - val_loss: 4.7227 - val_acc: 0.0000e+00\n",
            "Epoch 96/300\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.2244 - acc: 0.9167 - val_loss: 4.7489 - val_acc: 0.0000e+00\n",
            "Epoch 97/300\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.2083 - acc: 0.9500 - val_loss: 4.7934 - val_acc: 0.0000e+00\n",
            "Epoch 98/300\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.2262 - acc: 0.9333 - val_loss: 4.7461 - val_acc: 0.0000e+00\n",
            "Epoch 99/300\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.1980 - acc: 0.9500 - val_loss: 4.6502 - val_acc: 0.0000e+00\n",
            "Epoch 100/300\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.1799 - acc: 0.9500 - val_loss: 4.6830 - val_acc: 0.0000e+00\n",
            "Epoch 101/300\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.1916 - acc: 0.9667 - val_loss: 4.7868 - val_acc: 0.0000e+00\n",
            "Epoch 102/300\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.1818 - acc: 0.9667 - val_loss: 4.9319 - val_acc: 0.0000e+00\n",
            "Epoch 103/300\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.2172 - acc: 0.9000 - val_loss: 5.1150 - val_acc: 0.0000e+00\n",
            "Epoch 104/300\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.2283 - acc: 0.9500 - val_loss: 5.1427 - val_acc: 0.0000e+00\n",
            "Epoch 105/300\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.2144 - acc: 0.9167 - val_loss: 5.0213 - val_acc: 0.0000e+00\n",
            "Epoch 106/300\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.2229 - acc: 0.9333 - val_loss: 4.7083 - val_acc: 0.0000e+00\n",
            "Epoch 107/300\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.2056 - acc: 0.9167 - val_loss: 4.5068 - val_acc: 0.0000e+00\n",
            "Epoch 108/300\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.1983 - acc: 0.9333 - val_loss: 4.4939 - val_acc: 0.0000e+00\n",
            "Epoch 109/300\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.1879 - acc: 0.9500 - val_loss: 4.4573 - val_acc: 0.0000e+00\n",
            "Epoch 110/300\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.1718 - acc: 0.9333 - val_loss: 4.4464 - val_acc: 0.0000e+00\n",
            "Epoch 111/300\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.1951 - acc: 0.9500 - val_loss: 4.4679 - val_acc: 0.0000e+00\n",
            "Epoch 112/300\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.1961 - acc: 0.9167 - val_loss: 4.4030 - val_acc: 0.0000e+00\n",
            "Epoch 113/300\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.2071 - acc: 0.9667 - val_loss: 4.4120 - val_acc: 0.0000e+00\n",
            "Epoch 114/300\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.1735 - acc: 0.9833 - val_loss: 4.3846 - val_acc: 0.0000e+00\n",
            "Epoch 115/300\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.1693 - acc: 0.9500 - val_loss: 4.3544 - val_acc: 0.0000e+00\n",
            "Epoch 116/300\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.2277 - acc: 0.9000 - val_loss: 4.2032 - val_acc: 0.0000e+00\n",
            "Epoch 117/300\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.1773 - acc: 0.9667 - val_loss: 4.3292 - val_acc: 0.0000e+00\n",
            "Epoch 118/300\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.1729 - acc: 0.9667 - val_loss: 4.4516 - val_acc: 0.0000e+00\n",
            "Epoch 119/300\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.2041 - acc: 0.9667 - val_loss: 4.4841 - val_acc: 0.0000e+00\n",
            "Epoch 120/300\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.1771 - acc: 0.9667 - val_loss: 4.5804 - val_acc: 0.0000e+00\n",
            "Epoch 121/300\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.1729 - acc: 0.9667 - val_loss: 4.6620 - val_acc: 0.0000e+00\n",
            "Epoch 122/300\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.1638 - acc: 0.9833 - val_loss: 4.6691 - val_acc: 0.0000e+00\n",
            "Epoch 123/300\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.2427 - acc: 0.9167 - val_loss: 4.5630 - val_acc: 0.0000e+00\n",
            "Epoch 124/300\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.2101 - acc: 0.9167 - val_loss: 4.1092 - val_acc: 0.0000e+00\n",
            "Epoch 125/300\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.2047 - acc: 0.9667 - val_loss: 3.7874 - val_acc: 0.0000e+00\n",
            "Epoch 126/300\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.1501 - acc: 0.9833 - val_loss: 3.6501 - val_acc: 0.0000e+00\n",
            "Epoch 127/300\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.1933 - acc: 0.9667 - val_loss: 3.8149 - val_acc: 0.0000e+00\n",
            "Epoch 128/300\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.1781 - acc: 0.9500 - val_loss: 4.0099 - val_acc: 0.0000e+00\n",
            "Epoch 129/300\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.2427 - acc: 0.9167 - val_loss: 3.8689 - val_acc: 0.0000e+00\n",
            "Epoch 130/300\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.1715 - acc: 0.9667 - val_loss: 4.0767 - val_acc: 0.0000e+00\n",
            "Epoch 131/300\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.1575 - acc: 0.9667 - val_loss: 4.1962 - val_acc: 0.0000e+00\n",
            "Epoch 132/300\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.1683 - acc: 0.9500 - val_loss: 4.2353 - val_acc: 0.0000e+00\n",
            "Epoch 133/300\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.1942 - acc: 0.9333 - val_loss: 4.0484 - val_acc: 0.0000e+00\n",
            "Epoch 134/300\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.2339 - acc: 0.9333 - val_loss: 3.8826 - val_acc: 0.0000e+00\n",
            "Epoch 135/300\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.1955 - acc: 0.9500 - val_loss: 3.7163 - val_acc: 0.0000e+00\n",
            "Epoch 136/300\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.1416 - acc: 0.9667 - val_loss: 3.7676 - val_acc: 0.0000e+00\n",
            "Epoch 137/300\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.1504 - acc: 0.9833 - val_loss: 3.9108 - val_acc: 0.0000e+00\n",
            "Epoch 138/300\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.1886 - acc: 0.9333 - val_loss: 3.9786 - val_acc: 0.0000e+00\n",
            "Epoch 139/300\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.1957 - acc: 0.9333 - val_loss: 3.9626 - val_acc: 0.0000e+00\n",
            "Epoch 140/300\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.1829 - acc: 0.9333 - val_loss: 3.8667 - val_acc: 0.0000e+00\n",
            "Epoch 141/300\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.1956 - acc: 0.9500 - val_loss: 3.7741 - val_acc: 0.0000e+00\n",
            "Epoch 142/300\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.1938 - acc: 0.9333 - val_loss: 3.7195 - val_acc: 0.0000e+00\n",
            "Epoch 143/300\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.1789 - acc: 0.9500 - val_loss: 3.8935 - val_acc: 0.0000e+00\n",
            "Epoch 144/300\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.1632 - acc: 0.9500 - val_loss: 3.8536 - val_acc: 0.0000e+00\n",
            "Epoch 145/300\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.1513 - acc: 0.9667 - val_loss: 3.9539 - val_acc: 0.0000e+00\n",
            "Epoch 146/300\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.1748 - acc: 0.9500 - val_loss: 4.0348 - val_acc: 0.0000e+00\n",
            "Epoch 147/300\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.1373 - acc: 0.9833 - val_loss: 4.0818 - val_acc: 0.0000e+00\n",
            "Epoch 148/300\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.2163 - acc: 0.9500 - val_loss: 4.1446 - val_acc: 0.0000e+00\n",
            "Epoch 149/300\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.1474 - acc: 0.9833 - val_loss: 4.0833 - val_acc: 0.0000e+00\n",
            "Epoch 150/300\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.1917 - acc: 0.9333 - val_loss: 4.0926 - val_acc: 0.0000e+00\n",
            "Epoch 151/300\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.1559 - acc: 0.9667 - val_loss: 4.0407 - val_acc: 0.0000e+00\n",
            "Epoch 152/300\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.2095 - acc: 0.8833 - val_loss: 3.9526 - val_acc: 0.0000e+00\n",
            "Epoch 153/300\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.1905 - acc: 0.9667 - val_loss: 3.8443 - val_acc: 0.0000e+00\n",
            "Epoch 154/300\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.1596 - acc: 0.9667 - val_loss: 3.7300 - val_acc: 0.0000e+00\n",
            "Epoch 155/300\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.1456 - acc: 0.9500 - val_loss: 3.6976 - val_acc: 0.0000e+00\n",
            "Epoch 156/300\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.1572 - acc: 0.9667 - val_loss: 3.7930 - val_acc: 0.0000e+00\n",
            "Epoch 157/300\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.1730 - acc: 0.9500 - val_loss: 3.7402 - val_acc: 0.0000e+00\n",
            "Epoch 158/300\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.2018 - acc: 0.9000 - val_loss: 3.6240 - val_acc: 0.0500\n",
            "Epoch 159/300\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.1327 - acc: 0.9667 - val_loss: 3.5320 - val_acc: 0.0500\n",
            "Epoch 160/300\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.1512 - acc: 0.9500 - val_loss: 3.6046 - val_acc: 0.0500\n",
            "Epoch 161/300\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.1510 - acc: 0.9833 - val_loss: 3.7531 - val_acc: 0.0500\n",
            "Epoch 162/300\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.1738 - acc: 0.9333 - val_loss: 3.8638 - val_acc: 0.0000e+00\n",
            "Epoch 163/300\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.1611 - acc: 0.9500 - val_loss: 3.8234 - val_acc: 0.0500\n",
            "Epoch 164/300\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.1590 - acc: 0.9833 - val_loss: 3.7514 - val_acc: 0.0500\n",
            "Epoch 165/300\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.1496 - acc: 0.9667 - val_loss: 3.8429 - val_acc: 0.0500\n",
            "Epoch 166/300\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.1280 - acc: 0.9833 - val_loss: 3.9901 - val_acc: 0.0000e+00\n",
            "Epoch 167/300\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.1368 - acc: 0.9833 - val_loss: 4.0593 - val_acc: 0.0000e+00\n",
            "Epoch 168/300\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.1527 - acc: 0.9667 - val_loss: 3.9876 - val_acc: 0.0500\n",
            "Epoch 169/300\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.2204 - acc: 0.8667 - val_loss: 3.8362 - val_acc: 0.0000e+00\n",
            "Epoch 170/300\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.1976 - acc: 0.9500 - val_loss: 3.7150 - val_acc: 0.0000e+00\n",
            "Epoch 171/300\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.1712 - acc: 0.9667 - val_loss: 3.7689 - val_acc: 0.0000e+00\n",
            "Epoch 172/300\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.1592 - acc: 0.9500 - val_loss: 3.8250 - val_acc: 0.0000e+00\n",
            "Epoch 173/300\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.1925 - acc: 0.9500 - val_loss: 3.7298 - val_acc: 0.0000e+00\n",
            "Epoch 174/300\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.1285 - acc: 1.0000 - val_loss: 3.8489 - val_acc: 0.0000e+00\n",
            "Epoch 175/300\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.1190 - acc: 0.9667 - val_loss: 3.9813 - val_acc: 0.0000e+00\n",
            "Epoch 176/300\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.0951 - acc: 1.0000 - val_loss: 4.0286 - val_acc: 0.0000e+00\n",
            "Epoch 177/300\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.1696 - acc: 0.9500 - val_loss: 3.9063 - val_acc: 0.0500\n",
            "Epoch 178/300\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.1357 - acc: 0.9833 - val_loss: 3.8795 - val_acc: 0.0500\n",
            "Epoch 179/300\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.1396 - acc: 0.9667 - val_loss: 3.9339 - val_acc: 0.0000e+00\n",
            "Epoch 180/300\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.1444 - acc: 0.9833 - val_loss: 3.8401 - val_acc: 0.0500\n",
            "Epoch 181/300\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.1225 - acc: 0.9667 - val_loss: 3.7016 - val_acc: 0.0500\n",
            "Epoch 182/300\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.1287 - acc: 0.9667 - val_loss: 3.5467 - val_acc: 0.0500\n",
            "Epoch 183/300\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.1212 - acc: 0.9833 - val_loss: 3.5222 - val_acc: 0.0500\n",
            "Epoch 184/300\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.1386 - acc: 0.9667 - val_loss: 3.6156 - val_acc: 0.0500\n",
            "Epoch 185/300\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.1096 - acc: 0.9833 - val_loss: 3.7513 - val_acc: 0.0500\n",
            "Epoch 186/300\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.0983 - acc: 0.9833 - val_loss: 3.7934 - val_acc: 0.0500\n",
            "Epoch 187/300\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.1169 - acc: 0.9667 - val_loss: 3.7875 - val_acc: 0.0500\n",
            "Epoch 188/300\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.2049 - acc: 0.9333 - val_loss: 3.5544 - val_acc: 0.0500\n",
            "Epoch 189/300\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.1217 - acc: 0.9833 - val_loss: 3.6496 - val_acc: 0.0500\n",
            "Epoch 190/300\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.1141 - acc: 0.9833 - val_loss: 3.7980 - val_acc: 0.0500\n",
            "Epoch 191/300\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.1871 - acc: 0.9000 - val_loss: 3.7842 - val_acc: 0.0500\n",
            "Epoch 192/300\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.1464 - acc: 0.9500 - val_loss: 3.8538 - val_acc: 0.0500\n",
            "Epoch 193/300\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.1973 - acc: 0.9333 - val_loss: 3.8752 - val_acc: 0.0500\n",
            "Epoch 194/300\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.1841 - acc: 0.9167 - val_loss: 3.6672 - val_acc: 0.0500\n",
            "Epoch 195/300\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.1280 - acc: 0.9833 - val_loss: 3.5658 - val_acc: 0.0500\n",
            "Epoch 196/300\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.1716 - acc: 0.9667 - val_loss: 3.5388 - val_acc: 0.0000e+00\n",
            "Epoch 197/300\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.1474 - acc: 0.9500 - val_loss: 3.5660 - val_acc: 0.0500\n",
            "Epoch 198/300\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.1408 - acc: 0.9667 - val_loss: 3.6527 - val_acc: 0.0500\n",
            "Epoch 199/300\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.1549 - acc: 0.9500 - val_loss: 3.7299 - val_acc: 0.0500\n",
            "Epoch 200/300\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.1444 - acc: 0.9667 - val_loss: 3.6420 - val_acc: 0.0500\n",
            "Epoch 201/300\n",
            "\n",
            "Epoch 00201: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.1263 - acc: 0.9833 - val_loss: 3.6776 - val_acc: 0.0500\n",
            "Epoch 202/300\n",
            "\n",
            "Epoch 00202: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.1277 - acc: 0.9667 - val_loss: 3.7745 - val_acc: 0.0500\n",
            "Epoch 203/300\n",
            "\n",
            "Epoch 00203: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.1191 - acc: 0.9667 - val_loss: 3.7853 - val_acc: 0.0500\n",
            "Epoch 204/300\n",
            "\n",
            "Epoch 00204: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.1465 - acc: 0.9333 - val_loss: 3.6812 - val_acc: 0.0500\n",
            "Epoch 205/300\n",
            "\n",
            "Epoch 00205: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.1250 - acc: 0.9667 - val_loss: 3.7230 - val_acc: 0.0500\n",
            "Epoch 206/300\n",
            "\n",
            "Epoch 00206: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.1966 - acc: 0.9000 - val_loss: 3.6081 - val_acc: 0.0500\n",
            "Epoch 207/300\n",
            "\n",
            "Epoch 00207: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.1294 - acc: 0.9833 - val_loss: 3.4593 - val_acc: 0.0500\n",
            "Epoch 208/300\n",
            "\n",
            "Epoch 00208: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.1126 - acc: 1.0000 - val_loss: 3.4802 - val_acc: 0.0500\n",
            "Epoch 209/300\n",
            "\n",
            "Epoch 00209: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.1230 - acc: 0.9833 - val_loss: 3.6836 - val_acc: 0.0500\n",
            "Epoch 210/300\n",
            "\n",
            "Epoch 00210: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.0989 - acc: 0.9667 - val_loss: 3.7911 - val_acc: 0.0000e+00\n",
            "Epoch 211/300\n",
            "\n",
            "Epoch 00211: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.1062 - acc: 0.9833 - val_loss: 3.8348 - val_acc: 0.0000e+00\n",
            "Epoch 212/300\n",
            "\n",
            "Epoch 00212: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.1580 - acc: 0.9667 - val_loss: 3.7617 - val_acc: 0.0500\n",
            "Epoch 213/300\n",
            "\n",
            "Epoch 00213: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.1378 - acc: 0.9833 - val_loss: 3.6542 - val_acc: 0.0500\n",
            "Epoch 214/300\n",
            "\n",
            "Epoch 00214: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.1108 - acc: 1.0000 - val_loss: 3.6839 - val_acc: 0.0500\n",
            "Epoch 215/300\n",
            "\n",
            "Epoch 00215: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.0906 - acc: 0.9833 - val_loss: 3.7872 - val_acc: 0.0500\n",
            "Epoch 216/300\n",
            "\n",
            "Epoch 00216: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.1209 - acc: 0.9833 - val_loss: 4.0164 - val_acc: 0.0500\n",
            "Epoch 217/300\n",
            "\n",
            "Epoch 00217: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.1946 - acc: 0.9500 - val_loss: 4.1795 - val_acc: 0.0500\n",
            "Epoch 218/300\n",
            "\n",
            "Epoch 00218: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.1202 - acc: 0.9833 - val_loss: 3.9791 - val_acc: 0.0500\n",
            "Epoch 219/300\n",
            "\n",
            "Epoch 00219: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.0928 - acc: 1.0000 - val_loss: 3.9699 - val_acc: 0.0500\n",
            "Epoch 220/300\n",
            "\n",
            "Epoch 00220: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.1083 - acc: 0.9667 - val_loss: 4.0777 - val_acc: 0.0500\n",
            "Epoch 221/300\n",
            "\n",
            "Epoch 00221: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.1793 - acc: 0.9167 - val_loss: 4.1786 - val_acc: 0.0500\n",
            "Epoch 222/300\n",
            "\n",
            "Epoch 00222: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.1355 - acc: 0.9833 - val_loss: 4.1965 - val_acc: 0.0500\n",
            "Epoch 223/300\n",
            "\n",
            "Epoch 00223: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.1314 - acc: 0.9667 - val_loss: 4.2278 - val_acc: 0.0000e+00\n",
            "Epoch 224/300\n",
            "\n",
            "Epoch 00224: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.1506 - acc: 0.9500 - val_loss: 3.9645 - val_acc: 0.0000e+00\n",
            "Epoch 225/300\n",
            "\n",
            "Epoch 00225: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.1318 - acc: 0.9667 - val_loss: 4.0589 - val_acc: 0.0000e+00\n",
            "Epoch 226/300\n",
            "\n",
            "Epoch 00226: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.1178 - acc: 0.9667 - val_loss: 4.1373 - val_acc: 0.0000e+00\n",
            "Epoch 227/300\n",
            "\n",
            "Epoch 00227: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.1484 - acc: 0.9500 - val_loss: 4.1572 - val_acc: 0.0000e+00\n",
            "Epoch 228/300\n",
            "\n",
            "Epoch 00228: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.1662 - acc: 0.9333 - val_loss: 3.9178 - val_acc: 0.0000e+00\n",
            "Epoch 229/300\n",
            "\n",
            "Epoch 00229: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.2351 - acc: 0.8833 - val_loss: 3.8329 - val_acc: 0.0000e+00\n",
            "Epoch 230/300\n",
            "\n",
            "Epoch 00230: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.1112 - acc: 1.0000 - val_loss: 3.8298 - val_acc: 0.0000e+00\n",
            "Epoch 231/300\n",
            "\n",
            "Epoch 00231: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.1091 - acc: 0.9833 - val_loss: 3.9059 - val_acc: 0.0000e+00\n",
            "Epoch 232/300\n",
            "\n",
            "Epoch 00232: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.1330 - acc: 0.9667 - val_loss: 3.8160 - val_acc: 0.0500\n",
            "Epoch 233/300\n",
            "\n",
            "Epoch 00233: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.1437 - acc: 0.9500 - val_loss: 3.5943 - val_acc: 0.0500\n",
            "Epoch 234/300\n",
            "\n",
            "Epoch 00234: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.1061 - acc: 1.0000 - val_loss: 3.5368 - val_acc: 0.0500\n",
            "Epoch 235/300\n",
            "\n",
            "Epoch 00235: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.1113 - acc: 0.9833 - val_loss: 3.5449 - val_acc: 0.0500\n",
            "Epoch 236/300\n",
            "\n",
            "Epoch 00236: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.1374 - acc: 0.9667 - val_loss: 3.7015 - val_acc: 0.0500\n",
            "Epoch 237/300\n",
            "\n",
            "Epoch 00237: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.1175 - acc: 0.9833 - val_loss: 3.8371 - val_acc: 0.0500\n",
            "Epoch 238/300\n",
            "\n",
            "Epoch 00238: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.1137 - acc: 0.9833 - val_loss: 4.0158 - val_acc: 0.0500\n",
            "Epoch 239/300\n",
            "\n",
            "Epoch 00239: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.1350 - acc: 0.9667 - val_loss: 3.8917 - val_acc: 0.0500\n",
            "Epoch 240/300\n",
            "\n",
            "Epoch 00240: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.1214 - acc: 0.9667 - val_loss: 3.7149 - val_acc: 0.0500\n",
            "Epoch 241/300\n",
            "\n",
            "Epoch 00241: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.1191 - acc: 1.0000 - val_loss: 3.6794 - val_acc: 0.0500\n",
            "Epoch 242/300\n",
            "\n",
            "Epoch 00242: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.1341 - acc: 0.9833 - val_loss: 3.8007 - val_acc: 0.0500\n",
            "Epoch 243/300\n",
            "\n",
            "Epoch 00243: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.1171 - acc: 0.9833 - val_loss: 4.0496 - val_acc: 0.0500\n",
            "Epoch 244/300\n",
            "\n",
            "Epoch 00244: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.0850 - acc: 1.0000 - val_loss: 4.2184 - val_acc: 0.0500\n",
            "Epoch 245/300\n",
            "\n",
            "Epoch 00245: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.1116 - acc: 0.9500 - val_loss: 4.2936 - val_acc: 0.0500\n",
            "Epoch 246/300\n",
            "\n",
            "Epoch 00246: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.1380 - acc: 0.9667 - val_loss: 4.3499 - val_acc: 0.0500\n",
            "Epoch 247/300\n",
            "\n",
            "Epoch 00247: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.0986 - acc: 0.9667 - val_loss: 4.4111 - val_acc: 0.0500\n",
            "Epoch 248/300\n",
            "\n",
            "Epoch 00248: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.0862 - acc: 0.9833 - val_loss: 4.5046 - val_acc: 0.0500\n",
            "Epoch 249/300\n",
            "\n",
            "Epoch 00249: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.1212 - acc: 0.9500 - val_loss: 4.4608 - val_acc: 0.0500\n",
            "Epoch 250/300\n",
            "\n",
            "Epoch 00250: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.0985 - acc: 0.9833 - val_loss: 4.5197 - val_acc: 0.0500\n",
            "Epoch 251/300\n",
            "\n",
            "Epoch 00251: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.1008 - acc: 0.9833 - val_loss: 4.4900 - val_acc: 0.0000e+00\n",
            "Epoch 252/300\n",
            "\n",
            "Epoch 00252: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.1130 - acc: 0.9667 - val_loss: 4.2895 - val_acc: 0.0500\n",
            "Epoch 253/300\n",
            "\n",
            "Epoch 00253: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.0764 - acc: 0.9833 - val_loss: 4.1072 - val_acc: 0.0500\n",
            "Epoch 254/300\n",
            "\n",
            "Epoch 00254: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.1132 - acc: 0.9667 - val_loss: 3.9573 - val_acc: 0.0500\n",
            "Epoch 255/300\n",
            "\n",
            "Epoch 00255: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.1100 - acc: 0.9833 - val_loss: 4.0425 - val_acc: 0.0500\n",
            "Epoch 256/300\n",
            "\n",
            "Epoch 00256: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.1017 - acc: 0.9833 - val_loss: 4.3287 - val_acc: 0.0500\n",
            "Epoch 257/300\n",
            "\n",
            "Epoch 00257: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.1171 - acc: 0.9833 - val_loss: 4.5394 - val_acc: 0.0000e+00\n",
            "Epoch 258/300\n",
            "\n",
            "Epoch 00258: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.1217 - acc: 0.9667 - val_loss: 4.2874 - val_acc: 0.0500\n",
            "Epoch 259/300\n",
            "\n",
            "Epoch 00259: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.1016 - acc: 0.9833 - val_loss: 4.2836 - val_acc: 0.0500\n",
            "Epoch 260/300\n",
            "\n",
            "Epoch 00260: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.1221 - acc: 0.9500 - val_loss: 4.1973 - val_acc: 0.0500\n",
            "Epoch 261/300\n",
            "\n",
            "Epoch 00261: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.0827 - acc: 1.0000 - val_loss: 4.1170 - val_acc: 0.0500\n",
            "Epoch 262/300\n",
            "\n",
            "Epoch 00262: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.0992 - acc: 0.9833 - val_loss: 4.1804 - val_acc: 0.0500\n",
            "Epoch 263/300\n",
            "\n",
            "Epoch 00263: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.0745 - acc: 1.0000 - val_loss: 4.2927 - val_acc: 0.0500\n",
            "Epoch 264/300\n",
            "\n",
            "Epoch 00264: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.0636 - acc: 1.0000 - val_loss: 4.5583 - val_acc: 0.0000e+00\n",
            "Epoch 265/300\n",
            "\n",
            "Epoch 00265: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.1547 - acc: 0.9333 - val_loss: 4.5406 - val_acc: 0.0000e+00\n",
            "Epoch 266/300\n",
            "\n",
            "Epoch 00266: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.1013 - acc: 0.9667 - val_loss: 4.3141 - val_acc: 0.0000e+00\n",
            "Epoch 267/300\n",
            "\n",
            "Epoch 00267: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.1251 - acc: 0.9500 - val_loss: 4.0347 - val_acc: 0.0500\n",
            "Epoch 268/300\n",
            "\n",
            "Epoch 00268: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.1034 - acc: 0.9833 - val_loss: 3.8825 - val_acc: 0.0500\n",
            "Epoch 269/300\n",
            "\n",
            "Epoch 00269: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.0826 - acc: 1.0000 - val_loss: 3.9145 - val_acc: 0.0500\n",
            "Epoch 270/300\n",
            "\n",
            "Epoch 00270: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.0996 - acc: 0.9833 - val_loss: 4.0727 - val_acc: 0.0500\n",
            "Epoch 271/300\n",
            "\n",
            "Epoch 00271: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.1418 - acc: 0.9333 - val_loss: 4.1950 - val_acc: 0.0000e+00\n",
            "Epoch 272/300\n",
            "\n",
            "Epoch 00272: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.0992 - acc: 1.0000 - val_loss: 4.0075 - val_acc: 0.0000e+00\n",
            "Epoch 273/300\n",
            "\n",
            "Epoch 00273: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.1129 - acc: 0.9833 - val_loss: 4.0810 - val_acc: 0.0000e+00\n",
            "Epoch 274/300\n",
            "\n",
            "Epoch 00274: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.0971 - acc: 0.9833 - val_loss: 4.3397 - val_acc: 0.0000e+00\n",
            "Epoch 275/300\n",
            "\n",
            "Epoch 00275: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.0814 - acc: 0.9833 - val_loss: 4.5441 - val_acc: 0.0000e+00\n",
            "Epoch 276/300\n",
            "\n",
            "Epoch 00276: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.1280 - acc: 0.9500 - val_loss: 4.5050 - val_acc: 0.0000e+00\n",
            "Epoch 277/300\n",
            "\n",
            "Epoch 00277: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.1298 - acc: 0.9667 - val_loss: 4.0469 - val_acc: 0.0500\n",
            "Epoch 278/300\n",
            "\n",
            "Epoch 00278: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.1409 - acc: 0.9333 - val_loss: 3.8971 - val_acc: 0.0500\n",
            "Epoch 279/300\n",
            "\n",
            "Epoch 00279: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.0785 - acc: 1.0000 - val_loss: 3.8516 - val_acc: 0.0500\n",
            "Epoch 280/300\n",
            "\n",
            "Epoch 00280: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.1161 - acc: 0.9833 - val_loss: 4.2158 - val_acc: 0.0500\n",
            "Epoch 281/300\n",
            "\n",
            "Epoch 00281: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.0872 - acc: 1.0000 - val_loss: 4.5749 - val_acc: 0.0000e+00\n",
            "Epoch 282/300\n",
            "\n",
            "Epoch 00282: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.1333 - acc: 0.9500 - val_loss: 4.6988 - val_acc: 0.0000e+00\n",
            "Epoch 283/300\n",
            "\n",
            "Epoch 00283: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.0773 - acc: 0.9833 - val_loss: 4.5795 - val_acc: 0.0500\n",
            "Epoch 284/300\n",
            "\n",
            "Epoch 00284: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.0865 - acc: 0.9833 - val_loss: 4.3722 - val_acc: 0.0500\n",
            "Epoch 285/300\n",
            "\n",
            "Epoch 00285: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.0899 - acc: 0.9833 - val_loss: 4.1301 - val_acc: 0.0500\n",
            "Epoch 286/300\n",
            "\n",
            "Epoch 00286: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.0931 - acc: 1.0000 - val_loss: 3.9459 - val_acc: 0.0500\n",
            "Epoch 287/300\n",
            "\n",
            "Epoch 00287: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.0718 - acc: 1.0000 - val_loss: 4.0445 - val_acc: 0.0500\n",
            "Epoch 288/300\n",
            "\n",
            "Epoch 00288: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.0758 - acc: 0.9833 - val_loss: 4.1825 - val_acc: 0.0500\n",
            "Epoch 289/300\n",
            "\n",
            "Epoch 00289: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.0990 - acc: 0.9667 - val_loss: 4.1750 - val_acc: 0.0500\n",
            "Epoch 290/300\n",
            "\n",
            "Epoch 00290: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.0791 - acc: 1.0000 - val_loss: 4.0947 - val_acc: 0.0500\n",
            "Epoch 291/300\n",
            "\n",
            "Epoch 00291: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.0881 - acc: 0.9833 - val_loss: 3.9440 - val_acc: 0.0500\n",
            "Epoch 292/300\n",
            "\n",
            "Epoch 00292: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.0903 - acc: 0.9833 - val_loss: 4.0732 - val_acc: 0.0500\n",
            "Epoch 293/300\n",
            "\n",
            "Epoch 00293: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.1294 - acc: 0.9333 - val_loss: 4.1935 - val_acc: 0.0500\n",
            "Epoch 294/300\n",
            "\n",
            "Epoch 00294: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.1427 - acc: 0.9667 - val_loss: 3.9453 - val_acc: 0.0500\n",
            "Epoch 295/300\n",
            "\n",
            "Epoch 00295: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.0800 - acc: 0.9833 - val_loss: 3.9459 - val_acc: 0.0500\n",
            "Epoch 296/300\n",
            "\n",
            "Epoch 00296: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.0911 - acc: 1.0000 - val_loss: 4.0349 - val_acc: 0.0500\n",
            "Epoch 297/300\n",
            "\n",
            "Epoch 00297: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.0717 - acc: 1.0000 - val_loss: 4.1571 - val_acc: 0.0500\n",
            "Epoch 298/300\n",
            "\n",
            "Epoch 00298: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.1087 - acc: 0.9833 - val_loss: 4.3365 - val_acc: 0.0500\n",
            "Epoch 299/300\n",
            "\n",
            "Epoch 00299: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.0731 - acc: 0.9833 - val_loss: 4.3680 - val_acc: 0.0500\n",
            "Epoch 300/300\n",
            "\n",
            "Epoch 00300: val_loss did not improve from 1.31049\n",
            "60/60 - 0s - loss: 0.0821 - acc: 1.0000 - val_loss: 4.3577 - val_acc: 0.0500\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nOzdd3xb5b348c9Xsmx575U9CTEJKwk0\nQJkJo7SFcoFSSnvLKN3l3pb2R2/puN3QfQu9vRRogbZQdmkbCgECYZMEMp3tJI4T771lSc/vj3OO\nLMlyrCRW7ETf9+vll6UznyPZz/dZ5zlijEEppVTyco11ApRSSo0tDQRKKZXkNBAopVSS00CglFJJ\nTgOBUkolOQ0ESimV5DQQqKQgItNExIhIShzbfkpEXjsS6VJqPNBAoMYdEdktIj4RKYpa/p6dmU8b\nm5QpdWzSQKDGq13Ax5w3IjIfyBi75IwP8dRolDpYGgjUePUQ8Mmw9/8OPBi+gYjkisiDItIoIntE\n5HYRcdnr3CLyMxFpEpEq4NIY+94nIrUisk9EfiAi7ngSJiKPiUidiLSLyEoROSFsXbqI/NxOT7uI\nvCYi6fa6s0TkDRFpE5G9IvIpe/nLInJT2DEimqbsWtAXRGQ7sN1e9mv7GB0iskZE3h+2vVtE/ktE\ndopIp71+sojcLSI/j7qWZ0TkP+O5bnXs0kCgxqu3gBwRmWtn0NcAf4ra5jdALjADOAcrcFxvr/s0\n8EHgFGAhcGXUvn8E/MAse5sLgZuIz7PAbKAEeBf4c9i6nwELgDOAAuDrQFBEptr7/QYoBk4G1sZ5\nPoDLgdOBCvv9KvsYBcBfgMdExGuv+wpWbeoDQA5wA9ADPAB8LCxYFgFL7P1VMjPG6I/+jKsfYDdW\nBnU78GPgYmA5kAIYYBrgBnxARdh+nwFetl+/BHw2bN2F9r4pQCnQD6SHrf8YsMJ+/SngtTjTmmcf\nNxerYNULnBRju28ATw1zjJeBm8LeR5zfPv75I6Sj1TkvsBW4bJjtNgNL7ddfBJaN9fetP2P/o+2N\najx7CFgJTCeqWQgoAjzAnrBle4CJ9usJwN6odY6p9r61IuIsc0VtH5NdO/khcBVWyT4Ylp40wAvs\njLHr5GGWxysibSJyK3Aj1nUarJK/07l+oHM9AFyHFVivA359GGlSxwhtGlLjljFmD1an8QeAJ6NW\nNwEDWJm6Ywqwz35di5Uhhq9z7MWqERQZY/LsnxxjzAmM7FrgMqwaSy5W7QRA7DT1ATNj7Ld3mOUA\n3UR2hJfF2CY0TbDdH/B14Gog3xiTB7TbaRjpXH8CLhORk4C5wNPDbKeSiAYCNd7diNUs0h2+0BgT\nAB4Ffigi2XYb/FcY7Ed4FPiyiEwSkXzgtrB9a4HngZ+LSI6IuERkpoicE0d6srGCSDNW5v2jsOMG\ngfuBX4jIBLvTdrGIpGH1IywRkatFJEVECkXkZHvXtcAVIpIhIrPsax4pDX6gEUgRkW9j1Qgc9wLf\nF5HZYjlRRArtNNZg9S88BDxhjOmN45rVMU4DgRrXjDE7jTGrh1n9JazSdBXwGlan5/32ut8DzwHr\nsDp0o2sUnwRSgUqs9vXHgfI4kvQgVjPTPnvft6LW3wpswMpsW4A7AJcxphqrZvNVe/la4CR7n19i\n9XfUYzXd/JkDew74F7DNTksfkU1Hv8AKhM8DHcB9QHrY+geA+VjBQCnEGH0wjVLJRETOxqo5TTWa\nASi0RqBUUhERD3ALcK8GAeXQQKBUkhCRuUAbVhPYr8Y4OWoc0aYhpZRKclojUEqpJHfU3VBWVFRk\npk2bNtbJUEqpo8qaNWuajDHFsdYddYFg2rRprF493GhCpZRSsYjInuHWadOQUkolOQ0ESimV5DQQ\nKKVUkjvq+ghiGRgYoKamhr6+vrFOyhHj9XqZNGkSHo9nrJOilDrKHROBoKamhuzsbKZNm0bYtMLH\nLGMMzc3N1NTUMH369LFOjlLqKJewpiERuV9EGkRk4zDrRUT+R0R2iMh6ETn1UM/V19dHYWFhUgQB\nABGhsLAwqWpASqnESWQfwR+xniw1nEuwHvc3G7gZ+N/DOVmyBAFHsl2vUipxEhYIjDErsabbHc5l\nwIPG8haQJyLxTAOsVNLp7vfz2Oq99A0EeHTVXsZyapg1e1pZs2fov/bWuk5WbmuMWLa3pYfllfVx\nHbfLvsZg0Lq2fn+AR96pxh8IDtnWGMOjq/bS1e/HGMOT79bQ1uM7hKsZtGp3Cxtq2nmrqplfPL+V\nd3ZFXuPm2g5e294EQFVjFy9vbYh5nIaOPv65vpaWbh9/W7svYl177wC/eXE7975aRSBo8PmDPPxO\nNQNR1xgIGu59tYrfvLid9p4BgkHDD/9ZycZ97Yd1jcMZyz6CiUTOoV5jL6uN3lBEbsaqNTBlypTo\n1WOuubmZCy64AIC6ujrcbjfFxdYNfO+88w6pqakjHuP666/ntttuY86cOQlNqzo6LdtQy9ceX8/W\nuk7ufW0Xc8qyOWly3hFPhzGGrzy6lkDQ8OrXz4uomX7rbxvZuK+dd7+1FK/HDcBPnt3Cc5vqqPze\nxaSmHLjc+eLmer72+HrKc9M5a3YRr25r4rYnN5DlTeGDJ06I2Hbjvg6+/sR6mrt9nDQ5l688uo7r\n3jeFH1w+/5Cv7arfvQnApPx0alp7eXVHE099/szQ+tuf3si2uk7WfGspH7/3bWrb+9j43xeRlRaZ\njX7z6Y0sr6zH63HRNxBk8cxCSrK9ADz8TjU/X74NgJnFWfT7A3zjyQ1kpqXw4ZMGr3Hl9kZ+8M/N\nALjdwhkzi/j9q7uYW57DvIm5h3yNwzkqho8aY+4xxiw0xix0MtjxpLCwkLVr17J27Vo++9nP8p//\n+Z+h904QMMYQDA4t2Tj+8Ic/aBBQw6ptt/qD/rWpLuL9kbatvos9zT3UtPaypa4ztLyl28fq3S30\n+AK8WdUMWCX6l7c24A8aqlt6Rjx2Z58fgOWVzjX22u+H1ih2NFrnfmFzPa/apfQYFYdDUtNqnbel\ne7CG0djZz7vVrXT2+3mrqpkuO63RNSAgVFvrG7ASVN/eH1q3vLKe40qzyEx183xlPTsauqzriLrG\n5ZX1ZKS6mVOazfLKepZX1uF2CecfXzI6FxllLAPBPiKfKTuJwefNHhN27NhBRUUFH//4xznhhBOo\nra3l5ptvZuHChZxwwgl873vfC2171llnsXbtWvx+P3l5edx2222cdNJJLF68mIaG2FXQZBUMGuo7\nDpwR1kVllE6mEo/Gzv4hVfVY2nsH6PUF6OgboMfnj/v4fQOBUDOGzx+kuasfnz/I+po2alpjZ5jO\n9TqZVENnX+gaw6/Neb2zsYtN+9tDzSx7W3rYuK+dQDCySampK/a1tnb76PUFhix3MmkRK7Pq7vfT\n3jvAS1saCBpwCTyxpoaGzj7e3NlMt32MqsYuBgJBmrr6hxzT4XyGyyvrMcZQ32Ftu2JLAwOBINvr\nO9lc24ExhqpG68ml71a38vR7+0Kfq8P5Dve39bK+po1+f4Dmrn76/QEaOvpYu7eNtXvb6PH5ae8Z\noDUs0xeBS08sj1j20pZ6jH19yyvrmVGcCcBT7+1j7d62iL+3jNTIGsL+9l427mvnjR1NvFvdyqXz\nJ3DOnGJe2DwYCFZsbWBrXaddYDS8UFnPOccVc+mJ5azd28aT7+7jtGkF5GWM3LpwKMayaegZ4Isi\n8ghwOtBuP0v2sPz33zdRub/jsBMXrmJCDt/5UDzPNR9qy5YtPPjggyxcuBCAn/zkJxQUFOD3+znv\nvPO48sorqaioiNinvb2dc845h5/85Cd85Stf4f777+e2226Ldfik9NBbe/jRss28819LyM0Yeh/F\nX1dV8/+e2MA/vnQW8ybm8m51K1f89g3+cP0izptz4BJVe88A5/50BbdeNIfrzzzw0NxP3Pc2x5dl\ns7u5hwm5Xn51zSlxpf+Xy7fx3KY6Xv7aefzvyzv5wxu7+OiiyfzfK1V4PS7e/sbQ64oOfH9ft59v\n/20Tt11yPD95dgt/+fTpZKd5+PDdr/G1i+Zw57+2AvDzq07igrklvP/OFQDc8W/z+egiq3nVGMOF\nv1zJtadN4daLBmujwaDhsrtfZ9G0An5+9UkR5319RzPzJ+aS4haWV9azYV87+9t6mZSfTnmul1On\n5vOP9bWsq2lj6dwyUlNc+PxBdjZ2U9O6h18s38Yb3zifHO/Q763HDhr72/vYtL8jdM0dfX5+u2In\nv3zBalK595ML2dnYRUaqmx5fIFQ7cjLjth4f5/x0BV9Zehy/fmE7nf1+bjxrOn9bu48PnjiBZzfW\nhoLMR06ZSOX+DrK9g1nhoqkFHFeSzT/X1zIQCOJxu1i5rYnyXC8nTMjhtR1NoW2t0no9+Rke1ty+\nFJdLaI3qq/i/V3bybnVb6P1F80qZUpjOsg11vLC5gcxUN519fi761Uqe/PwZuERo6OxnaUUp8ybm\n8ovl26ht7+MzZ88Y8pmNloQFAhF5GDgXKBKRGuA7gAfAGPM7YBnWM1x3AD3A9YlKy1iaOXNmKAgA\nPPzww9x33334/X72799PZWXlkECQnp7OJZdcAsCCBQt49dVXj2iax7u/r9tPvz/IjsZOFkwtGLL+\n/1ZWAbC/rZd5E3NZtt4qX/xzfe2IgWDF1ga6fQF2NnYdcLuBQJDK/R30DwSpbuk5qBrBupo2djf3\n4PMH2bCvnbaeAe5/bRdFWWk0dfXz8rYGLjt5YsQ+TsblWL2nFYCfP29l+Ms21JLt9WCMFWhEID8j\nlWc31jKnLDu0X11YM0VXv5+Wbh//WL+fr154XKi9f8O+dqpbemjt9uHzz49o22/u7md6USYnTc7j\nzn9tZXNtB/6gYVt9J9csmsJXLzyOHK+Hh9+p5t3qVo4vy6auvY+qxi4y01Lo6vfzytZGPnRSZJs/\nQK8vgNslGGNYXllPXUcfx5VmUd3Sw10rtpOa4iItxcWyjbVUNXazeEYhXzx/Fl39fh56cw877O/s\npS0N9PgC/HNDLZ391vfy4Ju7GQgY/vz2HgYChv9YMptN+zv4x/r9DAQMTlfHz686iXPmFPPsButv\npq1ngOLsNLY3dHLChFxmFmeycnsT3hQXV5wykQ+dPIFXtzVx/+u7qOvoY0JeOi3dPs6aVcS3PljB\nJb9eyXt720j3uPntx08lL8PD8WU5lOV4cbuErn4/n3jfVE6bXsCXHn6P3U3d7GzsCjUD5WWk8tTn\nz6Cr38/p0wvj/hs7WAkLBMaYj42w3gBfGO3zHmrJPVEyMzNDr7dv386vf/1r3nnnHfLy8rjuuuti\n3gsQ3rnsdrvx++PPZI51TV39rKm2MsGdDd1DAoE/EAw1G3T2WSNKlm+22l9f2tJAIGhwu4Yfeuu0\nR0dnvNGqW3rwBw1b6zvj2j7cTjt9jV39VNmZ10DA8LlzZ/K/L+/k+cr6GIEg8u/EGTQ0ELBevFDZ\nQGaaO7Ts1Cl5nDgpj4ffqebyUwaP1dE3EHrd2m293t3cw46GLmaXWgHD+Qyc9vCzjxvsl+vs85Pj\n9XBhRSl3/msrfrupaSBgWFpRSl5GKufNKebhd6pZu7eNy0+eQEaqm52NXUzKzwgdP1Yg6Pb5yfGm\nMKski+WV9QSChmmFmUwpyOSFzfWcf3wxuekeXqisp8cX4OzjijllSj5gfbev2yV1J/3v2aXwqxZM\n4rE1NaF0etzCjWdN551dLaFtnc9z/qRcirLSQk0wrT0+CjJT2d3Uw3lzSijJ8eLzB/H5g0wuyOC8\nOSV4U9zc//oudjZ2MSEvndZuH8eX5TCnLJuirDQaOq3geV5Y+35eRiqnTSvgzapmZpVkccFca11d\nRx/LK+tZNC0/lAbnGhPpmLiz+GjR0dFBdnY2OTk51NbW8txzz3HxxQe61eLIau8d4JP3vc3Prjop\nlCnE65ZH3uP9s4u5csGkg9rvrpe2c+9ruzj/+BJ+cfXJI27/8tbG0D/tzqYu3q1u5XN/WhPKEMPb\nwFt7fOxs7GZPcw+LZxTyZlUza/a0cnx5Nh+5+3W6+v3cde2pLJpmBROncxOsIYDD+fLD79HcHZnx\nN3X1c+tj68jP8JCW4uZPb+/hA/PLmZDrZV9bLz++4kT+9NYelm2opbHT2rempYc9YZ2oF1aUsr2+\nk3+sr8UfCLJqdys/+dcWHrzhNJq6+inOTqOxc/C3w7m28NdLK8o4aVIuf3xjN0+9O9j11tE7GAha\nwpownq+s587ntnL+8SUsr6znpMl5bKvr5LN/WsOXzp9NU1c/hVmpdPQOkJPuYWZxFtOLMmnvHaAk\nO419rb28b4ZVYp1ZkhU67sziLDLSUli2oZZMe3TNiq0NPL+pjm8+vZHCzFSe/PwZZKSm0OMLkJGa\nwtKKUn60bAupbhenTS9g/sRcXthcz5K5peRleHjK7hOYUTRYyCrN8dLtC3DOT1ewN6pj+uazZ/DY\nmhpOm17A6t0tLJ5ZRLbXw5mzikj3uOkN61sotUf3FGRamfD3/1FJqtuFLxBkZnFW6BrCt5lZYqXj\ntyt2cs/KKlp6fBRkekLpaujsj/hMHEsrSnmzqpkZxZlkpKaQ7U1hze5WttV38a0PVgzZPpE0EBxB\np556KhUVFRx//PFMnTqVM888c+SdjqAdDV2sq2nn3erWgwoEwaDhH+trCQTNQQWCYNDwwJt7aOsZ\n4JWtQ0dfxFLV2EWKS5hamMHOhm7WZrdR39HPNYsm43FbTRiZaSn87pWdtPb42NVklb6/dP4sVu9p\nYXllHSluCZXKH1u9NxQInM7N0pw06oYJBDWtPTyzbv+Q5cbA43ap85QpebT1DPDY6r1kpaXQ3jvA\n1y46ntufjrzJftXuFgJBwxfOm8nEvAwmF2RwXGk2Xf176ezz85d3qlm3t43H19QQNHDjWdMRYG9r\nD396q5oPnljO3PIcrj1tCr9buZOAXav46+q9XHvaFNI8LkQItWnPKMqMrBHYgSDd4+bBN3dT39HP\nur1tNHT2c/ulcyk6Yxo/fW4rz1fWUd3cw7SiTLp9AXK8HkSEH14+jx5fgJx0Dy3dvlAT0pSCDFJc\ngj9omFGchdsttPUMsK+tl3SP1R7+rb9tpKXbR2NnPyu3NXLxvHJ6fQEyUt0srSjjR8u24AsEKcv1\n8qGTJtDY1c/lp0zA7RI+f+5MfP4gF51QFrqWshwrA9/T3MNlJ0+gIDOVP7y+m6KsNGaXZnPnv53I\nSZPz2LS/nePsv22vx83Prz6JvS09/PjZLXg9LnLSrSwx3y6NOyOSwMrww2/fyLcDQXFWGtlpKaFg\nDIRK86U5aWzYBzOLB4OW4+pFk+kdCIQCaGmON/RdLZqW+FpAOA0Eo+y73/1u6PWsWbNYu3Zt6L2I\n8NBDD8Xc77XXXgu9bmsb7Fi65ppruOaaa0Y/oTE4oyRaugdG2DJSU3c/gThG8kRbV9NGY2c/E/PS\nqevowxgz4h3T9R39lGSnMbskm20Nncwtz8Yl8KOPzMcV1uTz+JoaWroHQhn6zJIsFs8sYnllPafa\nVe1ZJVm8uHmwucgZsnfZyRNDN/xENyOFD/OLLk06OnoHQqX21h7rs3xxc/2QkvwbO62MY2lFGSfb\n9wQ4Y9Lbegd4eYtVO/nTW9bzRGYVZ7GkopS7V+wAYN7EXD57zkwAvnHJ3NBxP3/urNDrCbnp7Gvr\npSgrlYLM1NAQTRj8vi89sTwUxBrs9F1YUcaUwgxW72nh8TU19A0EQ9+Nk1meMatoyLUDeNwuphRk\nUNXUzYziTPr91me0u6mbS+aV8+KWeuo7+vnYaVNYtqGW5yvruXheOd12IJhelMmskix2NHRRkp1G\neqqbL5w3eE1fv/j4IecsyUkLvf7ZVSfx5s5m/vD67tDonqsXWQMUw/tLAD4w3xod9ONnt1Ca4w1d\no1PaDzejKIuu/sHPr8DO7EWEGcWZrKsZvNnL2b/UDlAziofWCLLSUiKuqzQnLTSKaHrR0MCRSEfF\nfQRqZHuau/nD67sOaV+fP8gvlm9jrz100SkpGmP46XNbuO2J9VQ3D1a3/2i3hwI89OZuXt5ileaj\n28kfX1PDrY+t49bH1rHCztRWbGnguU11vLSlnu/+vRK3S7ji1IkEgoaV25t4ws6QHJtrO7j/tV3s\naOji3leraOjsozTXy8ySTKqbe2js7CcvIzUiCADkZ3ho7fbR0NGHS6AoK42lFaXsbu7h9Z1Wqeu6\n06fQ3O3j3epWa8je5nrOnl3M5IIMggaa7aGOD725mw017fzx9V3c//puynOtf+75k3LJTfeQbt88\n5ejo83PuccUUZqbicQtFWWksr6wnL91qLnC7BI9bQoFgRlhpMcsevfLi5no6+/1MzEsP1WrK7PM6\nmcvMGJlLNKdJojTHS066h46+Ae5ZuZNdTd2hcfIftTPJCfbx55RmM6XQas+fUZQVGg/vDP3MjjHi\nJ9qM4ixErAzNSW/QwIQ8L++fbfU5XDKvjPOPL+GlLQ34A0F6ff7Q0MulFaUR1zwS5xxgBSLnuuP5\njPIzrSAZfoy8qFFbBZmp5GemRgSc/MzBbaLPk58RGQhi1QiGu4bSnLS4PuPRpDWCY8QT7+7jf17c\nzpULJh30H9Hbu5r5nxe3M9++Y9EpKda09nL3ip0AlOemc8uS2XT1+/nu3yu5/sxp3Hz2DL71t00U\n2qWf8FJ9vz/Ad/62EZdL6PcHqWnt4bzjS/jZ81up7+gjKy2Fhs5+PnbaZKYVWv8kP39+Kzsaurji\n1Imhktn/vLidZzfWsWhaPqt2t1KUlcrCqQVMLcjEHzRs2t9BfowhpPmZqbT0+Kjv6KM4Ow23S3i/\nXYL918Y6PG7hI6dO4ofLNrO8sp5Ut4v6DmvInjOUsK6jDwN862+bOHVKHmv3tpGb7uHWi+ZQub+D\nueU5VE/uweOW0OcE0Nk3QH5mKje9fwZNXf20dPt4u6oZXyBIiku4csEkXt3exL62XqYUZEQMpXTa\noFfttqY3+MHl8/ju3zeR4/WEAsbp0ws4bXoBC6aO3HwwoyiTldsarUDgTWHV7h5+tGwL2+q7KM2x\nPpcFU/K5dH45F55QyitbG0NNFUDMtu0c78jZxodOKicvw4PX46Y0LPPMy0jlk8cV02c3iXT1+3nq\nvX2s2t1Kjy9AWY71WVy1YBKrd7dwwoT47qKdnJ/BWbOK+OL5Vgm7PMfL0opSLjqhNK79r144OSKd\nXo87NDz1worSUAk9LcVtFTJ6BkKZPcDF88ro9vl5bXsT3b5AqEZw1uwiVu9pZVaMzzFaqPZQNPK2\no00DwTHCybw7+/wHHQh22tXRbfYIGKdGED4euqrJ2sZp/qlq7A41kzR3D94c1d47QF5GKm/Y7e1/\n+NQi/vJONXtbeggGrRuBegcCNHX5+P5lJ/CJxdNCtYWtdZ30+4PUdfRRnptO30CAV+w7N1fttkYK\nNXX5KM1Jo9QuKW6r7+TESUMzi4KMVKqauqjzuEPtx5MLMkhNcdHU5WNiXjq56R7eN6MwFAicIXtO\nzai+o5/1dnXfGQf+wA2nceKkyKkdgkHD716pCnVU9w0EyfGm8LlzrWabO/+1hYbOfgzwuXNmcutF\nc7j4VysBWDI3MqPKskf+1LZbNZlz5xTzyvHnRWwzuSCDRz+zeMg1xxJeI/C4JdQ09NKWBpbOLSU/\nw4PLJdz9cWvy3+jRSrFKsjnpI/99XXbyxNCxwkvaBZmpvH92cahWcPZxxaS6XSy3RwKlp1rXP6M4\ni8c+e0Zc1wiQmuLiTzedHnrvcgm//+TCA+wR6bZLhjY35Wek0uPr5UdXzKcoazBIlOZ4hwSCC08o\n48ITyvjQb15jw772UOHk1Cn5PHjDaXGlwfk7dTqfjyRtGjpGOCNAwjsDowWDhvU1bRhjeK+6NXQr\nfJXd9NDvt5oAWrp9bNzXHmrPzkpLCQ3JdALBzsYuno9x6399Rz87Gjr581t7yEh1s3hmoV2C8lHX\n0RfRpr7Erv471XDn/O/uaWNHQydv7mymxxcYMkdNSY43VHrr9wdj3m2Zn5lKS/cADR19lNj/YG6X\nMN2ufThV/AsrStnV1M2f397Domn55Ic1EazY2sCT79aEzl+W4w3VmsK5XEJJdlrEsvBgXJrjxR80\nBIImdK3O9AxOE4jDqRHsb+sjJ91z2LPMzrRLsqU5aRE1j5ZuHyu2NkRkZrFMyE3H64n8/GPdDHYg\nWWkpZNgZfPT5stJSOGNWIcs319Hj85OZOn7KpgWZqeSme0I1Xkdpjpd0jzsUtMI5tbb8GH0MI3H+\npseiRqCB4BjhTFnQ0Tv8PQevbGvkw3e9zq9f3M5HfvsGT9jDCqNvntpS18kHf/MaD9mdlAun5VPV\n2GXf9m8Fgn1tvbxV1TykmaCuo4/r/7iKFzY3sLSiFK/HTX5mKq3dA6HzlGSnsWhaPuW56cDQjrlb\nH1vH5Xe/wd/X7ScrLYXPnjOT3HRPRIZcFl7KjJGZFWRawae2vS+iyu+Utpxhgksrykh1u2jtGeDS\n+dbkt0VZaeR4U/jL29W8W93G9WdOoyzHywdPLB82Yz4uapSV06EKRJzfudar7NFV0aNDnIywqav/\noDPcWOaUZeP1uJhTmh2RJrA6hkfKsFwu4cRJeRHXEH2ckYhI6PuK1Ql7/vEl7G3ppb6jP2bmOlZm\nFGdyypS8Id/58WXZEf064U6ZnEd+hifUH3QwZpVYAx9OnnLkJxMcP+FXHRZnpE/4OPFoe5qtUv0a\n+67Uf6zfz5ULJrGzoTtiO+dW/3V7reaQhVPzeXlrI/Ud/aEOYWOsm3NuPnsqd6/YSV6Gh7aeAXY3\ndbO3pZcbzpzO1y+2pi0oyEjFFwiGmlme+NwZFGYNZgjRmZFTa3hq7T4+MK+cWy6YzY1nTuej97zJ\nlrpOSnO8ocDg8wdjZmb5GakEgob23oGIoOGUtpxOyLJcLyu/fh5d/f7QuHS3S1hx67k0dflw2R2e\nXzhv1pBO4XD/94kFvLSlgc//+V0gstQc3jTipPVHV8zn2x+qIMUdWRYLn+ogO462+JEUZqXx9n8t\nIcebwiOrBif7fd+MAt6qaonZvxLtj9cvYkNNOx+95y0gvqahaCU5aVQ1dcc835SCjNDrjHEUCO74\ntxNjLv/qhXO4ZcnsmOs+sY9eV8oAACAASURBVHga/7Zg0pDvNR6zSrJ471sXxpw2JdG0RjAKmpub\nOfnkkzn55JMpKytj4sSJofc+X/xzpN9///3U1dUdUhpCfQT9wweCOjsTd5ol1u1to7mrf9gx8609\nA1YJZbJVat3Z2BUxuVZhZipXLbBGnDhNJm/aI2EWTcsPTUXsZH5r9rSSnZbCpPz0iIm5stNSSIlx\nt68xVtOJ2yXkZnhCIzNKc9IQkVApNWZncVgtoSQsI3ZqBOGjP8pyvcwqyYoYeVSYlcacsmxml2aT\n4naR4/WE7lOIxetxU5wdXmqOHQic2ovH7YrZlxN+w9Jo1AgAcu0mJud4+RkeLrWndY5VQo+WkZoS\nqr2JQNYhNN84n0GsoB3++YynQOD1uEN/w+FSU1xDJpZzuF1yWCN+xiIIgAaCURHPNNTxONRAYIwZ\n7COwm4Ze3trA6T96IWIqXeduWaftv7VngAU/eAGw5mAHa3bFcPkZqcwutTLgqsYuGjr7mJiXjkus\nKv3kggy8HhfTCjPJz/CEbqoJHzftZH5r9rQyoyRrSFVbREIZxJSCDNJSXFxwfAkpLomYG2hmiTUk\n0ekoLjtA5hJe4ygPG4I4q9hqwplgZ2yjKTzjDn9dnJ0WmstmpIzX43aFmsAOtglmxPTZxyvN8bLU\n7qQO7wQ9EGeoZFZaypChuvGYkJdOiktiNpmURQQCbaQYC/qpJ9gDDzzA3Xffjc/n44wzzuCuu+4i\nGAxy/fXXs3btWowx3HzzzZSWlrJ27Vo++tGPkp6eHvcDbcBqSvHZHa1O09BbVS3Ud/Tz4uZ6rlpo\nldrrOwdL87NLsvj46VPoHQiS7nHR1e/nZ89vY0pBBrvD7hnIz0ylJDuNzFQ3Oxu7qe/oZ2phBj/4\nyDxOKM/B7RLu/9QiphZmUtvexwub63EJTC3MCDuG9c/f3jsQ6ryMVpCRSmNnPz++Yj4iMLUwk50N\nXRElpE+dMY2TJ+eGMlmnpB+rj2DxzEL++8MnIELEZF3zJuZw17WnDBmtMxrCM+7w1x63i8JMa0K5\neDoRs9JSaPH7Rq1GEEqTd3Dag7JcL3+8fhEVE3Li2jfLrrUdapquP3Mai2cUxmwyCR+zP55qBMnk\n2AsEz94GdRtG95hl8+GSnxz0bhs3buSpp57ijTfeICUlhZtvvplHHnmEmTNn0tTUxIYNVjrb2trI\ny8vjN7/5DXfddRcnnzzynDvhwkv9zqghp2P2+crBQBDerDMpP51PhU2z7DxSb0ZxVmQgyLCaFWaW\nZIWahk6bXhBRUj9jpjU+/8KKUl7YXM+k/IyIKnV4M02scekwmBksmDrYpDQxL7LUXpCZyvnHD2bg\ngzWCoZlTWoqbfz9j2pDlIjLkaVejZbgaAVjNWe29PjLjyOiy0lJo6fYdUlv8AdNnH8/53M4dYSbW\ncE6t7VDTVJLtDT2lK9axHeOpsziZHHuBYBx54YUXWLVqVWga6t7eXiZPnsxFF13E1q1b+fKXv8yl\nl17KhRdeeNDH7uwboKmzn2t//xbdYbe9v7O7lduf3sB2+56AV7Y2cu3v3yI33RORwYe3y8Jg5jCj\nKJOXsDoqO/v8oUx8RlEmb+9qobnLN2Rfx/lzSxAZOvY8vDlkxnA1gszU0A1I8RrsI0jMwzoOVkaq\nOzQlRXTJtizHS2Nnf1zDQZ1+gtHoLA7njPAKHwF0MAoyUuO6mexQOB3/42n4aDI59j71Qyi5J4ox\nhhtuuIHvf//7Q9atX7+eZ599lrvvvpsnnniCe+6556CO3djZj89+4tO2+sHhn+v2toVG+1xwfAk9\nvgDd/f7QdAaO6Mx8/qRcPnLKRP79jGm09Pgoyfbyu1d2Ds6wWJzF02utydbmlMUu1RdlpXHLBbOZ\nWx7Z3JDj9eASa4qB4WoEV5w6ifkxbgw7kCVzS9nZ0M3ksFEnY0lEQpl3dIZ/1cLJLJo+9NkJsTg3\nlY1201B+RiqfeN9ULppXNvLGMXxi8dQDjpw6HDneFJq6fNo0NEaOvUAwjixZsoQrr7ySW265haKi\nIpqbm+nu7iY9PR2v18tVV13F7NmzuemmmwDIzs6ms7NzhKOCPxikuz9AZloKf7rpdE774YsAoSGc\njkvml3PlgkkEgoaZ/7UMsMZGVzV2DwkEGakp/PKjVpPUL64+OTRPu9Om7XT+uqM6cKP9x5Ljhixz\nuYT8jFRae3wRfQfhllaUDrm5aiQzirO448rYQ/zGynCZ98UHkfk6NYLRbhpyuYTvXz7vkPe/7n1T\nRzE1kbK9Hpq6fNo0NEY0ECTQ/Pnz+c53vsOSJUsIBoN4PB5+97vf4Xa7ufHGG0Pz8txxxx0AXH/9\n9dx0000RncX9AwECQUNGWgr9/gAdvX76/QEMhnSPO6LddWphJm09gzOXOk00bpcwpSCD6pYe5pbn\n2IHgwM0D0UMznWGXh/rc1LwMD1neFNJSju1/9NEY6RMKBAlqhhmPnJqUGWE7lRjJ85d2hIRPQw1w\n7bXXcu211w7Z7r333huy7Oqrr+bqq6+OWLajsYtA0DBvYi61bX2hzuC0FBcBewTGfyyZza9e2M6E\nXC/r9lojglp7fBHPFPjmpXP5zENruHLBJFZuaxwyHW+0qYWZFGWlUVFuNddMK8ykLMfLVQsP7sEz\njhMn5Q2ZquBYNG9CbsSc9YciO0E1gvHs0++fwZcefo/J+eOjmS/ZiDncv9ojbOHChWb16tURyzZv\n3szcuXOH2ePotr7GKuFPK8ykuqWH/AwPZbleXCJs2bKFuXPnhuYM+uT97/Dq9ibuvPJErlowaUg7\ndTzz/R/I4e6v4vP9f1Ry32u7WPbl98c9vFOpkYjIGmNMzJn4tEYwDgRjBOMBf5AUe0bMQNBQ295L\n0Bhy0z24XZElaydzdm4Um1mcGTPDPtxMXIPAkZGoUUNKDeeY+Us7mkur1c09uEQwGIwBlwhtvT4y\nU1MIWveJ0e8P4nYJGXYmEasmVzEhhy11nUwtPPLT2KrRU5xlPdDmUGawVOpQHBOBwOv10tzcTGFh\n4VEZDPrsSdYM1vw6zs2XPT4/Bmu2Tq/HTVqKywoYxtDc3IzXGzny54eXz+eGM6fHPW2AGp+uXDCZ\nU6bkhx5bqVSiHRN/aZMmTaKmpobGxvgegD7e7GvrBTM4YkKw5vwJ2Av8malDhtV5vV4mTYrsuE1P\ndTMvxnz56uii36M60o6JQODxeJg+ffrIG46RLz38Hhtq2vj8ubNCD9EG+PGzm/GmuPn1i7VD9rlm\n0eTQtMFPff4M5k4Z+bGESil1KI798XxjbFdTN39ft589LT3cH/Zw+fbeAe57dRe/f7Uq5n7hN1cN\nN6WDUkqNBg0ECba80ppW+vozprOlrpNqe76fl7c24A+a0ENgwGoOcublP31GIdlpKYgQMc+9UkqN\nNg0Ew/D5g9z7ahX9/kDE8mDQ8IfXd9HeM4Axhj+9tSc0bBPg0dV72dtiZfZ/XVXNn9+uZm55Dp+y\nZ8J8vrKOf6zfz+9eiawJpKa4mFyQwdTCDEpz0shKS2FGcSaFmWkHfCCKUkodrmOijyARVm5r5Af/\n3MyM4syIqY8razv4779X4hLh3DnF3P70Rnp8fm4+eybd/X6+/vh6rlwwiR9+ZB7feHIDqSkubr+0\ngimFGcwpzeafG2rZWtdJIGg4bXoB7+xqAeDjp0/B63HjcQld/VbwuXheOdUtPTHTp5RSo0UDwTCc\n+fydZ/RGL69q7GJyQXrENs6D3V/cXM+NZ00naKznnl528kTAave/a8UOwHoO7HGl2Zzxk5cozEzl\nOx86YUgaPnfuzARcmVJKRdI2h2E4GX74w1ys5d2h385D351n/jq/W3sGeHS1NeJnZtgjG50O4Ky0\nFBbPLKQsx0tGqjvimbpKKXWkaY1gGFV2ht/Q2Re1fGiNwHkWcENY7eGRd6xAMD3sQSzzJ+YyuSCd\nBVPyQ7NwzpuQG/EgdaWUOtISGghE5GLg14AbuNcY85Oo9VOAB4A8e5vbjDHLEpmmeFU1WYFgaNOQ\ntXx/ex/ra9qBoTWChVPzWb2nlfJcb2jeGLDmg3/q82dGPNzj959ciEvrZUqpMZSwLEhE3MDdwCVA\nBfAxEamI2ux24FFjzCnANcBvE5Weg9Ha7Qs9Bzi8aSgYNOxq6go9S3fT/g7AChbGGOo7+shKS+Hy\nU6w+gRnFQ+f8KcpKiwgOuRkeskf5SVRKKXUwElkWPQ3YYYypMsb4gEeAy6K2MYAzz24usD+B6RnR\npv3tnH3nCl7e1gBYz5kNbxra395L30Aw4mavshwvPn+QxT9+iQff3ENJTlpofXj/gFJKjVeJbBqa\nCOwNe18DnB61zXeB50XkS0AmsCTWgUTkZuBmgClTpox6Qh1PvruP6pYefvCPzbhdwsXzyvjjG7sZ\nCATxuF2hfoOLTihjUn46Hb0DFGal8Z1nNoWahcpyvJTmeLn72lOZr/PFKKWOAmPdWfwx4I/GmJ+L\nyGLgIRGZZ4wJhm9kjLkHuAesB9MkIiHGGJ637wJu7vaxeEZh6CleDZ39TMxLD40kmlWSxeKZhQCs\n3t0ScRyn2efSE8sTkUyllBp1iQwE+4DJYe8n2cvC3QhcDGCMeVNEvEAR0JDAdAHWMwB++vxWBvxB\n8jI8fPz0qext6WVqYQZ7mntYWlFKmT2sc09zN/e+WkVdex853hSKsgbniY+eB2hfa2+ik66UUqMq\nkYFgFTBbRKZjBYBrgOiH91YDFwB/FJG5gBc4InNJL9tYy9/X7WdyQTp7W3pDTwn78RXz+b9XqvjQ\nSRNCHcZPv7ePR1fXAHDy5LyIZx6U53r5wPwybjhzOg++uYfr3jf1SCRfKaVGTcICgTHGLyJfBJ7D\nGhp6vzFmk4h8D1htjHkG+CrwexH5T6yO40+ZI/QQ5Z0NXRRnp/HQDadz7s9e5sXNDaSluDh9eiFn\nzCwCrEcFugRe3DxYQYnuAE5xu/jtxxcAsHBawZFIulJKjaqE9hHY9wQsi1r27bDXlcCZiUzDcKqa\nuplRlMmk/HQ8bqG528fxZdm4XYOlfa/HzeQCq6nIEWtIqFJKHc2S8lYmYww7GrqYWZJFitvFNPsZ\nv7GGe86w7wwuyU4jNcXFqfqAGKXUMSYpA0FLt4/23oFQJu+U8mOV9p3gcOasItZ/58LQaCGllDpW\nJF0gMMaEpo+YWWJl8k5mH6tGMLhNJl6Pe8h6pZQ62iVVILj31Sqmf2MZG+w5gpwawXGl1v0Cs0uH\nBoLj7GWz7W2UUupYM9Y3lB1RD79TDcD6mjYASrKtewAuPbGc3AwPJ0wYeifwqVPyuf9TCznnuJIj\nl1CllDqCkioQOJO7Vbf0kO5xk55qNfV43C7OmxM7oxeRiCeUKaXUsSapmoayvVbcq27poSAzdYSt\nlVIqOSRVIMiy5wFq6vKRn6lTPyulFCRpIADIz9AagVJKQZIFAqdPADQQKKWUI6kCQSA4OI2R9hEo\npZQlqQLBQGDwMQdaI1BKKUuSBYLBGoF2FiullCXJAoHWCJRSKlrSBgLtI1BKKUuSBYKwpiGtESil\nFJB0gUBrBEopFS2p5hoaCASZXpTJvy+eSmlO2lgnRymlxoUkCwSGCXlePnXm9LFOilJKjRtJ1TTk\nDwTxuJPqkpVSakRJlSv6AoYUV1JdslJKjSipcsWBQJDUFBnrZCil1LiSVIFAm4aUUmqopMoVB7Rp\nSCmlhkiqXNGnTUNKKTVEUgUCbRpSSqmhkipX1KYhpZQaKqlyRV8giEebhpRSKkJSBQJ/IEiqNg0p\npVSEpMkVA0FD0KBNQ0opFSWhuaKIXCwiW0Vkh4jcNsw2V4tIpYhsEpG/JCotzsyj2jSklFKREjbp\nnIi4gbuBpUANsEpEnjHGVIZtMxv4BnCmMaZVREoSlR4nEGjTkFJKRUpkrngasMMYU2WM8QGPAJdF\nbfNp4G5jTCuAMaYhUYkJ7lvLZ9x/RysESikVacRAICJfEpH8Qzj2RGBv2Psae1m444DjROR1EXlL\nRC4eJg03i8hqEVnd2Nh4CEkBV/XrfMPzMBl0H9L+Sil1rIqnRlCK1azzqN3mP5pl6hRgNnAu8DHg\n9yKSF72RMeYeY8xCY8zC4uLiQzqRL91qdcoZaDrkxCql1LFoxEBgjLkdK7O+D/gUsF1EfiQiM0fY\ndR8wOez9JHtZuBrgGWPMgDFmF7DNPteo688oAyDLd2g1CqWUOlbF1UdgjDFAnf3jB/KBx0XkzgPs\ntgqYLSLTRSQVuAZ4Jmqbp7FqA4hIEVZTUdXBXEC8fF6rJqGBQCmlIo04akhEbgE+CTQB9wJfM8YM\niIgL2A58PdZ+xhi/iHwReA5wA/cbYzaJyPeA1caYZ+x1F4pIJRCwj908GhcWrcdrNQ1l9GsgUEqp\ncPEMHy0ArjDG7AlfaIwJisgHD7SjMWYZsCxq2bfDXhvgK/ZPQvkkjTaTSUZffaJPpZRSR5V4moae\nBVqcNyKSIyKnAxhjNicqYaPNHwhSb/Lx9mmNQCmlwsUTCP4X6Ap732UvO6r4nEDQqzUCpZQKF08g\nELsJB7CahEjgHcmJMhAw1Jt8UnsTds+aUkodleIJBFUi8mUR8dg/t5CgkT2J5A8EqaMAT28jBANj\nnRyllBo34gkEnwXOwLoHoAY4Hbg5kYlKhIFAkDaThZgA9HeMdXKUUmrcGLGJx57/55ojkJaE8gUM\nA7itN1ojUEqpkHjuI/ACNwInAF5nuTHmhgSma9T5A0ECGgiUUmqIeJqGHgLKgIuAV7CmiuhMZKIS\nYSAQxB8KBP6xTYxSSo0j8QSCWcaYbwHdxpgHgEux+gmOKr6AIYg9X54GAqWUCoknEAzYv9tEZB6Q\nCyTsATKJ4g8E8RutESilVLR47ge4x34ewe1Yk8ZlAd9KaKoSYCC8j8AExzYxSik1jhwwENgTy3XY\nTxBbCcw4IqlKgBMn5VFcUQ470BqBUkqFOWDTkH0XcczZRY8275tRyEdOnWK90UCglFIh8fQRvCAi\nt4rIZBEpcH4SnrJEcNkVIA0ESikVEk8fwUft318IW2Y4GpuJQoFA+wiUUsoRz53F049EQo4Il44a\nUkqpaPHcWfzJWMuNMQ+OfnISTAOBUkoNEU/T0KKw117gAuBd4CgMBNpHoJRS0eJpGvpS+HsRyQMe\nSViKEkkDgVJKDRHPqKFo3cDR2W/gBAK9oUwppULi6SP4O9YoIbACRwXwaCITlTBixz2tESilVEg8\nfQQ/C3vtB/YYY2oSlJ7E0qYhpZQaIp5AUA3UGmP6AEQkXUSmGWN2JzRliaCBQCmlhoinj+AxILxR\nPWAvO/qEAoE+mEYppRzxBIIUY4zPeWO/Tk1ckhLIpU8oU0qpaPEEgkYR+bDzRkQuA5oSl6QE0hvK\nlFJqiHj6CD4L/FlE7rLf1wAx7zYe97SPQCmlhojnhrKdwPtEJMt+35XwVCVK6D4CbRpSSinHiE1D\nIvIjEckzxnQZY7pEJF9EfnAkEjfqtLNYKaWGiKeP4BJjTJvzxn5a2QcSl6QE0hvKlFJqiHgCgVtE\n0pw3IpIOpB1g+/FL+wiUUmqIeALBn4EXReRGEbkJWA48EM/BReRiEdkqIjtE5LYDbPdvImJEZGF8\nyT5E2jSklFJDxNNZfIeIrAOWYM059BwwdaT9RMQN3A0sxRpptEpEnjHGVEZtlw3cArx98Mk/SFoj\nUEqpIeKdfbQeKwhcBZwPbI5jn9OAHcaYKvsmtEeAy2Js933gDqAvzrQcOr2hTCmlhhg2EIjIcSLy\nHRHZAvwGa84hMcacZ4y5a7j9wkwE9oa9r7GXhZ/jVGCyMeafBzqQiNwsIqtFZHVjY2Mcpx7uQNpZ\nrJRS0Q5UI9iCVfr/oDHmLGPMb7DmGRoVIuICfgF8daRtjTH3GGMWGmMWFhcXH85JreYhvY9AKaVC\nDhQIrgBqgRUi8nsRuQCQgzj2PmBy2PtJ9jJHNjAPeFlEdgPvA545Ih3GWiNQSqmQYQOBMeZpY8w1\nwPHACuA/gBIR+V8RuTCOY68CZovIdBFJBa4Bngk7frsxpsgYM80YMw14C/iwMWb1YVzPyMStfQRK\nKRVmxM5iY0y3MeYvxpgPYZXq3wP+Xxz7+YEvYo0y2gw8aozZJCLfC5/E7ojTGoFSSkWIZ9K5EPuu\n4nvsn3i2XwYsi1r27WG2Pfdg0nLIXFojUEqpcIfy8Pqjm9YIlFIqQhIGArcGAqWUCpOEgSBFm4aU\nUipMEgYCt95HoJRSYZIwEGgfgVJKhUu+QCDaR6CUUuGSLxBoH4FSSkVIwkCg9xEopVS4JAwE2keg\nlFLhNBAopVSSS8JAoJ3FSikVLgkDgXYWK6VUuCQMBHpDmVJKhUvCQKB9BEopFS75AoHeUKaUUhGS\nLxBoH4FSSkVIwkCgN5QppVS4JAwE2keglFLhkjAQaB+BUkqFS8JAoH0ESikVLgkDgd5HoJRS4ZIw\nEGgfgVJKhUu+QKD3ESilVITkCwRaI1BKqQhJGgiCY50KpZQaN5IwEGjTkFJKhdNAoJRSSS4JA4H2\nESilVLjkDAQY7SdQSilbEgYCt/VbbypTSikgwYFARC4Wka0iskNEboux/isiUiki60XkRRGZmsj0\nWCe1A4E2DymlFJDAQCAibuBu4BKgAviYiFREbfYesNAYcyLwOHBnotIT4kqxfmsgUEopILE1gtOA\nHcaYKmOMD3gEuCx8A2PMCmNMj/32LWBSAtNjCQUCbRpSSilIbCCYCOwNe19jLxvOjcCzsVaIyM0i\nslpEVjc2Nh5eqtwe63fAd3jHUUqpY8S46CwWkeuAhcBPY603xtxjjFlojFlYXFx8eCdLzbJ++7oO\n7zhKKXWMSEngsfcBk8PeT7KXRRCRJcA3gXOMMf0JTI8lzQ4E/RoIlFIKElsjWAXMFpHpIpIKXAM8\nE76BiJwC/B/wYWNMQwLTMkhrBEopFSFhgcAY4we+CDwHbAYeNcZsEpHviciH7c1+CmQBj4nIWhF5\nZpjDjZ60bOt3f2fCT6WUUkeDRDYNYYxZBiyLWvbtsNdLEnn+mJwagQYCpZQCxkln8RGVpk1DSikV\nLgkDgdM0pIFAKaUgGQOBdhYrpVSE5AsELjd4MrSPQCmlbMkXCMCqFWggUEopIFkDQVqWNg0ppZQt\nSQNBtnYWK6WULTkDQWq21giUUsqWnIEgTfsIlFLKkZyBIFX7CJRSypGcgUBrBEopFZKcgSA1SzuL\nlVLKlpyBIC0H/L0Q0OcWK6VUkgYCnWZCKaUcyRkIvHnW796WsU2HUkqNA8kZCLLLrN+ddWObDqWU\nGgeSMxDkTLB+d9aObTqUUmocSM5A4NQIOjQQKKVUcgYCbx6keLVGoJRSJGsgEIHscg0ESilFsgYC\nsAOBdhYrpVTyBoKccujYP9apUEqpMZe8gcCpERgz1ilRSqkxlTLWCRgz2WXWNBNPfhou+y2kpI51\nitSxbMs/YdV9g+8nLYJpZ8Hrv7IKIyVz4aIfQv0mWP9XWPLfVl9WLJ118M+vgisFln4Pnr8dfN2H\nli5xwdm3wvbnYf/ag9t37gdh4Q2w6Wl498H49imcBZfcMfy1gVVTX/Y1GOg9uPTEIi445+vW51+3\nIXLd6Z+Bxq1Q9fLQ/eZdAadcZ73e+CS896fBddPOgvd/Zeg+e1fBK3eACVrv86bAB38JTdth+bcg\nMHD413P6Z+C4iw7/OFGSNxDMvABKHoYNj8GZt0DZ/LFOkTqWvfsQVL9lZfgd+2DP69bvXSshqxR2\nvgjnfdPKdF7/NSz+ImSVxD7WrpWw5R/W66xS2PwMlJwAnvSDT1ftOivDWvMHq5acXR7ffi1V1mCL\nhTfAmj9CzWoonnPgfbobrOs8/5vgzR1+u6pXrOsrOxHch1lAq10L+VOtIJwzcXDoeEOlNdVM9VtW\nIM6bMrhP8w5r1gEnEKz5gxUki46D9r3WMWMFgk1PQtUKKD8Zepqsaz33Ntj2L+tn4gLgAAEwHv7+\nw9t/GMkbCEor4EO/gvuWWiUsDQQqkTprYeoZcN3j8Obd8Nx/QeMWKzCc/ll4+nPQVTc4gKGzdvhA\nED7abf+71u/rHh+8UfJg3H061K23SrHv/wosuim+/f75Vdj4hJ2eOphxDlzz5wPvs+FxeOJGa/sD\nBQLn+m54DlIz4kvPcH6zwAp2GKvms/B6a/kDH4K2vdDVAGd/zQpOjr99AXa8GJaeOph5Hlz9ILxy\nJ6z4oZUhp6QNTXf+NPj0i1YN5JFrrWWddeDJhJtePHBNaAwlbx8BDJZ+tNNYJVpnrTVAAQb/7uo2\nQPaEsL/DWujcP/h6OB21IO7BY4gLMocJGiPJLh9sMsk+iECSXQ69rVbzTef++IJQvP9vnbVWoDjc\nIOCc07m+8DRmT7BqBZjB7yV8XVf94OzEHbWDn41zDbFGHHbUDq6P/k5zysdtEIBkDwRZpdZvHUaq\nEingt0qe0ZmEv89qqghlLrWRNYLhdNZCwXTrxkh/nxUE3IdYuc8ut44Bg80m8e4H0LIL+trj2zfe\nOb46a+NvohrxnMNcX3ZZ2PLoQFBm1ZC6G60HWPk6B/c9UCDojBEInO90tK4nQZI7EKSkQkbRYClM\nqUToqgdMWGYSniGVh2WQtYOl5ZECQXh7/sFk4NGi03Kw++1/L/59Q5njCP9vHbWHd00R5xzm+iJe\nR50rPJ1Ohh/9WUdfgzF2hm+vzyy2amrOdzpa15MgyR0IwKqyaY1AJVIoM3GaF8IyhZxyqxnEk2GX\nrtvsfeIJBPZxDqVvIHR+e19xWZnXwe7n9FHEEwhSM6xrHbFGUHdwzVQH4qTTlWIV+kLLwwNB1Lly\nwkr9zvfgLAtNWBl1Db2tEOgfXO9OsVocOo6OGkHydhY7dKoJlWjO35eTcXvSIT3fyjyy7bbj7DJr\nNEpon2Eyy/CSp9sTedxD4eybVXpwzUuHUiNwtjvQ/1swaHWaj3aNIKsMXGHlXie94obMoqh9wpp1\nnGG5zrL0fHCnDb2G3abHsQAACVVJREFU6O/Yed242QoQ4zwQJLRGICIXi8hWEdkhIrfFWJ8mIn+1\n178tItMSmZ6Ysst1FlKVWKFSZXhnZVRbcvYE2GeXrj0Zw/9N9rRAwGcdK9TUdBil51i1lHh48yAl\nfTDN0R2uw55vhP+3niYI+g+vlhNxPvs4QzqEw5p6XO7IdZnFVoDoCGuqcz4fJ2hHX4PzPjuqQ/pg\nP58xkrBAICJu4G7gEqAC+JiIVERtdiPQaoyZBfwSuCNR6RlWdrnVKTQaN3soFUunPconvGkiuvMx\nuwyw73IvP2n4UnN4yXM0+wgOtsTqZIgYK3Cl5cR5vhGaYmOVrA9HrH4ZGBwoEus8Lre1vtMezpua\nDWnZYceMUasZrkbgfKfjvEaQyKah04AdxpgqABF5BLgMqAzb5jLgu/brx4G7RESMOYLzPjhf1m8X\nDy0ZKDUanKac6KYJdypkFNjvwzKQCadA9ZvWGP9ovp7B/V0pg68PVVYJIIeW8WaXQ+sua994h0Zm\nl1k30sW6Noi8vtEwXKBzBooMd57sMqh82grgQzqTy2Drs5HX0NMSeb7oc47zzuJEBoKJwN6w9zVA\n9Lcf2sYY4xeRdqAQaArfSERuBm4GmDJlCqNq9oVw4kcHh5IpNdqK58D0cyKXLbzBugPVyUBP/KjV\nDJE/DU76mDXSKOiPfbyZ51r7BgesO5CnnnHoaXN74KIfwdTFB7/v4s9DVjHMWhr/PvOvhNbdYALD\nbzPr/NG7wTMlDS78IUw/e+i6Jd+1Pu9YzviSFQgAZkdN6bDwBkIl/XAlFZE3mZ1wuXXTYGYx5I5y\nvjXKJFGFbxG5ErjYGHOT/f4TwOnGmC+GbbPR3qbGfr/T3qYp1jEBFi5caFavXp2QNCul1LFKRNYY\nYxbGWpfIzuJ9wOSw95PsZTG3EZEUIBdoTmCalFJKRUlkIFgFzBaR6SKSClwDPBO1zTPAv9uvrwRe\nOqL9A0oppRLXR2C3+X8ReA5wA/cbYzaJyPeA1caYZ4D7gIdEZAfQghUslFJKHUEJvaHMGLMMWBa1\n7Nthr/uAqxKZBqWUUgemU0wopVSS00CglFJJTgOBUkolOQ0ESimV5BJ2Q1miiEgjsOcQdy8i6q7l\no5hey/ik1zI+6bXAVGNMzLnGj7pAcDhEZPVwd9YdbfRaxie9lvFJr+XAtGlIKaWSnAYCpZRKcskW\nCO4Z6wSMIr2W8UmvZXzSazmApOojUEopNVSy1QiUUkpF0UCglFJJLmkCgYhcLCJbRWSHiNw21uk5\nWCKyW0Q2iMhaEVltLysQkeUist3+nT/W6YxFRO4XkQb7QUTOsphpF8v/2N/TehE5dexSPtQw1/Jd\nEdlnfzdrReQDYeu+YV/LVhG5KPZRjzwRmSwiK0SkUkQ2icgt9vKj7ns5wLUcjd+LV0TeEZF19rX8\nt718uoi8baf5r/bU/ohImv1+h71+2iGd2BhzzP9gTYO9E5gBpALrgIqxTtdBXsNuoChq2Z3Abfbr\n24A7xjqdw6T9bOBUYONIaQc+ADwLCPA+4O2xTn8c1/Jd4NYY21bYf2tpwHT7b9A91tdgp60cONV+\nnQ1ss9N71H0vB7iWo/F7ESDLfu0B3rY/70eBa+zlvwM+Z7/+PPA7+/U1wF8P5bzJUiM4DdhhjKky\nxviAR4DLxjhNo+Ey4AH79QPA5WOYlmEZY1ZiPW/i/7d3vyFSVWEcx78PZrVkaFlIZGHWQhCZhUSF\n9MIosDcSCRZBEkIgFfUmeiH0qldBEZYESUWFFGRJvgpNJYIKo9JNkUoqKFn/BVpCLGa/Xpxn7DLu\nbDvmevd2fx8Y5t5zLzPP4ZndM+fcO+dU9Yp9CfCmis+BGRFxhlYy/+961KWXJcA7kkYk/QjspXwW\naydpWNJXuf07sIeyhnjj8jJGXXqZzHmRpGO5OzUfAhYB67O8Oy+dfK0H7ojoLIQ9fm1pCC4Hfq7s\n/8LYH5TJSMCmiPgyIh7OslmShnN7PzCrntBOS6/Ym5qrR3PI5LXKEF0j6pLDCTdSvn02Oi9ddYEG\n5iUipkTEDuAgsJnSYzki6c88pRrvybrk8aPAzH7fsy0Nwf/BQkk3AYuBRyLi9upBlb5hI+8FbnLs\n6WXgamA+MAw8V2844xcR04D3gCck/VY91rS8jFKXRuZF0glJ8ynrvN8MXDvR79mWhmAfcEVlf3aW\nNYakffl8ENhA+YAc6HTP8/lgfRH2rVfsjcuVpAP5x/sXsJZ/hhkmdV0iYirlH+c6Se9ncSPzMlpd\nmpqXDklHgG3ArZShuM6KktV4T9Ylj08Hfu33vdrSEHwBDOaV93MpF1U21hzTuEXEBRFxYWcbuAvY\nRanD8jxtOfBBPRGell6xbwQezLtUbgGOVoYqJqWusfJ7KLmBUpf78s6Oq4BBYPvZjm80OY78KrBH\n0vOVQ43LS6+6NDQvl0bEjNweAO6kXPPYBizN07rz0snXUmBr9uT6U/dV8rP1oNz18B1lvG1V3fH0\nGftcyl0OO4HdnfgpY4FbgO+Bj4CL6461R/xvU7rmxynjmyt6xU65a2JN5ukbYEHd8Y+jLm9lrEP5\nh3lZ5fxVWZdvgcV1x1+JayFl2GcI2JGPu5uYlzHq0sS8zAO+zph3AU9n+VxKY7UXeBc4L8vPz/29\neXzu6byvp5gwM2u5tgwNmZlZD24IzMxazg2BmVnLuSEwM2s5NwRmZi3nhsCsS0ScqMxYuSPO4Gy1\nETGnOnOp2WRwzr+fYtY6f6j8xN+sFdwjMBunKGtCPBtlXYjtEXFNls+JiK05udmWiLgyy2dFxIac\nW35nRNyWLzUlItbmfPOb8hekZrVxQ2B2qoGuoaFllWNHJV0PvAS8kGUvAm9ImgesA1Zn+WrgY0k3\nUNYw2J3lg8AaSdcBR4B7J7g+ZmPyL4vNukTEMUnTRin/CVgk6Yec5Gy/pJkRcZgyfcHxLB+WdElE\nHAJmSxqpvMYcYLOkwdx/Cpgq6ZmJr5nZ6NwjMOuPemz3Y6SyfQJfq7OauSEw68+yyvNnuf0pZUZb\ngAeAT3J7C7ASTi42Mv1sBWnWD38TMTvVQK4Q1fGhpM4tpBdFxBDlW/39WfYY8HpEPAkcAh7K8seB\nVyJiBeWb/0rKzKVmk4qvEZiNU14jWCDpcN2xmJ1JHhoyM2s59wjMzFrOPQIzs5ZzQ2Bm1nJuCMzM\nWs4NgZlZy7khMDNrub8BpJ0K+psipnAAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "# Evaluate on test data\n",
            "40/40 [==============================] - 0s 2ms/sample - loss: 2.0491 - acc: 0.5000\n",
            "test loss, test acc: [2.0490654368652033, 0.5]\n",
            "EEG_Deep/Data2A/parsed_P04T.mat\n",
            "Filtering of Training Data Finished\n",
            "EEG_Deep/Data2A/parsed_P04E.mat\n",
            "Filtering of Testing Data Finished\n",
            "(1, 2)\n",
            "Finding labels in training data\n",
            "Finding labels in testing data\n",
            "[2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1]\n",
            "(60, 12, 1536)\n",
            "(60,)\n",
            "(20, 12, 1536)\n",
            "(20,)\n",
            "[[0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]]\n",
            "X_train shape: (60, 1, 12, 1536)\n",
            "60 train samples\n",
            "20 val samples\n",
            "(40, 12, 1536)\n",
            "(40, 1)\n",
            "X_train shape: (40, 1, 12, 1536)\n",
            "40 train samples\n",
            "Train on 60 samples, validate on 20 samples\n",
            "Epoch 1/300\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 1.30196, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 1s - loss: 1.4010 - acc: 0.2667 - val_loss: 1.3020 - val_acc: 0.5000\n",
            "Epoch 2/300\n",
            "\n",
            "Epoch 00002: val_loss improved from 1.30196 to 1.23442, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 1.1306 - acc: 0.4667 - val_loss: 1.2344 - val_acc: 0.5000\n",
            "Epoch 3/300\n",
            "\n",
            "Epoch 00003: val_loss improved from 1.23442 to 1.18930, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.9816 - acc: 0.6333 - val_loss: 1.1893 - val_acc: 0.5000\n",
            "Epoch 4/300\n",
            "\n",
            "Epoch 00004: val_loss improved from 1.18930 to 1.15413, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.8881 - acc: 0.6333 - val_loss: 1.1541 - val_acc: 0.5000\n",
            "Epoch 5/300\n",
            "\n",
            "Epoch 00005: val_loss improved from 1.15413 to 1.12763, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.8442 - acc: 0.7167 - val_loss: 1.1276 - val_acc: 0.5000\n",
            "Epoch 6/300\n",
            "\n",
            "Epoch 00006: val_loss improved from 1.12763 to 1.10692, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.8132 - acc: 0.7333 - val_loss: 1.1069 - val_acc: 0.5500\n",
            "Epoch 7/300\n",
            "\n",
            "Epoch 00007: val_loss improved from 1.10692 to 1.08806, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.7749 - acc: 0.8000 - val_loss: 1.0881 - val_acc: 0.5500\n",
            "Epoch 8/300\n",
            "\n",
            "Epoch 00008: val_loss improved from 1.08806 to 1.07245, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.7522 - acc: 0.7833 - val_loss: 1.0724 - val_acc: 0.5500\n",
            "Epoch 9/300\n",
            "\n",
            "Epoch 00009: val_loss improved from 1.07245 to 1.05789, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.7371 - acc: 0.7833 - val_loss: 1.0579 - val_acc: 0.5500\n",
            "Epoch 10/300\n",
            "\n",
            "Epoch 00010: val_loss improved from 1.05789 to 1.04369, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.7303 - acc: 0.8167 - val_loss: 1.0437 - val_acc: 0.5500\n",
            "Epoch 11/300\n",
            "\n",
            "Epoch 00011: val_loss improved from 1.04369 to 1.03057, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.7075 - acc: 0.8000 - val_loss: 1.0306 - val_acc: 0.5500\n",
            "Epoch 12/300\n",
            "\n",
            "Epoch 00012: val_loss improved from 1.03057 to 1.01456, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6813 - acc: 0.8500 - val_loss: 1.0146 - val_acc: 0.5500\n",
            "Epoch 13/300\n",
            "\n",
            "Epoch 00013: val_loss improved from 1.01456 to 1.00030, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6497 - acc: 0.8833 - val_loss: 1.0003 - val_acc: 0.5500\n",
            "Epoch 14/300\n",
            "\n",
            "Epoch 00014: val_loss improved from 1.00030 to 0.98611, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6663 - acc: 0.8000 - val_loss: 0.9861 - val_acc: 0.5500\n",
            "Epoch 15/300\n",
            "\n",
            "Epoch 00015: val_loss improved from 0.98611 to 0.97227, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6786 - acc: 0.7333 - val_loss: 0.9723 - val_acc: 0.5500\n",
            "Epoch 16/300\n",
            "\n",
            "Epoch 00016: val_loss improved from 0.97227 to 0.95716, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6224 - acc: 0.8833 - val_loss: 0.9572 - val_acc: 0.5500\n",
            "Epoch 17/300\n",
            "\n",
            "Epoch 00017: val_loss improved from 0.95716 to 0.94359, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6349 - acc: 0.9000 - val_loss: 0.9436 - val_acc: 0.5500\n",
            "Epoch 18/300\n",
            "\n",
            "Epoch 00018: val_loss improved from 0.94359 to 0.93285, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6212 - acc: 0.9000 - val_loss: 0.9328 - val_acc: 0.5500\n",
            "Epoch 19/300\n",
            "\n",
            "Epoch 00019: val_loss improved from 0.93285 to 0.92183, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6034 - acc: 0.8167 - val_loss: 0.9218 - val_acc: 0.5500\n",
            "Epoch 20/300\n",
            "\n",
            "Epoch 00020: val_loss improved from 0.92183 to 0.91032, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5946 - acc: 0.8500 - val_loss: 0.9103 - val_acc: 0.5500\n",
            "Epoch 21/300\n",
            "\n",
            "Epoch 00021: val_loss improved from 0.91032 to 0.89684, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5973 - acc: 0.8500 - val_loss: 0.8968 - val_acc: 0.5500\n",
            "Epoch 22/300\n",
            "\n",
            "Epoch 00022: val_loss improved from 0.89684 to 0.88404, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5461 - acc: 0.9000 - val_loss: 0.8840 - val_acc: 0.5500\n",
            "Epoch 23/300\n",
            "\n",
            "Epoch 00023: val_loss improved from 0.88404 to 0.86980, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5159 - acc: 0.9000 - val_loss: 0.8698 - val_acc: 0.5500\n",
            "Epoch 24/300\n",
            "\n",
            "Epoch 00024: val_loss improved from 0.86980 to 0.85534, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4898 - acc: 0.8833 - val_loss: 0.8553 - val_acc: 0.5500\n",
            "Epoch 25/300\n",
            "\n",
            "Epoch 00025: val_loss improved from 0.85534 to 0.84426, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5431 - acc: 0.8333 - val_loss: 0.8443 - val_acc: 0.5500\n",
            "Epoch 26/300\n",
            "\n",
            "Epoch 00026: val_loss improved from 0.84426 to 0.82606, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4650 - acc: 0.9000 - val_loss: 0.8261 - val_acc: 0.5500\n",
            "Epoch 27/300\n",
            "\n",
            "Epoch 00027: val_loss improved from 0.82606 to 0.80964, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4686 - acc: 0.8833 - val_loss: 0.8096 - val_acc: 0.5500\n",
            "Epoch 28/300\n",
            "\n",
            "Epoch 00028: val_loss improved from 0.80964 to 0.78943, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4369 - acc: 0.9167 - val_loss: 0.7894 - val_acc: 0.5500\n",
            "Epoch 29/300\n",
            "\n",
            "Epoch 00029: val_loss improved from 0.78943 to 0.77077, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4243 - acc: 0.9167 - val_loss: 0.7708 - val_acc: 0.5500\n",
            "Epoch 30/300\n",
            "\n",
            "Epoch 00030: val_loss improved from 0.77077 to 0.75668, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4071 - acc: 0.9500 - val_loss: 0.7567 - val_acc: 0.5500\n",
            "Epoch 31/300\n",
            "\n",
            "Epoch 00031: val_loss improved from 0.75668 to 0.74492, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3911 - acc: 0.9833 - val_loss: 0.7449 - val_acc: 0.6000\n",
            "Epoch 32/300\n",
            "\n",
            "Epoch 00032: val_loss improved from 0.74492 to 0.73566, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4109 - acc: 0.9000 - val_loss: 0.7357 - val_acc: 0.6000\n",
            "Epoch 33/300\n",
            "\n",
            "Epoch 00033: val_loss improved from 0.73566 to 0.72683, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4062 - acc: 0.8333 - val_loss: 0.7268 - val_acc: 0.6000\n",
            "Epoch 34/300\n",
            "\n",
            "Epoch 00034: val_loss improved from 0.72683 to 0.71071, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3420 - acc: 0.9167 - val_loss: 0.7107 - val_acc: 0.7000\n",
            "Epoch 35/300\n",
            "\n",
            "Epoch 00035: val_loss improved from 0.71071 to 0.69765, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3650 - acc: 0.9333 - val_loss: 0.6977 - val_acc: 0.7000\n",
            "Epoch 36/300\n",
            "\n",
            "Epoch 00036: val_loss improved from 0.69765 to 0.68297, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3016 - acc: 0.9167 - val_loss: 0.6830 - val_acc: 0.7000\n",
            "Epoch 37/300\n",
            "\n",
            "Epoch 00037: val_loss improved from 0.68297 to 0.67153, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3223 - acc: 0.9667 - val_loss: 0.6715 - val_acc: 0.7000\n",
            "Epoch 38/300\n",
            "\n",
            "Epoch 00038: val_loss improved from 0.67153 to 0.65936, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3201 - acc: 0.9167 - val_loss: 0.6594 - val_acc: 0.7000\n",
            "Epoch 39/300\n",
            "\n",
            "Epoch 00039: val_loss improved from 0.65936 to 0.64776, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3239 - acc: 0.9500 - val_loss: 0.6478 - val_acc: 0.7000\n",
            "Epoch 40/300\n",
            "\n",
            "Epoch 00040: val_loss improved from 0.64776 to 0.63580, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3105 - acc: 0.9333 - val_loss: 0.6358 - val_acc: 0.7500\n",
            "Epoch 41/300\n",
            "\n",
            "Epoch 00041: val_loss improved from 0.63580 to 0.62735, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3012 - acc: 0.9333 - val_loss: 0.6273 - val_acc: 0.7500\n",
            "Epoch 42/300\n",
            "\n",
            "Epoch 00042: val_loss improved from 0.62735 to 0.62424, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2943 - acc: 0.9167 - val_loss: 0.6242 - val_acc: 0.7500\n",
            "Epoch 43/300\n",
            "\n",
            "Epoch 00043: val_loss improved from 0.62424 to 0.60769, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2770 - acc: 0.9500 - val_loss: 0.6077 - val_acc: 0.8000\n",
            "Epoch 44/300\n",
            "\n",
            "Epoch 00044: val_loss improved from 0.60769 to 0.59602, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2831 - acc: 0.8833 - val_loss: 0.5960 - val_acc: 0.8000\n",
            "Epoch 45/300\n",
            "\n",
            "Epoch 00045: val_loss improved from 0.59602 to 0.59426, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2840 - acc: 0.9333 - val_loss: 0.5943 - val_acc: 0.8000\n",
            "Epoch 46/300\n",
            "\n",
            "Epoch 00046: val_loss improved from 0.59426 to 0.58226, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3385 - acc: 0.8667 - val_loss: 0.5823 - val_acc: 0.8000\n",
            "Epoch 47/300\n",
            "\n",
            "Epoch 00047: val_loss improved from 0.58226 to 0.57943, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3295 - acc: 0.9167 - val_loss: 0.5794 - val_acc: 0.8000\n",
            "Epoch 48/300\n",
            "\n",
            "Epoch 00048: val_loss improved from 0.57943 to 0.55700, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3145 - acc: 0.9167 - val_loss: 0.5570 - val_acc: 0.8000\n",
            "Epoch 49/300\n",
            "\n",
            "Epoch 00049: val_loss improved from 0.55700 to 0.54944, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2324 - acc: 0.9833 - val_loss: 0.5494 - val_acc: 0.8000\n",
            "Epoch 50/300\n",
            "\n",
            "Epoch 00050: val_loss improved from 0.54944 to 0.54366, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2469 - acc: 0.9667 - val_loss: 0.5437 - val_acc: 0.8000\n",
            "Epoch 51/300\n",
            "\n",
            "Epoch 00051: val_loss improved from 0.54366 to 0.53428, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3241 - acc: 0.9000 - val_loss: 0.5343 - val_acc: 0.8000\n",
            "Epoch 52/300\n",
            "\n",
            "Epoch 00052: val_loss improved from 0.53428 to 0.53001, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2635 - acc: 0.9000 - val_loss: 0.5300 - val_acc: 0.8000\n",
            "Epoch 53/300\n",
            "\n",
            "Epoch 00053: val_loss improved from 0.53001 to 0.52489, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2147 - acc: 0.9667 - val_loss: 0.5249 - val_acc: 0.8000\n",
            "Epoch 54/300\n",
            "\n",
            "Epoch 00054: val_loss improved from 0.52489 to 0.51735, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2398 - acc: 0.9333 - val_loss: 0.5173 - val_acc: 0.8000\n",
            "Epoch 55/300\n",
            "\n",
            "Epoch 00055: val_loss improved from 0.51735 to 0.51316, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2355 - acc: 0.9833 - val_loss: 0.5132 - val_acc: 0.8000\n",
            "Epoch 56/300\n",
            "\n",
            "Epoch 00056: val_loss improved from 0.51316 to 0.50810, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2417 - acc: 0.9333 - val_loss: 0.5081 - val_acc: 0.8000\n",
            "Epoch 57/300\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.50810\n",
            "60/60 - 0s - loss: 0.2184 - acc: 0.9833 - val_loss: 0.5093 - val_acc: 0.8000\n",
            "Epoch 58/300\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.50810\n",
            "60/60 - 0s - loss: 0.2078 - acc: 0.9667 - val_loss: 0.5101 - val_acc: 0.8000\n",
            "Epoch 59/300\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.50810\n",
            "60/60 - 0s - loss: 0.2024 - acc: 1.0000 - val_loss: 0.5081 - val_acc: 0.8000\n",
            "Epoch 60/300\n",
            "\n",
            "Epoch 00060: val_loss improved from 0.50810 to 0.50306, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2201 - acc: 0.9500 - val_loss: 0.5031 - val_acc: 0.8000\n",
            "Epoch 61/300\n",
            "\n",
            "Epoch 00061: val_loss improved from 0.50306 to 0.49891, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3226 - acc: 0.9000 - val_loss: 0.4989 - val_acc: 0.8000\n",
            "Epoch 62/300\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.49891\n",
            "60/60 - 0s - loss: 0.2126 - acc: 0.9500 - val_loss: 0.5014 - val_acc: 0.8000\n",
            "Epoch 63/300\n",
            "\n",
            "Epoch 00063: val_loss improved from 0.49891 to 0.49533, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1864 - acc: 0.9833 - val_loss: 0.4953 - val_acc: 0.8000\n",
            "Epoch 64/300\n",
            "\n",
            "Epoch 00064: val_loss improved from 0.49533 to 0.48791, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2346 - acc: 0.9500 - val_loss: 0.4879 - val_acc: 0.8000\n",
            "Epoch 65/300\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.48791\n",
            "60/60 - 0s - loss: 0.2007 - acc: 1.0000 - val_loss: 0.4891 - val_acc: 0.8000\n",
            "Epoch 66/300\n",
            "\n",
            "Epoch 00066: val_loss improved from 0.48791 to 0.48703, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2308 - acc: 0.9500 - val_loss: 0.4870 - val_acc: 0.8000\n",
            "Epoch 67/300\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.48703\n",
            "60/60 - 0s - loss: 0.2025 - acc: 0.9500 - val_loss: 0.4898 - val_acc: 0.8000\n",
            "Epoch 68/300\n",
            "\n",
            "Epoch 00068: val_loss improved from 0.48703 to 0.48642, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2146 - acc: 0.9333 - val_loss: 0.4864 - val_acc: 0.8000\n",
            "Epoch 69/300\n",
            "\n",
            "Epoch 00069: val_loss improved from 0.48642 to 0.46710, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1964 - acc: 0.9500 - val_loss: 0.4671 - val_acc: 0.8000\n",
            "Epoch 70/300\n",
            "\n",
            "Epoch 00070: val_loss improved from 0.46710 to 0.45458, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1999 - acc: 0.9500 - val_loss: 0.4546 - val_acc: 0.8500\n",
            "Epoch 71/300\n",
            "\n",
            "Epoch 00071: val_loss improved from 0.45458 to 0.44964, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1967 - acc: 0.9667 - val_loss: 0.4496 - val_acc: 0.8500\n",
            "Epoch 72/300\n",
            "\n",
            "Epoch 00072: val_loss improved from 0.44964 to 0.44643, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2651 - acc: 0.9333 - val_loss: 0.4464 - val_acc: 0.9000\n",
            "Epoch 73/300\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.44643\n",
            "60/60 - 0s - loss: 0.1898 - acc: 0.9667 - val_loss: 0.4531 - val_acc: 0.9000\n",
            "Epoch 74/300\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.44643\n",
            "60/60 - 0s - loss: 0.1848 - acc: 0.9667 - val_loss: 0.4581 - val_acc: 0.9000\n",
            "Epoch 75/300\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.44643\n",
            "60/60 - 0s - loss: 0.1625 - acc: 0.9667 - val_loss: 0.4614 - val_acc: 0.9000\n",
            "Epoch 76/300\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.44643\n",
            "60/60 - 0s - loss: 0.1627 - acc: 0.9833 - val_loss: 0.4609 - val_acc: 0.9000\n",
            "Epoch 77/300\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.44643\n",
            "60/60 - 0s - loss: 0.1503 - acc: 0.9833 - val_loss: 0.4605 - val_acc: 0.8500\n",
            "Epoch 78/300\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.44643\n",
            "60/60 - 0s - loss: 0.2129 - acc: 0.9333 - val_loss: 0.4551 - val_acc: 0.8500\n",
            "Epoch 79/300\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.44643\n",
            "60/60 - 0s - loss: 0.1540 - acc: 0.9833 - val_loss: 0.4526 - val_acc: 0.8500\n",
            "Epoch 80/300\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.44643\n",
            "60/60 - 0s - loss: 0.1658 - acc: 0.9833 - val_loss: 0.4509 - val_acc: 0.8500\n",
            "Epoch 81/300\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.44643\n",
            "60/60 - 0s - loss: 0.1528 - acc: 0.9833 - val_loss: 0.4528 - val_acc: 0.8500\n",
            "Epoch 82/300\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.44643\n",
            "60/60 - 0s - loss: 0.2573 - acc: 0.8833 - val_loss: 0.4501 - val_acc: 0.9000\n",
            "Epoch 83/300\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.44643\n",
            "60/60 - 0s - loss: 0.1630 - acc: 0.9667 - val_loss: 0.4488 - val_acc: 0.9000\n",
            "Epoch 84/300\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.44643\n",
            "60/60 - 0s - loss: 0.1994 - acc: 0.9333 - val_loss: 0.4475 - val_acc: 0.9000\n",
            "Epoch 85/300\n",
            "\n",
            "Epoch 00085: val_loss improved from 0.44643 to 0.44491, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1520 - acc: 0.9833 - val_loss: 0.4449 - val_acc: 0.9000\n",
            "Epoch 86/300\n",
            "\n",
            "Epoch 00086: val_loss improved from 0.44491 to 0.44250, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1646 - acc: 0.9500 - val_loss: 0.4425 - val_acc: 0.9000\n",
            "Epoch 87/300\n",
            "\n",
            "Epoch 00087: val_loss improved from 0.44250 to 0.43776, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1864 - acc: 0.9667 - val_loss: 0.4378 - val_acc: 0.9000\n",
            "Epoch 88/300\n",
            "\n",
            "Epoch 00088: val_loss improved from 0.43776 to 0.43466, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1656 - acc: 0.9500 - val_loss: 0.4347 - val_acc: 0.9000\n",
            "Epoch 89/300\n",
            "\n",
            "Epoch 00089: val_loss improved from 0.43466 to 0.43169, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1452 - acc: 1.0000 - val_loss: 0.4317 - val_acc: 0.9000\n",
            "Epoch 90/300\n",
            "\n",
            "Epoch 00090: val_loss improved from 0.43169 to 0.42932, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1481 - acc: 0.9833 - val_loss: 0.4293 - val_acc: 0.9000\n",
            "Epoch 91/300\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.42932\n",
            "60/60 - 0s - loss: 0.1306 - acc: 0.9833 - val_loss: 0.4306 - val_acc: 0.9000\n",
            "Epoch 92/300\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.42932\n",
            "60/60 - 0s - loss: 0.1538 - acc: 0.9667 - val_loss: 0.4318 - val_acc: 0.9000\n",
            "Epoch 93/300\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.42932\n",
            "60/60 - 0s - loss: 0.1510 - acc: 0.9667 - val_loss: 0.4310 - val_acc: 0.9000\n",
            "Epoch 94/300\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.42932\n",
            "60/60 - 0s - loss: 0.1516 - acc: 0.9833 - val_loss: 0.4306 - val_acc: 0.9000\n",
            "Epoch 95/300\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.42932\n",
            "60/60 - 0s - loss: 0.1372 - acc: 1.0000 - val_loss: 0.4298 - val_acc: 0.9000\n",
            "Epoch 96/300\n",
            "\n",
            "Epoch 00096: val_loss improved from 0.42932 to 0.42931, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1301 - acc: 0.9833 - val_loss: 0.4293 - val_acc: 0.9000\n",
            "Epoch 97/300\n",
            "\n",
            "Epoch 00097: val_loss improved from 0.42931 to 0.42811, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1688 - acc: 0.9833 - val_loss: 0.4281 - val_acc: 0.9000\n",
            "Epoch 98/300\n",
            "\n",
            "Epoch 00098: val_loss improved from 0.42811 to 0.42630, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1188 - acc: 1.0000 - val_loss: 0.4263 - val_acc: 0.9000\n",
            "Epoch 99/300\n",
            "\n",
            "Epoch 00099: val_loss improved from 0.42630 to 0.42216, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1498 - acc: 0.9667 - val_loss: 0.4222 - val_acc: 0.9000\n",
            "Epoch 100/300\n",
            "\n",
            "Epoch 00100: val_loss improved from 0.42216 to 0.41482, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1428 - acc: 0.9667 - val_loss: 0.4148 - val_acc: 0.9000\n",
            "Epoch 101/300\n",
            "\n",
            "Epoch 00101: val_loss improved from 0.41482 to 0.41221, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1369 - acc: 0.9833 - val_loss: 0.4122 - val_acc: 0.9000\n",
            "Epoch 102/300\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.41221\n",
            "60/60 - 0s - loss: 0.1669 - acc: 0.9667 - val_loss: 0.4127 - val_acc: 0.9000\n",
            "Epoch 103/300\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.41221\n",
            "60/60 - 0s - loss: 0.1612 - acc: 0.9667 - val_loss: 0.4122 - val_acc: 0.9000\n",
            "Epoch 104/300\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.41221\n",
            "60/60 - 0s - loss: 0.1661 - acc: 0.9500 - val_loss: 0.4131 - val_acc: 0.9000\n",
            "Epoch 105/300\n",
            "\n",
            "Epoch 00105: val_loss improved from 0.41221 to 0.41211, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1446 - acc: 0.9667 - val_loss: 0.4121 - val_acc: 0.9000\n",
            "Epoch 106/300\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 0.41211\n",
            "60/60 - 0s - loss: 0.1493 - acc: 0.9667 - val_loss: 0.4152 - val_acc: 0.9000\n",
            "Epoch 107/300\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.41211\n",
            "60/60 - 0s - loss: 0.1928 - acc: 0.9833 - val_loss: 0.4219 - val_acc: 0.8500\n",
            "Epoch 108/300\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 0.41211\n",
            "60/60 - 0s - loss: 0.1719 - acc: 0.9500 - val_loss: 0.4244 - val_acc: 0.8500\n",
            "Epoch 109/300\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.41211\n",
            "60/60 - 0s - loss: 0.1517 - acc: 0.9500 - val_loss: 0.4225 - val_acc: 0.8500\n",
            "Epoch 110/300\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.41211\n",
            "60/60 - 0s - loss: 0.1251 - acc: 0.9833 - val_loss: 0.4227 - val_acc: 0.8500\n",
            "Epoch 111/300\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 0.41211\n",
            "60/60 - 0s - loss: 0.1902 - acc: 0.9333 - val_loss: 0.4208 - val_acc: 0.9000\n",
            "Epoch 112/300\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 0.41211\n",
            "60/60 - 0s - loss: 0.1480 - acc: 0.9833 - val_loss: 0.4162 - val_acc: 0.9000\n",
            "Epoch 113/300\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.41211\n",
            "60/60 - 0s - loss: 0.1201 - acc: 1.0000 - val_loss: 0.4140 - val_acc: 0.9000\n",
            "Epoch 114/300\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.41211\n",
            "60/60 - 0s - loss: 0.1029 - acc: 0.9833 - val_loss: 0.4151 - val_acc: 0.9000\n",
            "Epoch 115/300\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.41211\n",
            "60/60 - 0s - loss: 0.1565 - acc: 0.9667 - val_loss: 0.4144 - val_acc: 0.9000\n",
            "Epoch 116/300\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.41211\n",
            "60/60 - 0s - loss: 0.0895 - acc: 1.0000 - val_loss: 0.4191 - val_acc: 0.8500\n",
            "Epoch 117/300\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.41211\n",
            "60/60 - 0s - loss: 0.1383 - acc: 0.9667 - val_loss: 0.4211 - val_acc: 0.8500\n",
            "Epoch 118/300\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.41211\n",
            "60/60 - 0s - loss: 0.1308 - acc: 0.9500 - val_loss: 0.4188 - val_acc: 0.9000\n",
            "Epoch 119/300\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 0.41211\n",
            "60/60 - 0s - loss: 0.1411 - acc: 0.9667 - val_loss: 0.4155 - val_acc: 0.9000\n",
            "Epoch 120/300\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.41211\n",
            "60/60 - 0s - loss: 0.0927 - acc: 1.0000 - val_loss: 0.4146 - val_acc: 0.9000\n",
            "Epoch 121/300\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.41211\n",
            "60/60 - 0s - loss: 0.1010 - acc: 0.9833 - val_loss: 0.4130 - val_acc: 0.9000\n",
            "Epoch 122/300\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.41211\n",
            "60/60 - 0s - loss: 0.1253 - acc: 0.9667 - val_loss: 0.4122 - val_acc: 0.8500\n",
            "Epoch 123/300\n",
            "\n",
            "Epoch 00123: val_loss improved from 0.41211 to 0.41120, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1056 - acc: 0.9833 - val_loss: 0.4112 - val_acc: 0.9000\n",
            "Epoch 124/300\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.2083 - acc: 0.9000 - val_loss: 0.4171 - val_acc: 0.8500\n",
            "Epoch 125/300\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.1390 - acc: 0.9667 - val_loss: 0.4195 - val_acc: 0.8500\n",
            "Epoch 126/300\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.1407 - acc: 0.9667 - val_loss: 0.4218 - val_acc: 0.9000\n",
            "Epoch 127/300\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.1536 - acc: 0.9500 - val_loss: 0.4217 - val_acc: 0.9000\n",
            "Epoch 128/300\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.1004 - acc: 1.0000 - val_loss: 0.4214 - val_acc: 0.9000\n",
            "Epoch 129/300\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.1080 - acc: 0.9833 - val_loss: 0.4263 - val_acc: 0.9000\n",
            "Epoch 130/300\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.1199 - acc: 0.9833 - val_loss: 0.4340 - val_acc: 0.9000\n",
            "Epoch 131/300\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.1188 - acc: 0.9833 - val_loss: 0.4394 - val_acc: 0.9000\n",
            "Epoch 132/300\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.1139 - acc: 0.9833 - val_loss: 0.4463 - val_acc: 0.8500\n",
            "Epoch 133/300\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.1143 - acc: 1.0000 - val_loss: 0.4500 - val_acc: 0.8500\n",
            "Epoch 134/300\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.1807 - acc: 0.9500 - val_loss: 0.4429 - val_acc: 0.8500\n",
            "Epoch 135/300\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.1359 - acc: 0.9667 - val_loss: 0.4394 - val_acc: 0.8500\n",
            "Epoch 136/300\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.1191 - acc: 0.9667 - val_loss: 0.4445 - val_acc: 0.8500\n",
            "Epoch 137/300\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.1076 - acc: 0.9833 - val_loss: 0.4493 - val_acc: 0.8500\n",
            "Epoch 138/300\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.1232 - acc: 0.9667 - val_loss: 0.4445 - val_acc: 0.8500\n",
            "Epoch 139/300\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.1072 - acc: 1.0000 - val_loss: 0.4362 - val_acc: 0.8500\n",
            "Epoch 140/300\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.0774 - acc: 1.0000 - val_loss: 0.4352 - val_acc: 0.8500\n",
            "Epoch 141/300\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.1459 - acc: 0.9667 - val_loss: 0.4321 - val_acc: 0.8500\n",
            "Epoch 142/300\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.1254 - acc: 0.9667 - val_loss: 0.4351 - val_acc: 0.8500\n",
            "Epoch 143/300\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.1102 - acc: 1.0000 - val_loss: 0.4374 - val_acc: 0.8500\n",
            "Epoch 144/300\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.1596 - acc: 0.9667 - val_loss: 0.4401 - val_acc: 0.8500\n",
            "Epoch 145/300\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.0829 - acc: 1.0000 - val_loss: 0.4503 - val_acc: 0.8500\n",
            "Epoch 146/300\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.0963 - acc: 0.9833 - val_loss: 0.4522 - val_acc: 0.8500\n",
            "Epoch 147/300\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.1081 - acc: 0.9833 - val_loss: 0.4632 - val_acc: 0.8500\n",
            "Epoch 148/300\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.0926 - acc: 0.9833 - val_loss: 0.4626 - val_acc: 0.8500\n",
            "Epoch 149/300\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.1406 - acc: 0.9333 - val_loss: 0.4728 - val_acc: 0.8500\n",
            "Epoch 150/300\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.1024 - acc: 0.9833 - val_loss: 0.4578 - val_acc: 0.8500\n",
            "Epoch 151/300\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.1320 - acc: 0.9667 - val_loss: 0.4621 - val_acc: 0.8500\n",
            "Epoch 152/300\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.0830 - acc: 1.0000 - val_loss: 0.4727 - val_acc: 0.8500\n",
            "Epoch 153/300\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.0977 - acc: 0.9833 - val_loss: 0.4751 - val_acc: 0.8500\n",
            "Epoch 154/300\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.0678 - acc: 1.0000 - val_loss: 0.4813 - val_acc: 0.8500\n",
            "Epoch 155/300\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.1110 - acc: 0.9833 - val_loss: 0.4777 - val_acc: 0.8500\n",
            "Epoch 156/300\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.1198 - acc: 0.9667 - val_loss: 0.4700 - val_acc: 0.8500\n",
            "Epoch 157/300\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.0784 - acc: 1.0000 - val_loss: 0.4677 - val_acc: 0.8500\n",
            "Epoch 158/300\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.1005 - acc: 0.9833 - val_loss: 0.4593 - val_acc: 0.8500\n",
            "Epoch 159/300\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.1735 - acc: 0.9333 - val_loss: 0.4612 - val_acc: 0.8500\n",
            "Epoch 160/300\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.1294 - acc: 0.9500 - val_loss: 0.4694 - val_acc: 0.8500\n",
            "Epoch 161/300\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.1262 - acc: 0.9833 - val_loss: 0.4585 - val_acc: 0.8500\n",
            "Epoch 162/300\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.1013 - acc: 0.9667 - val_loss: 0.4558 - val_acc: 0.8500\n",
            "Epoch 163/300\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.1035 - acc: 0.9833 - val_loss: 0.4675 - val_acc: 0.8500\n",
            "Epoch 164/300\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.0687 - acc: 0.9833 - val_loss: 0.4768 - val_acc: 0.8500\n",
            "Epoch 165/300\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.1333 - acc: 0.9667 - val_loss: 0.4868 - val_acc: 0.8500\n",
            "Epoch 166/300\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.1075 - acc: 0.9667 - val_loss: 0.4932 - val_acc: 0.8500\n",
            "Epoch 167/300\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.1025 - acc: 0.9667 - val_loss: 0.4965 - val_acc: 0.8500\n",
            "Epoch 168/300\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.1642 - acc: 0.9167 - val_loss: 0.5161 - val_acc: 0.8500\n",
            "Epoch 169/300\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.1466 - acc: 0.9667 - val_loss: 0.5174 - val_acc: 0.8500\n",
            "Epoch 170/300\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.1019 - acc: 0.9833 - val_loss: 0.5151 - val_acc: 0.8500\n",
            "Epoch 171/300\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.1061 - acc: 0.9833 - val_loss: 0.4920 - val_acc: 0.8500\n",
            "Epoch 172/300\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.1160 - acc: 0.9500 - val_loss: 0.4678 - val_acc: 0.8500\n",
            "Epoch 173/300\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.0782 - acc: 1.0000 - val_loss: 0.4633 - val_acc: 0.8500\n",
            "Epoch 174/300\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.0963 - acc: 0.9667 - val_loss: 0.4888 - val_acc: 0.8500\n",
            "Epoch 175/300\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.1096 - acc: 0.9500 - val_loss: 0.5149 - val_acc: 0.8500\n",
            "Epoch 176/300\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.0853 - acc: 0.9833 - val_loss: 0.5130 - val_acc: 0.8500\n",
            "Epoch 177/300\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.1150 - acc: 0.9500 - val_loss: 0.5236 - val_acc: 0.8500\n",
            "Epoch 178/300\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.0771 - acc: 1.0000 - val_loss: 0.5041 - val_acc: 0.8500\n",
            "Epoch 179/300\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.0804 - acc: 1.0000 - val_loss: 0.4982 - val_acc: 0.8500\n",
            "Epoch 180/300\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.0995 - acc: 0.9833 - val_loss: 0.5077 - val_acc: 0.8500\n",
            "Epoch 181/300\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.0884 - acc: 0.9833 - val_loss: 0.5128 - val_acc: 0.8500\n",
            "Epoch 182/300\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.0971 - acc: 0.9667 - val_loss: 0.5289 - val_acc: 0.8500\n",
            "Epoch 183/300\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.0981 - acc: 1.0000 - val_loss: 0.5440 - val_acc: 0.8000\n",
            "Epoch 184/300\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.0797 - acc: 0.9833 - val_loss: 0.5679 - val_acc: 0.7500\n",
            "Epoch 185/300\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.0628 - acc: 1.0000 - val_loss: 0.5799 - val_acc: 0.7500\n",
            "Epoch 186/300\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.1269 - acc: 0.9667 - val_loss: 0.5574 - val_acc: 0.8000\n",
            "Epoch 187/300\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.1030 - acc: 0.9833 - val_loss: 0.5417 - val_acc: 0.8000\n",
            "Epoch 188/300\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.1386 - acc: 0.9500 - val_loss: 0.5374 - val_acc: 0.8000\n",
            "Epoch 189/300\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.0953 - acc: 0.9833 - val_loss: 0.5386 - val_acc: 0.8000\n",
            "Epoch 190/300\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.0776 - acc: 0.9833 - val_loss: 0.5311 - val_acc: 0.8500\n",
            "Epoch 191/300\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.1317 - acc: 0.9833 - val_loss: 0.5372 - val_acc: 0.8000\n",
            "Epoch 192/300\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.0689 - acc: 1.0000 - val_loss: 0.5260 - val_acc: 0.8500\n",
            "Epoch 193/300\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.0778 - acc: 0.9833 - val_loss: 0.5135 - val_acc: 0.8500\n",
            "Epoch 194/300\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.0642 - acc: 1.0000 - val_loss: 0.4925 - val_acc: 0.8500\n",
            "Epoch 195/300\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.0760 - acc: 0.9833 - val_loss: 0.4865 - val_acc: 0.8500\n",
            "Epoch 196/300\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.1829 - acc: 0.9500 - val_loss: 0.4778 - val_acc: 0.8500\n",
            "Epoch 197/300\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.0974 - acc: 0.9667 - val_loss: 0.4884 - val_acc: 0.8500\n",
            "Epoch 198/300\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.1486 - acc: 0.9333 - val_loss: 0.4867 - val_acc: 0.8500\n",
            "Epoch 199/300\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.0724 - acc: 1.0000 - val_loss: 0.4941 - val_acc: 0.8500\n",
            "Epoch 200/300\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.1115 - acc: 0.9667 - val_loss: 0.4791 - val_acc: 0.8500\n",
            "Epoch 201/300\n",
            "\n",
            "Epoch 00201: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.0873 - acc: 0.9833 - val_loss: 0.4699 - val_acc: 0.8500\n",
            "Epoch 202/300\n",
            "\n",
            "Epoch 00202: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.0933 - acc: 0.9667 - val_loss: 0.4612 - val_acc: 0.8500\n",
            "Epoch 203/300\n",
            "\n",
            "Epoch 00203: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.0706 - acc: 0.9833 - val_loss: 0.4656 - val_acc: 0.8500\n",
            "Epoch 204/300\n",
            "\n",
            "Epoch 00204: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.0728 - acc: 0.9833 - val_loss: 0.4681 - val_acc: 0.8500\n",
            "Epoch 205/300\n",
            "\n",
            "Epoch 00205: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.0859 - acc: 0.9667 - val_loss: 0.4729 - val_acc: 0.8500\n",
            "Epoch 206/300\n",
            "\n",
            "Epoch 00206: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.0753 - acc: 0.9833 - val_loss: 0.4892 - val_acc: 0.8500\n",
            "Epoch 207/300\n",
            "\n",
            "Epoch 00207: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.0661 - acc: 1.0000 - val_loss: 0.5078 - val_acc: 0.8500\n",
            "Epoch 208/300\n",
            "\n",
            "Epoch 00208: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.0614 - acc: 1.0000 - val_loss: 0.5151 - val_acc: 0.8500\n",
            "Epoch 209/300\n",
            "\n",
            "Epoch 00209: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.0605 - acc: 0.9833 - val_loss: 0.5221 - val_acc: 0.8500\n",
            "Epoch 210/300\n",
            "\n",
            "Epoch 00210: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.0914 - acc: 0.9833 - val_loss: 0.5206 - val_acc: 0.8500\n",
            "Epoch 211/300\n",
            "\n",
            "Epoch 00211: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.0808 - acc: 0.9667 - val_loss: 0.5027 - val_acc: 0.8500\n",
            "Epoch 212/300\n",
            "\n",
            "Epoch 00212: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.1893 - acc: 0.9500 - val_loss: 0.4925 - val_acc: 0.8500\n",
            "Epoch 213/300\n",
            "\n",
            "Epoch 00213: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.0626 - acc: 1.0000 - val_loss: 0.4724 - val_acc: 0.9000\n",
            "Epoch 214/300\n",
            "\n",
            "Epoch 00214: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.0551 - acc: 1.0000 - val_loss: 0.4730 - val_acc: 0.9000\n",
            "Epoch 215/300\n",
            "\n",
            "Epoch 00215: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.0996 - acc: 0.9500 - val_loss: 0.4909 - val_acc: 0.8500\n",
            "Epoch 216/300\n",
            "\n",
            "Epoch 00216: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.1431 - acc: 0.9500 - val_loss: 0.5313 - val_acc: 0.8500\n",
            "Epoch 217/300\n",
            "\n",
            "Epoch 00217: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.1124 - acc: 0.9333 - val_loss: 0.5713 - val_acc: 0.8000\n",
            "Epoch 218/300\n",
            "\n",
            "Epoch 00218: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.1127 - acc: 0.9667 - val_loss: 0.6221 - val_acc: 0.7500\n",
            "Epoch 219/300\n",
            "\n",
            "Epoch 00219: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.0685 - acc: 1.0000 - val_loss: 0.6005 - val_acc: 0.7500\n",
            "Epoch 220/300\n",
            "\n",
            "Epoch 00220: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.0701 - acc: 1.0000 - val_loss: 0.5696 - val_acc: 0.7500\n",
            "Epoch 221/300\n",
            "\n",
            "Epoch 00221: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.0698 - acc: 1.0000 - val_loss: 0.5640 - val_acc: 0.7500\n",
            "Epoch 222/300\n",
            "\n",
            "Epoch 00222: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.0670 - acc: 1.0000 - val_loss: 0.5708 - val_acc: 0.7500\n",
            "Epoch 223/300\n",
            "\n",
            "Epoch 00223: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.0596 - acc: 0.9833 - val_loss: 0.5849 - val_acc: 0.7500\n",
            "Epoch 224/300\n",
            "\n",
            "Epoch 00224: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.0770 - acc: 0.9667 - val_loss: 0.5989 - val_acc: 0.7500\n",
            "Epoch 225/300\n",
            "\n",
            "Epoch 00225: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.0450 - acc: 1.0000 - val_loss: 0.5890 - val_acc: 0.8000\n",
            "Epoch 226/300\n",
            "\n",
            "Epoch 00226: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.0600 - acc: 0.9833 - val_loss: 0.5824 - val_acc: 0.8000\n",
            "Epoch 227/300\n",
            "\n",
            "Epoch 00227: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.0725 - acc: 0.9833 - val_loss: 0.5919 - val_acc: 0.8000\n",
            "Epoch 228/300\n",
            "\n",
            "Epoch 00228: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.0830 - acc: 0.9833 - val_loss: 0.5788 - val_acc: 0.8000\n",
            "Epoch 229/300\n",
            "\n",
            "Epoch 00229: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.0654 - acc: 0.9833 - val_loss: 0.5802 - val_acc: 0.8000\n",
            "Epoch 230/300\n",
            "\n",
            "Epoch 00230: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.1137 - acc: 0.9667 - val_loss: 0.5957 - val_acc: 0.8000\n",
            "Epoch 231/300\n",
            "\n",
            "Epoch 00231: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.0763 - acc: 0.9833 - val_loss: 0.6020 - val_acc: 0.8000\n",
            "Epoch 232/300\n",
            "\n",
            "Epoch 00232: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.0660 - acc: 0.9833 - val_loss: 0.6023 - val_acc: 0.8000\n",
            "Epoch 233/300\n",
            "\n",
            "Epoch 00233: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.0612 - acc: 1.0000 - val_loss: 0.5996 - val_acc: 0.8000\n",
            "Epoch 234/300\n",
            "\n",
            "Epoch 00234: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.0812 - acc: 0.9667 - val_loss: 0.6156 - val_acc: 0.8000\n",
            "Epoch 235/300\n",
            "\n",
            "Epoch 00235: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.0637 - acc: 1.0000 - val_loss: 0.6330 - val_acc: 0.8000\n",
            "Epoch 236/300\n",
            "\n",
            "Epoch 00236: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.0559 - acc: 1.0000 - val_loss: 0.6402 - val_acc: 0.8000\n",
            "Epoch 237/300\n",
            "\n",
            "Epoch 00237: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.0570 - acc: 1.0000 - val_loss: 0.6171 - val_acc: 0.8000\n",
            "Epoch 238/300\n",
            "\n",
            "Epoch 00238: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.0469 - acc: 1.0000 - val_loss: 0.5735 - val_acc: 0.8500\n",
            "Epoch 239/300\n",
            "\n",
            "Epoch 00239: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.0472 - acc: 1.0000 - val_loss: 0.5376 - val_acc: 0.8500\n",
            "Epoch 240/300\n",
            "\n",
            "Epoch 00240: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.0661 - acc: 0.9667 - val_loss: 0.5248 - val_acc: 0.8500\n",
            "Epoch 241/300\n",
            "\n",
            "Epoch 00241: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.1257 - acc: 0.9667 - val_loss: 0.5353 - val_acc: 0.8500\n",
            "Epoch 242/300\n",
            "\n",
            "Epoch 00242: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.0866 - acc: 0.9500 - val_loss: 0.5260 - val_acc: 0.8500\n",
            "Epoch 243/300\n",
            "\n",
            "Epoch 00243: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.0956 - acc: 0.9667 - val_loss: 0.5223 - val_acc: 0.8500\n",
            "Epoch 244/300\n",
            "\n",
            "Epoch 00244: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.0284 - acc: 1.0000 - val_loss: 0.5247 - val_acc: 0.8000\n",
            "Epoch 245/300\n",
            "\n",
            "Epoch 00245: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.0654 - acc: 0.9833 - val_loss: 0.5388 - val_acc: 0.8000\n",
            "Epoch 246/300\n",
            "\n",
            "Epoch 00246: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.0838 - acc: 0.9833 - val_loss: 0.5471 - val_acc: 0.8000\n",
            "Epoch 247/300\n",
            "\n",
            "Epoch 00247: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.0733 - acc: 0.9667 - val_loss: 0.5604 - val_acc: 0.8000\n",
            "Epoch 248/300\n",
            "\n",
            "Epoch 00248: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.0588 - acc: 0.9833 - val_loss: 0.5622 - val_acc: 0.8000\n",
            "Epoch 249/300\n",
            "\n",
            "Epoch 00249: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.0433 - acc: 1.0000 - val_loss: 0.5584 - val_acc: 0.8000\n",
            "Epoch 250/300\n",
            "\n",
            "Epoch 00250: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.0743 - acc: 0.9667 - val_loss: 0.5668 - val_acc: 0.8000\n",
            "Epoch 251/300\n",
            "\n",
            "Epoch 00251: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.0397 - acc: 1.0000 - val_loss: 0.5692 - val_acc: 0.8000\n",
            "Epoch 252/300\n",
            "\n",
            "Epoch 00252: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.1133 - acc: 0.9500 - val_loss: 0.5519 - val_acc: 0.8000\n",
            "Epoch 253/300\n",
            "\n",
            "Epoch 00253: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.0545 - acc: 0.9833 - val_loss: 0.5468 - val_acc: 0.8000\n",
            "Epoch 254/300\n",
            "\n",
            "Epoch 00254: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.0739 - acc: 1.0000 - val_loss: 0.5466 - val_acc: 0.8000\n",
            "Epoch 255/300\n",
            "\n",
            "Epoch 00255: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.0862 - acc: 1.0000 - val_loss: 0.5528 - val_acc: 0.8000\n",
            "Epoch 256/300\n",
            "\n",
            "Epoch 00256: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.0727 - acc: 0.9833 - val_loss: 0.5904 - val_acc: 0.8000\n",
            "Epoch 257/300\n",
            "\n",
            "Epoch 00257: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.0766 - acc: 0.9833 - val_loss: 0.6099 - val_acc: 0.7500\n",
            "Epoch 258/300\n",
            "\n",
            "Epoch 00258: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.0586 - acc: 0.9833 - val_loss: 0.6019 - val_acc: 0.7500\n",
            "Epoch 259/300\n",
            "\n",
            "Epoch 00259: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.0894 - acc: 0.9833 - val_loss: 0.5764 - val_acc: 0.8000\n",
            "Epoch 260/300\n",
            "\n",
            "Epoch 00260: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.0801 - acc: 0.9833 - val_loss: 0.5669 - val_acc: 0.7500\n",
            "Epoch 261/300\n",
            "\n",
            "Epoch 00261: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.0803 - acc: 0.9833 - val_loss: 0.6020 - val_acc: 0.7500\n",
            "Epoch 262/300\n",
            "\n",
            "Epoch 00262: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.1026 - acc: 0.9833 - val_loss: 0.6154 - val_acc: 0.7500\n",
            "Epoch 263/300\n",
            "\n",
            "Epoch 00263: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.0546 - acc: 1.0000 - val_loss: 0.5994 - val_acc: 0.7500\n",
            "Epoch 264/300\n",
            "\n",
            "Epoch 00264: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.0440 - acc: 1.0000 - val_loss: 0.5788 - val_acc: 0.7500\n",
            "Epoch 265/300\n",
            "\n",
            "Epoch 00265: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.1686 - acc: 0.9333 - val_loss: 0.5366 - val_acc: 0.8000\n",
            "Epoch 266/300\n",
            "\n",
            "Epoch 00266: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.0528 - acc: 0.9833 - val_loss: 0.5498 - val_acc: 0.8000\n",
            "Epoch 267/300\n",
            "\n",
            "Epoch 00267: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.0585 - acc: 0.9833 - val_loss: 0.5850 - val_acc: 0.8000\n",
            "Epoch 268/300\n",
            "\n",
            "Epoch 00268: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.0889 - acc: 0.9500 - val_loss: 0.6100 - val_acc: 0.8000\n",
            "Epoch 269/300\n",
            "\n",
            "Epoch 00269: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.0770 - acc: 0.9833 - val_loss: 0.6555 - val_acc: 0.8000\n",
            "Epoch 270/300\n",
            "\n",
            "Epoch 00270: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.0922 - acc: 0.9667 - val_loss: 0.6324 - val_acc: 0.8000\n",
            "Epoch 271/300\n",
            "\n",
            "Epoch 00271: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.0505 - acc: 1.0000 - val_loss: 0.5749 - val_acc: 0.8000\n",
            "Epoch 272/300\n",
            "\n",
            "Epoch 00272: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.0635 - acc: 0.9833 - val_loss: 0.5272 - val_acc: 0.8000\n",
            "Epoch 273/300\n",
            "\n",
            "Epoch 00273: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.0396 - acc: 1.0000 - val_loss: 0.5105 - val_acc: 0.8000\n",
            "Epoch 274/300\n",
            "\n",
            "Epoch 00274: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.0484 - acc: 1.0000 - val_loss: 0.4989 - val_acc: 0.8000\n",
            "Epoch 275/300\n",
            "\n",
            "Epoch 00275: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.0564 - acc: 1.0000 - val_loss: 0.4933 - val_acc: 0.8000\n",
            "Epoch 276/300\n",
            "\n",
            "Epoch 00276: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.0900 - acc: 0.9500 - val_loss: 0.5140 - val_acc: 0.8000\n",
            "Epoch 277/300\n",
            "\n",
            "Epoch 00277: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.0867 - acc: 1.0000 - val_loss: 0.5001 - val_acc: 0.8500\n",
            "Epoch 278/300\n",
            "\n",
            "Epoch 00278: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.0444 - acc: 1.0000 - val_loss: 0.4971 - val_acc: 0.8500\n",
            "Epoch 279/300\n",
            "\n",
            "Epoch 00279: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.1035 - acc: 0.9500 - val_loss: 0.5030 - val_acc: 0.8500\n",
            "Epoch 280/300\n",
            "\n",
            "Epoch 00280: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.0402 - acc: 1.0000 - val_loss: 0.5163 - val_acc: 0.8000\n",
            "Epoch 281/300\n",
            "\n",
            "Epoch 00281: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.0299 - acc: 1.0000 - val_loss: 0.5407 - val_acc: 0.8000\n",
            "Epoch 282/300\n",
            "\n",
            "Epoch 00282: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.0648 - acc: 0.9833 - val_loss: 0.5683 - val_acc: 0.8000\n",
            "Epoch 283/300\n",
            "\n",
            "Epoch 00283: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.0926 - acc: 0.9833 - val_loss: 0.6327 - val_acc: 0.8000\n",
            "Epoch 284/300\n",
            "\n",
            "Epoch 00284: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.0965 - acc: 0.9667 - val_loss: 0.6331 - val_acc: 0.8000\n",
            "Epoch 285/300\n",
            "\n",
            "Epoch 00285: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.0736 - acc: 0.9833 - val_loss: 0.6274 - val_acc: 0.8000\n",
            "Epoch 286/300\n",
            "\n",
            "Epoch 00286: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.0586 - acc: 0.9833 - val_loss: 0.6441 - val_acc: 0.8000\n",
            "Epoch 287/300\n",
            "\n",
            "Epoch 00287: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.0720 - acc: 0.9833 - val_loss: 0.6591 - val_acc: 0.8000\n",
            "Epoch 288/300\n",
            "\n",
            "Epoch 00288: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.1099 - acc: 0.9833 - val_loss: 0.6647 - val_acc: 0.8000\n",
            "Epoch 289/300\n",
            "\n",
            "Epoch 00289: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.0477 - acc: 1.0000 - val_loss: 0.6611 - val_acc: 0.8000\n",
            "Epoch 290/300\n",
            "\n",
            "Epoch 00290: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.0595 - acc: 1.0000 - val_loss: 0.6291 - val_acc: 0.8000\n",
            "Epoch 291/300\n",
            "\n",
            "Epoch 00291: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.0576 - acc: 0.9833 - val_loss: 0.5964 - val_acc: 0.8000\n",
            "Epoch 292/300\n",
            "\n",
            "Epoch 00292: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.0342 - acc: 1.0000 - val_loss: 0.5589 - val_acc: 0.8000\n",
            "Epoch 293/300\n",
            "\n",
            "Epoch 00293: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.1044 - acc: 0.9667 - val_loss: 0.5215 - val_acc: 0.8000\n",
            "Epoch 294/300\n",
            "\n",
            "Epoch 00294: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.0994 - acc: 0.9667 - val_loss: 0.5236 - val_acc: 0.8000\n",
            "Epoch 295/300\n",
            "\n",
            "Epoch 00295: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.0809 - acc: 0.9833 - val_loss: 0.5275 - val_acc: 0.8000\n",
            "Epoch 296/300\n",
            "\n",
            "Epoch 00296: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.0420 - acc: 1.0000 - val_loss: 0.5198 - val_acc: 0.8000\n",
            "Epoch 297/300\n",
            "\n",
            "Epoch 00297: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.0777 - acc: 0.9833 - val_loss: 0.5181 - val_acc: 0.8000\n",
            "Epoch 298/300\n",
            "\n",
            "Epoch 00298: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.0823 - acc: 0.9667 - val_loss: 0.4827 - val_acc: 0.8000\n",
            "Epoch 299/300\n",
            "\n",
            "Epoch 00299: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.0343 - acc: 1.0000 - val_loss: 0.4656 - val_acc: 0.8000\n",
            "Epoch 300/300\n",
            "\n",
            "Epoch 00300: val_loss did not improve from 0.41120\n",
            "60/60 - 0s - loss: 0.0733 - acc: 0.9833 - val_loss: 0.4599 - val_acc: 0.7500\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "# Evaluate on test data\n",
            "40/40 [==============================] - 0s 2ms/sample - loss: 0.5335 - acc: 0.7250\n",
            "test loss, test acc: [0.5335107645136304, 0.725]\n",
            "EEG_Deep/Data2A/parsed_P05T.mat\n",
            "Filtering of Training Data Finished\n",
            "EEG_Deep/Data2A/parsed_P05E.mat\n",
            "Filtering of Testing Data Finished\n",
            "(1, 2)\n",
            "Finding labels in training data\n",
            "Finding labels in testing data\n",
            "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
            "(60, 12, 1536)\n",
            "(60,)\n",
            "(20, 12, 1536)\n",
            "(20,)\n",
            "[[1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]]\n",
            "X_train shape: (60, 1, 12, 1536)\n",
            "60 train samples\n",
            "20 val samples\n",
            "(40, 12, 1536)\n",
            "(40, 1)\n",
            "X_train shape: (40, 1, 12, 1536)\n",
            "40 train samples\n",
            "Train on 60 samples, validate on 20 samples\n",
            "Epoch 1/300\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 1.37608, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 1s - loss: 1.3220 - acc: 0.5667 - val_loss: 1.3761 - val_acc: 0.0500\n",
            "Epoch 2/300\n",
            "\n",
            "Epoch 00002: val_loss improved from 1.37608 to 1.37360, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 1.0348 - acc: 0.6833 - val_loss: 1.3736 - val_acc: 0.0000e+00\n",
            "Epoch 3/300\n",
            "\n",
            "Epoch 00003: val_loss improved from 1.37360 to 1.36896, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.9076 - acc: 0.6667 - val_loss: 1.3690 - val_acc: 0.0000e+00\n",
            "Epoch 4/300\n",
            "\n",
            "Epoch 00004: val_loss improved from 1.36896 to 1.35712, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.8027 - acc: 0.6667 - val_loss: 1.3571 - val_acc: 0.0000e+00\n",
            "Epoch 5/300\n",
            "\n",
            "Epoch 00005: val_loss improved from 1.35712 to 1.34713, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.7532 - acc: 0.6667 - val_loss: 1.3471 - val_acc: 0.0000e+00\n",
            "Epoch 6/300\n",
            "\n",
            "Epoch 00006: val_loss improved from 1.34713 to 1.33513, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.7203 - acc: 0.6667 - val_loss: 1.3351 - val_acc: 0.0000e+00\n",
            "Epoch 7/300\n",
            "\n",
            "Epoch 00007: val_loss improved from 1.33513 to 1.32677, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.7121 - acc: 0.6667 - val_loss: 1.3268 - val_acc: 0.0000e+00\n",
            "Epoch 8/300\n",
            "\n",
            "Epoch 00008: val_loss improved from 1.32677 to 1.31154, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.7030 - acc: 0.6667 - val_loss: 1.3115 - val_acc: 0.0000e+00\n",
            "Epoch 9/300\n",
            "\n",
            "Epoch 00009: val_loss improved from 1.31154 to 1.29971, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6812 - acc: 0.6667 - val_loss: 1.2997 - val_acc: 0.0000e+00\n",
            "Epoch 10/300\n",
            "\n",
            "Epoch 00010: val_loss improved from 1.29971 to 1.28795, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6525 - acc: 0.6667 - val_loss: 1.2879 - val_acc: 0.0000e+00\n",
            "Epoch 11/300\n",
            "\n",
            "Epoch 00011: val_loss improved from 1.28795 to 1.28038, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6503 - acc: 0.6667 - val_loss: 1.2804 - val_acc: 0.0000e+00\n",
            "Epoch 12/300\n",
            "\n",
            "Epoch 00012: val_loss improved from 1.28038 to 1.27288, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6350 - acc: 0.6667 - val_loss: 1.2729 - val_acc: 0.0000e+00\n",
            "Epoch 13/300\n",
            "\n",
            "Epoch 00013: val_loss improved from 1.27288 to 1.26815, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6199 - acc: 0.6667 - val_loss: 1.2682 - val_acc: 0.0000e+00\n",
            "Epoch 14/300\n",
            "\n",
            "Epoch 00014: val_loss improved from 1.26815 to 1.26370, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6151 - acc: 0.6833 - val_loss: 1.2637 - val_acc: 0.0000e+00\n",
            "Epoch 15/300\n",
            "\n",
            "Epoch 00015: val_loss improved from 1.26370 to 1.25752, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6220 - acc: 0.6667 - val_loss: 1.2575 - val_acc: 0.0000e+00\n",
            "Epoch 16/300\n",
            "\n",
            "Epoch 00016: val_loss improved from 1.25752 to 1.24838, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5947 - acc: 0.6833 - val_loss: 1.2484 - val_acc: 0.0000e+00\n",
            "Epoch 17/300\n",
            "\n",
            "Epoch 00017: val_loss improved from 1.24838 to 1.23876, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6052 - acc: 0.6833 - val_loss: 1.2388 - val_acc: 0.0000e+00\n",
            "Epoch 18/300\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 1.23876\n",
            "60/60 - 0s - loss: 0.5822 - acc: 0.7333 - val_loss: 1.2391 - val_acc: 0.0000e+00\n",
            "Epoch 19/300\n",
            "\n",
            "Epoch 00019: val_loss improved from 1.23876 to 1.23207, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5673 - acc: 0.6833 - val_loss: 1.2321 - val_acc: 0.0000e+00\n",
            "Epoch 20/300\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 1.23207\n",
            "60/60 - 0s - loss: 0.5695 - acc: 0.7167 - val_loss: 1.2332 - val_acc: 0.0000e+00\n",
            "Epoch 21/300\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 1.23207\n",
            "60/60 - 0s - loss: 0.5556 - acc: 0.7500 - val_loss: 1.2349 - val_acc: 0.0000e+00\n",
            "Epoch 22/300\n",
            "\n",
            "Epoch 00022: val_loss improved from 1.23207 to 1.23084, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5728 - acc: 0.7500 - val_loss: 1.2308 - val_acc: 0.0000e+00\n",
            "Epoch 23/300\n",
            "\n",
            "Epoch 00023: val_loss improved from 1.23084 to 1.22358, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5590 - acc: 0.7500 - val_loss: 1.2236 - val_acc: 0.0000e+00\n",
            "Epoch 24/300\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 1.22358\n",
            "60/60 - 0s - loss: 0.5260 - acc: 0.7667 - val_loss: 1.2264 - val_acc: 0.0000e+00\n",
            "Epoch 25/300\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 1.22358\n",
            "60/60 - 0s - loss: 0.5282 - acc: 0.8000 - val_loss: 1.2265 - val_acc: 0.0000e+00\n",
            "Epoch 26/300\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 1.22358\n",
            "60/60 - 0s - loss: 0.5184 - acc: 0.8333 - val_loss: 1.2356 - val_acc: 0.0000e+00\n",
            "Epoch 27/300\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 1.22358\n",
            "60/60 - 0s - loss: 0.4898 - acc: 0.8333 - val_loss: 1.2496 - val_acc: 0.0000e+00\n",
            "Epoch 28/300\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 1.22358\n",
            "60/60 - 0s - loss: 0.5048 - acc: 0.8333 - val_loss: 1.2598 - val_acc: 0.0000e+00\n",
            "Epoch 29/300\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 1.22358\n",
            "60/60 - 0s - loss: 0.5223 - acc: 0.7500 - val_loss: 1.2448 - val_acc: 0.1000\n",
            "Epoch 30/300\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 1.22358\n",
            "60/60 - 0s - loss: 0.5034 - acc: 0.8333 - val_loss: 1.2371 - val_acc: 0.1500\n",
            "Epoch 31/300\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 1.22358\n",
            "60/60 - 0s - loss: 0.4894 - acc: 0.8333 - val_loss: 1.2270 - val_acc: 0.2000\n",
            "Epoch 32/300\n",
            "\n",
            "Epoch 00032: val_loss improved from 1.22358 to 1.21506, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4992 - acc: 0.8167 - val_loss: 1.2151 - val_acc: 0.2000\n",
            "Epoch 33/300\n",
            "\n",
            "Epoch 00033: val_loss improved from 1.21506 to 1.21286, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4799 - acc: 0.8333 - val_loss: 1.2129 - val_acc: 0.2000\n",
            "Epoch 34/300\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 1.21286\n",
            "60/60 - 0s - loss: 0.4626 - acc: 0.8333 - val_loss: 1.2218 - val_acc: 0.2000\n",
            "Epoch 35/300\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 1.21286\n",
            "60/60 - 0s - loss: 0.4846 - acc: 0.8667 - val_loss: 1.2176 - val_acc: 0.2000\n",
            "Epoch 36/300\n",
            "\n",
            "Epoch 00036: val_loss improved from 1.21286 to 1.19171, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4603 - acc: 0.8500 - val_loss: 1.1917 - val_acc: 0.2000\n",
            "Epoch 37/300\n",
            "\n",
            "Epoch 00037: val_loss improved from 1.19171 to 1.16774, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4782 - acc: 0.8667 - val_loss: 1.1677 - val_acc: 0.2500\n",
            "Epoch 38/300\n",
            "\n",
            "Epoch 00038: val_loss improved from 1.16774 to 1.16649, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4518 - acc: 0.8667 - val_loss: 1.1665 - val_acc: 0.2500\n",
            "Epoch 39/300\n",
            "\n",
            "Epoch 00039: val_loss improved from 1.16649 to 1.14629, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4543 - acc: 0.8500 - val_loss: 1.1463 - val_acc: 0.3500\n",
            "Epoch 40/300\n",
            "\n",
            "Epoch 00040: val_loss improved from 1.14629 to 1.12074, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4216 - acc: 0.9167 - val_loss: 1.1207 - val_acc: 0.4500\n",
            "Epoch 41/300\n",
            "\n",
            "Epoch 00041: val_loss improved from 1.12074 to 1.10983, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4294 - acc: 0.9167 - val_loss: 1.1098 - val_acc: 0.4500\n",
            "Epoch 42/300\n",
            "\n",
            "Epoch 00042: val_loss improved from 1.10983 to 1.09943, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4394 - acc: 0.8833 - val_loss: 1.0994 - val_acc: 0.4500\n",
            "Epoch 43/300\n",
            "\n",
            "Epoch 00043: val_loss improved from 1.09943 to 1.08910, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4056 - acc: 0.9000 - val_loss: 1.0891 - val_acc: 0.4500\n",
            "Epoch 44/300\n",
            "\n",
            "Epoch 00044: val_loss improved from 1.08910 to 1.08151, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4120 - acc: 0.9000 - val_loss: 1.0815 - val_acc: 0.5000\n",
            "Epoch 45/300\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 1.08151\n",
            "60/60 - 0s - loss: 0.3936 - acc: 0.9167 - val_loss: 1.1009 - val_acc: 0.5000\n",
            "Epoch 46/300\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 1.08151\n",
            "60/60 - 0s - loss: 0.3704 - acc: 0.9500 - val_loss: 1.1071 - val_acc: 0.5000\n",
            "Epoch 47/300\n",
            "\n",
            "Epoch 00047: val_loss improved from 1.08151 to 1.07997, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4144 - acc: 0.9167 - val_loss: 1.0800 - val_acc: 0.5000\n",
            "Epoch 48/300\n",
            "\n",
            "Epoch 00048: val_loss improved from 1.07997 to 1.07780, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3770 - acc: 0.9000 - val_loss: 1.0778 - val_acc: 0.5000\n",
            "Epoch 49/300\n",
            "\n",
            "Epoch 00049: val_loss improved from 1.07780 to 1.06195, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3768 - acc: 0.9167 - val_loss: 1.0620 - val_acc: 0.5000\n",
            "Epoch 50/300\n",
            "\n",
            "Epoch 00050: val_loss improved from 1.06195 to 1.03921, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3845 - acc: 0.9000 - val_loss: 1.0392 - val_acc: 0.5500\n",
            "Epoch 51/300\n",
            "\n",
            "Epoch 00051: val_loss improved from 1.03921 to 1.02961, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3876 - acc: 0.9000 - val_loss: 1.0296 - val_acc: 0.5500\n",
            "Epoch 52/300\n",
            "\n",
            "Epoch 00052: val_loss improved from 1.02961 to 0.99859, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3500 - acc: 0.9000 - val_loss: 0.9986 - val_acc: 0.6000\n",
            "Epoch 53/300\n",
            "\n",
            "Epoch 00053: val_loss improved from 0.99859 to 0.97310, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3813 - acc: 0.9000 - val_loss: 0.9731 - val_acc: 0.6500\n",
            "Epoch 54/300\n",
            "\n",
            "Epoch 00054: val_loss improved from 0.97310 to 0.93793, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3342 - acc: 0.9333 - val_loss: 0.9379 - val_acc: 0.7000\n",
            "Epoch 55/300\n",
            "\n",
            "Epoch 00055: val_loss improved from 0.93793 to 0.92114, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3878 - acc: 0.9333 - val_loss: 0.9211 - val_acc: 0.7000\n",
            "Epoch 56/300\n",
            "\n",
            "Epoch 00056: val_loss improved from 0.92114 to 0.88211, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3282 - acc: 0.9500 - val_loss: 0.8821 - val_acc: 0.7000\n",
            "Epoch 57/300\n",
            "\n",
            "Epoch 00057: val_loss improved from 0.88211 to 0.85003, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3665 - acc: 0.9167 - val_loss: 0.8500 - val_acc: 0.7000\n",
            "Epoch 58/300\n",
            "\n",
            "Epoch 00058: val_loss improved from 0.85003 to 0.82027, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3438 - acc: 0.9333 - val_loss: 0.8203 - val_acc: 0.7000\n",
            "Epoch 59/300\n",
            "\n",
            "Epoch 00059: val_loss improved from 0.82027 to 0.80652, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3742 - acc: 0.8833 - val_loss: 0.8065 - val_acc: 0.7000\n",
            "Epoch 60/300\n",
            "\n",
            "Epoch 00060: val_loss improved from 0.80652 to 0.78619, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3568 - acc: 0.9333 - val_loss: 0.7862 - val_acc: 0.7000\n",
            "Epoch 61/300\n",
            "\n",
            "Epoch 00061: val_loss improved from 0.78619 to 0.77417, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3957 - acc: 0.8833 - val_loss: 0.7742 - val_acc: 0.7000\n",
            "Epoch 62/300\n",
            "\n",
            "Epoch 00062: val_loss improved from 0.77417 to 0.76873, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3185 - acc: 0.9333 - val_loss: 0.7687 - val_acc: 0.7500\n",
            "Epoch 63/300\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.76873\n",
            "60/60 - 0s - loss: 0.3225 - acc: 0.9500 - val_loss: 0.7733 - val_acc: 0.7500\n",
            "Epoch 64/300\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.76873\n",
            "60/60 - 0s - loss: 0.3669 - acc: 0.9000 - val_loss: 0.7800 - val_acc: 0.7000\n",
            "Epoch 65/300\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.76873\n",
            "60/60 - 0s - loss: 0.3295 - acc: 0.9500 - val_loss: 0.7830 - val_acc: 0.7000\n",
            "Epoch 66/300\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.76873\n",
            "60/60 - 0s - loss: 0.3204 - acc: 0.9333 - val_loss: 0.7906 - val_acc: 0.7000\n",
            "Epoch 67/300\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.76873\n",
            "60/60 - 0s - loss: 0.2941 - acc: 0.9333 - val_loss: 0.7814 - val_acc: 0.7000\n",
            "Epoch 68/300\n",
            "\n",
            "Epoch 00068: val_loss improved from 0.76873 to 0.76424, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3287 - acc: 0.9167 - val_loss: 0.7642 - val_acc: 0.7000\n",
            "Epoch 69/300\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.76424\n",
            "60/60 - 0s - loss: 0.3269 - acc: 0.9167 - val_loss: 0.7857 - val_acc: 0.7000\n",
            "Epoch 70/300\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.76424\n",
            "60/60 - 0s - loss: 0.3256 - acc: 0.9500 - val_loss: 0.7658 - val_acc: 0.7000\n",
            "Epoch 71/300\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.76424\n",
            "60/60 - 0s - loss: 0.3136 - acc: 0.9667 - val_loss: 0.7696 - val_acc: 0.7000\n",
            "Epoch 72/300\n",
            "\n",
            "Epoch 00072: val_loss improved from 0.76424 to 0.74391, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3511 - acc: 0.9000 - val_loss: 0.7439 - val_acc: 0.7000\n",
            "Epoch 73/300\n",
            "\n",
            "Epoch 00073: val_loss improved from 0.74391 to 0.73594, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3121 - acc: 0.9667 - val_loss: 0.7359 - val_acc: 0.7000\n",
            "Epoch 74/300\n",
            "\n",
            "Epoch 00074: val_loss improved from 0.73594 to 0.73412, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3136 - acc: 0.9167 - val_loss: 0.7341 - val_acc: 0.7000\n",
            "Epoch 75/300\n",
            "\n",
            "Epoch 00075: val_loss improved from 0.73412 to 0.69954, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3043 - acc: 0.9167 - val_loss: 0.6995 - val_acc: 0.7500\n",
            "Epoch 76/300\n",
            "\n",
            "Epoch 00076: val_loss improved from 0.69954 to 0.67930, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3195 - acc: 0.9000 - val_loss: 0.6793 - val_acc: 0.8000\n",
            "Epoch 77/300\n",
            "\n",
            "Epoch 00077: val_loss improved from 0.67930 to 0.67927, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3161 - acc: 0.9333 - val_loss: 0.6793 - val_acc: 0.7500\n",
            "Epoch 78/300\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.67927\n",
            "60/60 - 0s - loss: 0.2838 - acc: 0.9500 - val_loss: 0.6947 - val_acc: 0.7000\n",
            "Epoch 79/300\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.67927\n",
            "60/60 - 0s - loss: 0.2765 - acc: 0.9333 - val_loss: 0.7236 - val_acc: 0.7000\n",
            "Epoch 80/300\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.67927\n",
            "60/60 - 0s - loss: 0.2590 - acc: 0.9500 - val_loss: 0.7350 - val_acc: 0.7000\n",
            "Epoch 81/300\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.67927\n",
            "60/60 - 0s - loss: 0.2984 - acc: 0.9333 - val_loss: 0.6948 - val_acc: 0.7500\n",
            "Epoch 82/300\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.67927\n",
            "60/60 - 0s - loss: 0.2960 - acc: 0.9333 - val_loss: 0.6823 - val_acc: 0.7500\n",
            "Epoch 83/300\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.67927\n",
            "60/60 - 0s - loss: 0.2872 - acc: 0.9333 - val_loss: 0.6929 - val_acc: 0.7500\n",
            "Epoch 84/300\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.67927\n",
            "60/60 - 0s - loss: 0.2681 - acc: 0.9500 - val_loss: 0.6865 - val_acc: 0.7500\n",
            "Epoch 85/300\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.67927\n",
            "60/60 - 0s - loss: 0.2664 - acc: 0.9333 - val_loss: 0.6850 - val_acc: 0.7500\n",
            "Epoch 86/300\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.67927\n",
            "60/60 - 0s - loss: 0.2650 - acc: 0.9500 - val_loss: 0.7052 - val_acc: 0.7500\n",
            "Epoch 87/300\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.67927\n",
            "60/60 - 0s - loss: 0.2666 - acc: 0.9500 - val_loss: 0.7028 - val_acc: 0.7500\n",
            "Epoch 88/300\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.67927\n",
            "60/60 - 0s - loss: 0.2600 - acc: 0.9333 - val_loss: 0.6825 - val_acc: 0.8000\n",
            "Epoch 89/300\n",
            "\n",
            "Epoch 00089: val_loss improved from 0.67927 to 0.65374, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2546 - acc: 0.9500 - val_loss: 0.6537 - val_acc: 0.8000\n",
            "Epoch 90/300\n",
            "\n",
            "Epoch 00090: val_loss improved from 0.65374 to 0.60381, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2663 - acc: 0.9500 - val_loss: 0.6038 - val_acc: 0.8000\n",
            "Epoch 91/300\n",
            "\n",
            "Epoch 00091: val_loss improved from 0.60381 to 0.56324, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2585 - acc: 0.9500 - val_loss: 0.5632 - val_acc: 0.8000\n",
            "Epoch 92/300\n",
            "\n",
            "Epoch 00092: val_loss improved from 0.56324 to 0.55359, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2370 - acc: 0.9833 - val_loss: 0.5536 - val_acc: 0.8000\n",
            "Epoch 93/300\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.2606 - acc: 0.9667 - val_loss: 0.5590 - val_acc: 0.8000\n",
            "Epoch 94/300\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.2617 - acc: 0.9333 - val_loss: 0.5958 - val_acc: 0.8000\n",
            "Epoch 95/300\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.2846 - acc: 0.9167 - val_loss: 0.5566 - val_acc: 0.8000\n",
            "Epoch 96/300\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.2407 - acc: 0.9667 - val_loss: 0.5599 - val_acc: 0.8000\n",
            "Epoch 97/300\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.3181 - acc: 0.9167 - val_loss: 0.6284 - val_acc: 0.8000\n",
            "Epoch 98/300\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.2373 - acc: 0.9833 - val_loss: 0.6642 - val_acc: 0.7500\n",
            "Epoch 99/300\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.2403 - acc: 0.9333 - val_loss: 0.6885 - val_acc: 0.7500\n",
            "Epoch 100/300\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.2527 - acc: 0.9333 - val_loss: 0.6643 - val_acc: 0.7500\n",
            "Epoch 101/300\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.2307 - acc: 0.9833 - val_loss: 0.6713 - val_acc: 0.7500\n",
            "Epoch 102/300\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.2070 - acc: 0.9833 - val_loss: 0.7024 - val_acc: 0.7500\n",
            "Epoch 103/300\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.2288 - acc: 0.9667 - val_loss: 0.7686 - val_acc: 0.7500\n",
            "Epoch 104/300\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.2880 - acc: 0.9333 - val_loss: 0.8136 - val_acc: 0.7000\n",
            "Epoch 105/300\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.2256 - acc: 0.9833 - val_loss: 0.8761 - val_acc: 0.7000\n",
            "Epoch 106/300\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.2294 - acc: 0.9667 - val_loss: 0.9135 - val_acc: 0.7000\n",
            "Epoch 107/300\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.2406 - acc: 0.9000 - val_loss: 0.9474 - val_acc: 0.7000\n",
            "Epoch 108/300\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.2174 - acc: 0.9833 - val_loss: 0.9215 - val_acc: 0.7000\n",
            "Epoch 109/300\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.2034 - acc: 0.9833 - val_loss: 0.8794 - val_acc: 0.7000\n",
            "Epoch 110/300\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.1942 - acc: 0.9833 - val_loss: 0.8549 - val_acc: 0.7000\n",
            "Epoch 111/300\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.2244 - acc: 0.9667 - val_loss: 0.8298 - val_acc: 0.7000\n",
            "Epoch 112/300\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.2196 - acc: 0.9667 - val_loss: 0.8462 - val_acc: 0.7000\n",
            "Epoch 113/300\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.2259 - acc: 0.9333 - val_loss: 0.8560 - val_acc: 0.7000\n",
            "Epoch 114/300\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.1739 - acc: 0.9833 - val_loss: 0.8822 - val_acc: 0.7000\n",
            "Epoch 115/300\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.1828 - acc: 0.9833 - val_loss: 0.9185 - val_acc: 0.7000\n",
            "Epoch 116/300\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.2022 - acc: 0.9667 - val_loss: 0.9486 - val_acc: 0.7000\n",
            "Epoch 117/300\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.2196 - acc: 0.9500 - val_loss: 0.9769 - val_acc: 0.7000\n",
            "Epoch 118/300\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.2113 - acc: 0.9500 - val_loss: 1.0059 - val_acc: 0.7000\n",
            "Epoch 119/300\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.1822 - acc: 0.9667 - val_loss: 1.0150 - val_acc: 0.7000\n",
            "Epoch 120/300\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.2628 - acc: 0.9167 - val_loss: 0.9587 - val_acc: 0.7000\n",
            "Epoch 121/300\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.2044 - acc: 0.9667 - val_loss: 0.9515 - val_acc: 0.7000\n",
            "Epoch 122/300\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.2677 - acc: 0.9167 - val_loss: 0.9173 - val_acc: 0.7000\n",
            "Epoch 123/300\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.1994 - acc: 0.9833 - val_loss: 0.8881 - val_acc: 0.7000\n",
            "Epoch 124/300\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.1681 - acc: 0.9667 - val_loss: 0.8911 - val_acc: 0.7500\n",
            "Epoch 125/300\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.2073 - acc: 0.9667 - val_loss: 0.8637 - val_acc: 0.7500\n",
            "Epoch 126/300\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.1956 - acc: 0.9667 - val_loss: 0.8903 - val_acc: 0.7500\n",
            "Epoch 127/300\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.1789 - acc: 0.9500 - val_loss: 0.9127 - val_acc: 0.7000\n",
            "Epoch 128/300\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.2324 - acc: 0.9167 - val_loss: 0.9680 - val_acc: 0.7000\n",
            "Epoch 129/300\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.2016 - acc: 1.0000 - val_loss: 1.0805 - val_acc: 0.7000\n",
            "Epoch 130/300\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.1926 - acc: 0.9833 - val_loss: 1.1820 - val_acc: 0.7000\n",
            "Epoch 131/300\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.1795 - acc: 0.9667 - val_loss: 1.2503 - val_acc: 0.7000\n",
            "Epoch 132/300\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.2003 - acc: 0.9833 - val_loss: 1.2722 - val_acc: 0.7000\n",
            "Epoch 133/300\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.1772 - acc: 0.9500 - val_loss: 1.3719 - val_acc: 0.7000\n",
            "Epoch 134/300\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.1972 - acc: 0.9833 - val_loss: 1.4678 - val_acc: 0.6000\n",
            "Epoch 135/300\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.1706 - acc: 0.9833 - val_loss: 1.4638 - val_acc: 0.6000\n",
            "Epoch 136/300\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.2028 - acc: 0.9667 - val_loss: 1.3689 - val_acc: 0.7000\n",
            "Epoch 137/300\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.2112 - acc: 0.9500 - val_loss: 1.2722 - val_acc: 0.7000\n",
            "Epoch 138/300\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.1726 - acc: 0.9833 - val_loss: 1.1995 - val_acc: 0.7000\n",
            "Epoch 139/300\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.1648 - acc: 0.9833 - val_loss: 1.1268 - val_acc: 0.7000\n",
            "Epoch 140/300\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.1835 - acc: 0.9667 - val_loss: 1.0855 - val_acc: 0.7000\n",
            "Epoch 141/300\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.1920 - acc: 0.9667 - val_loss: 1.1315 - val_acc: 0.7000\n",
            "Epoch 142/300\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.1955 - acc: 0.9833 - val_loss: 1.2041 - val_acc: 0.7000\n",
            "Epoch 143/300\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.1833 - acc: 0.9500 - val_loss: 1.2726 - val_acc: 0.7000\n",
            "Epoch 144/300\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.1949 - acc: 0.9667 - val_loss: 1.2112 - val_acc: 0.7000\n",
            "Epoch 145/300\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.1812 - acc: 0.9667 - val_loss: 1.1144 - val_acc: 0.7000\n",
            "Epoch 146/300\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.1740 - acc: 0.9500 - val_loss: 1.0013 - val_acc: 0.7000\n",
            "Epoch 147/300\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.1551 - acc: 1.0000 - val_loss: 0.9536 - val_acc: 0.7000\n",
            "Epoch 148/300\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.2061 - acc: 0.9667 - val_loss: 0.9242 - val_acc: 0.7000\n",
            "Epoch 149/300\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.2033 - acc: 0.9333 - val_loss: 0.9096 - val_acc: 0.7000\n",
            "Epoch 150/300\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.1603 - acc: 0.9667 - val_loss: 1.0029 - val_acc: 0.7000\n",
            "Epoch 151/300\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.2052 - acc: 0.9667 - val_loss: 1.0255 - val_acc: 0.7000\n",
            "Epoch 152/300\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.1800 - acc: 0.9667 - val_loss: 1.0676 - val_acc: 0.7000\n",
            "Epoch 153/300\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.2097 - acc: 0.9500 - val_loss: 1.1067 - val_acc: 0.7000\n",
            "Epoch 154/300\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.1704 - acc: 0.9500 - val_loss: 1.1409 - val_acc: 0.7000\n",
            "Epoch 155/300\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.1848 - acc: 0.9500 - val_loss: 1.2327 - val_acc: 0.7000\n",
            "Epoch 156/300\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.1865 - acc: 0.9667 - val_loss: 1.3513 - val_acc: 0.7000\n",
            "Epoch 157/300\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.1861 - acc: 0.9667 - val_loss: 1.3991 - val_acc: 0.6500\n",
            "Epoch 158/300\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.1798 - acc: 0.9333 - val_loss: 1.3832 - val_acc: 0.7000\n",
            "Epoch 159/300\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.1656 - acc: 0.9833 - val_loss: 1.3528 - val_acc: 0.7000\n",
            "Epoch 160/300\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.1862 - acc: 0.9667 - val_loss: 1.3759 - val_acc: 0.7000\n",
            "Epoch 161/300\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.1656 - acc: 0.9500 - val_loss: 1.3621 - val_acc: 0.7000\n",
            "Epoch 162/300\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.1670 - acc: 0.9500 - val_loss: 1.3310 - val_acc: 0.7000\n",
            "Epoch 163/300\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.1602 - acc: 0.9500 - val_loss: 1.3402 - val_acc: 0.7000\n",
            "Epoch 164/300\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.1356 - acc: 1.0000 - val_loss: 1.3468 - val_acc: 0.7000\n",
            "Epoch 165/300\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.2087 - acc: 0.9833 - val_loss: 1.2571 - val_acc: 0.7000\n",
            "Epoch 166/300\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.1708 - acc: 0.9333 - val_loss: 1.0987 - val_acc: 0.7000\n",
            "Epoch 167/300\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.1886 - acc: 0.9500 - val_loss: 0.9504 - val_acc: 0.7500\n",
            "Epoch 168/300\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.2029 - acc: 0.9667 - val_loss: 0.9293 - val_acc: 0.7000\n",
            "Epoch 169/300\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.1598 - acc: 0.9833 - val_loss: 0.9570 - val_acc: 0.7000\n",
            "Epoch 170/300\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.1868 - acc: 0.9333 - val_loss: 1.1697 - val_acc: 0.7000\n",
            "Epoch 171/300\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.1581 - acc: 0.9833 - val_loss: 1.3625 - val_acc: 0.7000\n",
            "Epoch 172/300\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.1879 - acc: 0.9500 - val_loss: 1.4836 - val_acc: 0.6500\n",
            "Epoch 173/300\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.1576 - acc: 1.0000 - val_loss: 1.5928 - val_acc: 0.6000\n",
            "Epoch 174/300\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.1858 - acc: 0.9667 - val_loss: 1.5458 - val_acc: 0.6000\n",
            "Epoch 175/300\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.1585 - acc: 1.0000 - val_loss: 1.5370 - val_acc: 0.6000\n",
            "Epoch 176/300\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.2174 - acc: 0.9000 - val_loss: 1.4258 - val_acc: 0.6000\n",
            "Epoch 177/300\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.1949 - acc: 0.9333 - val_loss: 1.3348 - val_acc: 0.6500\n",
            "Epoch 178/300\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.1482 - acc: 0.9833 - val_loss: 1.3954 - val_acc: 0.6500\n",
            "Epoch 179/300\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.1155 - acc: 0.9833 - val_loss: 1.4343 - val_acc: 0.6500\n",
            "Epoch 180/300\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.1214 - acc: 1.0000 - val_loss: 1.5232 - val_acc: 0.6500\n",
            "Epoch 181/300\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.1493 - acc: 0.9833 - val_loss: 1.6499 - val_acc: 0.6000\n",
            "Epoch 182/300\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.1632 - acc: 0.9667 - val_loss: 1.6414 - val_acc: 0.6000\n",
            "Epoch 183/300\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.1345 - acc: 0.9667 - val_loss: 1.5752 - val_acc: 0.6000\n",
            "Epoch 184/300\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.1343 - acc: 1.0000 - val_loss: 1.4838 - val_acc: 0.6500\n",
            "Epoch 185/300\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.1746 - acc: 0.9667 - val_loss: 1.4420 - val_acc: 0.6500\n",
            "Epoch 186/300\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.1278 - acc: 1.0000 - val_loss: 1.5470 - val_acc: 0.6500\n",
            "Epoch 187/300\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.1989 - acc: 0.9333 - val_loss: 1.6232 - val_acc: 0.6500\n",
            "Epoch 188/300\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.1103 - acc: 1.0000 - val_loss: 1.7033 - val_acc: 0.6500\n",
            "Epoch 189/300\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.1460 - acc: 0.9667 - val_loss: 1.7115 - val_acc: 0.6500\n",
            "Epoch 190/300\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.1673 - acc: 0.9667 - val_loss: 1.6088 - val_acc: 0.6500\n",
            "Epoch 191/300\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.1197 - acc: 1.0000 - val_loss: 1.5775 - val_acc: 0.6500\n",
            "Epoch 192/300\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.1140 - acc: 1.0000 - val_loss: 1.5628 - val_acc: 0.6000\n",
            "Epoch 193/300\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.1584 - acc: 0.9333 - val_loss: 1.5527 - val_acc: 0.6000\n",
            "Epoch 194/300\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.1428 - acc: 0.9833 - val_loss: 1.6103 - val_acc: 0.6000\n",
            "Epoch 195/300\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.1704 - acc: 0.9833 - val_loss: 1.6132 - val_acc: 0.6000\n",
            "Epoch 196/300\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.1967 - acc: 0.9333 - val_loss: 1.6655 - val_acc: 0.6000\n",
            "Epoch 197/300\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.1197 - acc: 1.0000 - val_loss: 1.5755 - val_acc: 0.6000\n",
            "Epoch 198/300\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.1921 - acc: 0.9333 - val_loss: 1.5983 - val_acc: 0.6000\n",
            "Epoch 199/300\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.1272 - acc: 0.9833 - val_loss: 1.6279 - val_acc: 0.6000\n",
            "Epoch 200/300\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.1321 - acc: 0.9833 - val_loss: 1.6180 - val_acc: 0.6000\n",
            "Epoch 201/300\n",
            "\n",
            "Epoch 00201: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.1883 - acc: 0.9333 - val_loss: 1.5707 - val_acc: 0.6000\n",
            "Epoch 202/300\n",
            "\n",
            "Epoch 00202: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.1337 - acc: 1.0000 - val_loss: 1.5832 - val_acc: 0.6000\n",
            "Epoch 203/300\n",
            "\n",
            "Epoch 00203: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.0948 - acc: 0.9833 - val_loss: 1.6536 - val_acc: 0.6000\n",
            "Epoch 204/300\n",
            "\n",
            "Epoch 00204: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.1437 - acc: 0.9833 - val_loss: 1.6045 - val_acc: 0.6000\n",
            "Epoch 205/300\n",
            "\n",
            "Epoch 00205: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.1320 - acc: 0.9667 - val_loss: 1.5794 - val_acc: 0.6000\n",
            "Epoch 206/300\n",
            "\n",
            "Epoch 00206: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.1412 - acc: 0.9500 - val_loss: 1.6077 - val_acc: 0.6000\n",
            "Epoch 207/300\n",
            "\n",
            "Epoch 00207: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.1140 - acc: 0.9667 - val_loss: 1.5893 - val_acc: 0.6000\n",
            "Epoch 208/300\n",
            "\n",
            "Epoch 00208: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.1286 - acc: 0.9833 - val_loss: 1.5878 - val_acc: 0.6000\n",
            "Epoch 209/300\n",
            "\n",
            "Epoch 00209: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.1278 - acc: 1.0000 - val_loss: 1.5791 - val_acc: 0.6000\n",
            "Epoch 210/300\n",
            "\n",
            "Epoch 00210: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.1489 - acc: 1.0000 - val_loss: 1.6046 - val_acc: 0.6000\n",
            "Epoch 211/300\n",
            "\n",
            "Epoch 00211: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.1357 - acc: 0.9667 - val_loss: 1.6833 - val_acc: 0.6000\n",
            "Epoch 212/300\n",
            "\n",
            "Epoch 00212: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.2058 - acc: 0.9167 - val_loss: 1.9572 - val_acc: 0.4000\n",
            "Epoch 213/300\n",
            "\n",
            "Epoch 00213: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.1405 - acc: 1.0000 - val_loss: 2.1810 - val_acc: 0.4000\n",
            "Epoch 214/300\n",
            "\n",
            "Epoch 00214: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.1454 - acc: 0.9667 - val_loss: 2.2837 - val_acc: 0.4000\n",
            "Epoch 215/300\n",
            "\n",
            "Epoch 00215: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.1093 - acc: 0.9833 - val_loss: 2.5178 - val_acc: 0.3500\n",
            "Epoch 216/300\n",
            "\n",
            "Epoch 00216: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.1015 - acc: 0.9833 - val_loss: 2.6033 - val_acc: 0.3500\n",
            "Epoch 217/300\n",
            "\n",
            "Epoch 00217: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.1621 - acc: 0.9833 - val_loss: 2.4107 - val_acc: 0.3500\n",
            "Epoch 218/300\n",
            "\n",
            "Epoch 00218: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.1261 - acc: 1.0000 - val_loss: 2.2329 - val_acc: 0.4000\n",
            "Epoch 219/300\n",
            "\n",
            "Epoch 00219: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.1247 - acc: 1.0000 - val_loss: 2.0941 - val_acc: 0.4500\n",
            "Epoch 220/300\n",
            "\n",
            "Epoch 00220: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.1382 - acc: 0.9833 - val_loss: 1.9804 - val_acc: 0.4500\n",
            "Epoch 221/300\n",
            "\n",
            "Epoch 00221: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.1124 - acc: 1.0000 - val_loss: 1.9601 - val_acc: 0.4500\n",
            "Epoch 222/300\n",
            "\n",
            "Epoch 00222: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.1321 - acc: 0.9833 - val_loss: 2.0283 - val_acc: 0.4500\n",
            "Epoch 223/300\n",
            "\n",
            "Epoch 00223: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.1515 - acc: 0.9500 - val_loss: 2.0248 - val_acc: 0.4500\n",
            "Epoch 224/300\n",
            "\n",
            "Epoch 00224: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.1185 - acc: 0.9833 - val_loss: 2.0858 - val_acc: 0.4500\n",
            "Epoch 225/300\n",
            "\n",
            "Epoch 00225: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.1027 - acc: 1.0000 - val_loss: 2.3506 - val_acc: 0.4000\n",
            "Epoch 226/300\n",
            "\n",
            "Epoch 00226: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.1204 - acc: 0.9833 - val_loss: 2.6408 - val_acc: 0.3500\n",
            "Epoch 227/300\n",
            "\n",
            "Epoch 00227: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.1634 - acc: 0.9333 - val_loss: 2.6747 - val_acc: 0.3500\n",
            "Epoch 228/300\n",
            "\n",
            "Epoch 00228: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.1414 - acc: 0.9833 - val_loss: 2.5666 - val_acc: 0.3500\n",
            "Epoch 229/300\n",
            "\n",
            "Epoch 00229: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.1307 - acc: 0.9833 - val_loss: 2.3410 - val_acc: 0.4500\n",
            "Epoch 230/300\n",
            "\n",
            "Epoch 00230: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.1122 - acc: 0.9833 - val_loss: 2.3716 - val_acc: 0.4500\n",
            "Epoch 231/300\n",
            "\n",
            "Epoch 00231: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.1183 - acc: 0.9833 - val_loss: 2.4023 - val_acc: 0.4000\n",
            "Epoch 232/300\n",
            "\n",
            "Epoch 00232: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.1178 - acc: 1.0000 - val_loss: 2.3474 - val_acc: 0.4500\n",
            "Epoch 233/300\n",
            "\n",
            "Epoch 00233: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.1259 - acc: 0.9667 - val_loss: 2.3125 - val_acc: 0.4500\n",
            "Epoch 234/300\n",
            "\n",
            "Epoch 00234: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.1108 - acc: 0.9833 - val_loss: 2.3563 - val_acc: 0.4500\n",
            "Epoch 235/300\n",
            "\n",
            "Epoch 00235: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.1174 - acc: 1.0000 - val_loss: 2.3384 - val_acc: 0.4500\n",
            "Epoch 236/300\n",
            "\n",
            "Epoch 00236: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.1136 - acc: 0.9833 - val_loss: 2.3767 - val_acc: 0.4500\n",
            "Epoch 237/300\n",
            "\n",
            "Epoch 00237: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.1219 - acc: 0.9500 - val_loss: 2.2775 - val_acc: 0.4500\n",
            "Epoch 238/300\n",
            "\n",
            "Epoch 00238: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.1417 - acc: 0.9833 - val_loss: 2.3486 - val_acc: 0.4500\n",
            "Epoch 239/300\n",
            "\n",
            "Epoch 00239: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.1026 - acc: 0.9833 - val_loss: 2.4329 - val_acc: 0.4000\n",
            "Epoch 240/300\n",
            "\n",
            "Epoch 00240: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.1206 - acc: 0.9667 - val_loss: 2.6279 - val_acc: 0.4000\n",
            "Epoch 241/300\n",
            "\n",
            "Epoch 00241: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.1377 - acc: 1.0000 - val_loss: 2.8020 - val_acc: 0.4000\n",
            "Epoch 242/300\n",
            "\n",
            "Epoch 00242: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.1336 - acc: 0.9667 - val_loss: 2.9534 - val_acc: 0.3500\n",
            "Epoch 243/300\n",
            "\n",
            "Epoch 00243: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.0878 - acc: 1.0000 - val_loss: 2.9850 - val_acc: 0.3500\n",
            "Epoch 244/300\n",
            "\n",
            "Epoch 00244: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.1030 - acc: 0.9833 - val_loss: 2.8924 - val_acc: 0.3500\n",
            "Epoch 245/300\n",
            "\n",
            "Epoch 00245: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.1406 - acc: 0.9500 - val_loss: 2.7876 - val_acc: 0.3500\n",
            "Epoch 246/300\n",
            "\n",
            "Epoch 00246: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.1243 - acc: 0.9833 - val_loss: 2.7015 - val_acc: 0.4000\n",
            "Epoch 247/300\n",
            "\n",
            "Epoch 00247: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.1108 - acc: 0.9667 - val_loss: 2.5689 - val_acc: 0.4000\n",
            "Epoch 248/300\n",
            "\n",
            "Epoch 00248: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.1182 - acc: 0.9833 - val_loss: 2.4561 - val_acc: 0.4500\n",
            "Epoch 249/300\n",
            "\n",
            "Epoch 00249: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.0774 - acc: 1.0000 - val_loss: 2.4855 - val_acc: 0.4500\n",
            "Epoch 250/300\n",
            "\n",
            "Epoch 00250: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.0872 - acc: 1.0000 - val_loss: 2.4521 - val_acc: 0.4500\n",
            "Epoch 251/300\n",
            "\n",
            "Epoch 00251: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.0961 - acc: 0.9833 - val_loss: 2.3975 - val_acc: 0.4500\n",
            "Epoch 252/300\n",
            "\n",
            "Epoch 00252: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.1526 - acc: 0.9833 - val_loss: 2.4969 - val_acc: 0.4500\n",
            "Epoch 253/300\n",
            "\n",
            "Epoch 00253: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.0929 - acc: 0.9833 - val_loss: 2.5182 - val_acc: 0.4500\n",
            "Epoch 254/300\n",
            "\n",
            "Epoch 00254: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.1075 - acc: 0.9833 - val_loss: 2.7475 - val_acc: 0.4500\n",
            "Epoch 255/300\n",
            "\n",
            "Epoch 00255: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.0856 - acc: 1.0000 - val_loss: 2.8069 - val_acc: 0.4500\n",
            "Epoch 256/300\n",
            "\n",
            "Epoch 00256: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.1056 - acc: 1.0000 - val_loss: 3.0063 - val_acc: 0.3000\n",
            "Epoch 257/300\n",
            "\n",
            "Epoch 00257: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.1162 - acc: 1.0000 - val_loss: 3.1455 - val_acc: 0.3000\n",
            "Epoch 258/300\n",
            "\n",
            "Epoch 00258: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.0917 - acc: 0.9833 - val_loss: 3.3431 - val_acc: 0.2500\n",
            "Epoch 259/300\n",
            "\n",
            "Epoch 00259: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.1180 - acc: 0.9667 - val_loss: 3.4471 - val_acc: 0.2500\n",
            "Epoch 260/300\n",
            "\n",
            "Epoch 00260: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.1001 - acc: 1.0000 - val_loss: 3.6844 - val_acc: 0.2500\n",
            "Epoch 261/300\n",
            "\n",
            "Epoch 00261: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.1070 - acc: 0.9833 - val_loss: 3.7766 - val_acc: 0.2500\n",
            "Epoch 262/300\n",
            "\n",
            "Epoch 00262: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.1321 - acc: 0.9667 - val_loss: 3.4665 - val_acc: 0.2500\n",
            "Epoch 263/300\n",
            "\n",
            "Epoch 00263: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.0913 - acc: 0.9833 - val_loss: 3.4044 - val_acc: 0.2500\n",
            "Epoch 264/300\n",
            "\n",
            "Epoch 00264: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.1048 - acc: 0.9833 - val_loss: 3.4218 - val_acc: 0.2500\n",
            "Epoch 265/300\n",
            "\n",
            "Epoch 00265: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.1108 - acc: 0.9833 - val_loss: 3.5866 - val_acc: 0.2500\n",
            "Epoch 266/300\n",
            "\n",
            "Epoch 00266: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.0866 - acc: 1.0000 - val_loss: 3.9100 - val_acc: 0.2500\n",
            "Epoch 267/300\n",
            "\n",
            "Epoch 00267: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.1039 - acc: 0.9833 - val_loss: 4.0511 - val_acc: 0.2500\n",
            "Epoch 268/300\n",
            "\n",
            "Epoch 00268: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.0979 - acc: 1.0000 - val_loss: 3.8727 - val_acc: 0.2500\n",
            "Epoch 269/300\n",
            "\n",
            "Epoch 00269: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.1228 - acc: 0.9500 - val_loss: 3.4390 - val_acc: 0.2500\n",
            "Epoch 270/300\n",
            "\n",
            "Epoch 00270: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.0979 - acc: 0.9667 - val_loss: 3.2419 - val_acc: 0.2500\n",
            "Epoch 271/300\n",
            "\n",
            "Epoch 00271: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.1352 - acc: 0.9833 - val_loss: 3.2454 - val_acc: 0.2500\n",
            "Epoch 272/300\n",
            "\n",
            "Epoch 00272: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.1088 - acc: 0.9833 - val_loss: 3.3992 - val_acc: 0.2000\n",
            "Epoch 273/300\n",
            "\n",
            "Epoch 00273: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.1134 - acc: 0.9833 - val_loss: 3.5557 - val_acc: 0.2000\n",
            "Epoch 274/300\n",
            "\n",
            "Epoch 00274: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.1071 - acc: 1.0000 - val_loss: 4.1724 - val_acc: 0.2000\n",
            "Epoch 275/300\n",
            "\n",
            "Epoch 00275: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.1066 - acc: 0.9667 - val_loss: 4.3350 - val_acc: 0.2000\n",
            "Epoch 276/300\n",
            "\n",
            "Epoch 00276: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.0980 - acc: 0.9833 - val_loss: 4.2922 - val_acc: 0.2000\n",
            "Epoch 277/300\n",
            "\n",
            "Epoch 00277: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.0897 - acc: 1.0000 - val_loss: 4.0838 - val_acc: 0.2000\n",
            "Epoch 278/300\n",
            "\n",
            "Epoch 00278: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.1077 - acc: 0.9833 - val_loss: 4.2500 - val_acc: 0.1500\n",
            "Epoch 279/300\n",
            "\n",
            "Epoch 00279: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.1284 - acc: 0.9667 - val_loss: 4.6130 - val_acc: 0.1500\n",
            "Epoch 280/300\n",
            "\n",
            "Epoch 00280: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.0878 - acc: 1.0000 - val_loss: 4.9450 - val_acc: 0.1500\n",
            "Epoch 281/300\n",
            "\n",
            "Epoch 00281: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.1207 - acc: 0.9833 - val_loss: 4.9781 - val_acc: 0.1500\n",
            "Epoch 282/300\n",
            "\n",
            "Epoch 00282: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.1027 - acc: 1.0000 - val_loss: 4.6471 - val_acc: 0.2000\n",
            "Epoch 283/300\n",
            "\n",
            "Epoch 00283: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.0843 - acc: 1.0000 - val_loss: 4.4115 - val_acc: 0.2000\n",
            "Epoch 284/300\n",
            "\n",
            "Epoch 00284: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.1189 - acc: 0.9833 - val_loss: 4.0408 - val_acc: 0.2500\n",
            "Epoch 285/300\n",
            "\n",
            "Epoch 00285: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.1271 - acc: 0.9667 - val_loss: 3.5708 - val_acc: 0.2500\n",
            "Epoch 286/300\n",
            "\n",
            "Epoch 00286: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.0814 - acc: 1.0000 - val_loss: 3.4425 - val_acc: 0.3000\n",
            "Epoch 287/300\n",
            "\n",
            "Epoch 00287: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.1255 - acc: 0.9667 - val_loss: 3.5167 - val_acc: 0.2500\n",
            "Epoch 288/300\n",
            "\n",
            "Epoch 00288: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.1032 - acc: 0.9833 - val_loss: 3.3686 - val_acc: 0.2000\n",
            "Epoch 289/300\n",
            "\n",
            "Epoch 00289: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.1366 - acc: 0.9500 - val_loss: 3.4048 - val_acc: 0.2000\n",
            "Epoch 290/300\n",
            "\n",
            "Epoch 00290: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.1133 - acc: 0.9833 - val_loss: 3.4496 - val_acc: 0.2500\n",
            "Epoch 291/300\n",
            "\n",
            "Epoch 00291: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.1216 - acc: 0.9500 - val_loss: 3.5887 - val_acc: 0.2500\n",
            "Epoch 292/300\n",
            "\n",
            "Epoch 00292: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.0902 - acc: 0.9667 - val_loss: 3.7005 - val_acc: 0.2000\n",
            "Epoch 293/300\n",
            "\n",
            "Epoch 00293: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.0901 - acc: 0.9833 - val_loss: 3.7612 - val_acc: 0.2500\n",
            "Epoch 294/300\n",
            "\n",
            "Epoch 00294: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.0995 - acc: 0.9833 - val_loss: 3.8113 - val_acc: 0.2500\n",
            "Epoch 295/300\n",
            "\n",
            "Epoch 00295: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.1276 - acc: 0.9667 - val_loss: 3.8707 - val_acc: 0.2500\n",
            "Epoch 296/300\n",
            "\n",
            "Epoch 00296: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.0873 - acc: 1.0000 - val_loss: 3.5417 - val_acc: 0.2500\n",
            "Epoch 297/300\n",
            "\n",
            "Epoch 00297: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.0716 - acc: 1.0000 - val_loss: 3.2474 - val_acc: 0.2500\n",
            "Epoch 298/300\n",
            "\n",
            "Epoch 00298: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.0804 - acc: 1.0000 - val_loss: 3.0495 - val_acc: 0.2500\n",
            "Epoch 299/300\n",
            "\n",
            "Epoch 00299: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.1021 - acc: 0.9833 - val_loss: 3.1068 - val_acc: 0.2500\n",
            "Epoch 300/300\n",
            "\n",
            "Epoch 00300: val_loss did not improve from 0.55359\n",
            "60/60 - 0s - loss: 0.0913 - acc: 0.9833 - val_loss: 3.3370 - val_acc: 0.2500\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "# Evaluate on test data\n",
            "40/40 [==============================] - 0s 2ms/sample - loss: 1.3058 - acc: 0.4750\n",
            "test loss, test acc: [1.3057998315780424, 0.475]\n",
            "EEG_Deep/Data2A/parsed_P06T.mat\n",
            "Filtering of Training Data Finished\n",
            "EEG_Deep/Data2A/parsed_P06E.mat\n",
            "Filtering of Testing Data Finished\n",
            "(1, 2)\n",
            "Finding labels in training data\n",
            "Finding labels in testing data\n",
            "[2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1]\n",
            "(60, 12, 1536)\n",
            "(60,)\n",
            "(20, 12, 1536)\n",
            "(20,)\n",
            "[[0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]]\n",
            "X_train shape: (60, 1, 12, 1536)\n",
            "60 train samples\n",
            "20 val samples\n",
            "(40, 12, 1536)\n",
            "(40, 1)\n",
            "X_train shape: (40, 1, 12, 1536)\n",
            "40 train samples\n",
            "Train on 60 samples, validate on 20 samples\n",
            "Epoch 1/300\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 1.37214, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 1s - loss: 1.3538 - acc: 0.4667 - val_loss: 1.3721 - val_acc: 0.4500\n",
            "Epoch 2/300\n",
            "\n",
            "Epoch 00002: val_loss improved from 1.37214 to 1.35481, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 1.0976 - acc: 0.6167 - val_loss: 1.3548 - val_acc: 0.5000\n",
            "Epoch 3/300\n",
            "\n",
            "Epoch 00003: val_loss improved from 1.35481 to 1.33303, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.9628 - acc: 0.6500 - val_loss: 1.3330 - val_acc: 0.5500\n",
            "Epoch 4/300\n",
            "\n",
            "Epoch 00004: val_loss improved from 1.33303 to 1.30988, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.9006 - acc: 0.7500 - val_loss: 1.3099 - val_acc: 0.5500\n",
            "Epoch 5/300\n",
            "\n",
            "Epoch 00005: val_loss improved from 1.30988 to 1.28631, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.8383 - acc: 0.7667 - val_loss: 1.2863 - val_acc: 0.5500\n",
            "Epoch 6/300\n",
            "\n",
            "Epoch 00006: val_loss improved from 1.28631 to 1.26323, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.8068 - acc: 0.8000 - val_loss: 1.2632 - val_acc: 0.6000\n",
            "Epoch 7/300\n",
            "\n",
            "Epoch 00007: val_loss improved from 1.26323 to 1.24113, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.7774 - acc: 0.8333 - val_loss: 1.2411 - val_acc: 0.6000\n",
            "Epoch 8/300\n",
            "\n",
            "Epoch 00008: val_loss improved from 1.24113 to 1.22068, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.7727 - acc: 0.7667 - val_loss: 1.2207 - val_acc: 0.6000\n",
            "Epoch 9/300\n",
            "\n",
            "Epoch 00009: val_loss improved from 1.22068 to 1.19977, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.7454 - acc: 0.8833 - val_loss: 1.1998 - val_acc: 0.6000\n",
            "Epoch 10/300\n",
            "\n",
            "Epoch 00010: val_loss improved from 1.19977 to 1.17925, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.7349 - acc: 0.8000 - val_loss: 1.1793 - val_acc: 0.6500\n",
            "Epoch 11/300\n",
            "\n",
            "Epoch 00011: val_loss improved from 1.17925 to 1.15967, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.7188 - acc: 0.8667 - val_loss: 1.1597 - val_acc: 0.7000\n",
            "Epoch 12/300\n",
            "\n",
            "Epoch 00012: val_loss improved from 1.15967 to 1.14031, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.7098 - acc: 0.8167 - val_loss: 1.1403 - val_acc: 0.7500\n",
            "Epoch 13/300\n",
            "\n",
            "Epoch 00013: val_loss improved from 1.14031 to 1.12229, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.7061 - acc: 0.8333 - val_loss: 1.1223 - val_acc: 0.7500\n",
            "Epoch 14/300\n",
            "\n",
            "Epoch 00014: val_loss improved from 1.12229 to 1.10451, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6956 - acc: 0.9000 - val_loss: 1.1045 - val_acc: 0.7500\n",
            "Epoch 15/300\n",
            "\n",
            "Epoch 00015: val_loss improved from 1.10451 to 1.08844, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6763 - acc: 0.8000 - val_loss: 1.0884 - val_acc: 0.7500\n",
            "Epoch 16/300\n",
            "\n",
            "Epoch 00016: val_loss improved from 1.08844 to 1.07355, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6449 - acc: 0.9000 - val_loss: 1.0736 - val_acc: 0.8000\n",
            "Epoch 17/300\n",
            "\n",
            "Epoch 00017: val_loss improved from 1.07355 to 1.05934, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6388 - acc: 0.8667 - val_loss: 1.0593 - val_acc: 0.8000\n",
            "Epoch 18/300\n",
            "\n",
            "Epoch 00018: val_loss improved from 1.05934 to 1.04603, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6452 - acc: 0.8667 - val_loss: 1.0460 - val_acc: 0.8000\n",
            "Epoch 19/300\n",
            "\n",
            "Epoch 00019: val_loss improved from 1.04603 to 1.03319, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6312 - acc: 0.9333 - val_loss: 1.0332 - val_acc: 0.8000\n",
            "Epoch 20/300\n",
            "\n",
            "Epoch 00020: val_loss improved from 1.03319 to 1.02060, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6372 - acc: 0.8667 - val_loss: 1.0206 - val_acc: 0.7000\n",
            "Epoch 21/300\n",
            "\n",
            "Epoch 00021: val_loss improved from 1.02060 to 1.00920, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6284 - acc: 0.8333 - val_loss: 1.0092 - val_acc: 0.7000\n",
            "Epoch 22/300\n",
            "\n",
            "Epoch 00022: val_loss improved from 1.00920 to 0.99677, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6241 - acc: 0.8500 - val_loss: 0.9968 - val_acc: 0.7000\n",
            "Epoch 23/300\n",
            "\n",
            "Epoch 00023: val_loss improved from 0.99677 to 0.98456, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5850 - acc: 0.8833 - val_loss: 0.9846 - val_acc: 0.8000\n",
            "Epoch 24/300\n",
            "\n",
            "Epoch 00024: val_loss improved from 0.98456 to 0.97300, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6009 - acc: 0.8833 - val_loss: 0.9730 - val_acc: 0.8000\n",
            "Epoch 25/300\n",
            "\n",
            "Epoch 00025: val_loss improved from 0.97300 to 0.96230, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5545 - acc: 0.9333 - val_loss: 0.9623 - val_acc: 0.8500\n",
            "Epoch 26/300\n",
            "\n",
            "Epoch 00026: val_loss improved from 0.96230 to 0.95267, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5474 - acc: 0.9167 - val_loss: 0.9527 - val_acc: 0.6500\n",
            "Epoch 27/300\n",
            "\n",
            "Epoch 00027: val_loss improved from 0.95267 to 0.94449, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5502 - acc: 0.9167 - val_loss: 0.9445 - val_acc: 0.6000\n",
            "Epoch 28/300\n",
            "\n",
            "Epoch 00028: val_loss improved from 0.94449 to 0.93315, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5450 - acc: 0.9167 - val_loss: 0.9331 - val_acc: 0.6000\n",
            "Epoch 29/300\n",
            "\n",
            "Epoch 00029: val_loss improved from 0.93315 to 0.92045, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5168 - acc: 0.9167 - val_loss: 0.9204 - val_acc: 0.6000\n",
            "Epoch 30/300\n",
            "\n",
            "Epoch 00030: val_loss improved from 0.92045 to 0.90788, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4974 - acc: 0.9333 - val_loss: 0.9079 - val_acc: 0.6000\n",
            "Epoch 31/300\n",
            "\n",
            "Epoch 00031: val_loss improved from 0.90788 to 0.89339, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4936 - acc: 0.9167 - val_loss: 0.8934 - val_acc: 0.6000\n",
            "Epoch 32/300\n",
            "\n",
            "Epoch 00032: val_loss improved from 0.89339 to 0.87786, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4922 - acc: 0.9500 - val_loss: 0.8779 - val_acc: 0.6500\n",
            "Epoch 33/300\n",
            "\n",
            "Epoch 00033: val_loss improved from 0.87786 to 0.86287, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4828 - acc: 0.9167 - val_loss: 0.8629 - val_acc: 0.6500\n",
            "Epoch 34/300\n",
            "\n",
            "Epoch 00034: val_loss improved from 0.86287 to 0.84903, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4710 - acc: 0.9333 - val_loss: 0.8490 - val_acc: 0.6500\n",
            "Epoch 35/300\n",
            "\n",
            "Epoch 00035: val_loss improved from 0.84903 to 0.83807, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4645 - acc: 0.9167 - val_loss: 0.8381 - val_acc: 0.6500\n",
            "Epoch 36/300\n",
            "\n",
            "Epoch 00036: val_loss improved from 0.83807 to 0.82919, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4334 - acc: 0.8833 - val_loss: 0.8292 - val_acc: 0.6500\n",
            "Epoch 37/300\n",
            "\n",
            "Epoch 00037: val_loss improved from 0.82919 to 0.82202, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4610 - acc: 0.9167 - val_loss: 0.8220 - val_acc: 0.7000\n",
            "Epoch 38/300\n",
            "\n",
            "Epoch 00038: val_loss improved from 0.82202 to 0.80880, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4060 - acc: 0.8667 - val_loss: 0.8088 - val_acc: 0.7000\n",
            "Epoch 39/300\n",
            "\n",
            "Epoch 00039: val_loss improved from 0.80880 to 0.79185, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3931 - acc: 0.9333 - val_loss: 0.7918 - val_acc: 0.6500\n",
            "Epoch 40/300\n",
            "\n",
            "Epoch 00040: val_loss improved from 0.79185 to 0.77055, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4107 - acc: 0.9000 - val_loss: 0.7706 - val_acc: 0.7500\n",
            "Epoch 41/300\n",
            "\n",
            "Epoch 00041: val_loss improved from 0.77055 to 0.75706, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3786 - acc: 0.8833 - val_loss: 0.7571 - val_acc: 0.6500\n",
            "Epoch 42/300\n",
            "\n",
            "Epoch 00042: val_loss improved from 0.75706 to 0.75530, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4203 - acc: 0.8833 - val_loss: 0.7553 - val_acc: 0.6500\n",
            "Epoch 43/300\n",
            "\n",
            "Epoch 00043: val_loss improved from 0.75530 to 0.75177, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4104 - acc: 0.9000 - val_loss: 0.7518 - val_acc: 0.6500\n",
            "Epoch 44/300\n",
            "\n",
            "Epoch 00044: val_loss improved from 0.75177 to 0.73575, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3594 - acc: 0.9333 - val_loss: 0.7358 - val_acc: 0.7000\n",
            "Epoch 45/300\n",
            "\n",
            "Epoch 00045: val_loss improved from 0.73575 to 0.71773, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3620 - acc: 0.9000 - val_loss: 0.7177 - val_acc: 0.7500\n",
            "Epoch 46/300\n",
            "\n",
            "Epoch 00046: val_loss improved from 0.71773 to 0.69752, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3493 - acc: 0.9167 - val_loss: 0.6975 - val_acc: 0.8000\n",
            "Epoch 47/300\n",
            "\n",
            "Epoch 00047: val_loss improved from 0.69752 to 0.67832, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3320 - acc: 0.9500 - val_loss: 0.6783 - val_acc: 0.8500\n",
            "Epoch 48/300\n",
            "\n",
            "Epoch 00048: val_loss improved from 0.67832 to 0.67535, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3077 - acc: 0.9333 - val_loss: 0.6754 - val_acc: 0.8500\n",
            "Epoch 49/300\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.67535\n",
            "60/60 - 0s - loss: 0.3571 - acc: 0.9167 - val_loss: 0.6917 - val_acc: 0.8000\n",
            "Epoch 50/300\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.67535\n",
            "60/60 - 0s - loss: 0.3259 - acc: 0.9500 - val_loss: 0.7177 - val_acc: 0.7000\n",
            "Epoch 51/300\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.67535\n",
            "60/60 - 0s - loss: 0.3400 - acc: 0.9333 - val_loss: 0.7412 - val_acc: 0.6000\n",
            "Epoch 52/300\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.67535\n",
            "60/60 - 0s - loss: 0.3035 - acc: 0.9500 - val_loss: 0.7424 - val_acc: 0.6000\n",
            "Epoch 53/300\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.67535\n",
            "60/60 - 0s - loss: 0.3412 - acc: 0.8833 - val_loss: 0.7236 - val_acc: 0.6500\n",
            "Epoch 54/300\n",
            "\n",
            "Epoch 00054: val_loss improved from 0.67535 to 0.67048, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3224 - acc: 0.8833 - val_loss: 0.6705 - val_acc: 0.7500\n",
            "Epoch 55/300\n",
            "\n",
            "Epoch 00055: val_loss improved from 0.67048 to 0.62528, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3063 - acc: 0.9000 - val_loss: 0.6253 - val_acc: 0.8500\n",
            "Epoch 56/300\n",
            "\n",
            "Epoch 00056: val_loss improved from 0.62528 to 0.60361, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2942 - acc: 0.9500 - val_loss: 0.6036 - val_acc: 0.8500\n",
            "Epoch 57/300\n",
            "\n",
            "Epoch 00057: val_loss improved from 0.60361 to 0.57749, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3375 - acc: 0.8500 - val_loss: 0.5775 - val_acc: 0.8500\n",
            "Epoch 58/300\n",
            "\n",
            "Epoch 00058: val_loss improved from 0.57749 to 0.56741, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2771 - acc: 0.9167 - val_loss: 0.5674 - val_acc: 0.8500\n",
            "Epoch 59/300\n",
            "\n",
            "Epoch 00059: val_loss improved from 0.56741 to 0.56547, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3260 - acc: 0.9333 - val_loss: 0.5655 - val_acc: 0.8500\n",
            "Epoch 60/300\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.56547\n",
            "60/60 - 0s - loss: 0.2744 - acc: 0.8667 - val_loss: 0.5669 - val_acc: 0.8500\n",
            "Epoch 61/300\n",
            "\n",
            "Epoch 00061: val_loss improved from 0.56547 to 0.53894, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3336 - acc: 0.9167 - val_loss: 0.5389 - val_acc: 0.8500\n",
            "Epoch 62/300\n",
            "\n",
            "Epoch 00062: val_loss improved from 0.53894 to 0.52657, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2600 - acc: 0.9833 - val_loss: 0.5266 - val_acc: 0.8500\n",
            "Epoch 63/300\n",
            "\n",
            "Epoch 00063: val_loss improved from 0.52657 to 0.52606, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2181 - acc: 0.9500 - val_loss: 0.5261 - val_acc: 0.8500\n",
            "Epoch 64/300\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.52606\n",
            "60/60 - 0s - loss: 0.3105 - acc: 0.9000 - val_loss: 0.5283 - val_acc: 0.8500\n",
            "Epoch 65/300\n",
            "\n",
            "Epoch 00065: val_loss improved from 0.52606 to 0.50623, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2338 - acc: 0.9333 - val_loss: 0.5062 - val_acc: 0.8500\n",
            "Epoch 66/300\n",
            "\n",
            "Epoch 00066: val_loss improved from 0.50623 to 0.48215, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2278 - acc: 0.9667 - val_loss: 0.4822 - val_acc: 0.8500\n",
            "Epoch 67/300\n",
            "\n",
            "Epoch 00067: val_loss improved from 0.48215 to 0.46658, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2862 - acc: 0.9667 - val_loss: 0.4666 - val_acc: 0.8500\n",
            "Epoch 68/300\n",
            "\n",
            "Epoch 00068: val_loss improved from 0.46658 to 0.46525, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2409 - acc: 0.9667 - val_loss: 0.4652 - val_acc: 0.8500\n",
            "Epoch 69/300\n",
            "\n",
            "Epoch 00069: val_loss improved from 0.46525 to 0.46113, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2349 - acc: 0.9333 - val_loss: 0.4611 - val_acc: 0.8500\n",
            "Epoch 70/300\n",
            "\n",
            "Epoch 00070: val_loss improved from 0.46113 to 0.46107, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2355 - acc: 0.9333 - val_loss: 0.4611 - val_acc: 0.8500\n",
            "Epoch 71/300\n",
            "\n",
            "Epoch 00071: val_loss improved from 0.46107 to 0.44868, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2189 - acc: 0.9167 - val_loss: 0.4487 - val_acc: 0.8500\n",
            "Epoch 72/300\n",
            "\n",
            "Epoch 00072: val_loss improved from 0.44868 to 0.44164, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2452 - acc: 0.9500 - val_loss: 0.4416 - val_acc: 0.8500\n",
            "Epoch 73/300\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.44164\n",
            "60/60 - 0s - loss: 0.1917 - acc: 0.9500 - val_loss: 0.4455 - val_acc: 0.8500\n",
            "Epoch 74/300\n",
            "\n",
            "Epoch 00074: val_loss improved from 0.44164 to 0.43866, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2347 - acc: 0.9333 - val_loss: 0.4387 - val_acc: 0.8500\n",
            "Epoch 75/300\n",
            "\n",
            "Epoch 00075: val_loss improved from 0.43866 to 0.42646, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2046 - acc: 0.9500 - val_loss: 0.4265 - val_acc: 0.8500\n",
            "Epoch 76/300\n",
            "\n",
            "Epoch 00076: val_loss improved from 0.42646 to 0.42047, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2142 - acc: 0.9500 - val_loss: 0.4205 - val_acc: 0.9000\n",
            "Epoch 77/300\n",
            "\n",
            "Epoch 00077: val_loss improved from 0.42047 to 0.41552, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1994 - acc: 0.9500 - val_loss: 0.4155 - val_acc: 0.9000\n",
            "Epoch 78/300\n",
            "\n",
            "Epoch 00078: val_loss improved from 0.41552 to 0.40962, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2555 - acc: 0.9167 - val_loss: 0.4096 - val_acc: 0.9000\n",
            "Epoch 79/300\n",
            "\n",
            "Epoch 00079: val_loss improved from 0.40962 to 0.40349, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2452 - acc: 0.9167 - val_loss: 0.4035 - val_acc: 0.9500\n",
            "Epoch 80/300\n",
            "\n",
            "Epoch 00080: val_loss improved from 0.40349 to 0.39615, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1798 - acc: 0.9667 - val_loss: 0.3962 - val_acc: 0.9500\n",
            "Epoch 81/300\n",
            "\n",
            "Epoch 00081: val_loss improved from 0.39615 to 0.39525, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2608 - acc: 0.9000 - val_loss: 0.3953 - val_acc: 0.9000\n",
            "Epoch 82/300\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.39525\n",
            "60/60 - 0s - loss: 0.2051 - acc: 0.9333 - val_loss: 0.4032 - val_acc: 0.9000\n",
            "Epoch 83/300\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.39525\n",
            "60/60 - 0s - loss: 0.2143 - acc: 0.9500 - val_loss: 0.4021 - val_acc: 0.9000\n",
            "Epoch 84/300\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.39525\n",
            "60/60 - 0s - loss: 0.2027 - acc: 0.9333 - val_loss: 0.3990 - val_acc: 0.9000\n",
            "Epoch 85/300\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.39525\n",
            "60/60 - 0s - loss: 0.1698 - acc: 1.0000 - val_loss: 0.4009 - val_acc: 0.9500\n",
            "Epoch 86/300\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.39525\n",
            "60/60 - 0s - loss: 0.2669 - acc: 0.8833 - val_loss: 0.4084 - val_acc: 0.9000\n",
            "Epoch 87/300\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.39525\n",
            "60/60 - 0s - loss: 0.1951 - acc: 0.9500 - val_loss: 0.4161 - val_acc: 0.9000\n",
            "Epoch 88/300\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.39525\n",
            "60/60 - 0s - loss: 0.2361 - acc: 0.9333 - val_loss: 0.4171 - val_acc: 0.9000\n",
            "Epoch 89/300\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.39525\n",
            "60/60 - 0s - loss: 0.2277 - acc: 0.9500 - val_loss: 0.4245 - val_acc: 0.8500\n",
            "Epoch 90/300\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.39525\n",
            "60/60 - 0s - loss: 0.1626 - acc: 0.9500 - val_loss: 0.4260 - val_acc: 0.8500\n",
            "Epoch 91/300\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.39525\n",
            "60/60 - 0s - loss: 0.1633 - acc: 0.9667 - val_loss: 0.4255 - val_acc: 0.8500\n",
            "Epoch 92/300\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.39525\n",
            "60/60 - 0s - loss: 0.1767 - acc: 0.9833 - val_loss: 0.4268 - val_acc: 0.8500\n",
            "Epoch 93/300\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.39525\n",
            "60/60 - 0s - loss: 0.1342 - acc: 1.0000 - val_loss: 0.4247 - val_acc: 0.8500\n",
            "Epoch 94/300\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.39525\n",
            "60/60 - 0s - loss: 0.1834 - acc: 0.9667 - val_loss: 0.4055 - val_acc: 0.8500\n",
            "Epoch 95/300\n",
            "\n",
            "Epoch 00095: val_loss improved from 0.39525 to 0.38949, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1658 - acc: 0.9833 - val_loss: 0.3895 - val_acc: 0.9000\n",
            "Epoch 96/300\n",
            "\n",
            "Epoch 00096: val_loss improved from 0.38949 to 0.38607, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1700 - acc: 0.9333 - val_loss: 0.3861 - val_acc: 0.9000\n",
            "Epoch 97/300\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.1515 - acc: 0.9500 - val_loss: 0.3922 - val_acc: 0.9000\n",
            "Epoch 98/300\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.1594 - acc: 0.9833 - val_loss: 0.4012 - val_acc: 0.9000\n",
            "Epoch 99/300\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.1778 - acc: 0.9833 - val_loss: 0.4091 - val_acc: 0.9000\n",
            "Epoch 100/300\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.1501 - acc: 0.9667 - val_loss: 0.4115 - val_acc: 0.9000\n",
            "Epoch 101/300\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.1412 - acc: 0.9833 - val_loss: 0.4154 - val_acc: 0.9000\n",
            "Epoch 102/300\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.1319 - acc: 0.9833 - val_loss: 0.4191 - val_acc: 0.9000\n",
            "Epoch 103/300\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.1597 - acc: 0.9833 - val_loss: 0.4216 - val_acc: 0.9000\n",
            "Epoch 104/300\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.1578 - acc: 0.9667 - val_loss: 0.4254 - val_acc: 0.9000\n",
            "Epoch 105/300\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.1736 - acc: 0.9667 - val_loss: 0.4230 - val_acc: 0.9000\n",
            "Epoch 106/300\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.1854 - acc: 0.9667 - val_loss: 0.4161 - val_acc: 0.9000\n",
            "Epoch 107/300\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.1455 - acc: 0.9667 - val_loss: 0.4202 - val_acc: 0.9000\n",
            "Epoch 108/300\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.1415 - acc: 1.0000 - val_loss: 0.4247 - val_acc: 0.9000\n",
            "Epoch 109/300\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.1310 - acc: 0.9667 - val_loss: 0.4295 - val_acc: 0.9000\n",
            "Epoch 110/300\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.1187 - acc: 0.9833 - val_loss: 0.4346 - val_acc: 0.9000\n",
            "Epoch 111/300\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.1172 - acc: 0.9833 - val_loss: 0.4284 - val_acc: 0.9000\n",
            "Epoch 112/300\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.1220 - acc: 0.9833 - val_loss: 0.4202 - val_acc: 0.9000\n",
            "Epoch 113/300\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.1168 - acc: 1.0000 - val_loss: 0.4045 - val_acc: 0.9000\n",
            "Epoch 114/300\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.1279 - acc: 0.9833 - val_loss: 0.3958 - val_acc: 0.9000\n",
            "Epoch 115/300\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.1833 - acc: 0.9500 - val_loss: 0.3961 - val_acc: 0.9000\n",
            "Epoch 116/300\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.1569 - acc: 0.9500 - val_loss: 0.3989 - val_acc: 0.9000\n",
            "Epoch 117/300\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.1657 - acc: 0.9500 - val_loss: 0.4054 - val_acc: 0.9000\n",
            "Epoch 118/300\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.1529 - acc: 1.0000 - val_loss: 0.4222 - val_acc: 0.9000\n",
            "Epoch 119/300\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.1751 - acc: 0.9500 - val_loss: 0.4656 - val_acc: 0.8500\n",
            "Epoch 120/300\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.0958 - acc: 0.9833 - val_loss: 0.4885 - val_acc: 0.8500\n",
            "Epoch 121/300\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.1743 - acc: 0.9500 - val_loss: 0.5123 - val_acc: 0.8500\n",
            "Epoch 122/300\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.1312 - acc: 0.9667 - val_loss: 0.5167 - val_acc: 0.8500\n",
            "Epoch 123/300\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.1447 - acc: 0.9833 - val_loss: 0.5028 - val_acc: 0.8500\n",
            "Epoch 124/300\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.1782 - acc: 0.9667 - val_loss: 0.4894 - val_acc: 0.8500\n",
            "Epoch 125/300\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.1547 - acc: 0.9500 - val_loss: 0.5043 - val_acc: 0.8500\n",
            "Epoch 126/300\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.1142 - acc: 0.9833 - val_loss: 0.5215 - val_acc: 0.8500\n",
            "Epoch 127/300\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.1798 - acc: 0.9667 - val_loss: 0.5360 - val_acc: 0.8500\n",
            "Epoch 128/300\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.1399 - acc: 0.9500 - val_loss: 0.5529 - val_acc: 0.8500\n",
            "Epoch 129/300\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.1823 - acc: 0.9333 - val_loss: 0.5301 - val_acc: 0.8500\n",
            "Epoch 130/300\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.1028 - acc: 0.9833 - val_loss: 0.5044 - val_acc: 0.8500\n",
            "Epoch 131/300\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.1498 - acc: 0.9500 - val_loss: 0.5003 - val_acc: 0.8500\n",
            "Epoch 132/300\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.1546 - acc: 0.9667 - val_loss: 0.4783 - val_acc: 0.9000\n",
            "Epoch 133/300\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.1565 - acc: 0.9500 - val_loss: 0.4736 - val_acc: 0.9000\n",
            "Epoch 134/300\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.1411 - acc: 0.9667 - val_loss: 0.4729 - val_acc: 0.9000\n",
            "Epoch 135/300\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.1025 - acc: 1.0000 - val_loss: 0.4701 - val_acc: 0.9000\n",
            "Epoch 136/300\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.1407 - acc: 1.0000 - val_loss: 0.4788 - val_acc: 0.8500\n",
            "Epoch 137/300\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.1184 - acc: 0.9833 - val_loss: 0.4830 - val_acc: 0.8500\n",
            "Epoch 138/300\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.2006 - acc: 0.9333 - val_loss: 0.4851 - val_acc: 0.8500\n",
            "Epoch 139/300\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.0857 - acc: 0.9833 - val_loss: 0.4658 - val_acc: 0.8500\n",
            "Epoch 140/300\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.1126 - acc: 0.9833 - val_loss: 0.4560 - val_acc: 0.8500\n",
            "Epoch 141/300\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.0794 - acc: 1.0000 - val_loss: 0.4500 - val_acc: 0.8500\n",
            "Epoch 142/300\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.1322 - acc: 0.9500 - val_loss: 0.4453 - val_acc: 0.8500\n",
            "Epoch 143/300\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.1112 - acc: 0.9833 - val_loss: 0.4364 - val_acc: 0.9000\n",
            "Epoch 144/300\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.1269 - acc: 0.9833 - val_loss: 0.4234 - val_acc: 0.9000\n",
            "Epoch 145/300\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.1130 - acc: 0.9833 - val_loss: 0.4220 - val_acc: 0.9000\n",
            "Epoch 146/300\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.1243 - acc: 0.9833 - val_loss: 0.4271 - val_acc: 0.9000\n",
            "Epoch 147/300\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.0795 - acc: 0.9833 - val_loss: 0.4391 - val_acc: 0.9500\n",
            "Epoch 148/300\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.1023 - acc: 0.9833 - val_loss: 0.4458 - val_acc: 0.9500\n",
            "Epoch 149/300\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.1287 - acc: 0.9833 - val_loss: 0.4426 - val_acc: 0.9500\n",
            "Epoch 150/300\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.1245 - acc: 0.9667 - val_loss: 0.4434 - val_acc: 0.9500\n",
            "Epoch 151/300\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.1142 - acc: 0.9833 - val_loss: 0.4471 - val_acc: 0.9500\n",
            "Epoch 152/300\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.0718 - acc: 0.9833 - val_loss: 0.4544 - val_acc: 0.9000\n",
            "Epoch 153/300\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.1634 - acc: 0.9667 - val_loss: 0.4507 - val_acc: 0.9500\n",
            "Epoch 154/300\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.1302 - acc: 0.9500 - val_loss: 0.4399 - val_acc: 0.9500\n",
            "Epoch 155/300\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.1731 - acc: 0.9667 - val_loss: 0.4361 - val_acc: 0.9500\n",
            "Epoch 156/300\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.1030 - acc: 0.9833 - val_loss: 0.4295 - val_acc: 0.9000\n",
            "Epoch 157/300\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.0816 - acc: 1.0000 - val_loss: 0.4278 - val_acc: 0.9000\n",
            "Epoch 158/300\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.1502 - acc: 0.9500 - val_loss: 0.4425 - val_acc: 0.9000\n",
            "Epoch 159/300\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.0776 - acc: 1.0000 - val_loss: 0.4521 - val_acc: 0.9000\n",
            "Epoch 160/300\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.1028 - acc: 0.9500 - val_loss: 0.4484 - val_acc: 0.9000\n",
            "Epoch 161/300\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.1319 - acc: 0.9833 - val_loss: 0.4473 - val_acc: 0.9000\n",
            "Epoch 162/300\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.0801 - acc: 1.0000 - val_loss: 0.4472 - val_acc: 0.9000\n",
            "Epoch 163/300\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.0917 - acc: 1.0000 - val_loss: 0.4476 - val_acc: 0.9500\n",
            "Epoch 164/300\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.1106 - acc: 0.9667 - val_loss: 0.4492 - val_acc: 0.9500\n",
            "Epoch 165/300\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.0768 - acc: 1.0000 - val_loss: 0.4505 - val_acc: 0.9000\n",
            "Epoch 166/300\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.0959 - acc: 0.9833 - val_loss: 0.4487 - val_acc: 0.9000\n",
            "Epoch 167/300\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.1795 - acc: 0.9833 - val_loss: 0.4580 - val_acc: 0.9000\n",
            "Epoch 168/300\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.0743 - acc: 1.0000 - val_loss: 0.4679 - val_acc: 0.9000\n",
            "Epoch 169/300\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.1431 - acc: 0.9667 - val_loss: 0.4638 - val_acc: 0.9000\n",
            "Epoch 170/300\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.0927 - acc: 1.0000 - val_loss: 0.4590 - val_acc: 0.9000\n",
            "Epoch 171/300\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.1009 - acc: 0.9833 - val_loss: 0.4554 - val_acc: 0.9000\n",
            "Epoch 172/300\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.0907 - acc: 1.0000 - val_loss: 0.4548 - val_acc: 0.9000\n",
            "Epoch 173/300\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.0730 - acc: 0.9833 - val_loss: 0.4634 - val_acc: 0.8500\n",
            "Epoch 174/300\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.0755 - acc: 0.9833 - val_loss: 0.4815 - val_acc: 0.8500\n",
            "Epoch 175/300\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.1289 - acc: 0.9333 - val_loss: 0.4775 - val_acc: 0.8500\n",
            "Epoch 176/300\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.1197 - acc: 0.9833 - val_loss: 0.4633 - val_acc: 0.8500\n",
            "Epoch 177/300\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.0823 - acc: 0.9833 - val_loss: 0.4452 - val_acc: 0.8500\n",
            "Epoch 178/300\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.1044 - acc: 0.9667 - val_loss: 0.4372 - val_acc: 0.8500\n",
            "Epoch 179/300\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.1170 - acc: 0.9667 - val_loss: 0.4195 - val_acc: 0.8500\n",
            "Epoch 180/300\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.0800 - acc: 0.9833 - val_loss: 0.4024 - val_acc: 0.9000\n",
            "Epoch 181/300\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.0948 - acc: 1.0000 - val_loss: 0.4016 - val_acc: 0.9000\n",
            "Epoch 182/300\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.0905 - acc: 1.0000 - val_loss: 0.3976 - val_acc: 0.9000\n",
            "Epoch 183/300\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.0660 - acc: 1.0000 - val_loss: 0.4063 - val_acc: 0.9000\n",
            "Epoch 184/300\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.0849 - acc: 0.9833 - val_loss: 0.4140 - val_acc: 0.9500\n",
            "Epoch 185/300\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.1154 - acc: 0.9667 - val_loss: 0.4251 - val_acc: 0.9500\n",
            "Epoch 186/300\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.1239 - acc: 0.9500 - val_loss: 0.4351 - val_acc: 0.9500\n",
            "Epoch 187/300\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.0630 - acc: 1.0000 - val_loss: 0.4460 - val_acc: 0.9500\n",
            "Epoch 188/300\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.1111 - acc: 0.9667 - val_loss: 0.4288 - val_acc: 0.9000\n",
            "Epoch 189/300\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.1038 - acc: 0.9833 - val_loss: 0.4233 - val_acc: 0.9000\n",
            "Epoch 190/300\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.1066 - acc: 0.9833 - val_loss: 0.4122 - val_acc: 0.9000\n",
            "Epoch 191/300\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.0607 - acc: 1.0000 - val_loss: 0.4057 - val_acc: 0.9000\n",
            "Epoch 192/300\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.1443 - acc: 0.9333 - val_loss: 0.3995 - val_acc: 0.9000\n",
            "Epoch 193/300\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.1170 - acc: 0.9833 - val_loss: 0.4071 - val_acc: 0.9000\n",
            "Epoch 194/300\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.0822 - acc: 1.0000 - val_loss: 0.3997 - val_acc: 0.9000\n",
            "Epoch 195/300\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.0563 - acc: 1.0000 - val_loss: 0.3964 - val_acc: 0.9000\n",
            "Epoch 196/300\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.0856 - acc: 0.9667 - val_loss: 0.3913 - val_acc: 0.9000\n",
            "Epoch 197/300\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.0745 - acc: 1.0000 - val_loss: 0.3953 - val_acc: 0.9000\n",
            "Epoch 198/300\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.1410 - acc: 0.9500 - val_loss: 0.4000 - val_acc: 0.9000\n",
            "Epoch 199/300\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.0748 - acc: 0.9833 - val_loss: 0.3990 - val_acc: 0.9000\n",
            "Epoch 200/300\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.0917 - acc: 1.0000 - val_loss: 0.4025 - val_acc: 0.9000\n",
            "Epoch 201/300\n",
            "\n",
            "Epoch 00201: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.0541 - acc: 1.0000 - val_loss: 0.3971 - val_acc: 0.9000\n",
            "Epoch 202/300\n",
            "\n",
            "Epoch 00202: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.0728 - acc: 0.9833 - val_loss: 0.3992 - val_acc: 0.9000\n",
            "Epoch 203/300\n",
            "\n",
            "Epoch 00203: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.0778 - acc: 1.0000 - val_loss: 0.3956 - val_acc: 0.9000\n",
            "Epoch 204/300\n",
            "\n",
            "Epoch 00204: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.1160 - acc: 0.9833 - val_loss: 0.3949 - val_acc: 0.9000\n",
            "Epoch 205/300\n",
            "\n",
            "Epoch 00205: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.0546 - acc: 1.0000 - val_loss: 0.4029 - val_acc: 0.9000\n",
            "Epoch 206/300\n",
            "\n",
            "Epoch 00206: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.1909 - acc: 0.9167 - val_loss: 0.4251 - val_acc: 0.8500\n",
            "Epoch 207/300\n",
            "\n",
            "Epoch 00207: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.0863 - acc: 1.0000 - val_loss: 0.4403 - val_acc: 0.8500\n",
            "Epoch 208/300\n",
            "\n",
            "Epoch 00208: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.1299 - acc: 0.9500 - val_loss: 0.4428 - val_acc: 0.8500\n",
            "Epoch 209/300\n",
            "\n",
            "Epoch 00209: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.0678 - acc: 1.0000 - val_loss: 0.4344 - val_acc: 0.9000\n",
            "Epoch 210/300\n",
            "\n",
            "Epoch 00210: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.0619 - acc: 1.0000 - val_loss: 0.4496 - val_acc: 0.9000\n",
            "Epoch 211/300\n",
            "\n",
            "Epoch 00211: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.0709 - acc: 1.0000 - val_loss: 0.4718 - val_acc: 0.8500\n",
            "Epoch 212/300\n",
            "\n",
            "Epoch 00212: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.0580 - acc: 1.0000 - val_loss: 0.4859 - val_acc: 0.8500\n",
            "Epoch 213/300\n",
            "\n",
            "Epoch 00213: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.0490 - acc: 1.0000 - val_loss: 0.4745 - val_acc: 0.8500\n",
            "Epoch 214/300\n",
            "\n",
            "Epoch 00214: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.0744 - acc: 1.0000 - val_loss: 0.4779 - val_acc: 0.8500\n",
            "Epoch 215/300\n",
            "\n",
            "Epoch 00215: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.0468 - acc: 1.0000 - val_loss: 0.4910 - val_acc: 0.8500\n",
            "Epoch 216/300\n",
            "\n",
            "Epoch 00216: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.0682 - acc: 0.9667 - val_loss: 0.5318 - val_acc: 0.8500\n",
            "Epoch 217/300\n",
            "\n",
            "Epoch 00217: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.0673 - acc: 1.0000 - val_loss: 0.5507 - val_acc: 0.8500\n",
            "Epoch 218/300\n",
            "\n",
            "Epoch 00218: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.0501 - acc: 1.0000 - val_loss: 0.5684 - val_acc: 0.8500\n",
            "Epoch 219/300\n",
            "\n",
            "Epoch 00219: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.0774 - acc: 0.9833 - val_loss: 0.5627 - val_acc: 0.8500\n",
            "Epoch 220/300\n",
            "\n",
            "Epoch 00220: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.0690 - acc: 1.0000 - val_loss: 0.5393 - val_acc: 0.8500\n",
            "Epoch 221/300\n",
            "\n",
            "Epoch 00221: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.0372 - acc: 1.0000 - val_loss: 0.5161 - val_acc: 0.8500\n",
            "Epoch 222/300\n",
            "\n",
            "Epoch 00222: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.1315 - acc: 0.9667 - val_loss: 0.4848 - val_acc: 0.8500\n",
            "Epoch 223/300\n",
            "\n",
            "Epoch 00223: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.1222 - acc: 0.9500 - val_loss: 0.4616 - val_acc: 0.9000\n",
            "Epoch 224/300\n",
            "\n",
            "Epoch 00224: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.0521 - acc: 1.0000 - val_loss: 0.4424 - val_acc: 0.9000\n",
            "Epoch 225/300\n",
            "\n",
            "Epoch 00225: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.0738 - acc: 0.9833 - val_loss: 0.4369 - val_acc: 0.9500\n",
            "Epoch 226/300\n",
            "\n",
            "Epoch 00226: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.0287 - acc: 1.0000 - val_loss: 0.4378 - val_acc: 0.9000\n",
            "Epoch 227/300\n",
            "\n",
            "Epoch 00227: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.0627 - acc: 0.9833 - val_loss: 0.4425 - val_acc: 0.9000\n",
            "Epoch 228/300\n",
            "\n",
            "Epoch 00228: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.0807 - acc: 0.9833 - val_loss: 0.4476 - val_acc: 0.9000\n",
            "Epoch 229/300\n",
            "\n",
            "Epoch 00229: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.0550 - acc: 1.0000 - val_loss: 0.4522 - val_acc: 0.9000\n",
            "Epoch 230/300\n",
            "\n",
            "Epoch 00230: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.0399 - acc: 1.0000 - val_loss: 0.4556 - val_acc: 0.9500\n",
            "Epoch 231/300\n",
            "\n",
            "Epoch 00231: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.0518 - acc: 1.0000 - val_loss: 0.4601 - val_acc: 0.9500\n",
            "Epoch 232/300\n",
            "\n",
            "Epoch 00232: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.0782 - acc: 1.0000 - val_loss: 0.4714 - val_acc: 0.9500\n",
            "Epoch 233/300\n",
            "\n",
            "Epoch 00233: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.0547 - acc: 1.0000 - val_loss: 0.5015 - val_acc: 0.8500\n",
            "Epoch 234/300\n",
            "\n",
            "Epoch 00234: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.0531 - acc: 1.0000 - val_loss: 0.5543 - val_acc: 0.8000\n",
            "Epoch 235/300\n",
            "\n",
            "Epoch 00235: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.0707 - acc: 1.0000 - val_loss: 0.5702 - val_acc: 0.7500\n",
            "Epoch 236/300\n",
            "\n",
            "Epoch 00236: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.0589 - acc: 1.0000 - val_loss: 0.5100 - val_acc: 0.8500\n",
            "Epoch 237/300\n",
            "\n",
            "Epoch 00237: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.0539 - acc: 1.0000 - val_loss: 0.4825 - val_acc: 0.9500\n",
            "Epoch 238/300\n",
            "\n",
            "Epoch 00238: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.0568 - acc: 1.0000 - val_loss: 0.4715 - val_acc: 0.9500\n",
            "Epoch 239/300\n",
            "\n",
            "Epoch 00239: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.0581 - acc: 0.9833 - val_loss: 0.4727 - val_acc: 0.9500\n",
            "Epoch 240/300\n",
            "\n",
            "Epoch 00240: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.0557 - acc: 1.0000 - val_loss: 0.4804 - val_acc: 0.9500\n",
            "Epoch 241/300\n",
            "\n",
            "Epoch 00241: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.0903 - acc: 0.9833 - val_loss: 0.4836 - val_acc: 0.9500\n",
            "Epoch 242/300\n",
            "\n",
            "Epoch 00242: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.0748 - acc: 1.0000 - val_loss: 0.4816 - val_acc: 0.9500\n",
            "Epoch 243/300\n",
            "\n",
            "Epoch 00243: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.0736 - acc: 0.9833 - val_loss: 0.4878 - val_acc: 0.9000\n",
            "Epoch 244/300\n",
            "\n",
            "Epoch 00244: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.0833 - acc: 0.9667 - val_loss: 0.5145 - val_acc: 0.8500\n",
            "Epoch 245/300\n",
            "\n",
            "Epoch 00245: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.0762 - acc: 0.9667 - val_loss: 0.5582 - val_acc: 0.8500\n",
            "Epoch 246/300\n",
            "\n",
            "Epoch 00246: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.1210 - acc: 0.9500 - val_loss: 0.5559 - val_acc: 0.8500\n",
            "Epoch 247/300\n",
            "\n",
            "Epoch 00247: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.0631 - acc: 1.0000 - val_loss: 0.5238 - val_acc: 0.8500\n",
            "Epoch 248/300\n",
            "\n",
            "Epoch 00248: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.0553 - acc: 1.0000 - val_loss: 0.5012 - val_acc: 0.8500\n",
            "Epoch 249/300\n",
            "\n",
            "Epoch 00249: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.0498 - acc: 1.0000 - val_loss: 0.4752 - val_acc: 0.9000\n",
            "Epoch 250/300\n",
            "\n",
            "Epoch 00250: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.0430 - acc: 1.0000 - val_loss: 0.4535 - val_acc: 0.9000\n",
            "Epoch 251/300\n",
            "\n",
            "Epoch 00251: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.0593 - acc: 0.9833 - val_loss: 0.4454 - val_acc: 0.9500\n",
            "Epoch 252/300\n",
            "\n",
            "Epoch 00252: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.0549 - acc: 1.0000 - val_loss: 0.4394 - val_acc: 0.9500\n",
            "Epoch 253/300\n",
            "\n",
            "Epoch 00253: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.0543 - acc: 1.0000 - val_loss: 0.4291 - val_acc: 0.9500\n",
            "Epoch 254/300\n",
            "\n",
            "Epoch 00254: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.0647 - acc: 0.9667 - val_loss: 0.4364 - val_acc: 0.9500\n",
            "Epoch 255/300\n",
            "\n",
            "Epoch 00255: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.0712 - acc: 0.9833 - val_loss: 0.4564 - val_acc: 0.9500\n",
            "Epoch 256/300\n",
            "\n",
            "Epoch 00256: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.0793 - acc: 0.9833 - val_loss: 0.4485 - val_acc: 0.9000\n",
            "Epoch 257/300\n",
            "\n",
            "Epoch 00257: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.0313 - acc: 1.0000 - val_loss: 0.4457 - val_acc: 0.9000\n",
            "Epoch 258/300\n",
            "\n",
            "Epoch 00258: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.0340 - acc: 1.0000 - val_loss: 0.4457 - val_acc: 0.9000\n",
            "Epoch 259/300\n",
            "\n",
            "Epoch 00259: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.0255 - acc: 1.0000 - val_loss: 0.4462 - val_acc: 0.9000\n",
            "Epoch 260/300\n",
            "\n",
            "Epoch 00260: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.0734 - acc: 0.9667 - val_loss: 0.4515 - val_acc: 0.9000\n",
            "Epoch 261/300\n",
            "\n",
            "Epoch 00261: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.0416 - acc: 1.0000 - val_loss: 0.4653 - val_acc: 0.9000\n",
            "Epoch 262/300\n",
            "\n",
            "Epoch 00262: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.0493 - acc: 1.0000 - val_loss: 0.4654 - val_acc: 0.9000\n",
            "Epoch 263/300\n",
            "\n",
            "Epoch 00263: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.0620 - acc: 0.9833 - val_loss: 0.4659 - val_acc: 0.9000\n",
            "Epoch 264/300\n",
            "\n",
            "Epoch 00264: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.0371 - acc: 1.0000 - val_loss: 0.4777 - val_acc: 0.9000\n",
            "Epoch 265/300\n",
            "\n",
            "Epoch 00265: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.0361 - acc: 0.9833 - val_loss: 0.4876 - val_acc: 0.9000\n",
            "Epoch 266/300\n",
            "\n",
            "Epoch 00266: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.0474 - acc: 0.9833 - val_loss: 0.4912 - val_acc: 0.9000\n",
            "Epoch 267/300\n",
            "\n",
            "Epoch 00267: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.0477 - acc: 1.0000 - val_loss: 0.4876 - val_acc: 0.9000\n",
            "Epoch 268/300\n",
            "\n",
            "Epoch 00268: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.0440 - acc: 1.0000 - val_loss: 0.4754 - val_acc: 0.9000\n",
            "Epoch 269/300\n",
            "\n",
            "Epoch 00269: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.0819 - acc: 0.9833 - val_loss: 0.4644 - val_acc: 0.9000\n",
            "Epoch 270/300\n",
            "\n",
            "Epoch 00270: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.0519 - acc: 1.0000 - val_loss: 0.4622 - val_acc: 0.9000\n",
            "Epoch 271/300\n",
            "\n",
            "Epoch 00271: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.0704 - acc: 0.9833 - val_loss: 0.4716 - val_acc: 0.9500\n",
            "Epoch 272/300\n",
            "\n",
            "Epoch 00272: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.0471 - acc: 1.0000 - val_loss: 0.4881 - val_acc: 0.9500\n",
            "Epoch 273/300\n",
            "\n",
            "Epoch 00273: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.0601 - acc: 1.0000 - val_loss: 0.4869 - val_acc: 0.9500\n",
            "Epoch 274/300\n",
            "\n",
            "Epoch 00274: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.0327 - acc: 1.0000 - val_loss: 0.4774 - val_acc: 0.9000\n",
            "Epoch 275/300\n",
            "\n",
            "Epoch 00275: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.0404 - acc: 1.0000 - val_loss: 0.4788 - val_acc: 0.9000\n",
            "Epoch 276/300\n",
            "\n",
            "Epoch 00276: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.0338 - acc: 1.0000 - val_loss: 0.4826 - val_acc: 0.9000\n",
            "Epoch 277/300\n",
            "\n",
            "Epoch 00277: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.0312 - acc: 1.0000 - val_loss: 0.4930 - val_acc: 0.9000\n",
            "Epoch 278/300\n",
            "\n",
            "Epoch 00278: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.1076 - acc: 0.9667 - val_loss: 0.4922 - val_acc: 0.9000\n",
            "Epoch 279/300\n",
            "\n",
            "Epoch 00279: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.1113 - acc: 0.9667 - val_loss: 0.5056 - val_acc: 0.8500\n",
            "Epoch 280/300\n",
            "\n",
            "Epoch 00280: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.0549 - acc: 1.0000 - val_loss: 0.5094 - val_acc: 0.8500\n",
            "Epoch 281/300\n",
            "\n",
            "Epoch 00281: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.0896 - acc: 0.9667 - val_loss: 0.5083 - val_acc: 0.8500\n",
            "Epoch 282/300\n",
            "\n",
            "Epoch 00282: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.0637 - acc: 1.0000 - val_loss: 0.4902 - val_acc: 0.8500\n",
            "Epoch 283/300\n",
            "\n",
            "Epoch 00283: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.0989 - acc: 0.9833 - val_loss: 0.4735 - val_acc: 0.8500\n",
            "Epoch 284/300\n",
            "\n",
            "Epoch 00284: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.0491 - acc: 1.0000 - val_loss: 0.4561 - val_acc: 0.8500\n",
            "Epoch 285/300\n",
            "\n",
            "Epoch 00285: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.1102 - acc: 0.9500 - val_loss: 0.4352 - val_acc: 0.9000\n",
            "Epoch 286/300\n",
            "\n",
            "Epoch 00286: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.0758 - acc: 0.9833 - val_loss: 0.4247 - val_acc: 0.9500\n",
            "Epoch 287/300\n",
            "\n",
            "Epoch 00287: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.0545 - acc: 1.0000 - val_loss: 0.4181 - val_acc: 0.9000\n",
            "Epoch 288/300\n",
            "\n",
            "Epoch 00288: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.0426 - acc: 1.0000 - val_loss: 0.4150 - val_acc: 0.9000\n",
            "Epoch 289/300\n",
            "\n",
            "Epoch 00289: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.0471 - acc: 1.0000 - val_loss: 0.4142 - val_acc: 0.9000\n",
            "Epoch 290/300\n",
            "\n",
            "Epoch 00290: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.0383 - acc: 1.0000 - val_loss: 0.4164 - val_acc: 0.9000\n",
            "Epoch 291/300\n",
            "\n",
            "Epoch 00291: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.0365 - acc: 1.0000 - val_loss: 0.4221 - val_acc: 0.9000\n",
            "Epoch 292/300\n",
            "\n",
            "Epoch 00292: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.0583 - acc: 1.0000 - val_loss: 0.4285 - val_acc: 0.9500\n",
            "Epoch 293/300\n",
            "\n",
            "Epoch 00293: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.0373 - acc: 1.0000 - val_loss: 0.4328 - val_acc: 0.9000\n",
            "Epoch 294/300\n",
            "\n",
            "Epoch 00294: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.0516 - acc: 1.0000 - val_loss: 0.4367 - val_acc: 0.9500\n",
            "Epoch 295/300\n",
            "\n",
            "Epoch 00295: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.0377 - acc: 1.0000 - val_loss: 0.4386 - val_acc: 0.9000\n",
            "Epoch 296/300\n",
            "\n",
            "Epoch 00296: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.0281 - acc: 1.0000 - val_loss: 0.4388 - val_acc: 0.9500\n",
            "Epoch 297/300\n",
            "\n",
            "Epoch 00297: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.1075 - acc: 0.9833 - val_loss: 0.4476 - val_acc: 0.9500\n",
            "Epoch 298/300\n",
            "\n",
            "Epoch 00298: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.0407 - acc: 1.0000 - val_loss: 0.4742 - val_acc: 0.9500\n",
            "Epoch 299/300\n",
            "\n",
            "Epoch 00299: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.0443 - acc: 1.0000 - val_loss: 0.4920 - val_acc: 0.9000\n",
            "Epoch 300/300\n",
            "\n",
            "Epoch 00300: val_loss did not improve from 0.38607\n",
            "60/60 - 0s - loss: 0.0552 - acc: 0.9833 - val_loss: 0.5016 - val_acc: 0.9000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkcj9P3OfwUK7yLGmAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "# Evaluate on test data\n",
            "40/40 [==============================] - 0s 2ms/sample - loss: 0.8628 - acc: 0.7250\n",
            "test loss, test acc: [0.8627746537022176, 0.725]\n",
            "EEG_Deep/Data2A/parsed_P07T.mat\n",
            "Filtering of Training Data Finished\n",
            "EEG_Deep/Data2A/parsed_P07E.mat\n",
            "Filtering of Testing Data Finished\n",
            "(1, 2)\n",
            "Finding labels in training data\n",
            "Finding labels in testing data\n",
            "[2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1]\n",
            "(60, 12, 1536)\n",
            "(60,)\n",
            "(20, 12, 1536)\n",
            "(20,)\n",
            "[[0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]]\n",
            "X_train shape: (60, 1, 12, 1536)\n",
            "60 train samples\n",
            "20 val samples\n",
            "(40, 12, 1536)\n",
            "(40, 1)\n",
            "X_train shape: (40, 1, 12, 1536)\n",
            "40 train samples\n",
            "Train on 60 samples, validate on 20 samples\n",
            "Epoch 1/300\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 1.36783, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 1s - loss: 1.3521 - acc: 0.3500 - val_loss: 1.3678 - val_acc: 0.5500\n",
            "Epoch 2/300\n",
            "\n",
            "Epoch 00002: val_loss improved from 1.36783 to 1.34309, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 1.0852 - acc: 0.6000 - val_loss: 1.3431 - val_acc: 0.6000\n",
            "Epoch 3/300\n",
            "\n",
            "Epoch 00003: val_loss improved from 1.34309 to 1.31858, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.9495 - acc: 0.5833 - val_loss: 1.3186 - val_acc: 0.6000\n",
            "Epoch 4/300\n",
            "\n",
            "Epoch 00004: val_loss improved from 1.31858 to 1.29416, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.8626 - acc: 0.6833 - val_loss: 1.2942 - val_acc: 0.7000\n",
            "Epoch 5/300\n",
            "\n",
            "Epoch 00005: val_loss improved from 1.29416 to 1.27066, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.8327 - acc: 0.6167 - val_loss: 1.2707 - val_acc: 0.7000\n",
            "Epoch 6/300\n",
            "\n",
            "Epoch 00006: val_loss improved from 1.27066 to 1.24894, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.7829 - acc: 0.7333 - val_loss: 1.2489 - val_acc: 0.6500\n",
            "Epoch 7/300\n",
            "\n",
            "Epoch 00007: val_loss improved from 1.24894 to 1.22775, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.7684 - acc: 0.7667 - val_loss: 1.2278 - val_acc: 0.6500\n",
            "Epoch 8/300\n",
            "\n",
            "Epoch 00008: val_loss improved from 1.22775 to 1.20779, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.7520 - acc: 0.8000 - val_loss: 1.2078 - val_acc: 0.6000\n",
            "Epoch 9/300\n",
            "\n",
            "Epoch 00009: val_loss improved from 1.20779 to 1.18876, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.7264 - acc: 0.8667 - val_loss: 1.1888 - val_acc: 0.5500\n",
            "Epoch 10/300\n",
            "\n",
            "Epoch 00010: val_loss improved from 1.18876 to 1.17131, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.7145 - acc: 0.8500 - val_loss: 1.1713 - val_acc: 0.5500\n",
            "Epoch 11/300\n",
            "\n",
            "Epoch 00011: val_loss improved from 1.17131 to 1.15413, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.7030 - acc: 0.8667 - val_loss: 1.1541 - val_acc: 0.5500\n",
            "Epoch 12/300\n",
            "\n",
            "Epoch 00012: val_loss improved from 1.15413 to 1.13821, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6936 - acc: 0.8833 - val_loss: 1.1382 - val_acc: 0.5500\n",
            "Epoch 13/300\n",
            "\n",
            "Epoch 00013: val_loss improved from 1.13821 to 1.12385, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6842 - acc: 0.8500 - val_loss: 1.1239 - val_acc: 0.5500\n",
            "Epoch 14/300\n",
            "\n",
            "Epoch 00014: val_loss improved from 1.12385 to 1.11008, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6655 - acc: 0.8667 - val_loss: 1.1101 - val_acc: 0.6000\n",
            "Epoch 15/300\n",
            "\n",
            "Epoch 00015: val_loss improved from 1.11008 to 1.09613, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6553 - acc: 0.8500 - val_loss: 1.0961 - val_acc: 0.6000\n",
            "Epoch 16/300\n",
            "\n",
            "Epoch 00016: val_loss improved from 1.09613 to 1.08318, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6395 - acc: 0.8667 - val_loss: 1.0832 - val_acc: 0.6000\n",
            "Epoch 17/300\n",
            "\n",
            "Epoch 00017: val_loss improved from 1.08318 to 1.07056, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6352 - acc: 0.8000 - val_loss: 1.0706 - val_acc: 0.6000\n",
            "Epoch 18/300\n",
            "\n",
            "Epoch 00018: val_loss improved from 1.07056 to 1.05797, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6163 - acc: 0.8667 - val_loss: 1.0580 - val_acc: 0.6000\n",
            "Epoch 19/300\n",
            "\n",
            "Epoch 00019: val_loss improved from 1.05797 to 1.04582, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6291 - acc: 0.8667 - val_loss: 1.0458 - val_acc: 0.5500\n",
            "Epoch 20/300\n",
            "\n",
            "Epoch 00020: val_loss improved from 1.04582 to 1.03443, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6018 - acc: 0.8667 - val_loss: 1.0344 - val_acc: 0.5500\n",
            "Epoch 21/300\n",
            "\n",
            "Epoch 00021: val_loss improved from 1.03443 to 1.02365, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5991 - acc: 0.8167 - val_loss: 1.0237 - val_acc: 0.5500\n",
            "Epoch 22/300\n",
            "\n",
            "Epoch 00022: val_loss improved from 1.02365 to 1.01463, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5716 - acc: 0.9000 - val_loss: 1.0146 - val_acc: 0.5500\n",
            "Epoch 23/300\n",
            "\n",
            "Epoch 00023: val_loss improved from 1.01463 to 1.00630, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5720 - acc: 0.8833 - val_loss: 1.0063 - val_acc: 0.5500\n",
            "Epoch 24/300\n",
            "\n",
            "Epoch 00024: val_loss improved from 1.00630 to 0.99715, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5802 - acc: 0.8833 - val_loss: 0.9971 - val_acc: 0.5500\n",
            "Epoch 25/300\n",
            "\n",
            "Epoch 00025: val_loss improved from 0.99715 to 0.99001, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5494 - acc: 0.8667 - val_loss: 0.9900 - val_acc: 0.5500\n",
            "Epoch 26/300\n",
            "\n",
            "Epoch 00026: val_loss improved from 0.99001 to 0.98519, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5373 - acc: 0.9167 - val_loss: 0.9852 - val_acc: 0.5500\n",
            "Epoch 27/300\n",
            "\n",
            "Epoch 00027: val_loss improved from 0.98519 to 0.97913, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5367 - acc: 0.8833 - val_loss: 0.9791 - val_acc: 0.5000\n",
            "Epoch 28/300\n",
            "\n",
            "Epoch 00028: val_loss improved from 0.97913 to 0.97061, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5587 - acc: 0.8000 - val_loss: 0.9706 - val_acc: 0.6000\n",
            "Epoch 29/300\n",
            "\n",
            "Epoch 00029: val_loss improved from 0.97061 to 0.96319, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5209 - acc: 0.8667 - val_loss: 0.9632 - val_acc: 0.6000\n",
            "Epoch 30/300\n",
            "\n",
            "Epoch 00030: val_loss improved from 0.96319 to 0.95657, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5238 - acc: 0.9000 - val_loss: 0.9566 - val_acc: 0.6000\n",
            "Epoch 31/300\n",
            "\n",
            "Epoch 00031: val_loss improved from 0.95657 to 0.95170, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5333 - acc: 0.8500 - val_loss: 0.9517 - val_acc: 0.6000\n",
            "Epoch 32/300\n",
            "\n",
            "Epoch 00032: val_loss improved from 0.95170 to 0.94874, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5204 - acc: 0.8833 - val_loss: 0.9487 - val_acc: 0.5500\n",
            "Epoch 33/300\n",
            "\n",
            "Epoch 00033: val_loss improved from 0.94874 to 0.94821, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4904 - acc: 0.9000 - val_loss: 0.9482 - val_acc: 0.6000\n",
            "Epoch 34/300\n",
            "\n",
            "Epoch 00034: val_loss improved from 0.94821 to 0.94569, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5020 - acc: 0.9000 - val_loss: 0.9457 - val_acc: 0.6000\n",
            "Epoch 35/300\n",
            "\n",
            "Epoch 00035: val_loss improved from 0.94569 to 0.94299, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5070 - acc: 0.9000 - val_loss: 0.9430 - val_acc: 0.6000\n",
            "Epoch 36/300\n",
            "\n",
            "Epoch 00036: val_loss improved from 0.94299 to 0.93967, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4886 - acc: 0.9333 - val_loss: 0.9397 - val_acc: 0.6000\n",
            "Epoch 37/300\n",
            "\n",
            "Epoch 00037: val_loss improved from 0.93967 to 0.93409, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4851 - acc: 0.8833 - val_loss: 0.9341 - val_acc: 0.6000\n",
            "Epoch 38/300\n",
            "\n",
            "Epoch 00038: val_loss improved from 0.93409 to 0.92856, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4715 - acc: 0.9000 - val_loss: 0.9286 - val_acc: 0.6000\n",
            "Epoch 39/300\n",
            "\n",
            "Epoch 00039: val_loss improved from 0.92856 to 0.92384, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4613 - acc: 0.9167 - val_loss: 0.9238 - val_acc: 0.6000\n",
            "Epoch 40/300\n",
            "\n",
            "Epoch 00040: val_loss improved from 0.92384 to 0.91963, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4702 - acc: 0.9000 - val_loss: 0.9196 - val_acc: 0.6000\n",
            "Epoch 41/300\n",
            "\n",
            "Epoch 00041: val_loss improved from 0.91963 to 0.91762, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4484 - acc: 0.9167 - val_loss: 0.9176 - val_acc: 0.6000\n",
            "Epoch 42/300\n",
            "\n",
            "Epoch 00042: val_loss improved from 0.91762 to 0.91541, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4374 - acc: 0.9333 - val_loss: 0.9154 - val_acc: 0.6000\n",
            "Epoch 43/300\n",
            "\n",
            "Epoch 00043: val_loss improved from 0.91541 to 0.91337, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4582 - acc: 0.9500 - val_loss: 0.9134 - val_acc: 0.6000\n",
            "Epoch 44/300\n",
            "\n",
            "Epoch 00044: val_loss improved from 0.91337 to 0.91193, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4211 - acc: 0.9500 - val_loss: 0.9119 - val_acc: 0.6000\n",
            "Epoch 45/300\n",
            "\n",
            "Epoch 00045: val_loss improved from 0.91193 to 0.91155, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4446 - acc: 0.9333 - val_loss: 0.9115 - val_acc: 0.6000\n",
            "Epoch 46/300\n",
            "\n",
            "Epoch 00046: val_loss improved from 0.91155 to 0.91024, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4227 - acc: 0.9000 - val_loss: 0.9102 - val_acc: 0.6000\n",
            "Epoch 47/300\n",
            "\n",
            "Epoch 00047: val_loss improved from 0.91024 to 0.90942, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4241 - acc: 0.9500 - val_loss: 0.9094 - val_acc: 0.6000\n",
            "Epoch 48/300\n",
            "\n",
            "Epoch 00048: val_loss improved from 0.90942 to 0.90843, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3969 - acc: 0.9500 - val_loss: 0.9084 - val_acc: 0.6000\n",
            "Epoch 49/300\n",
            "\n",
            "Epoch 00049: val_loss improved from 0.90843 to 0.90544, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4376 - acc: 0.9500 - val_loss: 0.9054 - val_acc: 0.6000\n",
            "Epoch 50/300\n",
            "\n",
            "Epoch 00050: val_loss improved from 0.90544 to 0.90350, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4170 - acc: 0.9667 - val_loss: 0.9035 - val_acc: 0.6000\n",
            "Epoch 51/300\n",
            "\n",
            "Epoch 00051: val_loss improved from 0.90350 to 0.90150, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4330 - acc: 0.9333 - val_loss: 0.9015 - val_acc: 0.6500\n",
            "Epoch 52/300\n",
            "\n",
            "Epoch 00052: val_loss improved from 0.90150 to 0.89375, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3994 - acc: 0.9333 - val_loss: 0.8937 - val_acc: 0.6500\n",
            "Epoch 53/300\n",
            "\n",
            "Epoch 00053: val_loss improved from 0.89375 to 0.88588, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3885 - acc: 0.9167 - val_loss: 0.8859 - val_acc: 0.6500\n",
            "Epoch 54/300\n",
            "\n",
            "Epoch 00054: val_loss improved from 0.88588 to 0.88062, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4206 - acc: 0.9667 - val_loss: 0.8806 - val_acc: 0.6000\n",
            "Epoch 55/300\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.88062\n",
            "60/60 - 0s - loss: 0.4020 - acc: 0.9167 - val_loss: 0.8817 - val_acc: 0.6500\n",
            "Epoch 56/300\n",
            "\n",
            "Epoch 00056: val_loss improved from 0.88062 to 0.88048, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4158 - acc: 0.9500 - val_loss: 0.8805 - val_acc: 0.6500\n",
            "Epoch 57/300\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.88048\n",
            "60/60 - 0s - loss: 0.3908 - acc: 0.9667 - val_loss: 0.8813 - val_acc: 0.6500\n",
            "Epoch 58/300\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.88048\n",
            "60/60 - 0s - loss: 0.3599 - acc: 0.9667 - val_loss: 0.8829 - val_acc: 0.6500\n",
            "Epoch 59/300\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.88048\n",
            "60/60 - 0s - loss: 0.3762 - acc: 0.9500 - val_loss: 0.8812 - val_acc: 0.6500\n",
            "Epoch 60/300\n",
            "\n",
            "Epoch 00060: val_loss improved from 0.88048 to 0.87894, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3848 - acc: 0.9333 - val_loss: 0.8789 - val_acc: 0.6500\n",
            "Epoch 61/300\n",
            "\n",
            "Epoch 00061: val_loss improved from 0.87894 to 0.87698, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3645 - acc: 0.9667 - val_loss: 0.8770 - val_acc: 0.6000\n",
            "Epoch 62/300\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.3694 - acc: 0.9333 - val_loss: 0.8808 - val_acc: 0.6500\n",
            "Epoch 63/300\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.3948 - acc: 0.9167 - val_loss: 0.8832 - val_acc: 0.6500\n",
            "Epoch 64/300\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.3560 - acc: 0.9167 - val_loss: 0.8861 - val_acc: 0.6500\n",
            "Epoch 65/300\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.3695 - acc: 0.9667 - val_loss: 0.8845 - val_acc: 0.6000\n",
            "Epoch 66/300\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.3436 - acc: 0.9333 - val_loss: 0.8895 - val_acc: 0.6000\n",
            "Epoch 67/300\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.3415 - acc: 0.9500 - val_loss: 0.8942 - val_acc: 0.5500\n",
            "Epoch 68/300\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.3774 - acc: 0.9500 - val_loss: 0.8966 - val_acc: 0.5500\n",
            "Epoch 69/300\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.3445 - acc: 0.9500 - val_loss: 0.9036 - val_acc: 0.5500\n",
            "Epoch 70/300\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.3483 - acc: 0.9333 - val_loss: 0.9068 - val_acc: 0.5500\n",
            "Epoch 71/300\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.2934 - acc: 1.0000 - val_loss: 0.9086 - val_acc: 0.5500\n",
            "Epoch 72/300\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.3460 - acc: 0.9667 - val_loss: 0.9072 - val_acc: 0.5500\n",
            "Epoch 73/300\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.3327 - acc: 0.9500 - val_loss: 0.9093 - val_acc: 0.5500\n",
            "Epoch 74/300\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.3442 - acc: 0.9333 - val_loss: 0.9093 - val_acc: 0.4500\n",
            "Epoch 75/300\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.3167 - acc: 0.9667 - val_loss: 0.9081 - val_acc: 0.4500\n",
            "Epoch 76/300\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.3184 - acc: 0.9833 - val_loss: 0.9149 - val_acc: 0.4000\n",
            "Epoch 77/300\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.3292 - acc: 0.9333 - val_loss: 0.9149 - val_acc: 0.4000\n",
            "Epoch 78/300\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.3358 - acc: 0.9500 - val_loss: 0.9170 - val_acc: 0.4000\n",
            "Epoch 79/300\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.2852 - acc: 0.9833 - val_loss: 0.9239 - val_acc: 0.3500\n",
            "Epoch 80/300\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.2945 - acc: 1.0000 - val_loss: 0.9272 - val_acc: 0.3500\n",
            "Epoch 81/300\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.3277 - acc: 0.9333 - val_loss: 0.9333 - val_acc: 0.3500\n",
            "Epoch 82/300\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.3202 - acc: 0.9167 - val_loss: 0.9384 - val_acc: 0.3000\n",
            "Epoch 83/300\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.3079 - acc: 0.9667 - val_loss: 0.9438 - val_acc: 0.3000\n",
            "Epoch 84/300\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.3091 - acc: 0.9500 - val_loss: 0.9549 - val_acc: 0.3000\n",
            "Epoch 85/300\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.3350 - acc: 0.8667 - val_loss: 0.9486 - val_acc: 0.3000\n",
            "Epoch 86/300\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.2581 - acc: 0.9667 - val_loss: 0.9582 - val_acc: 0.3000\n",
            "Epoch 87/300\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.2765 - acc: 0.9500 - val_loss: 0.9628 - val_acc: 0.3000\n",
            "Epoch 88/300\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.3028 - acc: 0.9667 - val_loss: 0.9732 - val_acc: 0.3000\n",
            "Epoch 89/300\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.2901 - acc: 0.9333 - val_loss: 0.9985 - val_acc: 0.3000\n",
            "Epoch 90/300\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.2936 - acc: 0.9667 - val_loss: 1.0085 - val_acc: 0.3000\n",
            "Epoch 91/300\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.2731 - acc: 0.9500 - val_loss: 1.0285 - val_acc: 0.4000\n",
            "Epoch 92/300\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.2968 - acc: 0.9667 - val_loss: 1.0241 - val_acc: 0.4000\n",
            "Epoch 93/300\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.2826 - acc: 0.9500 - val_loss: 1.0256 - val_acc: 0.4000\n",
            "Epoch 94/300\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.3097 - acc: 0.9000 - val_loss: 1.0450 - val_acc: 0.4000\n",
            "Epoch 95/300\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.2598 - acc: 1.0000 - val_loss: 1.0528 - val_acc: 0.4000\n",
            "Epoch 96/300\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.2645 - acc: 0.9833 - val_loss: 1.0457 - val_acc: 0.4000\n",
            "Epoch 97/300\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.2579 - acc: 0.9333 - val_loss: 1.0539 - val_acc: 0.4000\n",
            "Epoch 98/300\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.2710 - acc: 0.9667 - val_loss: 1.0620 - val_acc: 0.4000\n",
            "Epoch 99/300\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.2675 - acc: 0.9333 - val_loss: 1.0631 - val_acc: 0.4000\n",
            "Epoch 100/300\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.2859 - acc: 0.9500 - val_loss: 1.0974 - val_acc: 0.4500\n",
            "Epoch 101/300\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.2437 - acc: 0.9667 - val_loss: 1.1474 - val_acc: 0.4500\n",
            "Epoch 102/300\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.2537 - acc: 1.0000 - val_loss: 1.1580 - val_acc: 0.4500\n",
            "Epoch 103/300\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.2510 - acc: 0.9667 - val_loss: 1.1654 - val_acc: 0.4500\n",
            "Epoch 104/300\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.2563 - acc: 0.9667 - val_loss: 1.2098 - val_acc: 0.4500\n",
            "Epoch 105/300\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.2279 - acc: 0.9833 - val_loss: 1.2051 - val_acc: 0.4500\n",
            "Epoch 106/300\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.2415 - acc: 0.9833 - val_loss: 1.1896 - val_acc: 0.4500\n",
            "Epoch 107/300\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.2720 - acc: 0.9667 - val_loss: 1.1866 - val_acc: 0.4500\n",
            "Epoch 108/300\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.2733 - acc: 0.9500 - val_loss: 1.2298 - val_acc: 0.4500\n",
            "Epoch 109/300\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.2186 - acc: 0.9833 - val_loss: 1.2714 - val_acc: 0.4500\n",
            "Epoch 110/300\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.2758 - acc: 0.9500 - val_loss: 1.2853 - val_acc: 0.5000\n",
            "Epoch 111/300\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.1932 - acc: 0.9833 - val_loss: 1.2717 - val_acc: 0.4500\n",
            "Epoch 112/300\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.2929 - acc: 0.9333 - val_loss: 1.2289 - val_acc: 0.4500\n",
            "Epoch 113/300\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.2343 - acc: 0.9833 - val_loss: 1.2118 - val_acc: 0.4500\n",
            "Epoch 114/300\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.2722 - acc: 0.9333 - val_loss: 1.1763 - val_acc: 0.4000\n",
            "Epoch 115/300\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.2228 - acc: 1.0000 - val_loss: 1.1527 - val_acc: 0.4500\n",
            "Epoch 116/300\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.2494 - acc: 0.9833 - val_loss: 1.1542 - val_acc: 0.4500\n",
            "Epoch 117/300\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.2833 - acc: 0.9333 - val_loss: 1.1577 - val_acc: 0.4500\n",
            "Epoch 118/300\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.2394 - acc: 0.9667 - val_loss: 1.1562 - val_acc: 0.4500\n",
            "Epoch 119/300\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.2245 - acc: 0.9667 - val_loss: 1.1653 - val_acc: 0.4500\n",
            "Epoch 120/300\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.2350 - acc: 0.9667 - val_loss: 1.1878 - val_acc: 0.4500\n",
            "Epoch 121/300\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.2346 - acc: 0.9500 - val_loss: 1.2073 - val_acc: 0.4500\n",
            "Epoch 122/300\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.2216 - acc: 0.9833 - val_loss: 1.2403 - val_acc: 0.4500\n",
            "Epoch 123/300\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.2349 - acc: 0.9500 - val_loss: 1.2249 - val_acc: 0.4500\n",
            "Epoch 124/300\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.1925 - acc: 0.9833 - val_loss: 1.2406 - val_acc: 0.4500\n",
            "Epoch 125/300\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.2061 - acc: 0.9333 - val_loss: 1.2393 - val_acc: 0.4500\n",
            "Epoch 126/300\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.1969 - acc: 0.9667 - val_loss: 1.1911 - val_acc: 0.4500\n",
            "Epoch 127/300\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.2494 - acc: 0.9167 - val_loss: 1.1725 - val_acc: 0.4500\n",
            "Epoch 128/300\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.2422 - acc: 0.9500 - val_loss: 1.1392 - val_acc: 0.4000\n",
            "Epoch 129/300\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.2042 - acc: 1.0000 - val_loss: 1.1157 - val_acc: 0.4000\n",
            "Epoch 130/300\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.2230 - acc: 0.9500 - val_loss: 1.0997 - val_acc: 0.4000\n",
            "Epoch 131/300\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.2149 - acc: 0.9500 - val_loss: 1.1011 - val_acc: 0.4000\n",
            "Epoch 132/300\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.1948 - acc: 0.9667 - val_loss: 1.1144 - val_acc: 0.4500\n",
            "Epoch 133/300\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.2320 - acc: 0.9500 - val_loss: 1.1116 - val_acc: 0.4500\n",
            "Epoch 134/300\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.2436 - acc: 0.9500 - val_loss: 1.1367 - val_acc: 0.4500\n",
            "Epoch 135/300\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.2565 - acc: 0.9167 - val_loss: 1.1217 - val_acc: 0.4000\n",
            "Epoch 136/300\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.2844 - acc: 0.9000 - val_loss: 1.1548 - val_acc: 0.4500\n",
            "Epoch 137/300\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.2102 - acc: 0.9667 - val_loss: 1.1019 - val_acc: 0.3000\n",
            "Epoch 138/300\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.1567 - acc: 0.9833 - val_loss: 1.0902 - val_acc: 0.3000\n",
            "Epoch 139/300\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.2060 - acc: 1.0000 - val_loss: 1.1053 - val_acc: 0.3000\n",
            "Epoch 140/300\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.2251 - acc: 0.9667 - val_loss: 1.0921 - val_acc: 0.3000\n",
            "Epoch 141/300\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.2317 - acc: 0.9667 - val_loss: 1.1005 - val_acc: 0.3500\n",
            "Epoch 142/300\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.1731 - acc: 0.9833 - val_loss: 1.0928 - val_acc: 0.3000\n",
            "Epoch 143/300\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.1899 - acc: 0.9833 - val_loss: 1.1161 - val_acc: 0.3500\n",
            "Epoch 144/300\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.1997 - acc: 0.9500 - val_loss: 1.1279 - val_acc: 0.3500\n",
            "Epoch 145/300\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.1948 - acc: 0.9500 - val_loss: 1.1264 - val_acc: 0.3500\n",
            "Epoch 146/300\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.2000 - acc: 0.9667 - val_loss: 1.1154 - val_acc: 0.3000\n",
            "Epoch 147/300\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.1530 - acc: 1.0000 - val_loss: 1.1233 - val_acc: 0.3500\n",
            "Epoch 148/300\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.2434 - acc: 0.9667 - val_loss: 1.1368 - val_acc: 0.3500\n",
            "Epoch 149/300\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.1767 - acc: 0.9667 - val_loss: 1.1301 - val_acc: 0.3500\n",
            "Epoch 150/300\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.1973 - acc: 0.9833 - val_loss: 1.1121 - val_acc: 0.3500\n",
            "Epoch 151/300\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.1647 - acc: 0.9667 - val_loss: 1.0918 - val_acc: 0.3500\n",
            "Epoch 152/300\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.1676 - acc: 0.9833 - val_loss: 1.1071 - val_acc: 0.4500\n",
            "Epoch 153/300\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.2013 - acc: 0.9833 - val_loss: 1.1279 - val_acc: 0.4500\n",
            "Epoch 154/300\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.1922 - acc: 0.9667 - val_loss: 1.1407 - val_acc: 0.4500\n",
            "Epoch 155/300\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.1804 - acc: 0.9833 - val_loss: 1.1733 - val_acc: 0.4500\n",
            "Epoch 156/300\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.2058 - acc: 0.9500 - val_loss: 1.2065 - val_acc: 0.4500\n",
            "Epoch 157/300\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.1589 - acc: 1.0000 - val_loss: 1.2102 - val_acc: 0.4500\n",
            "Epoch 158/300\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.1741 - acc: 0.9500 - val_loss: 1.2561 - val_acc: 0.4500\n",
            "Epoch 159/300\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.2020 - acc: 0.9500 - val_loss: 1.2918 - val_acc: 0.4500\n",
            "Epoch 160/300\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.1662 - acc: 0.9667 - val_loss: 1.2916 - val_acc: 0.4500\n",
            "Epoch 161/300\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.1789 - acc: 1.0000 - val_loss: 1.3200 - val_acc: 0.5000\n",
            "Epoch 162/300\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.1503 - acc: 0.9833 - val_loss: 1.3267 - val_acc: 0.5000\n",
            "Epoch 163/300\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.1658 - acc: 0.9667 - val_loss: 1.3766 - val_acc: 0.5000\n",
            "Epoch 164/300\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.2008 - acc: 0.9833 - val_loss: 1.3745 - val_acc: 0.5000\n",
            "Epoch 165/300\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.1617 - acc: 0.9833 - val_loss: 1.3830 - val_acc: 0.5000\n",
            "Epoch 166/300\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.1671 - acc: 0.9833 - val_loss: 1.3715 - val_acc: 0.5000\n",
            "Epoch 167/300\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.1934 - acc: 0.9500 - val_loss: 1.3552 - val_acc: 0.4500\n",
            "Epoch 168/300\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.1448 - acc: 0.9833 - val_loss: 1.3344 - val_acc: 0.4500\n",
            "Epoch 169/300\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.1740 - acc: 0.9833 - val_loss: 1.3737 - val_acc: 0.5000\n",
            "Epoch 170/300\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.2114 - acc: 0.9167 - val_loss: 1.4198 - val_acc: 0.5000\n",
            "Epoch 171/300\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.1643 - acc: 1.0000 - val_loss: 1.4049 - val_acc: 0.5000\n",
            "Epoch 172/300\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.1529 - acc: 0.9833 - val_loss: 1.2963 - val_acc: 0.4500\n",
            "Epoch 173/300\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.2151 - acc: 0.9333 - val_loss: 1.2292 - val_acc: 0.4500\n",
            "Epoch 174/300\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.1706 - acc: 0.9833 - val_loss: 1.1768 - val_acc: 0.3500\n",
            "Epoch 175/300\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.1345 - acc: 1.0000 - val_loss: 1.1713 - val_acc: 0.3500\n",
            "Epoch 176/300\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.1878 - acc: 0.9833 - val_loss: 1.1908 - val_acc: 0.3500\n",
            "Epoch 177/300\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.2040 - acc: 0.9500 - val_loss: 1.1871 - val_acc: 0.3000\n",
            "Epoch 178/300\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.1306 - acc: 1.0000 - val_loss: 1.2418 - val_acc: 0.4000\n",
            "Epoch 179/300\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.1574 - acc: 0.9667 - val_loss: 1.2853 - val_acc: 0.4500\n",
            "Epoch 180/300\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.1444 - acc: 1.0000 - val_loss: 1.2905 - val_acc: 0.4500\n",
            "Epoch 181/300\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.1264 - acc: 1.0000 - val_loss: 1.2910 - val_acc: 0.4500\n",
            "Epoch 182/300\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.1494 - acc: 1.0000 - val_loss: 1.2620 - val_acc: 0.4500\n",
            "Epoch 183/300\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.1349 - acc: 0.9833 - val_loss: 1.2493 - val_acc: 0.4500\n",
            "Epoch 184/300\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.1986 - acc: 0.9500 - val_loss: 1.2665 - val_acc: 0.4500\n",
            "Epoch 185/300\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.2243 - acc: 0.9500 - val_loss: 1.3277 - val_acc: 0.4500\n",
            "Epoch 186/300\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.1328 - acc: 1.0000 - val_loss: 1.3081 - val_acc: 0.4500\n",
            "Epoch 187/300\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.1302 - acc: 1.0000 - val_loss: 1.2626 - val_acc: 0.4500\n",
            "Epoch 188/300\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.1483 - acc: 0.9833 - val_loss: 1.2133 - val_acc: 0.3500\n",
            "Epoch 189/300\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.1407 - acc: 0.9833 - val_loss: 1.2177 - val_acc: 0.3500\n",
            "Epoch 190/300\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.1631 - acc: 1.0000 - val_loss: 1.2154 - val_acc: 0.3500\n",
            "Epoch 191/300\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.1362 - acc: 0.9833 - val_loss: 1.2379 - val_acc: 0.4500\n",
            "Epoch 192/300\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.1696 - acc: 0.9667 - val_loss: 1.2613 - val_acc: 0.4500\n",
            "Epoch 193/300\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.1976 - acc: 0.9167 - val_loss: 1.2604 - val_acc: 0.4500\n",
            "Epoch 194/300\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.1739 - acc: 0.9667 - val_loss: 1.2291 - val_acc: 0.4500\n",
            "Epoch 195/300\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.1158 - acc: 0.9833 - val_loss: 1.2026 - val_acc: 0.4500\n",
            "Epoch 196/300\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.1526 - acc: 0.9667 - val_loss: 1.2030 - val_acc: 0.4500\n",
            "Epoch 197/300\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.1702 - acc: 0.9667 - val_loss: 1.2217 - val_acc: 0.4500\n",
            "Epoch 198/300\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.1264 - acc: 1.0000 - val_loss: 1.2095 - val_acc: 0.4000\n",
            "Epoch 199/300\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.1433 - acc: 1.0000 - val_loss: 1.2298 - val_acc: 0.4000\n",
            "Epoch 200/300\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.1371 - acc: 0.9833 - val_loss: 1.2487 - val_acc: 0.4500\n",
            "Epoch 201/300\n",
            "\n",
            "Epoch 00201: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.1701 - acc: 0.9667 - val_loss: 1.2596 - val_acc: 0.4000\n",
            "Epoch 202/300\n",
            "\n",
            "Epoch 00202: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.1280 - acc: 1.0000 - val_loss: 1.3049 - val_acc: 0.4500\n",
            "Epoch 203/300\n",
            "\n",
            "Epoch 00203: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.1262 - acc: 0.9333 - val_loss: 1.3399 - val_acc: 0.4500\n",
            "Epoch 204/300\n",
            "\n",
            "Epoch 00204: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.2062 - acc: 0.9333 - val_loss: 1.3811 - val_acc: 0.4500\n",
            "Epoch 205/300\n",
            "\n",
            "Epoch 00205: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.1225 - acc: 1.0000 - val_loss: 1.3365 - val_acc: 0.4500\n",
            "Epoch 206/300\n",
            "\n",
            "Epoch 00206: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.1662 - acc: 0.9833 - val_loss: 1.2935 - val_acc: 0.4500\n",
            "Epoch 207/300\n",
            "\n",
            "Epoch 00207: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.1329 - acc: 1.0000 - val_loss: 1.2848 - val_acc: 0.4500\n",
            "Epoch 208/300\n",
            "\n",
            "Epoch 00208: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.1583 - acc: 0.9667 - val_loss: 1.2802 - val_acc: 0.4500\n",
            "Epoch 209/300\n",
            "\n",
            "Epoch 00209: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.1247 - acc: 0.9833 - val_loss: 1.2725 - val_acc: 0.5000\n",
            "Epoch 210/300\n",
            "\n",
            "Epoch 00210: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.1443 - acc: 1.0000 - val_loss: 1.2794 - val_acc: 0.5000\n",
            "Epoch 211/300\n",
            "\n",
            "Epoch 00211: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.2087 - acc: 0.9667 - val_loss: 1.2786 - val_acc: 0.5000\n",
            "Epoch 212/300\n",
            "\n",
            "Epoch 00212: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.1297 - acc: 1.0000 - val_loss: 1.3442 - val_acc: 0.5000\n",
            "Epoch 213/300\n",
            "\n",
            "Epoch 00213: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.1285 - acc: 0.9667 - val_loss: 1.3592 - val_acc: 0.5000\n",
            "Epoch 214/300\n",
            "\n",
            "Epoch 00214: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.1570 - acc: 0.9833 - val_loss: 1.3430 - val_acc: 0.5000\n",
            "Epoch 215/300\n",
            "\n",
            "Epoch 00215: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.1087 - acc: 1.0000 - val_loss: 1.3260 - val_acc: 0.5000\n",
            "Epoch 216/300\n",
            "\n",
            "Epoch 00216: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.1792 - acc: 0.9333 - val_loss: 1.2644 - val_acc: 0.5000\n",
            "Epoch 217/300\n",
            "\n",
            "Epoch 00217: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.1194 - acc: 1.0000 - val_loss: 1.2515 - val_acc: 0.4500\n",
            "Epoch 218/300\n",
            "\n",
            "Epoch 00218: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.1348 - acc: 0.9833 - val_loss: 1.2614 - val_acc: 0.4500\n",
            "Epoch 219/300\n",
            "\n",
            "Epoch 00219: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.1899 - acc: 0.9500 - val_loss: 1.2137 - val_acc: 0.4500\n",
            "Epoch 220/300\n",
            "\n",
            "Epoch 00220: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.1376 - acc: 0.9667 - val_loss: 1.2102 - val_acc: 0.5000\n",
            "Epoch 221/300\n",
            "\n",
            "Epoch 00221: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.1430 - acc: 0.9667 - val_loss: 1.1362 - val_acc: 0.4500\n",
            "Epoch 222/300\n",
            "\n",
            "Epoch 00222: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.1197 - acc: 1.0000 - val_loss: 1.0953 - val_acc: 0.5000\n",
            "Epoch 223/300\n",
            "\n",
            "Epoch 00223: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.1235 - acc: 0.9833 - val_loss: 1.0813 - val_acc: 0.4500\n",
            "Epoch 224/300\n",
            "\n",
            "Epoch 00224: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.1053 - acc: 0.9833 - val_loss: 1.1179 - val_acc: 0.4500\n",
            "Epoch 225/300\n",
            "\n",
            "Epoch 00225: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.1289 - acc: 1.0000 - val_loss: 1.1896 - val_acc: 0.4500\n",
            "Epoch 226/300\n",
            "\n",
            "Epoch 00226: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.1435 - acc: 1.0000 - val_loss: 1.2606 - val_acc: 0.5000\n",
            "Epoch 227/300\n",
            "\n",
            "Epoch 00227: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.1631 - acc: 0.9667 - val_loss: 1.2767 - val_acc: 0.5000\n",
            "Epoch 228/300\n",
            "\n",
            "Epoch 00228: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.1305 - acc: 0.9833 - val_loss: 1.2508 - val_acc: 0.5000\n",
            "Epoch 229/300\n",
            "\n",
            "Epoch 00229: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.2195 - acc: 0.9167 - val_loss: 1.2410 - val_acc: 0.4500\n",
            "Epoch 230/300\n",
            "\n",
            "Epoch 00230: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.1385 - acc: 0.9667 - val_loss: 1.2243 - val_acc: 0.4000\n",
            "Epoch 231/300\n",
            "\n",
            "Epoch 00231: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.1292 - acc: 0.9833 - val_loss: 1.2058 - val_acc: 0.4000\n",
            "Epoch 232/300\n",
            "\n",
            "Epoch 00232: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.1550 - acc: 1.0000 - val_loss: 1.1747 - val_acc: 0.4500\n",
            "Epoch 233/300\n",
            "\n",
            "Epoch 00233: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.1123 - acc: 1.0000 - val_loss: 1.1411 - val_acc: 0.4500\n",
            "Epoch 234/300\n",
            "\n",
            "Epoch 00234: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.1497 - acc: 0.9833 - val_loss: 1.1253 - val_acc: 0.4500\n",
            "Epoch 235/300\n",
            "\n",
            "Epoch 00235: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.1596 - acc: 0.9667 - val_loss: 1.1500 - val_acc: 0.4500\n",
            "Epoch 236/300\n",
            "\n",
            "Epoch 00236: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.1199 - acc: 0.9833 - val_loss: 1.2238 - val_acc: 0.5000\n",
            "Epoch 237/300\n",
            "\n",
            "Epoch 00237: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.1213 - acc: 0.9833 - val_loss: 1.3228 - val_acc: 0.5000\n",
            "Epoch 238/300\n",
            "\n",
            "Epoch 00238: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.1063 - acc: 1.0000 - val_loss: 1.3808 - val_acc: 0.5000\n",
            "Epoch 239/300\n",
            "\n",
            "Epoch 00239: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.1380 - acc: 0.9833 - val_loss: 1.3866 - val_acc: 0.5000\n",
            "Epoch 240/300\n",
            "\n",
            "Epoch 00240: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.1648 - acc: 0.9500 - val_loss: 1.3267 - val_acc: 0.5000\n",
            "Epoch 241/300\n",
            "\n",
            "Epoch 00241: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.1105 - acc: 0.9833 - val_loss: 1.2006 - val_acc: 0.5000\n",
            "Epoch 242/300\n",
            "\n",
            "Epoch 00242: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.1086 - acc: 1.0000 - val_loss: 1.1875 - val_acc: 0.5000\n",
            "Epoch 243/300\n",
            "\n",
            "Epoch 00243: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.1073 - acc: 0.9833 - val_loss: 1.2041 - val_acc: 0.5000\n",
            "Epoch 244/300\n",
            "\n",
            "Epoch 00244: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.1120 - acc: 0.9833 - val_loss: 1.2388 - val_acc: 0.5000\n",
            "Epoch 245/300\n",
            "\n",
            "Epoch 00245: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.1380 - acc: 0.9833 - val_loss: 1.3329 - val_acc: 0.5000\n",
            "Epoch 246/300\n",
            "\n",
            "Epoch 00246: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.1048 - acc: 1.0000 - val_loss: 1.3997 - val_acc: 0.5000\n",
            "Epoch 247/300\n",
            "\n",
            "Epoch 00247: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.1146 - acc: 1.0000 - val_loss: 1.4122 - val_acc: 0.5000\n",
            "Epoch 248/300\n",
            "\n",
            "Epoch 00248: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.1497 - acc: 0.9500 - val_loss: 1.4301 - val_acc: 0.5000\n",
            "Epoch 249/300\n",
            "\n",
            "Epoch 00249: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.1697 - acc: 0.9667 - val_loss: 1.3947 - val_acc: 0.5000\n",
            "Epoch 250/300\n",
            "\n",
            "Epoch 00250: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.1129 - acc: 1.0000 - val_loss: 1.3901 - val_acc: 0.5000\n",
            "Epoch 251/300\n",
            "\n",
            "Epoch 00251: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.0908 - acc: 1.0000 - val_loss: 1.3645 - val_acc: 0.5000\n",
            "Epoch 252/300\n",
            "\n",
            "Epoch 00252: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.1021 - acc: 1.0000 - val_loss: 1.3408 - val_acc: 0.5000\n",
            "Epoch 253/300\n",
            "\n",
            "Epoch 00253: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.1259 - acc: 0.9833 - val_loss: 1.2936 - val_acc: 0.4500\n",
            "Epoch 254/300\n",
            "\n",
            "Epoch 00254: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.0965 - acc: 1.0000 - val_loss: 1.2693 - val_acc: 0.5000\n",
            "Epoch 255/300\n",
            "\n",
            "Epoch 00255: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.1140 - acc: 0.9833 - val_loss: 1.2809 - val_acc: 0.5000\n",
            "Epoch 256/300\n",
            "\n",
            "Epoch 00256: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.1096 - acc: 0.9833 - val_loss: 1.3273 - val_acc: 0.5000\n",
            "Epoch 257/300\n",
            "\n",
            "Epoch 00257: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.1036 - acc: 1.0000 - val_loss: 1.2384 - val_acc: 0.4500\n",
            "Epoch 258/300\n",
            "\n",
            "Epoch 00258: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.1034 - acc: 1.0000 - val_loss: 1.1757 - val_acc: 0.4500\n",
            "Epoch 259/300\n",
            "\n",
            "Epoch 00259: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.1142 - acc: 0.9667 - val_loss: 1.1813 - val_acc: 0.4500\n",
            "Epoch 260/300\n",
            "\n",
            "Epoch 00260: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.1280 - acc: 0.9833 - val_loss: 1.1917 - val_acc: 0.4500\n",
            "Epoch 261/300\n",
            "\n",
            "Epoch 00261: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.1647 - acc: 0.9333 - val_loss: 1.1710 - val_acc: 0.3500\n",
            "Epoch 262/300\n",
            "\n",
            "Epoch 00262: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.1333 - acc: 1.0000 - val_loss: 1.1627 - val_acc: 0.4000\n",
            "Epoch 263/300\n",
            "\n",
            "Epoch 00263: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.1269 - acc: 0.9833 - val_loss: 1.1878 - val_acc: 0.4500\n",
            "Epoch 264/300\n",
            "\n",
            "Epoch 00264: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.1164 - acc: 1.0000 - val_loss: 1.2429 - val_acc: 0.4000\n",
            "Epoch 265/300\n",
            "\n",
            "Epoch 00265: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.1417 - acc: 0.9833 - val_loss: 1.3242 - val_acc: 0.4500\n",
            "Epoch 266/300\n",
            "\n",
            "Epoch 00266: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.1258 - acc: 0.9833 - val_loss: 1.1965 - val_acc: 0.4000\n",
            "Epoch 267/300\n",
            "\n",
            "Epoch 00267: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.1045 - acc: 1.0000 - val_loss: 1.1476 - val_acc: 0.4500\n",
            "Epoch 268/300\n",
            "\n",
            "Epoch 00268: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.1044 - acc: 0.9833 - val_loss: 1.1106 - val_acc: 0.4500\n",
            "Epoch 269/300\n",
            "\n",
            "Epoch 00269: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.1186 - acc: 0.9833 - val_loss: 1.1401 - val_acc: 0.5000\n",
            "Epoch 270/300\n",
            "\n",
            "Epoch 00270: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.1091 - acc: 0.9833 - val_loss: 1.3372 - val_acc: 0.5000\n",
            "Epoch 271/300\n",
            "\n",
            "Epoch 00271: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.1126 - acc: 0.9833 - val_loss: 1.6496 - val_acc: 0.5000\n",
            "Epoch 272/300\n",
            "\n",
            "Epoch 00272: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.0999 - acc: 0.9833 - val_loss: 1.7095 - val_acc: 0.5000\n",
            "Epoch 273/300\n",
            "\n",
            "Epoch 00273: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.1185 - acc: 1.0000 - val_loss: 1.5391 - val_acc: 0.5000\n",
            "Epoch 274/300\n",
            "\n",
            "Epoch 00274: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.1089 - acc: 0.9833 - val_loss: 1.4592 - val_acc: 0.5000\n",
            "Epoch 275/300\n",
            "\n",
            "Epoch 00275: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.1619 - acc: 0.9500 - val_loss: 1.3927 - val_acc: 0.5000\n",
            "Epoch 276/300\n",
            "\n",
            "Epoch 00276: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.1123 - acc: 1.0000 - val_loss: 1.3541 - val_acc: 0.5000\n",
            "Epoch 277/300\n",
            "\n",
            "Epoch 00277: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.1187 - acc: 0.9833 - val_loss: 1.2943 - val_acc: 0.5000\n",
            "Epoch 278/300\n",
            "\n",
            "Epoch 00278: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.0916 - acc: 0.9833 - val_loss: 1.4041 - val_acc: 0.5000\n",
            "Epoch 279/300\n",
            "\n",
            "Epoch 00279: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.0797 - acc: 1.0000 - val_loss: 1.4822 - val_acc: 0.5000\n",
            "Epoch 280/300\n",
            "\n",
            "Epoch 00280: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.1152 - acc: 0.9667 - val_loss: 1.5083 - val_acc: 0.5000\n",
            "Epoch 281/300\n",
            "\n",
            "Epoch 00281: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.0732 - acc: 1.0000 - val_loss: 1.4969 - val_acc: 0.5000\n",
            "Epoch 282/300\n",
            "\n",
            "Epoch 00282: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.0966 - acc: 0.9833 - val_loss: 1.4107 - val_acc: 0.5000\n",
            "Epoch 283/300\n",
            "\n",
            "Epoch 00283: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.1160 - acc: 0.9833 - val_loss: 1.4412 - val_acc: 0.5000\n",
            "Epoch 284/300\n",
            "\n",
            "Epoch 00284: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.0951 - acc: 1.0000 - val_loss: 1.4663 - val_acc: 0.5000\n",
            "Epoch 285/300\n",
            "\n",
            "Epoch 00285: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.0972 - acc: 0.9833 - val_loss: 1.5418 - val_acc: 0.5000\n",
            "Epoch 286/300\n",
            "\n",
            "Epoch 00286: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.0814 - acc: 1.0000 - val_loss: 1.5796 - val_acc: 0.5000\n",
            "Epoch 287/300\n",
            "\n",
            "Epoch 00287: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.0789 - acc: 1.0000 - val_loss: 1.5745 - val_acc: 0.5000\n",
            "Epoch 288/300\n",
            "\n",
            "Epoch 00288: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.1470 - acc: 0.9500 - val_loss: 1.5601 - val_acc: 0.5000\n",
            "Epoch 289/300\n",
            "\n",
            "Epoch 00289: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.0953 - acc: 0.9833 - val_loss: 1.6074 - val_acc: 0.5000\n",
            "Epoch 290/300\n",
            "\n",
            "Epoch 00290: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.1080 - acc: 1.0000 - val_loss: 1.6294 - val_acc: 0.5000\n",
            "Epoch 291/300\n",
            "\n",
            "Epoch 00291: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.0899 - acc: 1.0000 - val_loss: 1.6982 - val_acc: 0.5000\n",
            "Epoch 292/300\n",
            "\n",
            "Epoch 00292: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.1215 - acc: 0.9833 - val_loss: 1.7503 - val_acc: 0.5000\n",
            "Epoch 293/300\n",
            "\n",
            "Epoch 00293: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.1100 - acc: 0.9833 - val_loss: 1.7808 - val_acc: 0.5000\n",
            "Epoch 294/300\n",
            "\n",
            "Epoch 00294: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.1132 - acc: 0.9833 - val_loss: 1.6100 - val_acc: 0.5000\n",
            "Epoch 295/300\n",
            "\n",
            "Epoch 00295: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.0820 - acc: 1.0000 - val_loss: 1.4542 - val_acc: 0.5000\n",
            "Epoch 296/300\n",
            "\n",
            "Epoch 00296: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.1063 - acc: 1.0000 - val_loss: 1.4350 - val_acc: 0.5000\n",
            "Epoch 297/300\n",
            "\n",
            "Epoch 00297: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.0873 - acc: 0.9833 - val_loss: 1.4643 - val_acc: 0.5000\n",
            "Epoch 298/300\n",
            "\n",
            "Epoch 00298: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.0786 - acc: 1.0000 - val_loss: 1.5271 - val_acc: 0.5000\n",
            "Epoch 299/300\n",
            "\n",
            "Epoch 00299: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.0903 - acc: 1.0000 - val_loss: 1.4845 - val_acc: 0.5000\n",
            "Epoch 300/300\n",
            "\n",
            "Epoch 00300: val_loss did not improve from 0.87698\n",
            "60/60 - 0s - loss: 0.1175 - acc: 0.9667 - val_loss: 1.5721 - val_acc: 0.5000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nOy9d5xcVfn4/z7TZ+vsZrMl2d0kJCEh\njTRAOhiqoCiiH0BEED7Ix4byQUV/qIii6NfyUcCCiCAqRVBEpSu9JYFseg/Jluym7O7Mlukz5/fH\nLXNndmZ3UmZL9rxfr3nN3HvPvfc5d2ae5zzPc4qQUqJQKBSK8YttpAVQKBQKxciiDIFCoVCMc5Qh\nUCgUinGOMgQKhUIxzlGGQKFQKMY5yhAoFArFOEcZAsW4QAgxVQghhRCOPMpeJYR4bTjkUihGA8oQ\nKEYdQoidQoioEKIqY/8qXZlPHRnJFIojE2UIFKOV94DLjA0hxHygaOTEGR3k49EoFAeKMgSK0cqD\nwJWW7U8Bf7AWEEKUCyH+IITYJ4TYJYS4RQhh04/ZhRA/FkLsF0LsAC7Icu7vhBDtQog2IcT3hBD2\nfAQTQvxFCNEhhAgIIV4RQsy1HPMKIX6iyxMQQrwmhPDqx04RQrwhhPALIVqEEFfp+18SQlxruUZa\naEr3gj4nhNgKbNX3/Vy/Ro8Q4h0hxKmW8nYhxDeEENuFEL368QYhxN1CiJ9k1OVJIcSX86m34shF\nGQLFaOUtoEwIcYyuoC8F/phR5k6gHDgKOB3NcFytH/tv4EJgEbAUuCTj3PuBODBDL3MOcC358TQw\nE6gG3gX+ZDn2Y2AJcBJQCXwVSAohpujn3QlMBBYCTXneD+DDwAnAHH17hX6NSuDPwF+EEB792I1o\n3tQHgDLg00AQeAC4zGIsq4Cz9PMV4xkppXqp16h6ATvRFNQtwA+A84DnAQcggamAHYgCcyznfQZ4\nSf/8H+B6y7Fz9HMdQA0QAbyW45cBL+qfrwJey1NWn37dcrSGVQg4Nku5rwN/y3GNl4BrLdtp99ev\n//4h5Og27gtsBi7KUW4jcLb++fPAUyP9favXyL9UvFExmnkQeAWYRkZYCKgCnMAuy75dwGT98ySg\nJeOYwRT93HYhhLHPllE+K7p3cjvwMbSWfdIijxvwANuznNqQY3++pMkmhLgJuAatnhKt5W8k1we7\n1wPAFWiG9Qrg54cgk+IIQYWGFKMWKeUutKTxB4C/ZhzeD8TQlLpBI9Cmf25HU4jWYwYtaB5BlZTS\np7/KpJRzGZrLgYvQPJZyNO8EQOgyhYHpWc5rybEfoJ/0RHhtljLmNMF6PuCrwMeBCimlDwjoMgx1\nrz8CFwkhjgWOAZ7IUU4xjlCGQDHauQYtLNJv3SmlTACPArcLIUr1GPyNpPIIjwJfFELUCyEqgJst\n57YDzwE/EUKUCSFsQojpQojT85CnFM2IdKIp7+9brpsE7gN+KoSYpCdtTxRCuNHyCGcJIT4uhHAI\nISYIIRbqpzYBFwshioQQM/Q6DyVDHNgHOIQQ30LzCAzuBb4rhJgpNBYIISboMrai5RceBB6XUoby\nqLPiCEcZAsWoRkq5XUq5MsfhL6C1pncAr6ElPe/Tj/0WeBZYjZbQzfQorgRcwAa0+PpjQF0eIv0B\nLczUpp/7Vsbxm4C1aMq2C/ghYJNSNqN5Nv+r728CjtXP+RlavmMPWujmTwzOs8AzwBZdljDpoaOf\nohnC54Ae4HeA13L8AWA+mjFQKBBSqoVpFIrxhBDiNDTPaYpUCkCB8ggUinGFEMIJ3ADcq4yAwkAZ\nAoVinCCEOAbwo4XA/m+ExVGMIlRoSKFQKMY5yiNQKBSKcc6YG1BWVVUlp06dOtJiKBQKxZjinXfe\n2S+lnJjt2JgzBFOnTmXlyly9CRUKhUKRDSHErlzHVGhIoVAoxjnKECgUCsU4RxkChUKhGOeMuRxB\nNmKxGK2trYTD4ZEWZdjweDzU19fjdDpHWhSFQjHGOSIMQWtrK6WlpUydOhXLtMJHLFJKOjs7aW1t\nZdq0aSMtjkKhGOMULDQkhLhPCLFXCLEux3EhhPiFEGKbEGKNEGLxwd4rHA4zYcKEcWEEAIQQTJgw\nYVx5QAqFonAUMkdwP9rKUrk4H225v5nAdcCvDuVm48UIGIy3+ioUisJRMEMgpXwFbbrdXFwE/EFq\nvAX4hBD5TAOsOMwkk5JHV7QQiScGLfevNe3s640c1D3ebe5mTas/67Gte3p5c3vnAV3PH4zy96a2\noQtmIKXkLytbCEUHryvAs+s72O3PPV3/Wzs62dzRO+R1tuzp5e0dqfq9t7+flzbvHfK8zDoGo3Ee\nWdFMMimJxpM8sqKZcCzBr1/ezk+f28zdL24jGI0DWj3vf/097vz3Vrr7owD8vamNLv1zNp5Z185P\nn9/C1j1and7c3ml+Xrmzi3VtAd5t7mZ1i/Y9rmsLsHJnF5s6esz6bdvbx2tb97MzSx07AmGeWdcO\nwN6eMD9/YSt/entg1/a+SJy7X9zGPa9sJ55ImvsTScnDy7U6G3V8dGULfZG4Weah5c38/IWt7OnR\nvOUnVrXx0+c289Pnt7Bzfz8vbtrLe/u1pS3+vXEPP31uM+t3B8zz39nVzdrWAE0tft5t7gZg/e5A\n2vdnYFwPoM0f4rn1HVrdesP8a007Xf2p768nHOOxd1oJhGLc9Z+t3PvqDpJJSTiW4JcvbeNXL203\n/3+JpOTeV3dw13+20hOODfgeC8FI5ggmkz6Hequ+rz2zoBDiOjSvgcbGxszDI05nZyfLli0DoKOj\nA7vdzsSJ2gC+5cuX43K5hrzG1Vdfzc0338ysWbMKKms21rYF+Orjayj1ODh/fnZb3NIV5HN/fpdl\ns6v53VXHHfA9Lv7lGwDsvOOCAcd++vwWVrf4eePry/K+3vf+tZHH3mll6oRijm3w5X3e5j29fOWx\nNbgcNi5aODlnuUAoxvV/fIdrT5nG/3fBnKxlvvrYGmbVlvLbK5cOes+fPb+FTR29vHjTGQD86qVt\n/L1pN+u+cy5Oe+622A+e2sQjK1toqCxicWMFj7/bxjefWMe0qhJ6wzG+9vhamruC3P1ialXKunIP\nFy+uZ8f+fm79xwYA3E4b58yp5YaHm/ji+2dw4zkDf2NSSm58dDXBaIJte3v55SeWcNNfVrOo0cdd\nly/mkl+/CcCM6hJ8XieP/c9J3PLEOsKxBJN9Xrbv6+Olr5zJHU9vYv3uAPGkZF9vhC3fOx+XQ6vj\ng2/t5O4Xt7PylrN48K1d3PmfbQAsm11DbbnHlOVfa3bz/57dDMCs2jJOP1r7L722bT83/3UtTruN\njy6pZ01rgK8+tobecJxrTplGc2eQr/91LQBOh+CaU6bx5UebMKZTa+7s56l1HZwzp4ZfXLqILz/S\nRE84zuY9vfzmk9p3+NFfab/TY+rKiCeSPH/j6XznyQ20dAd5M+P3+ZPnt/Ds+g7W3noO1z6wko3t\nPay59RzuffU97nllB589Yzq/fGk7x9b7eG5DB99/ahOXHtfAwys0tXfi9Als39fPj57R6jqtqpjz\n5tXy9nudfO9fGwFo7Q6Z5X1FTj554tScv5dDYUx0H5VS3iOlXCqlXGoo2NHEhAkTaGpqoqmpieuv\nv54vf/nL5rZhBKSUJJPJnNf4/e9/PyJGADSlB7A7kDvnsKZVazWFh/AahiJbS3y3P8Se3giJZP4T\nIBotxfW7ew7o/l19WqtqsJYxwOoWP1LmfibJpKQ9EKI9MPQCX539UTr7Up7Ubn+YSDzJpvbBvYmk\nrsHWtWnPfpXeQm1q6aZTl/+dXdq+V75yJsUuO016a91av6YWP6tatHKrWrJ7ZT2hOEH9u2lq9pNI\nSjp6wvSE42nltu3toz0QJhJPsGF3D+2BMG3+ELsDYaSUNLV00xOKmfff2J76fnb7tWe5usXPquaU\nHLsznuGqZj8OmzBlSe036u/Pvq3XEaC7P8qeQAQp4UeXLOD9s6t5el0H0XiSphY/O/b3m3Vrz/Id\nb+7oYdu+Prr7o6xp89MeCNORUW5VczfReJKN7b1ma31NS8CU612LfEZ9/7Yq5eF198dY1dyN3Saw\nCWu9tHeHTaSVH+z/eaiMpCFoI31N2XpS680eEWzbto05c+bwiU98grlz59Le3s51113H0qVLmTt3\nLrfddptZ9pRTTqGpqYl4PI7P5+Pmm2/m2GOP5cQTT2Tv3qHDCIdCv+5aG+50Npr0P1lDRVHOMvmw\nzuKGG3T0hEkkJfv78g87VZd5cl5vMPy60fMHY4OWM/6Ue3L8+bqCUWIJSUdgaJkDwRg94bhp6Dr0\n52xVXNkwWsmGMjVkamrxE9DlX9MaQAit7IJ6n6lEjPpNqypmVXNKEa1u8ZPMYnANmRY3+tgdCLN+\nd4BEUpq/DSt7esKsa+shmkgSCMVo7goSjSdZ19bD/r4o/dEER1UVp8kMmIrUCC8tbtQ8ucxn3NTi\n5+QZVcyqKU17RkYdjH2ZBmFVs58il52JpW78wZhZp7pyD4safETiWuOhtTvECxv3mPXNVPAASQlS\nwqMrWwjHkvr9UrLs74vQ2q0ZsKbmbhonaP+Llbu6zEaT8W41BJF4kmn6s/GHoqxq9rNkSgXz631p\n9ThqYjFnzJpIJJ6kutTNZJ8352/xcDCSoaEngc8LIR4GTgAC+lqyh8R3/rGeDQfYShyKOZPK+PYH\n81nXfCCbNm3iD3/4A0uXaq7nHXfcQWVlJfF4nDPPPJNLLrmEOXPSQw+BQIDTTz+dO+64gxtvvJH7\n7ruPm2++OdvlDwu9kdwtIwPjT2f8mQ4Uh00QT0qamv0cN7XS3B9LJNmr5x3aA2Fqyjy5LpGGoaCs\nLcZ86A5qLTd/cHCPwPhT5nomhvLY3xchGk+a4Y/B7hkIxagsdpnnNjX7ufLE3DIY3lNTS4BAMMaO\nff0IoZ03dYKmTILRBBNL3bgcNhY2+vjtKzsIxxLmPc+YNZHfv76TZ9d3IAT0hOPs2N/PjOqStHsZ\nns358+p4t9nP0+u0eHdfOE7mVPXxpDQVqSEDwFPrUn9fu9Gib/HzKeOZ6Yr58Xfa6I3EzXtZn3Ff\nRAvVnDu3lo6yMM9u6DDvv7rVjxCwqb2XUDRBU4u23dodYn9fhFUtfuZPLicQitEdjJl1qiv3gG50\nhNAU/P2v76TU7eCUGVXc+eI2Yolk1jDd/W/sNM9b1eLnvHl15ndn7G+yGNeHl7eY/xHjuTyzroOO\nnrB57zNmTeS9/f3s6YmwYXcPV588lUg8yaMrW4gnNI/ltKOrOKqqmBc27mVhg4+u/uig/89DpZDd\nRx8C3gRmCSFahRDXCCGuF0Jcrxd5Cm2t2W1o68t+tlCyjCTTp083jQDAQw89xOLFi1m8eDEbN25k\nw4YNA87xer2cf/75ACxZsoSdO3cWVMY+3UXusLjo4ViCz//5Xbbu6SWeSJqtm97wwBZiPhS7tTZH\nZit4X2/EjOF2BEL8a007H7zzNTPW+8b2/XzziXWs3NnFzY+vYU2rn5v+stp06zd19JgJUoO9PWE+\n8+BKdu7v5zMPrkwLkxgtZcMzyIYW4tA9gp4wyaTk639dywfvfI2n12rKzvqn3NMT5jcvb+ePb+3i\nnle2c+Gdr3L3i9t4eHkzv355u8ULidIbjpnJzVUtfl7ctJcP3fUaF975Kl98aBUrd3Zx0d2v85kH\nV9Kv12tzRw9v6snKZbOr2R0Is2VPKqxUqxvPRQ0+4knJuraA6TGcMatalzHCstnaZ6NuUmr1enXr\nPtM4LTumGqdd8IxhCCLxrMbfOJ5rn+n16AZVSmkqZuPYmbMn4nLYeHXrPj7353eJxpOsadVCcosa\nfSxs9OEPxli5q5uP/fpN/MEYy2ZXE09KXtu2n52dQbNOH//Nm6xrC7Cw0UdFkYtAKGrWqaZM85YA\nTps5EbtN0NETZkFDOXU+L1LCrU+u55EVzab8UyYUMX1iMe2BMBOKXSyo9/HQ281ceOerXHjnq3zz\n7+uw2wSnzZzIqha/+R13ZPGqjX2GrMZ38ub2TqKJpFbXBh/BaIIP/OJV9vdFWNTgY2FDBQALG33U\nlnuyXvtwUTCPQEp52RDHJfC5w33fg225F4ri4mLz89atW/n5z3/O8uXL8fl8XHHFFVnHAliTy3a7\nnXj84JRvvvRl8Qhe3LSXf65pJ5ZIcssFc0xl0BcZPKSSC6OnR1t3ejzYes+OQJgXNu5lbVuAtW0B\nbj5vNk+tbeePbzWTkJKHV7RQ6nHw2DutLKgvBzQXviMQ5qiJqRbumzs6eXb9HiaUuHl2/R4uPc7P\nmfqf0PAEugcJDTV3BekOxphdW8qmjl527O/joeWaknhqXQfnz69LM5odPWF+/fJ2fEUuwrGEHk+O\nUFHkpDsYI6o/u+5gzAwPGde+55UdvLe/n0nlXp5cvZsSj4PVLX5WA6fOrDLr+M4urQPeuXNreWHj\n3rSQixFCOrqmFICdnUH8oSgOm+Ck6RP4r6UN+ENRbjx7Fm/v6KKppZtLltSzOxDmoeXN9IRjzKwu\nQQioryhiTl0Zq03DnzJcC+rLOfGoCfxGl9mog4HRG0d7ztrzbekOEU8k6Y8kCMeSfHjhJPqjCerK\nPRxVVUJduYcXN+8D4IZlM816LWzwUVOm/Tb+74UtrNzVzdlzavifM6bzwsa9vKj3SLpkSQOVxS46\n+6LMmFjCRxfX09wZZKueyyhxOyj1aKPvv3LuLJZOqWDplAqaWvxc8b4poPfC/tPbzfzpbe3zvMll\n/PepR5GUkn+ubueM2dVMLHHxl5WtZv1qSj1cfryPWCLJq1v3Ue51ckxdGZN9XhoqvTzZtJvO/igX\nzK8jlkhSUeTic2fOYLLPy/uOqqTIks+ZUV3KxBI3Hzx2EsFInJnVpZw7t5byIiefOnEKH144ma6+\nKC9s3IOUsiBdx4+IkcVjhZ6eHkpLSykrK6O9vZ1nn32W884bbKjF8GD80ff2REgmJTabYIf+p64t\nS7VEHDaR1lUvX6SUpiHJbIlb8xK7A2FWt/g5qqqYHfv7aWr1p4VRAN7bHwQw47PZrtmecU53cKBH\nEBgkNGTEc8+fV8emjt70lm5GqxZg+XtddAdjpnEx5M/MeQRCUdN7Ma795o5OPnTsJM6eU8MXHlqV\nFura25M636j3MXVlAOzvS8lfpxsCwyB0BEJ0B2P4ipw47TZ+eMkCs+yChnKzfk2W91K3g6oSPcTU\n4DMNQV8kbnqMV500lVNnTuQ3r+xIq4OVo2tK2LKnD0iFA/f3RfGHNHnPnlPLBQtSPdNqyzzs6tTq\nFoklWdXsZ1pVMb4iF6UeJ0UuO69v66TM4+A3VywhqncSMGRvrCziR5ccmyaDr8iFPxhjT084rTfS\n586cAcAJR00w923qGBhG/uT7ppg9yj6yqN7cb4SFrPz57WaSUjPylx3fyFfPmw3Aip1ddPZHOXVm\nFZcen+rp+J2L5gFQUeSizZ8KXRW7Hdx52aIB1zfK15Z7CMe0nIyvaOheiAfKmOg1dKSwePFi5syZ\nw+zZs7nyyis5+eSTR1okIBXuiSaSdOkKcnWWJN+M6hL6Iwfea8gaWsjsC20o7VKPg9e37ac3EufK\nE6doMdnmblPhbtZDITs7NQPV1R+lqkT7Q2TG+w15jXOsiWHDaAzmETS1aEnHU4/WWuRPr9Pi62cd\nU2PK0x4IU+px6MfTU1tXnzw163W7+2Nmfc+ZW4PRsFvY4DOV+WZLyGdvbxg91M7Ozn7cDhtTq4rJ\nxMireJx2KoqcdPSECQRjlHsHzkO1qKGCTR1GjF0L27T5Q6xtC6RCTI0VZvmkxDRoxW4HE4pdOO2a\nUCdOn2Dew3gWp8xI9eoz8hAdPWGz3rXl7jR5rIq6NxKjqcXPIr07sN0mTM/v2AYfNpvA47RTWewy\nn1Nd+cCckq/IiT8YZXcgbNYpF9mOl7jzn7/LWh9fUeo847o1WeQD0p6bETYdjLpyL5A99HQ4UIbg\nMHPrrbdy0003ATBjxgyamprMY0IIHnzwQbZs2cLzzz/PE088wRVXXAHAa6+9xsKFC3E4HPj9KSV8\n6aWXcu+995rbUsoByTsrnX2RtJh5Zg+RbOdaW/kdejdAo5uhPxQzFev06hLTaCSTKTkMmYx9xudE\nUns3kp5lHgc94TjxRNI8tyMQwu2wMbu21OwKesrMKmZWl9DUkvIIjJBKs956BC2MAZqi9wej7O0N\n0xuODTjHH4ya9/NnJIutA5YMjKRjvU/7863f3cPR1aXMqC5hT0DzmjoCYWZUl1DksrOurQeP04ZN\ngNMuuHhxPe4syWPrs5xWVczR1Voox4gBGzIbBq47GDMVQHNnkIoilxbq0BWHYSSsyrCmzENHIEx3\nMJq15biwwUciKVm3O8CqZj/FLrtZR0OGhRnjMkxj7XZgswmqSz3YbYL5k8upK/dQ6nGYoTkjnAXa\n7wWg3R9ixz7dw9TrY2A1BFv39LGvN8LCxtT9jTj5IotMNWUeEkmJy2FLU74GFUVO4knJjr19adfP\nRrnXiceZ/l2VePIPlNSWperj86aet3HfbIYKoKLYOejxAffRDU6hEsbKEIwx9vZG2Lq3z9x+aHkz\nJ9/xH3OU5ZLvvcAJt/+b3nCMl7fsY8F3njOV3uoWP7O/+Yzpkhr0R+Jmv+3W7hB7eiLmCGKt90WY\nIpedSeUe+iIxpJQc9Y2nzEEv077+FDc+upoP3vUa//fCVv7rN2/xg6c38snfvc03/77OHHtg/Dn+\n50/v8j9/fBfQWqN15R5TqZd5HBxVVcKihgre3dWdFgIBzNAAQH2F9id8cvVuFt72PMff/m8Wf/f5\nAQnpNW0BZn3zGTbs7jG9g55wnL29YRbe9jzPb0j1gEkkJRt2B1jY4GOCHioBWDxFa7UbXlObP8Sk\nci+TdWOxqKGCWbVlzJtcTrHbwbH1PlwZvVD8wSht3SEmFLvwOO0snlKB22FjTl0Z1aUe00Ow9uiZ\n5POY9TaUXmZOoN7Spbeu3EN7IIw/GKMii5I0lOzqFj/rdge4aNFkU06jLlMmFFFV4jKNmWG8DAVZ\nX+FlTl0ZXped+ooi/eWlrtyTJvtM/fPtT23ku//cgMMmqC5N9wissr+xfT8Ax9anlP6SKRX68095\nKXUWJZstXm4o5N5IfEhFK4RIkwGgJI8WeqYskO4RNFQUYROplnwuGTMNYy6Mctm6uh4OVI5gjNEb\njhOOJczBRps7emnzh+iLxHl1q/ZH6o3EWdMaYF1bgL5InJ2dQRYWudiyp5dIPMmWPb3mnx60XkPz\nJpeztk07Z2JpqmUTCEa1WGuZh1KP04xTAvzutff45oVa11dj4EttmYe1bQF2B0Ls9ofY1Rnk2lOP\n0o6Ve9myp4/Xt+2nQm+trm0LMG9SOf97ztEsmVLB7NpSbDbBwkYfj6y0DjwfyCSfFyHg7R1aIvVL\nZ83k/17Yyp6e9Nj82zu6iMaTbN3bS3cwZnbje3HTXvoicV7avJez59SYzyKWkNSUaa3e+686jvc6\n+znrmBoztr6lo5ddnUE+tqSe60+fzupWPydOn4CUEpuumH50yQJauoN88nfLAa2boT8YY21bwIzz\n33j20XxsaT0ep9Yqn1DsZn9fhBnVJbyl16m6zINNaCEaqyHYurePC+bX8bXzZnPc1JSSrC33srYt\ngMtuM+9jZUKxC5fdxnv7+wnHkhxVVcz9V2t1PPuYGl1Wwf1XH8+qFj/ffGKd2Qo1FOQPLp6P4Vd+\n+4NzCMUSFLns9ITilHnSlaHLYaO1O0RjZRHf/8j8AV00P7p4MmUeBzc83GQmmydZfpvLZlfzu08t\nNUcXG/WH7GEdSFfI2Z5BJnddvoi/vdtm5j4OxBD4ipy4HDai8WSaB3b5CY0sbPBlDc9ZZawtc2c9\nnkl1qZtbPziHpRaDeDhRhmAMkZSSkN77xghppPrFa6MUT51Zxatb96fF1zsCIWjwma3hzFZFbyRO\nfYWX2bXaAJ7ZdVpLc3ZtKd3BGHZbiNpyj/kHsSZqM0NPm/f0EoolzDJt/hAtXVo4p07/4wajCaLx\nMHt7w7R0hbjihCnUVxRpvTh0FjWmhyeyUeZxUO514g/GKHE7+OL7Z/K7194b0MXVeGb+YIxAKMqk\nci9t/hAvbtJ6q1hHufbqvaKMup40o4qTZmjhDqP194w+p8yixgrm15czX49jW5laVUxDZRF2myCR\nlNSVeWgPhNi8p5fPnjEdgImlbiZaWsh15R7NEFh6QJW6tTp2B2OpVqT+HCuKXWZvqPRraD2GsnkE\nQgjKi5xmrqXc60yro8G8yeVmyNBI6BsegbWHVkOlpTVdkR5qK/U4qC3z0NwV5NSZVZwyM/0eAEUu\nB+fOrQVSvyur8rTZBMt0A2XWsWzwsItVIefzO5pdW8bcySkv+0BCQ0II6sq1hLf1eZd6nGlJ6YEy\nGkY9P4/Aabdx1cmFm3JehYbGEOFYwox1xxJGzFtTXNv29tHaHeK0mROZPrE4Lb5utOiMnhuZcca+\niKZIFzb4WNMSMCdaO6auDH9Q649dW+4x/yBWQ5DZa6S1e+CUC2/pfeCt8dp4UvKfjVoXQGty0mBm\ndakZv7Z6L1ZK3A58utKoKXNrnoQeS852Tps/RCwhmaKPAjW6IG7e02vmVQzll00ZGIrHSB4vyGIA\nrNj1UIjXaae6zMOrW/eTSMoBMXgD4/nM1EM+AF6X3VRsmXHlbPFxw0jEkzLrcdBi6Dv1XkgVg/RA\nMYyh0aAozSOJ6rDb8OoeTonHkTPvYMXtsOGwCYLRBCVux6CD8yCVgM2ViLUq5FyhmcHOORCPAFLP\nvDzH885+P+2555sjKDTKEIxCIrEEXf0RM8kajSfp6o/QaYmXx/SWlxH/N2Z61AanVLDKMmLTMAiG\n0WjrDvH3pjYeWdHM35vaCOgt6kWNFfRG4ry+rROv005jZRE94bjZ+6LU9AhSCdu3sszKaFBX7sFh\nE1kNAWgK1Ug6ZmK3CbOlbcS1DQVjUOJxmkrS+MMbCsd4t56zUw89GD1vIvEkjZVFJJKS377yHr3h\nmDliOZsymFDixm4T7OvVWu2lnqH/+LXlHnxFTiqKnGbvqZyGQFcoU6uKzZG5RS67qdDLdY/AUIDW\n5KT1fgblOZS8z+sy5/fJZX6Duv4AACAASURBVCwg1ROoIxDGbhMDkqq5MIxoqduZtSdSJkII85xc\noRQrZo4gR2joQBSygfVZHrAhGOT7yIVRz6F6NQ0XyhCMQlr9IVq7QzR3aYprT0+Y1u4Q3cGoGWM1\nPQI9Xv+GPo3z3EnlLGr00dkfNUefGi06wxD8Y/Vubni4ia89vpYbHtZmYCzxOEwF9fq2/dSWe9Ja\nSZMrvFk9gtWWqaUbKr1pn886poYZ1SXmqORMQ/D6tv3MqinF60pX8AZnzKqmodLLiUdNoKbMbc7R\nYlDidgxIoJ529ERcdhsXLZyE0y44flpqOgtjbMT8yeVmgvRzZ07H67Tzsxe28JeVrWZYKZtHYLcJ\nMwF68oyBYY5szJ9czsyaUqZVaefNri1lQkn2uPC8yWVUlbipLnWbyqjIlfJ6jO9j7iRN/qlVA+d9\nmm5J1k6bMLCrKWjK3+g8NlifdKNbY3sgRInbkfdAJkP2Eo+DuZPKaKj0mnMPDXWO4fUMxszqUlwO\nG3OzNCAg1dr+Hz0Elw/G76jIZTeNcL7Mn1zO1AlFQ3oyVmbWlOKy25hZUzJ04WFA5QgOA4djGmqA\n++67j/PPP59QUvuDR+JJ4okk8aTE67QzZUIxDptgQ3uP6REY/fK37+ujqsSF12U3FbphLDJDQ9FE\nEpfdxtNfOpVlP3kZ0P6IR1UVm108a8s8aUri2Hqf2WPH6hEY0zf86JIFVJe6uer3KxACnvvS6bgc\nNj5133IzfFRd6jYTtaCFLwaL4V536lFce8o07DbBfx3XwKfvXwGAy24jmkhS6nGYf3qjZXXc1ErW\nfedcXA4b679zHr96aTsvb9FyATv2aXHguZPKWHHLWUTjSSaWujl/fh0n3P5v2vwhqvXkXWmOVuET\nnzuZrv5o3i25Wz84F4k2gPXaU6dRWZz7t/DxpQ1cvLgep91GidtBIBTD67SbdTSU1cIGH+tvyz6F\n9WSfl9XfOodYMklVDoNj9QIG8wgM5ZyUB9ZKNsoWu+1cd9pRfPqUadiGUK7GOfm0qmvLPay79dyc\nitdpt7H19vPNnnD5YDyHA/UGAD598jSuOmnqAZ0z2Hc4EowOKcY4+UxDnQ/33Xcfu1rbSEppKoxg\nNEEimcRht+Fy2LDZBE67jVgiiZSYc+4kZapVPLu21HTjbSIVGuruTw2imjOpjOkTS8xYeoneR9yY\n279OD2kYzKottYSGNI9gYqnbNARlHod5raoSN169ZWW9RpHLYbrENstgqlzYbAKH3YYQWp2Ncw3P\no9idup7V2zAURGY/cyOvXVvuodzrNBO1ZR4ndeVa/3tjFG2uQT4ep51JPu+Qis1aB7tNYLMJJvm8\nZg+hbBj1BCwegd0MdVgN82AKpLzImdMIZF7HN0goxu2wmYPHDsYQlLqdaXXK55x8wzpDtb6d+u8m\nX0rcDhw2cVCGwPidHiijxQiAMgQF54EHHuD4449n4cKFfPaznyWZTBKPx/nkJz/J/PnzmTdvHr/4\nxS945JFHaGpq4orLL+fj555KqUsg0BJo8aRMc1eddkEsMXBgmTG4xWG3sWCypmBn1ZbR0aMNEgtY\npmIwWuJGv28jPGMM3NFi2+mKxxoaKnbZKfc6TUPgdtqzDqKxKmKP04bP66TU4zBj+oPFjjMxrmWE\niKyhodw9SNIVi8MmqCoeqCRry7VePYMli4cT4/5el91sJQ+mtA8E45mUuh2DKjAhRFqYJ19KPI4D\nyilYr5+tp9NwIITWaBnp732kOPJq/fTN0LH28F6zdj6cf8cBn7Zu3Tr+9re/8cYbb+BwOLjuuut4\n+OGHmT59Ou179vLGincp9Tjx+/34fD7uvPNObrn9/9E4cy5lRR7cfXGCUW0ee0eaIbARCMXSFDuk\nD3df2Ohj+c4uFjb42NjeQ1d/lO5glAnFLjr7o2ZLfEZ1CS9v2WdOI2Ao5lp9xChofc8h1Wrri8SZ\n7PNS5LKbXUM9DjulHifFLnvaVNLWXilep9YDxmm3UeZ10hOODRk7tmJcy5iC2RoayjV9tWHMjHrX\nlHmytuZryz28vaPLzBEUu0bYEFhyBEbcvGKQsNKBYBqWPOLxxW4H3XpngnwpdTsOKKcABxYaKhQ+\nfeT2eGR81nqYeOGFF1ixYoU5DXUoFKKhoYFzzz2XzZs384UvfJHLLvkw55xzjnlOOJakyGVHCIHX\naacvohkCq0dQ5nXSF0mtKGVg7Sr34YWTae4McuasiTy0vJn1u3sIRhN8ZNFkOgJhTpup5TA+e8Z0\n3tvfz0cXa5NrHT+tkrPn1HDKjCoaKos4f16tOVlXsUub3qA3EsdX5MTrtJs9YYzW35UnTWWOZRCP\ntReIx2nn4sWTicaTFLkcdPZF8g6xgJY8bvOH+NDCSeztjTCh2MXJMyZw3tzaAfPrGyyYXM6y2dVU\nl3l4aHlzzikHass87OkJ0xOOUXwQCcPDjdEyLXLZOWl6FefNraWx8tAWBTIwWt35KN0PHTuJ/2za\nyzlza4Ysa3DO3Bom5jlQysBodAyWsyg0lyypz6vX0pHIkWcIDqLlXiiklHz605/mu9/97oD9jz33\nOstf/Q933303jz/+OPfccw8SLZFrhGm0EJCmaK2KqdzrxC6g9T1t21DO1gTmnEll/PqTS8zQzUv6\nVL+z68q4/SPzzXITStzcZ1mDuNjtSFuD91dXLDE/22yCBQ3lvL6tUxtRaQkrGDJ/TZ990cDqEbgd\nNq48hDVXl0ypMKcc+IU+U+OM6lJ+/cklOc+pKHbxu6uO43evaQ8rV5K3rtxDPClp7gyOivCAkY/x\nuuzMqC4ZtI4HSirnMLTS++p5s80ZNfPlvHl1WWfqHAzTIyjAzJr5cv3p+fcyOtJQOYICctZZZ/Ho\no4+yf7829UNnZyfNzc2079lLUiY558IPc9ttt/Huu9q8O0XFJQT7eikyDUHq68lsoXotoQujX3y2\n1m5lsYupE4rMcQaHGmdepE8C5nbYKbLI4HFkT4IaysbtOLDk3eHGlyWpbMUY4bl1b19es0EWmmJL\nsvhwY4aGRlDpZlJshobGZ4t8pBn5X/wRzPz58/n2t7/NWWedRTKZxOl08utf/5poUnLNNdeC1Fp8\nP/zhDwH42GVXcOtXb+Bn3y1mxfLlaYYgsyuc1TBMrSrWphHOoeQWNvh4omk3MPhI0nwwcgut3UFz\n1ScgZ28YQ9nkGiswXAw126Oxv7kryLFDjBgeDswumAXIVZg5hxEMw2RyIOMIFIcfZQgOI1JKrvjs\nTUwssUw+dfnlXH755WnlAqEYjz7zCgJtTpc2vzZB2zkXfoTTz7uIWbXaFAMJy3TS2WLWxq6p+pQJ\nucIeixorTENwqDFYY5RviduR1lrN1UPEuF8uj2G4qMgYgZyJ1YiOhtBQmd4yLoR3YngEh9ooOJyU\nmYPmRo9M44mR/8UfQSSSkmA0Tl/UTvkgeT0j7i/1cwLBGDabQMpU0gxI69qXzRBUl7n51ScWs6ix\ngtm1ZTmVxocXTqarP4rXZU9L5B4MVSVu7rp8EYsbK3jgzZ3m/lwegfHHPpCuhIXg2Hoft39kHsuO\nqc56fEKxi8piF1390VHRc+QjiyabMh1uvC47d1++2My3jAbOn1cLkgGjxxXDQ0F/8UKI84CfA3bg\nXinlHRnHpwD3AROBLuAKKWXrgAuNERLGhHBZFvu2ErPM0BiMJkhISUIfBWxtZTtsAiEEUkrsWeLr\nDpuN8+dqSTnr8n+ZlBc5+fLZR+dfkSG4cMEkTVan9vMRgqwLsYA20AxyG4rhwmYTfOKEKTmPCyFY\n1ODj35v2HtAKVYWistjFhxdNLtj1B/u9jASlHicfP65hpMUYtxSsmSaEsAN3A+cDc4DLhBBzMor9\nGPiDlHIBcBvwg4O932Crdg0XxopY8eRQhiAla284fSyA1RAIIXDaBIKBHsFoqK8h62CJYIfdRpnH\nMeKGIB+M/MdoeLYKxXBSSH/9eGCblHKHlDIKPAxclFFmDvAf/fOLWY7nhcfjobOzc0T+wBHL1NCG\nIbAq+mg8ae6PxhP0hmNEYglTcfaG49j1lr9NiAEK02m3mccNpJR0dnbi8YzszIVGAngoJe8rco14\naCgfjPzH9n19Q5RUKI4sChkamgxYl5hqBU7IKLMauBgtfPQRoFQIMUFKmTa3sRDiOuA6gMbGxgE3\nqq+vp7W1lX379h0+6fMgmZTsDoQpdmsTgwWjcbr6YwhAdmvL6LUHwhS77JR5nXQEwsR1o+B22NIH\nY+kzk23qSR+I0x2MEksk2RhIV/oej4f6+vrhqGZOit26IRgiETxlQlFBYt2HG2M6bOuMpQrFeGCk\ns2I3AXcJIa4CXgHagERmISnlPcA9AEuXLh3Q7Hc6nUybVrjVe3LR0hXkggdfBGDnHRdw/+vvces/\nNgDwxs3vp9Tj4Pxbn+MjiyZz49lTuPqBF/nM6Udx1jE11JZ5OPVH2rlfXDaTa07R5M8c2RiMxonE\nkodteoHDideZGvQ0GHddvnjER+rmg6/IxdvfWDYmjJZCcTgppCFoA6zZn3p9n4mUcjeaR4AQogT4\nqJTSzxghc4qH7mAq3t8eCNMfcej7ozS1aNW6cP4k5teXpy3pt2iQtU2LXA5Ga486a45gMMbSsP1c\ncxYpFEcyhQzcrgBmCiGmCSFcwKXAk9YCQogqIYQhw9fRehCNGYKWfv77eiNpk8B1BMJpC8I0tfhx\nO2zmesAOu83sKjrYVMyjmaI8cwQKhWJ0UzBDIKWMA58HngU2Ao9KKdcLIW4TQnxIL3YGsFkIsQWo\nAW4vlDwHQjiW4N5XdxAdohtoyOIRfOcf61nXFjC7S3b0hFMLwgSjrGruZv7k8rTRwhVF2vQPozHs\nkw+pZPHoTwQrFIrcFDRHIKV8CngqY9+3LJ8fAx4rpAwHw7PrO/jevzZydE0ppx09MWc5a2jon2va\nAW1B8w27e+jsi5jr3/pDMTr7oly8OL1f+JmzJuZctnAsYMw1pDwChWJsM9LJ4lGJEc83VvbKRTCm\nGYIXbjyNm/6yhqYWP74ibbnIYDRhridsrBVc50uf3uA7F8073KIPK0ZoKHNReYVCMbZQPn0WVjVr\nhqB9CEMQ0nMEXpfDXPGrVJ+DJxRNDDAkuSY8G6vkO45AoVCMbpQhyCAST7Bhdw8AHT2hQcsaoaEi\nZ2rB+L29YYpcDoKxxABDcqT1SClyqhyBQnEkoEJDOi1dQb7y2GquO+0oonrXznVtPVx2z1t84wPH\n8L1/beDWD83l639diz8YxWm3pa33axiCaDyJ12knFI2zpyfMpHIPu3WDcKR5BA67DZfDhnuEZxZV\nKBSHhjIEOn98axdv7ehi6x5teoF5k8tY2xYA4FO/X05Xf5TvP7WRphY/S6ZU8M6ubtoDYWz6hGuN\nlUV87bzZnDO3hq89tob+SILuYJSTp1eZhuBI8wgAvnXhHBYfwAL0CoVi9KF8eh2jtd7ZH6W2zJPW\nt99Y7tGYM/2GZTMBbRH3Ipe2SLcQgv85YzrTJ5bgddnp7I8gJdRXaAniiiLnERlLv+J9U5gz6dCm\ntlYoFCOLMgQ6EcuYgYUNvqwLmOzvjQDaSFljvEC26RWKXHb26WUb9AXHa3MsiKJQKBQjjTIEOn2R\n1CjhhY0+c7Uv60Ixe/SRwiUehzkILNuaskUuhzndhOFpHGn5AYVCceSgDIFObzhlCJZOqaBRX/7x\nqpOmmvuNXkClboe5yHZRljVlrV5CuddJdambxspBlixTKBSKEUQli3X6InFqyzz84rJFLJ1aiZSS\nRz9zIsdNreDUmRP5wkPvsqdHC/eUeBzmouxZPQJLLqDE7eBP155A1RgeQaxQKI5slEeg0x+JU+px\nmHPRCyE4flql+V7q0TwAm9BG0hqLsmc1BJY1b0s8DmbWlI7Z+YQUCsWRjzIEOn2ROCWe3A6SsaB5\nsVvrJWSEhrJNr2A1DqWjYP1bhUKhGAxlCHR6w3FT2WfDSBqX6mUGDQ1Z9g1mXBQKhWI0oAyBTp8e\nGsqFYSQMxW6EhrzZksUWL8FYzlGhUChGK8oQ6PQN4RGYhkB/rxjUI9DKuOxq+gWFQjH6UYZApy8S\np2SQeL7hCZToSePywZLF+j4VFlIoFGMBZQiAZFLSH41TMkgYx8gNmDkCI1mcxRAY+wbzMBQKhWK0\noAwB2gIzUg7egi/OFRoapNeQMgQKhWIsoAwBWn4AyCs0ZBiE+govlx7XwKlZlrJUhkChUIwllKYC\n+iLavED5jCMwyjjsNu746IKsZY2eRCpHoFAoxgIF9QiEEOcJITYLIbYJIW7OcrxRCPGiEGKVEGKN\nEOIDhZQnF8Y8Q6UHMI5gMIxwkfIIFArFWKBghkAIYQfuBs4H5gCXCSHmZBS7BXhUSrkIuBT4ZaHk\nGQxj5tHBPQLnkGUMvKrXkEKhGEMU0iM4HtgmpdwhpYwCDwMXZZSRgLGqSTmwu4Dy5MTIERRnGRxm\nYAwgK/cOPWWE26Et4ZhPWYVCoRhpCtlknQy0WLZbgRMyytwKPCeE+AJQDJyV7UJCiOuA6wAaGxsP\nu6B79UVkJpbmniF0ZnUJd162iGXHVA95PSEE933qOI6uKTlsMioUCkWhGOleQ5cB90sp64EPAA8K\nIQbIJKW8R0q5VEq5dOLEgb10DpX2QBinXTBhkBlChRB88NhJeY8UPmVmFdVH4BrFCoXiyKOQhqAN\naLBs1+v7rFwDPAogpXwT8ABVBZQpK3t6wtSUebDZxHDfWqFQKEacQhqCFcBMIcQ0IYQLLRn8ZEaZ\nZmAZgBDiGDRDsK+AMmWlPRAyl6ZUKBSK8UbBDIGUMg58HngW2IjWO2i9EOI2IcSH9GL/C/y3EGI1\n8BBwlZRSFkqmXHQEwtSqNYUVCsU4paD9G6WUTwFPZez7luXzBuDkQsowFFJKOnrCnD2nZiTFUCgU\nihFjpJPFI04gFCMcS1KjQkMKhWKcMu4NQXsgDEBduXeEJVEoFIqRYdwbgg7dEKgcgUKhGK8oQ9Cj\nDIFCoRjfjHtD0B4IIwRUDzKqWKFQKI5kxr0h6AiEmFjixmkf949CoVCMU8a99uvoiVCnwkIKhWIc\nowxBIKS6jioUinHNuDcE7YGw8ggUCsW4Zlwbgv5InN5wnFo1hkChUIxjxrUhSHUdVT2GFArF+GV8\nGwJjMFmZ8ggUCsX4ZVwbgr29miGoKVMegUKhGL+Ma0MQiiYBKHarReYVCsX4ZVwbgkg8AWiLzSsU\nCsV4ZVxrwEhc8whcyhAoFIpxzJAaUAjxBSFExXAIM9xEYrohUNNLKBSKcUw+GrAGWCGEeFQIcZ4Q\n4ohZ4T0ST+CwCRzKECgUinHMkBpQSnkLMBP4HXAVsFUI8X0hxPQCy1ZwIvGkyg8oFIpxT15aUF9Q\nvkN/xYEK4DEhxI8GO0/3IDYLIbYJIW7OcvxnQogm/bVFCOE/iDocNJF4ArfTPpy3VCgUilHHkP0m\nhRA3AFcC+4F7ga9IKWNCCBuwFfhqjvPswN3A2UArWnjpSX3BegCklF+2lP8CsOgQ6nLARGLKI1Ao\nFIp8OtBXAhdLKXdZd0opk0KICwc573hgm5RyB4AQ4mHgImBDjvKXAd/OQ55Dp2Mt/OsmPtUlaLLf\nOCy3VCgUitFKPs3hp4EuY0MIUSaEOAFASrlxkPMmAy2W7VZ93wCEEFOAacB/chy/TgixUgixct++\nfXmIPAS73oCWt5jX/yZTbIfhegqFQjGGyccQ/Aros2z36fsOJ5cCj0kpE9kOSinvkVIulVIunThx\n4qHfLRYyP1ba+w/9egqFQjGGyccQCD1ZDGghIfILKbUBDZbten1fNi4FHsrjmocHqyEQfYMUVCgU\niiOffAzBDiHEF4UQTv11A7Ajj/NWADOFENOEEC40Zf9kZiEhxGy0XkhvHojgh0Q8ZQgqhPIIFArF\n+CYfQ3A9cBJaa74VOAG4bqiTpJRx4PPAs8BG4FEp5XohxG1CiA9Zil4KPGz1OgpN674ukmjj4nwo\nj0ChUIxvhgzxSCn3oinrA0ZK+RTwVMa+b2Vs33ow1z4U+vv62C/LqaSHMnqH+/YKhUIxqshnHIEH\nuAaYC5iL+0opP11AuQqKiIcISRcBUUyZVIZAoVCMb/IJDT0I1ALnAi+jJX3HtPYU8TAh3PhlCSVJ\nFRpSKBTjm3wMwQwp5TeBfinlA8AFaHmCMYstESaMEz8llCR7RlochUKhGFHyMQQx/d0vhJgHlAPV\nhROp8NjiESK46JYlFCtDoFAoxjn5jAe4R1+P4Ba07p8lwDcLKlWBsSdDhKWLACUUxdtHWhyFQqEY\nUQY1BPrEcj1Sym7gFeCoYZGqwDgSEUKU0i1L8CSUR6BQKMY3g4aG9FHEWWcXHcs4khEtRyBLcCWC\nEI+OtEgKhUIxYuQTGnpBCHET8AhgDsOVUnblPmV045ARwtKFnxJtx2NXw8fuB7tzROXKm+a34a27\nQUqomQuzL4BXfwLJjKmaSmvhxM/Df74L8Qh4ffCBH4PDPTJyKxSKUUk+huC/9PfPWfZJxnCYyJWM\nEMbFiuQsbcemf8K+TVA7f2QFy5fVD8Gmf4G3AjY+CbEgrH8CJs5OlQkHoHc3uIph7V+gdJK2veQq\nmLxkxERXKBSjj3yWqpyW5TVmjQCAS2qGYLNs5NWT7tN2hrpHVqgDIdQFldPhtK9o213vQXEVfO6t\n1OvCn2rHOrdr7x/RJ4wdS/VUKBTDQj4ji6/Mtl9K+YfDL84wkEziIkYYl7ZdNEF7H0sKMtSteQPe\nCm27a0fqs4H1mMOjeQQAoWFdDVShUIwB8gkNHWf57AGWAe8CY9MQxMMAhKVmCIShMINjKOUR7Iby\nevBWattdO6B2QXoZ6zFvZcowjKV6KhSKYSGfSee+YN0WQviAhwsmUaExDIHuEdjGqkdQtyCl3GPB\n3B5BLAgV01LbY6meCoViWDiYldv70ZaVHJvEgkDKEDi9RVroJDSGWsqhLk2xF1mUf1FlehlvxjG7\nA9xlY6ueCoViWMgnR/APtF5CoBmOOcCjhRSqoMQ0jyCkh4bcDpsWOhkrLeVYOOUBWJV9pkdgKP5I\nj9Zt1CgzVuqpUCiGjXxyBD+2fI4Du6SUrQWSp+DEI/04gJjQ+tK7HDZNQQbHiII0FLm3AtzlIGwg\nkwMNgVEm0pPKFyhDoFAospCPIWgG2qWUYQAhhFcIMVVKubOgkhWIaCSIA3B5i6AP3A67FjoZKwrS\naghsNvD4UqGiTLwV4N+VOuatUMlihUIxgHxyBH8BkpbthL5vTBILa4OjK8vLcdoFPq9TC52MGUOg\nK/IiSyvf+m4l89hYMngKhWLYyMcQOKSU5mQ8+mdX4UQqLLGwlix+36x6XvrKmVQUu/QcwRhpKVs9\nAkgZhMxkcbZj3oqxU0+FQjFs5GMI9lkXmxdCXATsz+fiQojzhBCbhRDbhBA35yjzcSHEBiHEeiHE\nn/MT++CJRTSPwOkpZrLPq+00YudSDnLmKMEI7XgPwiPwVmoDypLJgWUVCsW4JZ8cwfXAn4QQd+nb\nrUDW0cZWhBB24G7gbP2cFUKIJ6WUGyxlZgJfB06WUnYLIQq+4E0iEgLA4famdnorIBGFaD+4Swot\nwqGR6RGYBiGLR5B5zFsBSIgEshsOhUIxLslnQNl24H1CiBJ9O99Ffo8HtkkpdwAIIR4GLgI2WMr8\nN3C3vt4BUsq9ByD7AbN/9w7KX/suAE53UeqAEToJdacbgngEXvsZRCxLNDvc2oye2UIxVt59UJvI\nzmDhJ7TttncOrRLNb4HNqU0mBwfoEejvT98MF901dmZbVSgUBSWfcQTfB34kpfTr2xXA/0opbxni\n1MlAi2W7lYFrHR+tX/N1wA7cKqV8JosM1wHXATQ2Ng4lck5eefxXXBzvYleyGntJVeqAOeq2C3wN\nFolXwEs/AIcXbHatm2YsCBNmwMLLc99ISvjnlwChGY5on2ZMtjwDwU5tANuhMO1UEEL7POUk2Ls+\nZRisNJygTT1RMUXbrtOnoVjzMCy9Ghrfd2hyKBSKI4J8QkPnSym/YWzoIZwPoC1deTjuPxM4A6gH\nXhFCzDeMjuWe9wD3ACxduvSgA/k+2UNEOjk9+jOe9FhDQxaPwEqwU3u/9nltiupQN/xw6tBdMBMx\nSMbh/bdoM4TefYJmZIKdcNIX4KxbD7YKA5nzIe2VjfolcP2rqe2aufDfL8Jvz1TdSBUKhUk+yWK7\nEMJcyUQI4QXyWdmkDbA0r6nX91lpBZ6UUsaklO8BW9AMQ0GocYbopgQQeJ321IFcE7KZ8XjdUBgD\nuIbqghnX8hA4jGR0JfibNeOQLZY/nBTlMHoKhWLcko8h+BPwbyHENUKIa4HngQfyOG8FMFMIMU0I\n4QIuBZ7MKPMEmjeAEKIKLVS0I0/ZDxhPPIBfajkAj9UQ5FKOmYlZcwDXEEpUn8YCp6VXUueO9GuN\nFNYwmEKhUJBfsviHQojVwFlocw49C0zJ47y4EOLzenk7cJ+Ucr0Q4jZgpZTySf3YOUKIDWgD1b4i\npew8+OoMjjsWYJ++PKXLYbGBuWbmDHZp8XxXRmJ5KCWqT2xnGoKiCoj2ps4fSdxlIOzKI1AoFCb5\n5AgA9qAZgY8B7wGP53OSlPIp4KmMfd+yfJbAjfqr4LhjPfhlJTcsm0l1qSW65XCDszi7R5Bteuch\nQ0O6R2AkhQebHG64EUJNNaFQKNLIaQiEEEcDl+mv/WiL1wsp5ZnDJNthxxMP0Csa+fLZRw88mE3B\nZzUEldDbPviNYnqOwGnJEVjPH2nUVBMKhcLCYDmCTcD7gQullKdIKe9EC9+MTaTEG+8hQGn240VZ\nWsmh7oGK21sx9HKP8Sw5Auv5I42aakKhUFgYzBBcDLQDLwohfiuEWAaI4RGrAMSCOGSUXpFj5HBO\nj8CXvi+f1nQso9eQNS8wKgyB8ggUCkWKnIZASvmElPJSYDbwIvAloFoI8SshxDnDJeBhQ1d8Pbay\n7MeztZKDXdlX/or2t6ewKQAAH7pJREFUQjxKTszQUEaOwFUCjlEwX99YWn9BoVAUnCG7j0op+6WU\nf5ZSfhBtLMAq4GsFl+xwo4d9cnsEGa1kKXMniwHCg4SHzGRxRo5gNOQHQOUIFApFGge0ZrGUsltK\neY+UclmhBCoYuuLrFYN5BJYZSGNBSERyG4LBet0MSBYbc/34spcfbrw+iPVrcykpFIpxz8EsXj82\n0Q1Bvy1XsrhSG/lrTDCXOarYWs56PBuZyeLB1gwYCXJNqaFQKMYl+Y4jGPvo8f9+ew5DYLTa3/ql\nNnq4ryN9f2a5NQ9D++qB17HZIaDPtWeMI3B6tc+jIVEMKTmW3wPFGTN/O9yw4L/SB9EpFIojmnFk\nCIbwCKpmafMIvfSD1D6bQ5tp1Ipvijb47J37c9/Lrg9Wc1omtqtbqE1cNxqomqmNLn71J9mPeytg\n7oeHVyaFQjFijB9DcNINfO29hST22bMfbzgObm7RFqgxcLgHTu9cVAlf3Z7KA2Tyi0VaItnm1LwD\ng2uePTT5Dye18+Hm5vS6AgRa4TenatNmKxSKccP4MQR2BwHhw2Hrz10m39XJnN701r6VokrNEDhH\neWglW10TMe09l5FTKBRHJOMnWQzEkxKbrcBj4oz4u/MQF58ZCQzjZiS7FQrFuGBcGYKklDgKbgj0\nHjmHugrZSGAYAuURKBTjinFlCOJJiX3YPIIcoaPRjN2pJZGVIVAoxhXjyhAkksnCGwJjrMBYNASg\nya1CQwrFuGJcGYJ4Yhg9AscYNQQOj/IIFIpxxrgyBMOaIxiLyWLQejspj0ChGFeMK0MwrDmCseoR\nOD2ppTYVCsW4YFwZgoRKFg+NwwMx5REoFOOJghoCIcR5QojNQohtQoibsxy/SgixTwjRpL+uLaQ8\nieQwhIaKxvA4AtCTxSpHoFCMJwo2slgIYQfuBs4GWoEVQognpZQbMoo+IqX8fKHksDKsHsGYDQ15\nIapCQwrFeKKQU0wcD2yTUu4AEEI8DFwEZBqCYWN4cgRjPFns8EKwc6SlSCfaDz27tcnyjgRC3bDr\njdTaFwa+Bqg7Vuu1teNlbVr0khptHqzRRs9uaHs39/GqmTBxlvY50gd9e6C0DnraUt9jLAzvvaxN\nbVI8ERpPyH4tKWHHS9rv4GARNph6MvS0Q+c2bd+Ukw5savj+/dD81sHLcDionQcVUw/7ZQtpCCYD\nLZbtViDbN/1RIcRpwBbgy1LKlswCQojrgOsAGhsbD1qgZFJitxU4LeIu037U5Q2FvU+hcI7C7qNv\n/wZe+TF8vSV9Ir+xykt3wNu/Hrjf7oZv7IZ3H4Snv6LvFPCV7VA8YVhFHJJ/fAm2DjKRYnkDfHmd\n9vnNu+DNX8KpN8LLP9QmPLQ7YfVD8M8vpc75381QWjvwWq0r4cHDMBvuKTdC059TU8wvuQo++PP8\nz3/uFk3mkeSCn8Jx1xz2y470pHP/AB6SUkaEEJ8BHgDen1lISnkPcA/A0qVLZebxfIkPR47AZoPP\nr9TWJx6LOLyjL1nc06atqBYOjJ7FfQ6F3natVffxB1P71v8VXvuZVsfedm0K9GXfhue/Cf17R58h\n6G2HKafAeT8YeGz5b2D1I1pLXghtVttIAPZv0XqkhQNQXAW9HYCAc2+HZ7+hew1ZDEFvu/b+sfuh\ncvrByfunj2leTN8eWHI17Hodevcc2DV626FmPnz4lwcnw+GgbHJBLltIQ9AGWJvF9fo+EymlNQZx\nL/CjAspDIimxiQIbAhg9S1IeDKMxWWwsCxrsOjIMQbALSmqhbkFq377N2nuoS3t5K1PHB1sWdaQI\ndUPNvPQ6GFQdDcmYNp25uzS1El7ndu092KUZglAXeMq1tTqM/Vnvpe+vPx7KD1IRlkyE7p2A1EJW\nXdtT182XYBeUTcpe5zFOIeMkK4CZQohpQggXcCnwpLWAEKLOsvkhYGMB5SGeTBbeIxjrOEehR2Ao\nkiNlac2Qf6BBMzoZhLq1l7cifd9ow5AxG5lLoRrvXdsH7vdWDL38q7ls7CGs8OetSN3feLYH+lyz\nfW9HCAUzBFLKOPB54Fk0Bf+olHK9EOI2IcSH9GJfFEKsF0KsBr4IXFUoeQASSbDblSEYFIc+oCwz\nkTmSGC23A23BjVayKVFjO9ilvdIMwSirdzyqtfZzGgJLXSClcPv36dsWDy+fega7tPzJoYzN8Vak\n7n/QhqBr9Cw3e5gpaI5ASvkU8FTGvm9ZPn8d+HohZbCSUB7B0Dg9gNRWL3O4R1oajSPOI8iiUIqs\nHoFf60GU2bIeLRjyFOVQipkt/MyQj/X7LKoc2vMxyh1KWNdbmf7ZW6ld18hjDIVp/JRHMOaJD1eO\nYCzjGIVrEgRzKJSxSCyseVy5PAIzR1ChLZNqc46+ehst96E8glCXpmgzFXzQ4uF5K7QGh7M49T0P\nuN8gYah8sZ5veATJOER68zs/7NfPHcP5v0EYV4YgORy9hsY6o22VskQMovqfdbS1jA+GXPFud7nW\n192aIxBCawmPtnqbdcjROra28GNBSESynx/qTl1jsHpayx0s1th+UaXFa8nTyAaHMH5jnHFlCOJJ\nqXIEQzHaVimzKofRFis/GMywSoZis9nA49MGPFk9Bm/F6Kv3UErRzBF0Z1fuoS5IxLVupGY9fbkN\nQfD/b+/co+yqygP++3JnJnMnGZOZJJBIiExChAQJGiNQlFSgQIKViFIIutRalfrAxlpsabXWKl2t\ntmUplorYyhIq4qOoUamKgpWqIBESAmggxEBeQF6TBzMJ89j9Y+9958yZ+zjn3nvu63y/tWadc889\n555v333nfPt77G/vq3wknpNVbKZS3EB8oX5rEVKlCEZGDRl1DRXHL7HZkIqgwUbG5VAsA6arF/Zt\nGdsH58vur41sUSn1UPSunsFCimC/VQLBz8j2FlZ4PkZQCd6i6JxmJyXGjb+Ucoc1OelSBLVYj6DZ\nybmGGkQRBP3jjeYrL4fcAyXPgy2c4ui3jdbuKA/FLvdgzyf7wL6Jn1Eoi8eY6mTrBO8T3Eb9bku5\nw5qc1CiC0VGDMSRfYqLZybmGGiRG4P8Bp8xqfYsg22tnvvp9sJk5jdbuwf02iF1s9rx39QT7z2+D\nx307Cym8oQGbwVatGEF4G9c1pBZBczM8avPiM6lpcZm0NZhF4EeOvQsaz1deDsX86+HMFr9ttHb7\n/P9iblb/YA/2n98O7p/4PXQF0jnD9wqeVy5hS6DTxRyiKoKBfbbsx+TuyuRoUOpda6hmjOQUgWqC\noviqqTvXw4LzKsvdLsXQoK0/U4zn3GTzGQvg2Udg1wY7qpw6G3b/1pYyiEp7F8w4cXyb+rfZh9Wk\ndph1sk0T9H7kcjh6eMy9k489T0Cmw6aGhglntoAdCQ8fgd2Pw6yXjj/fGFu2YahIVc7MZFtSwbf5\n4M6xiVXjEDhmkS0Gd+Sg9fO/8DwcmFADEvqfKu2zz/bC/gfhud/a173zYdt9th+feRie2ejaGXhA\nmxE4etAWbtzzhB2M+EqhFccI/H3c57R1QEe3vc+uDfbYzJe6MuzPA/43Yuz+1nut8mjRGGN6FIEb\naWiMoAT+H+buT9oHyKLXJ3ev7/15tGqO7V22dPELh+ELy21Ae8U/ja9cGZW3fw/6zrb7A/vgs6fZ\nBxDARf8Cd14NZ74vfzG1KNzxbth0Z/Fzps3L/0DxBdckEMzsdlVYbngVXL3Z1szx7HgQ/mNCjcaJ\nXHYrLL7YuvuuX1rY2lv+l3DuR+DmlbDgXFsi2j8kw/T9fvF7ds+B/qdtAbrOaVYBdEy1D9uhAbjn\nWtvOrpn2/GDwdudDcMuq8Z83NU8xujhk2m1J72Ctou7ZsPHr9g/gtCvgkhvhjittKm+mwyqmbC9s\nf8AWnGtR0qMIRrxFoIqgKNPnwRW3w1dX29FykvQ/DccshnM/Wlqm3vkwaxFsuceWcN7pauFfdmu0\n0fvzu+G7a8aPcA/utErg1R+En3/GWkEAj60tXxEcfhZmL4HXTliQb4wZBdZVeNW7YOZJ0H0sdHTZ\nYy97E/zuZ7DhNrtORFAR9D9ltyv/OX8xtiMH4dvvGYs7HD1klcAr3wELzx9/bvC72fukrY7avw0W\nXmDLNYeZc1rh9oFtf99ywEBPH/S8BE65xCqIYxbZyVzds6HzRfb8YPC2/2m7f/HnoGuGVSBzlxW/\nXxT+5If28zxv/pq1KgHu+rux72nvk9YNlGmz39mUw1ZpXX5L5TI0KKlRBMOjo4AqgkgsvNBNbkrY\nNz2wz44UT35dtPNPWmEfIPffCHu32AfE4otLXwc2BfO7a8YHJH37TjwP1t3sqlOS320TlaEj0NsX\nvU1BOqbYNgZp67BW2YbbJo7kvfyLL85fvvnIQbv1kwP99ce9cqJ8P/1HNwFs0J43sNe6yeacVl5b\nstPh5IvGH5vh4gQvvXDi+cHgrffbn3JJdX3yvX0T5fEy3ff5sQSJQRcPmOQUwaQ225be+dWTpcFI\njSLwriFVBBHwk5uSzlYpJz/cn7/vyXiZJJ3T7Kgu37yEbK/1VXvffiWKYHhwbC5GtfBxm3AmV6lM\nlnAGmN/mK96W7XXBXfeZ+7eCGa1dumRwgtfAvtJZSdWmrRMG9oyVxJjUZmV44ZCbd3B67WSpA6mJ\nnPpgscYIIpJ0/nq5+eH+/EO74s02FXEpjXnmJfjaM34BlIosgsHKqmTmI1f/KbSW9MB+O3GrUHHA\nTLt9oPnr/DavIugZPxr330Wt0iWzIYugVFZStWnP2r574Xmbrjo0YBfTMaPWrdii8wc8qVEEwy5G\nMEkVQTSSrnGTyw8vUxFAfGsi21vAIugZ/49eiTsiCUVQqP5TlGJsbdmAa8ht81kshSaA1aqkQjaQ\nzjlYhwWIvCIo9Jtv0fkDntQoglHNGopH0vnrudF43Id5nlz7ONeGYwRtnTYwG/ysilxDRxJwDRWo\n/zS4v3Ap6Ny1gTWo/baYRTCwd+LxWpBpt+mcA/tsPKfWD962Ttt3hX7zLVpjyJMaRTA2oUwVQSTC\no+dqU+5MTV/HBuIrkbCVExxRB//RJ5UZOjPGPkyq7hpyimWCRRDBtRbVIsj2WjeIz9gJHq8Vfhb1\nwL7au2L8ynxqEbQ2I6oI4pHtSbbYWSXVHHOTrcqwCIJtGuwfX+LAMxwqmxyV4SLB2EooZhGUemB6\nl0fw+vauief59ocnw9XyAeit0GqsPxCX9qx1VxZUBGoRtAQ+RqCuoYh09drJNCMxZu7GoZJqjt6f\nXFaMIBQsDs84BRu7KAf/oG2rkSIYiGAR5HUNFYgRwFj1U08tF2LxVmgUl1e1acvaWerP7ykgm1oE\nLcGo0RITscil8yVkFVRSxCtcNybOdS8ctssOehn8gy74WeUqv2IP2kpoyxMs9mmOpZRhW3Zs/sBw\nEUXl2783oAgqKbVRDtkeO8lveLAOFoHrs4M787+viqA1GNb00XgkvXB6JcXEcu6cuDGC0GIkweyU\nripYBDkffJUtgoyb3BS0CI4etLOiI1kE4XkEBWIEAAe357eSakFXLxzcUZ97+z47uNO6zjIddt7J\n5NDM5xYlUUUgIitEZJOIbBaRgnPuReRNImJEpArzyPMz4mYWa/poROKu4BSXwf32n68cf3olFoG/\ntx9R5/usRrMIYHzQF6JbVHEtAhibQVvrh18lGWGVkrMIdlgllO2x1mK2xyqGJPq0gUhsZrGIZIAb\ngPOB7cADIrLWGPNY6LxuYA1wf1KyAIxYPaAWQVT8CLn/aVsbpnNavOtHhu2otRCHny0/JS/fKD4K\nfpTZ/xRMnjq+zv04RVBhjCBfMLZS2jvHTyiLmn4bDhZLxqZqhgm2v3uObUOtUyaDban5PALXZ4d2\nWctxZNiWM5k8FUZHaitLHUiyxMTpwGZjzBYAEbkdWAU8Fjrvk8CngA8nKIvWGoqLX0jkjnfbra9g\nGZX/eqOtXlmM2UvKlO0YQMYXEIt0nWvTbZcFjrnql10z7GdiKnAN+RF3AqNHn97oyVkEJYK5weuG\nXGprvhm7mTZbVuRIv/1Opsxy33MNmRIoqOerktaKtkCMwBe4Gx1p2fUHwiSpCI4DguUrtwNnBE8Q\nkaXA8caY74tIQUUgIlcCVwLMmzevLGE0fTQm0+bCpTfbkfsProHdm+Jdv2+LLW625PLC58x9VXmy\nveIttkR23FHjsafAJV8YWy830wGL32D3s9Phbd+Ge68rUK8/AsVq+VRK0MUDY9ZBqclvbZ3jXUPF\nlNTlt9r1A05aaSuO1vphfPJF8PrP2tH5MYtqe+9cPacBax2d81HA2NhMeP5GC1K3onMiMgm4Dvjj\nUucaY24CbgJYtmyZKXF6XlQRlMHL3mi3d18bP1YwNGhH/Gf8afXlmtwNC86Jf50InLa68PvzXwsP\n3jIWsIxLohZB53iLIKd0Srih8lkEhehb7kpHA9OPL1/WcumYkr/kdS0Ixk2yvRMXAWpxkgwW7wCC\nv6a57pinG3gZ8FMR2QqcCaxNKmCsRecqIJx/H4UkZtjWgkxHFWIECVkEwRhBVKXjJ0oZY7fN2Ce1\nIPi9tHiGUD6SVAQPAAtFpE9EOoDVwFr/pjHmgDFmpjHmBGPMCcB9wMXGmHVJCOPTRye16FJziZKN\nWZLaGPtQTGJknDSZ9ipkDSXwsG0PZQ1FvVdbJ7m4RxJ1kFqF4PfY4nWF8pGYIjDGDANXAT8EfgN8\n3RjzqIh8QkRiRB2rQ84iyKgiiE1Xb7yS1CNDNsc9ieyZpKnEIihWy6dSwsHioRgWgT8/icqorULw\ne0yhRZBojMAYcydwZ+jYxwqc+9okZVHXUAVke+BADL/5cIL59EmT6WhMiyAY9IXodY2CBevUIiiM\nuobSwViwODVNrh5xS1IPJTgyTppMe/NYBJmO0iUgvFWWswia0EqrBe2hYHHKSM1TMVeGWmME8fHF\nwNxcjJIMJzgyTprMZFt91JSRnDY0YIO6SfzG2kPB4qHBaKUscmmRXhE0oXKuBW1qEaSCUa8INEYQ\nn2yPrVVfbKZwkCTz6ZMm0wGY8maTDh1J7kHrF07xDEd8qOcK1g0611AT9kktyLSDuMehBotbFy06\nVwH+HyNq5lCxmjaNji+/UI57aDjiKL0cfKkIb6mUmhOQuy6w8L1aBIURGeu7zhqW3m4QUqMIckXn\n1DUUn7iVSJMsvpY0mQ67LUcRJG0RBMtfRFU6ahFEp73TLpfZ1lFvSWpOihSBWgRlk41pESS1QEst\nyFkEZWQOJRmMzQV9BwL3iqB0xqWP6oSyorR3pTI+AHUsMVFrhjVGUD7+n2Mgqmuo2WMEVOAaSsgi\nCLp4sm4bKVjszjl6yMZ5mtFKqxVtnc35m60CqVEEI5o1VD6+yud33gff/xC84d9h0esLn59kPn3S\n5BRBYN3ir1xm6y5t/CY89Qt77Jy/gV3r4beBaTLDg/CSVycjV7srLnf9K6xizrTDjBMjXOcsie+u\nGf9amUjHFLUIWp1T507j3Wf30Z5JjTesekyZASs+BQe2wf03wvZ1xRVBkvn0SeP9w941NDoCT/wI\nuo+FrffaCqb7n4Knfwk7fg29fbZYneeklcnItfB8OPtqu7j8o9+yx2afWvq6F70YLrgWDj1jlccp\nb0xGvlbggmtTqyhTowjOWjCTsxbUuKxuK3Hme+x24zdKxwpawiJwrqEjBwDj1tI9Aif/IWz+sV3L\neXA/nPpHcMEnk5erqxfO+1vY+dCYIoiiaEXgrA8kK1ur0Hd2vSWoGzo8VuIRZZZx1Do4jUhYEXil\nt/dJu832uEXWd1jFUM/lHNXfr1QJVQRKPLK9djRcjKYOFoeyhnyxvf6n7bbLrWcbfF1LguUPUurG\nUKqPKgIlHtme0pVIhwbtyk751sZtdApZBMbNNPYWQfB1LZncbb9baE6LS2lIVBEo8ejqKR0jaOaJ\nSxMUQUjpZXvHP/xrXaBMZOz+zWhxKQ2JKgIlHpFiBAPN67/2imA4ZBF4sj3j3UH1SDf091SLQKkS\nqgiUeGR77Yh/aLDwOVHr4DQihVxDnq6QRVCPAmXeCtEYgVIlVBEo8cjNMi5iFSRZfC1pMqF5BMF2\n+pmn2QaxCJrV6lIaDlUESjyiVCJNsvha0oSrjwbb6RVAzjWTrY/l4/ugWZWt0nCoIlDikatEWkQR\ntIRFEAgWexeMb3t4W2vUIlCqTKKKQERWiMgmEdksItfkef89IrJRRNaLyP+JyOIk5VGqQK4SaRHX\nUDMvkh52DQ3uh975dt+PxMPbWpNTBBojUKpDYopARDLADcBKYDFwRZ4H/W3GmFONMS8HPg1cl5Q8\nSpWIYhE0c7C4LU+wuLfP7mfdgiXtWRsvqLdFoFlDSpVIstbQ6cBmY8wWABG5HVgFPOZPMMYE1z6c\nApSxUKxSU/wo+O5/gPs+n/+cfVtg5sLayVRNvEXw88/AQ7dC/zZYeKF1dYXnD2TrtJKV74NmVbZK\nw5GkIjgO2BZ4vR04I3ySiLwf+BDQAZyb74NE5ErgSoB58+ZVXVAlBu1ZWP5h2PN44XNmnQRL31o7\nmapJ22Q4+y9g72b7+phFsORyq9hevHTsvHM/CtOPr4+M88+Bs/4MZi+pz/2VlkOMSWYQLiKXAiuM\nMe9yr98KnGGMuarA+W8GLjTGvL3Y5y5btsysW7eu6vIqiqK0MiLya2PMsnzvJRks3gEEh0xz3bFC\n3A68IUF5FEVRlDwkqQgeABaKSJ+IdACrgbXBE0Qk6Eh+HfBEgvIoiqIoeUgsRmCMGRaRq4AfAhng\nS8aYR0XkE8A6Y8xa4CoR+QNgCNgPFHULKYqiKNUn0RXKjDF3AneGjn0ssL8myfsriqIopdGZxYqi\nKClHFYGiKErKUUWgKIqSclQRKIqipJzEJpQlhYjsBp4q8/KZwJ4qilNPtC2NibalMdG2wEuMMbPy\nvdF0iqASRGRdoZl1zYa2pTHRtjQm2pbiqGtIURQl5agiUBRFSTlpUwQ31VuAKqJtaUy0LY2JtqUI\nqYoRKIqiKBNJm0WgKIqihFBFoCiKknJSowhEZIWIbBKRzSJyTb3liYuIbBWRjSKyXkTWuWO9InKX\niDzhtnVaRLc4IvIlEXlORB4JHMsru1iud/30sIgsLfzJtadAWz4uIjtc36wXkYsC7/21a8smEbmw\nPlJPRESOF5F7ROQxEXlURNa4403XL0Xa0oz90ikivxKRDa4tf++O94nI/U7mr7nS/ojIZPd6s3v/\nhLJubIxp+T9sGewngfnYJTE3AIvrLVfMNmwFZoaOfRq4xu1fA3yq3nIWkH05sBR4pJTswEXA/wAC\nnAncX2/5I7Tl48DVec5d7H5rk4E+9xvM1LsNTrY5wFK33w087uRtun4p0pZm7BcBprr9duB+931/\nHVjtjt8IvNftvw+40e2vBr5Wzn3TYhGcDmw2xmwxxryAXQ1tVZ1lqgargC+7/S/ToCu8GWN+BuwL\nHS4k+yrgFmO5D5guInNqI2lpCrSlEKuA240xR40xvwM2Y3+LdccYs8sY86DbPwT8BrvOeNP1S5G2\nFKKR+8UYYw67l+3uz2DXc/+mOx7uF99f3wTOExGJe9+0KILjgG2B19sp/kNpRAzwIxH5tYhc6Y4d\na4zZ5fafAY6tj2hlUUj2Zu2rq5zL5EsBF11TtMW5E16BHX02db+E2gJN2C8ikhGR9cBzwF1Yi6Xf\nGDPsTgnKm2uLe/8AMCPuPdOiCFqB1xhjlgIrgfeLyPLgm8bahk2ZC9zMsjs+DywAXg7sAv61vuJE\nR0SmAv8NfNAYczD4XrP1S562NGW/GGNGjDEvx67zfjpwctL3TIsi2AEcH3g91x1rGowxO9z2OeBb\n2B/Is948d9vn6idhbArJ3nR9ZYx51v3zjgJfZMzN0NBtEZF27IPzK8aYO9zhpuyXfG1p1n7xGGP6\ngXuA38O64vyKkkF5c21x708D9sa9V1oUwQPAQhd578AGVdbWWabIiMgUEen2+8AFwCPYNvh1nt8O\nfKc+EpZFIdnXAm9zWSpnAgcCroqGJOQrvwTbN2DbstpldvQBC4Ff1Vq+fDg/8n8CvzHGXBd4q+n6\npVBbmrRfZonIdLefBc7HxjzuAS51p4X7xffXpcDdzpKLR72j5LX6w2Y9PI71t32k3vLElH0+Nsth\nA/Colx/rC/wJ8ATwY6C33rIWkP+rWNN8COvffGch2bFZEze4ftoILKu3/BHacquT9WH3jzkncP5H\nXFs2ASvrLX9Artdg3T4PA+vd30XN2C9F2tKM/bIEeMjJ/AjwMXd8PlZZbQa+AUx2xzvd683u/fnl\n3FdLTCiKoqSctLiGFEVRlAKoIlAURUk5qggURVFSjioCRVGUlKOKQFEUJeWoIlCUECIyEqhYuV6q\nWK1WRE4IVi5VlEagrfQpipI6Bo2d4q8oqUAtAkWJiNg1IT4tdl2IX4nIie74CSJytytu9hMRmeeO\nHysi33K15TeIyFnuozIi8kVXb/5HbgapotQNVQSKMpFsyDV0eeC9A8aYU4F/Az7jjn0O+LIxZgnw\nFeB6d/x64H+NMadh1zB41B1fCNxgjDkF6AfelHB7FKUoOrNYUUKIyGFjzNQ8x7cC5xpjtrgiZ88Y\nY2aIyB5s+YIhd3yXMWamiOwG5hpjjgY+4wTgLmPMQvf6r4B2Y8y1ybdMUfKjFoGixMMU2I/D0cD+\nCBqrU+qMKgJFicflge0v3f4vsBVtAd4C3Ov2fwK8F3KLjUyrlZCKEgcdiSjKRLJuhSjPD4wxPoW0\nR0Qexo7qr3DHPgDcLCIfBnYD73DH1wA3icg7sSP/92IrlypKQ6ExAkWJiIsRLDPG7Km3LIpSTdQ1\npCiKknLUIlAURUk5ahEoiqKkHFUEiqIoKUcVgaIoSspRRaAoipJyVBEoiqKknP8HJRp5io8JXHkA\nAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "# Evaluate on test data\n",
            "40/40 [==============================] - 0s 2ms/sample - loss: 0.4622 - acc: 0.7500\n",
            "test loss, test acc: [0.4622451936476864, 0.75]\n",
            "EEG_Deep/Data2A/parsed_P08T.mat\n",
            "Filtering of Training Data Finished\n",
            "EEG_Deep/Data2A/parsed_P08E.mat\n",
            "Filtering of Testing Data Finished\n",
            "(1, 2)\n",
            "Finding labels in training data\n",
            "Finding labels in testing data\n",
            "[2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1]\n",
            "(60, 12, 1536)\n",
            "(60,)\n",
            "(20, 12, 1536)\n",
            "(20,)\n",
            "[[0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]]\n",
            "X_train shape: (60, 1, 12, 1536)\n",
            "60 train samples\n",
            "20 val samples\n",
            "(40, 12, 1536)\n",
            "(40, 1)\n",
            "X_train shape: (40, 1, 12, 1536)\n",
            "40 train samples\n",
            "Train on 60 samples, validate on 20 samples\n",
            "Epoch 1/300\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 1.31589, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 1s - loss: 1.2784 - acc: 0.4667 - val_loss: 1.3159 - val_acc: 0.3500\n",
            "Epoch 2/300\n",
            "\n",
            "Epoch 00002: val_loss improved from 1.31589 to 1.26296, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 1.0936 - acc: 0.4833 - val_loss: 1.2630 - val_acc: 0.3000\n",
            "Epoch 3/300\n",
            "\n",
            "Epoch 00003: val_loss improved from 1.26296 to 1.22750, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.9500 - acc: 0.6500 - val_loss: 1.2275 - val_acc: 0.4000\n",
            "Epoch 4/300\n",
            "\n",
            "Epoch 00004: val_loss improved from 1.22750 to 1.20064, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.8674 - acc: 0.7333 - val_loss: 1.2006 - val_acc: 0.4500\n",
            "Epoch 5/300\n",
            "\n",
            "Epoch 00005: val_loss improved from 1.20064 to 1.17941, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.8255 - acc: 0.6833 - val_loss: 1.1794 - val_acc: 0.4500\n",
            "Epoch 6/300\n",
            "\n",
            "Epoch 00006: val_loss improved from 1.17941 to 1.16083, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.7669 - acc: 0.8333 - val_loss: 1.1608 - val_acc: 0.4500\n",
            "Epoch 7/300\n",
            "\n",
            "Epoch 00007: val_loss improved from 1.16083 to 1.14256, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.7389 - acc: 0.8333 - val_loss: 1.1426 - val_acc: 0.5000\n",
            "Epoch 8/300\n",
            "\n",
            "Epoch 00008: val_loss improved from 1.14256 to 1.12578, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.7150 - acc: 0.8500 - val_loss: 1.1258 - val_acc: 0.4500\n",
            "Epoch 9/300\n",
            "\n",
            "Epoch 00009: val_loss improved from 1.12578 to 1.10886, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.7006 - acc: 0.9167 - val_loss: 1.1089 - val_acc: 0.4500\n",
            "Epoch 10/300\n",
            "\n",
            "Epoch 00010: val_loss improved from 1.10886 to 1.09249, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6889 - acc: 0.8333 - val_loss: 1.0925 - val_acc: 0.5000\n",
            "Epoch 11/300\n",
            "\n",
            "Epoch 00011: val_loss improved from 1.09249 to 1.07812, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6602 - acc: 0.9500 - val_loss: 1.0781 - val_acc: 0.5500\n",
            "Epoch 12/300\n",
            "\n",
            "Epoch 00012: val_loss improved from 1.07812 to 1.06389, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6498 - acc: 0.8667 - val_loss: 1.0639 - val_acc: 0.5500\n",
            "Epoch 13/300\n",
            "\n",
            "Epoch 00013: val_loss improved from 1.06389 to 1.04958, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6205 - acc: 0.9167 - val_loss: 1.0496 - val_acc: 0.5500\n",
            "Epoch 14/300\n",
            "\n",
            "Epoch 00014: val_loss improved from 1.04958 to 1.03603, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6095 - acc: 0.9000 - val_loss: 1.0360 - val_acc: 0.5500\n",
            "Epoch 15/300\n",
            "\n",
            "Epoch 00015: val_loss improved from 1.03603 to 1.02411, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6204 - acc: 0.8667 - val_loss: 1.0241 - val_acc: 0.5000\n",
            "Epoch 16/300\n",
            "\n",
            "Epoch 00016: val_loss improved from 1.02411 to 1.01175, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6008 - acc: 0.8667 - val_loss: 1.0117 - val_acc: 0.5000\n",
            "Epoch 17/300\n",
            "\n",
            "Epoch 00017: val_loss improved from 1.01175 to 0.99965, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5858 - acc: 0.9000 - val_loss: 0.9997 - val_acc: 0.5000\n",
            "Epoch 18/300\n",
            "\n",
            "Epoch 00018: val_loss improved from 0.99965 to 0.98943, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5630 - acc: 0.9000 - val_loss: 0.9894 - val_acc: 0.5000\n",
            "Epoch 19/300\n",
            "\n",
            "Epoch 00019: val_loss improved from 0.98943 to 0.97873, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5600 - acc: 0.8500 - val_loss: 0.9787 - val_acc: 0.5500\n",
            "Epoch 20/300\n",
            "\n",
            "Epoch 00020: val_loss improved from 0.97873 to 0.96630, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5371 - acc: 0.8833 - val_loss: 0.9663 - val_acc: 0.6000\n",
            "Epoch 21/300\n",
            "\n",
            "Epoch 00021: val_loss improved from 0.96630 to 0.95481, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5366 - acc: 0.9333 - val_loss: 0.9548 - val_acc: 0.6000\n",
            "Epoch 22/300\n",
            "\n",
            "Epoch 00022: val_loss improved from 0.95481 to 0.94791, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5255 - acc: 0.9000 - val_loss: 0.9479 - val_acc: 0.6000\n",
            "Epoch 23/300\n",
            "\n",
            "Epoch 00023: val_loss improved from 0.94791 to 0.94188, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5095 - acc: 0.9333 - val_loss: 0.9419 - val_acc: 0.6000\n",
            "Epoch 24/300\n",
            "\n",
            "Epoch 00024: val_loss improved from 0.94188 to 0.93549, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5019 - acc: 0.9167 - val_loss: 0.9355 - val_acc: 0.6000\n",
            "Epoch 25/300\n",
            "\n",
            "Epoch 00025: val_loss improved from 0.93549 to 0.92897, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4748 - acc: 0.9333 - val_loss: 0.9290 - val_acc: 0.6000\n",
            "Epoch 26/300\n",
            "\n",
            "Epoch 00026: val_loss improved from 0.92897 to 0.92407, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4854 - acc: 0.9333 - val_loss: 0.9241 - val_acc: 0.6000\n",
            "Epoch 27/300\n",
            "\n",
            "Epoch 00027: val_loss improved from 0.92407 to 0.92079, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4860 - acc: 0.9000 - val_loss: 0.9208 - val_acc: 0.6000\n",
            "Epoch 28/300\n",
            "\n",
            "Epoch 00028: val_loss improved from 0.92079 to 0.91810, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4703 - acc: 0.9333 - val_loss: 0.9181 - val_acc: 0.6000\n",
            "Epoch 29/300\n",
            "\n",
            "Epoch 00029: val_loss improved from 0.91810 to 0.91672, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4542 - acc: 0.9333 - val_loss: 0.9167 - val_acc: 0.6000\n",
            "Epoch 30/300\n",
            "\n",
            "Epoch 00030: val_loss improved from 0.91672 to 0.91093, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4496 - acc: 0.9500 - val_loss: 0.9109 - val_acc: 0.6000\n",
            "Epoch 31/300\n",
            "\n",
            "Epoch 00031: val_loss improved from 0.91093 to 0.90388, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4546 - acc: 0.8833 - val_loss: 0.9039 - val_acc: 0.6000\n",
            "Epoch 32/300\n",
            "\n",
            "Epoch 00032: val_loss improved from 0.90388 to 0.89394, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4326 - acc: 0.9000 - val_loss: 0.8939 - val_acc: 0.6000\n",
            "Epoch 33/300\n",
            "\n",
            "Epoch 00033: val_loss improved from 0.89394 to 0.88294, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4086 - acc: 0.9333 - val_loss: 0.8829 - val_acc: 0.6000\n",
            "Epoch 34/300\n",
            "\n",
            "Epoch 00034: val_loss improved from 0.88294 to 0.87150, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4124 - acc: 0.9833 - val_loss: 0.8715 - val_acc: 0.6000\n",
            "Epoch 35/300\n",
            "\n",
            "Epoch 00035: val_loss improved from 0.87150 to 0.86687, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4134 - acc: 0.9000 - val_loss: 0.8669 - val_acc: 0.5500\n",
            "Epoch 36/300\n",
            "\n",
            "Epoch 00036: val_loss improved from 0.86687 to 0.86038, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4146 - acc: 0.9333 - val_loss: 0.8604 - val_acc: 0.5500\n",
            "Epoch 37/300\n",
            "\n",
            "Epoch 00037: val_loss improved from 0.86038 to 0.85266, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4077 - acc: 0.9500 - val_loss: 0.8527 - val_acc: 0.6000\n",
            "Epoch 38/300\n",
            "\n",
            "Epoch 00038: val_loss improved from 0.85266 to 0.84280, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3989 - acc: 0.9500 - val_loss: 0.8428 - val_acc: 0.6500\n",
            "Epoch 39/300\n",
            "\n",
            "Epoch 00039: val_loss improved from 0.84280 to 0.83791, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4245 - acc: 0.9167 - val_loss: 0.8379 - val_acc: 0.6500\n",
            "Epoch 40/300\n",
            "\n",
            "Epoch 00040: val_loss improved from 0.83791 to 0.83738, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3702 - acc: 0.9500 - val_loss: 0.8374 - val_acc: 0.5500\n",
            "Epoch 41/300\n",
            "\n",
            "Epoch 00041: val_loss improved from 0.83738 to 0.83596, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3724 - acc: 0.9667 - val_loss: 0.8360 - val_acc: 0.6000\n",
            "Epoch 42/300\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.83596\n",
            "60/60 - 0s - loss: 0.3947 - acc: 0.9500 - val_loss: 0.8375 - val_acc: 0.5500\n",
            "Epoch 43/300\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.83596\n",
            "60/60 - 0s - loss: 0.3925 - acc: 0.9333 - val_loss: 0.8393 - val_acc: 0.5500\n",
            "Epoch 44/300\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.83596\n",
            "60/60 - 0s - loss: 0.3639 - acc: 0.9333 - val_loss: 0.8384 - val_acc: 0.6000\n",
            "Epoch 45/300\n",
            "\n",
            "Epoch 00045: val_loss improved from 0.83596 to 0.83210, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3513 - acc: 0.9667 - val_loss: 0.8321 - val_acc: 0.6000\n",
            "Epoch 46/300\n",
            "\n",
            "Epoch 00046: val_loss improved from 0.83210 to 0.82684, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3781 - acc: 0.9167 - val_loss: 0.8268 - val_acc: 0.6000\n",
            "Epoch 47/300\n",
            "\n",
            "Epoch 00047: val_loss improved from 0.82684 to 0.82071, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3941 - acc: 0.9333 - val_loss: 0.8207 - val_acc: 0.6000\n",
            "Epoch 48/300\n",
            "\n",
            "Epoch 00048: val_loss improved from 0.82071 to 0.81909, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3596 - acc: 0.9500 - val_loss: 0.8191 - val_acc: 0.6000\n",
            "Epoch 49/300\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.81909\n",
            "60/60 - 0s - loss: 0.3655 - acc: 0.9833 - val_loss: 0.8193 - val_acc: 0.6500\n",
            "Epoch 50/300\n",
            "\n",
            "Epoch 00050: val_loss improved from 0.81909 to 0.81601, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3570 - acc: 0.9667 - val_loss: 0.8160 - val_acc: 0.6500\n",
            "Epoch 51/300\n",
            "\n",
            "Epoch 00051: val_loss improved from 0.81601 to 0.81338, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3668 - acc: 0.9500 - val_loss: 0.8134 - val_acc: 0.6500\n",
            "Epoch 52/300\n",
            "\n",
            "Epoch 00052: val_loss improved from 0.81338 to 0.80594, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3353 - acc: 0.9500 - val_loss: 0.8059 - val_acc: 0.6500\n",
            "Epoch 53/300\n",
            "\n",
            "Epoch 00053: val_loss improved from 0.80594 to 0.79957, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3562 - acc: 0.9500 - val_loss: 0.7996 - val_acc: 0.6000\n",
            "Epoch 54/300\n",
            "\n",
            "Epoch 00054: val_loss improved from 0.79957 to 0.79814, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3377 - acc: 0.9333 - val_loss: 0.7981 - val_acc: 0.6000\n",
            "Epoch 55/300\n",
            "\n",
            "Epoch 00055: val_loss improved from 0.79814 to 0.79251, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3318 - acc: 0.9500 - val_loss: 0.7925 - val_acc: 0.6000\n",
            "Epoch 56/300\n",
            "\n",
            "Epoch 00056: val_loss improved from 0.79251 to 0.78861, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3339 - acc: 0.9667 - val_loss: 0.7886 - val_acc: 0.6000\n",
            "Epoch 57/300\n",
            "\n",
            "Epoch 00057: val_loss improved from 0.78861 to 0.78487, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3165 - acc: 0.9500 - val_loss: 0.7849 - val_acc: 0.6500\n",
            "Epoch 58/300\n",
            "\n",
            "Epoch 00058: val_loss improved from 0.78487 to 0.77704, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3117 - acc: 0.9833 - val_loss: 0.7770 - val_acc: 0.6500\n",
            "Epoch 59/300\n",
            "\n",
            "Epoch 00059: val_loss improved from 0.77704 to 0.77313, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3094 - acc: 0.9667 - val_loss: 0.7731 - val_acc: 0.6500\n",
            "Epoch 60/300\n",
            "\n",
            "Epoch 00060: val_loss improved from 0.77313 to 0.77192, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3128 - acc: 0.9667 - val_loss: 0.7719 - val_acc: 0.6000\n",
            "Epoch 61/300\n",
            "\n",
            "Epoch 00061: val_loss improved from 0.77192 to 0.76688, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3170 - acc: 0.9667 - val_loss: 0.7669 - val_acc: 0.6500\n",
            "Epoch 62/300\n",
            "\n",
            "Epoch 00062: val_loss improved from 0.76688 to 0.76525, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3152 - acc: 0.9667 - val_loss: 0.7652 - val_acc: 0.6500\n",
            "Epoch 63/300\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.76525\n",
            "60/60 - 0s - loss: 0.3012 - acc: 0.9500 - val_loss: 0.7669 - val_acc: 0.6500\n",
            "Epoch 64/300\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.76525\n",
            "60/60 - 0s - loss: 0.2970 - acc: 0.9667 - val_loss: 0.7685 - val_acc: 0.6000\n",
            "Epoch 65/300\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.76525\n",
            "60/60 - 0s - loss: 0.3185 - acc: 0.9667 - val_loss: 0.7717 - val_acc: 0.6000\n",
            "Epoch 66/300\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.76525\n",
            "60/60 - 0s - loss: 0.2852 - acc: 0.9667 - val_loss: 0.7742 - val_acc: 0.6000\n",
            "Epoch 67/300\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.76525\n",
            "60/60 - 0s - loss: 0.2928 - acc: 0.9833 - val_loss: 0.7728 - val_acc: 0.5500\n",
            "Epoch 68/300\n",
            "\n",
            "Epoch 00068: val_loss improved from 0.76525 to 0.76387, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2851 - acc: 1.0000 - val_loss: 0.7639 - val_acc: 0.5500\n",
            "Epoch 69/300\n",
            "\n",
            "Epoch 00069: val_loss improved from 0.76387 to 0.75545, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3250 - acc: 0.9667 - val_loss: 0.7555 - val_acc: 0.5500\n",
            "Epoch 70/300\n",
            "\n",
            "Epoch 00070: val_loss improved from 0.75545 to 0.75430, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2762 - acc: 0.9833 - val_loss: 0.7543 - val_acc: 0.5500\n",
            "Epoch 71/300\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.75430\n",
            "60/60 - 0s - loss: 0.2820 - acc: 0.9667 - val_loss: 0.7567 - val_acc: 0.6000\n",
            "Epoch 72/300\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.75430\n",
            "60/60 - 0s - loss: 0.2963 - acc: 0.9667 - val_loss: 0.7563 - val_acc: 0.6000\n",
            "Epoch 73/300\n",
            "\n",
            "Epoch 00073: val_loss improved from 0.75430 to 0.75412, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2653 - acc: 0.9833 - val_loss: 0.7541 - val_acc: 0.5500\n",
            "Epoch 74/300\n",
            "\n",
            "Epoch 00074: val_loss improved from 0.75412 to 0.75192, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3023 - acc: 0.9667 - val_loss: 0.7519 - val_acc: 0.6000\n",
            "Epoch 75/300\n",
            "\n",
            "Epoch 00075: val_loss improved from 0.75192 to 0.74566, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2786 - acc: 0.9667 - val_loss: 0.7457 - val_acc: 0.6000\n",
            "Epoch 76/300\n",
            "\n",
            "Epoch 00076: val_loss improved from 0.74566 to 0.73936, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2797 - acc: 0.9667 - val_loss: 0.7394 - val_acc: 0.6000\n",
            "Epoch 77/300\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.73936\n",
            "60/60 - 0s - loss: 0.2922 - acc: 0.9667 - val_loss: 0.7418 - val_acc: 0.6000\n",
            "Epoch 78/300\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.73936\n",
            "60/60 - 0s - loss: 0.2648 - acc: 0.9833 - val_loss: 0.7460 - val_acc: 0.6000\n",
            "Epoch 79/300\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.73936\n",
            "60/60 - 0s - loss: 0.2811 - acc: 1.0000 - val_loss: 0.7486 - val_acc: 0.6000\n",
            "Epoch 80/300\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.73936\n",
            "60/60 - 0s - loss: 0.2596 - acc: 0.9833 - val_loss: 0.7478 - val_acc: 0.6000\n",
            "Epoch 81/300\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.73936\n",
            "60/60 - 0s - loss: 0.2492 - acc: 0.9833 - val_loss: 0.7492 - val_acc: 0.5500\n",
            "Epoch 82/300\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.73936\n",
            "60/60 - 0s - loss: 0.2628 - acc: 0.9667 - val_loss: 0.7444 - val_acc: 0.6000\n",
            "Epoch 83/300\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.73936\n",
            "60/60 - 0s - loss: 0.2706 - acc: 0.9500 - val_loss: 0.7400 - val_acc: 0.6000\n",
            "Epoch 84/300\n",
            "\n",
            "Epoch 00084: val_loss improved from 0.73936 to 0.73782, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2564 - acc: 0.9667 - val_loss: 0.7378 - val_acc: 0.6000\n",
            "Epoch 85/300\n",
            "\n",
            "Epoch 00085: val_loss improved from 0.73782 to 0.73635, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2650 - acc: 0.9667 - val_loss: 0.7363 - val_acc: 0.6000\n",
            "Epoch 86/300\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.73635\n",
            "60/60 - 0s - loss: 0.2584 - acc: 0.9667 - val_loss: 0.7455 - val_acc: 0.6000\n",
            "Epoch 87/300\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.73635\n",
            "60/60 - 0s - loss: 0.2599 - acc: 1.0000 - val_loss: 0.7505 - val_acc: 0.6000\n",
            "Epoch 88/300\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.73635\n",
            "60/60 - 0s - loss: 0.2416 - acc: 1.0000 - val_loss: 0.7464 - val_acc: 0.6000\n",
            "Epoch 89/300\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.73635\n",
            "60/60 - 0s - loss: 0.2543 - acc: 0.9833 - val_loss: 0.7401 - val_acc: 0.5500\n",
            "Epoch 90/300\n",
            "\n",
            "Epoch 00090: val_loss improved from 0.73635 to 0.73328, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2560 - acc: 0.9667 - val_loss: 0.7333 - val_acc: 0.5500\n",
            "Epoch 91/300\n",
            "\n",
            "Epoch 00091: val_loss improved from 0.73328 to 0.72908, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2557 - acc: 0.9833 - val_loss: 0.7291 - val_acc: 0.5500\n",
            "Epoch 92/300\n",
            "\n",
            "Epoch 00092: val_loss improved from 0.72908 to 0.72484, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2585 - acc: 0.9833 - val_loss: 0.7248 - val_acc: 0.5500\n",
            "Epoch 93/300\n",
            "\n",
            "Epoch 00093: val_loss improved from 0.72484 to 0.71708, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2440 - acc: 1.0000 - val_loss: 0.7171 - val_acc: 0.5500\n",
            "Epoch 94/300\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.71708\n",
            "60/60 - 0s - loss: 0.2591 - acc: 0.9500 - val_loss: 0.7174 - val_acc: 0.5500\n",
            "Epoch 95/300\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.71708\n",
            "60/60 - 0s - loss: 0.2406 - acc: 1.0000 - val_loss: 0.7239 - val_acc: 0.5500\n",
            "Epoch 96/300\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.71708\n",
            "60/60 - 0s - loss: 0.2324 - acc: 1.0000 - val_loss: 0.7251 - val_acc: 0.5500\n",
            "Epoch 97/300\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.71708\n",
            "60/60 - 0s - loss: 0.2555 - acc: 0.9833 - val_loss: 0.7258 - val_acc: 0.5500\n",
            "Epoch 98/300\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.71708\n",
            "60/60 - 0s - loss: 0.2570 - acc: 0.9833 - val_loss: 0.7341 - val_acc: 0.6000\n",
            "Epoch 99/300\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.71708\n",
            "60/60 - 0s - loss: 0.2572 - acc: 0.9667 - val_loss: 0.7305 - val_acc: 0.5500\n",
            "Epoch 100/300\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.71708\n",
            "60/60 - 0s - loss: 0.2592 - acc: 0.9833 - val_loss: 0.7333 - val_acc: 0.5500\n",
            "Epoch 101/300\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.71708\n",
            "60/60 - 0s - loss: 0.2543 - acc: 0.9667 - val_loss: 0.7407 - val_acc: 0.5500\n",
            "Epoch 102/300\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.71708\n",
            "60/60 - 0s - loss: 0.2408 - acc: 1.0000 - val_loss: 0.7438 - val_acc: 0.5500\n",
            "Epoch 103/300\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.71708\n",
            "60/60 - 0s - loss: 0.2240 - acc: 0.9833 - val_loss: 0.7327 - val_acc: 0.5500\n",
            "Epoch 104/300\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.71708\n",
            "60/60 - 0s - loss: 0.2136 - acc: 1.0000 - val_loss: 0.7188 - val_acc: 0.5500\n",
            "Epoch 105/300\n",
            "\n",
            "Epoch 00105: val_loss improved from 0.71708 to 0.71191, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2326 - acc: 1.0000 - val_loss: 0.7119 - val_acc: 0.5500\n",
            "Epoch 106/300\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 0.71191\n",
            "60/60 - 0s - loss: 0.2338 - acc: 1.0000 - val_loss: 0.7235 - val_acc: 0.5500\n",
            "Epoch 107/300\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.71191\n",
            "60/60 - 0s - loss: 0.2200 - acc: 0.9833 - val_loss: 0.7268 - val_acc: 0.5500\n",
            "Epoch 108/300\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 0.71191\n",
            "60/60 - 0s - loss: 0.2239 - acc: 0.9833 - val_loss: 0.7338 - val_acc: 0.5500\n",
            "Epoch 109/300\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.71191\n",
            "60/60 - 0s - loss: 0.2438 - acc: 1.0000 - val_loss: 0.7303 - val_acc: 0.5500\n",
            "Epoch 110/300\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.71191\n",
            "60/60 - 0s - loss: 0.2124 - acc: 1.0000 - val_loss: 0.7279 - val_acc: 0.5500\n",
            "Epoch 111/300\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 0.71191\n",
            "60/60 - 0s - loss: 0.1983 - acc: 1.0000 - val_loss: 0.7259 - val_acc: 0.5500\n",
            "Epoch 112/300\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 0.71191\n",
            "60/60 - 0s - loss: 0.2192 - acc: 1.0000 - val_loss: 0.7184 - val_acc: 0.5500\n",
            "Epoch 113/300\n",
            "\n",
            "Epoch 00113: val_loss improved from 0.71191 to 0.71188, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1933 - acc: 1.0000 - val_loss: 0.7119 - val_acc: 0.5500\n",
            "Epoch 114/300\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.71188\n",
            "60/60 - 0s - loss: 0.2088 - acc: 1.0000 - val_loss: 0.7121 - val_acc: 0.5500\n",
            "Epoch 115/300\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.71188\n",
            "60/60 - 0s - loss: 0.2013 - acc: 1.0000 - val_loss: 0.7192 - val_acc: 0.5500\n",
            "Epoch 116/300\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.71188\n",
            "60/60 - 0s - loss: 0.2451 - acc: 0.9833 - val_loss: 0.7279 - val_acc: 0.5500\n",
            "Epoch 117/300\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.71188\n",
            "60/60 - 0s - loss: 0.1887 - acc: 1.0000 - val_loss: 0.7295 - val_acc: 0.5500\n",
            "Epoch 118/300\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.71188\n",
            "60/60 - 0s - loss: 0.2337 - acc: 0.9500 - val_loss: 0.7222 - val_acc: 0.5500\n",
            "Epoch 119/300\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 0.71188\n",
            "60/60 - 0s - loss: 0.1900 - acc: 0.9833 - val_loss: 0.7164 - val_acc: 0.5500\n",
            "Epoch 120/300\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.71188\n",
            "60/60 - 0s - loss: 0.2094 - acc: 0.9833 - val_loss: 0.7181 - val_acc: 0.5500\n",
            "Epoch 121/300\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.71188\n",
            "60/60 - 0s - loss: 0.2249 - acc: 1.0000 - val_loss: 0.7201 - val_acc: 0.6000\n",
            "Epoch 122/300\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.71188\n",
            "60/60 - 0s - loss: 0.1823 - acc: 1.0000 - val_loss: 0.7298 - val_acc: 0.6000\n",
            "Epoch 123/300\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 0.71188\n",
            "60/60 - 0s - loss: 0.2075 - acc: 1.0000 - val_loss: 0.7315 - val_acc: 0.6000\n",
            "Epoch 124/300\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 0.71188\n",
            "60/60 - 0s - loss: 0.1736 - acc: 1.0000 - val_loss: 0.7317 - val_acc: 0.6000\n",
            "Epoch 125/300\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 0.71188\n",
            "60/60 - 0s - loss: 0.2110 - acc: 0.9833 - val_loss: 0.7294 - val_acc: 0.6000\n",
            "Epoch 126/300\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 0.71188\n",
            "60/60 - 0s - loss: 0.1836 - acc: 1.0000 - val_loss: 0.7310 - val_acc: 0.6000\n",
            "Epoch 127/300\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 0.71188\n",
            "60/60 - 0s - loss: 0.2131 - acc: 1.0000 - val_loss: 0.7420 - val_acc: 0.6000\n",
            "Epoch 128/300\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 0.71188\n",
            "60/60 - 0s - loss: 0.1993 - acc: 0.9833 - val_loss: 0.7423 - val_acc: 0.6000\n",
            "Epoch 129/300\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.71188\n",
            "60/60 - 0s - loss: 0.2074 - acc: 1.0000 - val_loss: 0.7362 - val_acc: 0.6000\n",
            "Epoch 130/300\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 0.71188\n",
            "60/60 - 0s - loss: 0.1941 - acc: 0.9833 - val_loss: 0.7236 - val_acc: 0.6000\n",
            "Epoch 131/300\n",
            "\n",
            "Epoch 00131: val_loss improved from 0.71188 to 0.71020, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2037 - acc: 0.9667 - val_loss: 0.7102 - val_acc: 0.6000\n",
            "Epoch 132/300\n",
            "\n",
            "Epoch 00132: val_loss improved from 0.71020 to 0.70521, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1973 - acc: 0.9833 - val_loss: 0.7052 - val_acc: 0.5500\n",
            "Epoch 133/300\n",
            "\n",
            "Epoch 00133: val_loss improved from 0.70521 to 0.70437, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1724 - acc: 1.0000 - val_loss: 0.7044 - val_acc: 0.6000\n",
            "Epoch 134/300\n",
            "\n",
            "Epoch 00134: val_loss improved from 0.70437 to 0.70397, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1741 - acc: 1.0000 - val_loss: 0.7040 - val_acc: 0.6000\n",
            "Epoch 135/300\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.1820 - acc: 0.9833 - val_loss: 0.7071 - val_acc: 0.6000\n",
            "Epoch 136/300\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.1834 - acc: 1.0000 - val_loss: 0.7205 - val_acc: 0.6000\n",
            "Epoch 137/300\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.1670 - acc: 0.9833 - val_loss: 0.7342 - val_acc: 0.6500\n",
            "Epoch 138/300\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.1758 - acc: 1.0000 - val_loss: 0.7281 - val_acc: 0.6500\n",
            "Epoch 139/300\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.1838 - acc: 0.9833 - val_loss: 0.7239 - val_acc: 0.6500\n",
            "Epoch 140/300\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.1743 - acc: 1.0000 - val_loss: 0.7224 - val_acc: 0.6500\n",
            "Epoch 141/300\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.1820 - acc: 1.0000 - val_loss: 0.7316 - val_acc: 0.6500\n",
            "Epoch 142/300\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.2116 - acc: 1.0000 - val_loss: 0.7432 - val_acc: 0.6500\n",
            "Epoch 143/300\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.1806 - acc: 1.0000 - val_loss: 0.7716 - val_acc: 0.6000\n",
            "Epoch 144/300\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.2008 - acc: 0.9833 - val_loss: 0.7724 - val_acc: 0.6000\n",
            "Epoch 145/300\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.1700 - acc: 1.0000 - val_loss: 0.7443 - val_acc: 0.6500\n",
            "Epoch 146/300\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.1918 - acc: 1.0000 - val_loss: 0.7301 - val_acc: 0.6500\n",
            "Epoch 147/300\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.1843 - acc: 0.9833 - val_loss: 0.7247 - val_acc: 0.6500\n",
            "Epoch 148/300\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.1787 - acc: 0.9833 - val_loss: 0.7213 - val_acc: 0.6000\n",
            "Epoch 149/300\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.1694 - acc: 1.0000 - val_loss: 0.7269 - val_acc: 0.6500\n",
            "Epoch 150/300\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.1738 - acc: 0.9833 - val_loss: 0.7241 - val_acc: 0.6500\n",
            "Epoch 151/300\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.1864 - acc: 1.0000 - val_loss: 0.7216 - val_acc: 0.6000\n",
            "Epoch 152/300\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.1744 - acc: 1.0000 - val_loss: 0.7336 - val_acc: 0.6500\n",
            "Epoch 153/300\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.1869 - acc: 1.0000 - val_loss: 0.7524 - val_acc: 0.6500\n",
            "Epoch 154/300\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.1854 - acc: 1.0000 - val_loss: 0.7611 - val_acc: 0.6500\n",
            "Epoch 155/300\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.1624 - acc: 1.0000 - val_loss: 0.7602 - val_acc: 0.6000\n",
            "Epoch 156/300\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.1571 - acc: 1.0000 - val_loss: 0.7419 - val_acc: 0.6000\n",
            "Epoch 157/300\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.1537 - acc: 1.0000 - val_loss: 0.7418 - val_acc: 0.6500\n",
            "Epoch 158/300\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.1631 - acc: 0.9833 - val_loss: 0.7409 - val_acc: 0.6500\n",
            "Epoch 159/300\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.1533 - acc: 1.0000 - val_loss: 0.7309 - val_acc: 0.6500\n",
            "Epoch 160/300\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.1699 - acc: 1.0000 - val_loss: 0.7259 - val_acc: 0.6500\n",
            "Epoch 161/300\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.1663 - acc: 0.9833 - val_loss: 0.7267 - val_acc: 0.6500\n",
            "Epoch 162/300\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.1651 - acc: 1.0000 - val_loss: 0.7226 - val_acc: 0.6500\n",
            "Epoch 163/300\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.1595 - acc: 0.9833 - val_loss: 0.7271 - val_acc: 0.6500\n",
            "Epoch 164/300\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.1749 - acc: 1.0000 - val_loss: 0.7200 - val_acc: 0.6500\n",
            "Epoch 165/300\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.1629 - acc: 1.0000 - val_loss: 0.7183 - val_acc: 0.7000\n",
            "Epoch 166/300\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.1466 - acc: 1.0000 - val_loss: 0.7256 - val_acc: 0.7000\n",
            "Epoch 167/300\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.1650 - acc: 1.0000 - val_loss: 0.7288 - val_acc: 0.6500\n",
            "Epoch 168/300\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.1590 - acc: 1.0000 - val_loss: 0.7340 - val_acc: 0.6500\n",
            "Epoch 169/300\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.1675 - acc: 1.0000 - val_loss: 0.7327 - val_acc: 0.6500\n",
            "Epoch 170/300\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.1539 - acc: 1.0000 - val_loss: 0.7164 - val_acc: 0.6000\n",
            "Epoch 171/300\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.1773 - acc: 0.9833 - val_loss: 0.7115 - val_acc: 0.6000\n",
            "Epoch 172/300\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.1483 - acc: 1.0000 - val_loss: 0.7256 - val_acc: 0.6500\n",
            "Epoch 173/300\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.1506 - acc: 1.0000 - val_loss: 0.7347 - val_acc: 0.6500\n",
            "Epoch 174/300\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.1679 - acc: 1.0000 - val_loss: 0.7449 - val_acc: 0.6500\n",
            "Epoch 175/300\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.1467 - acc: 1.0000 - val_loss: 0.7539 - val_acc: 0.6500\n",
            "Epoch 176/300\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.1602 - acc: 1.0000 - val_loss: 0.7375 - val_acc: 0.6500\n",
            "Epoch 177/300\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.1603 - acc: 1.0000 - val_loss: 0.7348 - val_acc: 0.6500\n",
            "Epoch 178/300\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.1626 - acc: 0.9833 - val_loss: 0.7470 - val_acc: 0.7000\n",
            "Epoch 179/300\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.1423 - acc: 1.0000 - val_loss: 0.7735 - val_acc: 0.6000\n",
            "Epoch 180/300\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.1637 - acc: 0.9833 - val_loss: 0.7796 - val_acc: 0.6000\n",
            "Epoch 181/300\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.1369 - acc: 1.0000 - val_loss: 0.7727 - val_acc: 0.6500\n",
            "Epoch 182/300\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.1333 - acc: 1.0000 - val_loss: 0.7660 - val_acc: 0.6500\n",
            "Epoch 183/300\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.1481 - acc: 1.0000 - val_loss: 0.7718 - val_acc: 0.6500\n",
            "Epoch 184/300\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.1597 - acc: 1.0000 - val_loss: 0.7720 - val_acc: 0.6500\n",
            "Epoch 185/300\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.1508 - acc: 1.0000 - val_loss: 0.7580 - val_acc: 0.6500\n",
            "Epoch 186/300\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.1649 - acc: 0.9833 - val_loss: 0.7518 - val_acc: 0.6500\n",
            "Epoch 187/300\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.1371 - acc: 1.0000 - val_loss: 0.7556 - val_acc: 0.6500\n",
            "Epoch 188/300\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.1407 - acc: 1.0000 - val_loss: 0.7573 - val_acc: 0.6500\n",
            "Epoch 189/300\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.1463 - acc: 1.0000 - val_loss: 0.7624 - val_acc: 0.6500\n",
            "Epoch 190/300\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.1370 - acc: 1.0000 - val_loss: 0.7581 - val_acc: 0.6500\n",
            "Epoch 191/300\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.1440 - acc: 0.9833 - val_loss: 0.7656 - val_acc: 0.6500\n",
            "Epoch 192/300\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.1422 - acc: 1.0000 - val_loss: 0.7653 - val_acc: 0.6500\n",
            "Epoch 193/300\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.1605 - acc: 0.9833 - val_loss: 0.7716 - val_acc: 0.6500\n",
            "Epoch 194/300\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.1439 - acc: 1.0000 - val_loss: 0.7743 - val_acc: 0.6000\n",
            "Epoch 195/300\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.1517 - acc: 1.0000 - val_loss: 0.7649 - val_acc: 0.6000\n",
            "Epoch 196/300\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.1566 - acc: 1.0000 - val_loss: 0.7586 - val_acc: 0.6000\n",
            "Epoch 197/300\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.1455 - acc: 1.0000 - val_loss: 0.7564 - val_acc: 0.6000\n",
            "Epoch 198/300\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.1345 - acc: 0.9833 - val_loss: 0.7665 - val_acc: 0.6000\n",
            "Epoch 199/300\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.1323 - acc: 1.0000 - val_loss: 0.7752 - val_acc: 0.6000\n",
            "Epoch 200/300\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.1376 - acc: 1.0000 - val_loss: 0.7854 - val_acc: 0.6000\n",
            "Epoch 201/300\n",
            "\n",
            "Epoch 00201: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.1500 - acc: 1.0000 - val_loss: 0.7652 - val_acc: 0.6500\n",
            "Epoch 202/300\n",
            "\n",
            "Epoch 00202: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.1483 - acc: 1.0000 - val_loss: 0.7660 - val_acc: 0.6500\n",
            "Epoch 203/300\n",
            "\n",
            "Epoch 00203: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.1276 - acc: 1.0000 - val_loss: 0.7483 - val_acc: 0.6500\n",
            "Epoch 204/300\n",
            "\n",
            "Epoch 00204: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.1285 - acc: 1.0000 - val_loss: 0.7392 - val_acc: 0.6000\n",
            "Epoch 205/300\n",
            "\n",
            "Epoch 00205: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.1445 - acc: 1.0000 - val_loss: 0.7368 - val_acc: 0.6000\n",
            "Epoch 206/300\n",
            "\n",
            "Epoch 00206: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.1184 - acc: 1.0000 - val_loss: 0.7529 - val_acc: 0.6500\n",
            "Epoch 207/300\n",
            "\n",
            "Epoch 00207: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.1377 - acc: 1.0000 - val_loss: 0.7499 - val_acc: 0.6500\n",
            "Epoch 208/300\n",
            "\n",
            "Epoch 00208: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.1291 - acc: 1.0000 - val_loss: 0.7358 - val_acc: 0.6500\n",
            "Epoch 209/300\n",
            "\n",
            "Epoch 00209: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.1320 - acc: 1.0000 - val_loss: 0.7297 - val_acc: 0.6000\n",
            "Epoch 210/300\n",
            "\n",
            "Epoch 00210: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.1459 - acc: 1.0000 - val_loss: 0.7675 - val_acc: 0.6500\n",
            "Epoch 211/300\n",
            "\n",
            "Epoch 00211: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.1498 - acc: 1.0000 - val_loss: 0.7840 - val_acc: 0.6000\n",
            "Epoch 212/300\n",
            "\n",
            "Epoch 00212: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.1567 - acc: 0.9833 - val_loss: 0.7581 - val_acc: 0.6500\n",
            "Epoch 213/300\n",
            "\n",
            "Epoch 00213: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.1261 - acc: 1.0000 - val_loss: 0.7700 - val_acc: 0.6500\n",
            "Epoch 214/300\n",
            "\n",
            "Epoch 00214: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.1223 - acc: 1.0000 - val_loss: 0.7821 - val_acc: 0.6500\n",
            "Epoch 215/300\n",
            "\n",
            "Epoch 00215: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.1180 - acc: 1.0000 - val_loss: 0.7851 - val_acc: 0.6500\n",
            "Epoch 216/300\n",
            "\n",
            "Epoch 00216: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.1256 - acc: 0.9833 - val_loss: 0.7624 - val_acc: 0.7000\n",
            "Epoch 217/300\n",
            "\n",
            "Epoch 00217: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.1148 - acc: 1.0000 - val_loss: 0.7508 - val_acc: 0.6500\n",
            "Epoch 218/300\n",
            "\n",
            "Epoch 00218: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.1410 - acc: 1.0000 - val_loss: 0.7527 - val_acc: 0.6500\n",
            "Epoch 219/300\n",
            "\n",
            "Epoch 00219: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.1359 - acc: 1.0000 - val_loss: 0.7707 - val_acc: 0.7000\n",
            "Epoch 220/300\n",
            "\n",
            "Epoch 00220: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.1319 - acc: 1.0000 - val_loss: 0.7878 - val_acc: 0.6500\n",
            "Epoch 221/300\n",
            "\n",
            "Epoch 00221: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.1423 - acc: 0.9833 - val_loss: 0.7694 - val_acc: 0.7000\n",
            "Epoch 222/300\n",
            "\n",
            "Epoch 00222: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.1056 - acc: 1.0000 - val_loss: 0.7622 - val_acc: 0.7000\n",
            "Epoch 223/300\n",
            "\n",
            "Epoch 00223: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.1291 - acc: 1.0000 - val_loss: 0.7606 - val_acc: 0.6000\n",
            "Epoch 224/300\n",
            "\n",
            "Epoch 00224: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.1230 - acc: 1.0000 - val_loss: 0.7730 - val_acc: 0.6500\n",
            "Epoch 225/300\n",
            "\n",
            "Epoch 00225: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.1088 - acc: 1.0000 - val_loss: 0.7785 - val_acc: 0.6500\n",
            "Epoch 226/300\n",
            "\n",
            "Epoch 00226: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.1360 - acc: 0.9833 - val_loss: 0.7908 - val_acc: 0.6500\n",
            "Epoch 227/300\n",
            "\n",
            "Epoch 00227: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.1383 - acc: 1.0000 - val_loss: 0.7995 - val_acc: 0.6500\n",
            "Epoch 228/300\n",
            "\n",
            "Epoch 00228: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.1287 - acc: 0.9833 - val_loss: 0.7879 - val_acc: 0.6500\n",
            "Epoch 229/300\n",
            "\n",
            "Epoch 00229: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.1227 - acc: 1.0000 - val_loss: 0.7767 - val_acc: 0.6500\n",
            "Epoch 230/300\n",
            "\n",
            "Epoch 00230: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.1153 - acc: 1.0000 - val_loss: 0.7909 - val_acc: 0.6000\n",
            "Epoch 231/300\n",
            "\n",
            "Epoch 00231: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.0983 - acc: 1.0000 - val_loss: 0.7825 - val_acc: 0.6000\n",
            "Epoch 232/300\n",
            "\n",
            "Epoch 00232: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.1188 - acc: 1.0000 - val_loss: 0.7566 - val_acc: 0.6000\n",
            "Epoch 233/300\n",
            "\n",
            "Epoch 00233: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.1221 - acc: 1.0000 - val_loss: 0.7613 - val_acc: 0.6000\n",
            "Epoch 234/300\n",
            "\n",
            "Epoch 00234: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.1430 - acc: 0.9833 - val_loss: 0.7775 - val_acc: 0.6000\n",
            "Epoch 235/300\n",
            "\n",
            "Epoch 00235: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.1070 - acc: 1.0000 - val_loss: 0.7979 - val_acc: 0.6000\n",
            "Epoch 236/300\n",
            "\n",
            "Epoch 00236: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.1163 - acc: 1.0000 - val_loss: 0.8197 - val_acc: 0.6000\n",
            "Epoch 237/300\n",
            "\n",
            "Epoch 00237: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.1568 - acc: 0.9833 - val_loss: 0.8001 - val_acc: 0.6500\n",
            "Epoch 238/300\n",
            "\n",
            "Epoch 00238: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.1341 - acc: 1.0000 - val_loss: 0.7763 - val_acc: 0.6000\n",
            "Epoch 239/300\n",
            "\n",
            "Epoch 00239: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.1358 - acc: 1.0000 - val_loss: 0.7743 - val_acc: 0.6000\n",
            "Epoch 240/300\n",
            "\n",
            "Epoch 00240: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.1095 - acc: 1.0000 - val_loss: 0.7841 - val_acc: 0.6000\n",
            "Epoch 241/300\n",
            "\n",
            "Epoch 00241: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.1522 - acc: 0.9833 - val_loss: 0.7957 - val_acc: 0.6500\n",
            "Epoch 242/300\n",
            "\n",
            "Epoch 00242: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.1114 - acc: 1.0000 - val_loss: 0.8146 - val_acc: 0.6000\n",
            "Epoch 243/300\n",
            "\n",
            "Epoch 00243: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.1135 - acc: 1.0000 - val_loss: 0.8195 - val_acc: 0.5500\n",
            "Epoch 244/300\n",
            "\n",
            "Epoch 00244: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.1046 - acc: 1.0000 - val_loss: 0.8036 - val_acc: 0.6500\n",
            "Epoch 245/300\n",
            "\n",
            "Epoch 00245: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.1126 - acc: 1.0000 - val_loss: 0.7787 - val_acc: 0.6500\n",
            "Epoch 246/300\n",
            "\n",
            "Epoch 00246: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.1144 - acc: 1.0000 - val_loss: 0.7714 - val_acc: 0.5500\n",
            "Epoch 247/300\n",
            "\n",
            "Epoch 00247: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.1132 - acc: 1.0000 - val_loss: 0.7698 - val_acc: 0.5500\n",
            "Epoch 248/300\n",
            "\n",
            "Epoch 00248: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.1369 - acc: 1.0000 - val_loss: 0.7679 - val_acc: 0.5500\n",
            "Epoch 249/300\n",
            "\n",
            "Epoch 00249: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.1310 - acc: 0.9833 - val_loss: 0.7641 - val_acc: 0.6000\n",
            "Epoch 250/300\n",
            "\n",
            "Epoch 00250: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.0918 - acc: 1.0000 - val_loss: 0.7480 - val_acc: 0.5500\n",
            "Epoch 251/300\n",
            "\n",
            "Epoch 00251: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.1223 - acc: 1.0000 - val_loss: 0.7614 - val_acc: 0.6000\n",
            "Epoch 252/300\n",
            "\n",
            "Epoch 00252: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.1205 - acc: 1.0000 - val_loss: 0.7668 - val_acc: 0.6500\n",
            "Epoch 253/300\n",
            "\n",
            "Epoch 00253: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.1168 - acc: 1.0000 - val_loss: 0.7729 - val_acc: 0.6500\n",
            "Epoch 254/300\n",
            "\n",
            "Epoch 00254: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.0970 - acc: 1.0000 - val_loss: 0.7582 - val_acc: 0.6500\n",
            "Epoch 255/300\n",
            "\n",
            "Epoch 00255: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.1246 - acc: 1.0000 - val_loss: 0.7497 - val_acc: 0.6000\n",
            "Epoch 256/300\n",
            "\n",
            "Epoch 00256: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.1134 - acc: 1.0000 - val_loss: 0.7586 - val_acc: 0.6000\n",
            "Epoch 257/300\n",
            "\n",
            "Epoch 00257: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.1000 - acc: 1.0000 - val_loss: 0.7610 - val_acc: 0.6000\n",
            "Epoch 258/300\n",
            "\n",
            "Epoch 00258: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.1056 - acc: 1.0000 - val_loss: 0.7504 - val_acc: 0.6000\n",
            "Epoch 259/300\n",
            "\n",
            "Epoch 00259: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.0990 - acc: 1.0000 - val_loss: 0.7428 - val_acc: 0.6000\n",
            "Epoch 260/300\n",
            "\n",
            "Epoch 00260: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.1059 - acc: 1.0000 - val_loss: 0.7571 - val_acc: 0.6000\n",
            "Epoch 261/300\n",
            "\n",
            "Epoch 00261: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.1136 - acc: 0.9833 - val_loss: 0.7562 - val_acc: 0.6000\n",
            "Epoch 262/300\n",
            "\n",
            "Epoch 00262: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.1171 - acc: 1.0000 - val_loss: 0.7701 - val_acc: 0.6500\n",
            "Epoch 263/300\n",
            "\n",
            "Epoch 00263: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.1061 - acc: 1.0000 - val_loss: 0.7718 - val_acc: 0.6000\n",
            "Epoch 264/300\n",
            "\n",
            "Epoch 00264: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.1167 - acc: 1.0000 - val_loss: 0.7744 - val_acc: 0.6000\n",
            "Epoch 265/300\n",
            "\n",
            "Epoch 00265: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.1129 - acc: 1.0000 - val_loss: 0.7974 - val_acc: 0.6000\n",
            "Epoch 266/300\n",
            "\n",
            "Epoch 00266: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.1149 - acc: 1.0000 - val_loss: 0.7995 - val_acc: 0.6000\n",
            "Epoch 267/300\n",
            "\n",
            "Epoch 00267: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.0946 - acc: 1.0000 - val_loss: 0.7939 - val_acc: 0.6000\n",
            "Epoch 268/300\n",
            "\n",
            "Epoch 00268: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.1162 - acc: 1.0000 - val_loss: 0.7991 - val_acc: 0.6000\n",
            "Epoch 269/300\n",
            "\n",
            "Epoch 00269: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.1163 - acc: 1.0000 - val_loss: 0.7785 - val_acc: 0.6500\n",
            "Epoch 270/300\n",
            "\n",
            "Epoch 00270: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.1039 - acc: 1.0000 - val_loss: 0.7849 - val_acc: 0.6500\n",
            "Epoch 271/300\n",
            "\n",
            "Epoch 00271: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.1008 - acc: 1.0000 - val_loss: 0.7820 - val_acc: 0.6500\n",
            "Epoch 272/300\n",
            "\n",
            "Epoch 00272: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.1118 - acc: 1.0000 - val_loss: 0.7880 - val_acc: 0.6500\n",
            "Epoch 273/300\n",
            "\n",
            "Epoch 00273: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.1054 - acc: 1.0000 - val_loss: 0.7874 - val_acc: 0.6500\n",
            "Epoch 274/300\n",
            "\n",
            "Epoch 00274: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.1021 - acc: 1.0000 - val_loss: 0.7742 - val_acc: 0.6500\n",
            "Epoch 275/300\n",
            "\n",
            "Epoch 00275: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.1106 - acc: 1.0000 - val_loss: 0.7576 - val_acc: 0.6000\n",
            "Epoch 276/300\n",
            "\n",
            "Epoch 00276: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.0928 - acc: 1.0000 - val_loss: 0.7575 - val_acc: 0.6500\n",
            "Epoch 277/300\n",
            "\n",
            "Epoch 00277: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.1159 - acc: 1.0000 - val_loss: 0.7582 - val_acc: 0.6500\n",
            "Epoch 278/300\n",
            "\n",
            "Epoch 00278: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.1026 - acc: 1.0000 - val_loss: 0.7734 - val_acc: 0.6500\n",
            "Epoch 279/300\n",
            "\n",
            "Epoch 00279: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.0943 - acc: 1.0000 - val_loss: 0.7994 - val_acc: 0.6500\n",
            "Epoch 280/300\n",
            "\n",
            "Epoch 00280: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.1094 - acc: 1.0000 - val_loss: 0.8192 - val_acc: 0.6500\n",
            "Epoch 281/300\n",
            "\n",
            "Epoch 00281: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.1069 - acc: 1.0000 - val_loss: 0.8159 - val_acc: 0.6500\n",
            "Epoch 282/300\n",
            "\n",
            "Epoch 00282: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.0939 - acc: 1.0000 - val_loss: 0.7867 - val_acc: 0.6000\n",
            "Epoch 283/300\n",
            "\n",
            "Epoch 00283: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.1049 - acc: 1.0000 - val_loss: 0.7877 - val_acc: 0.6000\n",
            "Epoch 284/300\n",
            "\n",
            "Epoch 00284: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.0955 - acc: 1.0000 - val_loss: 0.8136 - val_acc: 0.5500\n",
            "Epoch 285/300\n",
            "\n",
            "Epoch 00285: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.0967 - acc: 1.0000 - val_loss: 0.8192 - val_acc: 0.6000\n",
            "Epoch 286/300\n",
            "\n",
            "Epoch 00286: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.1037 - acc: 1.0000 - val_loss: 0.8333 - val_acc: 0.5500\n",
            "Epoch 287/300\n",
            "\n",
            "Epoch 00287: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.1071 - acc: 1.0000 - val_loss: 0.8060 - val_acc: 0.6000\n",
            "Epoch 288/300\n",
            "\n",
            "Epoch 00288: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.1068 - acc: 1.0000 - val_loss: 0.7925 - val_acc: 0.5500\n",
            "Epoch 289/300\n",
            "\n",
            "Epoch 00289: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.1074 - acc: 1.0000 - val_loss: 0.8038 - val_acc: 0.6000\n",
            "Epoch 290/300\n",
            "\n",
            "Epoch 00290: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.0962 - acc: 1.0000 - val_loss: 0.8186 - val_acc: 0.7000\n",
            "Epoch 291/300\n",
            "\n",
            "Epoch 00291: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.1026 - acc: 1.0000 - val_loss: 0.8122 - val_acc: 0.7000\n",
            "Epoch 292/300\n",
            "\n",
            "Epoch 00292: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.1008 - acc: 1.0000 - val_loss: 0.7909 - val_acc: 0.6500\n",
            "Epoch 293/300\n",
            "\n",
            "Epoch 00293: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.1025 - acc: 1.0000 - val_loss: 0.7854 - val_acc: 0.6500\n",
            "Epoch 294/300\n",
            "\n",
            "Epoch 00294: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.1065 - acc: 0.9833 - val_loss: 0.7813 - val_acc: 0.6500\n",
            "Epoch 295/300\n",
            "\n",
            "Epoch 00295: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.1005 - acc: 1.0000 - val_loss: 0.7777 - val_acc: 0.6500\n",
            "Epoch 296/300\n",
            "\n",
            "Epoch 00296: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.0774 - acc: 1.0000 - val_loss: 0.7948 - val_acc: 0.6500\n",
            "Epoch 297/300\n",
            "\n",
            "Epoch 00297: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.0997 - acc: 1.0000 - val_loss: 0.8066 - val_acc: 0.6500\n",
            "Epoch 298/300\n",
            "\n",
            "Epoch 00298: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.1016 - acc: 1.0000 - val_loss: 0.8008 - val_acc: 0.6500\n",
            "Epoch 299/300\n",
            "\n",
            "Epoch 00299: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.0952 - acc: 1.0000 - val_loss: 0.7965 - val_acc: 0.6000\n",
            "Epoch 300/300\n",
            "\n",
            "Epoch 00300: val_loss did not improve from 0.70397\n",
            "60/60 - 0s - loss: 0.0985 - acc: 1.0000 - val_loss: 0.8152 - val_acc: 0.6000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "# Evaluate on test data\n",
            "40/40 [==============================] - 0s 2ms/sample - loss: 1.4239 - acc: 0.4500\n",
            "test loss, test acc: [1.4238739734049886, 0.45]\n",
            "EEG_Deep/Data2A/parsed_P09T.mat\n",
            "Filtering of Training Data Finished\n",
            "EEG_Deep/Data2A/parsed_P09E.mat\n",
            "Filtering of Testing Data Finished\n",
            "(1, 2)\n",
            "Finding labels in training data\n",
            "Finding labels in testing data\n",
            "[1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2]\n",
            "(60, 12, 1536)\n",
            "(60,)\n",
            "(20, 12, 1536)\n",
            "(20,)\n",
            "[[1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]]\n",
            "X_train shape: (60, 1, 12, 1536)\n",
            "60 train samples\n",
            "20 val samples\n",
            "(40, 12, 1536)\n",
            "(40, 1)\n",
            "X_train shape: (40, 1, 12, 1536)\n",
            "40 train samples\n",
            "Train on 60 samples, validate on 20 samples\n",
            "Epoch 1/300\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 1.35774, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 1s - loss: 1.3259 - acc: 0.4167 - val_loss: 1.3577 - val_acc: 0.5000\n",
            "Epoch 2/300\n",
            "\n",
            "Epoch 00002: val_loss improved from 1.35774 to 1.29845, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 1.0940 - acc: 0.5500 - val_loss: 1.2985 - val_acc: 0.5000\n",
            "Epoch 3/300\n",
            "\n",
            "Epoch 00003: val_loss improved from 1.29845 to 1.25766, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.9614 - acc: 0.4833 - val_loss: 1.2577 - val_acc: 0.5000\n",
            "Epoch 4/300\n",
            "\n",
            "Epoch 00004: val_loss improved from 1.25766 to 1.22532, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.8570 - acc: 0.6833 - val_loss: 1.2253 - val_acc: 0.4500\n",
            "Epoch 5/300\n",
            "\n",
            "Epoch 00005: val_loss improved from 1.22532 to 1.19791, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.8235 - acc: 0.6833 - val_loss: 1.1979 - val_acc: 0.5500\n",
            "Epoch 6/300\n",
            "\n",
            "Epoch 00006: val_loss improved from 1.19791 to 1.17063, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.7876 - acc: 0.7333 - val_loss: 1.1706 - val_acc: 0.5500\n",
            "Epoch 7/300\n",
            "\n",
            "Epoch 00007: val_loss improved from 1.17063 to 1.14724, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.7649 - acc: 0.7667 - val_loss: 1.1472 - val_acc: 0.5500\n",
            "Epoch 8/300\n",
            "\n",
            "Epoch 00008: val_loss improved from 1.14724 to 1.12572, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.7570 - acc: 0.8000 - val_loss: 1.1257 - val_acc: 0.4000\n",
            "Epoch 9/300\n",
            "\n",
            "Epoch 00009: val_loss improved from 1.12572 to 1.10565, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.7281 - acc: 0.8000 - val_loss: 1.1057 - val_acc: 0.4500\n",
            "Epoch 10/300\n",
            "\n",
            "Epoch 00010: val_loss improved from 1.10565 to 1.08858, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6962 - acc: 0.8500 - val_loss: 1.0886 - val_acc: 0.5500\n",
            "Epoch 11/300\n",
            "\n",
            "Epoch 00011: val_loss improved from 1.08858 to 1.07087, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.7008 - acc: 0.8833 - val_loss: 1.0709 - val_acc: 0.5500\n",
            "Epoch 12/300\n",
            "\n",
            "Epoch 00012: val_loss improved from 1.07087 to 1.05379, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6974 - acc: 0.8500 - val_loss: 1.0538 - val_acc: 0.5000\n",
            "Epoch 13/300\n",
            "\n",
            "Epoch 00013: val_loss improved from 1.05379 to 1.03795, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6879 - acc: 0.8500 - val_loss: 1.0380 - val_acc: 0.5000\n",
            "Epoch 14/300\n",
            "\n",
            "Epoch 00014: val_loss improved from 1.03795 to 1.02337, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6717 - acc: 0.8500 - val_loss: 1.0234 - val_acc: 0.5000\n",
            "Epoch 15/300\n",
            "\n",
            "Epoch 00015: val_loss improved from 1.02337 to 1.01126, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6660 - acc: 0.8333 - val_loss: 1.0113 - val_acc: 0.5500\n",
            "Epoch 16/300\n",
            "\n",
            "Epoch 00016: val_loss improved from 1.01126 to 1.00101, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6349 - acc: 0.9000 - val_loss: 1.0010 - val_acc: 0.5500\n",
            "Epoch 17/300\n",
            "\n",
            "Epoch 00017: val_loss improved from 1.00101 to 0.99081, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6261 - acc: 0.8500 - val_loss: 0.9908 - val_acc: 0.5500\n",
            "Epoch 18/300\n",
            "\n",
            "Epoch 00018: val_loss improved from 0.99081 to 0.98125, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6309 - acc: 0.8167 - val_loss: 0.9812 - val_acc: 0.5500\n",
            "Epoch 19/300\n",
            "\n",
            "Epoch 00019: val_loss improved from 0.98125 to 0.97446, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6152 - acc: 0.9167 - val_loss: 0.9745 - val_acc: 0.6000\n",
            "Epoch 20/300\n",
            "\n",
            "Epoch 00020: val_loss improved from 0.97446 to 0.96751, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5872 - acc: 0.9167 - val_loss: 0.9675 - val_acc: 0.5000\n",
            "Epoch 21/300\n",
            "\n",
            "Epoch 00021: val_loss improved from 0.96751 to 0.96005, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6278 - acc: 0.7667 - val_loss: 0.9601 - val_acc: 0.5000\n",
            "Epoch 22/300\n",
            "\n",
            "Epoch 00022: val_loss improved from 0.96005 to 0.95356, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6048 - acc: 0.8333 - val_loss: 0.9536 - val_acc: 0.5000\n",
            "Epoch 23/300\n",
            "\n",
            "Epoch 00023: val_loss improved from 0.95356 to 0.94833, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5834 - acc: 0.8667 - val_loss: 0.9483 - val_acc: 0.5500\n",
            "Epoch 24/300\n",
            "\n",
            "Epoch 00024: val_loss improved from 0.94833 to 0.94417, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5860 - acc: 0.8333 - val_loss: 0.9442 - val_acc: 0.5500\n",
            "Epoch 25/300\n",
            "\n",
            "Epoch 00025: val_loss improved from 0.94417 to 0.93909, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5902 - acc: 0.8500 - val_loss: 0.9391 - val_acc: 0.6000\n",
            "Epoch 26/300\n",
            "\n",
            "Epoch 00026: val_loss improved from 0.93909 to 0.93396, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5568 - acc: 0.8667 - val_loss: 0.9340 - val_acc: 0.6000\n",
            "Epoch 27/300\n",
            "\n",
            "Epoch 00027: val_loss improved from 0.93396 to 0.92924, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5573 - acc: 0.8667 - val_loss: 0.9292 - val_acc: 0.5500\n",
            "Epoch 28/300\n",
            "\n",
            "Epoch 00028: val_loss improved from 0.92924 to 0.92474, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5592 - acc: 0.7833 - val_loss: 0.9247 - val_acc: 0.5500\n",
            "Epoch 29/300\n",
            "\n",
            "Epoch 00029: val_loss improved from 0.92474 to 0.92076, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5567 - acc: 0.8333 - val_loss: 0.9208 - val_acc: 0.4000\n",
            "Epoch 30/300\n",
            "\n",
            "Epoch 00030: val_loss improved from 0.92076 to 0.91457, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5355 - acc: 0.9000 - val_loss: 0.9146 - val_acc: 0.4000\n",
            "Epoch 31/300\n",
            "\n",
            "Epoch 00031: val_loss improved from 0.91457 to 0.90624, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5275 - acc: 0.9000 - val_loss: 0.9062 - val_acc: 0.4500\n",
            "Epoch 32/300\n",
            "\n",
            "Epoch 00032: val_loss improved from 0.90624 to 0.90006, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4956 - acc: 0.9167 - val_loss: 0.9001 - val_acc: 0.4500\n",
            "Epoch 33/300\n",
            "\n",
            "Epoch 00033: val_loss improved from 0.90006 to 0.89459, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5266 - acc: 0.8167 - val_loss: 0.8946 - val_acc: 0.5000\n",
            "Epoch 34/300\n",
            "\n",
            "Epoch 00034: val_loss improved from 0.89459 to 0.88669, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5332 - acc: 0.7833 - val_loss: 0.8867 - val_acc: 0.5500\n",
            "Epoch 35/300\n",
            "\n",
            "Epoch 00035: val_loss improved from 0.88669 to 0.87860, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5036 - acc: 0.9000 - val_loss: 0.8786 - val_acc: 0.5500\n",
            "Epoch 36/300\n",
            "\n",
            "Epoch 00036: val_loss improved from 0.87860 to 0.87236, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4803 - acc: 0.9167 - val_loss: 0.8724 - val_acc: 0.5500\n",
            "Epoch 37/300\n",
            "\n",
            "Epoch 00037: val_loss improved from 0.87236 to 0.86907, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4944 - acc: 0.8667 - val_loss: 0.8691 - val_acc: 0.5500\n",
            "Epoch 38/300\n",
            "\n",
            "Epoch 00038: val_loss improved from 0.86907 to 0.86447, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4945 - acc: 0.8333 - val_loss: 0.8645 - val_acc: 0.5500\n",
            "Epoch 39/300\n",
            "\n",
            "Epoch 00039: val_loss improved from 0.86447 to 0.85794, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4917 - acc: 0.9000 - val_loss: 0.8579 - val_acc: 0.5000\n",
            "Epoch 40/300\n",
            "\n",
            "Epoch 00040: val_loss improved from 0.85794 to 0.85404, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4852 - acc: 0.8000 - val_loss: 0.8540 - val_acc: 0.5000\n",
            "Epoch 41/300\n",
            "\n",
            "Epoch 00041: val_loss improved from 0.85404 to 0.85237, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5062 - acc: 0.8833 - val_loss: 0.8524 - val_acc: 0.5000\n",
            "Epoch 42/300\n",
            "\n",
            "Epoch 00042: val_loss improved from 0.85237 to 0.84885, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4688 - acc: 0.9000 - val_loss: 0.8488 - val_acc: 0.5000\n",
            "Epoch 43/300\n",
            "\n",
            "Epoch 00043: val_loss improved from 0.84885 to 0.84566, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4813 - acc: 0.8833 - val_loss: 0.8457 - val_acc: 0.5000\n",
            "Epoch 44/300\n",
            "\n",
            "Epoch 00044: val_loss improved from 0.84566 to 0.83989, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4961 - acc: 0.8833 - val_loss: 0.8399 - val_acc: 0.5000\n",
            "Epoch 45/300\n",
            "\n",
            "Epoch 00045: val_loss improved from 0.83989 to 0.83464, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4584 - acc: 0.9333 - val_loss: 0.8346 - val_acc: 0.5000\n",
            "Epoch 46/300\n",
            "\n",
            "Epoch 00046: val_loss improved from 0.83464 to 0.83072, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4524 - acc: 0.9333 - val_loss: 0.8307 - val_acc: 0.5000\n",
            "Epoch 47/300\n",
            "\n",
            "Epoch 00047: val_loss improved from 0.83072 to 0.82531, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4597 - acc: 0.8500 - val_loss: 0.8253 - val_acc: 0.5000\n",
            "Epoch 48/300\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.82531\n",
            "60/60 - 0s - loss: 0.4575 - acc: 0.9167 - val_loss: 0.8270 - val_acc: 0.5000\n",
            "Epoch 49/300\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.82531\n",
            "60/60 - 0s - loss: 0.4189 - acc: 0.9167 - val_loss: 0.8310 - val_acc: 0.5000\n",
            "Epoch 50/300\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.82531\n",
            "60/60 - 0s - loss: 0.4346 - acc: 0.9167 - val_loss: 0.8346 - val_acc: 0.4500\n",
            "Epoch 51/300\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.82531\n",
            "60/60 - 0s - loss: 0.4521 - acc: 0.9333 - val_loss: 0.8303 - val_acc: 0.4500\n",
            "Epoch 52/300\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.82531\n",
            "60/60 - 0s - loss: 0.4207 - acc: 0.9500 - val_loss: 0.8274 - val_acc: 0.4500\n",
            "Epoch 53/300\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.82531\n",
            "60/60 - 0s - loss: 0.4022 - acc: 0.9167 - val_loss: 0.8313 - val_acc: 0.4500\n",
            "Epoch 54/300\n",
            "\n",
            "Epoch 00054: val_loss improved from 0.82531 to 0.82338, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4474 - acc: 0.8500 - val_loss: 0.8234 - val_acc: 0.4500\n",
            "Epoch 55/300\n",
            "\n",
            "Epoch 00055: val_loss improved from 0.82338 to 0.81614, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4147 - acc: 0.9167 - val_loss: 0.8161 - val_acc: 0.4500\n",
            "Epoch 56/300\n",
            "\n",
            "Epoch 00056: val_loss improved from 0.81614 to 0.81578, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3904 - acc: 0.9833 - val_loss: 0.8158 - val_acc: 0.4500\n",
            "Epoch 57/300\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.81578\n",
            "60/60 - 0s - loss: 0.3745 - acc: 0.9667 - val_loss: 0.8214 - val_acc: 0.5000\n",
            "Epoch 58/300\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.81578\n",
            "60/60 - 0s - loss: 0.3995 - acc: 0.9167 - val_loss: 0.8325 - val_acc: 0.5000\n",
            "Epoch 59/300\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.81578\n",
            "60/60 - 0s - loss: 0.3999 - acc: 0.9167 - val_loss: 0.8407 - val_acc: 0.5000\n",
            "Epoch 60/300\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.81578\n",
            "60/60 - 0s - loss: 0.3752 - acc: 0.9500 - val_loss: 0.8384 - val_acc: 0.5000\n",
            "Epoch 61/300\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.81578\n",
            "60/60 - 0s - loss: 0.4043 - acc: 0.9167 - val_loss: 0.8310 - val_acc: 0.5000\n",
            "Epoch 62/300\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.81578\n",
            "60/60 - 0s - loss: 0.4068 - acc: 0.8667 - val_loss: 0.8204 - val_acc: 0.5000\n",
            "Epoch 63/300\n",
            "\n",
            "Epoch 00063: val_loss improved from 0.81578 to 0.81419, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3703 - acc: 0.9667 - val_loss: 0.8142 - val_acc: 0.5000\n",
            "Epoch 64/300\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.81419\n",
            "60/60 - 0s - loss: 0.3789 - acc: 0.9167 - val_loss: 0.8241 - val_acc: 0.5000\n",
            "Epoch 65/300\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.81419\n",
            "60/60 - 0s - loss: 0.3797 - acc: 0.9333 - val_loss: 0.8345 - val_acc: 0.5000\n",
            "Epoch 66/300\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.81419\n",
            "60/60 - 0s - loss: 0.3728 - acc: 0.9500 - val_loss: 0.8447 - val_acc: 0.5000\n",
            "Epoch 67/300\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.81419\n",
            "60/60 - 0s - loss: 0.4188 - acc: 0.8833 - val_loss: 0.8519 - val_acc: 0.5000\n",
            "Epoch 68/300\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.81419\n",
            "60/60 - 0s - loss: 0.3745 - acc: 0.9500 - val_loss: 0.8511 - val_acc: 0.5000\n",
            "Epoch 69/300\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.81419\n",
            "60/60 - 0s - loss: 0.3768 - acc: 0.9500 - val_loss: 0.8460 - val_acc: 0.5000\n",
            "Epoch 70/300\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.81419\n",
            "60/60 - 0s - loss: 0.3570 - acc: 0.9500 - val_loss: 0.8575 - val_acc: 0.5000\n",
            "Epoch 71/300\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.81419\n",
            "60/60 - 0s - loss: 0.3739 - acc: 0.9333 - val_loss: 0.8662 - val_acc: 0.5000\n",
            "Epoch 72/300\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.81419\n",
            "60/60 - 0s - loss: 0.3905 - acc: 0.9000 - val_loss: 0.8907 - val_acc: 0.5500\n",
            "Epoch 73/300\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.81419\n",
            "60/60 - 0s - loss: 0.3631 - acc: 0.9333 - val_loss: 0.8952 - val_acc: 0.5500\n",
            "Epoch 74/300\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.81419\n",
            "60/60 - 0s - loss: 0.3242 - acc: 0.9667 - val_loss: 0.9064 - val_acc: 0.5500\n",
            "Epoch 75/300\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.81419\n",
            "60/60 - 0s - loss: 0.3241 - acc: 0.9500 - val_loss: 0.9241 - val_acc: 0.5500\n",
            "Epoch 76/300\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.81419\n",
            "60/60 - 0s - loss: 0.3018 - acc: 0.9667 - val_loss: 0.9343 - val_acc: 0.5500\n",
            "Epoch 77/300\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.81419\n",
            "60/60 - 0s - loss: 0.3339 - acc: 0.9833 - val_loss: 0.9586 - val_acc: 0.5500\n",
            "Epoch 78/300\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.81419\n",
            "60/60 - 0s - loss: 0.3539 - acc: 0.9167 - val_loss: 0.9247 - val_acc: 0.5500\n",
            "Epoch 79/300\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.81419\n",
            "60/60 - 0s - loss: 0.3425 - acc: 0.9167 - val_loss: 0.9046 - val_acc: 0.5500\n",
            "Epoch 80/300\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.81419\n",
            "60/60 - 0s - loss: 0.3430 - acc: 0.9000 - val_loss: 0.9013 - val_acc: 0.5500\n",
            "Epoch 81/300\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.81419\n",
            "60/60 - 0s - loss: 0.3399 - acc: 0.9667 - val_loss: 0.9046 - val_acc: 0.5500\n",
            "Epoch 82/300\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.81419\n",
            "60/60 - 0s - loss: 0.3457 - acc: 0.9167 - val_loss: 0.9118 - val_acc: 0.5500\n",
            "Epoch 83/300\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.81419\n",
            "60/60 - 0s - loss: 0.3458 - acc: 0.9167 - val_loss: 0.9455 - val_acc: 0.5500\n",
            "Epoch 84/300\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.81419\n",
            "60/60 - 0s - loss: 0.3420 - acc: 0.9000 - val_loss: 0.9676 - val_acc: 0.5500\n",
            "Epoch 85/300\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.81419\n",
            "60/60 - 0s - loss: 0.3133 - acc: 0.9500 - val_loss: 0.9721 - val_acc: 0.5500\n",
            "Epoch 86/300\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.81419\n",
            "60/60 - 0s - loss: 0.3008 - acc: 0.9333 - val_loss: 0.9578 - val_acc: 0.5500\n",
            "Epoch 87/300\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.81419\n",
            "60/60 - 0s - loss: 0.3395 - acc: 0.9333 - val_loss: 0.9253 - val_acc: 0.5500\n",
            "Epoch 88/300\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.81419\n",
            "60/60 - 0s - loss: 0.3110 - acc: 0.9167 - val_loss: 0.9204 - val_acc: 0.5500\n",
            "Epoch 89/300\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.81419\n",
            "60/60 - 0s - loss: 0.2941 - acc: 0.9000 - val_loss: 0.9130 - val_acc: 0.5500\n",
            "Epoch 90/300\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.81419\n",
            "60/60 - 0s - loss: 0.3032 - acc: 0.9333 - val_loss: 0.9180 - val_acc: 0.5500\n",
            "Epoch 91/300\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.81419\n",
            "60/60 - 0s - loss: 0.3087 - acc: 0.9333 - val_loss: 0.9618 - val_acc: 0.5500\n",
            "Epoch 92/300\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.81419\n",
            "60/60 - 0s - loss: 0.2882 - acc: 0.9500 - val_loss: 1.0069 - val_acc: 0.5500\n",
            "Epoch 93/300\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.81419\n",
            "60/60 - 0s - loss: 0.3004 - acc: 0.9333 - val_loss: 1.0083 - val_acc: 0.5500\n",
            "Epoch 94/300\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.81419\n",
            "60/60 - 0s - loss: 0.3443 - acc: 0.9000 - val_loss: 0.9721 - val_acc: 0.5500\n",
            "Epoch 95/300\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.81419\n",
            "60/60 - 0s - loss: 0.2970 - acc: 0.9667 - val_loss: 0.9875 - val_acc: 0.5500\n",
            "Epoch 96/300\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.81419\n",
            "60/60 - 0s - loss: 0.3116 - acc: 0.9167 - val_loss: 0.9793 - val_acc: 0.5500\n",
            "Epoch 97/300\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.81419\n",
            "60/60 - 0s - loss: 0.2885 - acc: 1.0000 - val_loss: 1.0087 - val_acc: 0.5500\n",
            "Epoch 98/300\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.81419\n",
            "60/60 - 0s - loss: 0.2817 - acc: 0.9500 - val_loss: 1.0132 - val_acc: 0.5500\n",
            "Epoch 99/300\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.81419\n",
            "60/60 - 0s - loss: 0.2914 - acc: 0.9500 - val_loss: 1.0116 - val_acc: 0.5500\n",
            "Epoch 100/300\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.81419\n",
            "60/60 - 0s - loss: 0.2972 - acc: 0.9500 - val_loss: 1.0299 - val_acc: 0.5500\n",
            "Epoch 101/300\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.81419\n",
            "60/60 - 0s - loss: 0.2741 - acc: 0.9333 - val_loss: 1.0347 - val_acc: 0.5500\n",
            "Epoch 102/300\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.81419\n",
            "60/60 - 0s - loss: 0.3000 - acc: 0.9333 - val_loss: 1.1139 - val_acc: 0.5500\n",
            "Epoch 103/300\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.81419\n",
            "60/60 - 0s - loss: 0.2595 - acc: 0.9833 - val_loss: 1.1540 - val_acc: 0.5500\n",
            "Epoch 104/300\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.81419\n",
            "60/60 - 0s - loss: 0.2485 - acc: 1.0000 - val_loss: 1.1436 - val_acc: 0.5500\n",
            "Epoch 105/300\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 0.81419\n",
            "60/60 - 0s - loss: 0.2534 - acc: 0.9833 - val_loss: 1.1448 - val_acc: 0.5500\n",
            "Epoch 106/300\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 0.81419\n",
            "60/60 - 0s - loss: 0.3148 - acc: 0.9167 - val_loss: 1.0998 - val_acc: 0.5500\n",
            "Epoch 107/300\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.81419\n",
            "60/60 - 0s - loss: 0.2992 - acc: 0.9167 - val_loss: 1.0834 - val_acc: 0.5500\n",
            "Epoch 108/300\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 0.81419\n",
            "60/60 - 0s - loss: 0.3108 - acc: 0.9500 - val_loss: 1.1086 - val_acc: 0.5500\n",
            "Epoch 109/300\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.81419\n",
            "60/60 - 0s - loss: 0.3057 - acc: 0.9333 - val_loss: 1.1463 - val_acc: 0.5500\n",
            "Epoch 110/300\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.81419\n",
            "60/60 - 0s - loss: 0.2928 - acc: 0.9500 - val_loss: 1.1232 - val_acc: 0.5500\n",
            "Epoch 111/300\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 0.81419\n",
            "60/60 - 0s - loss: 0.3564 - acc: 0.9167 - val_loss: 1.0920 - val_acc: 0.5500\n",
            "Epoch 112/300\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 0.81419\n",
            "60/60 - 0s - loss: 0.2860 - acc: 0.9333 - val_loss: 1.1372 - val_acc: 0.5500\n",
            "Epoch 113/300\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.81419\n",
            "60/60 - 0s - loss: 0.2594 - acc: 0.9667 - val_loss: 1.2105 - val_acc: 0.5500\n",
            "Epoch 114/300\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.81419\n",
            "60/60 - 0s - loss: 0.2311 - acc: 1.0000 - val_loss: 1.2616 - val_acc: 0.5500\n",
            "Epoch 115/300\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.81419\n",
            "60/60 - 0s - loss: 0.2730 - acc: 0.9667 - val_loss: 1.2384 - val_acc: 0.5500\n",
            "Epoch 116/300\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.81419\n",
            "60/60 - 0s - loss: 0.2674 - acc: 0.9500 - val_loss: 1.2455 - val_acc: 0.5500\n",
            "Epoch 117/300\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.81419\n",
            "60/60 - 0s - loss: 0.2521 - acc: 0.9333 - val_loss: 1.2201 - val_acc: 0.5500\n",
            "Epoch 118/300\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.81419\n",
            "60/60 - 0s - loss: 0.2566 - acc: 0.9500 - val_loss: 1.2435 - val_acc: 0.5500\n",
            "Epoch 119/300\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 0.81419\n",
            "60/60 - 0s - loss: 0.2733 - acc: 0.9333 - val_loss: 1.2369 - val_acc: 0.5500\n",
            "Epoch 120/300\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.81419\n",
            "60/60 - 0s - loss: 0.2811 - acc: 0.9500 - val_loss: 1.1838 - val_acc: 0.5500\n",
            "Epoch 121/300\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.81419\n",
            "60/60 - 0s - loss: 0.2749 - acc: 0.9500 - val_loss: 1.0959 - val_acc: 0.5500\n",
            "Epoch 122/300\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.81419\n",
            "60/60 - 0s - loss: 0.2646 - acc: 0.9500 - val_loss: 1.0670 - val_acc: 0.5500\n",
            "Epoch 123/300\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 0.81419\n",
            "60/60 - 0s - loss: 0.3049 - acc: 0.9333 - val_loss: 1.1097 - val_acc: 0.5500\n",
            "Epoch 124/300\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 0.81419\n",
            "60/60 - 0s - loss: 0.2737 - acc: 0.9500 - val_loss: 1.1935 - val_acc: 0.5500\n",
            "Epoch 125/300\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 0.81419\n",
            "60/60 - 0s - loss: 0.2940 - acc: 0.9000 - val_loss: 1.2520 - val_acc: 0.5500\n",
            "Epoch 126/300\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 0.81419\n",
            "60/60 - 0s - loss: 0.2806 - acc: 0.9500 - val_loss: 1.2996 - val_acc: 0.5500\n",
            "Epoch 127/300\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 0.81419\n",
            "60/60 - 0s - loss: 0.2553 - acc: 0.9333 - val_loss: 1.2431 - val_acc: 0.5500\n",
            "Epoch 128/300\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 0.81419\n",
            "60/60 - 0s - loss: 0.2652 - acc: 0.9500 - val_loss: 1.2068 - val_acc: 0.5500\n",
            "Epoch 129/300\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.81419\n",
            "60/60 - 0s - loss: 0.2454 - acc: 0.9667 - val_loss: 1.1800 - val_acc: 0.5500\n",
            "Epoch 130/300\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 0.81419\n",
            "60/60 - 0s - loss: 0.2819 - acc: 0.9667 - val_loss: 1.2817 - val_acc: 0.5500\n",
            "Epoch 131/300\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 0.81419\n",
            "60/60 - 0s - loss: 0.2338 - acc: 0.9333 - val_loss: 1.2613 - val_acc: 0.5500\n",
            "Epoch 132/300\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 0.81419\n",
            "60/60 - 0s - loss: 0.2347 - acc: 0.9833 - val_loss: 1.2793 - val_acc: 0.5500\n",
            "Epoch 133/300\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.81419\n",
            "60/60 - 0s - loss: 0.2156 - acc: 0.9833 - val_loss: 1.3109 - val_acc: 0.5500\n",
            "Epoch 134/300\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.81419\n",
            "60/60 - 0s - loss: 0.2347 - acc: 0.9333 - val_loss: 1.4019 - val_acc: 0.5500\n",
            "Epoch 135/300\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 0.81419\n",
            "60/60 - 0s - loss: 0.2661 - acc: 0.9667 - val_loss: 1.4010 - val_acc: 0.5500\n",
            "Epoch 136/300\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.81419\n",
            "60/60 - 0s - loss: 0.2677 - acc: 0.9500 - val_loss: 1.3273 - val_acc: 0.5500\n",
            "Epoch 137/300\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 0.81419\n",
            "60/60 - 0s - loss: 0.2242 - acc: 0.9333 - val_loss: 1.2887 - val_acc: 0.5500\n",
            "Epoch 138/300\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.81419\n",
            "60/60 - 0s - loss: 0.2356 - acc: 0.9333 - val_loss: 1.3314 - val_acc: 0.5500\n",
            "Epoch 139/300\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.81419\n",
            "60/60 - 0s - loss: 0.2623 - acc: 0.9333 - val_loss: 1.3570 - val_acc: 0.5500\n",
            "Epoch 140/300\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 0.81419\n",
            "60/60 - 0s - loss: 0.2385 - acc: 0.9667 - val_loss: 1.3416 - val_acc: 0.5500\n",
            "Epoch 141/300\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.81419\n",
            "60/60 - 0s - loss: 0.2457 - acc: 0.9500 - val_loss: 1.3997 - val_acc: 0.5500\n",
            "Epoch 142/300\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.81419\n",
            "60/60 - 0s - loss: 0.2019 - acc: 1.0000 - val_loss: 1.4099 - val_acc: 0.5500\n",
            "Epoch 143/300\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 0.81419\n",
            "60/60 - 0s - loss: 0.2704 - acc: 0.9500 - val_loss: 1.3360 - val_acc: 0.5500\n",
            "Epoch 144/300\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 0.81419\n",
            "60/60 - 0s - loss: 0.2366 - acc: 0.9500 - val_loss: 1.3195 - val_acc: 0.5500\n",
            "Epoch 145/300\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.81419\n",
            "60/60 - 0s - loss: 0.2232 - acc: 0.9500 - val_loss: 1.2832 - val_acc: 0.5500\n",
            "Epoch 146/300\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.81419\n",
            "60/60 - 0s - loss: 0.2820 - acc: 0.9167 - val_loss: 1.3234 - val_acc: 0.5500\n",
            "Epoch 147/300\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.81419\n",
            "60/60 - 0s - loss: 0.2368 - acc: 0.9333 - val_loss: 1.3435 - val_acc: 0.5500\n",
            "Epoch 148/300\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 0.81419\n",
            "60/60 - 0s - loss: 0.2287 - acc: 0.9333 - val_loss: 1.2908 - val_acc: 0.5500\n",
            "Epoch 149/300\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.81419\n",
            "60/60 - 0s - loss: 0.2546 - acc: 0.9667 - val_loss: 1.2625 - val_acc: 0.5500\n",
            "Epoch 150/300\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.81419\n",
            "60/60 - 0s - loss: 0.2387 - acc: 0.9333 - val_loss: 1.2513 - val_acc: 0.5500\n",
            "Epoch 151/300\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.81419\n",
            "60/60 - 0s - loss: 0.2145 - acc: 0.9667 - val_loss: 1.3206 - val_acc: 0.5500\n",
            "Epoch 152/300\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.81419\n",
            "60/60 - 0s - loss: 0.2248 - acc: 0.9667 - val_loss: 1.4034 - val_acc: 0.5500\n",
            "Epoch 153/300\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 0.81419\n",
            "60/60 - 0s - loss: 0.2181 - acc: 0.9833 - val_loss: 1.5042 - val_acc: 0.5500\n",
            "Epoch 154/300\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.81419\n",
            "60/60 - 0s - loss: 0.2506 - acc: 0.9167 - val_loss: 1.4706 - val_acc: 0.5500\n",
            "Epoch 155/300\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.81419\n",
            "60/60 - 0s - loss: 0.2216 - acc: 0.9833 - val_loss: 1.4681 - val_acc: 0.5500\n",
            "Epoch 156/300\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.81419\n",
            "60/60 - 0s - loss: 0.2506 - acc: 0.9333 - val_loss: 1.3838 - val_acc: 0.5500\n",
            "Epoch 157/300\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.81419\n",
            "60/60 - 0s - loss: 0.1866 - acc: 0.9500 - val_loss: 1.4090 - val_acc: 0.5500\n",
            "Epoch 158/300\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.81419\n",
            "60/60 - 0s - loss: 0.2112 - acc: 0.9833 - val_loss: 1.3514 - val_acc: 0.5500\n",
            "Epoch 159/300\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.81419\n",
            "60/60 - 0s - loss: 0.2176 - acc: 0.9833 - val_loss: 1.3556 - val_acc: 0.5500\n",
            "Epoch 160/300\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.81419\n",
            "60/60 - 0s - loss: 0.2096 - acc: 0.9667 - val_loss: 1.2758 - val_acc: 0.5500\n",
            "Epoch 161/300\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.81419\n",
            "60/60 - 0s - loss: 0.1689 - acc: 1.0000 - val_loss: 1.2351 - val_acc: 0.5500\n",
            "Epoch 162/300\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.81419\n",
            "60/60 - 0s - loss: 0.2478 - acc: 0.9500 - val_loss: 1.2981 - val_acc: 0.5500\n",
            "Epoch 163/300\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.81419\n",
            "60/60 - 0s - loss: 0.2068 - acc: 0.9667 - val_loss: 1.3370 - val_acc: 0.5500\n",
            "Epoch 164/300\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.81419\n",
            "60/60 - 0s - loss: 0.1925 - acc: 1.0000 - val_loss: 1.2764 - val_acc: 0.5500\n",
            "Epoch 165/300\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.81419\n",
            "60/60 - 0s - loss: 0.1851 - acc: 0.9667 - val_loss: 1.2199 - val_acc: 0.5500\n",
            "Epoch 166/300\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 0.81419\n",
            "60/60 - 0s - loss: 0.1830 - acc: 0.9500 - val_loss: 1.2074 - val_acc: 0.5500\n",
            "Epoch 167/300\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.81419\n",
            "60/60 - 0s - loss: 0.1807 - acc: 0.9667 - val_loss: 1.2633 - val_acc: 0.5500\n",
            "Epoch 168/300\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.81419\n",
            "60/60 - 0s - loss: 0.2071 - acc: 0.9500 - val_loss: 1.2508 - val_acc: 0.5500\n",
            "Epoch 169/300\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 0.81419\n",
            "60/60 - 0s - loss: 0.1988 - acc: 0.9667 - val_loss: 1.2258 - val_acc: 0.5500\n",
            "Epoch 170/300\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.81419\n",
            "60/60 - 0s - loss: 0.2146 - acc: 0.9667 - val_loss: 1.1275 - val_acc: 0.5500\n",
            "Epoch 171/300\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.81419\n",
            "60/60 - 0s - loss: 0.2113 - acc: 0.9167 - val_loss: 1.0400 - val_acc: 0.5500\n",
            "Epoch 172/300\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.81419\n",
            "60/60 - 0s - loss: 0.1948 - acc: 0.9667 - val_loss: 1.0562 - val_acc: 0.5500\n",
            "Epoch 173/300\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.81419\n",
            "60/60 - 0s - loss: 0.1525 - acc: 1.0000 - val_loss: 1.0674 - val_acc: 0.5500\n",
            "Epoch 174/300\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.81419\n",
            "60/60 - 0s - loss: 0.2010 - acc: 0.9667 - val_loss: 1.1774 - val_acc: 0.5500\n",
            "Epoch 175/300\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.81419\n",
            "60/60 - 0s - loss: 0.2144 - acc: 0.9833 - val_loss: 1.2838 - val_acc: 0.5500\n",
            "Epoch 176/300\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 0.81419\n",
            "60/60 - 0s - loss: 0.1863 - acc: 0.9667 - val_loss: 1.3219 - val_acc: 0.5500\n",
            "Epoch 177/300\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.81419\n",
            "60/60 - 0s - loss: 0.2051 - acc: 0.9833 - val_loss: 1.2328 - val_acc: 0.5500\n",
            "Epoch 178/300\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.81419\n",
            "60/60 - 0s - loss: 0.1906 - acc: 1.0000 - val_loss: 1.0930 - val_acc: 0.4500\n",
            "Epoch 179/300\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.81419\n",
            "60/60 - 0s - loss: 0.1680 - acc: 0.9833 - val_loss: 1.0239 - val_acc: 0.4500\n",
            "Epoch 180/300\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.81419\n",
            "60/60 - 0s - loss: 0.1650 - acc: 1.0000 - val_loss: 1.0070 - val_acc: 0.4500\n",
            "Epoch 181/300\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.81419\n",
            "60/60 - 0s - loss: 0.1900 - acc: 0.9833 - val_loss: 1.0447 - val_acc: 0.4500\n",
            "Epoch 182/300\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.81419\n",
            "60/60 - 0s - loss: 0.1820 - acc: 0.9833 - val_loss: 1.0329 - val_acc: 0.4500\n",
            "Epoch 183/300\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.81419\n",
            "60/60 - 0s - loss: 0.1755 - acc: 0.9667 - val_loss: 1.0890 - val_acc: 0.4500\n",
            "Epoch 184/300\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 0.81419\n",
            "60/60 - 0s - loss: 0.1822 - acc: 0.9833 - val_loss: 1.1149 - val_acc: 0.5000\n",
            "Epoch 185/300\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.81419\n",
            "60/60 - 0s - loss: 0.1879 - acc: 0.9500 - val_loss: 1.1601 - val_acc: 0.5000\n",
            "Epoch 186/300\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.81419\n",
            "60/60 - 0s - loss: 0.1783 - acc: 0.9667 - val_loss: 1.0855 - val_acc: 0.5000\n",
            "Epoch 187/300\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 0.81419\n",
            "60/60 - 0s - loss: 0.1808 - acc: 0.9833 - val_loss: 1.0090 - val_acc: 0.5000\n",
            "Epoch 188/300\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.81419\n",
            "60/60 - 0s - loss: 0.1719 - acc: 1.0000 - val_loss: 1.0086 - val_acc: 0.5000\n",
            "Epoch 189/300\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.81419\n",
            "60/60 - 0s - loss: 0.1908 - acc: 0.9833 - val_loss: 0.9356 - val_acc: 0.4500\n",
            "Epoch 190/300\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.81419\n",
            "60/60 - 0s - loss: 0.1860 - acc: 0.9833 - val_loss: 0.9245 - val_acc: 0.4000\n",
            "Epoch 191/300\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.81419\n",
            "60/60 - 0s - loss: 0.1621 - acc: 0.9833 - val_loss: 0.9208 - val_acc: 0.4500\n",
            "Epoch 192/300\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 0.81419\n",
            "60/60 - 0s - loss: 0.1941 - acc: 1.0000 - val_loss: 0.9198 - val_acc: 0.4500\n",
            "Epoch 193/300\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.81419\n",
            "60/60 - 0s - loss: 0.1690 - acc: 0.9667 - val_loss: 0.8911 - val_acc: 0.4500\n",
            "Epoch 194/300\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.81419\n",
            "60/60 - 0s - loss: 0.1930 - acc: 0.9833 - val_loss: 0.9101 - val_acc: 0.5000\n",
            "Epoch 195/300\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.81419\n",
            "60/60 - 0s - loss: 0.1884 - acc: 0.9833 - val_loss: 1.0344 - val_acc: 0.5000\n",
            "Epoch 196/300\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.81419\n",
            "60/60 - 0s - loss: 0.1764 - acc: 0.9667 - val_loss: 1.0458 - val_acc: 0.5000\n",
            "Epoch 197/300\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.81419\n",
            "60/60 - 0s - loss: 0.1749 - acc: 0.9500 - val_loss: 0.9874 - val_acc: 0.5500\n",
            "Epoch 198/300\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.81419\n",
            "60/60 - 0s - loss: 0.2158 - acc: 0.9333 - val_loss: 0.8731 - val_acc: 0.5000\n",
            "Epoch 199/300\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.81419\n",
            "60/60 - 0s - loss: 0.1476 - acc: 0.9667 - val_loss: 0.8609 - val_acc: 0.5500\n",
            "Epoch 200/300\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.81419\n",
            "60/60 - 0s - loss: 0.1584 - acc: 0.9833 - val_loss: 0.8733 - val_acc: 0.5000\n",
            "Epoch 201/300\n",
            "\n",
            "Epoch 00201: val_loss did not improve from 0.81419\n",
            "60/60 - 0s - loss: 0.1983 - acc: 0.9667 - val_loss: 0.9080 - val_acc: 0.5000\n",
            "Epoch 202/300\n",
            "\n",
            "Epoch 00202: val_loss did not improve from 0.81419\n",
            "60/60 - 0s - loss: 0.1504 - acc: 0.9833 - val_loss: 0.8543 - val_acc: 0.5500\n",
            "Epoch 203/300\n",
            "\n",
            "Epoch 00203: val_loss improved from 0.81419 to 0.80988, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1862 - acc: 0.9667 - val_loss: 0.8099 - val_acc: 0.5500\n",
            "Epoch 204/300\n",
            "\n",
            "Epoch 00204: val_loss did not improve from 0.80988\n",
            "60/60 - 0s - loss: 0.1911 - acc: 0.9667 - val_loss: 0.8269 - val_acc: 0.5500\n",
            "Epoch 205/300\n",
            "\n",
            "Epoch 00205: val_loss did not improve from 0.80988\n",
            "60/60 - 0s - loss: 0.1989 - acc: 0.9500 - val_loss: 0.8620 - val_acc: 0.5000\n",
            "Epoch 206/300\n",
            "\n",
            "Epoch 00206: val_loss did not improve from 0.80988\n",
            "60/60 - 0s - loss: 0.1610 - acc: 0.9833 - val_loss: 0.9690 - val_acc: 0.5000\n",
            "Epoch 207/300\n",
            "\n",
            "Epoch 00207: val_loss did not improve from 0.80988\n",
            "60/60 - 0s - loss: 0.1686 - acc: 0.9667 - val_loss: 0.9905 - val_acc: 0.5000\n",
            "Epoch 208/300\n",
            "\n",
            "Epoch 00208: val_loss did not improve from 0.80988\n",
            "60/60 - 0s - loss: 0.2014 - acc: 0.9667 - val_loss: 0.9495 - val_acc: 0.4500\n",
            "Epoch 209/300\n",
            "\n",
            "Epoch 00209: val_loss did not improve from 0.80988\n",
            "60/60 - 0s - loss: 0.1802 - acc: 0.9500 - val_loss: 0.9751 - val_acc: 0.4500\n",
            "Epoch 210/300\n",
            "\n",
            "Epoch 00210: val_loss did not improve from 0.80988\n",
            "60/60 - 0s - loss: 0.1480 - acc: 1.0000 - val_loss: 1.0368 - val_acc: 0.5500\n",
            "Epoch 211/300\n",
            "\n",
            "Epoch 00211: val_loss did not improve from 0.80988\n",
            "60/60 - 0s - loss: 0.1675 - acc: 1.0000 - val_loss: 1.0297 - val_acc: 0.5000\n",
            "Epoch 212/300\n",
            "\n",
            "Epoch 00212: val_loss did not improve from 0.80988\n",
            "60/60 - 0s - loss: 0.1566 - acc: 0.9667 - val_loss: 1.0126 - val_acc: 0.4500\n",
            "Epoch 213/300\n",
            "\n",
            "Epoch 00213: val_loss did not improve from 0.80988\n",
            "60/60 - 0s - loss: 0.1487 - acc: 0.9833 - val_loss: 0.9949 - val_acc: 0.4500\n",
            "Epoch 214/300\n",
            "\n",
            "Epoch 00214: val_loss did not improve from 0.80988\n",
            "60/60 - 0s - loss: 0.1877 - acc: 0.9500 - val_loss: 1.0021 - val_acc: 0.4500\n",
            "Epoch 215/300\n",
            "\n",
            "Epoch 00215: val_loss did not improve from 0.80988\n",
            "60/60 - 0s - loss: 0.1349 - acc: 0.9833 - val_loss: 1.0471 - val_acc: 0.4500\n",
            "Epoch 216/300\n",
            "\n",
            "Epoch 00216: val_loss did not improve from 0.80988\n",
            "60/60 - 0s - loss: 0.1638 - acc: 0.9667 - val_loss: 1.1081 - val_acc: 0.4500\n",
            "Epoch 217/300\n",
            "\n",
            "Epoch 00217: val_loss did not improve from 0.80988\n",
            "60/60 - 0s - loss: 0.1879 - acc: 0.9667 - val_loss: 1.1263 - val_acc: 0.5000\n",
            "Epoch 218/300\n",
            "\n",
            "Epoch 00218: val_loss did not improve from 0.80988\n",
            "60/60 - 0s - loss: 0.1412 - acc: 1.0000 - val_loss: 1.0924 - val_acc: 0.5000\n",
            "Epoch 219/300\n",
            "\n",
            "Epoch 00219: val_loss did not improve from 0.80988\n",
            "60/60 - 0s - loss: 0.1796 - acc: 0.9833 - val_loss: 1.0793 - val_acc: 0.5000\n",
            "Epoch 220/300\n",
            "\n",
            "Epoch 00220: val_loss did not improve from 0.80988\n",
            "60/60 - 0s - loss: 0.1314 - acc: 1.0000 - val_loss: 1.0511 - val_acc: 0.5000\n",
            "Epoch 221/300\n",
            "\n",
            "Epoch 00221: val_loss did not improve from 0.80988\n",
            "60/60 - 0s - loss: 0.1346 - acc: 0.9833 - val_loss: 1.0101 - val_acc: 0.5000\n",
            "Epoch 222/300\n",
            "\n",
            "Epoch 00222: val_loss did not improve from 0.80988\n",
            "60/60 - 0s - loss: 0.1822 - acc: 0.9500 - val_loss: 0.9991 - val_acc: 0.5000\n",
            "Epoch 223/300\n",
            "\n",
            "Epoch 00223: val_loss did not improve from 0.80988\n",
            "60/60 - 0s - loss: 0.1603 - acc: 0.9667 - val_loss: 1.0522 - val_acc: 0.5000\n",
            "Epoch 224/300\n",
            "\n",
            "Epoch 00224: val_loss did not improve from 0.80988\n",
            "60/60 - 0s - loss: 0.1161 - acc: 1.0000 - val_loss: 1.0880 - val_acc: 0.5000\n",
            "Epoch 225/300\n",
            "\n",
            "Epoch 00225: val_loss did not improve from 0.80988\n",
            "60/60 - 0s - loss: 0.1668 - acc: 0.9667 - val_loss: 1.1675 - val_acc: 0.5000\n",
            "Epoch 226/300\n",
            "\n",
            "Epoch 00226: val_loss did not improve from 0.80988\n",
            "60/60 - 0s - loss: 0.1563 - acc: 0.9333 - val_loss: 1.1421 - val_acc: 0.5000\n",
            "Epoch 227/300\n",
            "\n",
            "Epoch 00227: val_loss did not improve from 0.80988\n",
            "60/60 - 0s - loss: 0.1569 - acc: 1.0000 - val_loss: 1.0713 - val_acc: 0.5500\n",
            "Epoch 228/300\n",
            "\n",
            "Epoch 00228: val_loss did not improve from 0.80988\n",
            "60/60 - 0s - loss: 0.1668 - acc: 0.9667 - val_loss: 1.1392 - val_acc: 0.5000\n",
            "Epoch 229/300\n",
            "\n",
            "Epoch 00229: val_loss did not improve from 0.80988\n",
            "60/60 - 0s - loss: 0.1183 - acc: 1.0000 - val_loss: 1.2157 - val_acc: 0.5000\n",
            "Epoch 230/300\n",
            "\n",
            "Epoch 00230: val_loss did not improve from 0.80988\n",
            "60/60 - 0s - loss: 0.1548 - acc: 1.0000 - val_loss: 1.2072 - val_acc: 0.5000\n",
            "Epoch 231/300\n",
            "\n",
            "Epoch 00231: val_loss did not improve from 0.80988\n",
            "60/60 - 0s - loss: 0.1803 - acc: 0.9833 - val_loss: 1.2158 - val_acc: 0.5000\n",
            "Epoch 232/300\n",
            "\n",
            "Epoch 00232: val_loss did not improve from 0.80988\n",
            "60/60 - 0s - loss: 0.1627 - acc: 0.9667 - val_loss: 1.1807 - val_acc: 0.5000\n",
            "Epoch 233/300\n",
            "\n",
            "Epoch 00233: val_loss did not improve from 0.80988\n",
            "60/60 - 0s - loss: 0.1416 - acc: 0.9667 - val_loss: 1.1918 - val_acc: 0.5000\n",
            "Epoch 234/300\n",
            "\n",
            "Epoch 00234: val_loss did not improve from 0.80988\n",
            "60/60 - 0s - loss: 0.1193 - acc: 1.0000 - val_loss: 1.2217 - val_acc: 0.5000\n",
            "Epoch 235/300\n",
            "\n",
            "Epoch 00235: val_loss did not improve from 0.80988\n",
            "60/60 - 0s - loss: 0.1148 - acc: 1.0000 - val_loss: 1.2293 - val_acc: 0.5000\n",
            "Epoch 236/300\n",
            "\n",
            "Epoch 00236: val_loss did not improve from 0.80988\n",
            "60/60 - 0s - loss: 0.1263 - acc: 1.0000 - val_loss: 1.2593 - val_acc: 0.4500\n",
            "Epoch 237/300\n",
            "\n",
            "Epoch 00237: val_loss did not improve from 0.80988\n",
            "60/60 - 0s - loss: 0.1540 - acc: 0.9667 - val_loss: 1.3187 - val_acc: 0.5000\n",
            "Epoch 238/300\n",
            "\n",
            "Epoch 00238: val_loss did not improve from 0.80988\n",
            "60/60 - 0s - loss: 0.1291 - acc: 1.0000 - val_loss: 1.2669 - val_acc: 0.5500\n",
            "Epoch 239/300\n",
            "\n",
            "Epoch 00239: val_loss did not improve from 0.80988\n",
            "60/60 - 0s - loss: 0.1291 - acc: 0.9833 - val_loss: 1.1718 - val_acc: 0.5500\n",
            "Epoch 240/300\n",
            "\n",
            "Epoch 00240: val_loss did not improve from 0.80988\n",
            "60/60 - 0s - loss: 0.1469 - acc: 1.0000 - val_loss: 1.1492 - val_acc: 0.5500\n",
            "Epoch 241/300\n",
            "\n",
            "Epoch 00241: val_loss did not improve from 0.80988\n",
            "60/60 - 0s - loss: 0.1475 - acc: 0.9667 - val_loss: 1.0560 - val_acc: 0.5000\n",
            "Epoch 242/300\n",
            "\n",
            "Epoch 00242: val_loss did not improve from 0.80988\n",
            "60/60 - 0s - loss: 0.1303 - acc: 0.9667 - val_loss: 1.0262 - val_acc: 0.5000\n",
            "Epoch 243/300\n",
            "\n",
            "Epoch 00243: val_loss did not improve from 0.80988\n",
            "60/60 - 0s - loss: 0.1142 - acc: 0.9833 - val_loss: 1.0556 - val_acc: 0.5000\n",
            "Epoch 244/300\n",
            "\n",
            "Epoch 00244: val_loss did not improve from 0.80988\n",
            "60/60 - 0s - loss: 0.1228 - acc: 1.0000 - val_loss: 1.0849 - val_acc: 0.5000\n",
            "Epoch 245/300\n",
            "\n",
            "Epoch 00245: val_loss did not improve from 0.80988\n",
            "60/60 - 0s - loss: 0.1571 - acc: 0.9667 - val_loss: 1.1517 - val_acc: 0.5000\n",
            "Epoch 246/300\n",
            "\n",
            "Epoch 00246: val_loss did not improve from 0.80988\n",
            "60/60 - 0s - loss: 0.1697 - acc: 0.9667 - val_loss: 1.1362 - val_acc: 0.5000\n",
            "Epoch 247/300\n",
            "\n",
            "Epoch 00247: val_loss did not improve from 0.80988\n",
            "60/60 - 0s - loss: 0.1340 - acc: 1.0000 - val_loss: 1.1368 - val_acc: 0.5000\n",
            "Epoch 248/300\n",
            "\n",
            "Epoch 00248: val_loss did not improve from 0.80988\n",
            "60/60 - 0s - loss: 0.1398 - acc: 0.9667 - val_loss: 1.0833 - val_acc: 0.5000\n",
            "Epoch 249/300\n",
            "\n",
            "Epoch 00249: val_loss did not improve from 0.80988\n",
            "60/60 - 0s - loss: 0.1602 - acc: 0.9667 - val_loss: 1.0942 - val_acc: 0.4500\n",
            "Epoch 250/300\n",
            "\n",
            "Epoch 00250: val_loss did not improve from 0.80988\n",
            "60/60 - 0s - loss: 0.1336 - acc: 0.9833 - val_loss: 1.1250 - val_acc: 0.5000\n",
            "Epoch 251/300\n",
            "\n",
            "Epoch 00251: val_loss did not improve from 0.80988\n",
            "60/60 - 0s - loss: 0.1393 - acc: 0.9667 - val_loss: 1.1779 - val_acc: 0.4500\n",
            "Epoch 252/300\n",
            "\n",
            "Epoch 00252: val_loss did not improve from 0.80988\n",
            "60/60 - 0s - loss: 0.1261 - acc: 1.0000 - val_loss: 1.1911 - val_acc: 0.4500\n",
            "Epoch 253/300\n",
            "\n",
            "Epoch 00253: val_loss did not improve from 0.80988\n",
            "60/60 - 0s - loss: 0.1345 - acc: 0.9667 - val_loss: 1.2065 - val_acc: 0.4500\n",
            "Epoch 254/300\n",
            "\n",
            "Epoch 00254: val_loss did not improve from 0.80988\n",
            "60/60 - 0s - loss: 0.1124 - acc: 0.9833 - val_loss: 1.2477 - val_acc: 0.4500\n",
            "Epoch 255/300\n",
            "\n",
            "Epoch 00255: val_loss did not improve from 0.80988\n",
            "60/60 - 0s - loss: 0.1348 - acc: 0.9833 - val_loss: 1.2643 - val_acc: 0.4500\n",
            "Epoch 256/300\n",
            "\n",
            "Epoch 00256: val_loss did not improve from 0.80988\n",
            "60/60 - 0s - loss: 0.1095 - acc: 1.0000 - val_loss: 1.2690 - val_acc: 0.4500\n",
            "Epoch 257/300\n",
            "\n",
            "Epoch 00257: val_loss did not improve from 0.80988\n",
            "60/60 - 0s - loss: 0.1020 - acc: 1.0000 - val_loss: 1.2562 - val_acc: 0.4500\n",
            "Epoch 258/300\n",
            "\n",
            "Epoch 00258: val_loss did not improve from 0.80988\n",
            "60/60 - 0s - loss: 0.1044 - acc: 1.0000 - val_loss: 1.2462 - val_acc: 0.4500\n",
            "Epoch 259/300\n",
            "\n",
            "Epoch 00259: val_loss did not improve from 0.80988\n",
            "60/60 - 0s - loss: 0.1269 - acc: 0.9667 - val_loss: 1.2124 - val_acc: 0.4500\n",
            "Epoch 260/300\n",
            "\n",
            "Epoch 00260: val_loss did not improve from 0.80988\n",
            "60/60 - 0s - loss: 0.1266 - acc: 0.9667 - val_loss: 1.2152 - val_acc: 0.5500\n",
            "Epoch 261/300\n",
            "\n",
            "Epoch 00261: val_loss did not improve from 0.80988\n",
            "60/60 - 0s - loss: 0.1271 - acc: 0.9833 - val_loss: 1.2419 - val_acc: 0.6000\n",
            "Epoch 262/300\n",
            "\n",
            "Epoch 00262: val_loss did not improve from 0.80988\n",
            "60/60 - 0s - loss: 0.1220 - acc: 0.9833 - val_loss: 1.2240 - val_acc: 0.6000\n",
            "Epoch 263/300\n",
            "\n",
            "Epoch 00263: val_loss did not improve from 0.80988\n",
            "60/60 - 0s - loss: 0.1322 - acc: 0.9833 - val_loss: 1.2345 - val_acc: 0.5000\n",
            "Epoch 264/300\n",
            "\n",
            "Epoch 00264: val_loss did not improve from 0.80988\n",
            "60/60 - 0s - loss: 0.1014 - acc: 1.0000 - val_loss: 1.2610 - val_acc: 0.5500\n",
            "Epoch 265/300\n",
            "\n",
            "Epoch 00265: val_loss did not improve from 0.80988\n",
            "60/60 - 0s - loss: 0.1321 - acc: 0.9667 - val_loss: 1.2492 - val_acc: 0.5000\n",
            "Epoch 266/300\n",
            "\n",
            "Epoch 00266: val_loss did not improve from 0.80988\n",
            "60/60 - 0s - loss: 0.1418 - acc: 0.9667 - val_loss: 1.2175 - val_acc: 0.4500\n",
            "Epoch 267/300\n",
            "\n",
            "Epoch 00267: val_loss did not improve from 0.80988\n",
            "60/60 - 0s - loss: 0.1053 - acc: 0.9833 - val_loss: 1.2300 - val_acc: 0.4500\n",
            "Epoch 268/300\n",
            "\n",
            "Epoch 00268: val_loss did not improve from 0.80988\n",
            "60/60 - 0s - loss: 0.1258 - acc: 1.0000 - val_loss: 1.2291 - val_acc: 0.5000\n",
            "Epoch 269/300\n",
            "\n",
            "Epoch 00269: val_loss did not improve from 0.80988\n",
            "60/60 - 0s - loss: 0.0978 - acc: 1.0000 - val_loss: 1.2082 - val_acc: 0.5000\n",
            "Epoch 270/300\n",
            "\n",
            "Epoch 00270: val_loss did not improve from 0.80988\n",
            "60/60 - 0s - loss: 0.1272 - acc: 0.9833 - val_loss: 1.2166 - val_acc: 0.4500\n",
            "Epoch 271/300\n",
            "\n",
            "Epoch 00271: val_loss did not improve from 0.80988\n",
            "60/60 - 0s - loss: 0.1288 - acc: 0.9833 - val_loss: 1.2200 - val_acc: 0.4500\n",
            "Epoch 272/300\n",
            "\n",
            "Epoch 00272: val_loss did not improve from 0.80988\n",
            "60/60 - 0s - loss: 0.1345 - acc: 0.9667 - val_loss: 1.2375 - val_acc: 0.4500\n",
            "Epoch 273/300\n",
            "\n",
            "Epoch 00273: val_loss did not improve from 0.80988\n",
            "60/60 - 0s - loss: 0.1186 - acc: 1.0000 - val_loss: 1.2458 - val_acc: 0.4500\n",
            "Epoch 274/300\n",
            "\n",
            "Epoch 00274: val_loss did not improve from 0.80988\n",
            "60/60 - 0s - loss: 0.0989 - acc: 0.9833 - val_loss: 1.2480 - val_acc: 0.4500\n",
            "Epoch 275/300\n",
            "\n",
            "Epoch 00275: val_loss did not improve from 0.80988\n",
            "60/60 - 0s - loss: 0.0907 - acc: 1.0000 - val_loss: 1.2672 - val_acc: 0.4500\n",
            "Epoch 276/300\n",
            "\n",
            "Epoch 00276: val_loss did not improve from 0.80988\n",
            "60/60 - 0s - loss: 0.0999 - acc: 1.0000 - val_loss: 1.2720 - val_acc: 0.4500\n",
            "Epoch 277/300\n",
            "\n",
            "Epoch 00277: val_loss did not improve from 0.80988\n",
            "60/60 - 0s - loss: 0.1028 - acc: 1.0000 - val_loss: 1.2805 - val_acc: 0.4500\n",
            "Epoch 278/300\n",
            "\n",
            "Epoch 00278: val_loss did not improve from 0.80988\n",
            "60/60 - 0s - loss: 0.1517 - acc: 0.9667 - val_loss: 1.2762 - val_acc: 0.4500\n",
            "Epoch 279/300\n",
            "\n",
            "Epoch 00279: val_loss did not improve from 0.80988\n",
            "60/60 - 0s - loss: 0.0881 - acc: 1.0000 - val_loss: 1.2278 - val_acc: 0.4500\n",
            "Epoch 280/300\n",
            "\n",
            "Epoch 00280: val_loss did not improve from 0.80988\n",
            "60/60 - 0s - loss: 0.1149 - acc: 1.0000 - val_loss: 1.2414 - val_acc: 0.5000\n",
            "Epoch 281/300\n",
            "\n",
            "Epoch 00281: val_loss did not improve from 0.80988\n",
            "60/60 - 0s - loss: 0.0835 - acc: 1.0000 - val_loss: 1.2685 - val_acc: 0.5000\n",
            "Epoch 282/300\n",
            "\n",
            "Epoch 00282: val_loss did not improve from 0.80988\n",
            "60/60 - 0s - loss: 0.1106 - acc: 0.9667 - val_loss: 1.2878 - val_acc: 0.4500\n",
            "Epoch 283/300\n",
            "\n",
            "Epoch 00283: val_loss did not improve from 0.80988\n",
            "60/60 - 0s - loss: 0.0863 - acc: 1.0000 - val_loss: 1.2920 - val_acc: 0.4500\n",
            "Epoch 284/300\n",
            "\n",
            "Epoch 00284: val_loss did not improve from 0.80988\n",
            "60/60 - 0s - loss: 0.1019 - acc: 0.9833 - val_loss: 1.2895 - val_acc: 0.5500\n",
            "Epoch 285/300\n",
            "\n",
            "Epoch 00285: val_loss did not improve from 0.80988\n",
            "60/60 - 0s - loss: 0.1049 - acc: 0.9833 - val_loss: 1.2818 - val_acc: 0.6000\n",
            "Epoch 286/300\n",
            "\n",
            "Epoch 00286: val_loss did not improve from 0.80988\n",
            "60/60 - 0s - loss: 0.1209 - acc: 0.9500 - val_loss: 1.2822 - val_acc: 0.5500\n",
            "Epoch 287/300\n",
            "\n",
            "Epoch 00287: val_loss did not improve from 0.80988\n",
            "60/60 - 0s - loss: 0.0709 - acc: 1.0000 - val_loss: 1.2566 - val_acc: 0.4500\n",
            "Epoch 288/300\n",
            "\n",
            "Epoch 00288: val_loss did not improve from 0.80988\n",
            "60/60 - 0s - loss: 0.0949 - acc: 1.0000 - val_loss: 1.2578 - val_acc: 0.4500\n",
            "Epoch 289/300\n",
            "\n",
            "Epoch 00289: val_loss did not improve from 0.80988\n",
            "60/60 - 0s - loss: 0.1051 - acc: 0.9833 - val_loss: 1.2983 - val_acc: 0.4500\n",
            "Epoch 290/300\n",
            "\n",
            "Epoch 00290: val_loss did not improve from 0.80988\n",
            "60/60 - 0s - loss: 0.1223 - acc: 0.9667 - val_loss: 1.3758 - val_acc: 0.4000\n",
            "Epoch 291/300\n",
            "\n",
            "Epoch 00291: val_loss did not improve from 0.80988\n",
            "60/60 - 0s - loss: 0.1299 - acc: 0.9500 - val_loss: 1.4009 - val_acc: 0.6000\n",
            "Epoch 292/300\n",
            "\n",
            "Epoch 00292: val_loss did not improve from 0.80988\n",
            "60/60 - 0s - loss: 0.0921 - acc: 0.9833 - val_loss: 1.4122 - val_acc: 0.5500\n",
            "Epoch 293/300\n",
            "\n",
            "Epoch 00293: val_loss did not improve from 0.80988\n",
            "60/60 - 0s - loss: 0.1140 - acc: 0.9667 - val_loss: 1.3999 - val_acc: 0.4500\n",
            "Epoch 294/300\n",
            "\n",
            "Epoch 00294: val_loss did not improve from 0.80988\n",
            "60/60 - 0s - loss: 0.1099 - acc: 1.0000 - val_loss: 1.4134 - val_acc: 0.4500\n",
            "Epoch 295/300\n",
            "\n",
            "Epoch 00295: val_loss did not improve from 0.80988\n",
            "60/60 - 0s - loss: 0.1150 - acc: 0.9833 - val_loss: 1.4658 - val_acc: 0.4500\n",
            "Epoch 296/300\n",
            "\n",
            "Epoch 00296: val_loss did not improve from 0.80988\n",
            "60/60 - 0s - loss: 0.1079 - acc: 0.9833 - val_loss: 1.4863 - val_acc: 0.5000\n",
            "Epoch 297/300\n",
            "\n",
            "Epoch 00297: val_loss did not improve from 0.80988\n",
            "60/60 - 0s - loss: 0.1517 - acc: 0.9833 - val_loss: 1.3948 - val_acc: 0.4500\n",
            "Epoch 298/300\n",
            "\n",
            "Epoch 00298: val_loss did not improve from 0.80988\n",
            "60/60 - 0s - loss: 0.1433 - acc: 0.9333 - val_loss: 1.4297 - val_acc: 0.4500\n",
            "Epoch 299/300\n",
            "\n",
            "Epoch 00299: val_loss did not improve from 0.80988\n",
            "60/60 - 0s - loss: 0.1253 - acc: 0.9833 - val_loss: 1.4674 - val_acc: 0.4500\n",
            "Epoch 300/300\n",
            "\n",
            "Epoch 00300: val_loss did not improve from 0.80988\n",
            "60/60 - 0s - loss: 0.1406 - acc: 0.9667 - val_loss: 1.4603 - val_acc: 0.5000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUZSM8/+lexRNHBTWTwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "# Evaluate on test data\n",
            "40/40 [==============================] - 0s 2ms/sample - loss: 0.5887 - acc: 0.6750\n",
            "test loss, test acc: [0.5886855410528369, 0.675]\n",
            "EEG_Deep/Data2A/parsed_P010T.mat\n",
            "Filtering of Training Data Finished\n",
            "EEG_Deep/Data2A/parsed_P010E.mat\n",
            "Filtering of Testing Data Finished\n",
            "(1, 2)\n",
            "Finding labels in training data\n",
            "Finding labels in testing data\n",
            "[2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1]\n",
            "(60, 12, 1536)\n",
            "(60,)\n",
            "(20, 12, 1536)\n",
            "(20,)\n",
            "[[0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]]\n",
            "X_train shape: (60, 1, 12, 1536)\n",
            "60 train samples\n",
            "20 val samples\n",
            "(40, 12, 1536)\n",
            "(40, 1)\n",
            "X_train shape: (40, 1, 12, 1536)\n",
            "40 train samples\n",
            "Train on 60 samples, validate on 20 samples\n",
            "Epoch 1/300\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 1.35982, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 1s - loss: 1.3539 - acc: 0.3167 - val_loss: 1.3598 - val_acc: 0.4500\n",
            "Epoch 2/300\n",
            "\n",
            "Epoch 00002: val_loss improved from 1.35982 to 1.32944, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 1.1046 - acc: 0.6000 - val_loss: 1.3294 - val_acc: 0.4500\n",
            "Epoch 3/300\n",
            "\n",
            "Epoch 00003: val_loss improved from 1.32944 to 1.30257, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.9595 - acc: 0.6833 - val_loss: 1.3026 - val_acc: 0.5000\n",
            "Epoch 4/300\n",
            "\n",
            "Epoch 00004: val_loss improved from 1.30257 to 1.27689, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.8660 - acc: 0.6833 - val_loss: 1.2769 - val_acc: 0.5000\n",
            "Epoch 5/300\n",
            "\n",
            "Epoch 00005: val_loss improved from 1.27689 to 1.25225, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.7986 - acc: 0.6667 - val_loss: 1.2522 - val_acc: 0.5000\n",
            "Epoch 6/300\n",
            "\n",
            "Epoch 00006: val_loss improved from 1.25225 to 1.22922, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.7608 - acc: 0.7500 - val_loss: 1.2292 - val_acc: 0.5000\n",
            "Epoch 7/300\n",
            "\n",
            "Epoch 00007: val_loss improved from 1.22922 to 1.20748, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.7194 - acc: 0.8167 - val_loss: 1.2075 - val_acc: 0.5000\n",
            "Epoch 8/300\n",
            "\n",
            "Epoch 00008: val_loss improved from 1.20748 to 1.18665, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6586 - acc: 0.9167 - val_loss: 1.1866 - val_acc: 0.5000\n",
            "Epoch 9/300\n",
            "\n",
            "Epoch 00009: val_loss improved from 1.18665 to 1.16689, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6595 - acc: 0.9000 - val_loss: 1.1669 - val_acc: 0.5000\n",
            "Epoch 10/300\n",
            "\n",
            "Epoch 00010: val_loss improved from 1.16689 to 1.14720, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6237 - acc: 0.8667 - val_loss: 1.1472 - val_acc: 0.5500\n",
            "Epoch 11/300\n",
            "\n",
            "Epoch 00011: val_loss improved from 1.14720 to 1.12726, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6111 - acc: 0.9167 - val_loss: 1.1273 - val_acc: 0.6500\n",
            "Epoch 12/300\n",
            "\n",
            "Epoch 00012: val_loss improved from 1.12726 to 1.10709, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5935 - acc: 0.9167 - val_loss: 1.1071 - val_acc: 0.6500\n",
            "Epoch 13/300\n",
            "\n",
            "Epoch 00013: val_loss improved from 1.10709 to 1.08761, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5694 - acc: 0.9000 - val_loss: 1.0876 - val_acc: 0.7500\n",
            "Epoch 14/300\n",
            "\n",
            "Epoch 00014: val_loss improved from 1.08761 to 1.06784, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5408 - acc: 0.9167 - val_loss: 1.0678 - val_acc: 0.8000\n",
            "Epoch 15/300\n",
            "\n",
            "Epoch 00015: val_loss improved from 1.06784 to 1.04814, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5584 - acc: 0.9000 - val_loss: 1.0481 - val_acc: 0.8000\n",
            "Epoch 16/300\n",
            "\n",
            "Epoch 00016: val_loss improved from 1.04814 to 1.02959, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5327 - acc: 0.9333 - val_loss: 1.0296 - val_acc: 0.8500\n",
            "Epoch 17/300\n",
            "\n",
            "Epoch 00017: val_loss improved from 1.02959 to 1.01104, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4880 - acc: 0.9167 - val_loss: 1.0110 - val_acc: 0.8500\n",
            "Epoch 18/300\n",
            "\n",
            "Epoch 00018: val_loss improved from 1.01104 to 0.99197, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5361 - acc: 0.9000 - val_loss: 0.9920 - val_acc: 0.8000\n",
            "Epoch 19/300\n",
            "\n",
            "Epoch 00019: val_loss improved from 0.99197 to 0.97282, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4887 - acc: 0.9167 - val_loss: 0.9728 - val_acc: 0.8000\n",
            "Epoch 20/300\n",
            "\n",
            "Epoch 00020: val_loss improved from 0.97282 to 0.95452, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5225 - acc: 0.8333 - val_loss: 0.9545 - val_acc: 0.8000\n",
            "Epoch 21/300\n",
            "\n",
            "Epoch 00021: val_loss improved from 0.95452 to 0.93820, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4528 - acc: 0.9167 - val_loss: 0.9382 - val_acc: 0.8000\n",
            "Epoch 22/300\n",
            "\n",
            "Epoch 00022: val_loss improved from 0.93820 to 0.91944, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4470 - acc: 0.9500 - val_loss: 0.9194 - val_acc: 0.9000\n",
            "Epoch 23/300\n",
            "\n",
            "Epoch 00023: val_loss improved from 0.91944 to 0.90154, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4707 - acc: 0.9167 - val_loss: 0.9015 - val_acc: 0.9000\n",
            "Epoch 24/300\n",
            "\n",
            "Epoch 00024: val_loss improved from 0.90154 to 0.88192, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4745 - acc: 0.9000 - val_loss: 0.8819 - val_acc: 0.9000\n",
            "Epoch 25/300\n",
            "\n",
            "Epoch 00025: val_loss improved from 0.88192 to 0.86596, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4040 - acc: 0.9667 - val_loss: 0.8660 - val_acc: 0.9000\n",
            "Epoch 26/300\n",
            "\n",
            "Epoch 00026: val_loss improved from 0.86596 to 0.84930, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4386 - acc: 0.9000 - val_loss: 0.8493 - val_acc: 0.9000\n",
            "Epoch 27/300\n",
            "\n",
            "Epoch 00027: val_loss improved from 0.84930 to 0.83185, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3990 - acc: 0.9500 - val_loss: 0.8318 - val_acc: 0.9000\n",
            "Epoch 28/300\n",
            "\n",
            "Epoch 00028: val_loss improved from 0.83185 to 0.81378, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4302 - acc: 0.9000 - val_loss: 0.8138 - val_acc: 0.9000\n",
            "Epoch 29/300\n",
            "\n",
            "Epoch 00029: val_loss improved from 0.81378 to 0.79871, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3681 - acc: 0.9167 - val_loss: 0.7987 - val_acc: 0.9000\n",
            "Epoch 30/300\n",
            "\n",
            "Epoch 00030: val_loss improved from 0.79871 to 0.78338, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4117 - acc: 0.9000 - val_loss: 0.7834 - val_acc: 0.9000\n",
            "Epoch 31/300\n",
            "\n",
            "Epoch 00031: val_loss improved from 0.78338 to 0.76905, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3961 - acc: 0.9167 - val_loss: 0.7691 - val_acc: 0.9000\n",
            "Epoch 32/300\n",
            "\n",
            "Epoch 00032: val_loss improved from 0.76905 to 0.75156, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3773 - acc: 0.9333 - val_loss: 0.7516 - val_acc: 0.9000\n",
            "Epoch 33/300\n",
            "\n",
            "Epoch 00033: val_loss improved from 0.75156 to 0.73254, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3800 - acc: 0.9167 - val_loss: 0.7325 - val_acc: 0.9000\n",
            "Epoch 34/300\n",
            "\n",
            "Epoch 00034: val_loss improved from 0.73254 to 0.71754, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4298 - acc: 0.8500 - val_loss: 0.7175 - val_acc: 0.9000\n",
            "Epoch 35/300\n",
            "\n",
            "Epoch 00035: val_loss improved from 0.71754 to 0.70629, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3736 - acc: 0.9167 - val_loss: 0.7063 - val_acc: 0.9000\n",
            "Epoch 36/300\n",
            "\n",
            "Epoch 00036: val_loss improved from 0.70629 to 0.69317, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3580 - acc: 0.9167 - val_loss: 0.6932 - val_acc: 0.9000\n",
            "Epoch 37/300\n",
            "\n",
            "Epoch 00037: val_loss improved from 0.69317 to 0.68304, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3617 - acc: 0.9167 - val_loss: 0.6830 - val_acc: 0.9000\n",
            "Epoch 38/300\n",
            "\n",
            "Epoch 00038: val_loss improved from 0.68304 to 0.67392, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3675 - acc: 0.9333 - val_loss: 0.6739 - val_acc: 0.9000\n",
            "Epoch 39/300\n",
            "\n",
            "Epoch 00039: val_loss improved from 0.67392 to 0.66201, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3542 - acc: 0.9333 - val_loss: 0.6620 - val_acc: 0.9000\n",
            "Epoch 40/300\n",
            "\n",
            "Epoch 00040: val_loss improved from 0.66201 to 0.64981, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3400 - acc: 0.9333 - val_loss: 0.6498 - val_acc: 0.9000\n",
            "Epoch 41/300\n",
            "\n",
            "Epoch 00041: val_loss improved from 0.64981 to 0.63937, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3568 - acc: 0.9500 - val_loss: 0.6394 - val_acc: 0.9000\n",
            "Epoch 42/300\n",
            "\n",
            "Epoch 00042: val_loss improved from 0.63937 to 0.63274, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3411 - acc: 0.9500 - val_loss: 0.6327 - val_acc: 0.9000\n",
            "Epoch 43/300\n",
            "\n",
            "Epoch 00043: val_loss improved from 0.63274 to 0.62584, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3301 - acc: 0.9333 - val_loss: 0.6258 - val_acc: 0.9000\n",
            "Epoch 44/300\n",
            "\n",
            "Epoch 00044: val_loss improved from 0.62584 to 0.62029, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3246 - acc: 0.9333 - val_loss: 0.6203 - val_acc: 0.9000\n",
            "Epoch 45/300\n",
            "\n",
            "Epoch 00045: val_loss improved from 0.62029 to 0.60994, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3382 - acc: 0.9333 - val_loss: 0.6099 - val_acc: 0.8500\n",
            "Epoch 46/300\n",
            "\n",
            "Epoch 00046: val_loss improved from 0.60994 to 0.60341, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3364 - acc: 0.8833 - val_loss: 0.6034 - val_acc: 0.8500\n",
            "Epoch 47/300\n",
            "\n",
            "Epoch 00047: val_loss improved from 0.60341 to 0.60166, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3503 - acc: 0.8833 - val_loss: 0.6017 - val_acc: 0.8500\n",
            "Epoch 48/300\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.60166\n",
            "60/60 - 0s - loss: 0.3409 - acc: 0.9333 - val_loss: 0.6025 - val_acc: 0.7000\n",
            "Epoch 49/300\n",
            "\n",
            "Epoch 00049: val_loss improved from 0.60166 to 0.58604, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3050 - acc: 0.9333 - val_loss: 0.5860 - val_acc: 0.8500\n",
            "Epoch 50/300\n",
            "\n",
            "Epoch 00050: val_loss improved from 0.58604 to 0.57201, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3374 - acc: 0.9500 - val_loss: 0.5720 - val_acc: 0.8500\n",
            "Epoch 51/300\n",
            "\n",
            "Epoch 00051: val_loss improved from 0.57201 to 0.56404, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2986 - acc: 0.9833 - val_loss: 0.5640 - val_acc: 0.8500\n",
            "Epoch 52/300\n",
            "\n",
            "Epoch 00052: val_loss improved from 0.56404 to 0.55796, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2933 - acc: 0.9500 - val_loss: 0.5580 - val_acc: 0.8500\n",
            "Epoch 53/300\n",
            "\n",
            "Epoch 00053: val_loss improved from 0.55796 to 0.55265, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2857 - acc: 0.9500 - val_loss: 0.5527 - val_acc: 0.8500\n",
            "Epoch 54/300\n",
            "\n",
            "Epoch 00054: val_loss improved from 0.55265 to 0.54919, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3189 - acc: 0.9000 - val_loss: 0.5492 - val_acc: 0.8000\n",
            "Epoch 55/300\n",
            "\n",
            "Epoch 00055: val_loss improved from 0.54919 to 0.53872, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2737 - acc: 0.9500 - val_loss: 0.5387 - val_acc: 0.8500\n",
            "Epoch 56/300\n",
            "\n",
            "Epoch 00056: val_loss improved from 0.53872 to 0.53222, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3442 - acc: 0.9000 - val_loss: 0.5322 - val_acc: 0.8500\n",
            "Epoch 57/300\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.53222\n",
            "60/60 - 0s - loss: 0.3202 - acc: 0.9500 - val_loss: 0.5404 - val_acc: 0.8000\n",
            "Epoch 58/300\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.53222\n",
            "60/60 - 0s - loss: 0.3063 - acc: 0.9667 - val_loss: 0.5468 - val_acc: 0.8000\n",
            "Epoch 59/300\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.53222\n",
            "60/60 - 0s - loss: 0.2567 - acc: 0.9333 - val_loss: 0.5477 - val_acc: 0.8000\n",
            "Epoch 60/300\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.53222\n",
            "60/60 - 0s - loss: 0.2931 - acc: 0.9500 - val_loss: 0.5536 - val_acc: 0.7000\n",
            "Epoch 61/300\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.53222\n",
            "60/60 - 0s - loss: 0.2669 - acc: 0.9667 - val_loss: 0.5572 - val_acc: 0.7000\n",
            "Epoch 62/300\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.53222\n",
            "60/60 - 0s - loss: 0.2912 - acc: 0.9333 - val_loss: 0.5543 - val_acc: 0.7500\n",
            "Epoch 63/300\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.53222\n",
            "60/60 - 0s - loss: 0.3236 - acc: 0.9000 - val_loss: 0.5663 - val_acc: 0.6000\n",
            "Epoch 64/300\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.53222\n",
            "60/60 - 0s - loss: 0.2761 - acc: 0.9167 - val_loss: 0.5615 - val_acc: 0.6500\n",
            "Epoch 65/300\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.53222\n",
            "60/60 - 0s - loss: 0.2858 - acc: 0.9333 - val_loss: 0.5602 - val_acc: 0.6500\n",
            "Epoch 66/300\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.53222\n",
            "60/60 - 0s - loss: 0.3090 - acc: 0.9000 - val_loss: 0.5668 - val_acc: 0.6000\n",
            "Epoch 67/300\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.53222\n",
            "60/60 - 0s - loss: 0.2962 - acc: 0.9333 - val_loss: 0.5655 - val_acc: 0.6000\n",
            "Epoch 68/300\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.53222\n",
            "60/60 - 0s - loss: 0.2893 - acc: 0.9333 - val_loss: 0.5613 - val_acc: 0.6000\n",
            "Epoch 69/300\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.53222\n",
            "60/60 - 0s - loss: 0.2549 - acc: 0.9667 - val_loss: 0.5591 - val_acc: 0.6000\n",
            "Epoch 70/300\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.53222\n",
            "60/60 - 0s - loss: 0.2790 - acc: 0.9667 - val_loss: 0.5544 - val_acc: 0.6000\n",
            "Epoch 71/300\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.53222\n",
            "60/60 - 0s - loss: 0.2718 - acc: 0.9333 - val_loss: 0.5681 - val_acc: 0.5500\n",
            "Epoch 72/300\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.53222\n",
            "60/60 - 0s - loss: 0.2880 - acc: 0.9500 - val_loss: 0.5626 - val_acc: 0.6000\n",
            "Epoch 73/300\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.53222\n",
            "60/60 - 0s - loss: 0.2068 - acc: 0.9667 - val_loss: 0.5812 - val_acc: 0.5500\n",
            "Epoch 74/300\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.53222\n",
            "60/60 - 0s - loss: 0.3172 - acc: 0.8833 - val_loss: 0.5899 - val_acc: 0.5500\n",
            "Epoch 75/300\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.53222\n",
            "60/60 - 0s - loss: 0.2530 - acc: 0.9333 - val_loss: 0.6132 - val_acc: 0.5500\n",
            "Epoch 76/300\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.53222\n",
            "60/60 - 0s - loss: 0.2434 - acc: 0.9167 - val_loss: 0.6380 - val_acc: 0.5000\n",
            "Epoch 77/300\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.53222\n",
            "60/60 - 0s - loss: 0.2454 - acc: 0.9167 - val_loss: 0.6412 - val_acc: 0.5000\n",
            "Epoch 78/300\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.53222\n",
            "60/60 - 0s - loss: 0.2855 - acc: 0.9333 - val_loss: 0.6278 - val_acc: 0.5000\n",
            "Epoch 79/300\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.53222\n",
            "60/60 - 0s - loss: 0.2868 - acc: 0.9167 - val_loss: 0.5978 - val_acc: 0.5500\n",
            "Epoch 80/300\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.53222\n",
            "60/60 - 0s - loss: 0.2600 - acc: 0.9500 - val_loss: 0.5894 - val_acc: 0.5500\n",
            "Epoch 81/300\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.53222\n",
            "60/60 - 0s - loss: 0.2608 - acc: 1.0000 - val_loss: 0.5840 - val_acc: 0.5500\n",
            "Epoch 82/300\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.53222\n",
            "60/60 - 0s - loss: 0.2269 - acc: 0.9833 - val_loss: 0.5844 - val_acc: 0.5500\n",
            "Epoch 83/300\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.53222\n",
            "60/60 - 0s - loss: 0.2685 - acc: 0.9500 - val_loss: 0.5767 - val_acc: 0.5500\n",
            "Epoch 84/300\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.53222\n",
            "60/60 - 0s - loss: 0.2624 - acc: 0.9833 - val_loss: 0.5783 - val_acc: 0.5500\n",
            "Epoch 85/300\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.53222\n",
            "60/60 - 0s - loss: 0.2595 - acc: 0.9500 - val_loss: 0.6105 - val_acc: 0.5500\n",
            "Epoch 86/300\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.53222\n",
            "60/60 - 0s - loss: 0.2127 - acc: 0.9833 - val_loss: 0.6005 - val_acc: 0.5500\n",
            "Epoch 87/300\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.53222\n",
            "60/60 - 0s - loss: 0.2485 - acc: 0.9500 - val_loss: 0.5882 - val_acc: 0.5500\n",
            "Epoch 88/300\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.53222\n",
            "60/60 - 0s - loss: 0.2525 - acc: 0.9167 - val_loss: 0.5775 - val_acc: 0.5500\n",
            "Epoch 89/300\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.53222\n",
            "60/60 - 0s - loss: 0.2185 - acc: 1.0000 - val_loss: 0.5653 - val_acc: 0.5500\n",
            "Epoch 90/300\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.53222\n",
            "60/60 - 0s - loss: 0.1925 - acc: 0.9667 - val_loss: 0.5643 - val_acc: 0.5500\n",
            "Epoch 91/300\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.53222\n",
            "60/60 - 0s - loss: 0.2684 - acc: 0.9167 - val_loss: 0.5950 - val_acc: 0.5500\n",
            "Epoch 92/300\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.53222\n",
            "60/60 - 0s - loss: 0.2583 - acc: 0.9333 - val_loss: 0.6559 - val_acc: 0.6000\n",
            "Epoch 93/300\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.53222\n",
            "60/60 - 0s - loss: 0.2876 - acc: 0.9167 - val_loss: 0.6523 - val_acc: 0.6000\n",
            "Epoch 94/300\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.53222\n",
            "60/60 - 0s - loss: 0.2481 - acc: 0.9333 - val_loss: 0.6770 - val_acc: 0.6000\n",
            "Epoch 95/300\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.53222\n",
            "60/60 - 0s - loss: 0.2506 - acc: 0.9833 - val_loss: 0.6678 - val_acc: 0.6000\n",
            "Epoch 96/300\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.53222\n",
            "60/60 - 0s - loss: 0.2598 - acc: 0.9167 - val_loss: 0.6135 - val_acc: 0.6000\n",
            "Epoch 97/300\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.53222\n",
            "60/60 - 0s - loss: 0.2916 - acc: 0.9167 - val_loss: 0.5914 - val_acc: 0.5500\n",
            "Epoch 98/300\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.53222\n",
            "60/60 - 0s - loss: 0.2049 - acc: 0.9833 - val_loss: 0.6248 - val_acc: 0.5500\n",
            "Epoch 99/300\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.53222\n",
            "60/60 - 0s - loss: 0.2351 - acc: 0.9500 - val_loss: 0.6630 - val_acc: 0.5500\n",
            "Epoch 100/300\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.53222\n",
            "60/60 - 0s - loss: 0.2276 - acc: 0.9667 - val_loss: 0.6926 - val_acc: 0.5500\n",
            "Epoch 101/300\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.53222\n",
            "60/60 - 0s - loss: 0.2134 - acc: 0.9667 - val_loss: 0.7226 - val_acc: 0.5500\n",
            "Epoch 102/300\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.53222\n",
            "60/60 - 0s - loss: 0.2257 - acc: 0.9833 - val_loss: 0.6900 - val_acc: 0.5500\n",
            "Epoch 103/300\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.53222\n",
            "60/60 - 0s - loss: 0.2302 - acc: 0.9500 - val_loss: 0.6667 - val_acc: 0.5500\n",
            "Epoch 104/300\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.53222\n",
            "60/60 - 0s - loss: 0.2169 - acc: 0.9667 - val_loss: 0.6690 - val_acc: 0.5500\n",
            "Epoch 105/300\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 0.53222\n",
            "60/60 - 0s - loss: 0.2171 - acc: 0.9833 - val_loss: 0.6557 - val_acc: 0.5500\n",
            "Epoch 106/300\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 0.53222\n",
            "60/60 - 0s - loss: 0.2096 - acc: 0.9833 - val_loss: 0.6274 - val_acc: 0.5500\n",
            "Epoch 107/300\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.53222\n",
            "60/60 - 0s - loss: 0.2483 - acc: 0.9333 - val_loss: 0.6176 - val_acc: 0.5500\n",
            "Epoch 108/300\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 0.53222\n",
            "60/60 - 0s - loss: 0.2209 - acc: 0.9500 - val_loss: 0.6079 - val_acc: 0.5500\n",
            "Epoch 109/300\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.53222\n",
            "60/60 - 0s - loss: 0.2073 - acc: 1.0000 - val_loss: 0.6063 - val_acc: 0.5500\n",
            "Epoch 110/300\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.53222\n",
            "60/60 - 0s - loss: 0.2380 - acc: 0.9667 - val_loss: 0.6075 - val_acc: 0.5500\n",
            "Epoch 111/300\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 0.53222\n",
            "60/60 - 0s - loss: 0.1967 - acc: 0.9833 - val_loss: 0.6020 - val_acc: 0.5500\n",
            "Epoch 112/300\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 0.53222\n",
            "60/60 - 0s - loss: 0.2160 - acc: 0.9667 - val_loss: 0.6008 - val_acc: 0.5500\n",
            "Epoch 113/300\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.53222\n",
            "60/60 - 0s - loss: 0.2064 - acc: 0.9667 - val_loss: 0.5978 - val_acc: 0.5500\n",
            "Epoch 114/300\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.53222\n",
            "60/60 - 0s - loss: 0.1604 - acc: 0.9667 - val_loss: 0.6294 - val_acc: 0.5500\n",
            "Epoch 115/300\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.53222\n",
            "60/60 - 0s - loss: 0.1822 - acc: 0.9833 - val_loss: 0.6673 - val_acc: 0.5500\n",
            "Epoch 116/300\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.53222\n",
            "60/60 - 0s - loss: 0.1832 - acc: 0.9833 - val_loss: 0.6871 - val_acc: 0.5500\n",
            "Epoch 117/300\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.53222\n",
            "60/60 - 0s - loss: 0.2237 - acc: 0.9500 - val_loss: 0.7025 - val_acc: 0.5000\n",
            "Epoch 118/300\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.53222\n",
            "60/60 - 0s - loss: 0.2072 - acc: 0.9667 - val_loss: 0.6484 - val_acc: 0.5500\n",
            "Epoch 119/300\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 0.53222\n",
            "60/60 - 0s - loss: 0.1933 - acc: 0.9833 - val_loss: 0.6095 - val_acc: 0.5500\n",
            "Epoch 120/300\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.53222\n",
            "60/60 - 0s - loss: 0.2105 - acc: 1.0000 - val_loss: 0.6058 - val_acc: 0.5500\n",
            "Epoch 121/300\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.53222\n",
            "60/60 - 0s - loss: 0.2024 - acc: 0.9500 - val_loss: 0.6203 - val_acc: 0.5500\n",
            "Epoch 122/300\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.53222\n",
            "60/60 - 0s - loss: 0.1928 - acc: 1.0000 - val_loss: 0.6224 - val_acc: 0.5500\n",
            "Epoch 123/300\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 0.53222\n",
            "60/60 - 0s - loss: 0.1709 - acc: 0.9833 - val_loss: 0.6333 - val_acc: 0.5500\n",
            "Epoch 124/300\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 0.53222\n",
            "60/60 - 0s - loss: 0.1733 - acc: 1.0000 - val_loss: 0.6494 - val_acc: 0.5500\n",
            "Epoch 125/300\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 0.53222\n",
            "60/60 - 0s - loss: 0.1775 - acc: 0.9667 - val_loss: 0.6637 - val_acc: 0.5500\n",
            "Epoch 126/300\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 0.53222\n",
            "60/60 - 0s - loss: 0.2008 - acc: 0.9833 - val_loss: 0.6545 - val_acc: 0.5500\n",
            "Epoch 127/300\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 0.53222\n",
            "60/60 - 0s - loss: 0.1897 - acc: 0.9833 - val_loss: 0.6708 - val_acc: 0.5500\n",
            "Epoch 128/300\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 0.53222\n",
            "60/60 - 0s - loss: 0.1837 - acc: 0.9667 - val_loss: 0.6847 - val_acc: 0.5500\n",
            "Epoch 129/300\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.53222\n",
            "60/60 - 0s - loss: 0.2113 - acc: 0.9333 - val_loss: 0.6676 - val_acc: 0.5500\n",
            "Epoch 130/300\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 0.53222\n",
            "60/60 - 0s - loss: 0.1763 - acc: 0.9667 - val_loss: 0.6740 - val_acc: 0.5500\n",
            "Epoch 131/300\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 0.53222\n",
            "60/60 - 0s - loss: 0.1650 - acc: 0.9833 - val_loss: 0.6814 - val_acc: 0.5500\n",
            "Epoch 132/300\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 0.53222\n",
            "60/60 - 0s - loss: 0.1819 - acc: 1.0000 - val_loss: 0.7368 - val_acc: 0.5500\n",
            "Epoch 133/300\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.53222\n",
            "60/60 - 0s - loss: 0.2028 - acc: 0.9500 - val_loss: 0.7940 - val_acc: 0.5000\n",
            "Epoch 134/300\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.53222\n",
            "60/60 - 0s - loss: 0.1474 - acc: 1.0000 - val_loss: 0.7773 - val_acc: 0.5000\n",
            "Epoch 135/300\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 0.53222\n",
            "60/60 - 0s - loss: 0.1903 - acc: 0.9833 - val_loss: 0.7866 - val_acc: 0.5000\n",
            "Epoch 136/300\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.53222\n",
            "60/60 - 0s - loss: 0.1943 - acc: 0.9667 - val_loss: 0.7598 - val_acc: 0.5000\n",
            "Epoch 137/300\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 0.53222\n",
            "60/60 - 0s - loss: 0.1961 - acc: 0.9500 - val_loss: 0.6848 - val_acc: 0.5500\n",
            "Epoch 138/300\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.53222\n",
            "60/60 - 0s - loss: 0.1734 - acc: 0.9667 - val_loss: 0.6344 - val_acc: 0.5500\n",
            "Epoch 139/300\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.53222\n",
            "60/60 - 0s - loss: 0.1521 - acc: 1.0000 - val_loss: 0.5806 - val_acc: 0.5500\n",
            "Epoch 140/300\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 0.53222\n",
            "60/60 - 0s - loss: 0.1834 - acc: 0.9667 - val_loss: 0.5486 - val_acc: 0.6500\n",
            "Epoch 141/300\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.53222\n",
            "60/60 - 0s - loss: 0.1939 - acc: 0.9667 - val_loss: 0.5689 - val_acc: 0.5500\n",
            "Epoch 142/300\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.53222\n",
            "60/60 - 0s - loss: 0.1599 - acc: 1.0000 - val_loss: 0.5882 - val_acc: 0.5500\n",
            "Epoch 143/300\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 0.53222\n",
            "60/60 - 0s - loss: 0.1449 - acc: 1.0000 - val_loss: 0.6003 - val_acc: 0.5500\n",
            "Epoch 144/300\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 0.53222\n",
            "60/60 - 0s - loss: 0.1751 - acc: 0.9667 - val_loss: 0.6151 - val_acc: 0.5500\n",
            "Epoch 145/300\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.53222\n",
            "60/60 - 0s - loss: 0.1528 - acc: 1.0000 - val_loss: 0.6019 - val_acc: 0.5500\n",
            "Epoch 146/300\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.53222\n",
            "60/60 - 0s - loss: 0.2092 - acc: 0.9667 - val_loss: 0.5902 - val_acc: 0.5500\n",
            "Epoch 147/300\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.53222\n",
            "60/60 - 0s - loss: 0.1397 - acc: 1.0000 - val_loss: 0.6121 - val_acc: 0.5500\n",
            "Epoch 148/300\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 0.53222\n",
            "60/60 - 0s - loss: 0.1371 - acc: 1.0000 - val_loss: 0.6681 - val_acc: 0.5500\n",
            "Epoch 149/300\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.53222\n",
            "60/60 - 0s - loss: 0.1227 - acc: 0.9833 - val_loss: 0.7046 - val_acc: 0.5500\n",
            "Epoch 150/300\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.53222\n",
            "60/60 - 0s - loss: 0.1340 - acc: 1.0000 - val_loss: 0.7447 - val_acc: 0.5500\n",
            "Epoch 151/300\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.53222\n",
            "60/60 - 0s - loss: 0.1637 - acc: 0.9833 - val_loss: 0.7714 - val_acc: 0.5500\n",
            "Epoch 152/300\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.53222\n",
            "60/60 - 0s - loss: 0.1436 - acc: 1.0000 - val_loss: 0.7519 - val_acc: 0.5500\n",
            "Epoch 153/300\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 0.53222\n",
            "60/60 - 0s - loss: 0.1737 - acc: 0.9833 - val_loss: 0.7196 - val_acc: 0.5500\n",
            "Epoch 154/300\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.53222\n",
            "60/60 - 0s - loss: 0.1620 - acc: 0.9833 - val_loss: 0.6795 - val_acc: 0.5500\n",
            "Epoch 155/300\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.53222\n",
            "60/60 - 0s - loss: 0.1744 - acc: 0.9500 - val_loss: 0.6277 - val_acc: 0.5500\n",
            "Epoch 156/300\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.53222\n",
            "60/60 - 0s - loss: 0.1802 - acc: 0.9667 - val_loss: 0.5684 - val_acc: 0.6500\n",
            "Epoch 157/300\n",
            "\n",
            "Epoch 00157: val_loss improved from 0.53222 to 0.53032, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1627 - acc: 0.9833 - val_loss: 0.5303 - val_acc: 0.7000\n",
            "Epoch 158/300\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.53032\n",
            "60/60 - 0s - loss: 0.1552 - acc: 0.9833 - val_loss: 0.5417 - val_acc: 0.7000\n",
            "Epoch 159/300\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.53032\n",
            "60/60 - 0s - loss: 0.1402 - acc: 1.0000 - val_loss: 0.5658 - val_acc: 0.7000\n",
            "Epoch 160/300\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.53032\n",
            "60/60 - 0s - loss: 0.1638 - acc: 0.9667 - val_loss: 0.6280 - val_acc: 0.5500\n",
            "Epoch 161/300\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.53032\n",
            "60/60 - 0s - loss: 0.1336 - acc: 0.9833 - val_loss: 0.6679 - val_acc: 0.5500\n",
            "Epoch 162/300\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.53032\n",
            "60/60 - 0s - loss: 0.1199 - acc: 1.0000 - val_loss: 0.6746 - val_acc: 0.6000\n",
            "Epoch 163/300\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.53032\n",
            "60/60 - 0s - loss: 0.1219 - acc: 1.0000 - val_loss: 0.6663 - val_acc: 0.6000\n",
            "Epoch 164/300\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.53032\n",
            "60/60 - 0s - loss: 0.1442 - acc: 1.0000 - val_loss: 0.6649 - val_acc: 0.6000\n",
            "Epoch 165/300\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.53032\n",
            "60/60 - 0s - loss: 0.1382 - acc: 1.0000 - val_loss: 0.6017 - val_acc: 0.6000\n",
            "Epoch 166/300\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 0.53032\n",
            "60/60 - 0s - loss: 0.1682 - acc: 0.9500 - val_loss: 0.5783 - val_acc: 0.7000\n",
            "Epoch 167/300\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.53032\n",
            "60/60 - 0s - loss: 0.1535 - acc: 1.0000 - val_loss: 0.5645 - val_acc: 0.7500\n",
            "Epoch 168/300\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.53032\n",
            "60/60 - 0s - loss: 0.1430 - acc: 0.9500 - val_loss: 0.5651 - val_acc: 0.7500\n",
            "Epoch 169/300\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 0.53032\n",
            "60/60 - 0s - loss: 0.1848 - acc: 0.9667 - val_loss: 0.5590 - val_acc: 0.7500\n",
            "Epoch 170/300\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.53032\n",
            "60/60 - 0s - loss: 0.1248 - acc: 0.9833 - val_loss: 0.5539 - val_acc: 0.7500\n",
            "Epoch 171/300\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.53032\n",
            "60/60 - 0s - loss: 0.1524 - acc: 0.9833 - val_loss: 0.5492 - val_acc: 0.7500\n",
            "Epoch 172/300\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.53032\n",
            "60/60 - 0s - loss: 0.1525 - acc: 1.0000 - val_loss: 0.5381 - val_acc: 0.7500\n",
            "Epoch 173/300\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.53032\n",
            "60/60 - 0s - loss: 0.1616 - acc: 0.9833 - val_loss: 0.5491 - val_acc: 0.7500\n",
            "Epoch 174/300\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.53032\n",
            "60/60 - 0s - loss: 0.1291 - acc: 1.0000 - val_loss: 0.5799 - val_acc: 0.7000\n",
            "Epoch 175/300\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.53032\n",
            "60/60 - 0s - loss: 0.1480 - acc: 1.0000 - val_loss: 0.5802 - val_acc: 0.7000\n",
            "Epoch 176/300\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 0.53032\n",
            "60/60 - 0s - loss: 0.1385 - acc: 0.9667 - val_loss: 0.6044 - val_acc: 0.7000\n",
            "Epoch 177/300\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.53032\n",
            "60/60 - 0s - loss: 0.1086 - acc: 1.0000 - val_loss: 0.5740 - val_acc: 0.7000\n",
            "Epoch 178/300\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.53032\n",
            "60/60 - 0s - loss: 0.1354 - acc: 1.0000 - val_loss: 0.5688 - val_acc: 0.7000\n",
            "Epoch 179/300\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.53032\n",
            "60/60 - 0s - loss: 0.1102 - acc: 1.0000 - val_loss: 0.5885 - val_acc: 0.7000\n",
            "Epoch 180/300\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.53032\n",
            "60/60 - 0s - loss: 0.1435 - acc: 0.9833 - val_loss: 0.5943 - val_acc: 0.7000\n",
            "Epoch 181/300\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.53032\n",
            "60/60 - 0s - loss: 0.1310 - acc: 1.0000 - val_loss: 0.6026 - val_acc: 0.7000\n",
            "Epoch 182/300\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.53032\n",
            "60/60 - 0s - loss: 0.1118 - acc: 0.9833 - val_loss: 0.6047 - val_acc: 0.6500\n",
            "Epoch 183/300\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.53032\n",
            "60/60 - 0s - loss: 0.1514 - acc: 1.0000 - val_loss: 0.5867 - val_acc: 0.7000\n",
            "Epoch 184/300\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 0.53032\n",
            "60/60 - 0s - loss: 0.1268 - acc: 1.0000 - val_loss: 0.6095 - val_acc: 0.6000\n",
            "Epoch 185/300\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.53032\n",
            "60/60 - 0s - loss: 0.1311 - acc: 0.9833 - val_loss: 0.5740 - val_acc: 0.7000\n",
            "Epoch 186/300\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.53032\n",
            "60/60 - 0s - loss: 0.1394 - acc: 0.9833 - val_loss: 0.5881 - val_acc: 0.6500\n",
            "Epoch 187/300\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 0.53032\n",
            "60/60 - 0s - loss: 0.1288 - acc: 0.9833 - val_loss: 0.6188 - val_acc: 0.6500\n",
            "Epoch 188/300\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.53032\n",
            "60/60 - 0s - loss: 0.1172 - acc: 1.0000 - val_loss: 0.6459 - val_acc: 0.6000\n",
            "Epoch 189/300\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.53032\n",
            "60/60 - 0s - loss: 0.1217 - acc: 0.9833 - val_loss: 0.6278 - val_acc: 0.6500\n",
            "Epoch 190/300\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.53032\n",
            "60/60 - 0s - loss: 0.1353 - acc: 1.0000 - val_loss: 0.6326 - val_acc: 0.6500\n",
            "Epoch 191/300\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.53032\n",
            "60/60 - 0s - loss: 0.1200 - acc: 1.0000 - val_loss: 0.6191 - val_acc: 0.7000\n",
            "Epoch 192/300\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 0.53032\n",
            "60/60 - 0s - loss: 0.1240 - acc: 1.0000 - val_loss: 0.6238 - val_acc: 0.7000\n",
            "Epoch 193/300\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.53032\n",
            "60/60 - 0s - loss: 0.1318 - acc: 1.0000 - val_loss: 0.6359 - val_acc: 0.7000\n",
            "Epoch 194/300\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.53032\n",
            "60/60 - 0s - loss: 0.1344 - acc: 0.9833 - val_loss: 0.6521 - val_acc: 0.6500\n",
            "Epoch 195/300\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.53032\n",
            "60/60 - 0s - loss: 0.1013 - acc: 1.0000 - val_loss: 0.6787 - val_acc: 0.6500\n",
            "Epoch 196/300\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.53032\n",
            "60/60 - 0s - loss: 0.1062 - acc: 0.9833 - val_loss: 0.6603 - val_acc: 0.6000\n",
            "Epoch 197/300\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.53032\n",
            "60/60 - 0s - loss: 0.1039 - acc: 1.0000 - val_loss: 0.6094 - val_acc: 0.6500\n",
            "Epoch 198/300\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.53032\n",
            "60/60 - 0s - loss: 0.1145 - acc: 1.0000 - val_loss: 0.5667 - val_acc: 0.6500\n",
            "Epoch 199/300\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.53032\n",
            "60/60 - 0s - loss: 0.1278 - acc: 0.9833 - val_loss: 0.5917 - val_acc: 0.6500\n",
            "Epoch 200/300\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.53032\n",
            "60/60 - 0s - loss: 0.1416 - acc: 0.9833 - val_loss: 0.6510 - val_acc: 0.6000\n",
            "Epoch 201/300\n",
            "\n",
            "Epoch 00201: val_loss did not improve from 0.53032\n",
            "60/60 - 0s - loss: 0.1005 - acc: 1.0000 - val_loss: 0.7297 - val_acc: 0.6000\n",
            "Epoch 202/300\n",
            "\n",
            "Epoch 00202: val_loss did not improve from 0.53032\n",
            "60/60 - 0s - loss: 0.0932 - acc: 1.0000 - val_loss: 0.7814 - val_acc: 0.6000\n",
            "Epoch 203/300\n",
            "\n",
            "Epoch 00203: val_loss did not improve from 0.53032\n",
            "60/60 - 0s - loss: 0.1051 - acc: 1.0000 - val_loss: 0.7945 - val_acc: 0.5500\n",
            "Epoch 204/300\n",
            "\n",
            "Epoch 00204: val_loss did not improve from 0.53032\n",
            "60/60 - 0s - loss: 0.1468 - acc: 0.9833 - val_loss: 0.7207 - val_acc: 0.6000\n",
            "Epoch 205/300\n",
            "\n",
            "Epoch 00205: val_loss did not improve from 0.53032\n",
            "60/60 - 0s - loss: 0.0984 - acc: 1.0000 - val_loss: 0.5602 - val_acc: 0.7500\n",
            "Epoch 206/300\n",
            "\n",
            "Epoch 00206: val_loss improved from 0.53032 to 0.48931, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1387 - acc: 0.9833 - val_loss: 0.4893 - val_acc: 0.8000\n",
            "Epoch 207/300\n",
            "\n",
            "Epoch 00207: val_loss improved from 0.48931 to 0.47132, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1785 - acc: 0.9500 - val_loss: 0.4713 - val_acc: 0.7500\n",
            "Epoch 208/300\n",
            "\n",
            "Epoch 00208: val_loss did not improve from 0.47132\n",
            "60/60 - 0s - loss: 0.1182 - acc: 1.0000 - val_loss: 0.4723 - val_acc: 0.7500\n",
            "Epoch 209/300\n",
            "\n",
            "Epoch 00209: val_loss did not improve from 0.47132\n",
            "60/60 - 0s - loss: 0.1105 - acc: 1.0000 - val_loss: 0.4857 - val_acc: 0.8000\n",
            "Epoch 210/300\n",
            "\n",
            "Epoch 00210: val_loss did not improve from 0.47132\n",
            "60/60 - 0s - loss: 0.1228 - acc: 0.9833 - val_loss: 0.5150 - val_acc: 0.8000\n",
            "Epoch 211/300\n",
            "\n",
            "Epoch 00211: val_loss did not improve from 0.47132\n",
            "60/60 - 0s - loss: 0.1178 - acc: 1.0000 - val_loss: 0.5770 - val_acc: 0.7500\n",
            "Epoch 212/300\n",
            "\n",
            "Epoch 00212: val_loss did not improve from 0.47132\n",
            "60/60 - 0s - loss: 0.1425 - acc: 0.9333 - val_loss: 0.6356 - val_acc: 0.6000\n",
            "Epoch 213/300\n",
            "\n",
            "Epoch 00213: val_loss did not improve from 0.47132\n",
            "60/60 - 0s - loss: 0.1219 - acc: 0.9833 - val_loss: 0.6489 - val_acc: 0.6500\n",
            "Epoch 214/300\n",
            "\n",
            "Epoch 00214: val_loss did not improve from 0.47132\n",
            "60/60 - 0s - loss: 0.1117 - acc: 0.9833 - val_loss: 0.6610 - val_acc: 0.6000\n",
            "Epoch 215/300\n",
            "\n",
            "Epoch 00215: val_loss did not improve from 0.47132\n",
            "60/60 - 0s - loss: 0.1529 - acc: 0.9833 - val_loss: 0.7022 - val_acc: 0.6000\n",
            "Epoch 216/300\n",
            "\n",
            "Epoch 00216: val_loss did not improve from 0.47132\n",
            "60/60 - 0s - loss: 0.1178 - acc: 0.9833 - val_loss: 0.7305 - val_acc: 0.5500\n",
            "Epoch 217/300\n",
            "\n",
            "Epoch 00217: val_loss did not improve from 0.47132\n",
            "60/60 - 0s - loss: 0.1480 - acc: 0.9667 - val_loss: 0.7797 - val_acc: 0.5500\n",
            "Epoch 218/300\n",
            "\n",
            "Epoch 00218: val_loss did not improve from 0.47132\n",
            "60/60 - 0s - loss: 0.1351 - acc: 1.0000 - val_loss: 0.7706 - val_acc: 0.5500\n",
            "Epoch 219/300\n",
            "\n",
            "Epoch 00219: val_loss did not improve from 0.47132\n",
            "60/60 - 0s - loss: 0.0929 - acc: 1.0000 - val_loss: 0.6650 - val_acc: 0.6000\n",
            "Epoch 220/300\n",
            "\n",
            "Epoch 00220: val_loss did not improve from 0.47132\n",
            "60/60 - 0s - loss: 0.1460 - acc: 0.9833 - val_loss: 0.5874 - val_acc: 0.7000\n",
            "Epoch 221/300\n",
            "\n",
            "Epoch 00221: val_loss did not improve from 0.47132\n",
            "60/60 - 0s - loss: 0.0940 - acc: 1.0000 - val_loss: 0.5787 - val_acc: 0.7000\n",
            "Epoch 222/300\n",
            "\n",
            "Epoch 00222: val_loss did not improve from 0.47132\n",
            "60/60 - 0s - loss: 0.0864 - acc: 1.0000 - val_loss: 0.6031 - val_acc: 0.7000\n",
            "Epoch 223/300\n",
            "\n",
            "Epoch 00223: val_loss did not improve from 0.47132\n",
            "60/60 - 0s - loss: 0.0929 - acc: 1.0000 - val_loss: 0.6379 - val_acc: 0.6000\n",
            "Epoch 224/300\n",
            "\n",
            "Epoch 00224: val_loss did not improve from 0.47132\n",
            "60/60 - 0s - loss: 0.1054 - acc: 1.0000 - val_loss: 0.6667 - val_acc: 0.6000\n",
            "Epoch 225/300\n",
            "\n",
            "Epoch 00225: val_loss did not improve from 0.47132\n",
            "60/60 - 0s - loss: 0.0977 - acc: 0.9833 - val_loss: 0.7180 - val_acc: 0.6000\n",
            "Epoch 226/300\n",
            "\n",
            "Epoch 00226: val_loss did not improve from 0.47132\n",
            "60/60 - 0s - loss: 0.1302 - acc: 1.0000 - val_loss: 0.7217 - val_acc: 0.6000\n",
            "Epoch 227/300\n",
            "\n",
            "Epoch 00227: val_loss did not improve from 0.47132\n",
            "60/60 - 0s - loss: 0.1136 - acc: 0.9833 - val_loss: 0.7165 - val_acc: 0.6000\n",
            "Epoch 228/300\n",
            "\n",
            "Epoch 00228: val_loss did not improve from 0.47132\n",
            "60/60 - 0s - loss: 0.0906 - acc: 0.9833 - val_loss: 0.7056 - val_acc: 0.6000\n",
            "Epoch 229/300\n",
            "\n",
            "Epoch 00229: val_loss did not improve from 0.47132\n",
            "60/60 - 0s - loss: 0.1043 - acc: 1.0000 - val_loss: 0.6311 - val_acc: 0.6500\n",
            "Epoch 230/300\n",
            "\n",
            "Epoch 00230: val_loss did not improve from 0.47132\n",
            "60/60 - 0s - loss: 0.1143 - acc: 1.0000 - val_loss: 0.5678 - val_acc: 0.7000\n",
            "Epoch 231/300\n",
            "\n",
            "Epoch 00231: val_loss did not improve from 0.47132\n",
            "60/60 - 0s - loss: 0.0955 - acc: 1.0000 - val_loss: 0.5200 - val_acc: 0.8000\n",
            "Epoch 232/300\n",
            "\n",
            "Epoch 00232: val_loss did not improve from 0.47132\n",
            "60/60 - 0s - loss: 0.1179 - acc: 1.0000 - val_loss: 0.4946 - val_acc: 0.8000\n",
            "Epoch 233/300\n",
            "\n",
            "Epoch 00233: val_loss did not improve from 0.47132\n",
            "60/60 - 0s - loss: 0.1215 - acc: 0.9833 - val_loss: 0.5060 - val_acc: 0.8000\n",
            "Epoch 234/300\n",
            "\n",
            "Epoch 00234: val_loss did not improve from 0.47132\n",
            "60/60 - 0s - loss: 0.1023 - acc: 0.9833 - val_loss: 0.5438 - val_acc: 0.8000\n",
            "Epoch 235/300\n",
            "\n",
            "Epoch 00235: val_loss did not improve from 0.47132\n",
            "60/60 - 0s - loss: 0.0979 - acc: 0.9833 - val_loss: 0.5565 - val_acc: 0.8000\n",
            "Epoch 236/300\n",
            "\n",
            "Epoch 00236: val_loss did not improve from 0.47132\n",
            "60/60 - 0s - loss: 0.1038 - acc: 0.9833 - val_loss: 0.5467 - val_acc: 0.8000\n",
            "Epoch 237/300\n",
            "\n",
            "Epoch 00237: val_loss did not improve from 0.47132\n",
            "60/60 - 0s - loss: 0.1028 - acc: 1.0000 - val_loss: 0.5443 - val_acc: 0.8000\n",
            "Epoch 238/300\n",
            "\n",
            "Epoch 00238: val_loss did not improve from 0.47132\n",
            "60/60 - 0s - loss: 0.1181 - acc: 0.9833 - val_loss: 0.5715 - val_acc: 0.7500\n",
            "Epoch 239/300\n",
            "\n",
            "Epoch 00239: val_loss did not improve from 0.47132\n",
            "60/60 - 0s - loss: 0.1132 - acc: 0.9833 - val_loss: 0.5832 - val_acc: 0.7000\n",
            "Epoch 240/300\n",
            "\n",
            "Epoch 00240: val_loss did not improve from 0.47132\n",
            "60/60 - 0s - loss: 0.1051 - acc: 1.0000 - val_loss: 0.4981 - val_acc: 0.8000\n",
            "Epoch 241/300\n",
            "\n",
            "Epoch 00241: val_loss improved from 0.47132 to 0.46231, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1382 - acc: 0.9833 - val_loss: 0.4623 - val_acc: 0.8000\n",
            "Epoch 242/300\n",
            "\n",
            "Epoch 00242: val_loss did not improve from 0.46231\n",
            "60/60 - 0s - loss: 0.1299 - acc: 0.9667 - val_loss: 0.4667 - val_acc: 0.8000\n",
            "Epoch 243/300\n",
            "\n",
            "Epoch 00243: val_loss did not improve from 0.46231\n",
            "60/60 - 0s - loss: 0.0917 - acc: 1.0000 - val_loss: 0.4788 - val_acc: 0.8000\n",
            "Epoch 244/300\n",
            "\n",
            "Epoch 00244: val_loss did not improve from 0.46231\n",
            "60/60 - 0s - loss: 0.0984 - acc: 0.9833 - val_loss: 0.5061 - val_acc: 0.8000\n",
            "Epoch 245/300\n",
            "\n",
            "Epoch 00245: val_loss did not improve from 0.46231\n",
            "60/60 - 0s - loss: 0.0851 - acc: 1.0000 - val_loss: 0.5285 - val_acc: 0.8000\n",
            "Epoch 246/300\n",
            "\n",
            "Epoch 00246: val_loss did not improve from 0.46231\n",
            "60/60 - 0s - loss: 0.1202 - acc: 0.9667 - val_loss: 0.5587 - val_acc: 0.8000\n",
            "Epoch 247/300\n",
            "\n",
            "Epoch 00247: val_loss did not improve from 0.46231\n",
            "60/60 - 0s - loss: 0.1120 - acc: 0.9833 - val_loss: 0.6248 - val_acc: 0.7000\n",
            "Epoch 248/300\n",
            "\n",
            "Epoch 00248: val_loss did not improve from 0.46231\n",
            "60/60 - 0s - loss: 0.1057 - acc: 0.9833 - val_loss: 0.6247 - val_acc: 0.7000\n",
            "Epoch 249/300\n",
            "\n",
            "Epoch 00249: val_loss did not improve from 0.46231\n",
            "60/60 - 0s - loss: 0.1071 - acc: 1.0000 - val_loss: 0.6000 - val_acc: 0.7500\n",
            "Epoch 250/300\n",
            "\n",
            "Epoch 00250: val_loss did not improve from 0.46231\n",
            "60/60 - 0s - loss: 0.0781 - acc: 1.0000 - val_loss: 0.6117 - val_acc: 0.7500\n",
            "Epoch 251/300\n",
            "\n",
            "Epoch 00251: val_loss did not improve from 0.46231\n",
            "60/60 - 0s - loss: 0.1241 - acc: 0.9833 - val_loss: 0.6121 - val_acc: 0.7000\n",
            "Epoch 252/300\n",
            "\n",
            "Epoch 00252: val_loss did not improve from 0.46231\n",
            "60/60 - 0s - loss: 0.0841 - acc: 1.0000 - val_loss: 0.5863 - val_acc: 0.7500\n",
            "Epoch 253/300\n",
            "\n",
            "Epoch 00253: val_loss did not improve from 0.46231\n",
            "60/60 - 0s - loss: 0.1046 - acc: 1.0000 - val_loss: 0.5616 - val_acc: 0.8000\n",
            "Epoch 254/300\n",
            "\n",
            "Epoch 00254: val_loss did not improve from 0.46231\n",
            "60/60 - 0s - loss: 0.1063 - acc: 1.0000 - val_loss: 0.5706 - val_acc: 0.7500\n",
            "Epoch 255/300\n",
            "\n",
            "Epoch 00255: val_loss did not improve from 0.46231\n",
            "60/60 - 0s - loss: 0.1031 - acc: 1.0000 - val_loss: 0.6023 - val_acc: 0.6500\n",
            "Epoch 256/300\n",
            "\n",
            "Epoch 00256: val_loss did not improve from 0.46231\n",
            "60/60 - 0s - loss: 0.0693 - acc: 1.0000 - val_loss: 0.5970 - val_acc: 0.6500\n",
            "Epoch 257/300\n",
            "\n",
            "Epoch 00257: val_loss did not improve from 0.46231\n",
            "60/60 - 0s - loss: 0.0940 - acc: 1.0000 - val_loss: 0.5496 - val_acc: 0.7500\n",
            "Epoch 258/300\n",
            "\n",
            "Epoch 00258: val_loss did not improve from 0.46231\n",
            "60/60 - 0s - loss: 0.0621 - acc: 1.0000 - val_loss: 0.5139 - val_acc: 0.8000\n",
            "Epoch 259/300\n",
            "\n",
            "Epoch 00259: val_loss did not improve from 0.46231\n",
            "60/60 - 0s - loss: 0.0987 - acc: 0.9833 - val_loss: 0.4947 - val_acc: 0.8000\n",
            "Epoch 260/300\n",
            "\n",
            "Epoch 00260: val_loss did not improve from 0.46231\n",
            "60/60 - 0s - loss: 0.0819 - acc: 1.0000 - val_loss: 0.5005 - val_acc: 0.8000\n",
            "Epoch 261/300\n",
            "\n",
            "Epoch 00261: val_loss did not improve from 0.46231\n",
            "60/60 - 0s - loss: 0.0982 - acc: 1.0000 - val_loss: 0.5427 - val_acc: 0.8000\n",
            "Epoch 262/300\n",
            "\n",
            "Epoch 00262: val_loss did not improve from 0.46231\n",
            "60/60 - 0s - loss: 0.0650 - acc: 1.0000 - val_loss: 0.5913 - val_acc: 0.6500\n",
            "Epoch 263/300\n",
            "\n",
            "Epoch 00263: val_loss did not improve from 0.46231\n",
            "60/60 - 0s - loss: 0.0832 - acc: 1.0000 - val_loss: 0.6295 - val_acc: 0.6500\n",
            "Epoch 264/300\n",
            "\n",
            "Epoch 00264: val_loss did not improve from 0.46231\n",
            "60/60 - 0s - loss: 0.0579 - acc: 1.0000 - val_loss: 0.6691 - val_acc: 0.6500\n",
            "Epoch 265/300\n",
            "\n",
            "Epoch 00265: val_loss did not improve from 0.46231\n",
            "60/60 - 0s - loss: 0.0784 - acc: 1.0000 - val_loss: 0.7161 - val_acc: 0.5500\n",
            "Epoch 266/300\n",
            "\n",
            "Epoch 00266: val_loss did not improve from 0.46231\n",
            "60/60 - 0s - loss: 0.0961 - acc: 1.0000 - val_loss: 0.6883 - val_acc: 0.6000\n",
            "Epoch 267/300\n",
            "\n",
            "Epoch 00267: val_loss did not improve from 0.46231\n",
            "60/60 - 0s - loss: 0.0785 - acc: 1.0000 - val_loss: 0.6672 - val_acc: 0.6000\n",
            "Epoch 268/300\n",
            "\n",
            "Epoch 00268: val_loss did not improve from 0.46231\n",
            "60/60 - 0s - loss: 0.0878 - acc: 1.0000 - val_loss: 0.6799 - val_acc: 0.6000\n",
            "Epoch 269/300\n",
            "\n",
            "Epoch 00269: val_loss did not improve from 0.46231\n",
            "60/60 - 0s - loss: 0.1011 - acc: 0.9833 - val_loss: 0.6399 - val_acc: 0.6500\n",
            "Epoch 270/300\n",
            "\n",
            "Epoch 00270: val_loss did not improve from 0.46231\n",
            "60/60 - 0s - loss: 0.0772 - acc: 1.0000 - val_loss: 0.6095 - val_acc: 0.7000\n",
            "Epoch 271/300\n",
            "\n",
            "Epoch 00271: val_loss did not improve from 0.46231\n",
            "60/60 - 0s - loss: 0.0690 - acc: 1.0000 - val_loss: 0.5661 - val_acc: 0.7000\n",
            "Epoch 272/300\n",
            "\n",
            "Epoch 00272: val_loss did not improve from 0.46231\n",
            "60/60 - 0s - loss: 0.0877 - acc: 0.9833 - val_loss: 0.5374 - val_acc: 0.7500\n",
            "Epoch 273/300\n",
            "\n",
            "Epoch 00273: val_loss did not improve from 0.46231\n",
            "60/60 - 0s - loss: 0.1027 - acc: 0.9833 - val_loss: 0.5255 - val_acc: 0.8000\n",
            "Epoch 274/300\n",
            "\n",
            "Epoch 00274: val_loss did not improve from 0.46231\n",
            "60/60 - 0s - loss: 0.0759 - acc: 1.0000 - val_loss: 0.4757 - val_acc: 0.8000\n",
            "Epoch 275/300\n",
            "\n",
            "Epoch 00275: val_loss did not improve from 0.46231\n",
            "60/60 - 0s - loss: 0.1068 - acc: 1.0000 - val_loss: 0.4624 - val_acc: 0.8000\n",
            "Epoch 276/300\n",
            "\n",
            "Epoch 00276: val_loss improved from 0.46231 to 0.45242, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.0847 - acc: 0.9833 - val_loss: 0.4524 - val_acc: 0.8500\n",
            "Epoch 277/300\n",
            "\n",
            "Epoch 00277: val_loss improved from 0.45242 to 0.44766, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.0995 - acc: 0.9833 - val_loss: 0.4477 - val_acc: 0.8500\n",
            "Epoch 278/300\n",
            "\n",
            "Epoch 00278: val_loss did not improve from 0.44766\n",
            "60/60 - 0s - loss: 0.0644 - acc: 1.0000 - val_loss: 0.4539 - val_acc: 0.8500\n",
            "Epoch 279/300\n",
            "\n",
            "Epoch 00279: val_loss did not improve from 0.44766\n",
            "60/60 - 0s - loss: 0.0584 - acc: 1.0000 - val_loss: 0.4570 - val_acc: 0.8500\n",
            "Epoch 280/300\n",
            "\n",
            "Epoch 00280: val_loss did not improve from 0.44766\n",
            "60/60 - 0s - loss: 0.0731 - acc: 1.0000 - val_loss: 0.4526 - val_acc: 0.8500\n",
            "Epoch 281/300\n",
            "\n",
            "Epoch 00281: val_loss improved from 0.44766 to 0.44608, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1492 - acc: 0.9333 - val_loss: 0.4461 - val_acc: 0.9000\n",
            "Epoch 282/300\n",
            "\n",
            "Epoch 00282: val_loss did not improve from 0.44608\n",
            "60/60 - 0s - loss: 0.0612 - acc: 1.0000 - val_loss: 0.4520 - val_acc: 0.8500\n",
            "Epoch 283/300\n",
            "\n",
            "Epoch 00283: val_loss did not improve from 0.44608\n",
            "60/60 - 0s - loss: 0.1334 - acc: 0.9833 - val_loss: 0.4623 - val_acc: 0.8000\n",
            "Epoch 284/300\n",
            "\n",
            "Epoch 00284: val_loss did not improve from 0.44608\n",
            "60/60 - 0s - loss: 0.0960 - acc: 0.9833 - val_loss: 0.4900 - val_acc: 0.8000\n",
            "Epoch 285/300\n",
            "\n",
            "Epoch 00285: val_loss did not improve from 0.44608\n",
            "60/60 - 0s - loss: 0.0806 - acc: 0.9833 - val_loss: 0.5061 - val_acc: 0.8000\n",
            "Epoch 286/300\n",
            "\n",
            "Epoch 00286: val_loss did not improve from 0.44608\n",
            "60/60 - 0s - loss: 0.1400 - acc: 0.9833 - val_loss: 0.5001 - val_acc: 0.8000\n",
            "Epoch 287/300\n",
            "\n",
            "Epoch 00287: val_loss did not improve from 0.44608\n",
            "60/60 - 0s - loss: 0.0838 - acc: 1.0000 - val_loss: 0.5059 - val_acc: 0.8000\n",
            "Epoch 288/300\n",
            "\n",
            "Epoch 00288: val_loss did not improve from 0.44608\n",
            "60/60 - 0s - loss: 0.1053 - acc: 0.9667 - val_loss: 0.4903 - val_acc: 0.8500\n",
            "Epoch 289/300\n",
            "\n",
            "Epoch 00289: val_loss did not improve from 0.44608\n",
            "60/60 - 0s - loss: 0.0891 - acc: 0.9833 - val_loss: 0.4943 - val_acc: 0.8500\n",
            "Epoch 290/300\n",
            "\n",
            "Epoch 00290: val_loss did not improve from 0.44608\n",
            "60/60 - 0s - loss: 0.1134 - acc: 0.9667 - val_loss: 0.4823 - val_acc: 0.8500\n",
            "Epoch 291/300\n",
            "\n",
            "Epoch 00291: val_loss did not improve from 0.44608\n",
            "60/60 - 0s - loss: 0.1063 - acc: 0.9833 - val_loss: 0.4686 - val_acc: 0.8500\n",
            "Epoch 292/300\n",
            "\n",
            "Epoch 00292: val_loss improved from 0.44608 to 0.44474, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.0920 - acc: 0.9833 - val_loss: 0.4447 - val_acc: 0.8500\n",
            "Epoch 293/300\n",
            "\n",
            "Epoch 00293: val_loss improved from 0.44474 to 0.41989, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.0903 - acc: 1.0000 - val_loss: 0.4199 - val_acc: 0.9000\n",
            "Epoch 294/300\n",
            "\n",
            "Epoch 00294: val_loss improved from 0.41989 to 0.40817, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1255 - acc: 0.9833 - val_loss: 0.4082 - val_acc: 0.9000\n",
            "Epoch 295/300\n",
            "\n",
            "Epoch 00295: val_loss improved from 0.40817 to 0.40568, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.0667 - acc: 1.0000 - val_loss: 0.4057 - val_acc: 0.9000\n",
            "Epoch 296/300\n",
            "\n",
            "Epoch 00296: val_loss did not improve from 0.40568\n",
            "60/60 - 0s - loss: 0.0776 - acc: 1.0000 - val_loss: 0.4134 - val_acc: 0.8500\n",
            "Epoch 297/300\n",
            "\n",
            "Epoch 00297: val_loss did not improve from 0.40568\n",
            "60/60 - 0s - loss: 0.0683 - acc: 1.0000 - val_loss: 0.4414 - val_acc: 0.8000\n",
            "Epoch 298/300\n",
            "\n",
            "Epoch 00298: val_loss did not improve from 0.40568\n",
            "60/60 - 0s - loss: 0.0841 - acc: 0.9833 - val_loss: 0.4880 - val_acc: 0.8000\n",
            "Epoch 299/300\n",
            "\n",
            "Epoch 00299: val_loss did not improve from 0.40568\n",
            "60/60 - 0s - loss: 0.1003 - acc: 0.9667 - val_loss: 0.5099 - val_acc: 0.8000\n",
            "Epoch 300/300\n",
            "\n",
            "Epoch 00300: val_loss did not improve from 0.40568\n",
            "60/60 - 0s - loss: 0.0644 - acc: 1.0000 - val_loss: 0.4801 - val_acc: 0.8000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "# Evaluate on test data\n",
            "40/40 [==============================] - 0s 2ms/sample - loss: 0.4524 - acc: 0.7750\n",
            "test loss, test acc: [0.45242336734663696, 0.775]\n",
            "[[0.55340289]\n",
            " [0.4836345 ]\n",
            " [2.04906544]\n",
            " [0.53351076]\n",
            " [1.30579983]\n",
            " [0.86277465]\n",
            " [0.46224519]\n",
            " [1.42387397]\n",
            " [0.58868554]\n",
            " [0.45242337]]\n",
            "[[0.67500001]\n",
            " [0.875     ]\n",
            " [0.5       ]\n",
            " [0.72500002]\n",
            " [0.47499999]\n",
            " [0.72500002]\n",
            " [0.75      ]\n",
            " [0.44999999]\n",
            " [0.67500001]\n",
            " [0.77499998]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kd6FTGs7vBI3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "df_accl_all = pd.DataFrame({'Class1vs2': acc_all[:, 0]})\n",
        "df_accl_all.to_csv (r'EEG_Deep/df_accl_allPatient_8_24_2560:4096.csv', index = None, header=True) #Don't forget to add '.csv' at the end of the path\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}