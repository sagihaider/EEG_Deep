{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "main_Single_Trial_Classification_PatientData_CrossSubject",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sagihaider/EEG_Deep/blob/master/main_Single_Trial_Classification_PatientData_CrossSubject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X110PE9MjrlE",
        "colab_type": "code",
        "outputId": "fd193ca7-8cf8-41e8-fd66-2761b606efad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        }
      },
      "source": [
        "!git clone https://github.com/sagihaider/EEG_Deep.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'EEG_Deep'...\n",
            "remote: Enumerating objects: 45, done.\u001b[K\n",
            "remote: Counting objects: 100% (45/45), done.\u001b[K\n",
            "remote: Compressing objects: 100% (41/41), done.\u001b[K\n",
            "remote: Total 198 (delta 24), reused 10 (delta 4), pack-reused 153\u001b[K\n",
            "Receiving objects: 100% (198/198), 858.05 MiB | 14.79 MiB/s, done.\n",
            "Resolving deltas: 100% (77/77), done.\n",
            "Checking out files: 100% (60/60), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ioLzMoiS60Qf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 65
        },
        "outputId": "0012892f-65e0-4cdc-c502-04246d608900"
      },
      "source": [
        "import scipy.io as spio\n",
        "import numpy as np\n",
        "from importlib.machinery import SourceFileLoader\n",
        "\n",
        "# EEGNet-specific imports\n",
        "from EEG_Deep.EEGModels import EEGNet, ShallowConvNet, DeepConvNet\n",
        "from tensorflow.keras import utils as np_utils\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras import backend as K\n",
        "K.set_image_data_format('channels_first')\n",
        "\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# tools for plotting confusion matrices\n",
        "from matplotlib import pyplot as plt\n",
        "from scipy.signal import butter, lfilter\n",
        "\n",
        "from numpy import array\n",
        "from sklearn.model_selection import KFold"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mo80Jg_Pn5lP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Band-pass Filter\n",
        "def butter_bandpass_filter(data, lowcut, highcut, fs, order=4):\n",
        "    nyq = 0.5 * fs\n",
        "    low = lowcut / nyq\n",
        "    high = highcut / nyq\n",
        "    b, a = butter(order, [low, high], btype='band')\n",
        "    y = lfilter(b, a, data)\n",
        "    return y\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZjtnE_6RSpum",
        "colab_type": "code",
        "outputId": "353f3c62-8c2d-4767-9f24-1f528e97c4dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from numpy import zeros\n",
        "K.clear_session()\n",
        "cols = 1\n",
        "rows = 10\n",
        "acc_all = zeros([rows, cols])\n",
        "loss_all = zeros([rows, cols])\n",
        "result=[]\n",
        "count = 0\n",
        "# data sample\n",
        "data = array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
        "# prepare cross validation\n",
        "kfold = KFold(10, True, 1)\n",
        "# enumerate splits\n",
        "for train, test in kfold.split(data):\n",
        "  count = count + 1\n",
        "  # print('train: %s, test: %s' % (data[train], data[test]))\n",
        "  r_X_tr = np.empty([0, 12, 4096])\n",
        "  Y_tr = np.empty([0,1])\n",
        "  r_X_ts = np.empty([0, 12, 4096])\n",
        "  Y_ts = np.empty([0,1])\n",
        "  X_tr = np.empty([720, 12, 4096])\n",
        "  X_ts = np.empty([40, 12, 4096])\n",
        "  \n",
        "\n",
        "  for x in data[train]:\n",
        "    print(x)\n",
        "    fName = 'EEG_Deep/Data2A/parsed_P0' + str(x) + 'T.mat'  # Load Data\n",
        "    print(fName)\n",
        "    mat = spio.loadmat(fName)\n",
        "    x_tr = mat['RawEEGData']\n",
        "    y_tr = mat['Labels']\n",
        "    print(r_X_tr.shape)\n",
        "    r_X_tr=np.append(r_X_tr, x_tr, axis=0)\n",
        "    Y_tr=np.append(Y_tr, y_tr, axis=0)\n",
        "    print(r_X_tr.shape)\n",
        "    print(Y_tr.shape)\n",
        "\n",
        "\n",
        "  for x in data[test]:\n",
        "    print(x)\n",
        "    subid = x \n",
        "    fName = 'EEG_Deep/Data2A/parsed_P0' + str(x) + 'E.mat'  # Load Data\n",
        "    print(fName)\n",
        "    mat = spio.loadmat(fName)\n",
        "    x_ts = mat['RawEEGData']\n",
        "    y_ts = mat['Labels']\n",
        "    print(r_X_ts.shape)\n",
        "    r_X_ts=np.append(r_X_ts, x_ts, axis=0)\n",
        "    Y_ts=np.append(Y_ts, y_ts, axis=0)\n",
        "    print(r_X_ts.shape)\n",
        "    print(Y_ts.shape)\n",
        "\n",
        "\n",
        "  ### Filter Training Data ###\n",
        "  print(\"Filtering of training data in progress\")\n",
        "  for t in range(r_X_tr.shape[0]):\n",
        "    tril = r_X_tr[t,:,:]\n",
        "    # tril = tril.transpose()\n",
        "    tril_filtered = butter_bandpass_filter(tril, \n",
        "                                              lowcut=8, \n",
        "                                              highcut=24, \n",
        "                                              fs=512,\n",
        "                                              order=4)\n",
        "    # tril_filtered = tril_filtered.transpose()\n",
        "    X_tr[t,:,:] = tril_filtered\n",
        "  \n",
        "  print(X_tr.shape)\n",
        "\n",
        "  ### Filter Test Data Data ###\n",
        "  print(\"Filtering of test data in progress\")\n",
        "  for t in range(r_X_ts.shape[0]):\n",
        "    tril = r_X_ts[t,:,:]\n",
        "    # tril = tril.transpose()\n",
        "    tril_filtered = butter_bandpass_filter(tril, \n",
        "                                              lowcut=8, \n",
        "                                              highcut=24, \n",
        "                                              fs=512,\n",
        "                                              order=4)\n",
        "    # tril_filtered = tril_filtered.transpose()\n",
        "    X_ts[t,:,:] = tril_filtered\n",
        "\n",
        "  print(X_ts.shape)\n",
        "\n",
        "  indices = np.arange(X_tr.shape[0])\n",
        "  np.random.shuffle(indices)\n",
        "\n",
        "  X_tr = X_tr[indices]\n",
        "  Y_tr = Y_tr[indices]\n",
        "\n",
        "  # split data of each subject in training and validation\n",
        "  X_train = X_tr[0:620,:,2560:4096]\n",
        "  Y_train = Y_tr[0:620].ravel()\n",
        "  X_val   = X_tr[620:,:,2560:4096]\n",
        "  Y_val   = Y_tr[620:].ravel()\n",
        "  print(Y_val)\n",
        "  print(np.shape(X_train))\n",
        "  print(np.shape(Y_train))\n",
        "  print(np.shape(X_val))\n",
        "  print(np.shape(Y_val))\n",
        "\n",
        "  # convert labels to one-hot encodings.\n",
        "  Y_train      = np_utils.to_categorical(Y_train-1, num_classes=2)\n",
        "  Y_val       = np_utils.to_categorical(Y_val-1, num_classes=2)\n",
        "  print(Y_val)\n",
        "\n",
        "  kernels, chans, samples = 1, 12, 1536\n",
        "  # convert data to NCHW (trials, kernels, channels, samples) format. Data \n",
        "  # contains 22 channels and 500 time-points. Set the number of kernels to 1.\n",
        "  X_train = X_train.reshape(X_train.shape[0], kernels, chans, samples)\n",
        "  X_val   = X_val.reshape(X_val.shape[0], kernels, chans, samples)\n",
        "   \n",
        "  print('X_train shape:', X_train.shape)\n",
        "  print(X_train.shape[0], 'train samples')\n",
        "  print(X_val.shape[0], 'val samples')\n",
        "\n",
        "  X_test      = X_ts[:,:,2560:4096]\n",
        "  Y_test      = Y_ts[:]\n",
        "  print(np.shape(X_test))\n",
        "  print(np.shape(Y_test))\n",
        "\n",
        "  #convert labels to one-hot encodings.\n",
        "  Y_test      = np_utils.to_categorical(Y_test-1, num_classes=2)\n",
        "\n",
        "  # convert data to NCHW (trials, kernels, channels, samples) format. Data \n",
        "  # contains 22 channels and 500 time-points. Set the number of kernels to 1.\n",
        "  X_test = X_test.reshape(X_test.shape[0], kernels, chans, samples)\n",
        "  print('X_train shape:', X_test.shape)\n",
        "  print(X_test.shape[0], 'train samples')\n",
        "\n",
        "  # configure the EEGNet-8,2,16 model with kernel length of 32 samples (other \n",
        "  # model configurations may do better, but this is a good starting point)\n",
        "  model = EEGNet(nb_classes = 2, Chans = 12, Samples = 1536,\n",
        "                 dropoutRate = 0.5, kernLength = 25, F1 = 8, \n",
        "                 D = 2, F2 = 16, norm_rate = 0.25, dropoutType = 'Dropout')\n",
        "  \n",
        "  model.summary()\n",
        "      \n",
        "  # compile the model and set the optimizers\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='adam', \n",
        "                metrics = ['accuracy'])\n",
        "\n",
        "  # count number of parameters in the model\n",
        "  numParams    = model.count_params() \n",
        "\n",
        "  # set a valid path for your system to record model checkpoints\n",
        "  checkpointer = ModelCheckpoint(filepath='/tmp/checkpoint.h5', verbose=1,\n",
        "                                 save_best_only=True)\n",
        "  \n",
        "  # the syntax is {class_1:weight_1, class_2:weight_2,...}. Here just setting\n",
        "  # the weights all to be 1\n",
        "  class_weights = {0:1, 1:1}\n",
        "\n",
        "  history = model.fit(X_train, Y_train, batch_size = 16, epochs = 300,\n",
        "                      verbose = 2, validation_data=(X_val, Y_val),\n",
        "                      callbacks=[checkpointer], class_weight = class_weights)\n",
        "  \n",
        "  # Plot training & validation accuracy values\n",
        "  plt.plot(history.history['acc'])\n",
        "  plt.plot(history.history['val_acc'])\n",
        "  plt.title('Model accuracy')\n",
        "  plt.ylabel('Accuracy')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.legend(['Train', 'Test'], loc='upper left')\n",
        "  plt.show()\n",
        "  figName = 'Accuracy_A0' + str(x) + '.png'  \n",
        "  plt.savefig(figName)\n",
        "\n",
        "  print('\\n# Evaluate on test data')\n",
        "  results = model.evaluate(X_test, Y_test, batch_size=1)\n",
        "  print('test loss, test acc:', results)\n",
        "\n",
        "  loss_all[subid - 1, 0] = results[0]\n",
        "  acc_all[subid - 1, 0] = results[1]\n",
        "  \n",
        "  from keras import backend as K \n",
        "  # Do some code, e.g. train and save model\n",
        "  K.clear_session()\n",
        "\n",
        "print(loss_all)\n",
        "print(acc_all)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n",
            "EEG_Deep/Data2A/parsed_P01T.mat\n",
            "(0, 12, 4096)\n",
            "(80, 12, 4096)\n",
            "(80, 1)\n",
            "2\n",
            "EEG_Deep/Data2A/parsed_P02T.mat\n",
            "(80, 12, 4096)\n",
            "(160, 12, 4096)\n",
            "(160, 1)\n",
            "4\n",
            "EEG_Deep/Data2A/parsed_P04T.mat\n",
            "(160, 12, 4096)\n",
            "(240, 12, 4096)\n",
            "(240, 1)\n",
            "5\n",
            "EEG_Deep/Data2A/parsed_P05T.mat\n",
            "(240, 12, 4096)\n",
            "(320, 12, 4096)\n",
            "(320, 1)\n",
            "6\n",
            "EEG_Deep/Data2A/parsed_P06T.mat\n",
            "(320, 12, 4096)\n",
            "(400, 12, 4096)\n",
            "(400, 1)\n",
            "7\n",
            "EEG_Deep/Data2A/parsed_P07T.mat\n",
            "(400, 12, 4096)\n",
            "(480, 12, 4096)\n",
            "(480, 1)\n",
            "8\n",
            "EEG_Deep/Data2A/parsed_P08T.mat\n",
            "(480, 12, 4096)\n",
            "(560, 12, 4096)\n",
            "(560, 1)\n",
            "9\n",
            "EEG_Deep/Data2A/parsed_P09T.mat\n",
            "(560, 12, 4096)\n",
            "(640, 12, 4096)\n",
            "(640, 1)\n",
            "10\n",
            "EEG_Deep/Data2A/parsed_P010T.mat\n",
            "(640, 12, 4096)\n",
            "(720, 12, 4096)\n",
            "(720, 1)\n",
            "3\n",
            "EEG_Deep/Data2A/parsed_P03E.mat\n",
            "(0, 12, 4096)\n",
            "(40, 12, 4096)\n",
            "(40, 1)\n",
            "Filtering of training data in progress\n",
            "(720, 12, 4096)\n",
            "Filtering of test data in progress\n",
            "(40, 12, 4096)\n",
            "[1. 1. 2. 2. 2. 1. 2. 1. 1. 2. 2. 2. 1. 2. 1. 1. 2. 1. 1. 1. 1. 1. 2. 2.\n",
            " 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 1. 2. 2. 1. 2. 1. 2. 1. 2. 2. 1. 2. 2. 2.\n",
            " 2. 1. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 1. 1. 1. 2. 1. 1. 1. 1.\n",
            " 1. 2. 2. 1. 2. 1. 1. 1. 2. 1. 1. 1. 1. 1. 2. 1. 1. 2. 2. 1. 2. 1. 2. 1.\n",
            " 1. 2. 2. 2.]\n",
            "(620, 12, 1536)\n",
            "(620,)\n",
            "(100, 12, 1536)\n",
            "(100,)\n",
            "[[1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]]\n",
            "X_train shape: (620, 1, 12, 1536)\n",
            "620 train samples\n",
            "100 val samples\n",
            "(40, 12, 1536)\n",
            "(40, 1)\n",
            "X_train shape: (40, 1, 12, 1536)\n",
            "40 train samples\n",
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 1, 12, 1536)]     0         \n",
            "_________________________________________________________________\n",
            "conv2d (Conv2D)              (None, 8, 12, 1536)       200       \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 8, 12, 1536)       32        \n",
            "_________________________________________________________________\n",
            "depthwise_conv2d (DepthwiseC (None, 16, 1, 1536)       192       \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 16, 1, 1536)       64        \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 16, 1, 1536)       0         \n",
            "_________________________________________________________________\n",
            "average_pooling2d (AveragePo (None, 16, 1, 384)        0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 16, 1, 384)        0         \n",
            "_________________________________________________________________\n",
            "separable_conv2d (SeparableC (None, 16, 1, 384)        512       \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 16, 1, 384)        64        \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 16, 1, 384)        0         \n",
            "_________________________________________________________________\n",
            "average_pooling2d_1 (Average (None, 16, 1, 48)         0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 16, 1, 48)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 768)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 2)                 1538      \n",
            "_________________________________________________________________\n",
            "softmax (Activation)         (None, 2)                 0         \n",
            "=================================================================\n",
            "Total params: 2,602\n",
            "Trainable params: 2,522\n",
            "Non-trainable params: 80\n",
            "_________________________________________________________________\n",
            "Train on 620 samples, validate on 100 samples\n",
            "Epoch 1/300\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.68659, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.6789 - acc: 0.5758 - val_loss: 0.6866 - val_acc: 0.5800\n",
            "Epoch 2/300\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.68659 to 0.67553, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.6290 - acc: 0.7016 - val_loss: 0.6755 - val_acc: 0.5500\n",
            "Epoch 3/300\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.67553 to 0.66901, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.5743 - acc: 0.7145 - val_loss: 0.6690 - val_acc: 0.5500\n",
            "Epoch 4/300\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.66901\n",
            "620/620 - 1s - loss: 0.5341 - acc: 0.7355 - val_loss: 0.7158 - val_acc: 0.5400\n",
            "Epoch 5/300\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.66901\n",
            "620/620 - 1s - loss: 0.5391 - acc: 0.7048 - val_loss: 0.7137 - val_acc: 0.5600\n",
            "Epoch 6/300\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.66901\n",
            "620/620 - 1s - loss: 0.5019 - acc: 0.7758 - val_loss: 0.7291 - val_acc: 0.5800\n",
            "Epoch 7/300\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.66901\n",
            "620/620 - 1s - loss: 0.5032 - acc: 0.7565 - val_loss: 0.7150 - val_acc: 0.6200\n",
            "Epoch 8/300\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.66901\n",
            "620/620 - 1s - loss: 0.4840 - acc: 0.7677 - val_loss: 0.7110 - val_acc: 0.6200\n",
            "Epoch 9/300\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.66901\n",
            "620/620 - 1s - loss: 0.4847 - acc: 0.7661 - val_loss: 0.7201 - val_acc: 0.6200\n",
            "Epoch 10/300\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.66901\n",
            "620/620 - 1s - loss: 0.4817 - acc: 0.7468 - val_loss: 0.6847 - val_acc: 0.6400\n",
            "Epoch 11/300\n",
            "\n",
            "Epoch 00011: val_loss improved from 0.66901 to 0.63353, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.4989 - acc: 0.7694 - val_loss: 0.6335 - val_acc: 0.6900\n",
            "Epoch 12/300\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.63353\n",
            "620/620 - 1s - loss: 0.5002 - acc: 0.7581 - val_loss: 0.6863 - val_acc: 0.6100\n",
            "Epoch 13/300\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.63353\n",
            "620/620 - 1s - loss: 0.4911 - acc: 0.7710 - val_loss: 0.6960 - val_acc: 0.6300\n",
            "Epoch 14/300\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.63353\n",
            "620/620 - 1s - loss: 0.4915 - acc: 0.7500 - val_loss: 0.6504 - val_acc: 0.6900\n",
            "Epoch 15/300\n",
            "\n",
            "Epoch 00015: val_loss improved from 0.63353 to 0.59353, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.4544 - acc: 0.7839 - val_loss: 0.5935 - val_acc: 0.7000\n",
            "Epoch 16/300\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.59353\n",
            "620/620 - 1s - loss: 0.4495 - acc: 0.8065 - val_loss: 0.6000 - val_acc: 0.6900\n",
            "Epoch 17/300\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.59353\n",
            "620/620 - 1s - loss: 0.4779 - acc: 0.7742 - val_loss: 0.6116 - val_acc: 0.6900\n",
            "Epoch 18/300\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.59353\n",
            "620/620 - 1s - loss: 0.4549 - acc: 0.7871 - val_loss: 0.6276 - val_acc: 0.7300\n",
            "Epoch 19/300\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.59353\n",
            "620/620 - 1s - loss: 0.4719 - acc: 0.7855 - val_loss: 0.5979 - val_acc: 0.7000\n",
            "Epoch 20/300\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.59353\n",
            "620/620 - 1s - loss: 0.4683 - acc: 0.7645 - val_loss: 0.6306 - val_acc: 0.6900\n",
            "Epoch 21/300\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.59353\n",
            "620/620 - 1s - loss: 0.4620 - acc: 0.7710 - val_loss: 0.6613 - val_acc: 0.6800\n",
            "Epoch 22/300\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.59353\n",
            "620/620 - 1s - loss: 0.4376 - acc: 0.7935 - val_loss: 0.6493 - val_acc: 0.7000\n",
            "Epoch 23/300\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.59353\n",
            "620/620 - 1s - loss: 0.4798 - acc: 0.7661 - val_loss: 0.5970 - val_acc: 0.7200\n",
            "Epoch 24/300\n",
            "\n",
            "Epoch 00024: val_loss improved from 0.59353 to 0.59268, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.4612 - acc: 0.7968 - val_loss: 0.5927 - val_acc: 0.7000\n",
            "Epoch 25/300\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.59268\n",
            "620/620 - 1s - loss: 0.4338 - acc: 0.8048 - val_loss: 0.5974 - val_acc: 0.7500\n",
            "Epoch 26/300\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.59268\n",
            "620/620 - 1s - loss: 0.4542 - acc: 0.7871 - val_loss: 0.6223 - val_acc: 0.7100\n",
            "Epoch 27/300\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.59268\n",
            "620/620 - 1s - loss: 0.4561 - acc: 0.7839 - val_loss: 0.6487 - val_acc: 0.6900\n",
            "Epoch 28/300\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.59268\n",
            "620/620 - 1s - loss: 0.4458 - acc: 0.7887 - val_loss: 0.6201 - val_acc: 0.7100\n",
            "Epoch 29/300\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.59268\n",
            "620/620 - 1s - loss: 0.4474 - acc: 0.8000 - val_loss: 0.6050 - val_acc: 0.7000\n",
            "Epoch 30/300\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.59268\n",
            "620/620 - 1s - loss: 0.4573 - acc: 0.8048 - val_loss: 0.6893 - val_acc: 0.6700\n",
            "Epoch 31/300\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.59268\n",
            "620/620 - 1s - loss: 0.4440 - acc: 0.8065 - val_loss: 0.6298 - val_acc: 0.7100\n",
            "Epoch 32/300\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.59268\n",
            "620/620 - 1s - loss: 0.4650 - acc: 0.7726 - val_loss: 0.5971 - val_acc: 0.7000\n",
            "Epoch 33/300\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.59268\n",
            "620/620 - 1s - loss: 0.4322 - acc: 0.7952 - val_loss: 0.6118 - val_acc: 0.7000\n",
            "Epoch 34/300\n",
            "\n",
            "Epoch 00034: val_loss improved from 0.59268 to 0.58540, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.4290 - acc: 0.7935 - val_loss: 0.5854 - val_acc: 0.6900\n",
            "Epoch 35/300\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.58540\n",
            "620/620 - 1s - loss: 0.4516 - acc: 0.8113 - val_loss: 0.6183 - val_acc: 0.7000\n",
            "Epoch 36/300\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.58540\n",
            "620/620 - 1s - loss: 0.4260 - acc: 0.8113 - val_loss: 0.5978 - val_acc: 0.7100\n",
            "Epoch 37/300\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.58540\n",
            "620/620 - 1s - loss: 0.4121 - acc: 0.8016 - val_loss: 0.6205 - val_acc: 0.7200\n",
            "Epoch 38/300\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.58540\n",
            "620/620 - 1s - loss: 0.4506 - acc: 0.8065 - val_loss: 0.5858 - val_acc: 0.7500\n",
            "Epoch 39/300\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.58540\n",
            "620/620 - 1s - loss: 0.4566 - acc: 0.7726 - val_loss: 0.6237 - val_acc: 0.6800\n",
            "Epoch 40/300\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.58540\n",
            "620/620 - 1s - loss: 0.4140 - acc: 0.8274 - val_loss: 0.6071 - val_acc: 0.7200\n",
            "Epoch 41/300\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.58540\n",
            "620/620 - 1s - loss: 0.4260 - acc: 0.7919 - val_loss: 0.5991 - val_acc: 0.7100\n",
            "Epoch 42/300\n",
            "\n",
            "Epoch 00042: val_loss improved from 0.58540 to 0.56528, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.4385 - acc: 0.7806 - val_loss: 0.5653 - val_acc: 0.7400\n",
            "Epoch 43/300\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.56528\n",
            "620/620 - 1s - loss: 0.4328 - acc: 0.7984 - val_loss: 0.6112 - val_acc: 0.7300\n",
            "Epoch 44/300\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.56528\n",
            "620/620 - 1s - loss: 0.4299 - acc: 0.7968 - val_loss: 0.5787 - val_acc: 0.7200\n",
            "Epoch 45/300\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.56528\n",
            "620/620 - 1s - loss: 0.4320 - acc: 0.8000 - val_loss: 0.5691 - val_acc: 0.7400\n",
            "Epoch 46/300\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.56528\n",
            "620/620 - 1s - loss: 0.4466 - acc: 0.7758 - val_loss: 0.5997 - val_acc: 0.7200\n",
            "Epoch 47/300\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.56528\n",
            "620/620 - 1s - loss: 0.4388 - acc: 0.7903 - val_loss: 0.6237 - val_acc: 0.6800\n",
            "Epoch 48/300\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.56528\n",
            "620/620 - 1s - loss: 0.4340 - acc: 0.8129 - val_loss: 0.6186 - val_acc: 0.7200\n",
            "Epoch 49/300\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.56528\n",
            "620/620 - 1s - loss: 0.3986 - acc: 0.8242 - val_loss: 0.5831 - val_acc: 0.7700\n",
            "Epoch 50/300\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.56528\n",
            "620/620 - 1s - loss: 0.4065 - acc: 0.8161 - val_loss: 0.6172 - val_acc: 0.7100\n",
            "Epoch 51/300\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.56528\n",
            "620/620 - 1s - loss: 0.4418 - acc: 0.7952 - val_loss: 0.6051 - val_acc: 0.7200\n",
            "Epoch 52/300\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.56528\n",
            "620/620 - 1s - loss: 0.4180 - acc: 0.8032 - val_loss: 0.5694 - val_acc: 0.7200\n",
            "Epoch 53/300\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.56528\n",
            "620/620 - 1s - loss: 0.4148 - acc: 0.8177 - val_loss: 0.6306 - val_acc: 0.7400\n",
            "Epoch 54/300\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.56528\n",
            "620/620 - 1s - loss: 0.4209 - acc: 0.7968 - val_loss: 0.6122 - val_acc: 0.7300\n",
            "Epoch 55/300\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.56528\n",
            "620/620 - 1s - loss: 0.4256 - acc: 0.8081 - val_loss: 0.6162 - val_acc: 0.6800\n",
            "Epoch 56/300\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.56528\n",
            "620/620 - 1s - loss: 0.4279 - acc: 0.8000 - val_loss: 0.5866 - val_acc: 0.7600\n",
            "Epoch 57/300\n",
            "\n",
            "Epoch 00057: val_loss improved from 0.56528 to 0.55590, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.4129 - acc: 0.8048 - val_loss: 0.5559 - val_acc: 0.7700\n",
            "Epoch 58/300\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.55590\n",
            "620/620 - 1s - loss: 0.4222 - acc: 0.8097 - val_loss: 0.5822 - val_acc: 0.7700\n",
            "Epoch 59/300\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.55590\n",
            "620/620 - 1s - loss: 0.4264 - acc: 0.8129 - val_loss: 0.6211 - val_acc: 0.6900\n",
            "Epoch 60/300\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.55590\n",
            "620/620 - 1s - loss: 0.4439 - acc: 0.8016 - val_loss: 0.5774 - val_acc: 0.7300\n",
            "Epoch 61/300\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.55590\n",
            "620/620 - 1s - loss: 0.3951 - acc: 0.8274 - val_loss: 0.6326 - val_acc: 0.7000\n",
            "Epoch 62/300\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.55590\n",
            "620/620 - 1s - loss: 0.4254 - acc: 0.8129 - val_loss: 0.5572 - val_acc: 0.7400\n",
            "Epoch 63/300\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.55590\n",
            "620/620 - 1s - loss: 0.4051 - acc: 0.8032 - val_loss: 0.6395 - val_acc: 0.7100\n",
            "Epoch 64/300\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.55590\n",
            "620/620 - 1s - loss: 0.4029 - acc: 0.8242 - val_loss: 0.5787 - val_acc: 0.7500\n",
            "Epoch 65/300\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.55590\n",
            "620/620 - 1s - loss: 0.3877 - acc: 0.8194 - val_loss: 0.5707 - val_acc: 0.7800\n",
            "Epoch 66/300\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.55590\n",
            "620/620 - 1s - loss: 0.4005 - acc: 0.8145 - val_loss: 0.5654 - val_acc: 0.7300\n",
            "Epoch 67/300\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.55590\n",
            "620/620 - 1s - loss: 0.4078 - acc: 0.8274 - val_loss: 0.5757 - val_acc: 0.7400\n",
            "Epoch 68/300\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.55590\n",
            "620/620 - 1s - loss: 0.4071 - acc: 0.8210 - val_loss: 0.5829 - val_acc: 0.7300\n",
            "Epoch 69/300\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.55590\n",
            "620/620 - 1s - loss: 0.3865 - acc: 0.8210 - val_loss: 0.6783 - val_acc: 0.6800\n",
            "Epoch 70/300\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.55590\n",
            "620/620 - 1s - loss: 0.4073 - acc: 0.8290 - val_loss: 0.5911 - val_acc: 0.7100\n",
            "Epoch 71/300\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.55590\n",
            "620/620 - 1s - loss: 0.4118 - acc: 0.8177 - val_loss: 0.6332 - val_acc: 0.6900\n",
            "Epoch 72/300\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.55590\n",
            "620/620 - 1s - loss: 0.4188 - acc: 0.8097 - val_loss: 0.6094 - val_acc: 0.6900\n",
            "Epoch 73/300\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.55590\n",
            "620/620 - 1s - loss: 0.3981 - acc: 0.8258 - val_loss: 0.6988 - val_acc: 0.6700\n",
            "Epoch 74/300\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.55590\n",
            "620/620 - 1s - loss: 0.3996 - acc: 0.8274 - val_loss: 0.6481 - val_acc: 0.7200\n",
            "Epoch 75/300\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.55590\n",
            "620/620 - 1s - loss: 0.4016 - acc: 0.8048 - val_loss: 0.6063 - val_acc: 0.7300\n",
            "Epoch 76/300\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.55590\n",
            "620/620 - 1s - loss: 0.4097 - acc: 0.8242 - val_loss: 0.6100 - val_acc: 0.7100\n",
            "Epoch 77/300\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.55590\n",
            "620/620 - 1s - loss: 0.4089 - acc: 0.8032 - val_loss: 0.5696 - val_acc: 0.7400\n",
            "Epoch 78/300\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.55590\n",
            "620/620 - 1s - loss: 0.4104 - acc: 0.8081 - val_loss: 0.6871 - val_acc: 0.6900\n",
            "Epoch 79/300\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.55590\n",
            "620/620 - 1s - loss: 0.4172 - acc: 0.7935 - val_loss: 0.6555 - val_acc: 0.6600\n",
            "Epoch 80/300\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.55590\n",
            "620/620 - 1s - loss: 0.4001 - acc: 0.8097 - val_loss: 0.6543 - val_acc: 0.7000\n",
            "Epoch 81/300\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.55590\n",
            "620/620 - 1s - loss: 0.4205 - acc: 0.7903 - val_loss: 0.7303 - val_acc: 0.6400\n",
            "Epoch 82/300\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.55590\n",
            "620/620 - 1s - loss: 0.4044 - acc: 0.8145 - val_loss: 0.6488 - val_acc: 0.7000\n",
            "Epoch 83/300\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.55590\n",
            "620/620 - 1s - loss: 0.4125 - acc: 0.7968 - val_loss: 0.5753 - val_acc: 0.7700\n",
            "Epoch 84/300\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.55590\n",
            "620/620 - 1s - loss: 0.4133 - acc: 0.8032 - val_loss: 0.6065 - val_acc: 0.7100\n",
            "Epoch 85/300\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.55590\n",
            "620/620 - 1s - loss: 0.4035 - acc: 0.7984 - val_loss: 0.6911 - val_acc: 0.6600\n",
            "Epoch 86/300\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.55590\n",
            "620/620 - 1s - loss: 0.4114 - acc: 0.7935 - val_loss: 0.6246 - val_acc: 0.7100\n",
            "Epoch 87/300\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.55590\n",
            "620/620 - 1s - loss: 0.4135 - acc: 0.8129 - val_loss: 0.6792 - val_acc: 0.6800\n",
            "Epoch 88/300\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.55590\n",
            "620/620 - 1s - loss: 0.4026 - acc: 0.8161 - val_loss: 0.6472 - val_acc: 0.7000\n",
            "Epoch 89/300\n",
            "\n",
            "Epoch 00089: val_loss improved from 0.55590 to 0.54546, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.3835 - acc: 0.8306 - val_loss: 0.5455 - val_acc: 0.7700\n",
            "Epoch 90/300\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.54546\n",
            "620/620 - 1s - loss: 0.4096 - acc: 0.8145 - val_loss: 0.5688 - val_acc: 0.7400\n",
            "Epoch 91/300\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.54546\n",
            "620/620 - 1s - loss: 0.3869 - acc: 0.8290 - val_loss: 0.6187 - val_acc: 0.7100\n",
            "Epoch 92/300\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.54546\n",
            "620/620 - 1s - loss: 0.4176 - acc: 0.8113 - val_loss: 0.5709 - val_acc: 0.7400\n",
            "Epoch 93/300\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.54546\n",
            "620/620 - 1s - loss: 0.4210 - acc: 0.8194 - val_loss: 0.5728 - val_acc: 0.7600\n",
            "Epoch 94/300\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.54546\n",
            "620/620 - 1s - loss: 0.3899 - acc: 0.8419 - val_loss: 0.5699 - val_acc: 0.7200\n",
            "Epoch 95/300\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.54546\n",
            "620/620 - 1s - loss: 0.3970 - acc: 0.8242 - val_loss: 0.5471 - val_acc: 0.7800\n",
            "Epoch 96/300\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.54546\n",
            "620/620 - 1s - loss: 0.4223 - acc: 0.7903 - val_loss: 0.5785 - val_acc: 0.7300\n",
            "Epoch 97/300\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.54546\n",
            "620/620 - 1s - loss: 0.3678 - acc: 0.8435 - val_loss: 0.5898 - val_acc: 0.7500\n",
            "Epoch 98/300\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.54546\n",
            "620/620 - 1s - loss: 0.4076 - acc: 0.8097 - val_loss: 0.6634 - val_acc: 0.7000\n",
            "Epoch 99/300\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.54546\n",
            "620/620 - 1s - loss: 0.3863 - acc: 0.8306 - val_loss: 0.5844 - val_acc: 0.7400\n",
            "Epoch 100/300\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.54546\n",
            "620/620 - 1s - loss: 0.3934 - acc: 0.8177 - val_loss: 0.5598 - val_acc: 0.7400\n",
            "Epoch 101/300\n",
            "\n",
            "Epoch 00101: val_loss improved from 0.54546 to 0.53868, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.3730 - acc: 0.8323 - val_loss: 0.5387 - val_acc: 0.8100\n",
            "Epoch 102/300\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.53868\n",
            "620/620 - 1s - loss: 0.3916 - acc: 0.8145 - val_loss: 0.5721 - val_acc: 0.7700\n",
            "Epoch 103/300\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.53868\n",
            "620/620 - 1s - loss: 0.4058 - acc: 0.8145 - val_loss: 0.5909 - val_acc: 0.7200\n",
            "Epoch 104/300\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.53868\n",
            "620/620 - 1s - loss: 0.3989 - acc: 0.8161 - val_loss: 0.5689 - val_acc: 0.7400\n",
            "Epoch 105/300\n",
            "\n",
            "Epoch 00105: val_loss improved from 0.53868 to 0.52664, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.3578 - acc: 0.8484 - val_loss: 0.5266 - val_acc: 0.7700\n",
            "Epoch 106/300\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 0.52664\n",
            "620/620 - 1s - loss: 0.3631 - acc: 0.8371 - val_loss: 0.5551 - val_acc: 0.7900\n",
            "Epoch 107/300\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.52664\n",
            "620/620 - 1s - loss: 0.3723 - acc: 0.8403 - val_loss: 0.6063 - val_acc: 0.7400\n",
            "Epoch 108/300\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 0.52664\n",
            "620/620 - 1s - loss: 0.3712 - acc: 0.8452 - val_loss: 0.6342 - val_acc: 0.7500\n",
            "Epoch 109/300\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.52664\n",
            "620/620 - 1s - loss: 0.3916 - acc: 0.8258 - val_loss: 0.5341 - val_acc: 0.7900\n",
            "Epoch 110/300\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.52664\n",
            "620/620 - 1s - loss: 0.3978 - acc: 0.8194 - val_loss: 0.5862 - val_acc: 0.7200\n",
            "Epoch 111/300\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 0.52664\n",
            "620/620 - 1s - loss: 0.3821 - acc: 0.8435 - val_loss: 0.5585 - val_acc: 0.7700\n",
            "Epoch 112/300\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 0.52664\n",
            "620/620 - 1s - loss: 0.3998 - acc: 0.8371 - val_loss: 0.5778 - val_acc: 0.7500\n",
            "Epoch 113/300\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.52664\n",
            "620/620 - 1s - loss: 0.3928 - acc: 0.8306 - val_loss: 0.6157 - val_acc: 0.7100\n",
            "Epoch 114/300\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.52664\n",
            "620/620 - 1s - loss: 0.3941 - acc: 0.8129 - val_loss: 0.5952 - val_acc: 0.7300\n",
            "Epoch 115/300\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.52664\n",
            "620/620 - 1s - loss: 0.3800 - acc: 0.8306 - val_loss: 0.6611 - val_acc: 0.7000\n",
            "Epoch 116/300\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.52664\n",
            "620/620 - 1s - loss: 0.4184 - acc: 0.8129 - val_loss: 0.5451 - val_acc: 0.7200\n",
            "Epoch 117/300\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.52664\n",
            "620/620 - 1s - loss: 0.3916 - acc: 0.8306 - val_loss: 0.5947 - val_acc: 0.7400\n",
            "Epoch 118/300\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.52664\n",
            "620/620 - 1s - loss: 0.3953 - acc: 0.8194 - val_loss: 0.5927 - val_acc: 0.7100\n",
            "Epoch 119/300\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 0.52664\n",
            "620/620 - 1s - loss: 0.4056 - acc: 0.8177 - val_loss: 0.5737 - val_acc: 0.7200\n",
            "Epoch 120/300\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.52664\n",
            "620/620 - 1s - loss: 0.3825 - acc: 0.8435 - val_loss: 0.5413 - val_acc: 0.8100\n",
            "Epoch 121/300\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.52664\n",
            "620/620 - 1s - loss: 0.3759 - acc: 0.8306 - val_loss: 0.5495 - val_acc: 0.7700\n",
            "Epoch 122/300\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.52664\n",
            "620/620 - 1s - loss: 0.3885 - acc: 0.8403 - val_loss: 0.5375 - val_acc: 0.7600\n",
            "Epoch 123/300\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 0.52664\n",
            "620/620 - 1s - loss: 0.3885 - acc: 0.8242 - val_loss: 0.6177 - val_acc: 0.7400\n",
            "Epoch 124/300\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 0.52664\n",
            "620/620 - 1s - loss: 0.3602 - acc: 0.8371 - val_loss: 0.6065 - val_acc: 0.7300\n",
            "Epoch 125/300\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 0.52664\n",
            "620/620 - 1s - loss: 0.3908 - acc: 0.8210 - val_loss: 0.6773 - val_acc: 0.6800\n",
            "Epoch 126/300\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 0.52664\n",
            "620/620 - 1s - loss: 0.3480 - acc: 0.8548 - val_loss: 0.6120 - val_acc: 0.7700\n",
            "Epoch 127/300\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 0.52664\n",
            "620/620 - 1s - loss: 0.3785 - acc: 0.8306 - val_loss: 0.6241 - val_acc: 0.7000\n",
            "Epoch 128/300\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 0.52664\n",
            "620/620 - 1s - loss: 0.3823 - acc: 0.8371 - val_loss: 0.5484 - val_acc: 0.7500\n",
            "Epoch 129/300\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.52664\n",
            "620/620 - 1s - loss: 0.4053 - acc: 0.8032 - val_loss: 0.5459 - val_acc: 0.7800\n",
            "Epoch 130/300\n",
            "\n",
            "Epoch 00130: val_loss improved from 0.52664 to 0.52426, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.3714 - acc: 0.8371 - val_loss: 0.5243 - val_acc: 0.7600\n",
            "Epoch 131/300\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 0.52426\n",
            "620/620 - 1s - loss: 0.3832 - acc: 0.8371 - val_loss: 0.5374 - val_acc: 0.7900\n",
            "Epoch 132/300\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 0.52426\n",
            "620/620 - 1s - loss: 0.3934 - acc: 0.8113 - val_loss: 0.5825 - val_acc: 0.7300\n",
            "Epoch 133/300\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.52426\n",
            "620/620 - 1s - loss: 0.3785 - acc: 0.8468 - val_loss: 0.5735 - val_acc: 0.7500\n",
            "Epoch 134/300\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.52426\n",
            "620/620 - 1s - loss: 0.4210 - acc: 0.7935 - val_loss: 0.5732 - val_acc: 0.7500\n",
            "Epoch 135/300\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 0.52426\n",
            "620/620 - 1s - loss: 0.3755 - acc: 0.8258 - val_loss: 0.5945 - val_acc: 0.7700\n",
            "Epoch 136/300\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.52426\n",
            "620/620 - 1s - loss: 0.3855 - acc: 0.8355 - val_loss: 0.5951 - val_acc: 0.7600\n",
            "Epoch 137/300\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 0.52426\n",
            "620/620 - 1s - loss: 0.3752 - acc: 0.8323 - val_loss: 0.5772 - val_acc: 0.7700\n",
            "Epoch 138/300\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.52426\n",
            "620/620 - 1s - loss: 0.3990 - acc: 0.8290 - val_loss: 0.6030 - val_acc: 0.7400\n",
            "Epoch 139/300\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.52426\n",
            "620/620 - 1s - loss: 0.3858 - acc: 0.8387 - val_loss: 0.5589 - val_acc: 0.7100\n",
            "Epoch 140/300\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 0.52426\n",
            "620/620 - 1s - loss: 0.3887 - acc: 0.8210 - val_loss: 0.5728 - val_acc: 0.7200\n",
            "Epoch 141/300\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.52426\n",
            "620/620 - 1s - loss: 0.3711 - acc: 0.8387 - val_loss: 0.6310 - val_acc: 0.7100\n",
            "Epoch 142/300\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.52426\n",
            "620/620 - 1s - loss: 0.3797 - acc: 0.8242 - val_loss: 0.6151 - val_acc: 0.7200\n",
            "Epoch 143/300\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 0.52426\n",
            "620/620 - 1s - loss: 0.3553 - acc: 0.8548 - val_loss: 0.5626 - val_acc: 0.7700\n",
            "Epoch 144/300\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 0.52426\n",
            "620/620 - 1s - loss: 0.3936 - acc: 0.8258 - val_loss: 0.5560 - val_acc: 0.7700\n",
            "Epoch 145/300\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.52426\n",
            "620/620 - 1s - loss: 0.3773 - acc: 0.8371 - val_loss: 0.5832 - val_acc: 0.7500\n",
            "Epoch 146/300\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.52426\n",
            "620/620 - 1s - loss: 0.3668 - acc: 0.8435 - val_loss: 0.5395 - val_acc: 0.7700\n",
            "Epoch 147/300\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.52426\n",
            "620/620 - 1s - loss: 0.3602 - acc: 0.8355 - val_loss: 0.5539 - val_acc: 0.7600\n",
            "Epoch 148/300\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 0.52426\n",
            "620/620 - 1s - loss: 0.3581 - acc: 0.8355 - val_loss: 0.6567 - val_acc: 0.6800\n",
            "Epoch 149/300\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.52426\n",
            "620/620 - 1s - loss: 0.3845 - acc: 0.8258 - val_loss: 0.5584 - val_acc: 0.7600\n",
            "Epoch 150/300\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.52426\n",
            "620/620 - 1s - loss: 0.3945 - acc: 0.8290 - val_loss: 0.5795 - val_acc: 0.7400\n",
            "Epoch 151/300\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.52426\n",
            "620/620 - 1s - loss: 0.3820 - acc: 0.8129 - val_loss: 0.5392 - val_acc: 0.7800\n",
            "Epoch 152/300\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.52426\n",
            "620/620 - 1s - loss: 0.3522 - acc: 0.8532 - val_loss: 0.5449 - val_acc: 0.7700\n",
            "Epoch 153/300\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 0.52426\n",
            "620/620 - 1s - loss: 0.3870 - acc: 0.8129 - val_loss: 0.5525 - val_acc: 0.7500\n",
            "Epoch 154/300\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.52426\n",
            "620/620 - 1s - loss: 0.3517 - acc: 0.8468 - val_loss: 0.6545 - val_acc: 0.7200\n",
            "Epoch 155/300\n",
            "\n",
            "Epoch 00155: val_loss improved from 0.52426 to 0.52272, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.3807 - acc: 0.8242 - val_loss: 0.5227 - val_acc: 0.7700\n",
            "Epoch 156/300\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.52272\n",
            "620/620 - 1s - loss: 0.4216 - acc: 0.8065 - val_loss: 0.5294 - val_acc: 0.7600\n",
            "Epoch 157/300\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.52272\n",
            "620/620 - 1s - loss: 0.3677 - acc: 0.8532 - val_loss: 0.5342 - val_acc: 0.7600\n",
            "Epoch 158/300\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.52272\n",
            "620/620 - 1s - loss: 0.3576 - acc: 0.8452 - val_loss: 0.5550 - val_acc: 0.7500\n",
            "Epoch 159/300\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.52272\n",
            "620/620 - 1s - loss: 0.3494 - acc: 0.8468 - val_loss: 0.5362 - val_acc: 0.8200\n",
            "Epoch 160/300\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.52272\n",
            "620/620 - 1s - loss: 0.3500 - acc: 0.8516 - val_loss: 0.5341 - val_acc: 0.7800\n",
            "Epoch 161/300\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.52272\n",
            "620/620 - 1s - loss: 0.3560 - acc: 0.8452 - val_loss: 0.5325 - val_acc: 0.7900\n",
            "Epoch 162/300\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.52272\n",
            "620/620 - 1s - loss: 0.3646 - acc: 0.8371 - val_loss: 0.5389 - val_acc: 0.7700\n",
            "Epoch 163/300\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.52272\n",
            "620/620 - 1s - loss: 0.3857 - acc: 0.8323 - val_loss: 0.5744 - val_acc: 0.7000\n",
            "Epoch 164/300\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.52272\n",
            "620/620 - 1s - loss: 0.3716 - acc: 0.8435 - val_loss: 0.5549 - val_acc: 0.7700\n",
            "Epoch 165/300\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.52272\n",
            "620/620 - 1s - loss: 0.3633 - acc: 0.8355 - val_loss: 0.5293 - val_acc: 0.7800\n",
            "Epoch 166/300\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 0.52272\n",
            "620/620 - 1s - loss: 0.3887 - acc: 0.8145 - val_loss: 0.5234 - val_acc: 0.7900\n",
            "Epoch 167/300\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.52272\n",
            "620/620 - 1s - loss: 0.3441 - acc: 0.8468 - val_loss: 0.5309 - val_acc: 0.7900\n",
            "Epoch 168/300\n",
            "\n",
            "Epoch 00168: val_loss improved from 0.52272 to 0.49600, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.3580 - acc: 0.8565 - val_loss: 0.4960 - val_acc: 0.7800\n",
            "Epoch 169/300\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 0.49600\n",
            "620/620 - 1s - loss: 0.3463 - acc: 0.8581 - val_loss: 0.5233 - val_acc: 0.8200\n",
            "Epoch 170/300\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.49600\n",
            "620/620 - 1s - loss: 0.3712 - acc: 0.8290 - val_loss: 0.5804 - val_acc: 0.7700\n",
            "Epoch 171/300\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.49600\n",
            "620/620 - 1s - loss: 0.3594 - acc: 0.8435 - val_loss: 0.6290 - val_acc: 0.7100\n",
            "Epoch 172/300\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.49600\n",
            "620/620 - 1s - loss: 0.3791 - acc: 0.8145 - val_loss: 0.6717 - val_acc: 0.7000\n",
            "Epoch 173/300\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.49600\n",
            "620/620 - 1s - loss: 0.3725 - acc: 0.8435 - val_loss: 0.5580 - val_acc: 0.7700\n",
            "Epoch 174/300\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.49600\n",
            "620/620 - 1s - loss: 0.3570 - acc: 0.8452 - val_loss: 0.6392 - val_acc: 0.7300\n",
            "Epoch 175/300\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.49600\n",
            "620/620 - 1s - loss: 0.3836 - acc: 0.8323 - val_loss: 0.5330 - val_acc: 0.7900\n",
            "Epoch 176/300\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 0.49600\n",
            "620/620 - 1s - loss: 0.3601 - acc: 0.8387 - val_loss: 0.5424 - val_acc: 0.7700\n",
            "Epoch 177/300\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.49600\n",
            "620/620 - 1s - loss: 0.3366 - acc: 0.8565 - val_loss: 0.7211 - val_acc: 0.6900\n",
            "Epoch 178/300\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.49600\n",
            "620/620 - 1s - loss: 0.3575 - acc: 0.8661 - val_loss: 0.6240 - val_acc: 0.7000\n",
            "Epoch 179/300\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.49600\n",
            "620/620 - 1s - loss: 0.3565 - acc: 0.8452 - val_loss: 0.5548 - val_acc: 0.7500\n",
            "Epoch 180/300\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.49600\n",
            "620/620 - 1s - loss: 0.3635 - acc: 0.8177 - val_loss: 0.6136 - val_acc: 0.7100\n",
            "Epoch 181/300\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.49600\n",
            "620/620 - 1s - loss: 0.3585 - acc: 0.8500 - val_loss: 0.5410 - val_acc: 0.7800\n",
            "Epoch 182/300\n",
            "\n",
            "Epoch 00182: val_loss improved from 0.49600 to 0.49098, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.3680 - acc: 0.8371 - val_loss: 0.4910 - val_acc: 0.8100\n",
            "Epoch 183/300\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.49098\n",
            "620/620 - 1s - loss: 0.3740 - acc: 0.8323 - val_loss: 0.6372 - val_acc: 0.7100\n",
            "Epoch 184/300\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 0.49098\n",
            "620/620 - 1s - loss: 0.3737 - acc: 0.8452 - val_loss: 0.6641 - val_acc: 0.6800\n",
            "Epoch 185/300\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.49098\n",
            "620/620 - 1s - loss: 0.3494 - acc: 0.8484 - val_loss: 0.5659 - val_acc: 0.7600\n",
            "Epoch 186/300\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.49098\n",
            "620/620 - 1s - loss: 0.3599 - acc: 0.8355 - val_loss: 0.5834 - val_acc: 0.7300\n",
            "Epoch 187/300\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 0.49098\n",
            "620/620 - 1s - loss: 0.3551 - acc: 0.8323 - val_loss: 0.5715 - val_acc: 0.7600\n",
            "Epoch 188/300\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.49098\n",
            "620/620 - 1s - loss: 0.3761 - acc: 0.8194 - val_loss: 0.5249 - val_acc: 0.7800\n",
            "Epoch 189/300\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.49098\n",
            "620/620 - 1s - loss: 0.3682 - acc: 0.8419 - val_loss: 0.5760 - val_acc: 0.7400\n",
            "Epoch 190/300\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.49098\n",
            "620/620 - 1s - loss: 0.3918 - acc: 0.8339 - val_loss: 0.5440 - val_acc: 0.7600\n",
            "Epoch 191/300\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.49098\n",
            "620/620 - 1s - loss: 0.3549 - acc: 0.8581 - val_loss: 0.5523 - val_acc: 0.7500\n",
            "Epoch 192/300\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 0.49098\n",
            "620/620 - 1s - loss: 0.3540 - acc: 0.8613 - val_loss: 0.7158 - val_acc: 0.6700\n",
            "Epoch 193/300\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.49098\n",
            "620/620 - 1s - loss: 0.3476 - acc: 0.8565 - val_loss: 0.5337 - val_acc: 0.7900\n",
            "Epoch 194/300\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.49098\n",
            "620/620 - 1s - loss: 0.3593 - acc: 0.8581 - val_loss: 0.4967 - val_acc: 0.7900\n",
            "Epoch 195/300\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.49098\n",
            "620/620 - 1s - loss: 0.3476 - acc: 0.8339 - val_loss: 0.5344 - val_acc: 0.7900\n",
            "Epoch 196/300\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.49098\n",
            "620/620 - 1s - loss: 0.3433 - acc: 0.8581 - val_loss: 0.5002 - val_acc: 0.7800\n",
            "Epoch 197/300\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.49098\n",
            "620/620 - 1s - loss: 0.3724 - acc: 0.8274 - val_loss: 0.5425 - val_acc: 0.7800\n",
            "Epoch 198/300\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.49098\n",
            "620/620 - 1s - loss: 0.3466 - acc: 0.8500 - val_loss: 0.5575 - val_acc: 0.7500\n",
            "Epoch 199/300\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.49098\n",
            "620/620 - 1s - loss: 0.3582 - acc: 0.8371 - val_loss: 0.5208 - val_acc: 0.7800\n",
            "Epoch 200/300\n",
            "\n",
            "Epoch 00200: val_loss improved from 0.49098 to 0.47408, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.3787 - acc: 0.8306 - val_loss: 0.4741 - val_acc: 0.8100\n",
            "Epoch 201/300\n",
            "\n",
            "Epoch 00201: val_loss did not improve from 0.47408\n",
            "620/620 - 1s - loss: 0.3553 - acc: 0.8484 - val_loss: 0.5072 - val_acc: 0.8100\n",
            "Epoch 202/300\n",
            "\n",
            "Epoch 00202: val_loss did not improve from 0.47408\n",
            "620/620 - 1s - loss: 0.3347 - acc: 0.8677 - val_loss: 0.6412 - val_acc: 0.7100\n",
            "Epoch 203/300\n",
            "\n",
            "Epoch 00203: val_loss did not improve from 0.47408\n",
            "620/620 - 1s - loss: 0.3545 - acc: 0.8468 - val_loss: 0.5417 - val_acc: 0.7400\n",
            "Epoch 204/300\n",
            "\n",
            "Epoch 00204: val_loss did not improve from 0.47408\n",
            "620/620 - 1s - loss: 0.3572 - acc: 0.8565 - val_loss: 0.5139 - val_acc: 0.7800\n",
            "Epoch 205/300\n",
            "\n",
            "Epoch 00205: val_loss did not improve from 0.47408\n",
            "620/620 - 1s - loss: 0.3596 - acc: 0.8516 - val_loss: 0.5193 - val_acc: 0.7700\n",
            "Epoch 206/300\n",
            "\n",
            "Epoch 00206: val_loss did not improve from 0.47408\n",
            "620/620 - 1s - loss: 0.3434 - acc: 0.8581 - val_loss: 0.5065 - val_acc: 0.7700\n",
            "Epoch 207/300\n",
            "\n",
            "Epoch 00207: val_loss did not improve from 0.47408\n",
            "620/620 - 1s - loss: 0.3579 - acc: 0.8452 - val_loss: 0.5566 - val_acc: 0.7500\n",
            "Epoch 208/300\n",
            "\n",
            "Epoch 00208: val_loss did not improve from 0.47408\n",
            "620/620 - 1s - loss: 0.3531 - acc: 0.8419 - val_loss: 0.5206 - val_acc: 0.7800\n",
            "Epoch 209/300\n",
            "\n",
            "Epoch 00209: val_loss did not improve from 0.47408\n",
            "620/620 - 1s - loss: 0.3451 - acc: 0.8468 - val_loss: 0.4940 - val_acc: 0.8000\n",
            "Epoch 210/300\n",
            "\n",
            "Epoch 00210: val_loss did not improve from 0.47408\n",
            "620/620 - 1s - loss: 0.3797 - acc: 0.8274 - val_loss: 0.5036 - val_acc: 0.7400\n",
            "Epoch 211/300\n",
            "\n",
            "Epoch 00211: val_loss did not improve from 0.47408\n",
            "620/620 - 1s - loss: 0.3843 - acc: 0.8065 - val_loss: 0.5136 - val_acc: 0.8000\n",
            "Epoch 212/300\n",
            "\n",
            "Epoch 00212: val_loss did not improve from 0.47408\n",
            "620/620 - 1s - loss: 0.3540 - acc: 0.8565 - val_loss: 0.5623 - val_acc: 0.7800\n",
            "Epoch 213/300\n",
            "\n",
            "Epoch 00213: val_loss did not improve from 0.47408\n",
            "620/620 - 1s - loss: 0.3423 - acc: 0.8371 - val_loss: 0.5821 - val_acc: 0.7500\n",
            "Epoch 214/300\n",
            "\n",
            "Epoch 00214: val_loss did not improve from 0.47408\n",
            "620/620 - 1s - loss: 0.3367 - acc: 0.8532 - val_loss: 0.5169 - val_acc: 0.7500\n",
            "Epoch 215/300\n",
            "\n",
            "Epoch 00215: val_loss did not improve from 0.47408\n",
            "620/620 - 1s - loss: 0.3236 - acc: 0.8645 - val_loss: 0.5916 - val_acc: 0.7500\n",
            "Epoch 216/300\n",
            "\n",
            "Epoch 00216: val_loss did not improve from 0.47408\n",
            "620/620 - 1s - loss: 0.3363 - acc: 0.8468 - val_loss: 0.5562 - val_acc: 0.7500\n",
            "Epoch 217/300\n",
            "\n",
            "Epoch 00217: val_loss did not improve from 0.47408\n",
            "620/620 - 1s - loss: 0.3272 - acc: 0.8629 - val_loss: 0.4981 - val_acc: 0.8000\n",
            "Epoch 218/300\n",
            "\n",
            "Epoch 00218: val_loss did not improve from 0.47408\n",
            "620/620 - 1s - loss: 0.3433 - acc: 0.8548 - val_loss: 0.5635 - val_acc: 0.7600\n",
            "Epoch 219/300\n",
            "\n",
            "Epoch 00219: val_loss did not improve from 0.47408\n",
            "620/620 - 1s - loss: 0.3541 - acc: 0.8419 - val_loss: 0.6414 - val_acc: 0.7000\n",
            "Epoch 220/300\n",
            "\n",
            "Epoch 00220: val_loss did not improve from 0.47408\n",
            "620/620 - 1s - loss: 0.3483 - acc: 0.8435 - val_loss: 0.5334 - val_acc: 0.7700\n",
            "Epoch 221/300\n",
            "\n",
            "Epoch 00221: val_loss did not improve from 0.47408\n",
            "620/620 - 1s - loss: 0.3486 - acc: 0.8629 - val_loss: 0.5379 - val_acc: 0.7700\n",
            "Epoch 222/300\n",
            "\n",
            "Epoch 00222: val_loss did not improve from 0.47408\n",
            "620/620 - 1s - loss: 0.3331 - acc: 0.8613 - val_loss: 0.5997 - val_acc: 0.7400\n",
            "Epoch 223/300\n",
            "\n",
            "Epoch 00223: val_loss did not improve from 0.47408\n",
            "620/620 - 1s - loss: 0.3588 - acc: 0.8419 - val_loss: 0.6455 - val_acc: 0.6900\n",
            "Epoch 224/300\n",
            "\n",
            "Epoch 00224: val_loss did not improve from 0.47408\n",
            "620/620 - 1s - loss: 0.3320 - acc: 0.8597 - val_loss: 0.6799 - val_acc: 0.6900\n",
            "Epoch 225/300\n",
            "\n",
            "Epoch 00225: val_loss did not improve from 0.47408\n",
            "620/620 - 1s - loss: 0.3407 - acc: 0.8500 - val_loss: 0.7608 - val_acc: 0.6400\n",
            "Epoch 226/300\n",
            "\n",
            "Epoch 00226: val_loss did not improve from 0.47408\n",
            "620/620 - 1s - loss: 0.3555 - acc: 0.8500 - val_loss: 0.5409 - val_acc: 0.7400\n",
            "Epoch 227/300\n",
            "\n",
            "Epoch 00227: val_loss did not improve from 0.47408\n",
            "620/620 - 1s - loss: 0.3403 - acc: 0.8516 - val_loss: 0.6877 - val_acc: 0.7100\n",
            "Epoch 228/300\n",
            "\n",
            "Epoch 00228: val_loss did not improve from 0.47408\n",
            "620/620 - 1s - loss: 0.3573 - acc: 0.8581 - val_loss: 0.5470 - val_acc: 0.7300\n",
            "Epoch 229/300\n",
            "\n",
            "Epoch 00229: val_loss did not improve from 0.47408\n",
            "620/620 - 1s - loss: 0.3581 - acc: 0.8113 - val_loss: 0.5171 - val_acc: 0.8000\n",
            "Epoch 230/300\n",
            "\n",
            "Epoch 00230: val_loss did not improve from 0.47408\n",
            "620/620 - 1s - loss: 0.3129 - acc: 0.8758 - val_loss: 0.5821 - val_acc: 0.7300\n",
            "Epoch 231/300\n",
            "\n",
            "Epoch 00231: val_loss did not improve from 0.47408\n",
            "620/620 - 1s - loss: 0.3370 - acc: 0.8532 - val_loss: 0.5460 - val_acc: 0.7700\n",
            "Epoch 232/300\n",
            "\n",
            "Epoch 00232: val_loss did not improve from 0.47408\n",
            "620/620 - 1s - loss: 0.3606 - acc: 0.8452 - val_loss: 0.4848 - val_acc: 0.8000\n",
            "Epoch 233/300\n",
            "\n",
            "Epoch 00233: val_loss did not improve from 0.47408\n",
            "620/620 - 1s - loss: 0.3214 - acc: 0.8645 - val_loss: 0.6187 - val_acc: 0.7200\n",
            "Epoch 234/300\n",
            "\n",
            "Epoch 00234: val_loss did not improve from 0.47408\n",
            "620/620 - 1s - loss: 0.3466 - acc: 0.8613 - val_loss: 0.5225 - val_acc: 0.7700\n",
            "Epoch 235/300\n",
            "\n",
            "Epoch 00235: val_loss did not improve from 0.47408\n",
            "620/620 - 1s - loss: 0.3571 - acc: 0.8435 - val_loss: 0.4966 - val_acc: 0.8000\n",
            "Epoch 236/300\n",
            "\n",
            "Epoch 00236: val_loss did not improve from 0.47408\n",
            "620/620 - 1s - loss: 0.3435 - acc: 0.8548 - val_loss: 0.5601 - val_acc: 0.7400\n",
            "Epoch 237/300\n",
            "\n",
            "Epoch 00237: val_loss did not improve from 0.47408\n",
            "620/620 - 1s - loss: 0.3413 - acc: 0.8548 - val_loss: 0.5878 - val_acc: 0.7700\n",
            "Epoch 238/300\n",
            "\n",
            "Epoch 00238: val_loss did not improve from 0.47408\n",
            "620/620 - 1s - loss: 0.3356 - acc: 0.8565 - val_loss: 0.5343 - val_acc: 0.7700\n",
            "Epoch 239/300\n",
            "\n",
            "Epoch 00239: val_loss did not improve from 0.47408\n",
            "620/620 - 1s - loss: 0.3381 - acc: 0.8516 - val_loss: 0.4862 - val_acc: 0.8200\n",
            "Epoch 240/300\n",
            "\n",
            "Epoch 00240: val_loss did not improve from 0.47408\n",
            "620/620 - 1s - loss: 0.3547 - acc: 0.8435 - val_loss: 0.5690 - val_acc: 0.7200\n",
            "Epoch 241/300\n",
            "\n",
            "Epoch 00241: val_loss did not improve from 0.47408\n",
            "620/620 - 1s - loss: 0.3821 - acc: 0.8290 - val_loss: 0.6029 - val_acc: 0.6800\n",
            "Epoch 242/300\n",
            "\n",
            "Epoch 00242: val_loss did not improve from 0.47408\n",
            "620/620 - 1s - loss: 0.3658 - acc: 0.8371 - val_loss: 0.6139 - val_acc: 0.7500\n",
            "Epoch 243/300\n",
            "\n",
            "Epoch 00243: val_loss did not improve from 0.47408\n",
            "620/620 - 1s - loss: 0.3522 - acc: 0.8565 - val_loss: 0.5739 - val_acc: 0.7300\n",
            "Epoch 244/300\n",
            "\n",
            "Epoch 00244: val_loss did not improve from 0.47408\n",
            "620/620 - 1s - loss: 0.3567 - acc: 0.8435 - val_loss: 0.5547 - val_acc: 0.7600\n",
            "Epoch 245/300\n",
            "\n",
            "Epoch 00245: val_loss did not improve from 0.47408\n",
            "620/620 - 1s - loss: 0.3716 - acc: 0.8419 - val_loss: 0.5797 - val_acc: 0.7500\n",
            "Epoch 246/300\n",
            "\n",
            "Epoch 00246: val_loss did not improve from 0.47408\n",
            "620/620 - 1s - loss: 0.3402 - acc: 0.8532 - val_loss: 0.5096 - val_acc: 0.8000\n",
            "Epoch 247/300\n",
            "\n",
            "Epoch 00247: val_loss did not improve from 0.47408\n",
            "620/620 - 1s - loss: 0.3403 - acc: 0.8516 - val_loss: 0.6538 - val_acc: 0.7100\n",
            "Epoch 248/300\n",
            "\n",
            "Epoch 00248: val_loss did not improve from 0.47408\n",
            "620/620 - 1s - loss: 0.3277 - acc: 0.8645 - val_loss: 0.5475 - val_acc: 0.7600\n",
            "Epoch 249/300\n",
            "\n",
            "Epoch 00249: val_loss did not improve from 0.47408\n",
            "620/620 - 1s - loss: 0.3441 - acc: 0.8500 - val_loss: 0.6187 - val_acc: 0.7100\n",
            "Epoch 250/300\n",
            "\n",
            "Epoch 00250: val_loss did not improve from 0.47408\n",
            "620/620 - 1s - loss: 0.3381 - acc: 0.8548 - val_loss: 0.5981 - val_acc: 0.7200\n",
            "Epoch 251/300\n",
            "\n",
            "Epoch 00251: val_loss did not improve from 0.47408\n",
            "620/620 - 1s - loss: 0.3218 - acc: 0.8710 - val_loss: 0.5429 - val_acc: 0.7700\n",
            "Epoch 252/300\n",
            "\n",
            "Epoch 00252: val_loss did not improve from 0.47408\n",
            "620/620 - 1s - loss: 0.3388 - acc: 0.8452 - val_loss: 0.5137 - val_acc: 0.8000\n",
            "Epoch 253/300\n",
            "\n",
            "Epoch 00253: val_loss did not improve from 0.47408\n",
            "620/620 - 1s - loss: 0.3130 - acc: 0.8871 - val_loss: 0.5228 - val_acc: 0.7800\n",
            "Epoch 254/300\n",
            "\n",
            "Epoch 00254: val_loss did not improve from 0.47408\n",
            "620/620 - 1s - loss: 0.3737 - acc: 0.8387 - val_loss: 0.5704 - val_acc: 0.7400\n",
            "Epoch 255/300\n",
            "\n",
            "Epoch 00255: val_loss did not improve from 0.47408\n",
            "620/620 - 1s - loss: 0.3484 - acc: 0.8468 - val_loss: 0.5714 - val_acc: 0.7500\n",
            "Epoch 256/300\n",
            "\n",
            "Epoch 00256: val_loss did not improve from 0.47408\n",
            "620/620 - 1s - loss: 0.3150 - acc: 0.8694 - val_loss: 0.5378 - val_acc: 0.7800\n",
            "Epoch 257/300\n",
            "\n",
            "Epoch 00257: val_loss did not improve from 0.47408\n",
            "620/620 - 1s - loss: 0.3476 - acc: 0.8387 - val_loss: 0.5651 - val_acc: 0.7800\n",
            "Epoch 258/300\n",
            "\n",
            "Epoch 00258: val_loss did not improve from 0.47408\n",
            "620/620 - 1s - loss: 0.3647 - acc: 0.8403 - val_loss: 0.5107 - val_acc: 0.8200\n",
            "Epoch 259/300\n",
            "\n",
            "Epoch 00259: val_loss did not improve from 0.47408\n",
            "620/620 - 1s - loss: 0.3213 - acc: 0.8710 - val_loss: 0.7166 - val_acc: 0.6800\n",
            "Epoch 260/300\n",
            "\n",
            "Epoch 00260: val_loss did not improve from 0.47408\n",
            "620/620 - 1s - loss: 0.3595 - acc: 0.8516 - val_loss: 0.5852 - val_acc: 0.7300\n",
            "Epoch 261/300\n",
            "\n",
            "Epoch 00261: val_loss did not improve from 0.47408\n",
            "620/620 - 1s - loss: 0.3434 - acc: 0.8419 - val_loss: 0.6799 - val_acc: 0.6900\n",
            "Epoch 262/300\n",
            "\n",
            "Epoch 00262: val_loss did not improve from 0.47408\n",
            "620/620 - 1s - loss: 0.3802 - acc: 0.8274 - val_loss: 0.6136 - val_acc: 0.7400\n",
            "Epoch 263/300\n",
            "\n",
            "Epoch 00263: val_loss did not improve from 0.47408\n",
            "620/620 - 1s - loss: 0.3434 - acc: 0.8500 - val_loss: 0.7312 - val_acc: 0.6800\n",
            "Epoch 264/300\n",
            "\n",
            "Epoch 00264: val_loss did not improve from 0.47408\n",
            "620/620 - 1s - loss: 0.3305 - acc: 0.8419 - val_loss: 0.5402 - val_acc: 0.7900\n",
            "Epoch 265/300\n",
            "\n",
            "Epoch 00265: val_loss did not improve from 0.47408\n",
            "620/620 - 1s - loss: 0.3436 - acc: 0.8403 - val_loss: 0.5279 - val_acc: 0.7800\n",
            "Epoch 266/300\n",
            "\n",
            "Epoch 00266: val_loss did not improve from 0.47408\n",
            "620/620 - 1s - loss: 0.3427 - acc: 0.8581 - val_loss: 0.4832 - val_acc: 0.8100\n",
            "Epoch 267/300\n",
            "\n",
            "Epoch 00267: val_loss did not improve from 0.47408\n",
            "620/620 - 1s - loss: 0.3654 - acc: 0.8419 - val_loss: 0.5211 - val_acc: 0.7600\n",
            "Epoch 268/300\n",
            "\n",
            "Epoch 00268: val_loss did not improve from 0.47408\n",
            "620/620 - 1s - loss: 0.3642 - acc: 0.8226 - val_loss: 0.5435 - val_acc: 0.7800\n",
            "Epoch 269/300\n",
            "\n",
            "Epoch 00269: val_loss did not improve from 0.47408\n",
            "620/620 - 1s - loss: 0.3480 - acc: 0.8371 - val_loss: 0.5549 - val_acc: 0.7800\n",
            "Epoch 270/300\n",
            "\n",
            "Epoch 00270: val_loss did not improve from 0.47408\n",
            "620/620 - 1s - loss: 0.3306 - acc: 0.8629 - val_loss: 0.5954 - val_acc: 0.7300\n",
            "Epoch 271/300\n",
            "\n",
            "Epoch 00271: val_loss did not improve from 0.47408\n",
            "620/620 - 1s - loss: 0.3511 - acc: 0.8581 - val_loss: 0.6153 - val_acc: 0.7200\n",
            "Epoch 272/300\n",
            "\n",
            "Epoch 00272: val_loss did not improve from 0.47408\n",
            "620/620 - 1s - loss: 0.3426 - acc: 0.8548 - val_loss: 0.6412 - val_acc: 0.7100\n",
            "Epoch 273/300\n",
            "\n",
            "Epoch 00273: val_loss did not improve from 0.47408\n",
            "620/620 - 1s - loss: 0.3246 - acc: 0.8548 - val_loss: 0.5027 - val_acc: 0.7600\n",
            "Epoch 274/300\n",
            "\n",
            "Epoch 00274: val_loss did not improve from 0.47408\n",
            "620/620 - 1s - loss: 0.3528 - acc: 0.8339 - val_loss: 0.5715 - val_acc: 0.7600\n",
            "Epoch 275/300\n",
            "\n",
            "Epoch 00275: val_loss did not improve from 0.47408\n",
            "620/620 - 1s - loss: 0.3546 - acc: 0.8371 - val_loss: 0.6257 - val_acc: 0.7100\n",
            "Epoch 276/300\n",
            "\n",
            "Epoch 00276: val_loss did not improve from 0.47408\n",
            "620/620 - 1s - loss: 0.3156 - acc: 0.8871 - val_loss: 0.6374 - val_acc: 0.7100\n",
            "Epoch 277/300\n",
            "\n",
            "Epoch 00277: val_loss did not improve from 0.47408\n",
            "620/620 - 1s - loss: 0.3345 - acc: 0.8694 - val_loss: 0.5528 - val_acc: 0.7800\n",
            "Epoch 278/300\n",
            "\n",
            "Epoch 00278: val_loss did not improve from 0.47408\n",
            "620/620 - 1s - loss: 0.3508 - acc: 0.8371 - val_loss: 0.4865 - val_acc: 0.8100\n",
            "Epoch 279/300\n",
            "\n",
            "Epoch 00279: val_loss did not improve from 0.47408\n",
            "620/620 - 1s - loss: 0.3363 - acc: 0.8484 - val_loss: 0.5128 - val_acc: 0.8000\n",
            "Epoch 280/300\n",
            "\n",
            "Epoch 00280: val_loss did not improve from 0.47408\n",
            "620/620 - 1s - loss: 0.3420 - acc: 0.8403 - val_loss: 0.4945 - val_acc: 0.7700\n",
            "Epoch 281/300\n",
            "\n",
            "Epoch 00281: val_loss did not improve from 0.47408\n",
            "620/620 - 1s - loss: 0.3279 - acc: 0.8613 - val_loss: 0.4966 - val_acc: 0.7900\n",
            "Epoch 282/300\n",
            "\n",
            "Epoch 00282: val_loss did not improve from 0.47408\n",
            "620/620 - 1s - loss: 0.3219 - acc: 0.8403 - val_loss: 0.5318 - val_acc: 0.7800\n",
            "Epoch 283/300\n",
            "\n",
            "Epoch 00283: val_loss did not improve from 0.47408\n",
            "620/620 - 1s - loss: 0.3119 - acc: 0.8710 - val_loss: 0.5647 - val_acc: 0.7600\n",
            "Epoch 284/300\n",
            "\n",
            "Epoch 00284: val_loss did not improve from 0.47408\n",
            "620/620 - 1s - loss: 0.3392 - acc: 0.8339 - val_loss: 0.5009 - val_acc: 0.8200\n",
            "Epoch 285/300\n",
            "\n",
            "Epoch 00285: val_loss did not improve from 0.47408\n",
            "620/620 - 1s - loss: 0.3546 - acc: 0.8435 - val_loss: 0.5499 - val_acc: 0.7600\n",
            "Epoch 286/300\n",
            "\n",
            "Epoch 00286: val_loss did not improve from 0.47408\n",
            "620/620 - 1s - loss: 0.3403 - acc: 0.8468 - val_loss: 0.6360 - val_acc: 0.7000\n",
            "Epoch 287/300\n",
            "\n",
            "Epoch 00287: val_loss did not improve from 0.47408\n",
            "620/620 - 1s - loss: 0.3694 - acc: 0.8452 - val_loss: 0.6859 - val_acc: 0.6700\n",
            "Epoch 288/300\n",
            "\n",
            "Epoch 00288: val_loss did not improve from 0.47408\n",
            "620/620 - 1s - loss: 0.3450 - acc: 0.8371 - val_loss: 0.5426 - val_acc: 0.7800\n",
            "Epoch 289/300\n",
            "\n",
            "Epoch 00289: val_loss did not improve from 0.47408\n",
            "620/620 - 1s - loss: 0.3300 - acc: 0.8435 - val_loss: 0.5489 - val_acc: 0.8000\n",
            "Epoch 290/300\n",
            "\n",
            "Epoch 00290: val_loss did not improve from 0.47408\n",
            "620/620 - 1s - loss: 0.3197 - acc: 0.8645 - val_loss: 0.5456 - val_acc: 0.7700\n",
            "Epoch 291/300\n",
            "\n",
            "Epoch 00291: val_loss did not improve from 0.47408\n",
            "620/620 - 1s - loss: 0.3204 - acc: 0.8500 - val_loss: 0.8097 - val_acc: 0.6500\n",
            "Epoch 292/300\n",
            "\n",
            "Epoch 00292: val_loss did not improve from 0.47408\n",
            "620/620 - 1s - loss: 0.3342 - acc: 0.8548 - val_loss: 0.5477 - val_acc: 0.7800\n",
            "Epoch 293/300\n",
            "\n",
            "Epoch 00293: val_loss did not improve from 0.47408\n",
            "620/620 - 1s - loss: 0.3778 - acc: 0.8210 - val_loss: 0.5254 - val_acc: 0.7700\n",
            "Epoch 294/300\n",
            "\n",
            "Epoch 00294: val_loss did not improve from 0.47408\n",
            "620/620 - 1s - loss: 0.3652 - acc: 0.8452 - val_loss: 0.5980 - val_acc: 0.7300\n",
            "Epoch 295/300\n",
            "\n",
            "Epoch 00295: val_loss did not improve from 0.47408\n",
            "620/620 - 1s - loss: 0.3324 - acc: 0.8532 - val_loss: 0.5751 - val_acc: 0.7400\n",
            "Epoch 296/300\n",
            "\n",
            "Epoch 00296: val_loss did not improve from 0.47408\n",
            "620/620 - 1s - loss: 0.3222 - acc: 0.8532 - val_loss: 0.6348 - val_acc: 0.7100\n",
            "Epoch 297/300\n",
            "\n",
            "Epoch 00297: val_loss did not improve from 0.47408\n",
            "620/620 - 1s - loss: 0.3148 - acc: 0.8645 - val_loss: 0.5128 - val_acc: 0.8300\n",
            "Epoch 298/300\n",
            "\n",
            "Epoch 00298: val_loss did not improve from 0.47408\n",
            "620/620 - 1s - loss: 0.3436 - acc: 0.8339 - val_loss: 0.6392 - val_acc: 0.7100\n",
            "Epoch 299/300\n",
            "\n",
            "Epoch 00299: val_loss did not improve from 0.47408\n",
            "620/620 - 1s - loss: 0.3134 - acc: 0.8516 - val_loss: 0.5715 - val_acc: 0.7800\n",
            "Epoch 300/300\n",
            "\n",
            "Epoch 00300: val_loss did not improve from 0.47408\n",
            "620/620 - 1s - loss: 0.3514 - acc: 0.8323 - val_loss: 0.5912 - val_acc: 0.7200\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nURRFCaICQlEURQny/wFUSaCahmcXlQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "# Evaluate on test data\n",
            "40/40 [==============================] - 0s 4ms/sample - loss: 0.6186 - acc: 0.6750\n",
            "test loss, test acc: [0.6186127429187763, 0.675]\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:107: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:111: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "1\n",
            "EEG_Deep/Data2A/parsed_P01T.mat\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(0, 12, 4096)\n",
            "(80, 12, 4096)\n",
            "(80, 1)\n",
            "2\n",
            "EEG_Deep/Data2A/parsed_P02T.mat\n",
            "(80, 12, 4096)\n",
            "(160, 12, 4096)\n",
            "(160, 1)\n",
            "3\n",
            "EEG_Deep/Data2A/parsed_P03T.mat\n",
            "(160, 12, 4096)\n",
            "(240, 12, 4096)\n",
            "(240, 1)\n",
            "4\n",
            "EEG_Deep/Data2A/parsed_P04T.mat\n",
            "(240, 12, 4096)\n",
            "(320, 12, 4096)\n",
            "(320, 1)\n",
            "5\n",
            "EEG_Deep/Data2A/parsed_P05T.mat\n",
            "(320, 12, 4096)\n",
            "(400, 12, 4096)\n",
            "(400, 1)\n",
            "6\n",
            "EEG_Deep/Data2A/parsed_P06T.mat\n",
            "(400, 12, 4096)\n",
            "(480, 12, 4096)\n",
            "(480, 1)\n",
            "7\n",
            "EEG_Deep/Data2A/parsed_P07T.mat\n",
            "(480, 12, 4096)\n",
            "(560, 12, 4096)\n",
            "(560, 1)\n",
            "8\n",
            "EEG_Deep/Data2A/parsed_P08T.mat\n",
            "(560, 12, 4096)\n",
            "(640, 12, 4096)\n",
            "(640, 1)\n",
            "9\n",
            "EEG_Deep/Data2A/parsed_P09T.mat\n",
            "(640, 12, 4096)\n",
            "(720, 12, 4096)\n",
            "(720, 1)\n",
            "10\n",
            "EEG_Deep/Data2A/parsed_P010E.mat\n",
            "(0, 12, 4096)\n",
            "(40, 12, 4096)\n",
            "(40, 1)\n",
            "Filtering of training data in progress\n",
            "(720, 12, 4096)\n",
            "Filtering of test data in progress\n",
            "(40, 12, 4096)\n",
            "[1. 2. 2. 1. 1. 1. 2. 2. 1. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 1.\n",
            " 1. 1. 2. 2. 2. 2. 2. 1. 1. 1. 2. 2. 1. 2. 1. 1. 1. 1. 2. 1. 1. 2. 2. 1.\n",
            " 1. 1. 1. 2. 2. 1. 1. 2. 2. 1. 1. 2. 1. 2. 1. 1. 2. 2. 1. 2. 2. 1. 1. 1.\n",
            " 1. 1. 1. 2. 2. 2. 2. 1. 1. 2. 2. 1. 2. 1. 1. 1. 2. 2. 1. 1. 2. 1. 1. 2.\n",
            " 2. 1. 2. 1.]\n",
            "(620, 12, 1536)\n",
            "(620,)\n",
            "(100, 12, 1536)\n",
            "(100,)\n",
            "[[1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]]\n",
            "X_train shape: (620, 1, 12, 1536)\n",
            "620 train samples\n",
            "100 val samples\n",
            "(40, 12, 1536)\n",
            "(40, 1)\n",
            "X_train shape: (40, 1, 12, 1536)\n",
            "40 train samples\n",
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 1, 12, 1536)]     0         \n",
            "_________________________________________________________________\n",
            "conv2d (Conv2D)              (None, 8, 12, 1536)       200       \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 8, 12, 1536)       32        \n",
            "_________________________________________________________________\n",
            "depthwise_conv2d (DepthwiseC (None, 16, 1, 1536)       192       \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 16, 1, 1536)       64        \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 16, 1, 1536)       0         \n",
            "_________________________________________________________________\n",
            "average_pooling2d (AveragePo (None, 16, 1, 384)        0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 16, 1, 384)        0         \n",
            "_________________________________________________________________\n",
            "separable_conv2d (SeparableC (None, 16, 1, 384)        512       \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 16, 1, 384)        64        \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 16, 1, 384)        0         \n",
            "_________________________________________________________________\n",
            "average_pooling2d_1 (Average (None, 16, 1, 48)         0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 16, 1, 48)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 768)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 2)                 1538      \n",
            "_________________________________________________________________\n",
            "softmax (Activation)         (None, 2)                 0         \n",
            "=================================================================\n",
            "Total params: 2,602\n",
            "Trainable params: 2,522\n",
            "Non-trainable params: 80\n",
            "_________________________________________________________________\n",
            "Train on 620 samples, validate on 100 samples\n",
            "Epoch 1/300\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.69289, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.6983 - acc: 0.5032 - val_loss: 0.6929 - val_acc: 0.5400\n",
            "Epoch 2/300\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.69289 to 0.69119, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.6737 - acc: 0.5935 - val_loss: 0.6912 - val_acc: 0.5100\n",
            "Epoch 3/300\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.69119\n",
            "620/620 - 1s - loss: 0.6467 - acc: 0.6419 - val_loss: 0.6952 - val_acc: 0.4800\n",
            "Epoch 4/300\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.69119\n",
            "620/620 - 1s - loss: 0.6104 - acc: 0.7000 - val_loss: 0.7366 - val_acc: 0.4700\n",
            "Epoch 5/300\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.69119\n",
            "620/620 - 1s - loss: 0.5513 - acc: 0.7419 - val_loss: 0.8187 - val_acc: 0.4700\n",
            "Epoch 6/300\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.69119\n",
            "620/620 - 1s - loss: 0.5295 - acc: 0.7516 - val_loss: 0.8593 - val_acc: 0.4700\n",
            "Epoch 7/300\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.69119\n",
            "620/620 - 1s - loss: 0.5051 - acc: 0.7742 - val_loss: 0.7667 - val_acc: 0.5100\n",
            "Epoch 8/300\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.69119\n",
            "620/620 - 1s - loss: 0.5032 - acc: 0.7629 - val_loss: 0.7839 - val_acc: 0.5100\n",
            "Epoch 9/300\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.69119\n",
            "620/620 - 1s - loss: 0.4987 - acc: 0.7339 - val_loss: 0.7187 - val_acc: 0.6000\n",
            "Epoch 10/300\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.69119\n",
            "620/620 - 1s - loss: 0.4841 - acc: 0.7790 - val_loss: 0.7189 - val_acc: 0.6200\n",
            "Epoch 11/300\n",
            "\n",
            "Epoch 00011: val_loss improved from 0.69119 to 0.68432, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.4765 - acc: 0.7806 - val_loss: 0.6843 - val_acc: 0.6400\n",
            "Epoch 12/300\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.68432\n",
            "620/620 - 1s - loss: 0.4968 - acc: 0.7484 - val_loss: 0.6909 - val_acc: 0.6500\n",
            "Epoch 13/300\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.68432\n",
            "620/620 - 1s - loss: 0.4692 - acc: 0.7935 - val_loss: 0.6899 - val_acc: 0.6600\n",
            "Epoch 14/300\n",
            "\n",
            "Epoch 00014: val_loss improved from 0.68432 to 0.62943, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.4609 - acc: 0.7742 - val_loss: 0.6294 - val_acc: 0.7000\n",
            "Epoch 15/300\n",
            "\n",
            "Epoch 00015: val_loss improved from 0.62943 to 0.59970, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.4643 - acc: 0.7661 - val_loss: 0.5997 - val_acc: 0.7100\n",
            "Epoch 16/300\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.59970\n",
            "620/620 - 1s - loss: 0.4706 - acc: 0.7661 - val_loss: 0.6437 - val_acc: 0.6900\n",
            "Epoch 17/300\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.59970\n",
            "620/620 - 1s - loss: 0.4791 - acc: 0.7629 - val_loss: 0.6362 - val_acc: 0.7100\n",
            "Epoch 18/300\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.59970\n",
            "620/620 - 1s - loss: 0.4815 - acc: 0.7742 - val_loss: 0.6261 - val_acc: 0.7000\n",
            "Epoch 19/300\n",
            "\n",
            "Epoch 00019: val_loss improved from 0.59970 to 0.59802, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.4773 - acc: 0.7871 - val_loss: 0.5980 - val_acc: 0.7200\n",
            "Epoch 20/300\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.59802\n",
            "620/620 - 1s - loss: 0.4479 - acc: 0.7919 - val_loss: 0.6463 - val_acc: 0.6800\n",
            "Epoch 21/300\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.59802\n",
            "620/620 - 1s - loss: 0.4579 - acc: 0.7871 - val_loss: 0.6063 - val_acc: 0.6900\n",
            "Epoch 22/300\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.59802\n",
            "620/620 - 1s - loss: 0.4614 - acc: 0.7823 - val_loss: 0.6850 - val_acc: 0.6800\n",
            "Epoch 23/300\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.59802\n",
            "620/620 - 1s - loss: 0.4634 - acc: 0.7694 - val_loss: 0.6322 - val_acc: 0.7000\n",
            "Epoch 24/300\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.59802\n",
            "620/620 - 1s - loss: 0.4525 - acc: 0.7871 - val_loss: 0.6012 - val_acc: 0.7000\n",
            "Epoch 25/300\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.59802\n",
            "620/620 - 1s - loss: 0.4811 - acc: 0.7952 - val_loss: 0.6054 - val_acc: 0.7400\n",
            "Epoch 26/300\n",
            "\n",
            "Epoch 00026: val_loss improved from 0.59802 to 0.55104, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.4587 - acc: 0.7952 - val_loss: 0.5510 - val_acc: 0.7300\n",
            "Epoch 27/300\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.55104\n",
            "620/620 - 1s - loss: 0.4540 - acc: 0.7806 - val_loss: 0.6148 - val_acc: 0.7200\n",
            "Epoch 28/300\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.55104\n",
            "620/620 - 1s - loss: 0.4551 - acc: 0.7677 - val_loss: 0.6331 - val_acc: 0.7100\n",
            "Epoch 29/300\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.55104\n",
            "620/620 - 1s - loss: 0.4699 - acc: 0.7952 - val_loss: 0.6689 - val_acc: 0.7100\n",
            "Epoch 30/300\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.55104\n",
            "620/620 - 1s - loss: 0.4542 - acc: 0.7855 - val_loss: 0.6130 - val_acc: 0.7400\n",
            "Epoch 31/300\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.55104\n",
            "620/620 - 1s - loss: 0.4410 - acc: 0.7968 - val_loss: 0.6454 - val_acc: 0.7100\n",
            "Epoch 32/300\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.55104\n",
            "620/620 - 1s - loss: 0.4287 - acc: 0.8032 - val_loss: 0.7126 - val_acc: 0.6800\n",
            "Epoch 33/300\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.55104\n",
            "620/620 - 1s - loss: 0.4445 - acc: 0.7806 - val_loss: 0.6831 - val_acc: 0.6900\n",
            "Epoch 34/300\n",
            "\n",
            "Epoch 00034: val_loss improved from 0.55104 to 0.54793, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.4456 - acc: 0.7790 - val_loss: 0.5479 - val_acc: 0.7500\n",
            "Epoch 35/300\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.54793\n",
            "620/620 - 1s - loss: 0.4455 - acc: 0.7935 - val_loss: 0.5962 - val_acc: 0.7000\n",
            "Epoch 36/300\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.54793\n",
            "620/620 - 1s - loss: 0.4394 - acc: 0.7806 - val_loss: 0.6133 - val_acc: 0.7200\n",
            "Epoch 37/300\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.54793\n",
            "620/620 - 1s - loss: 0.4370 - acc: 0.7871 - val_loss: 0.6625 - val_acc: 0.6600\n",
            "Epoch 38/300\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.54793\n",
            "620/620 - 1s - loss: 0.4489 - acc: 0.7952 - val_loss: 0.6867 - val_acc: 0.6900\n",
            "Epoch 39/300\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.54793\n",
            "620/620 - 1s - loss: 0.4456 - acc: 0.7935 - val_loss: 0.6746 - val_acc: 0.6800\n",
            "Epoch 40/300\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.54793\n",
            "620/620 - 1s - loss: 0.4076 - acc: 0.8339 - val_loss: 0.6318 - val_acc: 0.7200\n",
            "Epoch 41/300\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.54793\n",
            "620/620 - 1s - loss: 0.4222 - acc: 0.8016 - val_loss: 0.5782 - val_acc: 0.7400\n",
            "Epoch 42/300\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.54793\n",
            "620/620 - 1s - loss: 0.4600 - acc: 0.7952 - val_loss: 0.6296 - val_acc: 0.7000\n",
            "Epoch 43/300\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.54793\n",
            "620/620 - 1s - loss: 0.4505 - acc: 0.7952 - val_loss: 0.5959 - val_acc: 0.7000\n",
            "Epoch 44/300\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.54793\n",
            "620/620 - 1s - loss: 0.4387 - acc: 0.7887 - val_loss: 0.6331 - val_acc: 0.7000\n",
            "Epoch 45/300\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.54793\n",
            "620/620 - 1s - loss: 0.4274 - acc: 0.8145 - val_loss: 0.7748 - val_acc: 0.6600\n",
            "Epoch 46/300\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.54793\n",
            "620/620 - 1s - loss: 0.4420 - acc: 0.7823 - val_loss: 0.6882 - val_acc: 0.7000\n",
            "Epoch 47/300\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.54793\n",
            "620/620 - 1s - loss: 0.3973 - acc: 0.8371 - val_loss: 0.6049 - val_acc: 0.7500\n",
            "Epoch 48/300\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.54793\n",
            "620/620 - 1s - loss: 0.4185 - acc: 0.8048 - val_loss: 0.6489 - val_acc: 0.7000\n",
            "Epoch 49/300\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.54793\n",
            "620/620 - 1s - loss: 0.4260 - acc: 0.8016 - val_loss: 0.5906 - val_acc: 0.7200\n",
            "Epoch 50/300\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.54793\n",
            "620/620 - 1s - loss: 0.4442 - acc: 0.7952 - val_loss: 0.6376 - val_acc: 0.7200\n",
            "Epoch 51/300\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.54793\n",
            "620/620 - 1s - loss: 0.4216 - acc: 0.7935 - val_loss: 0.6245 - val_acc: 0.6800\n",
            "Epoch 52/300\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.54793\n",
            "620/620 - 1s - loss: 0.4214 - acc: 0.8065 - val_loss: 0.6422 - val_acc: 0.6900\n",
            "Epoch 53/300\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.54793\n",
            "620/620 - 1s - loss: 0.4221 - acc: 0.8032 - val_loss: 0.6356 - val_acc: 0.7400\n",
            "Epoch 54/300\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.54793\n",
            "620/620 - 1s - loss: 0.4239 - acc: 0.8145 - val_loss: 0.7009 - val_acc: 0.6900\n",
            "Epoch 55/300\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.54793\n",
            "620/620 - 1s - loss: 0.4404 - acc: 0.7887 - val_loss: 0.7044 - val_acc: 0.6700\n",
            "Epoch 56/300\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.54793\n",
            "620/620 - 1s - loss: 0.4353 - acc: 0.7871 - val_loss: 0.6189 - val_acc: 0.7000\n",
            "Epoch 57/300\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.54793\n",
            "620/620 - 1s - loss: 0.4091 - acc: 0.8081 - val_loss: 0.6288 - val_acc: 0.7000\n",
            "Epoch 58/300\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.54793\n",
            "620/620 - 1s - loss: 0.4221 - acc: 0.8065 - val_loss: 0.5631 - val_acc: 0.7200\n",
            "Epoch 59/300\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.54793\n",
            "620/620 - 1s - loss: 0.4175 - acc: 0.8129 - val_loss: 0.6149 - val_acc: 0.7400\n",
            "Epoch 60/300\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.54793\n",
            "620/620 - 1s - loss: 0.4364 - acc: 0.7855 - val_loss: 0.5856 - val_acc: 0.7300\n",
            "Epoch 61/300\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.54793\n",
            "620/620 - 1s - loss: 0.4295 - acc: 0.8129 - val_loss: 0.6226 - val_acc: 0.7000\n",
            "Epoch 62/300\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.54793\n",
            "620/620 - 1s - loss: 0.4131 - acc: 0.8258 - val_loss: 0.6296 - val_acc: 0.7100\n",
            "Epoch 63/300\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.54793\n",
            "620/620 - 1s - loss: 0.4101 - acc: 0.8145 - val_loss: 0.6724 - val_acc: 0.6700\n",
            "Epoch 64/300\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.54793\n",
            "620/620 - 1s - loss: 0.4162 - acc: 0.8065 - val_loss: 0.6383 - val_acc: 0.7200\n",
            "Epoch 65/300\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.54793\n",
            "620/620 - 1s - loss: 0.4363 - acc: 0.8258 - val_loss: 0.5786 - val_acc: 0.7100\n",
            "Epoch 66/300\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.54793\n",
            "620/620 - 1s - loss: 0.3916 - acc: 0.8290 - val_loss: 0.6466 - val_acc: 0.6700\n",
            "Epoch 67/300\n",
            "\n",
            "Epoch 00067: val_loss improved from 0.54793 to 0.53636, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.4105 - acc: 0.8242 - val_loss: 0.5364 - val_acc: 0.7500\n",
            "Epoch 68/300\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.53636\n",
            "620/620 - 1s - loss: 0.4070 - acc: 0.8274 - val_loss: 0.6001 - val_acc: 0.7200\n",
            "Epoch 69/300\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.53636\n",
            "620/620 - 1s - loss: 0.4385 - acc: 0.7952 - val_loss: 0.5739 - val_acc: 0.7100\n",
            "Epoch 70/300\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.53636\n",
            "620/620 - 1s - loss: 0.4342 - acc: 0.7968 - val_loss: 0.6162 - val_acc: 0.6800\n",
            "Epoch 71/300\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.53636\n",
            "620/620 - 1s - loss: 0.4089 - acc: 0.8016 - val_loss: 0.6111 - val_acc: 0.7000\n",
            "Epoch 72/300\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.53636\n",
            "620/620 - 1s - loss: 0.3918 - acc: 0.8145 - val_loss: 0.6192 - val_acc: 0.7400\n",
            "Epoch 73/300\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.53636\n",
            "620/620 - 1s - loss: 0.4042 - acc: 0.8097 - val_loss: 0.5713 - val_acc: 0.7300\n",
            "Epoch 74/300\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.53636\n",
            "620/620 - 1s - loss: 0.3840 - acc: 0.8177 - val_loss: 0.6982 - val_acc: 0.6900\n",
            "Epoch 75/300\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.53636\n",
            "620/620 - 1s - loss: 0.3859 - acc: 0.8403 - val_loss: 0.5680 - val_acc: 0.7700\n",
            "Epoch 76/300\n",
            "\n",
            "Epoch 00076: val_loss improved from 0.53636 to 0.52656, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.3979 - acc: 0.8323 - val_loss: 0.5266 - val_acc: 0.7500\n",
            "Epoch 77/300\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3979 - acc: 0.8323 - val_loss: 0.7116 - val_acc: 0.6900\n",
            "Epoch 78/300\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3993 - acc: 0.8145 - val_loss: 0.5781 - val_acc: 0.7100\n",
            "Epoch 79/300\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3967 - acc: 0.8081 - val_loss: 0.6370 - val_acc: 0.7000\n",
            "Epoch 80/300\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.4069 - acc: 0.8194 - val_loss: 0.5414 - val_acc: 0.7400\n",
            "Epoch 81/300\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.4063 - acc: 0.8048 - val_loss: 0.5493 - val_acc: 0.7400\n",
            "Epoch 82/300\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.4103 - acc: 0.7968 - val_loss: 0.5877 - val_acc: 0.6900\n",
            "Epoch 83/300\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.4184 - acc: 0.8242 - val_loss: 0.6139 - val_acc: 0.7400\n",
            "Epoch 84/300\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.4076 - acc: 0.8081 - val_loss: 0.5848 - val_acc: 0.7200\n",
            "Epoch 85/300\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3973 - acc: 0.8339 - val_loss: 0.6566 - val_acc: 0.7000\n",
            "Epoch 86/300\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.4097 - acc: 0.8161 - val_loss: 0.5685 - val_acc: 0.7000\n",
            "Epoch 87/300\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.4034 - acc: 0.8339 - val_loss: 0.6372 - val_acc: 0.7200\n",
            "Epoch 88/300\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3830 - acc: 0.8306 - val_loss: 0.6898 - val_acc: 0.7000\n",
            "Epoch 89/300\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3907 - acc: 0.8355 - val_loss: 0.6154 - val_acc: 0.7100\n",
            "Epoch 90/300\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3849 - acc: 0.8387 - val_loss: 0.6546 - val_acc: 0.7300\n",
            "Epoch 91/300\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.4003 - acc: 0.8177 - val_loss: 0.7199 - val_acc: 0.7400\n",
            "Epoch 92/300\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.4051 - acc: 0.8177 - val_loss: 0.7039 - val_acc: 0.6700\n",
            "Epoch 93/300\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3990 - acc: 0.8290 - val_loss: 0.6233 - val_acc: 0.7200\n",
            "Epoch 94/300\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3986 - acc: 0.8177 - val_loss: 0.5786 - val_acc: 0.7400\n",
            "Epoch 95/300\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3989 - acc: 0.8194 - val_loss: 0.5984 - val_acc: 0.7100\n",
            "Epoch 96/300\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3952 - acc: 0.8258 - val_loss: 0.6731 - val_acc: 0.7000\n",
            "Epoch 97/300\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.4154 - acc: 0.8242 - val_loss: 0.6774 - val_acc: 0.7200\n",
            "Epoch 98/300\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3974 - acc: 0.8403 - val_loss: 0.5932 - val_acc: 0.7500\n",
            "Epoch 99/300\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3764 - acc: 0.8355 - val_loss: 0.6226 - val_acc: 0.7100\n",
            "Epoch 100/300\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3818 - acc: 0.8194 - val_loss: 0.6726 - val_acc: 0.7000\n",
            "Epoch 101/300\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.4100 - acc: 0.8145 - val_loss: 0.6257 - val_acc: 0.7200\n",
            "Epoch 102/300\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3779 - acc: 0.8355 - val_loss: 0.7257 - val_acc: 0.6700\n",
            "Epoch 103/300\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3895 - acc: 0.8161 - val_loss: 0.6387 - val_acc: 0.7100\n",
            "Epoch 104/300\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3993 - acc: 0.8194 - val_loss: 0.7114 - val_acc: 0.6900\n",
            "Epoch 105/300\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3884 - acc: 0.8306 - val_loss: 0.6502 - val_acc: 0.7200\n",
            "Epoch 106/300\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3779 - acc: 0.8290 - val_loss: 0.6382 - val_acc: 0.7200\n",
            "Epoch 107/300\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3973 - acc: 0.8258 - val_loss: 0.6176 - val_acc: 0.7400\n",
            "Epoch 108/300\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3872 - acc: 0.8419 - val_loss: 0.7105 - val_acc: 0.6800\n",
            "Epoch 109/300\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3901 - acc: 0.8258 - val_loss: 0.6471 - val_acc: 0.7000\n",
            "Epoch 110/300\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3690 - acc: 0.8306 - val_loss: 0.6806 - val_acc: 0.7100\n",
            "Epoch 111/300\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3910 - acc: 0.8387 - val_loss: 0.6820 - val_acc: 0.6800\n",
            "Epoch 112/300\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3819 - acc: 0.8306 - val_loss: 0.6223 - val_acc: 0.7000\n",
            "Epoch 113/300\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3892 - acc: 0.8145 - val_loss: 0.6051 - val_acc: 0.7300\n",
            "Epoch 114/300\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3922 - acc: 0.8323 - val_loss: 0.6737 - val_acc: 0.7100\n",
            "Epoch 115/300\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3845 - acc: 0.8274 - val_loss: 0.6201 - val_acc: 0.7200\n",
            "Epoch 116/300\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3855 - acc: 0.8194 - val_loss: 0.7615 - val_acc: 0.6900\n",
            "Epoch 117/300\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3782 - acc: 0.8306 - val_loss: 0.6010 - val_acc: 0.7000\n",
            "Epoch 118/300\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3746 - acc: 0.8452 - val_loss: 0.6607 - val_acc: 0.6900\n",
            "Epoch 119/300\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3669 - acc: 0.8419 - val_loss: 0.6280 - val_acc: 0.7000\n",
            "Epoch 120/300\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3661 - acc: 0.8484 - val_loss: 0.6128 - val_acc: 0.7200\n",
            "Epoch 121/300\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3647 - acc: 0.8435 - val_loss: 0.5795 - val_acc: 0.7100\n",
            "Epoch 122/300\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3847 - acc: 0.8323 - val_loss: 0.6274 - val_acc: 0.7100\n",
            "Epoch 123/300\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3973 - acc: 0.8274 - val_loss: 0.5930 - val_acc: 0.7000\n",
            "Epoch 124/300\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3910 - acc: 0.8177 - val_loss: 0.6065 - val_acc: 0.7000\n",
            "Epoch 125/300\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3700 - acc: 0.8500 - val_loss: 0.8204 - val_acc: 0.6300\n",
            "Epoch 126/300\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3934 - acc: 0.8323 - val_loss: 0.7136 - val_acc: 0.6800\n",
            "Epoch 127/300\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3665 - acc: 0.8403 - val_loss: 0.7235 - val_acc: 0.7000\n",
            "Epoch 128/300\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3803 - acc: 0.8452 - val_loss: 0.6731 - val_acc: 0.7000\n",
            "Epoch 129/300\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3707 - acc: 0.8468 - val_loss: 0.7593 - val_acc: 0.6800\n",
            "Epoch 130/300\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3637 - acc: 0.8435 - val_loss: 0.6257 - val_acc: 0.7200\n",
            "Epoch 131/300\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3706 - acc: 0.8290 - val_loss: 0.7053 - val_acc: 0.7100\n",
            "Epoch 132/300\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3618 - acc: 0.8419 - val_loss: 0.5866 - val_acc: 0.7200\n",
            "Epoch 133/300\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3771 - acc: 0.8161 - val_loss: 0.6275 - val_acc: 0.7300\n",
            "Epoch 134/300\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3625 - acc: 0.8306 - val_loss: 0.7252 - val_acc: 0.7100\n",
            "Epoch 135/300\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3873 - acc: 0.8306 - val_loss: 0.7466 - val_acc: 0.7000\n",
            "Epoch 136/300\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3673 - acc: 0.8613 - val_loss: 0.6613 - val_acc: 0.7100\n",
            "Epoch 137/300\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3733 - acc: 0.8452 - val_loss: 0.6902 - val_acc: 0.7100\n",
            "Epoch 138/300\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3876 - acc: 0.8177 - val_loss: 0.7066 - val_acc: 0.7000\n",
            "Epoch 139/300\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3708 - acc: 0.8452 - val_loss: 0.7139 - val_acc: 0.7000\n",
            "Epoch 140/300\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3874 - acc: 0.8129 - val_loss: 0.6876 - val_acc: 0.6800\n",
            "Epoch 141/300\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3757 - acc: 0.8226 - val_loss: 0.6241 - val_acc: 0.7000\n",
            "Epoch 142/300\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3586 - acc: 0.8403 - val_loss: 0.6336 - val_acc: 0.7200\n",
            "Epoch 143/300\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3682 - acc: 0.8371 - val_loss: 0.7209 - val_acc: 0.7200\n",
            "Epoch 144/300\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3770 - acc: 0.8339 - val_loss: 0.6621 - val_acc: 0.7100\n",
            "Epoch 145/300\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3669 - acc: 0.8403 - val_loss: 0.6768 - val_acc: 0.7100\n",
            "Epoch 146/300\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3674 - acc: 0.8435 - val_loss: 0.6674 - val_acc: 0.6900\n",
            "Epoch 147/300\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3770 - acc: 0.8355 - val_loss: 0.6339 - val_acc: 0.7000\n",
            "Epoch 148/300\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3698 - acc: 0.8403 - val_loss: 0.9416 - val_acc: 0.6500\n",
            "Epoch 149/300\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.4225 - acc: 0.8210 - val_loss: 0.7077 - val_acc: 0.7000\n",
            "Epoch 150/300\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3706 - acc: 0.8339 - val_loss: 0.6089 - val_acc: 0.7400\n",
            "Epoch 151/300\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3874 - acc: 0.8306 - val_loss: 0.6544 - val_acc: 0.7000\n",
            "Epoch 152/300\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3505 - acc: 0.8468 - val_loss: 0.6954 - val_acc: 0.7000\n",
            "Epoch 153/300\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3816 - acc: 0.8226 - val_loss: 0.6269 - val_acc: 0.7100\n",
            "Epoch 154/300\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3691 - acc: 0.8532 - val_loss: 0.6143 - val_acc: 0.7200\n",
            "Epoch 155/300\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3810 - acc: 0.8306 - val_loss: 0.6779 - val_acc: 0.6800\n",
            "Epoch 156/300\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3864 - acc: 0.8258 - val_loss: 0.6100 - val_acc: 0.7200\n",
            "Epoch 157/300\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3821 - acc: 0.8210 - val_loss: 0.6304 - val_acc: 0.6900\n",
            "Epoch 158/300\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3830 - acc: 0.8306 - val_loss: 0.6117 - val_acc: 0.7200\n",
            "Epoch 159/300\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3562 - acc: 0.8387 - val_loss: 0.6280 - val_acc: 0.7400\n",
            "Epoch 160/300\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3457 - acc: 0.8355 - val_loss: 0.6488 - val_acc: 0.7200\n",
            "Epoch 161/300\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3347 - acc: 0.8645 - val_loss: 0.6660 - val_acc: 0.7100\n",
            "Epoch 162/300\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3571 - acc: 0.8452 - val_loss: 0.7139 - val_acc: 0.7000\n",
            "Epoch 163/300\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3644 - acc: 0.8290 - val_loss: 0.6869 - val_acc: 0.6700\n",
            "Epoch 164/300\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3637 - acc: 0.8484 - val_loss: 0.6053 - val_acc: 0.6700\n",
            "Epoch 165/300\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3544 - acc: 0.8484 - val_loss: 0.6155 - val_acc: 0.6700\n",
            "Epoch 166/300\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3719 - acc: 0.8419 - val_loss: 0.6999 - val_acc: 0.7000\n",
            "Epoch 167/300\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3651 - acc: 0.8323 - val_loss: 0.6336 - val_acc: 0.7200\n",
            "Epoch 168/300\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3490 - acc: 0.8548 - val_loss: 0.6027 - val_acc: 0.7200\n",
            "Epoch 169/300\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3649 - acc: 0.8355 - val_loss: 0.6162 - val_acc: 0.7100\n",
            "Epoch 170/300\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3647 - acc: 0.8484 - val_loss: 0.7991 - val_acc: 0.6700\n",
            "Epoch 171/300\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.4010 - acc: 0.8339 - val_loss: 0.6931 - val_acc: 0.6900\n",
            "Epoch 172/300\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3624 - acc: 0.8548 - val_loss: 0.6698 - val_acc: 0.7000\n",
            "Epoch 173/300\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3820 - acc: 0.8258 - val_loss: 0.6592 - val_acc: 0.6800\n",
            "Epoch 174/300\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3799 - acc: 0.8274 - val_loss: 0.6458 - val_acc: 0.7100\n",
            "Epoch 175/300\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3700 - acc: 0.8419 - val_loss: 0.6935 - val_acc: 0.7100\n",
            "Epoch 176/300\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3630 - acc: 0.8516 - val_loss: 0.6166 - val_acc: 0.6900\n",
            "Epoch 177/300\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3743 - acc: 0.8081 - val_loss: 0.7559 - val_acc: 0.7100\n",
            "Epoch 178/300\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3608 - acc: 0.8371 - val_loss: 0.6920 - val_acc: 0.7000\n",
            "Epoch 179/300\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3780 - acc: 0.8403 - val_loss: 0.6627 - val_acc: 0.6800\n",
            "Epoch 180/300\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3867 - acc: 0.8306 - val_loss: 0.5855 - val_acc: 0.6800\n",
            "Epoch 181/300\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3758 - acc: 0.8355 - val_loss: 0.6226 - val_acc: 0.7200\n",
            "Epoch 182/300\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3560 - acc: 0.8419 - val_loss: 0.5740 - val_acc: 0.7100\n",
            "Epoch 183/300\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3272 - acc: 0.8710 - val_loss: 0.6331 - val_acc: 0.6900\n",
            "Epoch 184/300\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3688 - acc: 0.8484 - val_loss: 0.6282 - val_acc: 0.7000\n",
            "Epoch 185/300\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.4021 - acc: 0.8145 - val_loss: 0.6723 - val_acc: 0.6900\n",
            "Epoch 186/300\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3391 - acc: 0.8419 - val_loss: 0.7228 - val_acc: 0.7100\n",
            "Epoch 187/300\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3580 - acc: 0.8468 - val_loss: 0.6368 - val_acc: 0.7200\n",
            "Epoch 188/300\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3567 - acc: 0.8548 - val_loss: 0.7412 - val_acc: 0.7000\n",
            "Epoch 189/300\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3391 - acc: 0.8516 - val_loss: 0.6822 - val_acc: 0.7000\n",
            "Epoch 190/300\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3207 - acc: 0.8629 - val_loss: 0.7465 - val_acc: 0.6900\n",
            "Epoch 191/300\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3326 - acc: 0.8548 - val_loss: 0.6179 - val_acc: 0.7100\n",
            "Epoch 192/300\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3386 - acc: 0.8581 - val_loss: 0.7810 - val_acc: 0.6700\n",
            "Epoch 193/300\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3662 - acc: 0.8306 - val_loss: 0.5867 - val_acc: 0.7200\n",
            "Epoch 194/300\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3540 - acc: 0.8419 - val_loss: 0.5877 - val_acc: 0.7100\n",
            "Epoch 195/300\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3522 - acc: 0.8484 - val_loss: 0.7050 - val_acc: 0.7000\n",
            "Epoch 196/300\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3606 - acc: 0.8452 - val_loss: 0.6648 - val_acc: 0.7000\n",
            "Epoch 197/300\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3442 - acc: 0.8565 - val_loss: 0.7764 - val_acc: 0.6800\n",
            "Epoch 198/300\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3656 - acc: 0.8323 - val_loss: 0.6839 - val_acc: 0.6900\n",
            "Epoch 199/300\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3643 - acc: 0.8452 - val_loss: 0.7290 - val_acc: 0.6900\n",
            "Epoch 200/300\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3584 - acc: 0.8452 - val_loss: 0.5915 - val_acc: 0.7000\n",
            "Epoch 201/300\n",
            "\n",
            "Epoch 00201: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3546 - acc: 0.8452 - val_loss: 0.6928 - val_acc: 0.6700\n",
            "Epoch 202/300\n",
            "\n",
            "Epoch 00202: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3445 - acc: 0.8484 - val_loss: 0.6877 - val_acc: 0.6900\n",
            "Epoch 203/300\n",
            "\n",
            "Epoch 00203: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3597 - acc: 0.8323 - val_loss: 0.5899 - val_acc: 0.7000\n",
            "Epoch 204/300\n",
            "\n",
            "Epoch 00204: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3386 - acc: 0.8581 - val_loss: 0.6576 - val_acc: 0.7100\n",
            "Epoch 205/300\n",
            "\n",
            "Epoch 00205: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3556 - acc: 0.8355 - val_loss: 0.6550 - val_acc: 0.7100\n",
            "Epoch 206/300\n",
            "\n",
            "Epoch 00206: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3338 - acc: 0.8500 - val_loss: 0.7033 - val_acc: 0.6800\n",
            "Epoch 207/300\n",
            "\n",
            "Epoch 00207: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3507 - acc: 0.8371 - val_loss: 0.7341 - val_acc: 0.7000\n",
            "Epoch 208/300\n",
            "\n",
            "Epoch 00208: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3629 - acc: 0.8468 - val_loss: 0.7583 - val_acc: 0.6800\n",
            "Epoch 209/300\n",
            "\n",
            "Epoch 00209: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3747 - acc: 0.8355 - val_loss: 0.7697 - val_acc: 0.6800\n",
            "Epoch 210/300\n",
            "\n",
            "Epoch 00210: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3594 - acc: 0.8177 - val_loss: 0.9077 - val_acc: 0.6000\n",
            "Epoch 211/300\n",
            "\n",
            "Epoch 00211: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3894 - acc: 0.8194 - val_loss: 0.6311 - val_acc: 0.7100\n",
            "Epoch 212/300\n",
            "\n",
            "Epoch 00212: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3232 - acc: 0.8661 - val_loss: 0.8736 - val_acc: 0.6300\n",
            "Epoch 213/300\n",
            "\n",
            "Epoch 00213: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3404 - acc: 0.8419 - val_loss: 0.8044 - val_acc: 0.6800\n",
            "Epoch 214/300\n",
            "\n",
            "Epoch 00214: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3512 - acc: 0.8371 - val_loss: 0.7650 - val_acc: 0.7000\n",
            "Epoch 215/300\n",
            "\n",
            "Epoch 00215: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3695 - acc: 0.8161 - val_loss: 0.6835 - val_acc: 0.6800\n",
            "Epoch 216/300\n",
            "\n",
            "Epoch 00216: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3523 - acc: 0.8435 - val_loss: 0.7607 - val_acc: 0.7100\n",
            "Epoch 217/300\n",
            "\n",
            "Epoch 00217: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3677 - acc: 0.8484 - val_loss: 0.6755 - val_acc: 0.6900\n",
            "Epoch 218/300\n",
            "\n",
            "Epoch 00218: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3455 - acc: 0.8548 - val_loss: 0.7137 - val_acc: 0.7000\n",
            "Epoch 219/300\n",
            "\n",
            "Epoch 00219: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3399 - acc: 0.8484 - val_loss: 0.6754 - val_acc: 0.6600\n",
            "Epoch 220/300\n",
            "\n",
            "Epoch 00220: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3305 - acc: 0.8484 - val_loss: 0.7548 - val_acc: 0.7000\n",
            "Epoch 221/300\n",
            "\n",
            "Epoch 00221: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3460 - acc: 0.8516 - val_loss: 0.7083 - val_acc: 0.7000\n",
            "Epoch 222/300\n",
            "\n",
            "Epoch 00222: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3662 - acc: 0.8339 - val_loss: 0.7291 - val_acc: 0.6900\n",
            "Epoch 223/300\n",
            "\n",
            "Epoch 00223: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3464 - acc: 0.8548 - val_loss: 0.6053 - val_acc: 0.7100\n",
            "Epoch 224/300\n",
            "\n",
            "Epoch 00224: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3606 - acc: 0.8323 - val_loss: 0.7602 - val_acc: 0.7000\n",
            "Epoch 225/300\n",
            "\n",
            "Epoch 00225: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3783 - acc: 0.8355 - val_loss: 0.7826 - val_acc: 0.6700\n",
            "Epoch 226/300\n",
            "\n",
            "Epoch 00226: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3360 - acc: 0.8565 - val_loss: 0.7797 - val_acc: 0.6800\n",
            "Epoch 227/300\n",
            "\n",
            "Epoch 00227: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3539 - acc: 0.8484 - val_loss: 0.6717 - val_acc: 0.6900\n",
            "Epoch 228/300\n",
            "\n",
            "Epoch 00228: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3803 - acc: 0.8274 - val_loss: 0.7410 - val_acc: 0.6300\n",
            "Epoch 229/300\n",
            "\n",
            "Epoch 00229: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3559 - acc: 0.8403 - val_loss: 0.8103 - val_acc: 0.6700\n",
            "Epoch 230/300\n",
            "\n",
            "Epoch 00230: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3336 - acc: 0.8726 - val_loss: 0.8397 - val_acc: 0.6400\n",
            "Epoch 231/300\n",
            "\n",
            "Epoch 00231: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3640 - acc: 0.8371 - val_loss: 0.8725 - val_acc: 0.6400\n",
            "Epoch 232/300\n",
            "\n",
            "Epoch 00232: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3576 - acc: 0.8419 - val_loss: 0.7621 - val_acc: 0.6500\n",
            "Epoch 233/300\n",
            "\n",
            "Epoch 00233: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3682 - acc: 0.8177 - val_loss: 0.6809 - val_acc: 0.6800\n",
            "Epoch 234/300\n",
            "\n",
            "Epoch 00234: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3553 - acc: 0.8403 - val_loss: 0.6111 - val_acc: 0.7100\n",
            "Epoch 235/300\n",
            "\n",
            "Epoch 00235: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3345 - acc: 0.8532 - val_loss: 0.5761 - val_acc: 0.7200\n",
            "Epoch 236/300\n",
            "\n",
            "Epoch 00236: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3215 - acc: 0.8677 - val_loss: 0.6329 - val_acc: 0.7100\n",
            "Epoch 237/300\n",
            "\n",
            "Epoch 00237: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3366 - acc: 0.8435 - val_loss: 0.7733 - val_acc: 0.6700\n",
            "Epoch 238/300\n",
            "\n",
            "Epoch 00238: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3517 - acc: 0.8452 - val_loss: 0.7744 - val_acc: 0.7000\n",
            "Epoch 239/300\n",
            "\n",
            "Epoch 00239: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3657 - acc: 0.8403 - val_loss: 0.8063 - val_acc: 0.7000\n",
            "Epoch 240/300\n",
            "\n",
            "Epoch 00240: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3425 - acc: 0.8565 - val_loss: 0.7330 - val_acc: 0.6900\n",
            "Epoch 241/300\n",
            "\n",
            "Epoch 00241: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3061 - acc: 0.8790 - val_loss: 0.7755 - val_acc: 0.6900\n",
            "Epoch 242/300\n",
            "\n",
            "Epoch 00242: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3442 - acc: 0.8435 - val_loss: 0.6768 - val_acc: 0.6900\n",
            "Epoch 243/300\n",
            "\n",
            "Epoch 00243: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3634 - acc: 0.8484 - val_loss: 0.7131 - val_acc: 0.6800\n",
            "Epoch 244/300\n",
            "\n",
            "Epoch 00244: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3566 - acc: 0.8339 - val_loss: 0.7198 - val_acc: 0.6800\n",
            "Epoch 245/300\n",
            "\n",
            "Epoch 00245: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3496 - acc: 0.8565 - val_loss: 0.6587 - val_acc: 0.7100\n",
            "Epoch 246/300\n",
            "\n",
            "Epoch 00246: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3247 - acc: 0.8661 - val_loss: 0.6568 - val_acc: 0.7000\n",
            "Epoch 247/300\n",
            "\n",
            "Epoch 00247: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3365 - acc: 0.8629 - val_loss: 0.7391 - val_acc: 0.6500\n",
            "Epoch 248/300\n",
            "\n",
            "Epoch 00248: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3323 - acc: 0.8645 - val_loss: 0.8303 - val_acc: 0.6400\n",
            "Epoch 249/300\n",
            "\n",
            "Epoch 00249: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3513 - acc: 0.8548 - val_loss: 0.6304 - val_acc: 0.6900\n",
            "Epoch 250/300\n",
            "\n",
            "Epoch 00250: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3792 - acc: 0.8274 - val_loss: 0.6826 - val_acc: 0.6700\n",
            "Epoch 251/300\n",
            "\n",
            "Epoch 00251: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3302 - acc: 0.8581 - val_loss: 0.7256 - val_acc: 0.7000\n",
            "Epoch 252/300\n",
            "\n",
            "Epoch 00252: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3467 - acc: 0.8516 - val_loss: 0.7585 - val_acc: 0.6900\n",
            "Epoch 253/300\n",
            "\n",
            "Epoch 00253: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3285 - acc: 0.8710 - val_loss: 0.7587 - val_acc: 0.6600\n",
            "Epoch 254/300\n",
            "\n",
            "Epoch 00254: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3600 - acc: 0.8419 - val_loss: 0.7200 - val_acc: 0.6900\n",
            "Epoch 255/300\n",
            "\n",
            "Epoch 00255: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3381 - acc: 0.8565 - val_loss: 0.7111 - val_acc: 0.6900\n",
            "Epoch 256/300\n",
            "\n",
            "Epoch 00256: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3228 - acc: 0.8774 - val_loss: 0.8547 - val_acc: 0.6400\n",
            "Epoch 257/300\n",
            "\n",
            "Epoch 00257: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3494 - acc: 0.8468 - val_loss: 0.6904 - val_acc: 0.7000\n",
            "Epoch 258/300\n",
            "\n",
            "Epoch 00258: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3663 - acc: 0.8419 - val_loss: 0.6135 - val_acc: 0.6900\n",
            "Epoch 259/300\n",
            "\n",
            "Epoch 00259: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3286 - acc: 0.8597 - val_loss: 0.8071 - val_acc: 0.6800\n",
            "Epoch 260/300\n",
            "\n",
            "Epoch 00260: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3225 - acc: 0.8565 - val_loss: 0.6859 - val_acc: 0.6800\n",
            "Epoch 261/300\n",
            "\n",
            "Epoch 00261: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3596 - acc: 0.8387 - val_loss: 0.6730 - val_acc: 0.7100\n",
            "Epoch 262/300\n",
            "\n",
            "Epoch 00262: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3338 - acc: 0.8452 - val_loss: 0.6596 - val_acc: 0.7100\n",
            "Epoch 263/300\n",
            "\n",
            "Epoch 00263: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3467 - acc: 0.8548 - val_loss: 0.7139 - val_acc: 0.7100\n",
            "Epoch 264/300\n",
            "\n",
            "Epoch 00264: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3291 - acc: 0.8629 - val_loss: 0.7409 - val_acc: 0.7100\n",
            "Epoch 265/300\n",
            "\n",
            "Epoch 00265: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3130 - acc: 0.8774 - val_loss: 0.7769 - val_acc: 0.7200\n",
            "Epoch 266/300\n",
            "\n",
            "Epoch 00266: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3348 - acc: 0.8548 - val_loss: 0.6847 - val_acc: 0.7000\n",
            "Epoch 267/300\n",
            "\n",
            "Epoch 00267: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3243 - acc: 0.8565 - val_loss: 0.8146 - val_acc: 0.6800\n",
            "Epoch 268/300\n",
            "\n",
            "Epoch 00268: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3729 - acc: 0.8339 - val_loss: 0.7425 - val_acc: 0.7200\n",
            "Epoch 269/300\n",
            "\n",
            "Epoch 00269: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3499 - acc: 0.8484 - val_loss: 0.7552 - val_acc: 0.6800\n",
            "Epoch 270/300\n",
            "\n",
            "Epoch 00270: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3320 - acc: 0.8613 - val_loss: 0.6885 - val_acc: 0.7000\n",
            "Epoch 271/300\n",
            "\n",
            "Epoch 00271: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3399 - acc: 0.8516 - val_loss: 0.7026 - val_acc: 0.7200\n",
            "Epoch 272/300\n",
            "\n",
            "Epoch 00272: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3401 - acc: 0.8532 - val_loss: 0.7356 - val_acc: 0.6900\n",
            "Epoch 273/300\n",
            "\n",
            "Epoch 00273: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3477 - acc: 0.8532 - val_loss: 0.9291 - val_acc: 0.6000\n",
            "Epoch 274/300\n",
            "\n",
            "Epoch 00274: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3585 - acc: 0.8500 - val_loss: 0.6631 - val_acc: 0.7200\n",
            "Epoch 275/300\n",
            "\n",
            "Epoch 00275: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3646 - acc: 0.8435 - val_loss: 0.6894 - val_acc: 0.7000\n",
            "Epoch 276/300\n",
            "\n",
            "Epoch 00276: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3519 - acc: 0.8435 - val_loss: 0.6733 - val_acc: 0.7000\n",
            "Epoch 277/300\n",
            "\n",
            "Epoch 00277: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3417 - acc: 0.8581 - val_loss: 0.6336 - val_acc: 0.6900\n",
            "Epoch 278/300\n",
            "\n",
            "Epoch 00278: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3131 - acc: 0.8710 - val_loss: 0.7780 - val_acc: 0.6700\n",
            "Epoch 279/300\n",
            "\n",
            "Epoch 00279: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3529 - acc: 0.8613 - val_loss: 0.7043 - val_acc: 0.7000\n",
            "Epoch 280/300\n",
            "\n",
            "Epoch 00280: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3626 - acc: 0.8435 - val_loss: 0.7532 - val_acc: 0.6700\n",
            "Epoch 281/300\n",
            "\n",
            "Epoch 00281: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3357 - acc: 0.8565 - val_loss: 0.6815 - val_acc: 0.7000\n",
            "Epoch 282/300\n",
            "\n",
            "Epoch 00282: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3282 - acc: 0.8468 - val_loss: 0.6943 - val_acc: 0.7100\n",
            "Epoch 283/300\n",
            "\n",
            "Epoch 00283: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3457 - acc: 0.8468 - val_loss: 0.7265 - val_acc: 0.7000\n",
            "Epoch 284/300\n",
            "\n",
            "Epoch 00284: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3235 - acc: 0.8710 - val_loss: 0.6380 - val_acc: 0.7100\n",
            "Epoch 285/300\n",
            "\n",
            "Epoch 00285: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3308 - acc: 0.8613 - val_loss: 0.7193 - val_acc: 0.7000\n",
            "Epoch 286/300\n",
            "\n",
            "Epoch 00286: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3485 - acc: 0.8532 - val_loss: 0.7920 - val_acc: 0.6800\n",
            "Epoch 287/300\n",
            "\n",
            "Epoch 00287: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3350 - acc: 0.8597 - val_loss: 0.8276 - val_acc: 0.6800\n",
            "Epoch 288/300\n",
            "\n",
            "Epoch 00288: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3342 - acc: 0.8468 - val_loss: 0.7249 - val_acc: 0.7000\n",
            "Epoch 289/300\n",
            "\n",
            "Epoch 00289: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3558 - acc: 0.8500 - val_loss: 0.6005 - val_acc: 0.7000\n",
            "Epoch 290/300\n",
            "\n",
            "Epoch 00290: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3160 - acc: 0.8726 - val_loss: 0.7059 - val_acc: 0.7000\n",
            "Epoch 291/300\n",
            "\n",
            "Epoch 00291: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3439 - acc: 0.8484 - val_loss: 0.5945 - val_acc: 0.7000\n",
            "Epoch 292/300\n",
            "\n",
            "Epoch 00292: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3382 - acc: 0.8500 - val_loss: 0.6586 - val_acc: 0.7000\n",
            "Epoch 293/300\n",
            "\n",
            "Epoch 00293: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3371 - acc: 0.8677 - val_loss: 0.6971 - val_acc: 0.7000\n",
            "Epoch 294/300\n",
            "\n",
            "Epoch 00294: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3379 - acc: 0.8597 - val_loss: 0.8498 - val_acc: 0.6500\n",
            "Epoch 295/300\n",
            "\n",
            "Epoch 00295: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3345 - acc: 0.8516 - val_loss: 0.7112 - val_acc: 0.6900\n",
            "Epoch 296/300\n",
            "\n",
            "Epoch 00296: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3123 - acc: 0.8806 - val_loss: 0.7683 - val_acc: 0.6800\n",
            "Epoch 297/300\n",
            "\n",
            "Epoch 00297: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3265 - acc: 0.8532 - val_loss: 0.8946 - val_acc: 0.6500\n",
            "Epoch 298/300\n",
            "\n",
            "Epoch 00298: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3340 - acc: 0.8500 - val_loss: 0.6951 - val_acc: 0.7000\n",
            "Epoch 299/300\n",
            "\n",
            "Epoch 00299: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3411 - acc: 0.8452 - val_loss: 0.8568 - val_acc: 0.6500\n",
            "Epoch 300/300\n",
            "\n",
            "Epoch 00300: val_loss did not improve from 0.52656\n",
            "620/620 - 1s - loss: 0.3275 - acc: 0.8710 - val_loss: 0.7703 - val_acc: 0.7000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nxszfqQUS9QAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "# Evaluate on test data\n",
            "40/40 [==============================] - 0s 3ms/sample - loss: 0.6474 - acc: 0.6500\n",
            "test loss, test acc: [0.6474127248278819, 0.65]\n",
            "1\n",
            "EEG_Deep/Data2A/parsed_P01T.mat\n",
            "(0, 12, 4096)\n",
            "(80, 12, 4096)\n",
            "(80, 1)\n",
            "2\n",
            "EEG_Deep/Data2A/parsed_P02T.mat\n",
            "(80, 12, 4096)\n",
            "(160, 12, 4096)\n",
            "(160, 1)\n",
            "3\n",
            "EEG_Deep/Data2A/parsed_P03T.mat\n",
            "(160, 12, 4096)\n",
            "(240, 12, 4096)\n",
            "(240, 1)\n",
            "4\n",
            "EEG_Deep/Data2A/parsed_P04T.mat\n",
            "(240, 12, 4096)\n",
            "(320, 12, 4096)\n",
            "(320, 1)\n",
            "5\n",
            "EEG_Deep/Data2A/parsed_P05T.mat\n",
            "(320, 12, 4096)\n",
            "(400, 12, 4096)\n",
            "(400, 1)\n",
            "6\n",
            "EEG_Deep/Data2A/parsed_P06T.mat\n",
            "(400, 12, 4096)\n",
            "(480, 12, 4096)\n",
            "(480, 1)\n",
            "8\n",
            "EEG_Deep/Data2A/parsed_P08T.mat\n",
            "(480, 12, 4096)\n",
            "(560, 12, 4096)\n",
            "(560, 1)\n",
            "9\n",
            "EEG_Deep/Data2A/parsed_P09T.mat\n",
            "(560, 12, 4096)\n",
            "(640, 12, 4096)\n",
            "(640, 1)\n",
            "10\n",
            "EEG_Deep/Data2A/parsed_P010T.mat\n",
            "(640, 12, 4096)\n",
            "(720, 12, 4096)\n",
            "(720, 1)\n",
            "7\n",
            "EEG_Deep/Data2A/parsed_P07E.mat\n",
            "(0, 12, 4096)\n",
            "(40, 12, 4096)\n",
            "(40, 1)\n",
            "Filtering of training data in progress\n",
            "(720, 12, 4096)\n",
            "Filtering of test data in progress\n",
            "(40, 12, 4096)\n",
            "[1. 1. 2. 1. 2. 1. 2. 1. 1. 2. 1. 1. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2.\n",
            " 2. 1. 1. 1. 2. 2. 2. 2. 1. 1. 1. 1. 2. 2. 2. 2. 2. 1. 2. 2. 1. 2. 1. 2.\n",
            " 2. 1. 1. 2. 2. 1. 1. 2. 2. 1. 1. 2. 2. 1. 1. 1. 2. 1. 1. 1. 1. 2. 2. 2.\n",
            " 2. 2. 2. 2. 2. 1. 1. 2. 2. 1. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 1. 1. 1. 2.\n",
            " 1. 1. 2. 1.]\n",
            "(620, 12, 1536)\n",
            "(620,)\n",
            "(100, 12, 1536)\n",
            "(100,)\n",
            "[[1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]]\n",
            "X_train shape: (620, 1, 12, 1536)\n",
            "620 train samples\n",
            "100 val samples\n",
            "(40, 12, 1536)\n",
            "(40, 1)\n",
            "X_train shape: (40, 1, 12, 1536)\n",
            "40 train samples\n",
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 1, 12, 1536)]     0         \n",
            "_________________________________________________________________\n",
            "conv2d (Conv2D)              (None, 8, 12, 1536)       200       \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 8, 12, 1536)       32        \n",
            "_________________________________________________________________\n",
            "depthwise_conv2d (DepthwiseC (None, 16, 1, 1536)       192       \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 16, 1, 1536)       64        \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 16, 1, 1536)       0         \n",
            "_________________________________________________________________\n",
            "average_pooling2d (AveragePo (None, 16, 1, 384)        0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 16, 1, 384)        0         \n",
            "_________________________________________________________________\n",
            "separable_conv2d (SeparableC (None, 16, 1, 384)        512       \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 16, 1, 384)        64        \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 16, 1, 384)        0         \n",
            "_________________________________________________________________\n",
            "average_pooling2d_1 (Average (None, 16, 1, 48)         0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 16, 1, 48)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 768)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 2)                 1538      \n",
            "_________________________________________________________________\n",
            "softmax (Activation)         (None, 2)                 0         \n",
            "=================================================================\n",
            "Total params: 2,602\n",
            "Trainable params: 2,522\n",
            "Non-trainable params: 80\n",
            "_________________________________________________________________\n",
            "Train on 620 samples, validate on 100 samples\n",
            "Epoch 1/300\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.69258, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.6982 - acc: 0.5016 - val_loss: 0.6926 - val_acc: 0.4900\n",
            "Epoch 2/300\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.69258 to 0.69081, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.6775 - acc: 0.5742 - val_loss: 0.6908 - val_acc: 0.5100\n",
            "Epoch 3/300\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.69081 to 0.68366, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.6611 - acc: 0.6113 - val_loss: 0.6837 - val_acc: 0.5800\n",
            "Epoch 4/300\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.68366 to 0.67389, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.6512 - acc: 0.6387 - val_loss: 0.6739 - val_acc: 0.6700\n",
            "Epoch 5/300\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.67389 to 0.65476, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.6297 - acc: 0.6500 - val_loss: 0.6548 - val_acc: 0.7600\n",
            "Epoch 6/300\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.65476 to 0.64686, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.6220 - acc: 0.6694 - val_loss: 0.6469 - val_acc: 0.6400\n",
            "Epoch 7/300\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.64686 to 0.62212, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.6029 - acc: 0.6790 - val_loss: 0.6221 - val_acc: 0.6900\n",
            "Epoch 8/300\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.62212 to 0.58277, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.5651 - acc: 0.7258 - val_loss: 0.5828 - val_acc: 0.7300\n",
            "Epoch 9/300\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.58277 to 0.57184, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.5541 - acc: 0.7226 - val_loss: 0.5718 - val_acc: 0.6900\n",
            "Epoch 10/300\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.57184 to 0.51119, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.5243 - acc: 0.7419 - val_loss: 0.5112 - val_acc: 0.7600\n",
            "Epoch 11/300\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.51119\n",
            "620/620 - 1s - loss: 0.5339 - acc: 0.7484 - val_loss: 0.5198 - val_acc: 0.7300\n",
            "Epoch 12/300\n",
            "\n",
            "Epoch 00012: val_loss improved from 0.51119 to 0.50425, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.5195 - acc: 0.7516 - val_loss: 0.5042 - val_acc: 0.7100\n",
            "Epoch 13/300\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.50425\n",
            "620/620 - 1s - loss: 0.5205 - acc: 0.7661 - val_loss: 0.5141 - val_acc: 0.7100\n",
            "Epoch 14/300\n",
            "\n",
            "Epoch 00014: val_loss improved from 0.50425 to 0.48216, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.5088 - acc: 0.7565 - val_loss: 0.4822 - val_acc: 0.7800\n",
            "Epoch 15/300\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.48216\n",
            "620/620 - 1s - loss: 0.5050 - acc: 0.7435 - val_loss: 0.4946 - val_acc: 0.7500\n",
            "Epoch 16/300\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.48216\n",
            "620/620 - 1s - loss: 0.4889 - acc: 0.7871 - val_loss: 0.4834 - val_acc: 0.7400\n",
            "Epoch 17/300\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.48216\n",
            "620/620 - 1s - loss: 0.4905 - acc: 0.7694 - val_loss: 0.5091 - val_acc: 0.7300\n",
            "Epoch 18/300\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.48216\n",
            "620/620 - 1s - loss: 0.5062 - acc: 0.7629 - val_loss: 0.5018 - val_acc: 0.7500\n",
            "Epoch 19/300\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.48216\n",
            "620/620 - 1s - loss: 0.4832 - acc: 0.7774 - val_loss: 0.5402 - val_acc: 0.6900\n",
            "Epoch 20/300\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.48216\n",
            "620/620 - 1s - loss: 0.5052 - acc: 0.7548 - val_loss: 0.5180 - val_acc: 0.7000\n",
            "Epoch 21/300\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.48216\n",
            "620/620 - 1s - loss: 0.4889 - acc: 0.7516 - val_loss: 0.4954 - val_acc: 0.7700\n",
            "Epoch 22/300\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.48216\n",
            "620/620 - 1s - loss: 0.4743 - acc: 0.7677 - val_loss: 0.6058 - val_acc: 0.6500\n",
            "Epoch 23/300\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.48216\n",
            "620/620 - 1s - loss: 0.5155 - acc: 0.7500 - val_loss: 0.5304 - val_acc: 0.6800\n",
            "Epoch 24/300\n",
            "\n",
            "Epoch 00024: val_loss improved from 0.48216 to 0.44996, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.4631 - acc: 0.7919 - val_loss: 0.4500 - val_acc: 0.7800\n",
            "Epoch 25/300\n",
            "\n",
            "Epoch 00025: val_loss improved from 0.44996 to 0.43399, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.4727 - acc: 0.7903 - val_loss: 0.4340 - val_acc: 0.8100\n",
            "Epoch 26/300\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.43399\n",
            "620/620 - 1s - loss: 0.4665 - acc: 0.7677 - val_loss: 0.4789 - val_acc: 0.7500\n",
            "Epoch 27/300\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.43399\n",
            "620/620 - 1s - loss: 0.4665 - acc: 0.7823 - val_loss: 0.4708 - val_acc: 0.7600\n",
            "Epoch 28/300\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.43399\n",
            "620/620 - 1s - loss: 0.4863 - acc: 0.7661 - val_loss: 0.5094 - val_acc: 0.7200\n",
            "Epoch 29/300\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.43399\n",
            "620/620 - 1s - loss: 0.4751 - acc: 0.7790 - val_loss: 0.6104 - val_acc: 0.6400\n",
            "Epoch 30/300\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.43399\n",
            "620/620 - 1s - loss: 0.4736 - acc: 0.7806 - val_loss: 0.4814 - val_acc: 0.7300\n",
            "Epoch 31/300\n",
            "\n",
            "Epoch 00031: val_loss improved from 0.43399 to 0.43384, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.4718 - acc: 0.7839 - val_loss: 0.4338 - val_acc: 0.7900\n",
            "Epoch 32/300\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.4793 - acc: 0.7548 - val_loss: 0.4601 - val_acc: 0.7800\n",
            "Epoch 33/300\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.4664 - acc: 0.7839 - val_loss: 0.4839 - val_acc: 0.7400\n",
            "Epoch 34/300\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.4577 - acc: 0.7758 - val_loss: 0.4763 - val_acc: 0.7500\n",
            "Epoch 35/300\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.4715 - acc: 0.7565 - val_loss: 0.5576 - val_acc: 0.6900\n",
            "Epoch 36/300\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.4649 - acc: 0.7742 - val_loss: 0.4982 - val_acc: 0.7300\n",
            "Epoch 37/300\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.4465 - acc: 0.7984 - val_loss: 0.5133 - val_acc: 0.7100\n",
            "Epoch 38/300\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.4520 - acc: 0.7903 - val_loss: 0.5102 - val_acc: 0.7100\n",
            "Epoch 39/300\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.4627 - acc: 0.7629 - val_loss: 0.4913 - val_acc: 0.7500\n",
            "Epoch 40/300\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.4534 - acc: 0.7823 - val_loss: 0.4865 - val_acc: 0.7400\n",
            "Epoch 41/300\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.4542 - acc: 0.7694 - val_loss: 0.4945 - val_acc: 0.7300\n",
            "Epoch 42/300\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.4636 - acc: 0.7806 - val_loss: 0.5620 - val_acc: 0.6800\n",
            "Epoch 43/300\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.4279 - acc: 0.8210 - val_loss: 0.5448 - val_acc: 0.6900\n",
            "Epoch 44/300\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.4400 - acc: 0.7871 - val_loss: 0.4730 - val_acc: 0.7700\n",
            "Epoch 45/300\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.4558 - acc: 0.7871 - val_loss: 0.5037 - val_acc: 0.7300\n",
            "Epoch 46/300\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.4500 - acc: 0.8032 - val_loss: 0.5686 - val_acc: 0.6600\n",
            "Epoch 47/300\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.4446 - acc: 0.7968 - val_loss: 0.4926 - val_acc: 0.7100\n",
            "Epoch 48/300\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.4386 - acc: 0.8032 - val_loss: 0.5284 - val_acc: 0.6700\n",
            "Epoch 49/300\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.4520 - acc: 0.7726 - val_loss: 0.5280 - val_acc: 0.6800\n",
            "Epoch 50/300\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.4254 - acc: 0.7952 - val_loss: 0.5185 - val_acc: 0.7100\n",
            "Epoch 51/300\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.4418 - acc: 0.7887 - val_loss: 0.5162 - val_acc: 0.7200\n",
            "Epoch 52/300\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.4522 - acc: 0.7903 - val_loss: 0.5323 - val_acc: 0.7100\n",
            "Epoch 53/300\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.4197 - acc: 0.8032 - val_loss: 0.5435 - val_acc: 0.6900\n",
            "Epoch 54/300\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.4536 - acc: 0.7887 - val_loss: 0.5387 - val_acc: 0.7100\n",
            "Epoch 55/300\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.4355 - acc: 0.8113 - val_loss: 0.4807 - val_acc: 0.7300\n",
            "Epoch 56/300\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.4397 - acc: 0.7984 - val_loss: 0.4999 - val_acc: 0.7300\n",
            "Epoch 57/300\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.4336 - acc: 0.7919 - val_loss: 0.5410 - val_acc: 0.7100\n",
            "Epoch 58/300\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.4276 - acc: 0.8097 - val_loss: 0.5824 - val_acc: 0.6800\n",
            "Epoch 59/300\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.4147 - acc: 0.7919 - val_loss: 0.4873 - val_acc: 0.7300\n",
            "Epoch 60/300\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.4212 - acc: 0.8129 - val_loss: 0.5080 - val_acc: 0.7400\n",
            "Epoch 61/300\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.4625 - acc: 0.7645 - val_loss: 0.4588 - val_acc: 0.7500\n",
            "Epoch 62/300\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.4114 - acc: 0.8097 - val_loss: 0.5602 - val_acc: 0.6800\n",
            "Epoch 63/300\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.4213 - acc: 0.8161 - val_loss: 0.4976 - val_acc: 0.7300\n",
            "Epoch 64/300\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.4390 - acc: 0.8048 - val_loss: 0.4752 - val_acc: 0.7400\n",
            "Epoch 65/300\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.4221 - acc: 0.7935 - val_loss: 0.4821 - val_acc: 0.7500\n",
            "Epoch 66/300\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.4109 - acc: 0.8081 - val_loss: 0.5306 - val_acc: 0.6900\n",
            "Epoch 67/300\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.4147 - acc: 0.8113 - val_loss: 0.5256 - val_acc: 0.7200\n",
            "Epoch 68/300\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.4362 - acc: 0.7952 - val_loss: 0.5150 - val_acc: 0.7300\n",
            "Epoch 69/300\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.4310 - acc: 0.8161 - val_loss: 0.5028 - val_acc: 0.7500\n",
            "Epoch 70/300\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.4321 - acc: 0.8161 - val_loss: 0.4458 - val_acc: 0.8000\n",
            "Epoch 71/300\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.4298 - acc: 0.7984 - val_loss: 0.4822 - val_acc: 0.7500\n",
            "Epoch 72/300\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.4108 - acc: 0.8177 - val_loss: 0.4665 - val_acc: 0.7600\n",
            "Epoch 73/300\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.4157 - acc: 0.8065 - val_loss: 0.5233 - val_acc: 0.7300\n",
            "Epoch 74/300\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.4034 - acc: 0.8065 - val_loss: 0.5329 - val_acc: 0.7100\n",
            "Epoch 75/300\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.4111 - acc: 0.8210 - val_loss: 0.4836 - val_acc: 0.7500\n",
            "Epoch 76/300\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.4236 - acc: 0.7984 - val_loss: 0.5828 - val_acc: 0.6900\n",
            "Epoch 77/300\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.4013 - acc: 0.8194 - val_loss: 0.5593 - val_acc: 0.6900\n",
            "Epoch 78/300\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.4363 - acc: 0.7758 - val_loss: 0.6305 - val_acc: 0.6500\n",
            "Epoch 79/300\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.4292 - acc: 0.7952 - val_loss: 0.5297 - val_acc: 0.7100\n",
            "Epoch 80/300\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.4127 - acc: 0.8274 - val_loss: 0.4604 - val_acc: 0.7700\n",
            "Epoch 81/300\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.4117 - acc: 0.8048 - val_loss: 0.5003 - val_acc: 0.7200\n",
            "Epoch 82/300\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.4227 - acc: 0.7903 - val_loss: 0.5293 - val_acc: 0.7200\n",
            "Epoch 83/300\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.4170 - acc: 0.8048 - val_loss: 0.5127 - val_acc: 0.7200\n",
            "Epoch 84/300\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.4254 - acc: 0.8097 - val_loss: 0.5637 - val_acc: 0.6900\n",
            "Epoch 85/300\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.4479 - acc: 0.7935 - val_loss: 0.5445 - val_acc: 0.6800\n",
            "Epoch 86/300\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.4191 - acc: 0.8129 - val_loss: 0.5680 - val_acc: 0.6900\n",
            "Epoch 87/300\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.4167 - acc: 0.8081 - val_loss: 0.6013 - val_acc: 0.6700\n",
            "Epoch 88/300\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.4229 - acc: 0.7952 - val_loss: 0.5150 - val_acc: 0.7400\n",
            "Epoch 89/300\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.4050 - acc: 0.8145 - val_loss: 0.5907 - val_acc: 0.6700\n",
            "Epoch 90/300\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.4124 - acc: 0.8113 - val_loss: 0.5829 - val_acc: 0.6900\n",
            "Epoch 91/300\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.4173 - acc: 0.8065 - val_loss: 0.5550 - val_acc: 0.6900\n",
            "Epoch 92/300\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3818 - acc: 0.8387 - val_loss: 0.5649 - val_acc: 0.7000\n",
            "Epoch 93/300\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.4197 - acc: 0.7984 - val_loss: 0.5180 - val_acc: 0.7300\n",
            "Epoch 94/300\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3885 - acc: 0.8355 - val_loss: 0.5336 - val_acc: 0.7400\n",
            "Epoch 95/300\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.4473 - acc: 0.8016 - val_loss: 0.5974 - val_acc: 0.6800\n",
            "Epoch 96/300\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.4003 - acc: 0.8210 - val_loss: 0.5299 - val_acc: 0.7100\n",
            "Epoch 97/300\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.4008 - acc: 0.8274 - val_loss: 0.4941 - val_acc: 0.7600\n",
            "Epoch 98/300\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3895 - acc: 0.8306 - val_loss: 0.5076 - val_acc: 0.7400\n",
            "Epoch 99/300\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.4022 - acc: 0.8194 - val_loss: 0.5254 - val_acc: 0.7100\n",
            "Epoch 100/300\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3848 - acc: 0.8161 - val_loss: 0.5853 - val_acc: 0.6600\n",
            "Epoch 101/300\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.4014 - acc: 0.8210 - val_loss: 0.5517 - val_acc: 0.6900\n",
            "Epoch 102/300\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3907 - acc: 0.8403 - val_loss: 0.5913 - val_acc: 0.6900\n",
            "Epoch 103/300\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.4066 - acc: 0.8339 - val_loss: 0.5803 - val_acc: 0.7200\n",
            "Epoch 104/300\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3967 - acc: 0.8355 - val_loss: 0.6190 - val_acc: 0.6500\n",
            "Epoch 105/300\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3959 - acc: 0.8097 - val_loss: 0.6166 - val_acc: 0.6500\n",
            "Epoch 106/300\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3961 - acc: 0.8242 - val_loss: 0.5216 - val_acc: 0.7100\n",
            "Epoch 107/300\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3814 - acc: 0.8387 - val_loss: 0.6020 - val_acc: 0.6700\n",
            "Epoch 108/300\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.4033 - acc: 0.8177 - val_loss: 0.5851 - val_acc: 0.6900\n",
            "Epoch 109/300\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3912 - acc: 0.8274 - val_loss: 0.6884 - val_acc: 0.6500\n",
            "Epoch 110/300\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.4042 - acc: 0.8258 - val_loss: 0.6454 - val_acc: 0.6600\n",
            "Epoch 111/300\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.4107 - acc: 0.8194 - val_loss: 0.6031 - val_acc: 0.7100\n",
            "Epoch 112/300\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3951 - acc: 0.8242 - val_loss: 0.5390 - val_acc: 0.6900\n",
            "Epoch 113/300\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3992 - acc: 0.8371 - val_loss: 0.5729 - val_acc: 0.7000\n",
            "Epoch 114/300\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3925 - acc: 0.8339 - val_loss: 0.4597 - val_acc: 0.7600\n",
            "Epoch 115/300\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.4171 - acc: 0.8145 - val_loss: 0.5442 - val_acc: 0.7100\n",
            "Epoch 116/300\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3926 - acc: 0.8403 - val_loss: 0.6006 - val_acc: 0.6800\n",
            "Epoch 117/300\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3744 - acc: 0.8403 - val_loss: 0.5660 - val_acc: 0.7000\n",
            "Epoch 118/300\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3871 - acc: 0.8339 - val_loss: 0.6087 - val_acc: 0.6700\n",
            "Epoch 119/300\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.4116 - acc: 0.8242 - val_loss: 0.5380 - val_acc: 0.6800\n",
            "Epoch 120/300\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.4010 - acc: 0.8274 - val_loss: 0.5693 - val_acc: 0.7000\n",
            "Epoch 121/300\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3912 - acc: 0.8258 - val_loss: 0.5603 - val_acc: 0.7000\n",
            "Epoch 122/300\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3897 - acc: 0.8306 - val_loss: 0.6010 - val_acc: 0.7100\n",
            "Epoch 123/300\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3625 - acc: 0.8452 - val_loss: 0.6970 - val_acc: 0.6400\n",
            "Epoch 124/300\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3911 - acc: 0.8355 - val_loss: 0.6068 - val_acc: 0.6600\n",
            "Epoch 125/300\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3994 - acc: 0.8258 - val_loss: 0.6378 - val_acc: 0.6700\n",
            "Epoch 126/300\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3939 - acc: 0.8032 - val_loss: 0.5844 - val_acc: 0.6900\n",
            "Epoch 127/300\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3948 - acc: 0.8145 - val_loss: 0.6101 - val_acc: 0.6800\n",
            "Epoch 128/300\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3900 - acc: 0.8210 - val_loss: 0.5339 - val_acc: 0.7200\n",
            "Epoch 129/300\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3943 - acc: 0.8258 - val_loss: 0.5751 - val_acc: 0.6700\n",
            "Epoch 130/300\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3789 - acc: 0.8468 - val_loss: 0.5657 - val_acc: 0.7100\n",
            "Epoch 131/300\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3681 - acc: 0.8419 - val_loss: 0.5916 - val_acc: 0.6900\n",
            "Epoch 132/300\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.4010 - acc: 0.8194 - val_loss: 0.6889 - val_acc: 0.6600\n",
            "Epoch 133/300\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3727 - acc: 0.8452 - val_loss: 0.5816 - val_acc: 0.6800\n",
            "Epoch 134/300\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3613 - acc: 0.8355 - val_loss: 0.4888 - val_acc: 0.7300\n",
            "Epoch 135/300\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3675 - acc: 0.8355 - val_loss: 0.5648 - val_acc: 0.7100\n",
            "Epoch 136/300\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3776 - acc: 0.8274 - val_loss: 0.6420 - val_acc: 0.6600\n",
            "Epoch 137/300\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3667 - acc: 0.8435 - val_loss: 0.5629 - val_acc: 0.7100\n",
            "Epoch 138/300\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3742 - acc: 0.8371 - val_loss: 0.5418 - val_acc: 0.7000\n",
            "Epoch 139/300\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3877 - acc: 0.8323 - val_loss: 0.5864 - val_acc: 0.7000\n",
            "Epoch 140/300\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3818 - acc: 0.8242 - val_loss: 0.5274 - val_acc: 0.7000\n",
            "Epoch 141/300\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3849 - acc: 0.8371 - val_loss: 0.6691 - val_acc: 0.6400\n",
            "Epoch 142/300\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3696 - acc: 0.8355 - val_loss: 0.5931 - val_acc: 0.6600\n",
            "Epoch 143/300\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3738 - acc: 0.8210 - val_loss: 0.5278 - val_acc: 0.7100\n",
            "Epoch 144/300\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3834 - acc: 0.8355 - val_loss: 0.6568 - val_acc: 0.6400\n",
            "Epoch 145/300\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3818 - acc: 0.8403 - val_loss: 0.5503 - val_acc: 0.7100\n",
            "Epoch 146/300\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3733 - acc: 0.8258 - val_loss: 0.5526 - val_acc: 0.7100\n",
            "Epoch 147/300\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3726 - acc: 0.8371 - val_loss: 0.5252 - val_acc: 0.7200\n",
            "Epoch 148/300\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3596 - acc: 0.8419 - val_loss: 0.5982 - val_acc: 0.6900\n",
            "Epoch 149/300\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3945 - acc: 0.8210 - val_loss: 0.5003 - val_acc: 0.7600\n",
            "Epoch 150/300\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.4037 - acc: 0.8097 - val_loss: 0.6161 - val_acc: 0.6800\n",
            "Epoch 151/300\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3991 - acc: 0.8145 - val_loss: 0.5664 - val_acc: 0.7200\n",
            "Epoch 152/300\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3946 - acc: 0.8129 - val_loss: 0.5491 - val_acc: 0.7300\n",
            "Epoch 153/300\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3959 - acc: 0.8065 - val_loss: 0.5666 - val_acc: 0.7100\n",
            "Epoch 154/300\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3628 - acc: 0.8387 - val_loss: 0.6951 - val_acc: 0.6600\n",
            "Epoch 155/300\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.4092 - acc: 0.8081 - val_loss: 0.5982 - val_acc: 0.6800\n",
            "Epoch 156/300\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3600 - acc: 0.8258 - val_loss: 0.6544 - val_acc: 0.6600\n",
            "Epoch 157/300\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3824 - acc: 0.8129 - val_loss: 0.5565 - val_acc: 0.6800\n",
            "Epoch 158/300\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3801 - acc: 0.8403 - val_loss: 0.5798 - val_acc: 0.6900\n",
            "Epoch 159/300\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3802 - acc: 0.8403 - val_loss: 0.6528 - val_acc: 0.6600\n",
            "Epoch 160/300\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3858 - acc: 0.8339 - val_loss: 0.6681 - val_acc: 0.6600\n",
            "Epoch 161/300\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3954 - acc: 0.8258 - val_loss: 0.6713 - val_acc: 0.6500\n",
            "Epoch 162/300\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3786 - acc: 0.8177 - val_loss: 0.6350 - val_acc: 0.6700\n",
            "Epoch 163/300\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3773 - acc: 0.8371 - val_loss: 0.5823 - val_acc: 0.7100\n",
            "Epoch 164/300\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3910 - acc: 0.8097 - val_loss: 0.5514 - val_acc: 0.7000\n",
            "Epoch 165/300\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3681 - acc: 0.8613 - val_loss: 0.5108 - val_acc: 0.7300\n",
            "Epoch 166/300\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3627 - acc: 0.8581 - val_loss: 0.5394 - val_acc: 0.7200\n",
            "Epoch 167/300\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3674 - acc: 0.8435 - val_loss: 0.5881 - val_acc: 0.7000\n",
            "Epoch 168/300\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3682 - acc: 0.8355 - val_loss: 0.4901 - val_acc: 0.7700\n",
            "Epoch 169/300\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3729 - acc: 0.8403 - val_loss: 0.5999 - val_acc: 0.7100\n",
            "Epoch 170/300\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3784 - acc: 0.8258 - val_loss: 0.5713 - val_acc: 0.7000\n",
            "Epoch 171/300\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3495 - acc: 0.8387 - val_loss: 0.5081 - val_acc: 0.7400\n",
            "Epoch 172/300\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3563 - acc: 0.8468 - val_loss: 0.5498 - val_acc: 0.7100\n",
            "Epoch 173/300\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.4015 - acc: 0.8161 - val_loss: 0.6470 - val_acc: 0.6500\n",
            "Epoch 174/300\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3797 - acc: 0.8323 - val_loss: 0.5239 - val_acc: 0.7400\n",
            "Epoch 175/300\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3873 - acc: 0.8194 - val_loss: 0.5327 - val_acc: 0.7200\n",
            "Epoch 176/300\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3680 - acc: 0.8371 - val_loss: 0.6378 - val_acc: 0.6500\n",
            "Epoch 177/300\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3751 - acc: 0.8387 - val_loss: 0.6497 - val_acc: 0.6500\n",
            "Epoch 178/300\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3781 - acc: 0.8194 - val_loss: 0.6315 - val_acc: 0.6600\n",
            "Epoch 179/300\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3451 - acc: 0.8435 - val_loss: 0.5743 - val_acc: 0.7100\n",
            "Epoch 180/300\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3733 - acc: 0.8242 - val_loss: 0.5549 - val_acc: 0.7200\n",
            "Epoch 181/300\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3844 - acc: 0.8274 - val_loss: 0.6022 - val_acc: 0.6800\n",
            "Epoch 182/300\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3742 - acc: 0.8274 - val_loss: 0.5487 - val_acc: 0.7000\n",
            "Epoch 183/300\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3763 - acc: 0.8081 - val_loss: 0.5732 - val_acc: 0.7100\n",
            "Epoch 184/300\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3545 - acc: 0.8387 - val_loss: 0.5835 - val_acc: 0.7000\n",
            "Epoch 185/300\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3640 - acc: 0.8387 - val_loss: 0.5147 - val_acc: 0.7300\n",
            "Epoch 186/300\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3532 - acc: 0.8484 - val_loss: 0.5829 - val_acc: 0.7100\n",
            "Epoch 187/300\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3707 - acc: 0.8177 - val_loss: 0.5690 - val_acc: 0.7000\n",
            "Epoch 188/300\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3658 - acc: 0.8306 - val_loss: 0.8328 - val_acc: 0.6400\n",
            "Epoch 189/300\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3661 - acc: 0.8419 - val_loss: 0.6816 - val_acc: 0.6500\n",
            "Epoch 190/300\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3461 - acc: 0.8403 - val_loss: 0.6237 - val_acc: 0.6800\n",
            "Epoch 191/300\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3833 - acc: 0.8194 - val_loss: 0.5905 - val_acc: 0.6800\n",
            "Epoch 192/300\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3750 - acc: 0.8306 - val_loss: 0.6130 - val_acc: 0.6800\n",
            "Epoch 193/300\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3670 - acc: 0.8565 - val_loss: 0.5229 - val_acc: 0.7100\n",
            "Epoch 194/300\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3578 - acc: 0.8371 - val_loss: 0.5683 - val_acc: 0.7100\n",
            "Epoch 195/300\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3505 - acc: 0.8387 - val_loss: 0.5518 - val_acc: 0.7200\n",
            "Epoch 196/300\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3583 - acc: 0.8565 - val_loss: 0.5839 - val_acc: 0.6700\n",
            "Epoch 197/300\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3760 - acc: 0.8274 - val_loss: 0.6051 - val_acc: 0.6700\n",
            "Epoch 198/300\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3713 - acc: 0.8258 - val_loss: 0.5333 - val_acc: 0.7000\n",
            "Epoch 199/300\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3678 - acc: 0.8548 - val_loss: 0.6520 - val_acc: 0.6700\n",
            "Epoch 200/300\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3723 - acc: 0.8177 - val_loss: 0.5896 - val_acc: 0.6700\n",
            "Epoch 201/300\n",
            "\n",
            "Epoch 00201: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3422 - acc: 0.8597 - val_loss: 0.6615 - val_acc: 0.6700\n",
            "Epoch 202/300\n",
            "\n",
            "Epoch 00202: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3537 - acc: 0.8419 - val_loss: 0.7394 - val_acc: 0.6300\n",
            "Epoch 203/300\n",
            "\n",
            "Epoch 00203: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3721 - acc: 0.8387 - val_loss: 0.6497 - val_acc: 0.6500\n",
            "Epoch 204/300\n",
            "\n",
            "Epoch 00204: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3640 - acc: 0.8290 - val_loss: 0.6941 - val_acc: 0.6600\n",
            "Epoch 205/300\n",
            "\n",
            "Epoch 00205: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3631 - acc: 0.8339 - val_loss: 0.6212 - val_acc: 0.6600\n",
            "Epoch 206/300\n",
            "\n",
            "Epoch 00206: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3788 - acc: 0.8355 - val_loss: 0.7038 - val_acc: 0.6600\n",
            "Epoch 207/300\n",
            "\n",
            "Epoch 00207: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3589 - acc: 0.8355 - val_loss: 0.6787 - val_acc: 0.6500\n",
            "Epoch 208/300\n",
            "\n",
            "Epoch 00208: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3655 - acc: 0.8371 - val_loss: 0.6657 - val_acc: 0.6600\n",
            "Epoch 209/300\n",
            "\n",
            "Epoch 00209: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3637 - acc: 0.8355 - val_loss: 0.6234 - val_acc: 0.6700\n",
            "Epoch 210/300\n",
            "\n",
            "Epoch 00210: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3677 - acc: 0.8323 - val_loss: 0.5496 - val_acc: 0.6900\n",
            "Epoch 211/300\n",
            "\n",
            "Epoch 00211: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3674 - acc: 0.8290 - val_loss: 0.7955 - val_acc: 0.6400\n",
            "Epoch 212/300\n",
            "\n",
            "Epoch 00212: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3550 - acc: 0.8371 - val_loss: 0.5630 - val_acc: 0.6900\n",
            "Epoch 213/300\n",
            "\n",
            "Epoch 00213: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3584 - acc: 0.8419 - val_loss: 0.6745 - val_acc: 0.6500\n",
            "Epoch 214/300\n",
            "\n",
            "Epoch 00214: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3504 - acc: 0.8581 - val_loss: 0.6145 - val_acc: 0.6900\n",
            "Epoch 215/300\n",
            "\n",
            "Epoch 00215: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3410 - acc: 0.8484 - val_loss: 0.5902 - val_acc: 0.7000\n",
            "Epoch 216/300\n",
            "\n",
            "Epoch 00216: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3400 - acc: 0.8581 - val_loss: 0.5798 - val_acc: 0.7000\n",
            "Epoch 217/300\n",
            "\n",
            "Epoch 00217: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3400 - acc: 0.8468 - val_loss: 0.7469 - val_acc: 0.6400\n",
            "Epoch 218/300\n",
            "\n",
            "Epoch 00218: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3828 - acc: 0.8387 - val_loss: 0.6576 - val_acc: 0.6800\n",
            "Epoch 219/300\n",
            "\n",
            "Epoch 00219: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3540 - acc: 0.8435 - val_loss: 0.6765 - val_acc: 0.6600\n",
            "Epoch 220/300\n",
            "\n",
            "Epoch 00220: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3492 - acc: 0.8500 - val_loss: 0.7248 - val_acc: 0.6500\n",
            "Epoch 221/300\n",
            "\n",
            "Epoch 00221: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3746 - acc: 0.8419 - val_loss: 0.5500 - val_acc: 0.6900\n",
            "Epoch 222/300\n",
            "\n",
            "Epoch 00222: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3543 - acc: 0.8468 - val_loss: 0.6055 - val_acc: 0.7000\n",
            "Epoch 223/300\n",
            "\n",
            "Epoch 00223: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3847 - acc: 0.8113 - val_loss: 0.5557 - val_acc: 0.6900\n",
            "Epoch 224/300\n",
            "\n",
            "Epoch 00224: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3633 - acc: 0.8387 - val_loss: 0.5828 - val_acc: 0.7000\n",
            "Epoch 225/300\n",
            "\n",
            "Epoch 00225: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3542 - acc: 0.8516 - val_loss: 0.6935 - val_acc: 0.6600\n",
            "Epoch 226/300\n",
            "\n",
            "Epoch 00226: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3499 - acc: 0.8581 - val_loss: 0.6746 - val_acc: 0.6600\n",
            "Epoch 227/300\n",
            "\n",
            "Epoch 00227: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3608 - acc: 0.8419 - val_loss: 0.5588 - val_acc: 0.7000\n",
            "Epoch 228/300\n",
            "\n",
            "Epoch 00228: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3744 - acc: 0.8339 - val_loss: 0.5707 - val_acc: 0.7000\n",
            "Epoch 229/300\n",
            "\n",
            "Epoch 00229: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3648 - acc: 0.8387 - val_loss: 0.5675 - val_acc: 0.7200\n",
            "Epoch 230/300\n",
            "\n",
            "Epoch 00230: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3532 - acc: 0.8500 - val_loss: 0.5714 - val_acc: 0.6900\n",
            "Epoch 231/300\n",
            "\n",
            "Epoch 00231: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3675 - acc: 0.8532 - val_loss: 0.5922 - val_acc: 0.6800\n",
            "Epoch 232/300\n",
            "\n",
            "Epoch 00232: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3705 - acc: 0.8339 - val_loss: 0.6643 - val_acc: 0.6800\n",
            "Epoch 233/300\n",
            "\n",
            "Epoch 00233: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3350 - acc: 0.8484 - val_loss: 0.4919 - val_acc: 0.7400\n",
            "Epoch 234/300\n",
            "\n",
            "Epoch 00234: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3610 - acc: 0.8371 - val_loss: 0.5676 - val_acc: 0.6900\n",
            "Epoch 235/300\n",
            "\n",
            "Epoch 00235: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3554 - acc: 0.8532 - val_loss: 0.6903 - val_acc: 0.6700\n",
            "Epoch 236/300\n",
            "\n",
            "Epoch 00236: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3461 - acc: 0.8516 - val_loss: 0.7474 - val_acc: 0.6300\n",
            "Epoch 237/300\n",
            "\n",
            "Epoch 00237: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3631 - acc: 0.8323 - val_loss: 0.7895 - val_acc: 0.6400\n",
            "Epoch 238/300\n",
            "\n",
            "Epoch 00238: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3456 - acc: 0.8516 - val_loss: 0.6401 - val_acc: 0.6900\n",
            "Epoch 239/300\n",
            "\n",
            "Epoch 00239: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3574 - acc: 0.8516 - val_loss: 0.5322 - val_acc: 0.7100\n",
            "Epoch 240/300\n",
            "\n",
            "Epoch 00240: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3382 - acc: 0.8677 - val_loss: 0.6567 - val_acc: 0.7000\n",
            "Epoch 241/300\n",
            "\n",
            "Epoch 00241: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3648 - acc: 0.8258 - val_loss: 0.7492 - val_acc: 0.6500\n",
            "Epoch 242/300\n",
            "\n",
            "Epoch 00242: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3742 - acc: 0.8339 - val_loss: 0.5169 - val_acc: 0.7100\n",
            "Epoch 243/300\n",
            "\n",
            "Epoch 00243: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3833 - acc: 0.8258 - val_loss: 0.5711 - val_acc: 0.7000\n",
            "Epoch 244/300\n",
            "\n",
            "Epoch 00244: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3769 - acc: 0.8242 - val_loss: 0.5847 - val_acc: 0.7000\n",
            "Epoch 245/300\n",
            "\n",
            "Epoch 00245: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3672 - acc: 0.8452 - val_loss: 0.6014 - val_acc: 0.7000\n",
            "Epoch 246/300\n",
            "\n",
            "Epoch 00246: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3603 - acc: 0.8419 - val_loss: 0.5446 - val_acc: 0.6900\n",
            "Epoch 247/300\n",
            "\n",
            "Epoch 00247: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3763 - acc: 0.8258 - val_loss: 0.6005 - val_acc: 0.6800\n",
            "Epoch 248/300\n",
            "\n",
            "Epoch 00248: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3692 - acc: 0.8484 - val_loss: 0.5870 - val_acc: 0.7000\n",
            "Epoch 249/300\n",
            "\n",
            "Epoch 00249: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3474 - acc: 0.8548 - val_loss: 0.5994 - val_acc: 0.7000\n",
            "Epoch 250/300\n",
            "\n",
            "Epoch 00250: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3496 - acc: 0.8484 - val_loss: 0.5958 - val_acc: 0.6800\n",
            "Epoch 251/300\n",
            "\n",
            "Epoch 00251: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3632 - acc: 0.8371 - val_loss: 0.6642 - val_acc: 0.6500\n",
            "Epoch 252/300\n",
            "\n",
            "Epoch 00252: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3765 - acc: 0.8258 - val_loss: 0.6357 - val_acc: 0.6600\n",
            "Epoch 253/300\n",
            "\n",
            "Epoch 00253: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3651 - acc: 0.8387 - val_loss: 0.5906 - val_acc: 0.7000\n",
            "Epoch 254/300\n",
            "\n",
            "Epoch 00254: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3667 - acc: 0.8323 - val_loss: 0.4863 - val_acc: 0.7100\n",
            "Epoch 255/300\n",
            "\n",
            "Epoch 00255: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3696 - acc: 0.8565 - val_loss: 0.5706 - val_acc: 0.7000\n",
            "Epoch 256/300\n",
            "\n",
            "Epoch 00256: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3671 - acc: 0.8452 - val_loss: 0.7995 - val_acc: 0.6400\n",
            "Epoch 257/300\n",
            "\n",
            "Epoch 00257: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3741 - acc: 0.8403 - val_loss: 0.7343 - val_acc: 0.6300\n",
            "Epoch 258/300\n",
            "\n",
            "Epoch 00258: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3674 - acc: 0.8290 - val_loss: 0.5990 - val_acc: 0.6700\n",
            "Epoch 259/300\n",
            "\n",
            "Epoch 00259: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3564 - acc: 0.8484 - val_loss: 0.6542 - val_acc: 0.6600\n",
            "Epoch 260/300\n",
            "\n",
            "Epoch 00260: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3517 - acc: 0.8452 - val_loss: 0.6530 - val_acc: 0.6700\n",
            "Epoch 261/300\n",
            "\n",
            "Epoch 00261: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3572 - acc: 0.8355 - val_loss: 0.6343 - val_acc: 0.6600\n",
            "Epoch 262/300\n",
            "\n",
            "Epoch 00262: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3517 - acc: 0.8242 - val_loss: 0.6110 - val_acc: 0.6600\n",
            "Epoch 263/300\n",
            "\n",
            "Epoch 00263: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3506 - acc: 0.8452 - val_loss: 0.6342 - val_acc: 0.6700\n",
            "Epoch 264/300\n",
            "\n",
            "Epoch 00264: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3640 - acc: 0.8258 - val_loss: 0.5854 - val_acc: 0.7000\n",
            "Epoch 265/300\n",
            "\n",
            "Epoch 00265: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3498 - acc: 0.8435 - val_loss: 0.6382 - val_acc: 0.6900\n",
            "Epoch 266/300\n",
            "\n",
            "Epoch 00266: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3369 - acc: 0.8677 - val_loss: 0.6080 - val_acc: 0.7000\n",
            "Epoch 267/300\n",
            "\n",
            "Epoch 00267: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3381 - acc: 0.8565 - val_loss: 0.4979 - val_acc: 0.7400\n",
            "Epoch 268/300\n",
            "\n",
            "Epoch 00268: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3572 - acc: 0.8452 - val_loss: 0.5459 - val_acc: 0.7200\n",
            "Epoch 269/300\n",
            "\n",
            "Epoch 00269: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3404 - acc: 0.8452 - val_loss: 0.6673 - val_acc: 0.6500\n",
            "Epoch 270/300\n",
            "\n",
            "Epoch 00270: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3536 - acc: 0.8500 - val_loss: 0.6508 - val_acc: 0.6700\n",
            "Epoch 271/300\n",
            "\n",
            "Epoch 00271: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3188 - acc: 0.8774 - val_loss: 0.4691 - val_acc: 0.7600\n",
            "Epoch 272/300\n",
            "\n",
            "Epoch 00272: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3332 - acc: 0.8581 - val_loss: 0.6162 - val_acc: 0.6700\n",
            "Epoch 273/300\n",
            "\n",
            "Epoch 00273: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3377 - acc: 0.8645 - val_loss: 0.6364 - val_acc: 0.6600\n",
            "Epoch 274/300\n",
            "\n",
            "Epoch 00274: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3273 - acc: 0.8726 - val_loss: 0.6335 - val_acc: 0.6700\n",
            "Epoch 275/300\n",
            "\n",
            "Epoch 00275: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3321 - acc: 0.8597 - val_loss: 0.5873 - val_acc: 0.7000\n",
            "Epoch 276/300\n",
            "\n",
            "Epoch 00276: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3550 - acc: 0.8516 - val_loss: 0.5109 - val_acc: 0.7200\n",
            "Epoch 277/300\n",
            "\n",
            "Epoch 00277: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3507 - acc: 0.8419 - val_loss: 0.5362 - val_acc: 0.7200\n",
            "Epoch 278/300\n",
            "\n",
            "Epoch 00278: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3366 - acc: 0.8258 - val_loss: 0.7927 - val_acc: 0.6300\n",
            "Epoch 279/300\n",
            "\n",
            "Epoch 00279: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3564 - acc: 0.8532 - val_loss: 0.5390 - val_acc: 0.7300\n",
            "Epoch 280/300\n",
            "\n",
            "Epoch 00280: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3797 - acc: 0.8226 - val_loss: 0.5648 - val_acc: 0.6900\n",
            "Epoch 281/300\n",
            "\n",
            "Epoch 00281: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3456 - acc: 0.8597 - val_loss: 0.6024 - val_acc: 0.7000\n",
            "Epoch 282/300\n",
            "\n",
            "Epoch 00282: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3536 - acc: 0.8548 - val_loss: 0.4988 - val_acc: 0.7300\n",
            "Epoch 283/300\n",
            "\n",
            "Epoch 00283: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3396 - acc: 0.8403 - val_loss: 0.7068 - val_acc: 0.6500\n",
            "Epoch 284/300\n",
            "\n",
            "Epoch 00284: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3224 - acc: 0.8758 - val_loss: 0.5801 - val_acc: 0.7100\n",
            "Epoch 285/300\n",
            "\n",
            "Epoch 00285: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3454 - acc: 0.8532 - val_loss: 0.6089 - val_acc: 0.6900\n",
            "Epoch 286/300\n",
            "\n",
            "Epoch 00286: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3565 - acc: 0.8435 - val_loss: 0.5865 - val_acc: 0.7000\n",
            "Epoch 287/300\n",
            "\n",
            "Epoch 00287: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3552 - acc: 0.8484 - val_loss: 0.5417 - val_acc: 0.7000\n",
            "Epoch 288/300\n",
            "\n",
            "Epoch 00288: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3310 - acc: 0.8677 - val_loss: 0.5562 - val_acc: 0.7300\n",
            "Epoch 289/300\n",
            "\n",
            "Epoch 00289: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3106 - acc: 0.8661 - val_loss: 0.7056 - val_acc: 0.6700\n",
            "Epoch 290/300\n",
            "\n",
            "Epoch 00290: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3420 - acc: 0.8452 - val_loss: 0.5916 - val_acc: 0.6900\n",
            "Epoch 291/300\n",
            "\n",
            "Epoch 00291: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3406 - acc: 0.8500 - val_loss: 0.6022 - val_acc: 0.7000\n",
            "Epoch 292/300\n",
            "\n",
            "Epoch 00292: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3353 - acc: 0.8484 - val_loss: 0.5981 - val_acc: 0.6800\n",
            "Epoch 293/300\n",
            "\n",
            "Epoch 00293: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3456 - acc: 0.8387 - val_loss: 0.5546 - val_acc: 0.7200\n",
            "Epoch 294/300\n",
            "\n",
            "Epoch 00294: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3561 - acc: 0.8468 - val_loss: 0.5951 - val_acc: 0.7000\n",
            "Epoch 295/300\n",
            "\n",
            "Epoch 00295: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3644 - acc: 0.8355 - val_loss: 0.6393 - val_acc: 0.6700\n",
            "Epoch 296/300\n",
            "\n",
            "Epoch 00296: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3323 - acc: 0.8581 - val_loss: 0.5905 - val_acc: 0.6900\n",
            "Epoch 297/300\n",
            "\n",
            "Epoch 00297: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3561 - acc: 0.8500 - val_loss: 0.5712 - val_acc: 0.6900\n",
            "Epoch 298/300\n",
            "\n",
            "Epoch 00298: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3629 - acc: 0.8500 - val_loss: 0.5399 - val_acc: 0.7000\n",
            "Epoch 299/300\n",
            "\n",
            "Epoch 00299: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3637 - acc: 0.8484 - val_loss: 0.7724 - val_acc: 0.6500\n",
            "Epoch 300/300\n",
            "\n",
            "Epoch 00300: val_loss did not improve from 0.43384\n",
            "620/620 - 1s - loss: 0.3275 - acc: 0.8645 - val_loss: 0.5742 - val_acc: 0.6800\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIzgiZ9mqCwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "# Evaluate on test data\n",
            "40/40 [==============================] - 0s 3ms/sample - loss: 0.9740 - acc: 0.5000\n",
            "test loss, test acc: [0.9740450734272599, 0.5]\n",
            "1\n",
            "EEG_Deep/Data2A/parsed_P01T.mat\n",
            "(0, 12, 4096)\n",
            "(80, 12, 4096)\n",
            "(80, 1)\n",
            "2\n",
            "EEG_Deep/Data2A/parsed_P02T.mat\n",
            "(80, 12, 4096)\n",
            "(160, 12, 4096)\n",
            "(160, 1)\n",
            "3\n",
            "EEG_Deep/Data2A/parsed_P03T.mat\n",
            "(160, 12, 4096)\n",
            "(240, 12, 4096)\n",
            "(240, 1)\n",
            "4\n",
            "EEG_Deep/Data2A/parsed_P04T.mat\n",
            "(240, 12, 4096)\n",
            "(320, 12, 4096)\n",
            "(320, 1)\n",
            "6\n",
            "EEG_Deep/Data2A/parsed_P06T.mat\n",
            "(320, 12, 4096)\n",
            "(400, 12, 4096)\n",
            "(400, 1)\n",
            "7\n",
            "EEG_Deep/Data2A/parsed_P07T.mat\n",
            "(400, 12, 4096)\n",
            "(480, 12, 4096)\n",
            "(480, 1)\n",
            "8\n",
            "EEG_Deep/Data2A/parsed_P08T.mat\n",
            "(480, 12, 4096)\n",
            "(560, 12, 4096)\n",
            "(560, 1)\n",
            "9\n",
            "EEG_Deep/Data2A/parsed_P09T.mat\n",
            "(560, 12, 4096)\n",
            "(640, 12, 4096)\n",
            "(640, 1)\n",
            "10\n",
            "EEG_Deep/Data2A/parsed_P010T.mat\n",
            "(640, 12, 4096)\n",
            "(720, 12, 4096)\n",
            "(720, 1)\n",
            "5\n",
            "EEG_Deep/Data2A/parsed_P05E.mat\n",
            "(0, 12, 4096)\n",
            "(40, 12, 4096)\n",
            "(40, 1)\n",
            "Filtering of training data in progress\n",
            "(720, 12, 4096)\n",
            "Filtering of test data in progress\n",
            "(40, 12, 4096)\n",
            "[1. 2. 1. 1. 1. 1. 2. 1. 1. 1. 2. 2. 2. 2. 1. 1. 2. 2. 1. 2. 2. 2. 2. 2.\n",
            " 1. 2. 2. 2. 1. 1. 1. 1. 1. 2. 1. 1. 2. 1. 1. 2. 1. 1. 1. 1. 2. 1. 2. 1.\n",
            " 2. 2. 2. 2. 2. 1. 1. 1. 2. 2. 1. 1. 2. 1. 1. 1. 1. 2. 1. 2. 2. 1. 2. 2.\n",
            " 1. 1. 1. 2. 1. 1. 2. 2. 1. 1. 2. 2. 1. 2. 1. 1. 1. 2. 1. 1. 1. 2. 1. 1.\n",
            " 2. 2. 1. 2.]\n",
            "(620, 12, 1536)\n",
            "(620,)\n",
            "(100, 12, 1536)\n",
            "(100,)\n",
            "[[1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]]\n",
            "X_train shape: (620, 1, 12, 1536)\n",
            "620 train samples\n",
            "100 val samples\n",
            "(40, 12, 1536)\n",
            "(40, 1)\n",
            "X_train shape: (40, 1, 12, 1536)\n",
            "40 train samples\n",
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 1, 12, 1536)]     0         \n",
            "_________________________________________________________________\n",
            "conv2d (Conv2D)              (None, 8, 12, 1536)       200       \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 8, 12, 1536)       32        \n",
            "_________________________________________________________________\n",
            "depthwise_conv2d (DepthwiseC (None, 16, 1, 1536)       192       \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 16, 1, 1536)       64        \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 16, 1, 1536)       0         \n",
            "_________________________________________________________________\n",
            "average_pooling2d (AveragePo (None, 16, 1, 384)        0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 16, 1, 384)        0         \n",
            "_________________________________________________________________\n",
            "separable_conv2d (SeparableC (None, 16, 1, 384)        512       \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 16, 1, 384)        64        \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 16, 1, 384)        0         \n",
            "_________________________________________________________________\n",
            "average_pooling2d_1 (Average (None, 16, 1, 48)         0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 16, 1, 48)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 768)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 2)                 1538      \n",
            "_________________________________________________________________\n",
            "softmax (Activation)         (None, 2)                 0         \n",
            "=================================================================\n",
            "Total params: 2,602\n",
            "Trainable params: 2,522\n",
            "Non-trainable params: 80\n",
            "_________________________________________________________________\n",
            "Train on 620 samples, validate on 100 samples\n",
            "Epoch 1/300\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.69369, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.6996 - acc: 0.4806 - val_loss: 0.6937 - val_acc: 0.4900\n",
            "Epoch 2/300\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.69369\n",
            "620/620 - 1s - loss: 0.6776 - acc: 0.5661 - val_loss: 0.6940 - val_acc: 0.4900\n",
            "Epoch 3/300\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.69369\n",
            "620/620 - 1s - loss: 0.6540 - acc: 0.6581 - val_loss: 0.6968 - val_acc: 0.4500\n",
            "Epoch 4/300\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.69369 to 0.69164, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.6335 - acc: 0.6516 - val_loss: 0.6916 - val_acc: 0.4500\n",
            "Epoch 5/300\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.69164 to 0.68065, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.6154 - acc: 0.6774 - val_loss: 0.6807 - val_acc: 0.4700\n",
            "Epoch 6/300\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.68065 to 0.66215, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.5996 - acc: 0.6790 - val_loss: 0.6621 - val_acc: 0.5400\n",
            "Epoch 7/300\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.66215\n",
            "620/620 - 1s - loss: 0.5879 - acc: 0.6968 - val_loss: 0.6893 - val_acc: 0.4600\n",
            "Epoch 8/300\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.66215\n",
            "620/620 - 1s - loss: 0.5920 - acc: 0.6952 - val_loss: 0.7007 - val_acc: 0.4600\n",
            "Epoch 9/300\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.66215 to 0.65904, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.5540 - acc: 0.7258 - val_loss: 0.6590 - val_acc: 0.5100\n",
            "Epoch 10/300\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.65904\n",
            "620/620 - 1s - loss: 0.5399 - acc: 0.7484 - val_loss: 0.7757 - val_acc: 0.4500\n",
            "Epoch 11/300\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.65904\n",
            "620/620 - 1s - loss: 0.5279 - acc: 0.7161 - val_loss: 0.6988 - val_acc: 0.4900\n",
            "Epoch 12/300\n",
            "\n",
            "Epoch 00012: val_loss improved from 0.65904 to 0.60568, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.5424 - acc: 0.7129 - val_loss: 0.6057 - val_acc: 0.6400\n",
            "Epoch 13/300\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.60568\n",
            "620/620 - 1s - loss: 0.5320 - acc: 0.7532 - val_loss: 0.6510 - val_acc: 0.5800\n",
            "Epoch 14/300\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.60568\n",
            "620/620 - 1s - loss: 0.5334 - acc: 0.7226 - val_loss: 0.6271 - val_acc: 0.5900\n",
            "Epoch 15/300\n",
            "\n",
            "Epoch 00015: val_loss improved from 0.60568 to 0.59861, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.5310 - acc: 0.7355 - val_loss: 0.5986 - val_acc: 0.6200\n",
            "Epoch 16/300\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.59861\n",
            "620/620 - 1s - loss: 0.5194 - acc: 0.7500 - val_loss: 0.6426 - val_acc: 0.5800\n",
            "Epoch 17/300\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.59861\n",
            "620/620 - 1s - loss: 0.5209 - acc: 0.7419 - val_loss: 0.6246 - val_acc: 0.6200\n",
            "Epoch 18/300\n",
            "\n",
            "Epoch 00018: val_loss improved from 0.59861 to 0.57659, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.5267 - acc: 0.7274 - val_loss: 0.5766 - val_acc: 0.6800\n",
            "Epoch 19/300\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.57659\n",
            "620/620 - 1s - loss: 0.5133 - acc: 0.7468 - val_loss: 0.5875 - val_acc: 0.6600\n",
            "Epoch 20/300\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.57659\n",
            "620/620 - 1s - loss: 0.5037 - acc: 0.7629 - val_loss: 0.5954 - val_acc: 0.6600\n",
            "Epoch 21/300\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.57659\n",
            "620/620 - 1s - loss: 0.5088 - acc: 0.7484 - val_loss: 0.6490 - val_acc: 0.5700\n",
            "Epoch 22/300\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.57659\n",
            "620/620 - 1s - loss: 0.5117 - acc: 0.7468 - val_loss: 0.6212 - val_acc: 0.6200\n",
            "Epoch 23/300\n",
            "\n",
            "Epoch 00023: val_loss improved from 0.57659 to 0.53694, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.5187 - acc: 0.7435 - val_loss: 0.5369 - val_acc: 0.7400\n",
            "Epoch 24/300\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.53694\n",
            "620/620 - 1s - loss: 0.4997 - acc: 0.7452 - val_loss: 0.5924 - val_acc: 0.6600\n",
            "Epoch 25/300\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.53694\n",
            "620/620 - 1s - loss: 0.5064 - acc: 0.7516 - val_loss: 0.5432 - val_acc: 0.7500\n",
            "Epoch 26/300\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.53694\n",
            "620/620 - 1s - loss: 0.5005 - acc: 0.7516 - val_loss: 0.5741 - val_acc: 0.6600\n",
            "Epoch 27/300\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.53694\n",
            "620/620 - 1s - loss: 0.4894 - acc: 0.7565 - val_loss: 0.6241 - val_acc: 0.6300\n",
            "Epoch 28/300\n",
            "\n",
            "Epoch 00028: val_loss improved from 0.53694 to 0.50823, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.5085 - acc: 0.7419 - val_loss: 0.5082 - val_acc: 0.7700\n",
            "Epoch 29/300\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.50823\n",
            "620/620 - 1s - loss: 0.4942 - acc: 0.7274 - val_loss: 0.6071 - val_acc: 0.6600\n",
            "Epoch 30/300\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.50823\n",
            "620/620 - 1s - loss: 0.5039 - acc: 0.7419 - val_loss: 0.5889 - val_acc: 0.6700\n",
            "Epoch 31/300\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.50823\n",
            "620/620 - 1s - loss: 0.4777 - acc: 0.7919 - val_loss: 0.6086 - val_acc: 0.6600\n",
            "Epoch 32/300\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.50823\n",
            "620/620 - 1s - loss: 0.4748 - acc: 0.7645 - val_loss: 0.6289 - val_acc: 0.6400\n",
            "Epoch 33/300\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.50823\n",
            "620/620 - 1s - loss: 0.4968 - acc: 0.7435 - val_loss: 0.6117 - val_acc: 0.6500\n",
            "Epoch 34/300\n",
            "\n",
            "Epoch 00034: val_loss improved from 0.50823 to 0.50668, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.4828 - acc: 0.7694 - val_loss: 0.5067 - val_acc: 0.7500\n",
            "Epoch 35/300\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.50668\n",
            "620/620 - 1s - loss: 0.4758 - acc: 0.7823 - val_loss: 0.5417 - val_acc: 0.7100\n",
            "Epoch 36/300\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.50668\n",
            "620/620 - 1s - loss: 0.4730 - acc: 0.7742 - val_loss: 0.5802 - val_acc: 0.6700\n",
            "Epoch 37/300\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.50668\n",
            "620/620 - 1s - loss: 0.4855 - acc: 0.7823 - val_loss: 0.5997 - val_acc: 0.6400\n",
            "Epoch 38/300\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.50668\n",
            "620/620 - 1s - loss: 0.4793 - acc: 0.7565 - val_loss: 0.5341 - val_acc: 0.7300\n",
            "Epoch 39/300\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.50668\n",
            "620/620 - 1s - loss: 0.5020 - acc: 0.7565 - val_loss: 0.5482 - val_acc: 0.7200\n",
            "Epoch 40/300\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.50668\n",
            "620/620 - 1s - loss: 0.5015 - acc: 0.7581 - val_loss: 0.5527 - val_acc: 0.6800\n",
            "Epoch 41/300\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.50668\n",
            "620/620 - 1s - loss: 0.4511 - acc: 0.7919 - val_loss: 0.6414 - val_acc: 0.6100\n",
            "Epoch 42/300\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.50668\n",
            "620/620 - 1s - loss: 0.4659 - acc: 0.7694 - val_loss: 0.6452 - val_acc: 0.5900\n",
            "Epoch 43/300\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.50668\n",
            "620/620 - 1s - loss: 0.4786 - acc: 0.7500 - val_loss: 0.5978 - val_acc: 0.6400\n",
            "Epoch 44/300\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.50668\n",
            "620/620 - 1s - loss: 0.4704 - acc: 0.7629 - val_loss: 0.5957 - val_acc: 0.6500\n",
            "Epoch 45/300\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.50668\n",
            "620/620 - 1s - loss: 0.4575 - acc: 0.7806 - val_loss: 0.5306 - val_acc: 0.7100\n",
            "Epoch 46/300\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.50668\n",
            "620/620 - 1s - loss: 0.4640 - acc: 0.7790 - val_loss: 0.5296 - val_acc: 0.7400\n",
            "Epoch 47/300\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.50668\n",
            "620/620 - 1s - loss: 0.4648 - acc: 0.7645 - val_loss: 0.5455 - val_acc: 0.7300\n",
            "Epoch 48/300\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.50668\n",
            "620/620 - 1s - loss: 0.4615 - acc: 0.7645 - val_loss: 0.6273 - val_acc: 0.6400\n",
            "Epoch 49/300\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.50668\n",
            "620/620 - 1s - loss: 0.4608 - acc: 0.7758 - val_loss: 0.5784 - val_acc: 0.6600\n",
            "Epoch 50/300\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.50668\n",
            "620/620 - 1s - loss: 0.4686 - acc: 0.7710 - val_loss: 0.5578 - val_acc: 0.6900\n",
            "Epoch 51/300\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.50668\n",
            "620/620 - 1s - loss: 0.4515 - acc: 0.7677 - val_loss: 0.5730 - val_acc: 0.6600\n",
            "Epoch 52/300\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.50668\n",
            "620/620 - 1s - loss: 0.4872 - acc: 0.7532 - val_loss: 0.5783 - val_acc: 0.6600\n",
            "Epoch 53/300\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.50668\n",
            "620/620 - 1s - loss: 0.4716 - acc: 0.7855 - val_loss: 0.5287 - val_acc: 0.7200\n",
            "Epoch 54/300\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.50668\n",
            "620/620 - 1s - loss: 0.4603 - acc: 0.7500 - val_loss: 0.6859 - val_acc: 0.6100\n",
            "Epoch 55/300\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.50668\n",
            "620/620 - 1s - loss: 0.4489 - acc: 0.7774 - val_loss: 0.6417 - val_acc: 0.6400\n",
            "Epoch 56/300\n",
            "\n",
            "Epoch 00056: val_loss improved from 0.50668 to 0.49884, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.4595 - acc: 0.7919 - val_loss: 0.4988 - val_acc: 0.7600\n",
            "Epoch 57/300\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.49884\n",
            "620/620 - 1s - loss: 0.4406 - acc: 0.7952 - val_loss: 0.5546 - val_acc: 0.6900\n",
            "Epoch 58/300\n",
            "\n",
            "Epoch 00058: val_loss improved from 0.49884 to 0.49740, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.4455 - acc: 0.7903 - val_loss: 0.4974 - val_acc: 0.7300\n",
            "Epoch 59/300\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.49740\n",
            "620/620 - 1s - loss: 0.4547 - acc: 0.7758 - val_loss: 0.5198 - val_acc: 0.7300\n",
            "Epoch 60/300\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.49740\n",
            "620/620 - 1s - loss: 0.4746 - acc: 0.7742 - val_loss: 0.6115 - val_acc: 0.6100\n",
            "Epoch 61/300\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.49740\n",
            "620/620 - 1s - loss: 0.4414 - acc: 0.7952 - val_loss: 0.4998 - val_acc: 0.7300\n",
            "Epoch 62/300\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.49740\n",
            "620/620 - 1s - loss: 0.4132 - acc: 0.8194 - val_loss: 0.5297 - val_acc: 0.7300\n",
            "Epoch 63/300\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.49740\n",
            "620/620 - 1s - loss: 0.4305 - acc: 0.8016 - val_loss: 0.5291 - val_acc: 0.7100\n",
            "Epoch 64/300\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.49740\n",
            "620/620 - 1s - loss: 0.4453 - acc: 0.7839 - val_loss: 0.5249 - val_acc: 0.7100\n",
            "Epoch 65/300\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.49740\n",
            "620/620 - 1s - loss: 0.4387 - acc: 0.7855 - val_loss: 0.5727 - val_acc: 0.6400\n",
            "Epoch 66/300\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.49740\n",
            "620/620 - 1s - loss: 0.4452 - acc: 0.7903 - val_loss: 0.5759 - val_acc: 0.6500\n",
            "Epoch 67/300\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.49740\n",
            "620/620 - 1s - loss: 0.4611 - acc: 0.7758 - val_loss: 0.5407 - val_acc: 0.6500\n",
            "Epoch 68/300\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.49740\n",
            "620/620 - 1s - loss: 0.4281 - acc: 0.8065 - val_loss: 0.5929 - val_acc: 0.6400\n",
            "Epoch 69/300\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.49740\n",
            "620/620 - 1s - loss: 0.4727 - acc: 0.7823 - val_loss: 0.6209 - val_acc: 0.6300\n",
            "Epoch 70/300\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.49740\n",
            "620/620 - 1s - loss: 0.4442 - acc: 0.7919 - val_loss: 0.5560 - val_acc: 0.6900\n",
            "Epoch 71/300\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.49740\n",
            "620/620 - 1s - loss: 0.4537 - acc: 0.7855 - val_loss: 0.5393 - val_acc: 0.7000\n",
            "Epoch 72/300\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.49740\n",
            "620/620 - 1s - loss: 0.4720 - acc: 0.7726 - val_loss: 0.6301 - val_acc: 0.6200\n",
            "Epoch 73/300\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.49740\n",
            "620/620 - 1s - loss: 0.4385 - acc: 0.8097 - val_loss: 0.5541 - val_acc: 0.6700\n",
            "Epoch 74/300\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.49740\n",
            "620/620 - 1s - loss: 0.4532 - acc: 0.7855 - val_loss: 0.5523 - val_acc: 0.6800\n",
            "Epoch 75/300\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.49740\n",
            "620/620 - 1s - loss: 0.4251 - acc: 0.7887 - val_loss: 0.5224 - val_acc: 0.6700\n",
            "Epoch 76/300\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.49740\n",
            "620/620 - 1s - loss: 0.4654 - acc: 0.7694 - val_loss: 0.5912 - val_acc: 0.6300\n",
            "Epoch 77/300\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.49740\n",
            "620/620 - 1s - loss: 0.4309 - acc: 0.7952 - val_loss: 0.6084 - val_acc: 0.6300\n",
            "Epoch 78/300\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.49740\n",
            "620/620 - 1s - loss: 0.4394 - acc: 0.8000 - val_loss: 0.5630 - val_acc: 0.6600\n",
            "Epoch 79/300\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.49740\n",
            "620/620 - 1s - loss: 0.4302 - acc: 0.8145 - val_loss: 0.5987 - val_acc: 0.6400\n",
            "Epoch 80/300\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.49740\n",
            "620/620 - 1s - loss: 0.4234 - acc: 0.8129 - val_loss: 0.6191 - val_acc: 0.6200\n",
            "Epoch 81/300\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.49740\n",
            "620/620 - 1s - loss: 0.4164 - acc: 0.8097 - val_loss: 0.5487 - val_acc: 0.6900\n",
            "Epoch 82/300\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.49740\n",
            "620/620 - 1s - loss: 0.4312 - acc: 0.8016 - val_loss: 0.6303 - val_acc: 0.6300\n",
            "Epoch 83/300\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.49740\n",
            "620/620 - 1s - loss: 0.4365 - acc: 0.7790 - val_loss: 0.6049 - val_acc: 0.6300\n",
            "Epoch 84/300\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.49740\n",
            "620/620 - 1s - loss: 0.4490 - acc: 0.7871 - val_loss: 0.5896 - val_acc: 0.6400\n",
            "Epoch 85/300\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.49740\n",
            "620/620 - 1s - loss: 0.4355 - acc: 0.7935 - val_loss: 0.5853 - val_acc: 0.6400\n",
            "Epoch 86/300\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.49740\n",
            "620/620 - 1s - loss: 0.4222 - acc: 0.8081 - val_loss: 0.5735 - val_acc: 0.6700\n",
            "Epoch 87/300\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.49740\n",
            "620/620 - 1s - loss: 0.4239 - acc: 0.8113 - val_loss: 0.6205 - val_acc: 0.6400\n",
            "Epoch 88/300\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.49740\n",
            "620/620 - 1s - loss: 0.4213 - acc: 0.8194 - val_loss: 0.6148 - val_acc: 0.6300\n",
            "Epoch 89/300\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.49740\n",
            "620/620 - 1s - loss: 0.4265 - acc: 0.8145 - val_loss: 0.6193 - val_acc: 0.6200\n",
            "Epoch 90/300\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.49740\n",
            "620/620 - 1s - loss: 0.4347 - acc: 0.7887 - val_loss: 0.5767 - val_acc: 0.6700\n",
            "Epoch 91/300\n",
            "\n",
            "Epoch 00091: val_loss improved from 0.49740 to 0.47370, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.4452 - acc: 0.7887 - val_loss: 0.4737 - val_acc: 0.7600\n",
            "Epoch 92/300\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.4229 - acc: 0.7903 - val_loss: 0.6004 - val_acc: 0.6500\n",
            "Epoch 93/300\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.4161 - acc: 0.7984 - val_loss: 0.5622 - val_acc: 0.6600\n",
            "Epoch 94/300\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.4434 - acc: 0.7871 - val_loss: 0.6105 - val_acc: 0.6200\n",
            "Epoch 95/300\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.4537 - acc: 0.7935 - val_loss: 0.6113 - val_acc: 0.6300\n",
            "Epoch 96/300\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.4439 - acc: 0.7806 - val_loss: 0.4992 - val_acc: 0.7600\n",
            "Epoch 97/300\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.4230 - acc: 0.8081 - val_loss: 0.5577 - val_acc: 0.6700\n",
            "Epoch 98/300\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.4289 - acc: 0.7935 - val_loss: 0.6416 - val_acc: 0.6400\n",
            "Epoch 99/300\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.4245 - acc: 0.8016 - val_loss: 0.6199 - val_acc: 0.6600\n",
            "Epoch 100/300\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.4183 - acc: 0.8081 - val_loss: 0.6367 - val_acc: 0.6400\n",
            "Epoch 101/300\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.4387 - acc: 0.8081 - val_loss: 0.5569 - val_acc: 0.6800\n",
            "Epoch 102/300\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.4160 - acc: 0.7935 - val_loss: 0.7351 - val_acc: 0.5900\n",
            "Epoch 103/300\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.4339 - acc: 0.7919 - val_loss: 0.6985 - val_acc: 0.6100\n",
            "Epoch 104/300\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.4243 - acc: 0.8032 - val_loss: 0.6916 - val_acc: 0.6100\n",
            "Epoch 105/300\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.4399 - acc: 0.7935 - val_loss: 0.5937 - val_acc: 0.6300\n",
            "Epoch 106/300\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.4436 - acc: 0.7968 - val_loss: 0.5484 - val_acc: 0.7100\n",
            "Epoch 107/300\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.4269 - acc: 0.8000 - val_loss: 0.5973 - val_acc: 0.6800\n",
            "Epoch 108/300\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.4091 - acc: 0.8065 - val_loss: 0.6263 - val_acc: 0.6400\n",
            "Epoch 109/300\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.4076 - acc: 0.8226 - val_loss: 0.5983 - val_acc: 0.6600\n",
            "Epoch 110/300\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.3965 - acc: 0.8339 - val_loss: 0.5687 - val_acc: 0.6800\n",
            "Epoch 111/300\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.4293 - acc: 0.8016 - val_loss: 0.6067 - val_acc: 0.6400\n",
            "Epoch 112/300\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.4349 - acc: 0.7935 - val_loss: 0.6023 - val_acc: 0.6300\n",
            "Epoch 113/300\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.4160 - acc: 0.8145 - val_loss: 0.6653 - val_acc: 0.6300\n",
            "Epoch 114/300\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.4277 - acc: 0.7871 - val_loss: 0.6354 - val_acc: 0.6400\n",
            "Epoch 115/300\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.4087 - acc: 0.8226 - val_loss: 0.6180 - val_acc: 0.6800\n",
            "Epoch 116/300\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.4113 - acc: 0.8016 - val_loss: 0.5991 - val_acc: 0.6500\n",
            "Epoch 117/300\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.4166 - acc: 0.8081 - val_loss: 0.5458 - val_acc: 0.6800\n",
            "Epoch 118/300\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.4091 - acc: 0.8065 - val_loss: 0.5612 - val_acc: 0.6600\n",
            "Epoch 119/300\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.4426 - acc: 0.7710 - val_loss: 0.6583 - val_acc: 0.6200\n",
            "Epoch 120/300\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.4163 - acc: 0.8129 - val_loss: 0.6267 - val_acc: 0.6100\n",
            "Epoch 121/300\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.3858 - acc: 0.8242 - val_loss: 0.6333 - val_acc: 0.6400\n",
            "Epoch 122/300\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.4293 - acc: 0.7984 - val_loss: 0.7262 - val_acc: 0.6000\n",
            "Epoch 123/300\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.4050 - acc: 0.8226 - val_loss: 0.7885 - val_acc: 0.5900\n",
            "Epoch 124/300\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.4043 - acc: 0.8065 - val_loss: 0.5262 - val_acc: 0.7400\n",
            "Epoch 125/300\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.4165 - acc: 0.8016 - val_loss: 0.5578 - val_acc: 0.7000\n",
            "Epoch 126/300\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.4232 - acc: 0.8113 - val_loss: 0.6161 - val_acc: 0.6600\n",
            "Epoch 127/300\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.4027 - acc: 0.8242 - val_loss: 0.5174 - val_acc: 0.7100\n",
            "Epoch 128/300\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.3949 - acc: 0.8194 - val_loss: 0.5292 - val_acc: 0.7200\n",
            "Epoch 129/300\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.4140 - acc: 0.8097 - val_loss: 0.5414 - val_acc: 0.7000\n",
            "Epoch 130/300\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.4327 - acc: 0.8032 - val_loss: 0.6381 - val_acc: 0.6000\n",
            "Epoch 131/300\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.4209 - acc: 0.8129 - val_loss: 0.5510 - val_acc: 0.6800\n",
            "Epoch 132/300\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.4106 - acc: 0.8081 - val_loss: 0.5423 - val_acc: 0.7100\n",
            "Epoch 133/300\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.4282 - acc: 0.8032 - val_loss: 0.6213 - val_acc: 0.6500\n",
            "Epoch 134/300\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.4026 - acc: 0.8258 - val_loss: 0.7183 - val_acc: 0.6100\n",
            "Epoch 135/300\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.4071 - acc: 0.8145 - val_loss: 0.5586 - val_acc: 0.6800\n",
            "Epoch 136/300\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.4254 - acc: 0.8048 - val_loss: 0.5445 - val_acc: 0.6600\n",
            "Epoch 137/300\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.3985 - acc: 0.8226 - val_loss: 0.6627 - val_acc: 0.6300\n",
            "Epoch 138/300\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.3934 - acc: 0.8000 - val_loss: 0.6213 - val_acc: 0.6400\n",
            "Epoch 139/300\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.4137 - acc: 0.7984 - val_loss: 0.5636 - val_acc: 0.6900\n",
            "Epoch 140/300\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.3965 - acc: 0.8145 - val_loss: 0.6188 - val_acc: 0.6400\n",
            "Epoch 141/300\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.4103 - acc: 0.8016 - val_loss: 0.6906 - val_acc: 0.6100\n",
            "Epoch 142/300\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.3952 - acc: 0.8194 - val_loss: 0.6124 - val_acc: 0.6400\n",
            "Epoch 143/300\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.3747 - acc: 0.8290 - val_loss: 0.6179 - val_acc: 0.6400\n",
            "Epoch 144/300\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.4111 - acc: 0.8097 - val_loss: 0.6567 - val_acc: 0.6200\n",
            "Epoch 145/300\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.4029 - acc: 0.8145 - val_loss: 0.6169 - val_acc: 0.6500\n",
            "Epoch 146/300\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.3903 - acc: 0.8468 - val_loss: 0.6868 - val_acc: 0.5900\n",
            "Epoch 147/300\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.4100 - acc: 0.8032 - val_loss: 0.5801 - val_acc: 0.6600\n",
            "Epoch 148/300\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.3948 - acc: 0.8242 - val_loss: 0.5741 - val_acc: 0.6600\n",
            "Epoch 149/300\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.4143 - acc: 0.8177 - val_loss: 0.6623 - val_acc: 0.6500\n",
            "Epoch 150/300\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.3954 - acc: 0.8081 - val_loss: 0.7352 - val_acc: 0.6100\n",
            "Epoch 151/300\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.4251 - acc: 0.8032 - val_loss: 0.5060 - val_acc: 0.7700\n",
            "Epoch 152/300\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.4103 - acc: 0.8129 - val_loss: 0.5310 - val_acc: 0.7100\n",
            "Epoch 153/300\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.4099 - acc: 0.8177 - val_loss: 0.5627 - val_acc: 0.6600\n",
            "Epoch 154/300\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.4016 - acc: 0.8177 - val_loss: 0.5828 - val_acc: 0.6600\n",
            "Epoch 155/300\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.3903 - acc: 0.8339 - val_loss: 0.5997 - val_acc: 0.6600\n",
            "Epoch 156/300\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.4028 - acc: 0.8081 - val_loss: 0.6731 - val_acc: 0.6500\n",
            "Epoch 157/300\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.3995 - acc: 0.7984 - val_loss: 0.5512 - val_acc: 0.7200\n",
            "Epoch 158/300\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.4061 - acc: 0.8113 - val_loss: 0.7565 - val_acc: 0.6100\n",
            "Epoch 159/300\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.3921 - acc: 0.8194 - val_loss: 0.5193 - val_acc: 0.7200\n",
            "Epoch 160/300\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.3816 - acc: 0.8355 - val_loss: 0.6916 - val_acc: 0.6400\n",
            "Epoch 161/300\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.3892 - acc: 0.8226 - val_loss: 0.6589 - val_acc: 0.6600\n",
            "Epoch 162/300\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.4188 - acc: 0.7935 - val_loss: 0.5416 - val_acc: 0.7100\n",
            "Epoch 163/300\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.4191 - acc: 0.8048 - val_loss: 0.6528 - val_acc: 0.6400\n",
            "Epoch 164/300\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.3951 - acc: 0.8274 - val_loss: 0.7996 - val_acc: 0.5700\n",
            "Epoch 165/300\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.4122 - acc: 0.8226 - val_loss: 0.6346 - val_acc: 0.6300\n",
            "Epoch 166/300\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.4070 - acc: 0.8145 - val_loss: 0.5575 - val_acc: 0.7100\n",
            "Epoch 167/300\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.3957 - acc: 0.8194 - val_loss: 0.6548 - val_acc: 0.6400\n",
            "Epoch 168/300\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.4058 - acc: 0.8242 - val_loss: 0.6042 - val_acc: 0.6700\n",
            "Epoch 169/300\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.3724 - acc: 0.8371 - val_loss: 0.5536 - val_acc: 0.6700\n",
            "Epoch 170/300\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.3976 - acc: 0.8145 - val_loss: 0.7133 - val_acc: 0.6100\n",
            "Epoch 171/300\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.4240 - acc: 0.8048 - val_loss: 0.5980 - val_acc: 0.6700\n",
            "Epoch 172/300\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.3661 - acc: 0.8484 - val_loss: 0.6741 - val_acc: 0.6400\n",
            "Epoch 173/300\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.4188 - acc: 0.8016 - val_loss: 0.7218 - val_acc: 0.6200\n",
            "Epoch 174/300\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.4256 - acc: 0.7887 - val_loss: 0.7129 - val_acc: 0.6200\n",
            "Epoch 175/300\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.4212 - acc: 0.8000 - val_loss: 0.6520 - val_acc: 0.6200\n",
            "Epoch 176/300\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.3901 - acc: 0.8210 - val_loss: 0.6402 - val_acc: 0.6400\n",
            "Epoch 177/300\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.4196 - acc: 0.8048 - val_loss: 0.7252 - val_acc: 0.5900\n",
            "Epoch 178/300\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.3949 - acc: 0.8210 - val_loss: 0.6030 - val_acc: 0.6800\n",
            "Epoch 179/300\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.3963 - acc: 0.8226 - val_loss: 0.7325 - val_acc: 0.6100\n",
            "Epoch 180/300\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.4257 - acc: 0.8081 - val_loss: 0.6916 - val_acc: 0.6300\n",
            "Epoch 181/300\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.3903 - acc: 0.8145 - val_loss: 0.6113 - val_acc: 0.6400\n",
            "Epoch 182/300\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.3709 - acc: 0.8258 - val_loss: 0.8152 - val_acc: 0.5900\n",
            "Epoch 183/300\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.3993 - acc: 0.8113 - val_loss: 0.6383 - val_acc: 0.6300\n",
            "Epoch 184/300\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.3972 - acc: 0.8177 - val_loss: 0.8409 - val_acc: 0.5500\n",
            "Epoch 185/300\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.3810 - acc: 0.8306 - val_loss: 0.7003 - val_acc: 0.6300\n",
            "Epoch 186/300\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.3892 - acc: 0.8306 - val_loss: 0.5914 - val_acc: 0.6500\n",
            "Epoch 187/300\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.3645 - acc: 0.8323 - val_loss: 0.5820 - val_acc: 0.6700\n",
            "Epoch 188/300\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.4047 - acc: 0.8097 - val_loss: 0.5161 - val_acc: 0.7300\n",
            "Epoch 189/300\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.3774 - acc: 0.8323 - val_loss: 0.5856 - val_acc: 0.6600\n",
            "Epoch 190/300\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.3878 - acc: 0.8371 - val_loss: 0.6014 - val_acc: 0.6500\n",
            "Epoch 191/300\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.3799 - acc: 0.8355 - val_loss: 0.7117 - val_acc: 0.5900\n",
            "Epoch 192/300\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.3802 - acc: 0.8274 - val_loss: 0.5669 - val_acc: 0.6800\n",
            "Epoch 193/300\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.4120 - acc: 0.7919 - val_loss: 0.6012 - val_acc: 0.6700\n",
            "Epoch 194/300\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.3941 - acc: 0.8210 - val_loss: 0.6237 - val_acc: 0.6500\n",
            "Epoch 195/300\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.3809 - acc: 0.8403 - val_loss: 0.6978 - val_acc: 0.6000\n",
            "Epoch 196/300\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.3891 - acc: 0.8161 - val_loss: 0.5872 - val_acc: 0.6400\n",
            "Epoch 197/300\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.4101 - acc: 0.8000 - val_loss: 0.5889 - val_acc: 0.6700\n",
            "Epoch 198/300\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.4022 - acc: 0.8161 - val_loss: 0.6972 - val_acc: 0.6000\n",
            "Epoch 199/300\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.3910 - acc: 0.8161 - val_loss: 0.6782 - val_acc: 0.6200\n",
            "Epoch 200/300\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.3772 - acc: 0.8242 - val_loss: 0.6291 - val_acc: 0.6500\n",
            "Epoch 201/300\n",
            "\n",
            "Epoch 00201: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.3702 - acc: 0.8339 - val_loss: 0.5046 - val_acc: 0.7300\n",
            "Epoch 202/300\n",
            "\n",
            "Epoch 00202: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.3996 - acc: 0.8081 - val_loss: 0.5471 - val_acc: 0.6800\n",
            "Epoch 203/300\n",
            "\n",
            "Epoch 00203: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.3835 - acc: 0.8290 - val_loss: 0.6121 - val_acc: 0.6500\n",
            "Epoch 204/300\n",
            "\n",
            "Epoch 00204: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.4060 - acc: 0.8113 - val_loss: 0.7581 - val_acc: 0.5800\n",
            "Epoch 205/300\n",
            "\n",
            "Epoch 00205: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.3935 - acc: 0.8065 - val_loss: 0.7348 - val_acc: 0.6000\n",
            "Epoch 206/300\n",
            "\n",
            "Epoch 00206: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.3810 - acc: 0.8323 - val_loss: 0.6470 - val_acc: 0.6200\n",
            "Epoch 207/300\n",
            "\n",
            "Epoch 00207: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.3624 - acc: 0.8419 - val_loss: 0.6613 - val_acc: 0.6400\n",
            "Epoch 208/300\n",
            "\n",
            "Epoch 00208: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.3756 - acc: 0.8371 - val_loss: 0.6140 - val_acc: 0.6700\n",
            "Epoch 209/300\n",
            "\n",
            "Epoch 00209: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.3915 - acc: 0.8177 - val_loss: 0.5498 - val_acc: 0.6900\n",
            "Epoch 210/300\n",
            "\n",
            "Epoch 00210: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.3936 - acc: 0.8177 - val_loss: 0.6838 - val_acc: 0.6200\n",
            "Epoch 211/300\n",
            "\n",
            "Epoch 00211: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.3926 - acc: 0.8306 - val_loss: 0.6977 - val_acc: 0.6200\n",
            "Epoch 212/300\n",
            "\n",
            "Epoch 00212: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.3771 - acc: 0.8323 - val_loss: 0.5999 - val_acc: 0.6800\n",
            "Epoch 213/300\n",
            "\n",
            "Epoch 00213: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.3831 - acc: 0.8323 - val_loss: 0.5597 - val_acc: 0.6700\n",
            "Epoch 214/300\n",
            "\n",
            "Epoch 00214: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.3700 - acc: 0.8306 - val_loss: 0.6792 - val_acc: 0.6500\n",
            "Epoch 215/300\n",
            "\n",
            "Epoch 00215: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.3758 - acc: 0.8290 - val_loss: 0.6048 - val_acc: 0.6700\n",
            "Epoch 216/300\n",
            "\n",
            "Epoch 00216: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.3918 - acc: 0.8242 - val_loss: 0.8240 - val_acc: 0.5700\n",
            "Epoch 217/300\n",
            "\n",
            "Epoch 00217: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.3727 - acc: 0.8323 - val_loss: 0.7046 - val_acc: 0.5800\n",
            "Epoch 218/300\n",
            "\n",
            "Epoch 00218: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.3698 - acc: 0.8516 - val_loss: 0.6214 - val_acc: 0.6800\n",
            "Epoch 219/300\n",
            "\n",
            "Epoch 00219: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.3953 - acc: 0.8306 - val_loss: 0.6466 - val_acc: 0.6300\n",
            "Epoch 220/300\n",
            "\n",
            "Epoch 00220: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.3785 - acc: 0.8403 - val_loss: 0.6325 - val_acc: 0.6300\n",
            "Epoch 221/300\n",
            "\n",
            "Epoch 00221: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.3951 - acc: 0.8258 - val_loss: 0.5004 - val_acc: 0.7300\n",
            "Epoch 222/300\n",
            "\n",
            "Epoch 00222: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.3759 - acc: 0.8323 - val_loss: 0.6380 - val_acc: 0.6600\n",
            "Epoch 223/300\n",
            "\n",
            "Epoch 00223: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.3682 - acc: 0.8339 - val_loss: 0.6985 - val_acc: 0.6000\n",
            "Epoch 224/300\n",
            "\n",
            "Epoch 00224: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.3804 - acc: 0.8371 - val_loss: 0.6446 - val_acc: 0.6400\n",
            "Epoch 225/300\n",
            "\n",
            "Epoch 00225: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.3809 - acc: 0.8113 - val_loss: 0.5667 - val_acc: 0.6700\n",
            "Epoch 226/300\n",
            "\n",
            "Epoch 00226: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.3747 - acc: 0.8484 - val_loss: 0.5165 - val_acc: 0.7000\n",
            "Epoch 227/300\n",
            "\n",
            "Epoch 00227: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.3878 - acc: 0.8129 - val_loss: 0.6794 - val_acc: 0.6300\n",
            "Epoch 228/300\n",
            "\n",
            "Epoch 00228: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.3816 - acc: 0.8339 - val_loss: 0.6380 - val_acc: 0.6600\n",
            "Epoch 229/300\n",
            "\n",
            "Epoch 00229: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.3741 - acc: 0.8258 - val_loss: 0.6413 - val_acc: 0.6200\n",
            "Epoch 230/300\n",
            "\n",
            "Epoch 00230: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.3733 - acc: 0.8387 - val_loss: 0.7110 - val_acc: 0.6200\n",
            "Epoch 231/300\n",
            "\n",
            "Epoch 00231: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.3787 - acc: 0.8258 - val_loss: 0.6288 - val_acc: 0.6500\n",
            "Epoch 232/300\n",
            "\n",
            "Epoch 00232: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.3740 - acc: 0.8210 - val_loss: 0.6593 - val_acc: 0.6200\n",
            "Epoch 233/300\n",
            "\n",
            "Epoch 00233: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.3862 - acc: 0.8306 - val_loss: 0.7505 - val_acc: 0.6000\n",
            "Epoch 234/300\n",
            "\n",
            "Epoch 00234: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.3803 - acc: 0.8306 - val_loss: 0.5815 - val_acc: 0.6500\n",
            "Epoch 235/300\n",
            "\n",
            "Epoch 00235: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.3645 - acc: 0.8145 - val_loss: 0.5413 - val_acc: 0.6900\n",
            "Epoch 236/300\n",
            "\n",
            "Epoch 00236: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.3885 - acc: 0.8290 - val_loss: 0.6804 - val_acc: 0.6300\n",
            "Epoch 237/300\n",
            "\n",
            "Epoch 00237: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.3839 - acc: 0.8226 - val_loss: 0.7100 - val_acc: 0.6200\n",
            "Epoch 238/300\n",
            "\n",
            "Epoch 00238: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.3744 - acc: 0.8355 - val_loss: 0.5906 - val_acc: 0.6700\n",
            "Epoch 239/300\n",
            "\n",
            "Epoch 00239: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.3874 - acc: 0.8161 - val_loss: 0.5092 - val_acc: 0.7300\n",
            "Epoch 240/300\n",
            "\n",
            "Epoch 00240: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.3652 - acc: 0.8387 - val_loss: 0.6572 - val_acc: 0.6600\n",
            "Epoch 241/300\n",
            "\n",
            "Epoch 00241: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.3699 - acc: 0.8435 - val_loss: 0.6122 - val_acc: 0.6300\n",
            "Epoch 242/300\n",
            "\n",
            "Epoch 00242: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.3631 - acc: 0.8500 - val_loss: 0.6879 - val_acc: 0.6500\n",
            "Epoch 243/300\n",
            "\n",
            "Epoch 00243: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.4169 - acc: 0.8097 - val_loss: 0.7373 - val_acc: 0.6100\n",
            "Epoch 244/300\n",
            "\n",
            "Epoch 00244: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.3850 - acc: 0.8258 - val_loss: 0.6187 - val_acc: 0.6500\n",
            "Epoch 245/300\n",
            "\n",
            "Epoch 00245: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.3647 - acc: 0.8371 - val_loss: 0.6961 - val_acc: 0.6300\n",
            "Epoch 246/300\n",
            "\n",
            "Epoch 00246: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.3820 - acc: 0.8274 - val_loss: 0.6859 - val_acc: 0.6400\n",
            "Epoch 247/300\n",
            "\n",
            "Epoch 00247: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.3964 - acc: 0.8242 - val_loss: 0.6655 - val_acc: 0.6500\n",
            "Epoch 248/300\n",
            "\n",
            "Epoch 00248: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.3710 - acc: 0.8274 - val_loss: 0.8109 - val_acc: 0.5800\n",
            "Epoch 249/300\n",
            "\n",
            "Epoch 00249: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.3803 - acc: 0.8323 - val_loss: 0.6195 - val_acc: 0.6400\n",
            "Epoch 250/300\n",
            "\n",
            "Epoch 00250: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.3567 - acc: 0.8387 - val_loss: 0.6935 - val_acc: 0.6300\n",
            "Epoch 251/300\n",
            "\n",
            "Epoch 00251: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.3841 - acc: 0.8274 - val_loss: 0.6686 - val_acc: 0.6200\n",
            "Epoch 252/300\n",
            "\n",
            "Epoch 00252: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.3519 - acc: 0.8387 - val_loss: 0.6098 - val_acc: 0.6700\n",
            "Epoch 253/300\n",
            "\n",
            "Epoch 00253: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.3514 - acc: 0.8323 - val_loss: 0.6173 - val_acc: 0.6600\n",
            "Epoch 254/300\n",
            "\n",
            "Epoch 00254: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.3438 - acc: 0.8694 - val_loss: 0.6103 - val_acc: 0.6700\n",
            "Epoch 255/300\n",
            "\n",
            "Epoch 00255: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.3757 - acc: 0.8468 - val_loss: 0.7248 - val_acc: 0.6300\n",
            "Epoch 256/300\n",
            "\n",
            "Epoch 00256: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.3669 - acc: 0.8306 - val_loss: 0.6450 - val_acc: 0.6600\n",
            "Epoch 257/300\n",
            "\n",
            "Epoch 00257: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.3616 - acc: 0.8516 - val_loss: 0.6761 - val_acc: 0.6200\n",
            "Epoch 258/300\n",
            "\n",
            "Epoch 00258: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.3905 - acc: 0.8323 - val_loss: 0.7210 - val_acc: 0.6200\n",
            "Epoch 259/300\n",
            "\n",
            "Epoch 00259: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.3806 - acc: 0.8226 - val_loss: 0.5634 - val_acc: 0.7000\n",
            "Epoch 260/300\n",
            "\n",
            "Epoch 00260: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.3731 - acc: 0.8339 - val_loss: 0.6987 - val_acc: 0.6200\n",
            "Epoch 261/300\n",
            "\n",
            "Epoch 00261: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.3433 - acc: 0.8565 - val_loss: 0.6565 - val_acc: 0.6400\n",
            "Epoch 262/300\n",
            "\n",
            "Epoch 00262: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.3754 - acc: 0.8339 - val_loss: 0.5956 - val_acc: 0.6400\n",
            "Epoch 263/300\n",
            "\n",
            "Epoch 00263: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.3838 - acc: 0.8274 - val_loss: 0.5741 - val_acc: 0.6600\n",
            "Epoch 264/300\n",
            "\n",
            "Epoch 00264: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.3805 - acc: 0.8371 - val_loss: 0.6918 - val_acc: 0.6300\n",
            "Epoch 265/300\n",
            "\n",
            "Epoch 00265: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.3689 - acc: 0.8452 - val_loss: 0.7339 - val_acc: 0.5900\n",
            "Epoch 266/300\n",
            "\n",
            "Epoch 00266: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.3875 - acc: 0.8468 - val_loss: 0.6767 - val_acc: 0.6100\n",
            "Epoch 267/300\n",
            "\n",
            "Epoch 00267: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.3645 - acc: 0.8468 - val_loss: 0.7026 - val_acc: 0.6100\n",
            "Epoch 268/300\n",
            "\n",
            "Epoch 00268: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.3528 - acc: 0.8371 - val_loss: 0.6792 - val_acc: 0.6000\n",
            "Epoch 269/300\n",
            "\n",
            "Epoch 00269: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.3590 - acc: 0.8548 - val_loss: 0.6718 - val_acc: 0.6500\n",
            "Epoch 270/300\n",
            "\n",
            "Epoch 00270: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.3738 - acc: 0.8387 - val_loss: 0.6837 - val_acc: 0.6500\n",
            "Epoch 271/300\n",
            "\n",
            "Epoch 00271: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.3633 - acc: 0.8548 - val_loss: 0.5458 - val_acc: 0.6900\n",
            "Epoch 272/300\n",
            "\n",
            "Epoch 00272: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.3867 - acc: 0.8290 - val_loss: 0.6408 - val_acc: 0.6500\n",
            "Epoch 273/300\n",
            "\n",
            "Epoch 00273: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.3753 - acc: 0.8323 - val_loss: 0.5949 - val_acc: 0.6700\n",
            "Epoch 274/300\n",
            "\n",
            "Epoch 00274: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.3727 - acc: 0.8290 - val_loss: 0.7561 - val_acc: 0.5800\n",
            "Epoch 275/300\n",
            "\n",
            "Epoch 00275: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.3834 - acc: 0.8323 - val_loss: 0.5781 - val_acc: 0.6600\n",
            "Epoch 276/300\n",
            "\n",
            "Epoch 00276: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.3884 - acc: 0.8339 - val_loss: 0.7210 - val_acc: 0.6200\n",
            "Epoch 277/300\n",
            "\n",
            "Epoch 00277: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.3776 - acc: 0.8210 - val_loss: 0.6760 - val_acc: 0.6200\n",
            "Epoch 278/300\n",
            "\n",
            "Epoch 00278: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.3722 - acc: 0.8242 - val_loss: 0.7898 - val_acc: 0.5900\n",
            "Epoch 279/300\n",
            "\n",
            "Epoch 00279: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.3716 - acc: 0.8274 - val_loss: 0.7159 - val_acc: 0.6100\n",
            "Epoch 280/300\n",
            "\n",
            "Epoch 00280: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.3798 - acc: 0.8323 - val_loss: 0.5643 - val_acc: 0.6600\n",
            "Epoch 281/300\n",
            "\n",
            "Epoch 00281: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.3726 - acc: 0.8468 - val_loss: 0.5412 - val_acc: 0.7000\n",
            "Epoch 282/300\n",
            "\n",
            "Epoch 00282: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.3726 - acc: 0.8242 - val_loss: 0.6915 - val_acc: 0.6200\n",
            "Epoch 283/300\n",
            "\n",
            "Epoch 00283: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.3718 - acc: 0.8500 - val_loss: 0.7775 - val_acc: 0.5900\n",
            "Epoch 284/300\n",
            "\n",
            "Epoch 00284: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.4025 - acc: 0.8161 - val_loss: 0.6175 - val_acc: 0.6600\n",
            "Epoch 285/300\n",
            "\n",
            "Epoch 00285: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.3672 - acc: 0.8532 - val_loss: 0.7173 - val_acc: 0.6100\n",
            "Epoch 286/300\n",
            "\n",
            "Epoch 00286: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.4149 - acc: 0.8081 - val_loss: 0.5902 - val_acc: 0.6600\n",
            "Epoch 287/300\n",
            "\n",
            "Epoch 00287: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.3711 - acc: 0.8371 - val_loss: 0.8135 - val_acc: 0.6100\n",
            "Epoch 288/300\n",
            "\n",
            "Epoch 00288: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.3483 - acc: 0.8468 - val_loss: 0.5926 - val_acc: 0.7000\n",
            "Epoch 289/300\n",
            "\n",
            "Epoch 00289: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.3641 - acc: 0.8387 - val_loss: 0.6537 - val_acc: 0.6300\n",
            "Epoch 290/300\n",
            "\n",
            "Epoch 00290: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.3710 - acc: 0.8355 - val_loss: 0.6184 - val_acc: 0.6500\n",
            "Epoch 291/300\n",
            "\n",
            "Epoch 00291: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.3625 - acc: 0.8242 - val_loss: 0.6052 - val_acc: 0.6500\n",
            "Epoch 292/300\n",
            "\n",
            "Epoch 00292: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.3634 - acc: 0.8468 - val_loss: 0.7295 - val_acc: 0.6100\n",
            "Epoch 293/300\n",
            "\n",
            "Epoch 00293: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.3689 - acc: 0.8613 - val_loss: 0.7642 - val_acc: 0.5700\n",
            "Epoch 294/300\n",
            "\n",
            "Epoch 00294: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.3834 - acc: 0.8210 - val_loss: 0.6416 - val_acc: 0.6600\n",
            "Epoch 295/300\n",
            "\n",
            "Epoch 00295: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.3754 - acc: 0.8274 - val_loss: 0.6518 - val_acc: 0.6200\n",
            "Epoch 296/300\n",
            "\n",
            "Epoch 00296: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.3794 - acc: 0.8484 - val_loss: 0.8884 - val_acc: 0.5600\n",
            "Epoch 297/300\n",
            "\n",
            "Epoch 00297: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.3727 - acc: 0.8258 - val_loss: 0.6132 - val_acc: 0.6500\n",
            "Epoch 298/300\n",
            "\n",
            "Epoch 00298: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.3770 - acc: 0.8403 - val_loss: 0.6942 - val_acc: 0.5900\n",
            "Epoch 299/300\n",
            "\n",
            "Epoch 00299: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.3955 - acc: 0.8306 - val_loss: 0.5937 - val_acc: 0.6900\n",
            "Epoch 300/300\n",
            "\n",
            "Epoch 00300: val_loss did not improve from 0.47370\n",
            "620/620 - 1s - loss: 0.3770 - acc: 0.8387 - val_loss: 0.6589 - val_acc: 0.6500\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUBRFyTj/H45NhA+XY05NAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "# Evaluate on test data\n",
            "40/40 [==============================] - 0s 3ms/sample - loss: 0.4521 - acc: 0.7500\n",
            "test loss, test acc: [0.45206004474312067, 0.75]\n",
            "2\n",
            "EEG_Deep/Data2A/parsed_P02T.mat\n",
            "(0, 12, 4096)\n",
            "(80, 12, 4096)\n",
            "(80, 1)\n",
            "3\n",
            "EEG_Deep/Data2A/parsed_P03T.mat\n",
            "(80, 12, 4096)\n",
            "(160, 12, 4096)\n",
            "(160, 1)\n",
            "4\n",
            "EEG_Deep/Data2A/parsed_P04T.mat\n",
            "(160, 12, 4096)\n",
            "(240, 12, 4096)\n",
            "(240, 1)\n",
            "5\n",
            "EEG_Deep/Data2A/parsed_P05T.mat\n",
            "(240, 12, 4096)\n",
            "(320, 12, 4096)\n",
            "(320, 1)\n",
            "6\n",
            "EEG_Deep/Data2A/parsed_P06T.mat\n",
            "(320, 12, 4096)\n",
            "(400, 12, 4096)\n",
            "(400, 1)\n",
            "7\n",
            "EEG_Deep/Data2A/parsed_P07T.mat\n",
            "(400, 12, 4096)\n",
            "(480, 12, 4096)\n",
            "(480, 1)\n",
            "8\n",
            "EEG_Deep/Data2A/parsed_P08T.mat\n",
            "(480, 12, 4096)\n",
            "(560, 12, 4096)\n",
            "(560, 1)\n",
            "9\n",
            "EEG_Deep/Data2A/parsed_P09T.mat\n",
            "(560, 12, 4096)\n",
            "(640, 12, 4096)\n",
            "(640, 1)\n",
            "10\n",
            "EEG_Deep/Data2A/parsed_P010T.mat\n",
            "(640, 12, 4096)\n",
            "(720, 12, 4096)\n",
            "(720, 1)\n",
            "1\n",
            "EEG_Deep/Data2A/parsed_P01E.mat\n",
            "(0, 12, 4096)\n",
            "(40, 12, 4096)\n",
            "(40, 1)\n",
            "Filtering of training data in progress\n",
            "(720, 12, 4096)\n",
            "Filtering of test data in progress\n",
            "(40, 12, 4096)\n",
            "[2. 2. 1. 1. 1. 1. 1. 2. 1. 1. 2. 1. 2. 1. 1. 1. 1. 2. 2. 1. 1. 1. 1. 1.\n",
            " 2. 2. 1. 2. 1. 2. 1. 1. 2. 2. 2. 1. 2. 1. 2. 2. 1. 1. 1. 2. 1. 1. 2. 1.\n",
            " 2. 1. 2. 1. 1. 2. 1. 2. 2. 2. 2. 1. 1. 2. 2. 2. 1. 1. 2. 2. 1. 2. 2. 2.\n",
            " 2. 2. 2. 2. 1. 2. 2. 1. 2. 1. 2. 1. 1. 2. 1. 1. 1. 1. 2. 1. 1. 2. 2. 1.\n",
            " 2. 1. 1. 2.]\n",
            "(620, 12, 1536)\n",
            "(620,)\n",
            "(100, 12, 1536)\n",
            "(100,)\n",
            "[[0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]]\n",
            "X_train shape: (620, 1, 12, 1536)\n",
            "620 train samples\n",
            "100 val samples\n",
            "(40, 12, 1536)\n",
            "(40, 1)\n",
            "X_train shape: (40, 1, 12, 1536)\n",
            "40 train samples\n",
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 1, 12, 1536)]     0         \n",
            "_________________________________________________________________\n",
            "conv2d (Conv2D)              (None, 8, 12, 1536)       200       \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 8, 12, 1536)       32        \n",
            "_________________________________________________________________\n",
            "depthwise_conv2d (DepthwiseC (None, 16, 1, 1536)       192       \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 16, 1, 1536)       64        \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 16, 1, 1536)       0         \n",
            "_________________________________________________________________\n",
            "average_pooling2d (AveragePo (None, 16, 1, 384)        0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 16, 1, 384)        0         \n",
            "_________________________________________________________________\n",
            "separable_conv2d (SeparableC (None, 16, 1, 384)        512       \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 16, 1, 384)        64        \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 16, 1, 384)        0         \n",
            "_________________________________________________________________\n",
            "average_pooling2d_1 (Average (None, 16, 1, 48)         0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 16, 1, 48)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 768)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 2)                 1538      \n",
            "_________________________________________________________________\n",
            "softmax (Activation)         (None, 2)                 0         \n",
            "=================================================================\n",
            "Total params: 2,602\n",
            "Trainable params: 2,522\n",
            "Non-trainable params: 80\n",
            "_________________________________________________________________\n",
            "Train on 620 samples, validate on 100 samples\n",
            "Epoch 1/300\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.69314, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.6935 - acc: 0.5435 - val_loss: 0.6931 - val_acc: 0.5300\n",
            "Epoch 2/300\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.69314 to 0.68985, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.6723 - acc: 0.5742 - val_loss: 0.6898 - val_acc: 0.5300\n",
            "Epoch 3/300\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.68985 to 0.68708, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.6547 - acc: 0.6194 - val_loss: 0.6871 - val_acc: 0.5800\n",
            "Epoch 4/300\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.68708\n",
            "620/620 - 1s - loss: 0.6235 - acc: 0.6694 - val_loss: 0.6924 - val_acc: 0.5200\n",
            "Epoch 5/300\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.68708 to 0.68662, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.5992 - acc: 0.6887 - val_loss: 0.6866 - val_acc: 0.5600\n",
            "Epoch 6/300\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.68662 to 0.68056, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.5591 - acc: 0.7242 - val_loss: 0.6806 - val_acc: 0.5700\n",
            "Epoch 7/300\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.68056 to 0.66129, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.5494 - acc: 0.7129 - val_loss: 0.6613 - val_acc: 0.6400\n",
            "Epoch 8/300\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.66129 to 0.63090, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.5421 - acc: 0.7242 - val_loss: 0.6309 - val_acc: 0.6500\n",
            "Epoch 9/300\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.63090\n",
            "620/620 - 1s - loss: 0.5179 - acc: 0.7387 - val_loss: 0.6561 - val_acc: 0.6200\n",
            "Epoch 10/300\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.63090\n",
            "620/620 - 1s - loss: 0.5078 - acc: 0.7532 - val_loss: 0.7023 - val_acc: 0.6200\n",
            "Epoch 11/300\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.63090\n",
            "620/620 - 1s - loss: 0.5038 - acc: 0.7500 - val_loss: 0.6804 - val_acc: 0.6200\n",
            "Epoch 12/300\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.63090\n",
            "620/620 - 1s - loss: 0.5210 - acc: 0.7387 - val_loss: 0.6648 - val_acc: 0.6200\n",
            "Epoch 13/300\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.63090\n",
            "620/620 - 1s - loss: 0.5059 - acc: 0.7468 - val_loss: 0.7024 - val_acc: 0.6100\n",
            "Epoch 14/300\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.63090\n",
            "620/620 - 1s - loss: 0.4949 - acc: 0.7548 - val_loss: 0.7151 - val_acc: 0.6000\n",
            "Epoch 15/300\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.63090\n",
            "620/620 - 1s - loss: 0.4855 - acc: 0.7742 - val_loss: 0.6428 - val_acc: 0.6600\n",
            "Epoch 16/300\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.63090\n",
            "620/620 - 1s - loss: 0.5037 - acc: 0.7306 - val_loss: 0.6426 - val_acc: 0.6700\n",
            "Epoch 17/300\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.63090\n",
            "620/620 - 1s - loss: 0.5118 - acc: 0.7355 - val_loss: 0.6653 - val_acc: 0.6500\n",
            "Epoch 18/300\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.63090\n",
            "620/620 - 1s - loss: 0.5184 - acc: 0.7387 - val_loss: 0.6758 - val_acc: 0.6700\n",
            "Epoch 19/300\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.63090\n",
            "620/620 - 1s - loss: 0.4878 - acc: 0.7500 - val_loss: 0.6880 - val_acc: 0.6600\n",
            "Epoch 20/300\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.63090\n",
            "620/620 - 1s - loss: 0.5051 - acc: 0.7355 - val_loss: 0.6679 - val_acc: 0.6600\n",
            "Epoch 21/300\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.63090\n",
            "620/620 - 1s - loss: 0.4940 - acc: 0.7565 - val_loss: 0.6377 - val_acc: 0.7100\n",
            "Epoch 22/300\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.63090\n",
            "620/620 - 1s - loss: 0.4757 - acc: 0.7694 - val_loss: 0.6759 - val_acc: 0.6800\n",
            "Epoch 23/300\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.63090\n",
            "620/620 - 1s - loss: 0.4805 - acc: 0.7710 - val_loss: 0.7049 - val_acc: 0.6400\n",
            "Epoch 24/300\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.63090\n",
            "620/620 - 1s - loss: 0.4714 - acc: 0.7694 - val_loss: 0.7762 - val_acc: 0.6000\n",
            "Epoch 25/300\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.63090\n",
            "620/620 - 1s - loss: 0.4783 - acc: 0.7645 - val_loss: 0.6816 - val_acc: 0.6800\n",
            "Epoch 26/300\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.63090\n",
            "620/620 - 1s - loss: 0.4845 - acc: 0.7532 - val_loss: 0.6768 - val_acc: 0.6500\n",
            "Epoch 27/300\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.63090\n",
            "620/620 - 1s - loss: 0.4809 - acc: 0.7694 - val_loss: 0.6886 - val_acc: 0.6600\n",
            "Epoch 28/300\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.63090\n",
            "620/620 - 1s - loss: 0.4581 - acc: 0.7645 - val_loss: 0.7082 - val_acc: 0.6600\n",
            "Epoch 29/300\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.63090\n",
            "620/620 - 1s - loss: 0.4702 - acc: 0.7677 - val_loss: 0.6555 - val_acc: 0.6600\n",
            "Epoch 30/300\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.63090\n",
            "620/620 - 1s - loss: 0.4511 - acc: 0.7823 - val_loss: 0.6522 - val_acc: 0.7200\n",
            "Epoch 31/300\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.63090\n",
            "620/620 - 1s - loss: 0.4613 - acc: 0.7823 - val_loss: 0.7211 - val_acc: 0.6300\n",
            "Epoch 32/300\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.63090\n",
            "620/620 - 1s - loss: 0.4747 - acc: 0.7790 - val_loss: 0.7445 - val_acc: 0.6100\n",
            "Epoch 33/300\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.63090\n",
            "620/620 - 1s - loss: 0.4617 - acc: 0.7952 - val_loss: 0.6956 - val_acc: 0.6700\n",
            "Epoch 34/300\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.63090\n",
            "620/620 - 1s - loss: 0.4733 - acc: 0.7726 - val_loss: 0.6536 - val_acc: 0.6700\n",
            "Epoch 35/300\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.63090\n",
            "620/620 - 1s - loss: 0.4581 - acc: 0.7710 - val_loss: 0.7062 - val_acc: 0.6500\n",
            "Epoch 36/300\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.63090\n",
            "620/620 - 1s - loss: 0.4678 - acc: 0.7677 - val_loss: 0.7102 - val_acc: 0.6500\n",
            "Epoch 37/300\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.63090\n",
            "620/620 - 1s - loss: 0.4678 - acc: 0.7855 - val_loss: 0.7571 - val_acc: 0.6200\n",
            "Epoch 38/300\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.63090\n",
            "620/620 - 1s - loss: 0.4619 - acc: 0.7774 - val_loss: 0.7588 - val_acc: 0.6000\n",
            "Epoch 39/300\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.63090\n",
            "620/620 - 1s - loss: 0.4453 - acc: 0.7613 - val_loss: 0.7470 - val_acc: 0.6000\n",
            "Epoch 40/300\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.63090\n",
            "620/620 - 1s - loss: 0.4263 - acc: 0.8016 - val_loss: 0.7437 - val_acc: 0.6100\n",
            "Epoch 41/300\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.63090\n",
            "620/620 - 1s - loss: 0.4701 - acc: 0.7790 - val_loss: 0.6711 - val_acc: 0.6200\n",
            "Epoch 42/300\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.63090\n",
            "620/620 - 1s - loss: 0.4572 - acc: 0.7677 - val_loss: 0.7711 - val_acc: 0.5700\n",
            "Epoch 43/300\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.63090\n",
            "620/620 - 1s - loss: 0.4597 - acc: 0.7887 - val_loss: 0.8020 - val_acc: 0.6200\n",
            "Epoch 44/300\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.63090\n",
            "620/620 - 1s - loss: 0.4611 - acc: 0.7806 - val_loss: 0.7443 - val_acc: 0.6100\n",
            "Epoch 45/300\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.63090\n",
            "620/620 - 1s - loss: 0.4299 - acc: 0.7823 - val_loss: 0.7476 - val_acc: 0.6400\n",
            "Epoch 46/300\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.63090\n",
            "620/620 - 1s - loss: 0.4363 - acc: 0.8097 - val_loss: 0.7063 - val_acc: 0.6800\n",
            "Epoch 47/300\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.63090\n",
            "620/620 - 1s - loss: 0.4463 - acc: 0.7823 - val_loss: 0.7336 - val_acc: 0.5900\n",
            "Epoch 48/300\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.63090\n",
            "620/620 - 1s - loss: 0.4791 - acc: 0.7774 - val_loss: 0.8041 - val_acc: 0.6200\n",
            "Epoch 49/300\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.63090\n",
            "620/620 - 1s - loss: 0.4307 - acc: 0.8032 - val_loss: 0.8352 - val_acc: 0.6200\n",
            "Epoch 50/300\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.63090\n",
            "620/620 - 1s - loss: 0.4575 - acc: 0.7984 - val_loss: 0.7360 - val_acc: 0.6300\n",
            "Epoch 51/300\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.63090\n",
            "620/620 - 1s - loss: 0.4390 - acc: 0.7984 - val_loss: 0.7711 - val_acc: 0.6400\n",
            "Epoch 52/300\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.63090\n",
            "620/620 - 1s - loss: 0.4274 - acc: 0.8210 - val_loss: 0.8080 - val_acc: 0.5900\n",
            "Epoch 53/300\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.63090\n",
            "620/620 - 1s - loss: 0.4286 - acc: 0.7968 - val_loss: 0.7273 - val_acc: 0.6600\n",
            "Epoch 54/300\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.63090\n",
            "620/620 - 1s - loss: 0.4361 - acc: 0.7968 - val_loss: 0.6499 - val_acc: 0.7100\n",
            "Epoch 55/300\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.63090\n",
            "620/620 - 1s - loss: 0.4375 - acc: 0.7968 - val_loss: 0.7285 - val_acc: 0.6400\n",
            "Epoch 56/300\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.63090\n",
            "620/620 - 1s - loss: 0.4444 - acc: 0.7952 - val_loss: 0.7320 - val_acc: 0.6200\n",
            "Epoch 57/300\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.63090\n",
            "620/620 - 1s - loss: 0.4297 - acc: 0.8016 - val_loss: 0.7532 - val_acc: 0.6500\n",
            "Epoch 58/300\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.63090\n",
            "620/620 - 1s - loss: 0.4319 - acc: 0.8145 - val_loss: 0.6986 - val_acc: 0.6800\n",
            "Epoch 59/300\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.63090\n",
            "620/620 - 1s - loss: 0.4533 - acc: 0.7919 - val_loss: 0.6787 - val_acc: 0.7200\n",
            "Epoch 60/300\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.63090\n",
            "620/620 - 1s - loss: 0.4476 - acc: 0.7758 - val_loss: 0.7176 - val_acc: 0.6500\n",
            "Epoch 61/300\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.63090\n",
            "620/620 - 1s - loss: 0.4386 - acc: 0.8032 - val_loss: 0.6523 - val_acc: 0.6800\n",
            "Epoch 62/300\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.63090\n",
            "620/620 - 1s - loss: 0.4208 - acc: 0.8016 - val_loss: 0.7064 - val_acc: 0.6700\n",
            "Epoch 63/300\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.63090\n",
            "620/620 - 1s - loss: 0.4498 - acc: 0.7919 - val_loss: 0.6780 - val_acc: 0.6700\n",
            "Epoch 64/300\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.63090\n",
            "620/620 - 1s - loss: 0.4341 - acc: 0.8177 - val_loss: 0.8198 - val_acc: 0.6300\n",
            "Epoch 65/300\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.63090\n",
            "620/620 - 1s - loss: 0.4350 - acc: 0.7968 - val_loss: 0.7771 - val_acc: 0.6300\n",
            "Epoch 66/300\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.63090\n",
            "620/620 - 1s - loss: 0.4384 - acc: 0.7887 - val_loss: 0.8027 - val_acc: 0.6600\n",
            "Epoch 67/300\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.63090\n",
            "620/620 - 1s - loss: 0.4390 - acc: 0.7968 - val_loss: 0.7370 - val_acc: 0.6500\n",
            "Epoch 68/300\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.63090\n",
            "620/620 - 1s - loss: 0.4587 - acc: 0.7855 - val_loss: 0.6432 - val_acc: 0.7100\n",
            "Epoch 69/300\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.63090\n",
            "620/620 - 1s - loss: 0.4391 - acc: 0.8113 - val_loss: 0.6364 - val_acc: 0.6800\n",
            "Epoch 70/300\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.63090\n",
            "620/620 - 1s - loss: 0.4233 - acc: 0.8210 - val_loss: 0.7557 - val_acc: 0.6400\n",
            "Epoch 71/300\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.63090\n",
            "620/620 - 1s - loss: 0.4395 - acc: 0.7855 - val_loss: 0.7569 - val_acc: 0.6100\n",
            "Epoch 72/300\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.63090\n",
            "620/620 - 1s - loss: 0.4445 - acc: 0.7903 - val_loss: 0.7777 - val_acc: 0.6500\n",
            "Epoch 73/300\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.63090\n",
            "620/620 - 1s - loss: 0.4280 - acc: 0.7952 - val_loss: 0.7373 - val_acc: 0.6200\n",
            "Epoch 74/300\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.63090\n",
            "620/620 - 1s - loss: 0.4434 - acc: 0.7887 - val_loss: 0.8242 - val_acc: 0.6000\n",
            "Epoch 75/300\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.63090\n",
            "620/620 - 1s - loss: 0.3971 - acc: 0.8161 - val_loss: 0.7104 - val_acc: 0.6900\n",
            "Epoch 76/300\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.63090\n",
            "620/620 - 1s - loss: 0.4239 - acc: 0.8129 - val_loss: 0.7648 - val_acc: 0.6000\n",
            "Epoch 77/300\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.63090\n",
            "620/620 - 1s - loss: 0.4453 - acc: 0.7597 - val_loss: 0.6974 - val_acc: 0.6500\n",
            "Epoch 78/300\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.63090\n",
            "620/620 - 1s - loss: 0.4405 - acc: 0.7919 - val_loss: 0.6724 - val_acc: 0.7200\n",
            "Epoch 79/300\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.63090\n",
            "620/620 - 1s - loss: 0.4280 - acc: 0.8097 - val_loss: 0.6793 - val_acc: 0.6700\n",
            "Epoch 80/300\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.63090\n",
            "620/620 - 1s - loss: 0.4251 - acc: 0.7935 - val_loss: 0.6388 - val_acc: 0.7100\n",
            "Epoch 81/300\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.63090\n",
            "620/620 - 1s - loss: 0.4220 - acc: 0.8226 - val_loss: 0.6914 - val_acc: 0.6800\n",
            "Epoch 82/300\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.63090\n",
            "620/620 - 1s - loss: 0.4221 - acc: 0.8145 - val_loss: 0.8371 - val_acc: 0.6300\n",
            "Epoch 83/300\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.63090\n",
            "620/620 - 1s - loss: 0.4399 - acc: 0.7887 - val_loss: 0.6570 - val_acc: 0.7000\n",
            "Epoch 84/300\n",
            "\n",
            "Epoch 00084: val_loss improved from 0.63090 to 0.61958, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.4255 - acc: 0.8113 - val_loss: 0.6196 - val_acc: 0.7100\n",
            "Epoch 85/300\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.4327 - acc: 0.7823 - val_loss: 0.7323 - val_acc: 0.6300\n",
            "Epoch 86/300\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.4323 - acc: 0.8097 - val_loss: 0.6703 - val_acc: 0.7000\n",
            "Epoch 87/300\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3891 - acc: 0.8468 - val_loss: 0.7693 - val_acc: 0.6500\n",
            "Epoch 88/300\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.4186 - acc: 0.8145 - val_loss: 0.8526 - val_acc: 0.5800\n",
            "Epoch 89/300\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.4378 - acc: 0.8129 - val_loss: 0.7006 - val_acc: 0.6900\n",
            "Epoch 90/300\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.4222 - acc: 0.8016 - val_loss: 0.7101 - val_acc: 0.6500\n",
            "Epoch 91/300\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.4099 - acc: 0.8339 - val_loss: 0.8188 - val_acc: 0.6600\n",
            "Epoch 92/300\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.4202 - acc: 0.8145 - val_loss: 0.7610 - val_acc: 0.5800\n",
            "Epoch 93/300\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3929 - acc: 0.8355 - val_loss: 0.7443 - val_acc: 0.6200\n",
            "Epoch 94/300\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.4057 - acc: 0.8129 - val_loss: 0.7693 - val_acc: 0.6400\n",
            "Epoch 95/300\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.4184 - acc: 0.8065 - val_loss: 0.7121 - val_acc: 0.6700\n",
            "Epoch 96/300\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.4257 - acc: 0.8065 - val_loss: 0.7721 - val_acc: 0.5900\n",
            "Epoch 97/300\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.4113 - acc: 0.8177 - val_loss: 0.8956 - val_acc: 0.6200\n",
            "Epoch 98/300\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.4117 - acc: 0.8226 - val_loss: 0.7099 - val_acc: 0.6700\n",
            "Epoch 99/300\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.4083 - acc: 0.8145 - val_loss: 0.7161 - val_acc: 0.6200\n",
            "Epoch 100/300\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3985 - acc: 0.8145 - val_loss: 0.7396 - val_acc: 0.6500\n",
            "Epoch 101/300\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.4057 - acc: 0.8194 - val_loss: 0.7547 - val_acc: 0.6500\n",
            "Epoch 102/300\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.4113 - acc: 0.8081 - val_loss: 0.8322 - val_acc: 0.6300\n",
            "Epoch 103/300\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.4090 - acc: 0.8113 - val_loss: 0.7527 - val_acc: 0.6400\n",
            "Epoch 104/300\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.4157 - acc: 0.7806 - val_loss: 0.6503 - val_acc: 0.7200\n",
            "Epoch 105/300\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3988 - acc: 0.8194 - val_loss: 0.7299 - val_acc: 0.7000\n",
            "Epoch 106/300\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.4152 - acc: 0.8145 - val_loss: 0.7010 - val_acc: 0.7000\n",
            "Epoch 107/300\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.4055 - acc: 0.8081 - val_loss: 0.8678 - val_acc: 0.6100\n",
            "Epoch 108/300\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.4103 - acc: 0.8226 - val_loss: 0.8284 - val_acc: 0.6100\n",
            "Epoch 109/300\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.4169 - acc: 0.7984 - val_loss: 0.7270 - val_acc: 0.6600\n",
            "Epoch 110/300\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.4087 - acc: 0.8113 - val_loss: 0.8867 - val_acc: 0.5900\n",
            "Epoch 111/300\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3996 - acc: 0.8177 - val_loss: 0.7244 - val_acc: 0.6600\n",
            "Epoch 112/300\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.4164 - acc: 0.8113 - val_loss: 0.7567 - val_acc: 0.6400\n",
            "Epoch 113/300\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.4139 - acc: 0.8129 - val_loss: 0.6671 - val_acc: 0.6700\n",
            "Epoch 114/300\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3960 - acc: 0.8258 - val_loss: 0.6860 - val_acc: 0.6800\n",
            "Epoch 115/300\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3777 - acc: 0.8323 - val_loss: 0.7751 - val_acc: 0.6400\n",
            "Epoch 116/300\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3907 - acc: 0.8226 - val_loss: 0.7151 - val_acc: 0.6800\n",
            "Epoch 117/300\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3958 - acc: 0.8194 - val_loss: 0.7198 - val_acc: 0.6900\n",
            "Epoch 118/300\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3860 - acc: 0.8210 - val_loss: 0.6778 - val_acc: 0.6800\n",
            "Epoch 119/300\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.4249 - acc: 0.8016 - val_loss: 0.7460 - val_acc: 0.6300\n",
            "Epoch 120/300\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.4172 - acc: 0.8016 - val_loss: 0.6859 - val_acc: 0.6900\n",
            "Epoch 121/300\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.4121 - acc: 0.7968 - val_loss: 0.6378 - val_acc: 0.7100\n",
            "Epoch 122/300\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3809 - acc: 0.8468 - val_loss: 0.7096 - val_acc: 0.6900\n",
            "Epoch 123/300\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.4028 - acc: 0.8177 - val_loss: 0.6643 - val_acc: 0.7000\n",
            "Epoch 124/300\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3981 - acc: 0.8065 - val_loss: 0.7261 - val_acc: 0.6600\n",
            "Epoch 125/300\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.4044 - acc: 0.7968 - val_loss: 0.7798 - val_acc: 0.6700\n",
            "Epoch 126/300\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3975 - acc: 0.8065 - val_loss: 0.8057 - val_acc: 0.6100\n",
            "Epoch 127/300\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.4075 - acc: 0.8016 - val_loss: 0.7067 - val_acc: 0.7000\n",
            "Epoch 128/300\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.4171 - acc: 0.8065 - val_loss: 0.7081 - val_acc: 0.6600\n",
            "Epoch 129/300\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3845 - acc: 0.8323 - val_loss: 0.6801 - val_acc: 0.6800\n",
            "Epoch 130/300\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.4156 - acc: 0.8081 - val_loss: 0.7301 - val_acc: 0.6700\n",
            "Epoch 131/300\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.4021 - acc: 0.8194 - val_loss: 0.7405 - val_acc: 0.6400\n",
            "Epoch 132/300\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3744 - acc: 0.8484 - val_loss: 0.7224 - val_acc: 0.6800\n",
            "Epoch 133/300\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3895 - acc: 0.8081 - val_loss: 0.7750 - val_acc: 0.6700\n",
            "Epoch 134/300\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3891 - acc: 0.8274 - val_loss: 0.7128 - val_acc: 0.6900\n",
            "Epoch 135/300\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.4182 - acc: 0.8048 - val_loss: 0.6413 - val_acc: 0.7000\n",
            "Epoch 136/300\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.4255 - acc: 0.7984 - val_loss: 0.6803 - val_acc: 0.6800\n",
            "Epoch 137/300\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.4077 - acc: 0.8048 - val_loss: 0.6960 - val_acc: 0.6600\n",
            "Epoch 138/300\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3947 - acc: 0.8226 - val_loss: 0.7470 - val_acc: 0.6200\n",
            "Epoch 139/300\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3886 - acc: 0.8290 - val_loss: 0.6891 - val_acc: 0.7000\n",
            "Epoch 140/300\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3849 - acc: 0.8323 - val_loss: 0.7048 - val_acc: 0.6900\n",
            "Epoch 141/300\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3806 - acc: 0.8403 - val_loss: 0.7948 - val_acc: 0.6400\n",
            "Epoch 142/300\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3961 - acc: 0.8210 - val_loss: 0.7497 - val_acc: 0.6500\n",
            "Epoch 143/300\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3946 - acc: 0.8097 - val_loss: 0.6680 - val_acc: 0.7100\n",
            "Epoch 144/300\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3794 - acc: 0.8177 - val_loss: 0.7523 - val_acc: 0.6400\n",
            "Epoch 145/300\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3727 - acc: 0.8371 - val_loss: 0.8324 - val_acc: 0.6400\n",
            "Epoch 146/300\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.4143 - acc: 0.8161 - val_loss: 0.6492 - val_acc: 0.6900\n",
            "Epoch 147/300\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3845 - acc: 0.8306 - val_loss: 0.7228 - val_acc: 0.6900\n",
            "Epoch 148/300\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.4277 - acc: 0.7855 - val_loss: 0.7388 - val_acc: 0.6600\n",
            "Epoch 149/300\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3933 - acc: 0.8177 - val_loss: 0.6635 - val_acc: 0.7000\n",
            "Epoch 150/300\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3924 - acc: 0.8113 - val_loss: 0.6754 - val_acc: 0.6800\n",
            "Epoch 151/300\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3900 - acc: 0.8210 - val_loss: 0.7046 - val_acc: 0.6700\n",
            "Epoch 152/300\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.4085 - acc: 0.8194 - val_loss: 0.7069 - val_acc: 0.6700\n",
            "Epoch 153/300\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3917 - acc: 0.8177 - val_loss: 0.6718 - val_acc: 0.6800\n",
            "Epoch 154/300\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3989 - acc: 0.8177 - val_loss: 0.8222 - val_acc: 0.6000\n",
            "Epoch 155/300\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3696 - acc: 0.8339 - val_loss: 0.7287 - val_acc: 0.6800\n",
            "Epoch 156/300\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3810 - acc: 0.8323 - val_loss: 0.7465 - val_acc: 0.6600\n",
            "Epoch 157/300\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3838 - acc: 0.8290 - val_loss: 0.7314 - val_acc: 0.6600\n",
            "Epoch 158/300\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3815 - acc: 0.8258 - val_loss: 0.7398 - val_acc: 0.6600\n",
            "Epoch 159/300\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3919 - acc: 0.8355 - val_loss: 0.7591 - val_acc: 0.6200\n",
            "Epoch 160/300\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3811 - acc: 0.8274 - val_loss: 0.8102 - val_acc: 0.6200\n",
            "Epoch 161/300\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3864 - acc: 0.8323 - val_loss: 0.8181 - val_acc: 0.6200\n",
            "Epoch 162/300\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3935 - acc: 0.8323 - val_loss: 0.7657 - val_acc: 0.6500\n",
            "Epoch 163/300\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3932 - acc: 0.8161 - val_loss: 0.6852 - val_acc: 0.6900\n",
            "Epoch 164/300\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.4080 - acc: 0.8129 - val_loss: 0.6590 - val_acc: 0.7000\n",
            "Epoch 165/300\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3802 - acc: 0.8274 - val_loss: 0.7881 - val_acc: 0.6700\n",
            "Epoch 166/300\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3721 - acc: 0.8194 - val_loss: 0.8085 - val_acc: 0.6400\n",
            "Epoch 167/300\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.4002 - acc: 0.8177 - val_loss: 0.7590 - val_acc: 0.6900\n",
            "Epoch 168/300\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3867 - acc: 0.8339 - val_loss: 0.7014 - val_acc: 0.6800\n",
            "Epoch 169/300\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3696 - acc: 0.8371 - val_loss: 0.6971 - val_acc: 0.7100\n",
            "Epoch 170/300\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3613 - acc: 0.8242 - val_loss: 0.7494 - val_acc: 0.6200\n",
            "Epoch 171/300\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.4070 - acc: 0.8097 - val_loss: 0.6365 - val_acc: 0.6900\n",
            "Epoch 172/300\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3718 - acc: 0.8452 - val_loss: 0.7484 - val_acc: 0.6000\n",
            "Epoch 173/300\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.4079 - acc: 0.8274 - val_loss: 0.7843 - val_acc: 0.5800\n",
            "Epoch 174/300\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3790 - acc: 0.8355 - val_loss: 0.6393 - val_acc: 0.7100\n",
            "Epoch 175/300\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3896 - acc: 0.8242 - val_loss: 0.8098 - val_acc: 0.6300\n",
            "Epoch 176/300\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3858 - acc: 0.8339 - val_loss: 0.7599 - val_acc: 0.6300\n",
            "Epoch 177/300\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3762 - acc: 0.8194 - val_loss: 0.7605 - val_acc: 0.6100\n",
            "Epoch 178/300\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3776 - acc: 0.8290 - val_loss: 0.8051 - val_acc: 0.6000\n",
            "Epoch 179/300\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3827 - acc: 0.8290 - val_loss: 0.7095 - val_acc: 0.7000\n",
            "Epoch 180/300\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3645 - acc: 0.8387 - val_loss: 0.7762 - val_acc: 0.6600\n",
            "Epoch 181/300\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3672 - acc: 0.8419 - val_loss: 0.7137 - val_acc: 0.6900\n",
            "Epoch 182/300\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3850 - acc: 0.8226 - val_loss: 0.8021 - val_acc: 0.6100\n",
            "Epoch 183/300\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3879 - acc: 0.8323 - val_loss: 0.7820 - val_acc: 0.6100\n",
            "Epoch 184/300\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3666 - acc: 0.8403 - val_loss: 0.6534 - val_acc: 0.6900\n",
            "Epoch 185/300\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3915 - acc: 0.8194 - val_loss: 0.8279 - val_acc: 0.6000\n",
            "Epoch 186/300\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3784 - acc: 0.8403 - val_loss: 0.7783 - val_acc: 0.6000\n",
            "Epoch 187/300\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3626 - acc: 0.8565 - val_loss: 0.8141 - val_acc: 0.6000\n",
            "Epoch 188/300\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3645 - acc: 0.8355 - val_loss: 0.6412 - val_acc: 0.7000\n",
            "Epoch 189/300\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3885 - acc: 0.8194 - val_loss: 0.7882 - val_acc: 0.6000\n",
            "Epoch 190/300\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3844 - acc: 0.8226 - val_loss: 0.7218 - val_acc: 0.6700\n",
            "Epoch 191/300\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3623 - acc: 0.8581 - val_loss: 0.7466 - val_acc: 0.6500\n",
            "Epoch 192/300\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3805 - acc: 0.8323 - val_loss: 0.8950 - val_acc: 0.5500\n",
            "Epoch 193/300\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3804 - acc: 0.8323 - val_loss: 0.6587 - val_acc: 0.7000\n",
            "Epoch 194/300\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3793 - acc: 0.8339 - val_loss: 0.7634 - val_acc: 0.6400\n",
            "Epoch 195/300\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3785 - acc: 0.8339 - val_loss: 0.7346 - val_acc: 0.6700\n",
            "Epoch 196/300\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3687 - acc: 0.8242 - val_loss: 0.7616 - val_acc: 0.6200\n",
            "Epoch 197/300\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3555 - acc: 0.8452 - val_loss: 0.6499 - val_acc: 0.6800\n",
            "Epoch 198/300\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3759 - acc: 0.8323 - val_loss: 0.7610 - val_acc: 0.6400\n",
            "Epoch 199/300\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3763 - acc: 0.8452 - val_loss: 0.7434 - val_acc: 0.6500\n",
            "Epoch 200/300\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3779 - acc: 0.8161 - val_loss: 0.7030 - val_acc: 0.6800\n",
            "Epoch 201/300\n",
            "\n",
            "Epoch 00201: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3894 - acc: 0.8145 - val_loss: 0.7561 - val_acc: 0.6600\n",
            "Epoch 202/300\n",
            "\n",
            "Epoch 00202: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3800 - acc: 0.8323 - val_loss: 0.6371 - val_acc: 0.7100\n",
            "Epoch 203/300\n",
            "\n",
            "Epoch 00203: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3867 - acc: 0.8258 - val_loss: 0.6805 - val_acc: 0.6700\n",
            "Epoch 204/300\n",
            "\n",
            "Epoch 00204: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3625 - acc: 0.8419 - val_loss: 0.8056 - val_acc: 0.5900\n",
            "Epoch 205/300\n",
            "\n",
            "Epoch 00205: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3888 - acc: 0.8177 - val_loss: 0.7895 - val_acc: 0.6000\n",
            "Epoch 206/300\n",
            "\n",
            "Epoch 00206: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3962 - acc: 0.8177 - val_loss: 0.8228 - val_acc: 0.6200\n",
            "Epoch 207/300\n",
            "\n",
            "Epoch 00207: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.4094 - acc: 0.8048 - val_loss: 0.7338 - val_acc: 0.6600\n",
            "Epoch 208/300\n",
            "\n",
            "Epoch 00208: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3843 - acc: 0.8435 - val_loss: 0.7580 - val_acc: 0.6300\n",
            "Epoch 209/300\n",
            "\n",
            "Epoch 00209: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3697 - acc: 0.8468 - val_loss: 0.7485 - val_acc: 0.6800\n",
            "Epoch 210/300\n",
            "\n",
            "Epoch 00210: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3715 - acc: 0.8387 - val_loss: 0.7597 - val_acc: 0.6300\n",
            "Epoch 211/300\n",
            "\n",
            "Epoch 00211: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3750 - acc: 0.8500 - val_loss: 0.8598 - val_acc: 0.6300\n",
            "Epoch 212/300\n",
            "\n",
            "Epoch 00212: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3755 - acc: 0.8274 - val_loss: 0.8565 - val_acc: 0.5800\n",
            "Epoch 213/300\n",
            "\n",
            "Epoch 00213: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3698 - acc: 0.8484 - val_loss: 0.8124 - val_acc: 0.5800\n",
            "Epoch 214/300\n",
            "\n",
            "Epoch 00214: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3659 - acc: 0.8371 - val_loss: 0.7610 - val_acc: 0.6400\n",
            "Epoch 215/300\n",
            "\n",
            "Epoch 00215: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3542 - acc: 0.8516 - val_loss: 0.7415 - val_acc: 0.6700\n",
            "Epoch 216/300\n",
            "\n",
            "Epoch 00216: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3946 - acc: 0.8242 - val_loss: 0.7057 - val_acc: 0.6600\n",
            "Epoch 217/300\n",
            "\n",
            "Epoch 00217: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3778 - acc: 0.8242 - val_loss: 0.7577 - val_acc: 0.6000\n",
            "Epoch 218/300\n",
            "\n",
            "Epoch 00218: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3737 - acc: 0.8306 - val_loss: 0.7273 - val_acc: 0.6700\n",
            "Epoch 219/300\n",
            "\n",
            "Epoch 00219: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3775 - acc: 0.8226 - val_loss: 0.7507 - val_acc: 0.6400\n",
            "Epoch 220/300\n",
            "\n",
            "Epoch 00220: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3891 - acc: 0.8435 - val_loss: 0.6757 - val_acc: 0.6900\n",
            "Epoch 221/300\n",
            "\n",
            "Epoch 00221: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3714 - acc: 0.8371 - val_loss: 0.8900 - val_acc: 0.5600\n",
            "Epoch 222/300\n",
            "\n",
            "Epoch 00222: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3771 - acc: 0.8355 - val_loss: 0.7566 - val_acc: 0.6200\n",
            "Epoch 223/300\n",
            "\n",
            "Epoch 00223: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3658 - acc: 0.8339 - val_loss: 0.7148 - val_acc: 0.6800\n",
            "Epoch 224/300\n",
            "\n",
            "Epoch 00224: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3778 - acc: 0.8323 - val_loss: 0.6801 - val_acc: 0.6600\n",
            "Epoch 225/300\n",
            "\n",
            "Epoch 00225: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3715 - acc: 0.8226 - val_loss: 0.7231 - val_acc: 0.6700\n",
            "Epoch 226/300\n",
            "\n",
            "Epoch 00226: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3787 - acc: 0.8435 - val_loss: 0.7258 - val_acc: 0.6700\n",
            "Epoch 227/300\n",
            "\n",
            "Epoch 00227: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3791 - acc: 0.8355 - val_loss: 0.7358 - val_acc: 0.6700\n",
            "Epoch 228/300\n",
            "\n",
            "Epoch 00228: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3607 - acc: 0.8290 - val_loss: 0.8392 - val_acc: 0.5800\n",
            "Epoch 229/300\n",
            "\n",
            "Epoch 00229: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3474 - acc: 0.8532 - val_loss: 0.6824 - val_acc: 0.7100\n",
            "Epoch 230/300\n",
            "\n",
            "Epoch 00230: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3636 - acc: 0.8371 - val_loss: 0.9632 - val_acc: 0.5600\n",
            "Epoch 231/300\n",
            "\n",
            "Epoch 00231: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3786 - acc: 0.8226 - val_loss: 0.7566 - val_acc: 0.6500\n",
            "Epoch 232/300\n",
            "\n",
            "Epoch 00232: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3564 - acc: 0.8484 - val_loss: 0.6513 - val_acc: 0.7100\n",
            "Epoch 233/300\n",
            "\n",
            "Epoch 00233: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3742 - acc: 0.8355 - val_loss: 0.7654 - val_acc: 0.6600\n",
            "Epoch 234/300\n",
            "\n",
            "Epoch 00234: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3762 - acc: 0.8468 - val_loss: 0.6817 - val_acc: 0.6800\n",
            "Epoch 235/300\n",
            "\n",
            "Epoch 00235: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3702 - acc: 0.8290 - val_loss: 0.6913 - val_acc: 0.6700\n",
            "Epoch 236/300\n",
            "\n",
            "Epoch 00236: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3743 - acc: 0.8226 - val_loss: 0.6551 - val_acc: 0.7100\n",
            "Epoch 237/300\n",
            "\n",
            "Epoch 00237: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3557 - acc: 0.8468 - val_loss: 0.8599 - val_acc: 0.6200\n",
            "Epoch 238/300\n",
            "\n",
            "Epoch 00238: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3523 - acc: 0.8355 - val_loss: 0.8149 - val_acc: 0.6000\n",
            "Epoch 239/300\n",
            "\n",
            "Epoch 00239: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3728 - acc: 0.8290 - val_loss: 0.7663 - val_acc: 0.6100\n",
            "Epoch 240/300\n",
            "\n",
            "Epoch 00240: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3421 - acc: 0.8645 - val_loss: 0.9795 - val_acc: 0.5800\n",
            "Epoch 241/300\n",
            "\n",
            "Epoch 00241: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3567 - acc: 0.8435 - val_loss: 0.6573 - val_acc: 0.7300\n",
            "Epoch 242/300\n",
            "\n",
            "Epoch 00242: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3386 - acc: 0.8516 - val_loss: 0.8481 - val_acc: 0.6300\n",
            "Epoch 243/300\n",
            "\n",
            "Epoch 00243: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3792 - acc: 0.8339 - val_loss: 0.7460 - val_acc: 0.6800\n",
            "Epoch 244/300\n",
            "\n",
            "Epoch 00244: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3925 - acc: 0.8242 - val_loss: 0.7013 - val_acc: 0.6300\n",
            "Epoch 245/300\n",
            "\n",
            "Epoch 00245: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3655 - acc: 0.8468 - val_loss: 0.7472 - val_acc: 0.6400\n",
            "Epoch 246/300\n",
            "\n",
            "Epoch 00246: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3647 - acc: 0.8242 - val_loss: 0.6809 - val_acc: 0.6400\n",
            "Epoch 247/300\n",
            "\n",
            "Epoch 00247: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3750 - acc: 0.8306 - val_loss: 0.7821 - val_acc: 0.6000\n",
            "Epoch 248/300\n",
            "\n",
            "Epoch 00248: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3820 - acc: 0.8258 - val_loss: 0.7436 - val_acc: 0.5800\n",
            "Epoch 249/300\n",
            "\n",
            "Epoch 00249: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3559 - acc: 0.8484 - val_loss: 0.9348 - val_acc: 0.5600\n",
            "Epoch 250/300\n",
            "\n",
            "Epoch 00250: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3528 - acc: 0.8403 - val_loss: 0.7688 - val_acc: 0.6400\n",
            "Epoch 251/300\n",
            "\n",
            "Epoch 00251: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3800 - acc: 0.8419 - val_loss: 0.6938 - val_acc: 0.6900\n",
            "Epoch 252/300\n",
            "\n",
            "Epoch 00252: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3601 - acc: 0.8516 - val_loss: 0.7532 - val_acc: 0.6700\n",
            "Epoch 253/300\n",
            "\n",
            "Epoch 00253: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3709 - acc: 0.8258 - val_loss: 0.9080 - val_acc: 0.5800\n",
            "Epoch 254/300\n",
            "\n",
            "Epoch 00254: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3831 - acc: 0.8242 - val_loss: 0.7568 - val_acc: 0.6200\n",
            "Epoch 255/300\n",
            "\n",
            "Epoch 00255: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3820 - acc: 0.8242 - val_loss: 0.8942 - val_acc: 0.6000\n",
            "Epoch 256/300\n",
            "\n",
            "Epoch 00256: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3665 - acc: 0.8177 - val_loss: 0.8928 - val_acc: 0.5800\n",
            "Epoch 257/300\n",
            "\n",
            "Epoch 00257: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3745 - acc: 0.8355 - val_loss: 0.8197 - val_acc: 0.6100\n",
            "Epoch 258/300\n",
            "\n",
            "Epoch 00258: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3900 - acc: 0.8194 - val_loss: 0.6578 - val_acc: 0.6700\n",
            "Epoch 259/300\n",
            "\n",
            "Epoch 00259: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3802 - acc: 0.8258 - val_loss: 0.7718 - val_acc: 0.6600\n",
            "Epoch 260/300\n",
            "\n",
            "Epoch 00260: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3673 - acc: 0.8258 - val_loss: 0.7418 - val_acc: 0.6400\n",
            "Epoch 261/300\n",
            "\n",
            "Epoch 00261: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3905 - acc: 0.8306 - val_loss: 0.7784 - val_acc: 0.6100\n",
            "Epoch 262/300\n",
            "\n",
            "Epoch 00262: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3720 - acc: 0.8290 - val_loss: 0.7757 - val_acc: 0.5800\n",
            "Epoch 263/300\n",
            "\n",
            "Epoch 00263: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3476 - acc: 0.8500 - val_loss: 0.8388 - val_acc: 0.5900\n",
            "Epoch 264/300\n",
            "\n",
            "Epoch 00264: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3487 - acc: 0.8290 - val_loss: 0.8094 - val_acc: 0.6000\n",
            "Epoch 265/300\n",
            "\n",
            "Epoch 00265: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3538 - acc: 0.8403 - val_loss: 0.7219 - val_acc: 0.6400\n",
            "Epoch 266/300\n",
            "\n",
            "Epoch 00266: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3611 - acc: 0.8419 - val_loss: 0.7121 - val_acc: 0.6700\n",
            "Epoch 267/300\n",
            "\n",
            "Epoch 00267: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3349 - acc: 0.8597 - val_loss: 0.7250 - val_acc: 0.6700\n",
            "Epoch 268/300\n",
            "\n",
            "Epoch 00268: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3755 - acc: 0.8468 - val_loss: 0.7061 - val_acc: 0.6700\n",
            "Epoch 269/300\n",
            "\n",
            "Epoch 00269: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3627 - acc: 0.8323 - val_loss: 0.8696 - val_acc: 0.5900\n",
            "Epoch 270/300\n",
            "\n",
            "Epoch 00270: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3488 - acc: 0.8435 - val_loss: 0.8856 - val_acc: 0.5900\n",
            "Epoch 271/300\n",
            "\n",
            "Epoch 00271: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3693 - acc: 0.8339 - val_loss: 0.8137 - val_acc: 0.5500\n",
            "Epoch 272/300\n",
            "\n",
            "Epoch 00272: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3629 - acc: 0.8355 - val_loss: 0.7450 - val_acc: 0.6600\n",
            "Epoch 273/300\n",
            "\n",
            "Epoch 00273: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3633 - acc: 0.8403 - val_loss: 0.6832 - val_acc: 0.6900\n",
            "Epoch 274/300\n",
            "\n",
            "Epoch 00274: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3735 - acc: 0.8290 - val_loss: 0.7896 - val_acc: 0.6000\n",
            "Epoch 275/300\n",
            "\n",
            "Epoch 00275: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3605 - acc: 0.8371 - val_loss: 0.7658 - val_acc: 0.6700\n",
            "Epoch 276/300\n",
            "\n",
            "Epoch 00276: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3653 - acc: 0.8339 - val_loss: 0.8170 - val_acc: 0.6100\n",
            "Epoch 277/300\n",
            "\n",
            "Epoch 00277: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3484 - acc: 0.8532 - val_loss: 0.7038 - val_acc: 0.6500\n",
            "Epoch 278/300\n",
            "\n",
            "Epoch 00278: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3614 - acc: 0.8419 - val_loss: 0.8681 - val_acc: 0.6200\n",
            "Epoch 279/300\n",
            "\n",
            "Epoch 00279: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3579 - acc: 0.8387 - val_loss: 0.7958 - val_acc: 0.6000\n",
            "Epoch 280/300\n",
            "\n",
            "Epoch 00280: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3518 - acc: 0.8323 - val_loss: 0.8141 - val_acc: 0.6200\n",
            "Epoch 281/300\n",
            "\n",
            "Epoch 00281: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3366 - acc: 0.8468 - val_loss: 0.8220 - val_acc: 0.6400\n",
            "Epoch 282/300\n",
            "\n",
            "Epoch 00282: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3803 - acc: 0.8290 - val_loss: 0.6650 - val_acc: 0.7200\n",
            "Epoch 283/300\n",
            "\n",
            "Epoch 00283: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3558 - acc: 0.8419 - val_loss: 0.7598 - val_acc: 0.6400\n",
            "Epoch 284/300\n",
            "\n",
            "Epoch 00284: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3631 - acc: 0.8581 - val_loss: 0.8264 - val_acc: 0.6000\n",
            "Epoch 285/300\n",
            "\n",
            "Epoch 00285: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3869 - acc: 0.8339 - val_loss: 0.8243 - val_acc: 0.6200\n",
            "Epoch 286/300\n",
            "\n",
            "Epoch 00286: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3470 - acc: 0.8468 - val_loss: 0.9675 - val_acc: 0.5500\n",
            "Epoch 287/300\n",
            "\n",
            "Epoch 00287: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3630 - acc: 0.8306 - val_loss: 1.0082 - val_acc: 0.5600\n",
            "Epoch 288/300\n",
            "\n",
            "Epoch 00288: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3373 - acc: 0.8468 - val_loss: 0.9853 - val_acc: 0.5700\n",
            "Epoch 289/300\n",
            "\n",
            "Epoch 00289: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3540 - acc: 0.8484 - val_loss: 0.8218 - val_acc: 0.6100\n",
            "Epoch 290/300\n",
            "\n",
            "Epoch 00290: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3625 - acc: 0.8419 - val_loss: 0.8576 - val_acc: 0.5800\n",
            "Epoch 291/300\n",
            "\n",
            "Epoch 00291: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3450 - acc: 0.8371 - val_loss: 0.8928 - val_acc: 0.5800\n",
            "Epoch 292/300\n",
            "\n",
            "Epoch 00292: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3417 - acc: 0.8500 - val_loss: 0.7970 - val_acc: 0.6200\n",
            "Epoch 293/300\n",
            "\n",
            "Epoch 00293: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3671 - acc: 0.8387 - val_loss: 0.7766 - val_acc: 0.6000\n",
            "Epoch 294/300\n",
            "\n",
            "Epoch 00294: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3723 - acc: 0.8339 - val_loss: 0.6646 - val_acc: 0.7100\n",
            "Epoch 295/300\n",
            "\n",
            "Epoch 00295: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3583 - acc: 0.8597 - val_loss: 0.7963 - val_acc: 0.6000\n",
            "Epoch 296/300\n",
            "\n",
            "Epoch 00296: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3229 - acc: 0.8661 - val_loss: 0.7981 - val_acc: 0.6300\n",
            "Epoch 297/300\n",
            "\n",
            "Epoch 00297: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3177 - acc: 0.8597 - val_loss: 0.7919 - val_acc: 0.6300\n",
            "Epoch 298/300\n",
            "\n",
            "Epoch 00298: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3143 - acc: 0.8661 - val_loss: 0.7015 - val_acc: 0.6800\n",
            "Epoch 299/300\n",
            "\n",
            "Epoch 00299: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3746 - acc: 0.8355 - val_loss: 0.7632 - val_acc: 0.6700\n",
            "Epoch 300/300\n",
            "\n",
            "Epoch 00300: val_loss did not improve from 0.61958\n",
            "620/620 - 1s - loss: 0.3719 - acc: 0.8210 - val_loss: 0.8086 - val_acc: 0.6000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "# Evaluate on test data\n",
            "40/40 [==============================] - 0s 3ms/sample - loss: 0.6786 - acc: 0.6250\n",
            "test loss, test acc: [0.6785794630181045, 0.625]\n",
            "1\n",
            "EEG_Deep/Data2A/parsed_P01T.mat\n",
            "(0, 12, 4096)\n",
            "(80, 12, 4096)\n",
            "(80, 1)\n",
            "2\n",
            "EEG_Deep/Data2A/parsed_P02T.mat\n",
            "(80, 12, 4096)\n",
            "(160, 12, 4096)\n",
            "(160, 1)\n",
            "3\n",
            "EEG_Deep/Data2A/parsed_P03T.mat\n",
            "(160, 12, 4096)\n",
            "(240, 12, 4096)\n",
            "(240, 1)\n",
            "5\n",
            "EEG_Deep/Data2A/parsed_P05T.mat\n",
            "(240, 12, 4096)\n",
            "(320, 12, 4096)\n",
            "(320, 1)\n",
            "6\n",
            "EEG_Deep/Data2A/parsed_P06T.mat\n",
            "(320, 12, 4096)\n",
            "(400, 12, 4096)\n",
            "(400, 1)\n",
            "7\n",
            "EEG_Deep/Data2A/parsed_P07T.mat\n",
            "(400, 12, 4096)\n",
            "(480, 12, 4096)\n",
            "(480, 1)\n",
            "8\n",
            "EEG_Deep/Data2A/parsed_P08T.mat\n",
            "(480, 12, 4096)\n",
            "(560, 12, 4096)\n",
            "(560, 1)\n",
            "9\n",
            "EEG_Deep/Data2A/parsed_P09T.mat\n",
            "(560, 12, 4096)\n",
            "(640, 12, 4096)\n",
            "(640, 1)\n",
            "10\n",
            "EEG_Deep/Data2A/parsed_P010T.mat\n",
            "(640, 12, 4096)\n",
            "(720, 12, 4096)\n",
            "(720, 1)\n",
            "4\n",
            "EEG_Deep/Data2A/parsed_P04E.mat\n",
            "(0, 12, 4096)\n",
            "(40, 12, 4096)\n",
            "(40, 1)\n",
            "Filtering of training data in progress\n",
            "(720, 12, 4096)\n",
            "Filtering of test data in progress\n",
            "(40, 12, 4096)\n",
            "[1. 2. 2. 1. 2. 1. 1. 2. 1. 2. 2. 1. 1. 1. 2. 2. 1. 2. 2. 2. 1. 1. 2. 2.\n",
            " 1. 2. 1. 2. 1. 1. 1. 1. 1. 1. 1. 2. 1. 2. 2. 2. 1. 1. 1. 2. 2. 2. 1. 2.\n",
            " 2. 2. 1. 1. 1. 2. 2. 2. 1. 2. 1. 2. 2. 1. 1. 2. 1. 1. 2. 1. 1. 2. 2. 1.\n",
            " 2. 2. 2. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 1. 1. 1. 2. 1. 1. 1.\n",
            " 1. 2. 1. 1.]\n",
            "(620, 12, 1536)\n",
            "(620,)\n",
            "(100, 12, 1536)\n",
            "(100,)\n",
            "[[1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]]\n",
            "X_train shape: (620, 1, 12, 1536)\n",
            "620 train samples\n",
            "100 val samples\n",
            "(40, 12, 1536)\n",
            "(40, 1)\n",
            "X_train shape: (40, 1, 12, 1536)\n",
            "40 train samples\n",
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 1, 12, 1536)]     0         \n",
            "_________________________________________________________________\n",
            "conv2d (Conv2D)              (None, 8, 12, 1536)       200       \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 8, 12, 1536)       32        \n",
            "_________________________________________________________________\n",
            "depthwise_conv2d (DepthwiseC (None, 16, 1, 1536)       192       \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 16, 1, 1536)       64        \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 16, 1, 1536)       0         \n",
            "_________________________________________________________________\n",
            "average_pooling2d (AveragePo (None, 16, 1, 384)        0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 16, 1, 384)        0         \n",
            "_________________________________________________________________\n",
            "separable_conv2d (SeparableC (None, 16, 1, 384)        512       \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 16, 1, 384)        64        \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 16, 1, 384)        0         \n",
            "_________________________________________________________________\n",
            "average_pooling2d_1 (Average (None, 16, 1, 48)         0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 16, 1, 48)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 768)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 2)                 1538      \n",
            "_________________________________________________________________\n",
            "softmax (Activation)         (None, 2)                 0         \n",
            "=================================================================\n",
            "Total params: 2,602\n",
            "Trainable params: 2,522\n",
            "Non-trainable params: 80\n",
            "_________________________________________________________________\n",
            "Train on 620 samples, validate on 100 samples\n",
            "Epoch 1/300\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.69217, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.6910 - acc: 0.5129 - val_loss: 0.6922 - val_acc: 0.4500\n",
            "Epoch 2/300\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.69217 to 0.68701, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.6586 - acc: 0.6210 - val_loss: 0.6870 - val_acc: 0.5800\n",
            "Epoch 3/300\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.68701 to 0.67967, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.6349 - acc: 0.6419 - val_loss: 0.6797 - val_acc: 0.6000\n",
            "Epoch 4/300\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.67967\n",
            "620/620 - 1s - loss: 0.6146 - acc: 0.6661 - val_loss: 0.6838 - val_acc: 0.4900\n",
            "Epoch 5/300\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.67967\n",
            "620/620 - 1s - loss: 0.5828 - acc: 0.6887 - val_loss: 0.6962 - val_acc: 0.4900\n",
            "Epoch 6/300\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.67967\n",
            "620/620 - 1s - loss: 0.5850 - acc: 0.7016 - val_loss: 0.6964 - val_acc: 0.5000\n",
            "Epoch 7/300\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.67967 to 0.67690, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.5652 - acc: 0.7065 - val_loss: 0.6769 - val_acc: 0.5900\n",
            "Epoch 8/300\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.67690 to 0.65545, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.5621 - acc: 0.7032 - val_loss: 0.6555 - val_acc: 0.5900\n",
            "Epoch 9/300\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.65545 to 0.64660, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.5609 - acc: 0.7081 - val_loss: 0.6466 - val_acc: 0.6000\n",
            "Epoch 10/300\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.64660 to 0.63416, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.5509 - acc: 0.7145 - val_loss: 0.6342 - val_acc: 0.6000\n",
            "Epoch 11/300\n",
            "\n",
            "Epoch 00011: val_loss improved from 0.63416 to 0.62374, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.5389 - acc: 0.7210 - val_loss: 0.6237 - val_acc: 0.6200\n",
            "Epoch 12/300\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.62374\n",
            "620/620 - 1s - loss: 0.5525 - acc: 0.7226 - val_loss: 0.6431 - val_acc: 0.6200\n",
            "Epoch 13/300\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.62374\n",
            "620/620 - 1s - loss: 0.5318 - acc: 0.7387 - val_loss: 0.6264 - val_acc: 0.6300\n",
            "Epoch 14/300\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.62374\n",
            "620/620 - 1s - loss: 0.5426 - acc: 0.7226 - val_loss: 0.6608 - val_acc: 0.5800\n",
            "Epoch 15/300\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.62374\n",
            "620/620 - 1s - loss: 0.5272 - acc: 0.7339 - val_loss: 0.6674 - val_acc: 0.6100\n",
            "Epoch 16/300\n",
            "\n",
            "Epoch 00016: val_loss improved from 0.62374 to 0.59202, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.5324 - acc: 0.7355 - val_loss: 0.5920 - val_acc: 0.6700\n",
            "Epoch 17/300\n",
            "\n",
            "Epoch 00017: val_loss improved from 0.59202 to 0.56395, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.5248 - acc: 0.7274 - val_loss: 0.5640 - val_acc: 0.6700\n",
            "Epoch 18/300\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.56395\n",
            "620/620 - 1s - loss: 0.5435 - acc: 0.7258 - val_loss: 0.5793 - val_acc: 0.6700\n",
            "Epoch 19/300\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.56395\n",
            "620/620 - 1s - loss: 0.5072 - acc: 0.7758 - val_loss: 0.5965 - val_acc: 0.6700\n",
            "Epoch 20/300\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.56395\n",
            "620/620 - 1s - loss: 0.5208 - acc: 0.7484 - val_loss: 0.5846 - val_acc: 0.6700\n",
            "Epoch 21/300\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.56395\n",
            "620/620 - 1s - loss: 0.5010 - acc: 0.7516 - val_loss: 0.5939 - val_acc: 0.6700\n",
            "Epoch 22/300\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.56395\n",
            "620/620 - 1s - loss: 0.5401 - acc: 0.7258 - val_loss: 0.6126 - val_acc: 0.6400\n",
            "Epoch 23/300\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.56395\n",
            "620/620 - 1s - loss: 0.5333 - acc: 0.7339 - val_loss: 0.5806 - val_acc: 0.7000\n",
            "Epoch 24/300\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.56395\n",
            "620/620 - 1s - loss: 0.5029 - acc: 0.7613 - val_loss: 0.6557 - val_acc: 0.6100\n",
            "Epoch 25/300\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.56395\n",
            "620/620 - 1s - loss: 0.4935 - acc: 0.7597 - val_loss: 0.6981 - val_acc: 0.5900\n",
            "Epoch 26/300\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.56395\n",
            "620/620 - 1s - loss: 0.4999 - acc: 0.7548 - val_loss: 0.6712 - val_acc: 0.6100\n",
            "Epoch 27/300\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.56395\n",
            "620/620 - 1s - loss: 0.5255 - acc: 0.7387 - val_loss: 0.6334 - val_acc: 0.6200\n",
            "Epoch 28/300\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.56395\n",
            "620/620 - 1s - loss: 0.5145 - acc: 0.7355 - val_loss: 0.6755 - val_acc: 0.5900\n",
            "Epoch 29/300\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.56395\n",
            "620/620 - 1s - loss: 0.4993 - acc: 0.7548 - val_loss: 0.6318 - val_acc: 0.6100\n",
            "Epoch 30/300\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.56395\n",
            "620/620 - 1s - loss: 0.4973 - acc: 0.7532 - val_loss: 0.6113 - val_acc: 0.6500\n",
            "Epoch 31/300\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.56395\n",
            "620/620 - 1s - loss: 0.4799 - acc: 0.7629 - val_loss: 0.6068 - val_acc: 0.6800\n",
            "Epoch 32/300\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.56395\n",
            "620/620 - 1s - loss: 0.4893 - acc: 0.7581 - val_loss: 0.5817 - val_acc: 0.6900\n",
            "Epoch 33/300\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.56395\n",
            "620/620 - 1s - loss: 0.4967 - acc: 0.7710 - val_loss: 0.6023 - val_acc: 0.6300\n",
            "Epoch 34/300\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.56395\n",
            "620/620 - 1s - loss: 0.4855 - acc: 0.7565 - val_loss: 0.6455 - val_acc: 0.6300\n",
            "Epoch 35/300\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.56395\n",
            "620/620 - 1s - loss: 0.5021 - acc: 0.7468 - val_loss: 0.5902 - val_acc: 0.6800\n",
            "Epoch 36/300\n",
            "\n",
            "Epoch 00036: val_loss improved from 0.56395 to 0.54731, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.5014 - acc: 0.7597 - val_loss: 0.5473 - val_acc: 0.7000\n",
            "Epoch 37/300\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.54731\n",
            "620/620 - 1s - loss: 0.4867 - acc: 0.7742 - val_loss: 0.5689 - val_acc: 0.7100\n",
            "Epoch 38/300\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.54731\n",
            "620/620 - 1s - loss: 0.4890 - acc: 0.7694 - val_loss: 0.5732 - val_acc: 0.6900\n",
            "Epoch 39/300\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.54731\n",
            "620/620 - 1s - loss: 0.4766 - acc: 0.7726 - val_loss: 0.6455 - val_acc: 0.6200\n",
            "Epoch 40/300\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.54731\n",
            "620/620 - 1s - loss: 0.4826 - acc: 0.7613 - val_loss: 0.6371 - val_acc: 0.6200\n",
            "Epoch 41/300\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.54731\n",
            "620/620 - 1s - loss: 0.4810 - acc: 0.7661 - val_loss: 0.6278 - val_acc: 0.6500\n",
            "Epoch 42/300\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.54731\n",
            "620/620 - 1s - loss: 0.4551 - acc: 0.7903 - val_loss: 0.5997 - val_acc: 0.6700\n",
            "Epoch 43/300\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.54731\n",
            "620/620 - 1s - loss: 0.4511 - acc: 0.8032 - val_loss: 0.5679 - val_acc: 0.7100\n",
            "Epoch 44/300\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.54731\n",
            "620/620 - 1s - loss: 0.4591 - acc: 0.7758 - val_loss: 0.5621 - val_acc: 0.7000\n",
            "Epoch 45/300\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.54731\n",
            "620/620 - 1s - loss: 0.5005 - acc: 0.7419 - val_loss: 0.6580 - val_acc: 0.6200\n",
            "Epoch 46/300\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.54731\n",
            "620/620 - 1s - loss: 0.4839 - acc: 0.7613 - val_loss: 0.5757 - val_acc: 0.6700\n",
            "Epoch 47/300\n",
            "\n",
            "Epoch 00047: val_loss improved from 0.54731 to 0.54197, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.4622 - acc: 0.7871 - val_loss: 0.5420 - val_acc: 0.7300\n",
            "Epoch 48/300\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.54197\n",
            "620/620 - 1s - loss: 0.4575 - acc: 0.7871 - val_loss: 0.5606 - val_acc: 0.6900\n",
            "Epoch 49/300\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.54197\n",
            "620/620 - 1s - loss: 0.4560 - acc: 0.7839 - val_loss: 0.6022 - val_acc: 0.6600\n",
            "Epoch 50/300\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.54197\n",
            "620/620 - 1s - loss: 0.4608 - acc: 0.7839 - val_loss: 0.6494 - val_acc: 0.6300\n",
            "Epoch 51/300\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.54197\n",
            "620/620 - 1s - loss: 0.4479 - acc: 0.7855 - val_loss: 0.7163 - val_acc: 0.6000\n",
            "Epoch 52/300\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.54197\n",
            "620/620 - 1s - loss: 0.4792 - acc: 0.7645 - val_loss: 0.6437 - val_acc: 0.6000\n",
            "Epoch 53/300\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.54197\n",
            "620/620 - 1s - loss: 0.4676 - acc: 0.7919 - val_loss: 0.6092 - val_acc: 0.6600\n",
            "Epoch 54/300\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.54197\n",
            "620/620 - 1s - loss: 0.4810 - acc: 0.7710 - val_loss: 0.5854 - val_acc: 0.6600\n",
            "Epoch 55/300\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.54197\n",
            "620/620 - 1s - loss: 0.4780 - acc: 0.7597 - val_loss: 0.6077 - val_acc: 0.6500\n",
            "Epoch 56/300\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.54197\n",
            "620/620 - 1s - loss: 0.4480 - acc: 0.7839 - val_loss: 0.6026 - val_acc: 0.6500\n",
            "Epoch 57/300\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.54197\n",
            "620/620 - 1s - loss: 0.4758 - acc: 0.7774 - val_loss: 0.6265 - val_acc: 0.6200\n",
            "Epoch 58/300\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.54197\n",
            "620/620 - 1s - loss: 0.4472 - acc: 0.7903 - val_loss: 0.5873 - val_acc: 0.6600\n",
            "Epoch 59/300\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.54197\n",
            "620/620 - 1s - loss: 0.4684 - acc: 0.7871 - val_loss: 0.6511 - val_acc: 0.6500\n",
            "Epoch 60/300\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.54197\n",
            "620/620 - 1s - loss: 0.4389 - acc: 0.7968 - val_loss: 0.5767 - val_acc: 0.6800\n",
            "Epoch 61/300\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.54197\n",
            "620/620 - 1s - loss: 0.4606 - acc: 0.7839 - val_loss: 0.6369 - val_acc: 0.6400\n",
            "Epoch 62/300\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.54197\n",
            "620/620 - 1s - loss: 0.4575 - acc: 0.7919 - val_loss: 0.6230 - val_acc: 0.6300\n",
            "Epoch 63/300\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.54197\n",
            "620/620 - 1s - loss: 0.4489 - acc: 0.7887 - val_loss: 0.7083 - val_acc: 0.6200\n",
            "Epoch 64/300\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.54197\n",
            "620/620 - 1s - loss: 0.4568 - acc: 0.7871 - val_loss: 0.6349 - val_acc: 0.6500\n",
            "Epoch 65/300\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.54197\n",
            "620/620 - 1s - loss: 0.4530 - acc: 0.7871 - val_loss: 0.5592 - val_acc: 0.6900\n",
            "Epoch 66/300\n",
            "\n",
            "Epoch 00066: val_loss improved from 0.54197 to 0.52031, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.4508 - acc: 0.8065 - val_loss: 0.5203 - val_acc: 0.7500\n",
            "Epoch 67/300\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.52031\n",
            "620/620 - 1s - loss: 0.4772 - acc: 0.7661 - val_loss: 0.5516 - val_acc: 0.7000\n",
            "Epoch 68/300\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.52031\n",
            "620/620 - 1s - loss: 0.4190 - acc: 0.8016 - val_loss: 0.5731 - val_acc: 0.7000\n",
            "Epoch 69/300\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.52031\n",
            "620/620 - 1s - loss: 0.4558 - acc: 0.7952 - val_loss: 0.6596 - val_acc: 0.6400\n",
            "Epoch 70/300\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.52031\n",
            "620/620 - 1s - loss: 0.4511 - acc: 0.7774 - val_loss: 0.5958 - val_acc: 0.6600\n",
            "Epoch 71/300\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.52031\n",
            "620/620 - 1s - loss: 0.4343 - acc: 0.7790 - val_loss: 0.7018 - val_acc: 0.6300\n",
            "Epoch 72/300\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.52031\n",
            "620/620 - 1s - loss: 0.4587 - acc: 0.7919 - val_loss: 0.6152 - val_acc: 0.6500\n",
            "Epoch 73/300\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.52031\n",
            "620/620 - 1s - loss: 0.4305 - acc: 0.8016 - val_loss: 0.5684 - val_acc: 0.6800\n",
            "Epoch 74/300\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.52031\n",
            "620/620 - 1s - loss: 0.4539 - acc: 0.7839 - val_loss: 0.6265 - val_acc: 0.6500\n",
            "Epoch 75/300\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.52031\n",
            "620/620 - 1s - loss: 0.4668 - acc: 0.7919 - val_loss: 0.7362 - val_acc: 0.5800\n",
            "Epoch 76/300\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.52031\n",
            "620/620 - 1s - loss: 0.4679 - acc: 0.7597 - val_loss: 0.6270 - val_acc: 0.6200\n",
            "Epoch 77/300\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.52031\n",
            "620/620 - 1s - loss: 0.4737 - acc: 0.7597 - val_loss: 0.5849 - val_acc: 0.7000\n",
            "Epoch 78/300\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.52031\n",
            "620/620 - 1s - loss: 0.4424 - acc: 0.8032 - val_loss: 0.6581 - val_acc: 0.6500\n",
            "Epoch 79/300\n",
            "\n",
            "Epoch 00079: val_loss improved from 0.52031 to 0.51258, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.4314 - acc: 0.7903 - val_loss: 0.5126 - val_acc: 0.7300\n",
            "Epoch 80/300\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.4366 - acc: 0.7710 - val_loss: 0.5765 - val_acc: 0.6500\n",
            "Epoch 81/300\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.4531 - acc: 0.7806 - val_loss: 0.6204 - val_acc: 0.6100\n",
            "Epoch 82/300\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.4433 - acc: 0.7871 - val_loss: 0.6196 - val_acc: 0.6400\n",
            "Epoch 83/300\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.4697 - acc: 0.7758 - val_loss: 0.6507 - val_acc: 0.6300\n",
            "Epoch 84/300\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.4165 - acc: 0.8177 - val_loss: 0.6026 - val_acc: 0.6700\n",
            "Epoch 85/300\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.4252 - acc: 0.8000 - val_loss: 0.6784 - val_acc: 0.6200\n",
            "Epoch 86/300\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.4611 - acc: 0.7742 - val_loss: 0.5879 - val_acc: 0.6800\n",
            "Epoch 87/300\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.4515 - acc: 0.7968 - val_loss: 0.5758 - val_acc: 0.6900\n",
            "Epoch 88/300\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.4374 - acc: 0.8161 - val_loss: 0.5702 - val_acc: 0.7100\n",
            "Epoch 89/300\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.4363 - acc: 0.8081 - val_loss: 0.6613 - val_acc: 0.6500\n",
            "Epoch 90/300\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.4217 - acc: 0.8226 - val_loss: 0.5300 - val_acc: 0.6900\n",
            "Epoch 91/300\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.4346 - acc: 0.7839 - val_loss: 0.5894 - val_acc: 0.6700\n",
            "Epoch 92/300\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.4271 - acc: 0.7935 - val_loss: 0.6459 - val_acc: 0.6400\n",
            "Epoch 93/300\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.4183 - acc: 0.8145 - val_loss: 0.5978 - val_acc: 0.6700\n",
            "Epoch 94/300\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.4251 - acc: 0.7968 - val_loss: 0.6920 - val_acc: 0.6000\n",
            "Epoch 95/300\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.4268 - acc: 0.7903 - val_loss: 0.6142 - val_acc: 0.6600\n",
            "Epoch 96/300\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.4318 - acc: 0.7984 - val_loss: 0.7025 - val_acc: 0.6200\n",
            "Epoch 97/300\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.4114 - acc: 0.8016 - val_loss: 0.6162 - val_acc: 0.6300\n",
            "Epoch 98/300\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.4232 - acc: 0.7935 - val_loss: 0.6072 - val_acc: 0.6700\n",
            "Epoch 99/300\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.4360 - acc: 0.7887 - val_loss: 0.7000 - val_acc: 0.6000\n",
            "Epoch 100/300\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.4219 - acc: 0.8000 - val_loss: 0.6760 - val_acc: 0.6000\n",
            "Epoch 101/300\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.4425 - acc: 0.7742 - val_loss: 0.7256 - val_acc: 0.6200\n",
            "Epoch 102/300\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.4264 - acc: 0.8000 - val_loss: 0.5592 - val_acc: 0.6900\n",
            "Epoch 103/300\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.4215 - acc: 0.8065 - val_loss: 0.6703 - val_acc: 0.6400\n",
            "Epoch 104/300\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.4534 - acc: 0.8048 - val_loss: 0.6405 - val_acc: 0.6400\n",
            "Epoch 105/300\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.4474 - acc: 0.7968 - val_loss: 0.5756 - val_acc: 0.6700\n",
            "Epoch 106/300\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.4309 - acc: 0.8048 - val_loss: 0.5640 - val_acc: 0.6800\n",
            "Epoch 107/300\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.4143 - acc: 0.8113 - val_loss: 0.6943 - val_acc: 0.6400\n",
            "Epoch 108/300\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.4242 - acc: 0.8161 - val_loss: 0.5429 - val_acc: 0.7200\n",
            "Epoch 109/300\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.4068 - acc: 0.8129 - val_loss: 0.6083 - val_acc: 0.6500\n",
            "Epoch 110/300\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.4296 - acc: 0.8032 - val_loss: 0.6314 - val_acc: 0.6500\n",
            "Epoch 111/300\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.4354 - acc: 0.7952 - val_loss: 0.6451 - val_acc: 0.6400\n",
            "Epoch 112/300\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.4145 - acc: 0.8274 - val_loss: 0.5767 - val_acc: 0.6600\n",
            "Epoch 113/300\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.4110 - acc: 0.8274 - val_loss: 0.5698 - val_acc: 0.6900\n",
            "Epoch 114/300\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.4477 - acc: 0.7935 - val_loss: 0.6540 - val_acc: 0.6400\n",
            "Epoch 115/300\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.4227 - acc: 0.7903 - val_loss: 0.6211 - val_acc: 0.6600\n",
            "Epoch 116/300\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.4201 - acc: 0.8000 - val_loss: 0.6248 - val_acc: 0.6400\n",
            "Epoch 117/300\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.3994 - acc: 0.8274 - val_loss: 0.6164 - val_acc: 0.6500\n",
            "Epoch 118/300\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.3977 - acc: 0.8194 - val_loss: 0.6221 - val_acc: 0.6300\n",
            "Epoch 119/300\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.4233 - acc: 0.8097 - val_loss: 0.6261 - val_acc: 0.6600\n",
            "Epoch 120/300\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.4129 - acc: 0.8161 - val_loss: 0.6625 - val_acc: 0.6000\n",
            "Epoch 121/300\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.4281 - acc: 0.7968 - val_loss: 0.7596 - val_acc: 0.5900\n",
            "Epoch 122/300\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.4249 - acc: 0.8194 - val_loss: 0.7145 - val_acc: 0.6200\n",
            "Epoch 123/300\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.4099 - acc: 0.8226 - val_loss: 0.7102 - val_acc: 0.5900\n",
            "Epoch 124/300\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.4101 - acc: 0.8097 - val_loss: 0.6723 - val_acc: 0.6000\n",
            "Epoch 125/300\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.4129 - acc: 0.8177 - val_loss: 0.6213 - val_acc: 0.6800\n",
            "Epoch 126/300\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.4091 - acc: 0.8065 - val_loss: 0.5546 - val_acc: 0.7000\n",
            "Epoch 127/300\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.4079 - acc: 0.8129 - val_loss: 0.5752 - val_acc: 0.6900\n",
            "Epoch 128/300\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.4266 - acc: 0.7984 - val_loss: 0.5704 - val_acc: 0.7000\n",
            "Epoch 129/300\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.4136 - acc: 0.8129 - val_loss: 0.5860 - val_acc: 0.6700\n",
            "Epoch 130/300\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.3979 - acc: 0.8274 - val_loss: 0.6676 - val_acc: 0.6100\n",
            "Epoch 131/300\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.4152 - acc: 0.8032 - val_loss: 0.5855 - val_acc: 0.6700\n",
            "Epoch 132/300\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.4091 - acc: 0.8129 - val_loss: 0.6276 - val_acc: 0.6700\n",
            "Epoch 133/300\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.4178 - acc: 0.7935 - val_loss: 0.6505 - val_acc: 0.6400\n",
            "Epoch 134/300\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.4073 - acc: 0.8161 - val_loss: 0.6396 - val_acc: 0.6500\n",
            "Epoch 135/300\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.4074 - acc: 0.8000 - val_loss: 0.7045 - val_acc: 0.6100\n",
            "Epoch 136/300\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.4137 - acc: 0.7887 - val_loss: 0.6180 - val_acc: 0.6600\n",
            "Epoch 137/300\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.4004 - acc: 0.8161 - val_loss: 0.6954 - val_acc: 0.6200\n",
            "Epoch 138/300\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.4013 - acc: 0.8113 - val_loss: 0.7030 - val_acc: 0.6400\n",
            "Epoch 139/300\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.4183 - acc: 0.8226 - val_loss: 0.6557 - val_acc: 0.6500\n",
            "Epoch 140/300\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.4194 - acc: 0.8194 - val_loss: 0.6773 - val_acc: 0.6500\n",
            "Epoch 141/300\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.3940 - acc: 0.8145 - val_loss: 0.6642 - val_acc: 0.6400\n",
            "Epoch 142/300\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.3960 - acc: 0.8161 - val_loss: 0.5774 - val_acc: 0.6600\n",
            "Epoch 143/300\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.4040 - acc: 0.8194 - val_loss: 0.6771 - val_acc: 0.6300\n",
            "Epoch 144/300\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.4224 - acc: 0.8194 - val_loss: 0.6100 - val_acc: 0.6800\n",
            "Epoch 145/300\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.4160 - acc: 0.8097 - val_loss: 0.6409 - val_acc: 0.6300\n",
            "Epoch 146/300\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.4044 - acc: 0.8113 - val_loss: 0.5855 - val_acc: 0.6800\n",
            "Epoch 147/300\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.3997 - acc: 0.8129 - val_loss: 0.6026 - val_acc: 0.6700\n",
            "Epoch 148/300\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.4180 - acc: 0.8177 - val_loss: 0.6647 - val_acc: 0.6600\n",
            "Epoch 149/300\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.4141 - acc: 0.8194 - val_loss: 0.6169 - val_acc: 0.6500\n",
            "Epoch 150/300\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.4061 - acc: 0.8177 - val_loss: 0.8353 - val_acc: 0.5700\n",
            "Epoch 151/300\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.4095 - acc: 0.8210 - val_loss: 0.6510 - val_acc: 0.6200\n",
            "Epoch 152/300\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.4189 - acc: 0.8016 - val_loss: 0.6145 - val_acc: 0.6600\n",
            "Epoch 153/300\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.3995 - acc: 0.8161 - val_loss: 0.6599 - val_acc: 0.6300\n",
            "Epoch 154/300\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.3950 - acc: 0.8210 - val_loss: 0.6602 - val_acc: 0.6500\n",
            "Epoch 155/300\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.4016 - acc: 0.8065 - val_loss: 0.6721 - val_acc: 0.6400\n",
            "Epoch 156/300\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.4209 - acc: 0.8161 - val_loss: 0.6652 - val_acc: 0.6300\n",
            "Epoch 157/300\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.3870 - acc: 0.8371 - val_loss: 0.8127 - val_acc: 0.5700\n",
            "Epoch 158/300\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.4099 - acc: 0.8258 - val_loss: 0.6075 - val_acc: 0.6600\n",
            "Epoch 159/300\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.4064 - acc: 0.8339 - val_loss: 0.6054 - val_acc: 0.6600\n",
            "Epoch 160/300\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.3760 - acc: 0.8242 - val_loss: 0.6405 - val_acc: 0.6700\n",
            "Epoch 161/300\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.4150 - acc: 0.8113 - val_loss: 0.6021 - val_acc: 0.6900\n",
            "Epoch 162/300\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.3856 - acc: 0.8419 - val_loss: 0.7006 - val_acc: 0.6200\n",
            "Epoch 163/300\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.3685 - acc: 0.8435 - val_loss: 0.5717 - val_acc: 0.6600\n",
            "Epoch 164/300\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.3878 - acc: 0.8290 - val_loss: 0.6003 - val_acc: 0.6700\n",
            "Epoch 165/300\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.4077 - acc: 0.7984 - val_loss: 0.7126 - val_acc: 0.6400\n",
            "Epoch 166/300\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.4031 - acc: 0.8065 - val_loss: 0.6097 - val_acc: 0.6700\n",
            "Epoch 167/300\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.3980 - acc: 0.8145 - val_loss: 0.6232 - val_acc: 0.6800\n",
            "Epoch 168/300\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.3990 - acc: 0.8097 - val_loss: 0.5810 - val_acc: 0.6500\n",
            "Epoch 169/300\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.3857 - acc: 0.8177 - val_loss: 0.6560 - val_acc: 0.6500\n",
            "Epoch 170/300\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.4059 - acc: 0.8161 - val_loss: 0.6591 - val_acc: 0.6500\n",
            "Epoch 171/300\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.3922 - acc: 0.8290 - val_loss: 0.6627 - val_acc: 0.6500\n",
            "Epoch 172/300\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.4015 - acc: 0.8129 - val_loss: 0.6117 - val_acc: 0.6600\n",
            "Epoch 173/300\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.4126 - acc: 0.8129 - val_loss: 0.6465 - val_acc: 0.6600\n",
            "Epoch 174/300\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.4282 - acc: 0.8032 - val_loss: 0.6878 - val_acc: 0.6400\n",
            "Epoch 175/300\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.3846 - acc: 0.8355 - val_loss: 0.6442 - val_acc: 0.6600\n",
            "Epoch 176/300\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.3989 - acc: 0.8161 - val_loss: 0.5988 - val_acc: 0.6600\n",
            "Epoch 177/300\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.3941 - acc: 0.8274 - val_loss: 0.5855 - val_acc: 0.7000\n",
            "Epoch 178/300\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.3907 - acc: 0.8339 - val_loss: 0.6721 - val_acc: 0.6700\n",
            "Epoch 179/300\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.4069 - acc: 0.8097 - val_loss: 0.7519 - val_acc: 0.5900\n",
            "Epoch 180/300\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.4059 - acc: 0.8242 - val_loss: 0.6937 - val_acc: 0.6300\n",
            "Epoch 181/300\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.3882 - acc: 0.8323 - val_loss: 0.5482 - val_acc: 0.7100\n",
            "Epoch 182/300\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.4105 - acc: 0.8048 - val_loss: 0.5764 - val_acc: 0.6800\n",
            "Epoch 183/300\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.4096 - acc: 0.7984 - val_loss: 0.6668 - val_acc: 0.6800\n",
            "Epoch 184/300\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.3909 - acc: 0.8290 - val_loss: 0.6023 - val_acc: 0.6900\n",
            "Epoch 185/300\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.3968 - acc: 0.8113 - val_loss: 0.6396 - val_acc: 0.6800\n",
            "Epoch 186/300\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.3923 - acc: 0.8290 - val_loss: 0.6805 - val_acc: 0.6100\n",
            "Epoch 187/300\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.4218 - acc: 0.8048 - val_loss: 0.7452 - val_acc: 0.6200\n",
            "Epoch 188/300\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.3810 - acc: 0.8339 - val_loss: 0.6649 - val_acc: 0.6600\n",
            "Epoch 189/300\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.3812 - acc: 0.8339 - val_loss: 0.6166 - val_acc: 0.6600\n",
            "Epoch 190/300\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.3921 - acc: 0.8387 - val_loss: 0.7725 - val_acc: 0.6200\n",
            "Epoch 191/300\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.4026 - acc: 0.8145 - val_loss: 0.6343 - val_acc: 0.6600\n",
            "Epoch 192/300\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.3697 - acc: 0.8452 - val_loss: 0.6070 - val_acc: 0.6400\n",
            "Epoch 193/300\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.4082 - acc: 0.8048 - val_loss: 0.7334 - val_acc: 0.6200\n",
            "Epoch 194/300\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.4032 - acc: 0.8194 - val_loss: 0.6175 - val_acc: 0.7000\n",
            "Epoch 195/300\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.3572 - acc: 0.8565 - val_loss: 0.6948 - val_acc: 0.6700\n",
            "Epoch 196/300\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.3766 - acc: 0.8274 - val_loss: 0.6937 - val_acc: 0.6400\n",
            "Epoch 197/300\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.3979 - acc: 0.8161 - val_loss: 0.6503 - val_acc: 0.6200\n",
            "Epoch 198/300\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.4040 - acc: 0.8113 - val_loss: 0.6734 - val_acc: 0.6100\n",
            "Epoch 199/300\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.3988 - acc: 0.8194 - val_loss: 0.6707 - val_acc: 0.6300\n",
            "Epoch 200/300\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.3820 - acc: 0.8419 - val_loss: 0.5934 - val_acc: 0.6700\n",
            "Epoch 201/300\n",
            "\n",
            "Epoch 00201: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.3960 - acc: 0.8274 - val_loss: 0.6673 - val_acc: 0.6500\n",
            "Epoch 202/300\n",
            "\n",
            "Epoch 00202: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.3675 - acc: 0.8468 - val_loss: 0.7417 - val_acc: 0.6100\n",
            "Epoch 203/300\n",
            "\n",
            "Epoch 00203: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.3878 - acc: 0.8177 - val_loss: 0.5680 - val_acc: 0.6900\n",
            "Epoch 204/300\n",
            "\n",
            "Epoch 00204: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.3925 - acc: 0.8145 - val_loss: 0.7272 - val_acc: 0.5800\n",
            "Epoch 205/300\n",
            "\n",
            "Epoch 00205: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.3949 - acc: 0.8177 - val_loss: 0.6893 - val_acc: 0.6500\n",
            "Epoch 206/300\n",
            "\n",
            "Epoch 00206: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.3886 - acc: 0.8161 - val_loss: 0.6686 - val_acc: 0.6600\n",
            "Epoch 207/300\n",
            "\n",
            "Epoch 00207: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.3841 - acc: 0.8339 - val_loss: 0.6807 - val_acc: 0.6600\n",
            "Epoch 208/300\n",
            "\n",
            "Epoch 00208: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.3725 - acc: 0.8274 - val_loss: 0.6824 - val_acc: 0.6700\n",
            "Epoch 209/300\n",
            "\n",
            "Epoch 00209: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.3659 - acc: 0.8258 - val_loss: 0.6795 - val_acc: 0.6800\n",
            "Epoch 210/300\n",
            "\n",
            "Epoch 00210: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.3622 - acc: 0.8435 - val_loss: 0.6156 - val_acc: 0.6800\n",
            "Epoch 211/300\n",
            "\n",
            "Epoch 00211: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.3563 - acc: 0.8387 - val_loss: 0.7011 - val_acc: 0.6600\n",
            "Epoch 212/300\n",
            "\n",
            "Epoch 00212: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.3627 - acc: 0.8355 - val_loss: 0.6483 - val_acc: 0.6800\n",
            "Epoch 213/300\n",
            "\n",
            "Epoch 00213: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.3971 - acc: 0.8177 - val_loss: 0.6611 - val_acc: 0.6700\n",
            "Epoch 214/300\n",
            "\n",
            "Epoch 00214: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.3890 - acc: 0.8290 - val_loss: 0.6280 - val_acc: 0.6700\n",
            "Epoch 215/300\n",
            "\n",
            "Epoch 00215: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.4000 - acc: 0.8161 - val_loss: 0.6031 - val_acc: 0.6900\n",
            "Epoch 216/300\n",
            "\n",
            "Epoch 00216: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.3798 - acc: 0.8226 - val_loss: 0.6121 - val_acc: 0.6800\n",
            "Epoch 217/300\n",
            "\n",
            "Epoch 00217: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.3683 - acc: 0.8371 - val_loss: 0.6824 - val_acc: 0.6500\n",
            "Epoch 218/300\n",
            "\n",
            "Epoch 00218: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.4067 - acc: 0.8145 - val_loss: 0.7048 - val_acc: 0.6100\n",
            "Epoch 219/300\n",
            "\n",
            "Epoch 00219: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.3949 - acc: 0.8274 - val_loss: 0.6440 - val_acc: 0.6300\n",
            "Epoch 220/300\n",
            "\n",
            "Epoch 00220: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.3747 - acc: 0.8452 - val_loss: 0.7774 - val_acc: 0.6000\n",
            "Epoch 221/300\n",
            "\n",
            "Epoch 00221: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.3690 - acc: 0.8323 - val_loss: 0.6742 - val_acc: 0.6500\n",
            "Epoch 222/300\n",
            "\n",
            "Epoch 00222: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.3978 - acc: 0.8339 - val_loss: 0.6383 - val_acc: 0.6600\n",
            "Epoch 223/300\n",
            "\n",
            "Epoch 00223: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.3899 - acc: 0.8226 - val_loss: 0.6779 - val_acc: 0.6500\n",
            "Epoch 224/300\n",
            "\n",
            "Epoch 00224: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.3657 - acc: 0.8500 - val_loss: 0.5747 - val_acc: 0.6700\n",
            "Epoch 225/300\n",
            "\n",
            "Epoch 00225: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.3750 - acc: 0.8258 - val_loss: 0.6305 - val_acc: 0.6500\n",
            "Epoch 226/300\n",
            "\n",
            "Epoch 00226: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.3978 - acc: 0.8177 - val_loss: 0.7417 - val_acc: 0.6000\n",
            "Epoch 227/300\n",
            "\n",
            "Epoch 00227: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.3465 - acc: 0.8677 - val_loss: 0.6164 - val_acc: 0.6700\n",
            "Epoch 228/300\n",
            "\n",
            "Epoch 00228: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.3863 - acc: 0.8210 - val_loss: 0.6509 - val_acc: 0.6500\n",
            "Epoch 229/300\n",
            "\n",
            "Epoch 00229: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.3660 - acc: 0.8274 - val_loss: 0.6733 - val_acc: 0.6700\n",
            "Epoch 230/300\n",
            "\n",
            "Epoch 00230: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.3747 - acc: 0.8274 - val_loss: 0.6638 - val_acc: 0.6600\n",
            "Epoch 231/300\n",
            "\n",
            "Epoch 00231: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.3800 - acc: 0.8323 - val_loss: 0.6780 - val_acc: 0.6600\n",
            "Epoch 232/300\n",
            "\n",
            "Epoch 00232: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.3707 - acc: 0.8339 - val_loss: 0.7764 - val_acc: 0.6300\n",
            "Epoch 233/300\n",
            "\n",
            "Epoch 00233: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.3842 - acc: 0.8323 - val_loss: 0.7313 - val_acc: 0.6300\n",
            "Epoch 234/300\n",
            "\n",
            "Epoch 00234: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.3728 - acc: 0.8371 - val_loss: 0.6140 - val_acc: 0.6700\n",
            "Epoch 235/300\n",
            "\n",
            "Epoch 00235: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.3711 - acc: 0.8452 - val_loss: 0.7213 - val_acc: 0.6200\n",
            "Epoch 236/300\n",
            "\n",
            "Epoch 00236: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.3695 - acc: 0.8452 - val_loss: 0.5874 - val_acc: 0.6900\n",
            "Epoch 237/300\n",
            "\n",
            "Epoch 00237: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.3668 - acc: 0.8403 - val_loss: 0.6305 - val_acc: 0.6600\n",
            "Epoch 238/300\n",
            "\n",
            "Epoch 00238: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.4140 - acc: 0.8226 - val_loss: 0.6602 - val_acc: 0.6400\n",
            "Epoch 239/300\n",
            "\n",
            "Epoch 00239: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.3897 - acc: 0.8048 - val_loss: 0.6984 - val_acc: 0.6500\n",
            "Epoch 240/300\n",
            "\n",
            "Epoch 00240: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.4035 - acc: 0.8113 - val_loss: 0.6547 - val_acc: 0.6500\n",
            "Epoch 241/300\n",
            "\n",
            "Epoch 00241: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.3634 - acc: 0.8387 - val_loss: 0.6659 - val_acc: 0.6300\n",
            "Epoch 242/300\n",
            "\n",
            "Epoch 00242: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.3797 - acc: 0.8290 - val_loss: 0.7811 - val_acc: 0.6100\n",
            "Epoch 243/300\n",
            "\n",
            "Epoch 00243: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.3577 - acc: 0.8468 - val_loss: 0.5673 - val_acc: 0.6800\n",
            "Epoch 244/300\n",
            "\n",
            "Epoch 00244: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.3743 - acc: 0.8532 - val_loss: 0.6819 - val_acc: 0.6300\n",
            "Epoch 245/300\n",
            "\n",
            "Epoch 00245: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.4038 - acc: 0.8161 - val_loss: 0.6892 - val_acc: 0.6300\n",
            "Epoch 246/300\n",
            "\n",
            "Epoch 00246: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.3662 - acc: 0.8387 - val_loss: 0.7781 - val_acc: 0.6000\n",
            "Epoch 247/300\n",
            "\n",
            "Epoch 00247: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.3554 - acc: 0.8581 - val_loss: 0.7659 - val_acc: 0.5900\n",
            "Epoch 248/300\n",
            "\n",
            "Epoch 00248: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.3445 - acc: 0.8516 - val_loss: 0.6000 - val_acc: 0.6800\n",
            "Epoch 249/300\n",
            "\n",
            "Epoch 00249: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.3570 - acc: 0.8484 - val_loss: 0.7048 - val_acc: 0.6300\n",
            "Epoch 250/300\n",
            "\n",
            "Epoch 00250: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.3352 - acc: 0.8661 - val_loss: 0.7372 - val_acc: 0.6200\n",
            "Epoch 251/300\n",
            "\n",
            "Epoch 00251: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.3678 - acc: 0.8435 - val_loss: 0.6419 - val_acc: 0.6300\n",
            "Epoch 252/300\n",
            "\n",
            "Epoch 00252: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.3703 - acc: 0.8161 - val_loss: 0.6841 - val_acc: 0.6600\n",
            "Epoch 253/300\n",
            "\n",
            "Epoch 00253: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.3790 - acc: 0.8274 - val_loss: 0.5968 - val_acc: 0.6900\n",
            "Epoch 254/300\n",
            "\n",
            "Epoch 00254: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.3717 - acc: 0.8323 - val_loss: 0.7327 - val_acc: 0.6100\n",
            "Epoch 255/300\n",
            "\n",
            "Epoch 00255: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.4116 - acc: 0.7952 - val_loss: 0.6867 - val_acc: 0.6700\n",
            "Epoch 256/300\n",
            "\n",
            "Epoch 00256: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.3442 - acc: 0.8661 - val_loss: 0.7067 - val_acc: 0.6400\n",
            "Epoch 257/300\n",
            "\n",
            "Epoch 00257: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.3819 - acc: 0.8452 - val_loss: 0.7638 - val_acc: 0.6300\n",
            "Epoch 258/300\n",
            "\n",
            "Epoch 00258: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.3584 - acc: 0.8500 - val_loss: 0.6682 - val_acc: 0.6500\n",
            "Epoch 259/300\n",
            "\n",
            "Epoch 00259: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.3478 - acc: 0.8452 - val_loss: 0.6880 - val_acc: 0.6300\n",
            "Epoch 260/300\n",
            "\n",
            "Epoch 00260: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.3861 - acc: 0.8242 - val_loss: 0.6460 - val_acc: 0.6500\n",
            "Epoch 261/300\n",
            "\n",
            "Epoch 00261: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.3681 - acc: 0.8355 - val_loss: 0.6142 - val_acc: 0.6800\n",
            "Epoch 262/300\n",
            "\n",
            "Epoch 00262: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.3669 - acc: 0.8403 - val_loss: 0.6325 - val_acc: 0.6800\n",
            "Epoch 263/300\n",
            "\n",
            "Epoch 00263: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.3703 - acc: 0.8355 - val_loss: 0.5524 - val_acc: 0.6800\n",
            "Epoch 264/300\n",
            "\n",
            "Epoch 00264: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.3648 - acc: 0.8403 - val_loss: 0.6761 - val_acc: 0.6400\n",
            "Epoch 265/300\n",
            "\n",
            "Epoch 00265: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.3846 - acc: 0.8258 - val_loss: 0.7004 - val_acc: 0.6300\n",
            "Epoch 266/300\n",
            "\n",
            "Epoch 00266: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.3702 - acc: 0.8532 - val_loss: 0.6702 - val_acc: 0.6700\n",
            "Epoch 267/300\n",
            "\n",
            "Epoch 00267: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.3954 - acc: 0.8194 - val_loss: 0.7448 - val_acc: 0.6300\n",
            "Epoch 268/300\n",
            "\n",
            "Epoch 00268: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.3540 - acc: 0.8500 - val_loss: 0.6016 - val_acc: 0.6700\n",
            "Epoch 269/300\n",
            "\n",
            "Epoch 00269: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.3569 - acc: 0.8484 - val_loss: 0.6248 - val_acc: 0.6500\n",
            "Epoch 270/300\n",
            "\n",
            "Epoch 00270: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.3701 - acc: 0.8306 - val_loss: 0.7051 - val_acc: 0.6000\n",
            "Epoch 271/300\n",
            "\n",
            "Epoch 00271: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.3831 - acc: 0.8403 - val_loss: 0.7243 - val_acc: 0.6200\n",
            "Epoch 272/300\n",
            "\n",
            "Epoch 00272: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.3859 - acc: 0.8226 - val_loss: 0.6560 - val_acc: 0.6500\n",
            "Epoch 273/300\n",
            "\n",
            "Epoch 00273: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.3677 - acc: 0.8403 - val_loss: 0.7187 - val_acc: 0.6000\n",
            "Epoch 274/300\n",
            "\n",
            "Epoch 00274: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.3820 - acc: 0.8323 - val_loss: 0.6171 - val_acc: 0.6600\n",
            "Epoch 275/300\n",
            "\n",
            "Epoch 00275: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.3680 - acc: 0.8403 - val_loss: 0.6965 - val_acc: 0.6300\n",
            "Epoch 276/300\n",
            "\n",
            "Epoch 00276: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.3611 - acc: 0.8403 - val_loss: 0.5821 - val_acc: 0.6900\n",
            "Epoch 277/300\n",
            "\n",
            "Epoch 00277: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.3622 - acc: 0.8371 - val_loss: 0.6547 - val_acc: 0.6700\n",
            "Epoch 278/300\n",
            "\n",
            "Epoch 00278: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.3458 - acc: 0.8419 - val_loss: 0.8606 - val_acc: 0.5800\n",
            "Epoch 279/300\n",
            "\n",
            "Epoch 00279: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.3670 - acc: 0.8468 - val_loss: 0.7505 - val_acc: 0.5900\n",
            "Epoch 280/300\n",
            "\n",
            "Epoch 00280: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.3529 - acc: 0.8419 - val_loss: 0.5943 - val_acc: 0.7100\n",
            "Epoch 281/300\n",
            "\n",
            "Epoch 00281: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.3590 - acc: 0.8419 - val_loss: 0.6763 - val_acc: 0.6400\n",
            "Epoch 282/300\n",
            "\n",
            "Epoch 00282: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.3740 - acc: 0.8339 - val_loss: 0.7423 - val_acc: 0.6100\n",
            "Epoch 283/300\n",
            "\n",
            "Epoch 00283: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.3612 - acc: 0.8258 - val_loss: 0.7753 - val_acc: 0.6300\n",
            "Epoch 284/300\n",
            "\n",
            "Epoch 00284: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.3777 - acc: 0.8548 - val_loss: 0.6435 - val_acc: 0.6700\n",
            "Epoch 285/300\n",
            "\n",
            "Epoch 00285: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.3972 - acc: 0.8048 - val_loss: 0.6874 - val_acc: 0.5900\n",
            "Epoch 286/300\n",
            "\n",
            "Epoch 00286: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.3641 - acc: 0.8452 - val_loss: 0.8140 - val_acc: 0.5800\n",
            "Epoch 287/300\n",
            "\n",
            "Epoch 00287: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.3738 - acc: 0.8274 - val_loss: 0.7165 - val_acc: 0.6200\n",
            "Epoch 288/300\n",
            "\n",
            "Epoch 00288: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.3566 - acc: 0.8500 - val_loss: 0.6545 - val_acc: 0.6300\n",
            "Epoch 289/300\n",
            "\n",
            "Epoch 00289: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.3831 - acc: 0.8323 - val_loss: 0.6264 - val_acc: 0.6600\n",
            "Epoch 290/300\n",
            "\n",
            "Epoch 00290: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.3701 - acc: 0.8339 - val_loss: 0.6064 - val_acc: 0.6700\n",
            "Epoch 291/300\n",
            "\n",
            "Epoch 00291: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.3536 - acc: 0.8339 - val_loss: 0.6570 - val_acc: 0.6600\n",
            "Epoch 292/300\n",
            "\n",
            "Epoch 00292: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.3541 - acc: 0.8419 - val_loss: 0.6100 - val_acc: 0.6800\n",
            "Epoch 293/300\n",
            "\n",
            "Epoch 00293: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.3742 - acc: 0.8371 - val_loss: 0.8304 - val_acc: 0.6100\n",
            "Epoch 294/300\n",
            "\n",
            "Epoch 00294: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.3627 - acc: 0.8323 - val_loss: 0.7665 - val_acc: 0.6100\n",
            "Epoch 295/300\n",
            "\n",
            "Epoch 00295: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.3708 - acc: 0.8290 - val_loss: 0.7489 - val_acc: 0.6100\n",
            "Epoch 296/300\n",
            "\n",
            "Epoch 00296: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.3536 - acc: 0.8435 - val_loss: 0.6777 - val_acc: 0.6300\n",
            "Epoch 297/300\n",
            "\n",
            "Epoch 00297: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.3778 - acc: 0.8323 - val_loss: 0.6281 - val_acc: 0.6600\n",
            "Epoch 298/300\n",
            "\n",
            "Epoch 00298: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.3668 - acc: 0.8306 - val_loss: 0.7454 - val_acc: 0.6200\n",
            "Epoch 299/300\n",
            "\n",
            "Epoch 00299: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.3514 - acc: 0.8661 - val_loss: 0.5862 - val_acc: 0.7000\n",
            "Epoch 300/300\n",
            "\n",
            "Epoch 00300: val_loss did not improve from 0.51258\n",
            "620/620 - 1s - loss: 0.3472 - acc: 0.8532 - val_loss: 0.6754 - val_acc: 0.6700\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n5/gXZwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "# Evaluate on test data\n",
            "40/40 [==============================] - 0s 3ms/sample - loss: 0.6262 - acc: 0.6250\n",
            "test loss, test acc: [0.6262168535031378, 0.625]\n",
            "1\n",
            "EEG_Deep/Data2A/parsed_P01T.mat\n",
            "(0, 12, 4096)\n",
            "(80, 12, 4096)\n",
            "(80, 1)\n",
            "3\n",
            "EEG_Deep/Data2A/parsed_P03T.mat\n",
            "(80, 12, 4096)\n",
            "(160, 12, 4096)\n",
            "(160, 1)\n",
            "4\n",
            "EEG_Deep/Data2A/parsed_P04T.mat\n",
            "(160, 12, 4096)\n",
            "(240, 12, 4096)\n",
            "(240, 1)\n",
            "5\n",
            "EEG_Deep/Data2A/parsed_P05T.mat\n",
            "(240, 12, 4096)\n",
            "(320, 12, 4096)\n",
            "(320, 1)\n",
            "6\n",
            "EEG_Deep/Data2A/parsed_P06T.mat\n",
            "(320, 12, 4096)\n",
            "(400, 12, 4096)\n",
            "(400, 1)\n",
            "7\n",
            "EEG_Deep/Data2A/parsed_P07T.mat\n",
            "(400, 12, 4096)\n",
            "(480, 12, 4096)\n",
            "(480, 1)\n",
            "8\n",
            "EEG_Deep/Data2A/parsed_P08T.mat\n",
            "(480, 12, 4096)\n",
            "(560, 12, 4096)\n",
            "(560, 1)\n",
            "9\n",
            "EEG_Deep/Data2A/parsed_P09T.mat\n",
            "(560, 12, 4096)\n",
            "(640, 12, 4096)\n",
            "(640, 1)\n",
            "10\n",
            "EEG_Deep/Data2A/parsed_P010T.mat\n",
            "(640, 12, 4096)\n",
            "(720, 12, 4096)\n",
            "(720, 1)\n",
            "2\n",
            "EEG_Deep/Data2A/parsed_P02E.mat\n",
            "(0, 12, 4096)\n",
            "(40, 12, 4096)\n",
            "(40, 1)\n",
            "Filtering of training data in progress\n",
            "(720, 12, 4096)\n",
            "Filtering of test data in progress\n",
            "(40, 12, 4096)\n",
            "[1. 1. 1. 1. 2. 2. 1. 2. 1. 1. 1. 2. 1. 1. 2. 1. 2. 2. 2. 2. 2. 1. 1. 1.\n",
            " 2. 2. 1. 1. 1. 2. 2. 1. 2. 1. 1. 2. 1. 1. 2. 2. 1. 1. 2. 2. 1. 1. 2. 1.\n",
            " 2. 1. 1. 1. 2. 1. 1. 1. 2. 2. 1. 1. 2. 1. 1. 1. 1. 1. 1. 1. 2. 2. 2. 2.\n",
            " 1. 1. 1. 2. 2. 2. 1. 1. 1. 2. 1. 1. 2. 1. 2. 1. 1. 2. 1. 2. 1. 2. 2. 1.\n",
            " 1. 2. 2. 2.]\n",
            "(620, 12, 1536)\n",
            "(620,)\n",
            "(100, 12, 1536)\n",
            "(100,)\n",
            "[[1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]]\n",
            "X_train shape: (620, 1, 12, 1536)\n",
            "620 train samples\n",
            "100 val samples\n",
            "(40, 12, 1536)\n",
            "(40, 1)\n",
            "X_train shape: (40, 1, 12, 1536)\n",
            "40 train samples\n",
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 1, 12, 1536)]     0         \n",
            "_________________________________________________________________\n",
            "conv2d (Conv2D)              (None, 8, 12, 1536)       200       \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 8, 12, 1536)       32        \n",
            "_________________________________________________________________\n",
            "depthwise_conv2d (DepthwiseC (None, 16, 1, 1536)       192       \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 16, 1, 1536)       64        \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 16, 1, 1536)       0         \n",
            "_________________________________________________________________\n",
            "average_pooling2d (AveragePo (None, 16, 1, 384)        0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 16, 1, 384)        0         \n",
            "_________________________________________________________________\n",
            "separable_conv2d (SeparableC (None, 16, 1, 384)        512       \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 16, 1, 384)        64        \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 16, 1, 384)        0         \n",
            "_________________________________________________________________\n",
            "average_pooling2d_1 (Average (None, 16, 1, 48)         0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 16, 1, 48)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 768)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 2)                 1538      \n",
            "_________________________________________________________________\n",
            "softmax (Activation)         (None, 2)                 0         \n",
            "=================================================================\n",
            "Total params: 2,602\n",
            "Trainable params: 2,522\n",
            "Non-trainable params: 80\n",
            "_________________________________________________________________\n",
            "Train on 620 samples, validate on 100 samples\n",
            "Epoch 1/300\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.69467, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.7020 - acc: 0.5016 - val_loss: 0.6947 - val_acc: 0.4600\n",
            "Epoch 2/300\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.69467 to 0.69337, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.6751 - acc: 0.5597 - val_loss: 0.6934 - val_acc: 0.4900\n",
            "Epoch 3/300\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.69337\n",
            "620/620 - 1s - loss: 0.6372 - acc: 0.6661 - val_loss: 0.7026 - val_acc: 0.4400\n",
            "Epoch 4/300\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.69337\n",
            "620/620 - 1s - loss: 0.6010 - acc: 0.6694 - val_loss: 0.7200 - val_acc: 0.4500\n",
            "Epoch 5/300\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.69337\n",
            "620/620 - 1s - loss: 0.5732 - acc: 0.6935 - val_loss: 0.7390 - val_acc: 0.4700\n",
            "Epoch 6/300\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.69337\n",
            "620/620 - 1s - loss: 0.5624 - acc: 0.7016 - val_loss: 0.7064 - val_acc: 0.4700\n",
            "Epoch 7/300\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.69337\n",
            "620/620 - 1s - loss: 0.5429 - acc: 0.7419 - val_loss: 0.7016 - val_acc: 0.4900\n",
            "Epoch 8/300\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.69337 to 0.64773, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.5323 - acc: 0.7210 - val_loss: 0.6477 - val_acc: 0.5500\n",
            "Epoch 9/300\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.64773 to 0.64217, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.5251 - acc: 0.7419 - val_loss: 0.6422 - val_acc: 0.5600\n",
            "Epoch 10/300\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.64217 to 0.62558, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.5422 - acc: 0.7145 - val_loss: 0.6256 - val_acc: 0.6100\n",
            "Epoch 11/300\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.62558\n",
            "620/620 - 1s - loss: 0.5300 - acc: 0.7355 - val_loss: 0.6550 - val_acc: 0.5800\n",
            "Epoch 12/300\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.62558\n",
            "620/620 - 1s - loss: 0.5261 - acc: 0.7516 - val_loss: 0.6581 - val_acc: 0.5600\n",
            "Epoch 13/300\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.62558\n",
            "620/620 - 1s - loss: 0.5108 - acc: 0.7548 - val_loss: 0.6557 - val_acc: 0.6000\n",
            "Epoch 14/300\n",
            "\n",
            "Epoch 00014: val_loss improved from 0.62558 to 0.60314, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.5022 - acc: 0.7371 - val_loss: 0.6031 - val_acc: 0.6500\n",
            "Epoch 15/300\n",
            "\n",
            "Epoch 00015: val_loss improved from 0.60314 to 0.59865, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.5092 - acc: 0.7468 - val_loss: 0.5987 - val_acc: 0.6400\n",
            "Epoch 16/300\n",
            "\n",
            "Epoch 00016: val_loss improved from 0.59865 to 0.59207, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.5360 - acc: 0.7306 - val_loss: 0.5921 - val_acc: 0.6400\n",
            "Epoch 17/300\n",
            "\n",
            "Epoch 00017: val_loss improved from 0.59207 to 0.58547, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.5232 - acc: 0.7242 - val_loss: 0.5855 - val_acc: 0.6900\n",
            "Epoch 18/300\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.58547\n",
            "620/620 - 1s - loss: 0.4866 - acc: 0.7581 - val_loss: 0.6044 - val_acc: 0.6700\n",
            "Epoch 19/300\n",
            "\n",
            "Epoch 00019: val_loss improved from 0.58547 to 0.54168, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.5013 - acc: 0.7452 - val_loss: 0.5417 - val_acc: 0.7400\n",
            "Epoch 20/300\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.5144 - acc: 0.7355 - val_loss: 0.5751 - val_acc: 0.7000\n",
            "Epoch 21/300\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.5064 - acc: 0.7452 - val_loss: 0.5902 - val_acc: 0.6700\n",
            "Epoch 22/300\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.5025 - acc: 0.7452 - val_loss: 0.6211 - val_acc: 0.5900\n",
            "Epoch 23/300\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.4890 - acc: 0.7452 - val_loss: 0.5938 - val_acc: 0.6700\n",
            "Epoch 24/300\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.5118 - acc: 0.7387 - val_loss: 0.6091 - val_acc: 0.6700\n",
            "Epoch 25/300\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.5144 - acc: 0.7290 - val_loss: 0.6665 - val_acc: 0.5600\n",
            "Epoch 26/300\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.4769 - acc: 0.7871 - val_loss: 0.7495 - val_acc: 0.5400\n",
            "Epoch 27/300\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.4887 - acc: 0.7532 - val_loss: 0.6765 - val_acc: 0.6000\n",
            "Epoch 28/300\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.4767 - acc: 0.7661 - val_loss: 0.6809 - val_acc: 0.5700\n",
            "Epoch 29/300\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.5028 - acc: 0.7435 - val_loss: 0.6618 - val_acc: 0.5900\n",
            "Epoch 30/300\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.4785 - acc: 0.7661 - val_loss: 0.6311 - val_acc: 0.6300\n",
            "Epoch 31/300\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.4869 - acc: 0.7726 - val_loss: 0.6104 - val_acc: 0.6700\n",
            "Epoch 32/300\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.4712 - acc: 0.7774 - val_loss: 0.6204 - val_acc: 0.6900\n",
            "Epoch 33/300\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.4757 - acc: 0.7694 - val_loss: 0.6127 - val_acc: 0.6800\n",
            "Epoch 34/300\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.4661 - acc: 0.7823 - val_loss: 0.5840 - val_acc: 0.7200\n",
            "Epoch 35/300\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.4876 - acc: 0.7548 - val_loss: 0.6086 - val_acc: 0.6800\n",
            "Epoch 36/300\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.4977 - acc: 0.7419 - val_loss: 0.5921 - val_acc: 0.6900\n",
            "Epoch 37/300\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.4971 - acc: 0.7613 - val_loss: 0.6326 - val_acc: 0.6500\n",
            "Epoch 38/300\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.4674 - acc: 0.7790 - val_loss: 0.6304 - val_acc: 0.6600\n",
            "Epoch 39/300\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.4722 - acc: 0.7790 - val_loss: 0.6193 - val_acc: 0.6800\n",
            "Epoch 40/300\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.4858 - acc: 0.7677 - val_loss: 0.6050 - val_acc: 0.6700\n",
            "Epoch 41/300\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.4796 - acc: 0.7581 - val_loss: 0.6136 - val_acc: 0.6500\n",
            "Epoch 42/300\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.4739 - acc: 0.7629 - val_loss: 0.6552 - val_acc: 0.6300\n",
            "Epoch 43/300\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.4706 - acc: 0.7661 - val_loss: 0.6725 - val_acc: 0.6000\n",
            "Epoch 44/300\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.4893 - acc: 0.7532 - val_loss: 0.6683 - val_acc: 0.6200\n",
            "Epoch 45/300\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.4754 - acc: 0.7645 - val_loss: 0.6667 - val_acc: 0.6200\n",
            "Epoch 46/300\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.4613 - acc: 0.7742 - val_loss: 0.5893 - val_acc: 0.7200\n",
            "Epoch 47/300\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.4638 - acc: 0.7871 - val_loss: 0.7140 - val_acc: 0.5800\n",
            "Epoch 48/300\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.4691 - acc: 0.7694 - val_loss: 0.6780 - val_acc: 0.5900\n",
            "Epoch 49/300\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.4531 - acc: 0.7790 - val_loss: 0.6262 - val_acc: 0.6700\n",
            "Epoch 50/300\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.4488 - acc: 0.7919 - val_loss: 0.5965 - val_acc: 0.6900\n",
            "Epoch 51/300\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.4558 - acc: 0.7871 - val_loss: 0.6327 - val_acc: 0.6300\n",
            "Epoch 52/300\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.4631 - acc: 0.7839 - val_loss: 0.7318 - val_acc: 0.5500\n",
            "Epoch 53/300\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.4779 - acc: 0.7581 - val_loss: 0.5874 - val_acc: 0.6900\n",
            "Epoch 54/300\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.4686 - acc: 0.7823 - val_loss: 0.6252 - val_acc: 0.6700\n",
            "Epoch 55/300\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.4509 - acc: 0.7887 - val_loss: 0.7032 - val_acc: 0.6100\n",
            "Epoch 56/300\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.4645 - acc: 0.7581 - val_loss: 0.6744 - val_acc: 0.5900\n",
            "Epoch 57/300\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.4548 - acc: 0.7935 - val_loss: 0.6695 - val_acc: 0.6000\n",
            "Epoch 58/300\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.4337 - acc: 0.8000 - val_loss: 0.7175 - val_acc: 0.6000\n",
            "Epoch 59/300\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.4547 - acc: 0.7726 - val_loss: 0.6467 - val_acc: 0.6500\n",
            "Epoch 60/300\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.4376 - acc: 0.7823 - val_loss: 0.6739 - val_acc: 0.6300\n",
            "Epoch 61/300\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.4467 - acc: 0.7726 - val_loss: 0.6637 - val_acc: 0.6300\n",
            "Epoch 62/300\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.4613 - acc: 0.7758 - val_loss: 0.6313 - val_acc: 0.6600\n",
            "Epoch 63/300\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.4581 - acc: 0.7774 - val_loss: 0.6545 - val_acc: 0.6500\n",
            "Epoch 64/300\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.4698 - acc: 0.7661 - val_loss: 0.5732 - val_acc: 0.7200\n",
            "Epoch 65/300\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.4424 - acc: 0.8048 - val_loss: 0.6550 - val_acc: 0.6400\n",
            "Epoch 66/300\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.4321 - acc: 0.7919 - val_loss: 0.6053 - val_acc: 0.6700\n",
            "Epoch 67/300\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.4379 - acc: 0.7871 - val_loss: 0.6929 - val_acc: 0.5800\n",
            "Epoch 68/300\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.4434 - acc: 0.8065 - val_loss: 0.5973 - val_acc: 0.6700\n",
            "Epoch 69/300\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.4281 - acc: 0.7935 - val_loss: 0.7066 - val_acc: 0.5800\n",
            "Epoch 70/300\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.4355 - acc: 0.7968 - val_loss: 0.5962 - val_acc: 0.6800\n",
            "Epoch 71/300\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.4479 - acc: 0.7823 - val_loss: 0.6261 - val_acc: 0.6400\n",
            "Epoch 72/300\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.4272 - acc: 0.7984 - val_loss: 0.6556 - val_acc: 0.6300\n",
            "Epoch 73/300\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.4383 - acc: 0.7952 - val_loss: 0.7784 - val_acc: 0.5500\n",
            "Epoch 74/300\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.4122 - acc: 0.8065 - val_loss: 0.6694 - val_acc: 0.5900\n",
            "Epoch 75/300\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.4482 - acc: 0.7935 - val_loss: 0.7045 - val_acc: 0.5600\n",
            "Epoch 76/300\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.4426 - acc: 0.7968 - val_loss: 0.7308 - val_acc: 0.5300\n",
            "Epoch 77/300\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.4297 - acc: 0.8065 - val_loss: 0.7828 - val_acc: 0.5400\n",
            "Epoch 78/300\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.4406 - acc: 0.7968 - val_loss: 0.6044 - val_acc: 0.6800\n",
            "Epoch 79/300\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.4533 - acc: 0.7806 - val_loss: 0.7279 - val_acc: 0.5500\n",
            "Epoch 80/300\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.4195 - acc: 0.7952 - val_loss: 0.5919 - val_acc: 0.6700\n",
            "Epoch 81/300\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.4466 - acc: 0.7935 - val_loss: 0.6891 - val_acc: 0.6100\n",
            "Epoch 82/300\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.4488 - acc: 0.7919 - val_loss: 0.7896 - val_acc: 0.5700\n",
            "Epoch 83/300\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.4516 - acc: 0.7823 - val_loss: 0.6905 - val_acc: 0.5600\n",
            "Epoch 84/300\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.4191 - acc: 0.8113 - val_loss: 0.6787 - val_acc: 0.6100\n",
            "Epoch 85/300\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.4467 - acc: 0.7935 - val_loss: 0.6746 - val_acc: 0.5800\n",
            "Epoch 86/300\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.4459 - acc: 0.8016 - val_loss: 0.6393 - val_acc: 0.6200\n",
            "Epoch 87/300\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.4295 - acc: 0.8048 - val_loss: 0.6655 - val_acc: 0.6100\n",
            "Epoch 88/300\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.4330 - acc: 0.7952 - val_loss: 0.6107 - val_acc: 0.6300\n",
            "Epoch 89/300\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.4167 - acc: 0.8000 - val_loss: 0.6778 - val_acc: 0.6000\n",
            "Epoch 90/300\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.4306 - acc: 0.7968 - val_loss: 0.6773 - val_acc: 0.6200\n",
            "Epoch 91/300\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.4298 - acc: 0.7952 - val_loss: 0.7441 - val_acc: 0.5700\n",
            "Epoch 92/300\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.4400 - acc: 0.7790 - val_loss: 0.7325 - val_acc: 0.5700\n",
            "Epoch 93/300\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.4312 - acc: 0.8032 - val_loss: 0.6986 - val_acc: 0.5800\n",
            "Epoch 94/300\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.4437 - acc: 0.8065 - val_loss: 0.6401 - val_acc: 0.6300\n",
            "Epoch 95/300\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.4346 - acc: 0.7952 - val_loss: 0.7012 - val_acc: 0.6000\n",
            "Epoch 96/300\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.4253 - acc: 0.8242 - val_loss: 0.6466 - val_acc: 0.6400\n",
            "Epoch 97/300\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.4431 - acc: 0.7887 - val_loss: 0.7182 - val_acc: 0.5900\n",
            "Epoch 98/300\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.4072 - acc: 0.8113 - val_loss: 0.6558 - val_acc: 0.6100\n",
            "Epoch 99/300\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.4268 - acc: 0.7919 - val_loss: 0.6771 - val_acc: 0.6200\n",
            "Epoch 100/300\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.4213 - acc: 0.7984 - val_loss: 0.6403 - val_acc: 0.6500\n",
            "Epoch 101/300\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.4287 - acc: 0.7952 - val_loss: 0.7924 - val_acc: 0.5600\n",
            "Epoch 102/300\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.4069 - acc: 0.8161 - val_loss: 0.7755 - val_acc: 0.5700\n",
            "Epoch 103/300\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.4229 - acc: 0.8048 - val_loss: 0.6604 - val_acc: 0.6300\n",
            "Epoch 104/300\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.4319 - acc: 0.7903 - val_loss: 0.8003 - val_acc: 0.5500\n",
            "Epoch 105/300\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.4132 - acc: 0.8145 - val_loss: 0.7049 - val_acc: 0.5900\n",
            "Epoch 106/300\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.4138 - acc: 0.8016 - val_loss: 0.6167 - val_acc: 0.6500\n",
            "Epoch 107/300\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.4092 - acc: 0.8145 - val_loss: 0.7780 - val_acc: 0.5300\n",
            "Epoch 108/300\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.4348 - acc: 0.8177 - val_loss: 0.7258 - val_acc: 0.5600\n",
            "Epoch 109/300\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.4394 - acc: 0.7871 - val_loss: 0.6272 - val_acc: 0.6400\n",
            "Epoch 110/300\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.3966 - acc: 0.8161 - val_loss: 0.9022 - val_acc: 0.5300\n",
            "Epoch 111/300\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.3955 - acc: 0.8339 - val_loss: 0.6815 - val_acc: 0.6000\n",
            "Epoch 112/300\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.4115 - acc: 0.8065 - val_loss: 0.7657 - val_acc: 0.5400\n",
            "Epoch 113/300\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.4431 - acc: 0.7903 - val_loss: 0.6843 - val_acc: 0.5900\n",
            "Epoch 114/300\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.4133 - acc: 0.8016 - val_loss: 0.7385 - val_acc: 0.5900\n",
            "Epoch 115/300\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.3894 - acc: 0.8306 - val_loss: 0.7380 - val_acc: 0.6000\n",
            "Epoch 116/300\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.3868 - acc: 0.8274 - val_loss: 0.6340 - val_acc: 0.6200\n",
            "Epoch 117/300\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.4144 - acc: 0.8048 - val_loss: 0.6900 - val_acc: 0.6200\n",
            "Epoch 118/300\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.4520 - acc: 0.7903 - val_loss: 0.6847 - val_acc: 0.5800\n",
            "Epoch 119/300\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.4330 - acc: 0.8000 - val_loss: 0.7405 - val_acc: 0.5800\n",
            "Epoch 120/300\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.4278 - acc: 0.7984 - val_loss: 0.6490 - val_acc: 0.6300\n",
            "Epoch 121/300\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.4361 - acc: 0.7903 - val_loss: 0.6337 - val_acc: 0.6200\n",
            "Epoch 122/300\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.4032 - acc: 0.8194 - val_loss: 0.7635 - val_acc: 0.5800\n",
            "Epoch 123/300\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.4220 - acc: 0.8032 - val_loss: 0.6801 - val_acc: 0.5900\n",
            "Epoch 124/300\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.4199 - acc: 0.7984 - val_loss: 0.6869 - val_acc: 0.6100\n",
            "Epoch 125/300\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.4144 - acc: 0.8161 - val_loss: 0.6394 - val_acc: 0.6400\n",
            "Epoch 126/300\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.4020 - acc: 0.8113 - val_loss: 0.7943 - val_acc: 0.5800\n",
            "Epoch 127/300\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.4124 - acc: 0.8129 - val_loss: 0.6813 - val_acc: 0.6100\n",
            "Epoch 128/300\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.4211 - acc: 0.8065 - val_loss: 0.8044 - val_acc: 0.5400\n",
            "Epoch 129/300\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.4405 - acc: 0.7855 - val_loss: 0.6881 - val_acc: 0.6000\n",
            "Epoch 130/300\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.4094 - acc: 0.8065 - val_loss: 0.6269 - val_acc: 0.6500\n",
            "Epoch 131/300\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.4161 - acc: 0.8113 - val_loss: 0.6159 - val_acc: 0.6400\n",
            "Epoch 132/300\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.4046 - acc: 0.8194 - val_loss: 0.7058 - val_acc: 0.5800\n",
            "Epoch 133/300\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.3955 - acc: 0.8161 - val_loss: 0.8009 - val_acc: 0.5700\n",
            "Epoch 134/300\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.4220 - acc: 0.8161 - val_loss: 0.6909 - val_acc: 0.5900\n",
            "Epoch 135/300\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.3835 - acc: 0.8097 - val_loss: 0.7237 - val_acc: 0.5700\n",
            "Epoch 136/300\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.4056 - acc: 0.8355 - val_loss: 0.8263 - val_acc: 0.5400\n",
            "Epoch 137/300\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.3917 - acc: 0.8387 - val_loss: 0.7049 - val_acc: 0.6000\n",
            "Epoch 138/300\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.3973 - acc: 0.8081 - val_loss: 0.7205 - val_acc: 0.5700\n",
            "Epoch 139/300\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.4002 - acc: 0.8258 - val_loss: 0.7167 - val_acc: 0.5700\n",
            "Epoch 140/300\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.4242 - acc: 0.8032 - val_loss: 0.6563 - val_acc: 0.6000\n",
            "Epoch 141/300\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.3995 - acc: 0.8145 - val_loss: 0.7306 - val_acc: 0.5500\n",
            "Epoch 142/300\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.3889 - acc: 0.8339 - val_loss: 0.8526 - val_acc: 0.5600\n",
            "Epoch 143/300\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.4165 - acc: 0.8113 - val_loss: 0.7706 - val_acc: 0.5600\n",
            "Epoch 144/300\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.3918 - acc: 0.8161 - val_loss: 0.8587 - val_acc: 0.5500\n",
            "Epoch 145/300\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.4180 - acc: 0.8000 - val_loss: 0.7815 - val_acc: 0.5600\n",
            "Epoch 146/300\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.4068 - acc: 0.8129 - val_loss: 0.7125 - val_acc: 0.5900\n",
            "Epoch 147/300\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.3843 - acc: 0.8210 - val_loss: 0.6128 - val_acc: 0.6200\n",
            "Epoch 148/300\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.3848 - acc: 0.8306 - val_loss: 0.8631 - val_acc: 0.5300\n",
            "Epoch 149/300\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.4302 - acc: 0.7968 - val_loss: 0.7395 - val_acc: 0.6000\n",
            "Epoch 150/300\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.3983 - acc: 0.8194 - val_loss: 0.5794 - val_acc: 0.6800\n",
            "Epoch 151/300\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.3811 - acc: 0.8226 - val_loss: 0.7973 - val_acc: 0.5500\n",
            "Epoch 152/300\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.4121 - acc: 0.8113 - val_loss: 0.6544 - val_acc: 0.6300\n",
            "Epoch 153/300\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.4093 - acc: 0.8065 - val_loss: 0.6899 - val_acc: 0.6000\n",
            "Epoch 154/300\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.4078 - acc: 0.8032 - val_loss: 0.7963 - val_acc: 0.5500\n",
            "Epoch 155/300\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.3975 - acc: 0.8145 - val_loss: 0.7165 - val_acc: 0.5900\n",
            "Epoch 156/300\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.3856 - acc: 0.8306 - val_loss: 0.7617 - val_acc: 0.5800\n",
            "Epoch 157/300\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.4258 - acc: 0.8048 - val_loss: 0.7457 - val_acc: 0.5800\n",
            "Epoch 158/300\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.3983 - acc: 0.8258 - val_loss: 0.7106 - val_acc: 0.5800\n",
            "Epoch 159/300\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.3901 - acc: 0.8403 - val_loss: 0.9136 - val_acc: 0.5400\n",
            "Epoch 160/300\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.4004 - acc: 0.8258 - val_loss: 0.7324 - val_acc: 0.5600\n",
            "Epoch 161/300\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.4038 - acc: 0.8194 - val_loss: 0.7592 - val_acc: 0.5600\n",
            "Epoch 162/300\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.4085 - acc: 0.8194 - val_loss: 0.7217 - val_acc: 0.5900\n",
            "Epoch 163/300\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.3964 - acc: 0.8210 - val_loss: 0.6816 - val_acc: 0.5900\n",
            "Epoch 164/300\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.3858 - acc: 0.8290 - val_loss: 0.7095 - val_acc: 0.5800\n",
            "Epoch 165/300\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.3797 - acc: 0.8339 - val_loss: 0.7088 - val_acc: 0.6000\n",
            "Epoch 166/300\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.4072 - acc: 0.8016 - val_loss: 0.7907 - val_acc: 0.5700\n",
            "Epoch 167/300\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.3721 - acc: 0.8419 - val_loss: 0.7020 - val_acc: 0.5900\n",
            "Epoch 168/300\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.4017 - acc: 0.8258 - val_loss: 0.7055 - val_acc: 0.5900\n",
            "Epoch 169/300\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.4036 - acc: 0.8097 - val_loss: 0.6911 - val_acc: 0.5700\n",
            "Epoch 170/300\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.4071 - acc: 0.8032 - val_loss: 0.6682 - val_acc: 0.6000\n",
            "Epoch 171/300\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.3721 - acc: 0.8435 - val_loss: 0.6726 - val_acc: 0.5900\n",
            "Epoch 172/300\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.3771 - acc: 0.8145 - val_loss: 0.7321 - val_acc: 0.5900\n",
            "Epoch 173/300\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.3933 - acc: 0.8258 - val_loss: 0.8556 - val_acc: 0.5500\n",
            "Epoch 174/300\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.4032 - acc: 0.8097 - val_loss: 0.7474 - val_acc: 0.5900\n",
            "Epoch 175/300\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.4103 - acc: 0.8048 - val_loss: 0.7287 - val_acc: 0.5800\n",
            "Epoch 176/300\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.4135 - acc: 0.8097 - val_loss: 0.6934 - val_acc: 0.6100\n",
            "Epoch 177/300\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.3863 - acc: 0.8355 - val_loss: 0.9733 - val_acc: 0.5100\n",
            "Epoch 178/300\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.3750 - acc: 0.8323 - val_loss: 0.8741 - val_acc: 0.5800\n",
            "Epoch 179/300\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.3790 - acc: 0.8177 - val_loss: 0.9525 - val_acc: 0.5000\n",
            "Epoch 180/300\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.3776 - acc: 0.8500 - val_loss: 0.7443 - val_acc: 0.5800\n",
            "Epoch 181/300\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.3933 - acc: 0.8226 - val_loss: 0.6644 - val_acc: 0.6300\n",
            "Epoch 182/300\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.3900 - acc: 0.8323 - val_loss: 0.7136 - val_acc: 0.6200\n",
            "Epoch 183/300\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.4110 - acc: 0.8161 - val_loss: 0.8416 - val_acc: 0.5400\n",
            "Epoch 184/300\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.3795 - acc: 0.8161 - val_loss: 0.7426 - val_acc: 0.5700\n",
            "Epoch 185/300\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.3699 - acc: 0.8452 - val_loss: 0.6839 - val_acc: 0.6000\n",
            "Epoch 186/300\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.4098 - acc: 0.8161 - val_loss: 0.6975 - val_acc: 0.5700\n",
            "Epoch 187/300\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.4155 - acc: 0.8081 - val_loss: 0.7647 - val_acc: 0.5700\n",
            "Epoch 188/300\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.4043 - acc: 0.8226 - val_loss: 0.7973 - val_acc: 0.6000\n",
            "Epoch 189/300\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.3860 - acc: 0.8226 - val_loss: 0.6266 - val_acc: 0.6600\n",
            "Epoch 190/300\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.3812 - acc: 0.8177 - val_loss: 0.6837 - val_acc: 0.6000\n",
            "Epoch 191/300\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.4112 - acc: 0.8194 - val_loss: 0.7474 - val_acc: 0.5700\n",
            "Epoch 192/300\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.4190 - acc: 0.7984 - val_loss: 0.5663 - val_acc: 0.7100\n",
            "Epoch 193/300\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.3977 - acc: 0.8290 - val_loss: 0.8344 - val_acc: 0.5600\n",
            "Epoch 194/300\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.3994 - acc: 0.8484 - val_loss: 0.6179 - val_acc: 0.6300\n",
            "Epoch 195/300\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.4088 - acc: 0.8226 - val_loss: 0.8377 - val_acc: 0.5400\n",
            "Epoch 196/300\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.3985 - acc: 0.8113 - val_loss: 0.7533 - val_acc: 0.5700\n",
            "Epoch 197/300\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.3807 - acc: 0.8242 - val_loss: 0.6991 - val_acc: 0.6000\n",
            "Epoch 198/300\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.4052 - acc: 0.8097 - val_loss: 0.5925 - val_acc: 0.6700\n",
            "Epoch 199/300\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.4181 - acc: 0.8242 - val_loss: 0.6808 - val_acc: 0.6000\n",
            "Epoch 200/300\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.3803 - acc: 0.8065 - val_loss: 0.7693 - val_acc: 0.5700\n",
            "Epoch 201/300\n",
            "\n",
            "Epoch 00201: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.3955 - acc: 0.8161 - val_loss: 0.7820 - val_acc: 0.5900\n",
            "Epoch 202/300\n",
            "\n",
            "Epoch 00202: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.3876 - acc: 0.8306 - val_loss: 0.7050 - val_acc: 0.6100\n",
            "Epoch 203/300\n",
            "\n",
            "Epoch 00203: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.4117 - acc: 0.8145 - val_loss: 0.7208 - val_acc: 0.6000\n",
            "Epoch 204/300\n",
            "\n",
            "Epoch 00204: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.3902 - acc: 0.8081 - val_loss: 0.6239 - val_acc: 0.6700\n",
            "Epoch 205/300\n",
            "\n",
            "Epoch 00205: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.3797 - acc: 0.8226 - val_loss: 0.7328 - val_acc: 0.5800\n",
            "Epoch 206/300\n",
            "\n",
            "Epoch 00206: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.3898 - acc: 0.8177 - val_loss: 0.7815 - val_acc: 0.5800\n",
            "Epoch 207/300\n",
            "\n",
            "Epoch 00207: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.3755 - acc: 0.8306 - val_loss: 0.7220 - val_acc: 0.5900\n",
            "Epoch 208/300\n",
            "\n",
            "Epoch 00208: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.3763 - acc: 0.8371 - val_loss: 0.7312 - val_acc: 0.5900\n",
            "Epoch 209/300\n",
            "\n",
            "Epoch 00209: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.3761 - acc: 0.8210 - val_loss: 0.6078 - val_acc: 0.6600\n",
            "Epoch 210/300\n",
            "\n",
            "Epoch 00210: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.3828 - acc: 0.8081 - val_loss: 0.6068 - val_acc: 0.6600\n",
            "Epoch 211/300\n",
            "\n",
            "Epoch 00211: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.3912 - acc: 0.8339 - val_loss: 0.6340 - val_acc: 0.6500\n",
            "Epoch 212/300\n",
            "\n",
            "Epoch 00212: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.3582 - acc: 0.8452 - val_loss: 0.7669 - val_acc: 0.5800\n",
            "Epoch 213/300\n",
            "\n",
            "Epoch 00213: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.3730 - acc: 0.8177 - val_loss: 0.6764 - val_acc: 0.6200\n",
            "Epoch 214/300\n",
            "\n",
            "Epoch 00214: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.4054 - acc: 0.8145 - val_loss: 0.7366 - val_acc: 0.5800\n",
            "Epoch 215/300\n",
            "\n",
            "Epoch 00215: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.3793 - acc: 0.8452 - val_loss: 0.6650 - val_acc: 0.6000\n",
            "Epoch 216/300\n",
            "\n",
            "Epoch 00216: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.3984 - acc: 0.8258 - val_loss: 0.8260 - val_acc: 0.5800\n",
            "Epoch 217/300\n",
            "\n",
            "Epoch 00217: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.4080 - acc: 0.8194 - val_loss: 0.7143 - val_acc: 0.6000\n",
            "Epoch 218/300\n",
            "\n",
            "Epoch 00218: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.3912 - acc: 0.8258 - val_loss: 0.7329 - val_acc: 0.6100\n",
            "Epoch 219/300\n",
            "\n",
            "Epoch 00219: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.3752 - acc: 0.8339 - val_loss: 0.7086 - val_acc: 0.6000\n",
            "Epoch 220/300\n",
            "\n",
            "Epoch 00220: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.3934 - acc: 0.8226 - val_loss: 0.6193 - val_acc: 0.6300\n",
            "Epoch 221/300\n",
            "\n",
            "Epoch 00221: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.3668 - acc: 0.8435 - val_loss: 0.7057 - val_acc: 0.6100\n",
            "Epoch 222/300\n",
            "\n",
            "Epoch 00222: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.3757 - acc: 0.8242 - val_loss: 0.7654 - val_acc: 0.5700\n",
            "Epoch 223/300\n",
            "\n",
            "Epoch 00223: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.3666 - acc: 0.8339 - val_loss: 0.8751 - val_acc: 0.5500\n",
            "Epoch 224/300\n",
            "\n",
            "Epoch 00224: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.3709 - acc: 0.8161 - val_loss: 0.7154 - val_acc: 0.5800\n",
            "Epoch 225/300\n",
            "\n",
            "Epoch 00225: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.4029 - acc: 0.8226 - val_loss: 0.8166 - val_acc: 0.5500\n",
            "Epoch 226/300\n",
            "\n",
            "Epoch 00226: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.3580 - acc: 0.8452 - val_loss: 0.7737 - val_acc: 0.5800\n",
            "Epoch 227/300\n",
            "\n",
            "Epoch 00227: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.3704 - acc: 0.8242 - val_loss: 0.8499 - val_acc: 0.5700\n",
            "Epoch 228/300\n",
            "\n",
            "Epoch 00228: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.3769 - acc: 0.8258 - val_loss: 0.8381 - val_acc: 0.5800\n",
            "Epoch 229/300\n",
            "\n",
            "Epoch 00229: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.3913 - acc: 0.8113 - val_loss: 0.6648 - val_acc: 0.6100\n",
            "Epoch 230/300\n",
            "\n",
            "Epoch 00230: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.3772 - acc: 0.8419 - val_loss: 0.7792 - val_acc: 0.5900\n",
            "Epoch 231/300\n",
            "\n",
            "Epoch 00231: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.3783 - acc: 0.8177 - val_loss: 0.6341 - val_acc: 0.6300\n",
            "Epoch 232/300\n",
            "\n",
            "Epoch 00232: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.4007 - acc: 0.8210 - val_loss: 0.7245 - val_acc: 0.6200\n",
            "Epoch 233/300\n",
            "\n",
            "Epoch 00233: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.3879 - acc: 0.8194 - val_loss: 0.7724 - val_acc: 0.6000\n",
            "Epoch 234/300\n",
            "\n",
            "Epoch 00234: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.4063 - acc: 0.8081 - val_loss: 0.7957 - val_acc: 0.5600\n",
            "Epoch 235/300\n",
            "\n",
            "Epoch 00235: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.3967 - acc: 0.8177 - val_loss: 0.8037 - val_acc: 0.5500\n",
            "Epoch 236/300\n",
            "\n",
            "Epoch 00236: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.3593 - acc: 0.8355 - val_loss: 0.8805 - val_acc: 0.5700\n",
            "Epoch 237/300\n",
            "\n",
            "Epoch 00237: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.3695 - acc: 0.8323 - val_loss: 0.6674 - val_acc: 0.6200\n",
            "Epoch 238/300\n",
            "\n",
            "Epoch 00238: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.3835 - acc: 0.8355 - val_loss: 0.8558 - val_acc: 0.5500\n",
            "Epoch 239/300\n",
            "\n",
            "Epoch 00239: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.3949 - acc: 0.8177 - val_loss: 0.6545 - val_acc: 0.6400\n",
            "Epoch 240/300\n",
            "\n",
            "Epoch 00240: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.3853 - acc: 0.8226 - val_loss: 0.6355 - val_acc: 0.6300\n",
            "Epoch 241/300\n",
            "\n",
            "Epoch 00241: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.3625 - acc: 0.8565 - val_loss: 0.7404 - val_acc: 0.5800\n",
            "Epoch 242/300\n",
            "\n",
            "Epoch 00242: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.3800 - acc: 0.8452 - val_loss: 0.8498 - val_acc: 0.5600\n",
            "Epoch 243/300\n",
            "\n",
            "Epoch 00243: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.3629 - acc: 0.8565 - val_loss: 0.7484 - val_acc: 0.5800\n",
            "Epoch 244/300\n",
            "\n",
            "Epoch 00244: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.3687 - acc: 0.8468 - val_loss: 0.8210 - val_acc: 0.5500\n",
            "Epoch 245/300\n",
            "\n",
            "Epoch 00245: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.3727 - acc: 0.8274 - val_loss: 0.7852 - val_acc: 0.5700\n",
            "Epoch 246/300\n",
            "\n",
            "Epoch 00246: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.3489 - acc: 0.8484 - val_loss: 0.6594 - val_acc: 0.6200\n",
            "Epoch 247/300\n",
            "\n",
            "Epoch 00247: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.3767 - acc: 0.8452 - val_loss: 0.7175 - val_acc: 0.5700\n",
            "Epoch 248/300\n",
            "\n",
            "Epoch 00248: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.3781 - acc: 0.8355 - val_loss: 0.8027 - val_acc: 0.5700\n",
            "Epoch 249/300\n",
            "\n",
            "Epoch 00249: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.3643 - acc: 0.8403 - val_loss: 0.8092 - val_acc: 0.5600\n",
            "Epoch 250/300\n",
            "\n",
            "Epoch 00250: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.3509 - acc: 0.8484 - val_loss: 0.9842 - val_acc: 0.5200\n",
            "Epoch 251/300\n",
            "\n",
            "Epoch 00251: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.3572 - acc: 0.8484 - val_loss: 0.7207 - val_acc: 0.5900\n",
            "Epoch 252/300\n",
            "\n",
            "Epoch 00252: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.4000 - acc: 0.8129 - val_loss: 0.7699 - val_acc: 0.5500\n",
            "Epoch 253/300\n",
            "\n",
            "Epoch 00253: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.3825 - acc: 0.8048 - val_loss: 0.7065 - val_acc: 0.5900\n",
            "Epoch 254/300\n",
            "\n",
            "Epoch 00254: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.3707 - acc: 0.8274 - val_loss: 0.7832 - val_acc: 0.5400\n",
            "Epoch 255/300\n",
            "\n",
            "Epoch 00255: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.3594 - acc: 0.8597 - val_loss: 0.7646 - val_acc: 0.5800\n",
            "Epoch 256/300\n",
            "\n",
            "Epoch 00256: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.3638 - acc: 0.8500 - val_loss: 0.6609 - val_acc: 0.6200\n",
            "Epoch 257/300\n",
            "\n",
            "Epoch 00257: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.3575 - acc: 0.8452 - val_loss: 0.7370 - val_acc: 0.5700\n",
            "Epoch 258/300\n",
            "\n",
            "Epoch 00258: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.3649 - acc: 0.8371 - val_loss: 0.9052 - val_acc: 0.5700\n",
            "Epoch 259/300\n",
            "\n",
            "Epoch 00259: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.3896 - acc: 0.8355 - val_loss: 0.7363 - val_acc: 0.5900\n",
            "Epoch 260/300\n",
            "\n",
            "Epoch 00260: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.3854 - acc: 0.8500 - val_loss: 0.7555 - val_acc: 0.5800\n",
            "Epoch 261/300\n",
            "\n",
            "Epoch 00261: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.4029 - acc: 0.8145 - val_loss: 0.7342 - val_acc: 0.5900\n",
            "Epoch 262/300\n",
            "\n",
            "Epoch 00262: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.3617 - acc: 0.8500 - val_loss: 0.6771 - val_acc: 0.6000\n",
            "Epoch 263/300\n",
            "\n",
            "Epoch 00263: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.3777 - acc: 0.8306 - val_loss: 0.8214 - val_acc: 0.5900\n",
            "Epoch 264/300\n",
            "\n",
            "Epoch 00264: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.3741 - acc: 0.8387 - val_loss: 0.7131 - val_acc: 0.6000\n",
            "Epoch 265/300\n",
            "\n",
            "Epoch 00265: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.3802 - acc: 0.8210 - val_loss: 0.6464 - val_acc: 0.6100\n",
            "Epoch 266/300\n",
            "\n",
            "Epoch 00266: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.3864 - acc: 0.8290 - val_loss: 0.6194 - val_acc: 0.6200\n",
            "Epoch 267/300\n",
            "\n",
            "Epoch 00267: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.3788 - acc: 0.8323 - val_loss: 0.7285 - val_acc: 0.6000\n",
            "Epoch 268/300\n",
            "\n",
            "Epoch 00268: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.3855 - acc: 0.8145 - val_loss: 0.6742 - val_acc: 0.6100\n",
            "Epoch 269/300\n",
            "\n",
            "Epoch 00269: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.3734 - acc: 0.8452 - val_loss: 0.7073 - val_acc: 0.6300\n",
            "Epoch 270/300\n",
            "\n",
            "Epoch 00270: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.3671 - acc: 0.8371 - val_loss: 0.6912 - val_acc: 0.6100\n",
            "Epoch 271/300\n",
            "\n",
            "Epoch 00271: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.3739 - acc: 0.8274 - val_loss: 0.7068 - val_acc: 0.5900\n",
            "Epoch 272/300\n",
            "\n",
            "Epoch 00272: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.3633 - acc: 0.8419 - val_loss: 0.6807 - val_acc: 0.6100\n",
            "Epoch 273/300\n",
            "\n",
            "Epoch 00273: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.3763 - acc: 0.8306 - val_loss: 0.6691 - val_acc: 0.6000\n",
            "Epoch 274/300\n",
            "\n",
            "Epoch 00274: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.3549 - acc: 0.8435 - val_loss: 0.8108 - val_acc: 0.5900\n",
            "Epoch 275/300\n",
            "\n",
            "Epoch 00275: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.3952 - acc: 0.8274 - val_loss: 0.7422 - val_acc: 0.5900\n",
            "Epoch 276/300\n",
            "\n",
            "Epoch 00276: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.3556 - acc: 0.8500 - val_loss: 0.7593 - val_acc: 0.5900\n",
            "Epoch 277/300\n",
            "\n",
            "Epoch 00277: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.3529 - acc: 0.8484 - val_loss: 0.8202 - val_acc: 0.5800\n",
            "Epoch 278/300\n",
            "\n",
            "Epoch 00278: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.3696 - acc: 0.8323 - val_loss: 0.7450 - val_acc: 0.5900\n",
            "Epoch 279/300\n",
            "\n",
            "Epoch 00279: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.3910 - acc: 0.8258 - val_loss: 0.7979 - val_acc: 0.5900\n",
            "Epoch 280/300\n",
            "\n",
            "Epoch 00280: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.3634 - acc: 0.8387 - val_loss: 0.8766 - val_acc: 0.5700\n",
            "Epoch 281/300\n",
            "\n",
            "Epoch 00281: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.3970 - acc: 0.8403 - val_loss: 0.6680 - val_acc: 0.6000\n",
            "Epoch 282/300\n",
            "\n",
            "Epoch 00282: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.3683 - acc: 0.8274 - val_loss: 0.6718 - val_acc: 0.6100\n",
            "Epoch 283/300\n",
            "\n",
            "Epoch 00283: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.3854 - acc: 0.8258 - val_loss: 0.6890 - val_acc: 0.6200\n",
            "Epoch 284/300\n",
            "\n",
            "Epoch 00284: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.3777 - acc: 0.8339 - val_loss: 0.7627 - val_acc: 0.5900\n",
            "Epoch 285/300\n",
            "\n",
            "Epoch 00285: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.3829 - acc: 0.8323 - val_loss: 0.6511 - val_acc: 0.6000\n",
            "Epoch 286/300\n",
            "\n",
            "Epoch 00286: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.3614 - acc: 0.8403 - val_loss: 0.8242 - val_acc: 0.5800\n",
            "Epoch 287/300\n",
            "\n",
            "Epoch 00287: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.3989 - acc: 0.8290 - val_loss: 0.7825 - val_acc: 0.5800\n",
            "Epoch 288/300\n",
            "\n",
            "Epoch 00288: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.3832 - acc: 0.8290 - val_loss: 0.7482 - val_acc: 0.6100\n",
            "Epoch 289/300\n",
            "\n",
            "Epoch 00289: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.3597 - acc: 0.8419 - val_loss: 0.6791 - val_acc: 0.5900\n",
            "Epoch 290/300\n",
            "\n",
            "Epoch 00290: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.3786 - acc: 0.8403 - val_loss: 0.5849 - val_acc: 0.6600\n",
            "Epoch 291/300\n",
            "\n",
            "Epoch 00291: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.3506 - acc: 0.8452 - val_loss: 0.5876 - val_acc: 0.6600\n",
            "Epoch 292/300\n",
            "\n",
            "Epoch 00292: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.3688 - acc: 0.8452 - val_loss: 0.7104 - val_acc: 0.6100\n",
            "Epoch 293/300\n",
            "\n",
            "Epoch 00293: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.3804 - acc: 0.8194 - val_loss: 0.6728 - val_acc: 0.6300\n",
            "Epoch 294/300\n",
            "\n",
            "Epoch 00294: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.3660 - acc: 0.8387 - val_loss: 0.8939 - val_acc: 0.5500\n",
            "Epoch 295/300\n",
            "\n",
            "Epoch 00295: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.3747 - acc: 0.8339 - val_loss: 0.6826 - val_acc: 0.6000\n",
            "Epoch 296/300\n",
            "\n",
            "Epoch 00296: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.3732 - acc: 0.8435 - val_loss: 0.7462 - val_acc: 0.5600\n",
            "Epoch 297/300\n",
            "\n",
            "Epoch 00297: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.3651 - acc: 0.8339 - val_loss: 0.7417 - val_acc: 0.5900\n",
            "Epoch 298/300\n",
            "\n",
            "Epoch 00298: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.3672 - acc: 0.8387 - val_loss: 0.7479 - val_acc: 0.6000\n",
            "Epoch 299/300\n",
            "\n",
            "Epoch 00299: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.3692 - acc: 0.8290 - val_loss: 0.7081 - val_acc: 0.6000\n",
            "Epoch 300/300\n",
            "\n",
            "Epoch 00300: val_loss did not improve from 0.54168\n",
            "620/620 - 1s - loss: 0.3763 - acc: 0.8419 - val_loss: 0.6766 - val_acc: 0.5900\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "# Evaluate on test data\n",
            "40/40 [==============================] - 0s 3ms/sample - loss: 0.6046 - acc: 0.7250\n",
            "test loss, test acc: [0.6045952931046485, 0.725]\n",
            "1\n",
            "EEG_Deep/Data2A/parsed_P01T.mat\n",
            "(0, 12, 4096)\n",
            "(80, 12, 4096)\n",
            "(80, 1)\n",
            "2\n",
            "EEG_Deep/Data2A/parsed_P02T.mat\n",
            "(80, 12, 4096)\n",
            "(160, 12, 4096)\n",
            "(160, 1)\n",
            "3\n",
            "EEG_Deep/Data2A/parsed_P03T.mat\n",
            "(160, 12, 4096)\n",
            "(240, 12, 4096)\n",
            "(240, 1)\n",
            "4\n",
            "EEG_Deep/Data2A/parsed_P04T.mat\n",
            "(240, 12, 4096)\n",
            "(320, 12, 4096)\n",
            "(320, 1)\n",
            "5\n",
            "EEG_Deep/Data2A/parsed_P05T.mat\n",
            "(320, 12, 4096)\n",
            "(400, 12, 4096)\n",
            "(400, 1)\n",
            "6\n",
            "EEG_Deep/Data2A/parsed_P06T.mat\n",
            "(400, 12, 4096)\n",
            "(480, 12, 4096)\n",
            "(480, 1)\n",
            "7\n",
            "EEG_Deep/Data2A/parsed_P07T.mat\n",
            "(480, 12, 4096)\n",
            "(560, 12, 4096)\n",
            "(560, 1)\n",
            "9\n",
            "EEG_Deep/Data2A/parsed_P09T.mat\n",
            "(560, 12, 4096)\n",
            "(640, 12, 4096)\n",
            "(640, 1)\n",
            "10\n",
            "EEG_Deep/Data2A/parsed_P010T.mat\n",
            "(640, 12, 4096)\n",
            "(720, 12, 4096)\n",
            "(720, 1)\n",
            "8\n",
            "EEG_Deep/Data2A/parsed_P08E.mat\n",
            "(0, 12, 4096)\n",
            "(40, 12, 4096)\n",
            "(40, 1)\n",
            "Filtering of training data in progress\n",
            "(720, 12, 4096)\n",
            "Filtering of test data in progress\n",
            "(40, 12, 4096)\n",
            "[2. 1. 2. 2. 1. 1. 1. 2. 1. 2. 1. 2. 1. 2. 1. 2. 1. 1. 2. 1. 2. 1. 1. 1.\n",
            " 1. 1. 1. 2. 1. 2. 1. 1. 2. 2. 2. 2. 2. 1. 1. 2. 1. 2. 2. 1. 1. 1. 1. 2.\n",
            " 2. 1. 2. 1. 1. 2. 1. 2. 1. 1. 1. 1. 1. 2. 1. 1. 1. 2. 2. 1. 1. 1. 2. 2.\n",
            " 2. 2. 2. 1. 2. 2. 1. 2. 1. 1. 2. 2. 2. 1. 2. 2. 1. 1. 2. 2. 2. 2. 2. 2.\n",
            " 2. 1. 2. 1.]\n",
            "(620, 12, 1536)\n",
            "(620,)\n",
            "(100, 12, 1536)\n",
            "(100,)\n",
            "[[0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]]\n",
            "X_train shape: (620, 1, 12, 1536)\n",
            "620 train samples\n",
            "100 val samples\n",
            "(40, 12, 1536)\n",
            "(40, 1)\n",
            "X_train shape: (40, 1, 12, 1536)\n",
            "40 train samples\n",
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 1, 12, 1536)]     0         \n",
            "_________________________________________________________________\n",
            "conv2d (Conv2D)              (None, 8, 12, 1536)       200       \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 8, 12, 1536)       32        \n",
            "_________________________________________________________________\n",
            "depthwise_conv2d (DepthwiseC (None, 16, 1, 1536)       192       \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 16, 1, 1536)       64        \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 16, 1, 1536)       0         \n",
            "_________________________________________________________________\n",
            "average_pooling2d (AveragePo (None, 16, 1, 384)        0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 16, 1, 384)        0         \n",
            "_________________________________________________________________\n",
            "separable_conv2d (SeparableC (None, 16, 1, 384)        512       \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 16, 1, 384)        64        \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 16, 1, 384)        0         \n",
            "_________________________________________________________________\n",
            "average_pooling2d_1 (Average (None, 16, 1, 48)         0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 16, 1, 48)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 768)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 2)                 1538      \n",
            "_________________________________________________________________\n",
            "softmax (Activation)         (None, 2)                 0         \n",
            "=================================================================\n",
            "Total params: 2,602\n",
            "Trainable params: 2,522\n",
            "Non-trainable params: 80\n",
            "_________________________________________________________________\n",
            "Train on 620 samples, validate on 100 samples\n",
            "Epoch 1/300\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.68996, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.7057 - acc: 0.4919 - val_loss: 0.6900 - val_acc: 0.5000\n",
            "Epoch 2/300\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.68996 to 0.68591, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.6823 - acc: 0.5661 - val_loss: 0.6859 - val_acc: 0.5900\n",
            "Epoch 3/300\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.68591\n",
            "620/620 - 1s - loss: 0.6599 - acc: 0.6339 - val_loss: 0.6873 - val_acc: 0.5500\n",
            "Epoch 4/300\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.68591 to 0.68398, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.6474 - acc: 0.6452 - val_loss: 0.6840 - val_acc: 0.6100\n",
            "Epoch 5/300\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.68398 to 0.67940, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.6275 - acc: 0.6774 - val_loss: 0.6794 - val_acc: 0.5600\n",
            "Epoch 6/300\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.67940\n",
            "620/620 - 1s - loss: 0.6060 - acc: 0.6968 - val_loss: 0.6802 - val_acc: 0.5800\n",
            "Epoch 7/300\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.67940 to 0.65201, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.5804 - acc: 0.7274 - val_loss: 0.6520 - val_acc: 0.7100\n",
            "Epoch 8/300\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.65201 to 0.64248, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.5656 - acc: 0.7065 - val_loss: 0.6425 - val_acc: 0.7300\n",
            "Epoch 9/300\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.64248 to 0.61482, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.5551 - acc: 0.7177 - val_loss: 0.6148 - val_acc: 0.7600\n",
            "Epoch 10/300\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.61482 to 0.60829, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.5076 - acc: 0.7387 - val_loss: 0.6083 - val_acc: 0.7100\n",
            "Epoch 11/300\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.60829\n",
            "620/620 - 1s - loss: 0.4741 - acc: 0.8048 - val_loss: 0.6468 - val_acc: 0.6600\n",
            "Epoch 12/300\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.60829\n",
            "620/620 - 1s - loss: 0.4972 - acc: 0.7565 - val_loss: 0.6910 - val_acc: 0.6300\n",
            "Epoch 13/300\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.60829\n",
            "620/620 - 1s - loss: 0.4609 - acc: 0.7839 - val_loss: 0.6140 - val_acc: 0.6900\n",
            "Epoch 14/300\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.60829\n",
            "620/620 - 1s - loss: 0.4726 - acc: 0.7710 - val_loss: 0.6220 - val_acc: 0.6800\n",
            "Epoch 15/300\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.60829\n",
            "620/620 - 1s - loss: 0.4691 - acc: 0.7629 - val_loss: 0.6219 - val_acc: 0.6900\n",
            "Epoch 16/300\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.60829\n",
            "620/620 - 1s - loss: 0.4578 - acc: 0.7935 - val_loss: 0.6977 - val_acc: 0.7000\n",
            "Epoch 17/300\n",
            "\n",
            "Epoch 00017: val_loss improved from 0.60829 to 0.58495, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.4663 - acc: 0.7839 - val_loss: 0.5850 - val_acc: 0.7100\n",
            "Epoch 18/300\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.58495\n",
            "620/620 - 1s - loss: 0.4593 - acc: 0.7887 - val_loss: 0.6869 - val_acc: 0.6900\n",
            "Epoch 19/300\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.58495\n",
            "620/620 - 1s - loss: 0.4595 - acc: 0.7694 - val_loss: 0.6776 - val_acc: 0.7000\n",
            "Epoch 20/300\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.58495\n",
            "620/620 - 1s - loss: 0.4690 - acc: 0.7726 - val_loss: 0.6791 - val_acc: 0.6800\n",
            "Epoch 21/300\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.58495\n",
            "620/620 - 1s - loss: 0.4553 - acc: 0.7887 - val_loss: 0.6533 - val_acc: 0.6900\n",
            "Epoch 22/300\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.58495\n",
            "620/620 - 1s - loss: 0.4385 - acc: 0.7887 - val_loss: 0.7087 - val_acc: 0.6800\n",
            "Epoch 23/300\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.58495\n",
            "620/620 - 1s - loss: 0.4579 - acc: 0.7823 - val_loss: 0.6241 - val_acc: 0.7100\n",
            "Epoch 24/300\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.58495\n",
            "620/620 - 1s - loss: 0.4172 - acc: 0.8161 - val_loss: 0.7372 - val_acc: 0.6800\n",
            "Epoch 25/300\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.58495\n",
            "620/620 - 1s - loss: 0.4419 - acc: 0.7952 - val_loss: 0.6787 - val_acc: 0.7000\n",
            "Epoch 26/300\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.58495\n",
            "620/620 - 1s - loss: 0.4424 - acc: 0.7823 - val_loss: 0.7417 - val_acc: 0.6500\n",
            "Epoch 27/300\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.58495\n",
            "620/620 - 1s - loss: 0.4278 - acc: 0.8113 - val_loss: 0.7326 - val_acc: 0.6800\n",
            "Epoch 28/300\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.58495\n",
            "620/620 - 1s - loss: 0.4572 - acc: 0.8048 - val_loss: 0.7232 - val_acc: 0.6500\n",
            "Epoch 29/300\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.58495\n",
            "620/620 - 1s - loss: 0.4458 - acc: 0.7806 - val_loss: 0.5915 - val_acc: 0.7300\n",
            "Epoch 30/300\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.58495\n",
            "620/620 - 1s - loss: 0.4284 - acc: 0.8048 - val_loss: 0.7679 - val_acc: 0.6800\n",
            "Epoch 31/300\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.58495\n",
            "620/620 - 1s - loss: 0.4301 - acc: 0.8161 - val_loss: 0.6782 - val_acc: 0.6800\n",
            "Epoch 32/300\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.58495\n",
            "620/620 - 1s - loss: 0.4389 - acc: 0.7871 - val_loss: 0.6494 - val_acc: 0.6900\n",
            "Epoch 33/300\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.58495\n",
            "620/620 - 1s - loss: 0.4323 - acc: 0.7984 - val_loss: 0.6261 - val_acc: 0.7000\n",
            "Epoch 34/300\n",
            "\n",
            "Epoch 00034: val_loss improved from 0.58495 to 0.58263, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.4363 - acc: 0.7887 - val_loss: 0.5826 - val_acc: 0.7300\n",
            "Epoch 35/300\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.58263\n",
            "620/620 - 1s - loss: 0.4205 - acc: 0.8226 - val_loss: 0.6026 - val_acc: 0.7300\n",
            "Epoch 36/300\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.58263\n",
            "620/620 - 1s - loss: 0.4186 - acc: 0.8048 - val_loss: 0.7680 - val_acc: 0.6200\n",
            "Epoch 37/300\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.58263\n",
            "620/620 - 1s - loss: 0.4062 - acc: 0.8065 - val_loss: 0.6419 - val_acc: 0.7100\n",
            "Epoch 38/300\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.58263\n",
            "620/620 - 1s - loss: 0.4285 - acc: 0.8161 - val_loss: 0.5858 - val_acc: 0.7500\n",
            "Epoch 39/300\n",
            "\n",
            "Epoch 00039: val_loss improved from 0.58263 to 0.58230, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.4177 - acc: 0.8194 - val_loss: 0.5823 - val_acc: 0.7000\n",
            "Epoch 40/300\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.58230\n",
            "620/620 - 1s - loss: 0.4246 - acc: 0.8129 - val_loss: 0.6949 - val_acc: 0.6600\n",
            "Epoch 41/300\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.58230\n",
            "620/620 - 1s - loss: 0.4202 - acc: 0.8113 - val_loss: 0.6898 - val_acc: 0.6900\n",
            "Epoch 42/300\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.58230\n",
            "620/620 - 1s - loss: 0.4099 - acc: 0.8065 - val_loss: 0.6106 - val_acc: 0.7500\n",
            "Epoch 43/300\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.58230\n",
            "620/620 - 1s - loss: 0.4102 - acc: 0.8081 - val_loss: 0.6190 - val_acc: 0.7300\n",
            "Epoch 44/300\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.58230\n",
            "620/620 - 1s - loss: 0.4047 - acc: 0.8242 - val_loss: 0.6330 - val_acc: 0.6900\n",
            "Epoch 45/300\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.58230\n",
            "620/620 - 1s - loss: 0.4226 - acc: 0.8081 - val_loss: 0.6249 - val_acc: 0.7100\n",
            "Epoch 46/300\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.58230\n",
            "620/620 - 1s - loss: 0.4215 - acc: 0.8290 - val_loss: 0.6740 - val_acc: 0.7100\n",
            "Epoch 47/300\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.58230\n",
            "620/620 - 1s - loss: 0.4037 - acc: 0.8210 - val_loss: 0.6592 - val_acc: 0.6900\n",
            "Epoch 48/300\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.58230\n",
            "620/620 - 1s - loss: 0.3977 - acc: 0.8435 - val_loss: 0.6312 - val_acc: 0.6900\n",
            "Epoch 49/300\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.58230\n",
            "620/620 - 1s - loss: 0.3984 - acc: 0.8339 - val_loss: 0.7131 - val_acc: 0.6700\n",
            "Epoch 50/300\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.58230\n",
            "620/620 - 1s - loss: 0.4110 - acc: 0.8258 - val_loss: 0.6585 - val_acc: 0.6800\n",
            "Epoch 51/300\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.58230\n",
            "620/620 - 1s - loss: 0.3940 - acc: 0.8161 - val_loss: 0.7010 - val_acc: 0.6500\n",
            "Epoch 52/300\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.58230\n",
            "620/620 - 1s - loss: 0.4167 - acc: 0.7984 - val_loss: 0.6722 - val_acc: 0.6900\n",
            "Epoch 53/300\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.58230\n",
            "620/620 - 1s - loss: 0.4021 - acc: 0.8032 - val_loss: 0.7516 - val_acc: 0.6500\n",
            "Epoch 54/300\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.58230\n",
            "620/620 - 1s - loss: 0.4137 - acc: 0.7935 - val_loss: 0.6515 - val_acc: 0.6900\n",
            "Epoch 55/300\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.58230\n",
            "620/620 - 1s - loss: 0.4022 - acc: 0.8081 - val_loss: 0.5916 - val_acc: 0.7500\n",
            "Epoch 56/300\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.58230\n",
            "620/620 - 1s - loss: 0.4005 - acc: 0.8161 - val_loss: 0.6632 - val_acc: 0.7200\n",
            "Epoch 57/300\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.58230\n",
            "620/620 - 1s - loss: 0.4058 - acc: 0.8065 - val_loss: 0.7036 - val_acc: 0.6800\n",
            "Epoch 58/300\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.58230\n",
            "620/620 - 1s - loss: 0.4056 - acc: 0.8129 - val_loss: 0.6687 - val_acc: 0.6800\n",
            "Epoch 59/300\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.58230\n",
            "620/620 - 1s - loss: 0.4037 - acc: 0.8097 - val_loss: 0.6416 - val_acc: 0.7000\n",
            "Epoch 60/300\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.58230\n",
            "620/620 - 1s - loss: 0.4018 - acc: 0.8177 - val_loss: 0.6376 - val_acc: 0.7300\n",
            "Epoch 61/300\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.58230\n",
            "620/620 - 1s - loss: 0.4114 - acc: 0.8129 - val_loss: 0.6159 - val_acc: 0.7500\n",
            "Epoch 62/300\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.58230\n",
            "620/620 - 1s - loss: 0.3943 - acc: 0.8194 - val_loss: 0.6966 - val_acc: 0.6900\n",
            "Epoch 63/300\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.58230\n",
            "620/620 - 1s - loss: 0.3931 - acc: 0.8161 - val_loss: 0.6718 - val_acc: 0.7100\n",
            "Epoch 64/300\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.58230\n",
            "620/620 - 1s - loss: 0.3966 - acc: 0.8290 - val_loss: 0.6943 - val_acc: 0.6700\n",
            "Epoch 65/300\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.58230\n",
            "620/620 - 1s - loss: 0.3833 - acc: 0.8323 - val_loss: 0.6382 - val_acc: 0.7000\n",
            "Epoch 66/300\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.58230\n",
            "620/620 - 1s - loss: 0.3859 - acc: 0.8161 - val_loss: 0.6127 - val_acc: 0.7200\n",
            "Epoch 67/300\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.58230\n",
            "620/620 - 1s - loss: 0.4119 - acc: 0.8032 - val_loss: 0.6221 - val_acc: 0.6800\n",
            "Epoch 68/300\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.58230\n",
            "620/620 - 1s - loss: 0.4268 - acc: 0.8065 - val_loss: 0.6474 - val_acc: 0.6900\n",
            "Epoch 69/300\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.58230\n",
            "620/620 - 1s - loss: 0.3926 - acc: 0.8210 - val_loss: 0.6474 - val_acc: 0.7100\n",
            "Epoch 70/300\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.58230\n",
            "620/620 - 1s - loss: 0.3984 - acc: 0.8371 - val_loss: 0.6740 - val_acc: 0.7000\n",
            "Epoch 71/300\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.58230\n",
            "620/620 - 1s - loss: 0.4027 - acc: 0.8048 - val_loss: 0.6013 - val_acc: 0.7300\n",
            "Epoch 72/300\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.58230\n",
            "620/620 - 1s - loss: 0.3883 - acc: 0.8194 - val_loss: 0.6573 - val_acc: 0.6900\n",
            "Epoch 73/300\n",
            "\n",
            "Epoch 00073: val_loss improved from 0.58230 to 0.58183, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.3987 - acc: 0.8129 - val_loss: 0.5818 - val_acc: 0.7200\n",
            "Epoch 74/300\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.58183\n",
            "620/620 - 1s - loss: 0.4091 - acc: 0.7919 - val_loss: 0.6772 - val_acc: 0.6600\n",
            "Epoch 75/300\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.58183\n",
            "620/620 - 1s - loss: 0.3783 - acc: 0.8323 - val_loss: 0.7363 - val_acc: 0.6800\n",
            "Epoch 76/300\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.58183\n",
            "620/620 - 1s - loss: 0.3764 - acc: 0.8371 - val_loss: 0.7211 - val_acc: 0.6900\n",
            "Epoch 77/300\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.58183\n",
            "620/620 - 1s - loss: 0.3954 - acc: 0.8161 - val_loss: 0.6378 - val_acc: 0.6900\n",
            "Epoch 78/300\n",
            "\n",
            "Epoch 00078: val_loss improved from 0.58183 to 0.57226, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.3793 - acc: 0.8355 - val_loss: 0.5723 - val_acc: 0.7400\n",
            "Epoch 79/300\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.57226\n",
            "620/620 - 1s - loss: 0.3684 - acc: 0.8371 - val_loss: 0.6025 - val_acc: 0.7400\n",
            "Epoch 80/300\n",
            "\n",
            "Epoch 00080: val_loss improved from 0.57226 to 0.56624, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.3810 - acc: 0.8226 - val_loss: 0.5662 - val_acc: 0.7300\n",
            "Epoch 81/300\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.56624\n",
            "620/620 - 1s - loss: 0.3792 - acc: 0.8194 - val_loss: 0.6665 - val_acc: 0.6900\n",
            "Epoch 82/300\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.56624\n",
            "620/620 - 1s - loss: 0.3835 - acc: 0.8274 - val_loss: 0.6249 - val_acc: 0.7200\n",
            "Epoch 83/300\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.56624\n",
            "620/620 - 1s - loss: 0.3751 - acc: 0.8258 - val_loss: 0.6131 - val_acc: 0.7500\n",
            "Epoch 84/300\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.56624\n",
            "620/620 - 1s - loss: 0.3809 - acc: 0.8339 - val_loss: 0.5704 - val_acc: 0.7300\n",
            "Epoch 85/300\n",
            "\n",
            "Epoch 00085: val_loss improved from 0.56624 to 0.52688, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.3804 - acc: 0.8258 - val_loss: 0.5269 - val_acc: 0.7300\n",
            "Epoch 86/300\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.4044 - acc: 0.8177 - val_loss: 0.6100 - val_acc: 0.7300\n",
            "Epoch 87/300\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3838 - acc: 0.8226 - val_loss: 0.6694 - val_acc: 0.7000\n",
            "Epoch 88/300\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3762 - acc: 0.8419 - val_loss: 0.6182 - val_acc: 0.7400\n",
            "Epoch 89/300\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3725 - acc: 0.8339 - val_loss: 0.5373 - val_acc: 0.7700\n",
            "Epoch 90/300\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3901 - acc: 0.8210 - val_loss: 0.6400 - val_acc: 0.7000\n",
            "Epoch 91/300\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3703 - acc: 0.8323 - val_loss: 0.6400 - val_acc: 0.7100\n",
            "Epoch 92/300\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3794 - acc: 0.8500 - val_loss: 0.7088 - val_acc: 0.6900\n",
            "Epoch 93/300\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3513 - acc: 0.8484 - val_loss: 0.7191 - val_acc: 0.7100\n",
            "Epoch 94/300\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3852 - acc: 0.8306 - val_loss: 0.6596 - val_acc: 0.7000\n",
            "Epoch 95/300\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3914 - acc: 0.8242 - val_loss: 0.7384 - val_acc: 0.6400\n",
            "Epoch 96/300\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3891 - acc: 0.8290 - val_loss: 0.6408 - val_acc: 0.7000\n",
            "Epoch 97/300\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3861 - acc: 0.8323 - val_loss: 0.7092 - val_acc: 0.6800\n",
            "Epoch 98/300\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3697 - acc: 0.8274 - val_loss: 0.7483 - val_acc: 0.6900\n",
            "Epoch 99/300\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3736 - acc: 0.8403 - val_loss: 0.6429 - val_acc: 0.7600\n",
            "Epoch 100/300\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3765 - acc: 0.8403 - val_loss: 0.6145 - val_acc: 0.7500\n",
            "Epoch 101/300\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3876 - acc: 0.8355 - val_loss: 0.7531 - val_acc: 0.6900\n",
            "Epoch 102/300\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3622 - acc: 0.8323 - val_loss: 0.6993 - val_acc: 0.6900\n",
            "Epoch 103/300\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3947 - acc: 0.8242 - val_loss: 0.6413 - val_acc: 0.7000\n",
            "Epoch 104/300\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3352 - acc: 0.8613 - val_loss: 0.5761 - val_acc: 0.7600\n",
            "Epoch 105/300\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3503 - acc: 0.8516 - val_loss: 0.6136 - val_acc: 0.7300\n",
            "Epoch 106/300\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3682 - acc: 0.8387 - val_loss: 0.6808 - val_acc: 0.7200\n",
            "Epoch 107/300\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3672 - acc: 0.8339 - val_loss: 0.6800 - val_acc: 0.7300\n",
            "Epoch 108/300\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3923 - acc: 0.8081 - val_loss: 0.6362 - val_acc: 0.7200\n",
            "Epoch 109/300\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3809 - acc: 0.8355 - val_loss: 0.5629 - val_acc: 0.7700\n",
            "Epoch 110/300\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3835 - acc: 0.8161 - val_loss: 0.6565 - val_acc: 0.7100\n",
            "Epoch 111/300\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3739 - acc: 0.8290 - val_loss: 0.7207 - val_acc: 0.6900\n",
            "Epoch 112/300\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3628 - acc: 0.8306 - val_loss: 0.6839 - val_acc: 0.7000\n",
            "Epoch 113/300\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3656 - acc: 0.8419 - val_loss: 0.6646 - val_acc: 0.7300\n",
            "Epoch 114/300\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3601 - acc: 0.8661 - val_loss: 0.6627 - val_acc: 0.7300\n",
            "Epoch 115/300\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3638 - acc: 0.8387 - val_loss: 0.5854 - val_acc: 0.7700\n",
            "Epoch 116/300\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3897 - acc: 0.8242 - val_loss: 0.7364 - val_acc: 0.6800\n",
            "Epoch 117/300\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3556 - acc: 0.8548 - val_loss: 0.6665 - val_acc: 0.7400\n",
            "Epoch 118/300\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3614 - acc: 0.8323 - val_loss: 0.6495 - val_acc: 0.7300\n",
            "Epoch 119/300\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3551 - acc: 0.8468 - val_loss: 0.6567 - val_acc: 0.7200\n",
            "Epoch 120/300\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3688 - acc: 0.8290 - val_loss: 0.6215 - val_acc: 0.6900\n",
            "Epoch 121/300\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3792 - acc: 0.8387 - val_loss: 0.7293 - val_acc: 0.6600\n",
            "Epoch 122/300\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3538 - acc: 0.8629 - val_loss: 0.7221 - val_acc: 0.7200\n",
            "Epoch 123/300\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3712 - acc: 0.8339 - val_loss: 0.6845 - val_acc: 0.7400\n",
            "Epoch 124/300\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3566 - acc: 0.8452 - val_loss: 0.6552 - val_acc: 0.7400\n",
            "Epoch 125/300\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3683 - acc: 0.8323 - val_loss: 0.6487 - val_acc: 0.7200\n",
            "Epoch 126/300\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3893 - acc: 0.8081 - val_loss: 0.5951 - val_acc: 0.7500\n",
            "Epoch 127/300\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3954 - acc: 0.8210 - val_loss: 0.6504 - val_acc: 0.7200\n",
            "Epoch 128/300\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3627 - acc: 0.8355 - val_loss: 0.7096 - val_acc: 0.6800\n",
            "Epoch 129/300\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3545 - acc: 0.8355 - val_loss: 0.6218 - val_acc: 0.7300\n",
            "Epoch 130/300\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3352 - acc: 0.8613 - val_loss: 0.8089 - val_acc: 0.6600\n",
            "Epoch 131/300\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3455 - acc: 0.8597 - val_loss: 0.6864 - val_acc: 0.7200\n",
            "Epoch 132/300\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3480 - acc: 0.8548 - val_loss: 0.7297 - val_acc: 0.7200\n",
            "Epoch 133/300\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3658 - acc: 0.8516 - val_loss: 0.7570 - val_acc: 0.6900\n",
            "Epoch 134/300\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3641 - acc: 0.8419 - val_loss: 0.6758 - val_acc: 0.6800\n",
            "Epoch 135/300\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3501 - acc: 0.8484 - val_loss: 0.7934 - val_acc: 0.6800\n",
            "Epoch 136/300\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3583 - acc: 0.8403 - val_loss: 0.7983 - val_acc: 0.6800\n",
            "Epoch 137/300\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3503 - acc: 0.8484 - val_loss: 0.6796 - val_acc: 0.7300\n",
            "Epoch 138/300\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3694 - acc: 0.8274 - val_loss: 0.7179 - val_acc: 0.7000\n",
            "Epoch 139/300\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3890 - acc: 0.8323 - val_loss: 0.6404 - val_acc: 0.7200\n",
            "Epoch 140/300\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3779 - acc: 0.8500 - val_loss: 0.7001 - val_acc: 0.7000\n",
            "Epoch 141/300\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3557 - acc: 0.8371 - val_loss: 0.6716 - val_acc: 0.7500\n",
            "Epoch 142/300\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3958 - acc: 0.8016 - val_loss: 0.6268 - val_acc: 0.7600\n",
            "Epoch 143/300\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3362 - acc: 0.8694 - val_loss: 0.8181 - val_acc: 0.7100\n",
            "Epoch 144/300\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3634 - acc: 0.8290 - val_loss: 0.7108 - val_acc: 0.7300\n",
            "Epoch 145/300\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3575 - acc: 0.8484 - val_loss: 0.7477 - val_acc: 0.7400\n",
            "Epoch 146/300\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3853 - acc: 0.8323 - val_loss: 0.7167 - val_acc: 0.7400\n",
            "Epoch 147/300\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3666 - acc: 0.8484 - val_loss: 0.7438 - val_acc: 0.7100\n",
            "Epoch 148/300\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3437 - acc: 0.8468 - val_loss: 0.6461 - val_acc: 0.7500\n",
            "Epoch 149/300\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3488 - acc: 0.8371 - val_loss: 0.8321 - val_acc: 0.7000\n",
            "Epoch 150/300\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3441 - acc: 0.8532 - val_loss: 0.8712 - val_acc: 0.7000\n",
            "Epoch 151/300\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3663 - acc: 0.8371 - val_loss: 0.8067 - val_acc: 0.6700\n",
            "Epoch 152/300\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3400 - acc: 0.8435 - val_loss: 0.7406 - val_acc: 0.7100\n",
            "Epoch 153/300\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3874 - acc: 0.8194 - val_loss: 0.6962 - val_acc: 0.7200\n",
            "Epoch 154/300\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3657 - acc: 0.8435 - val_loss: 0.7075 - val_acc: 0.6900\n",
            "Epoch 155/300\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3590 - acc: 0.8371 - val_loss: 0.6611 - val_acc: 0.7500\n",
            "Epoch 156/300\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3711 - acc: 0.8419 - val_loss: 0.7459 - val_acc: 0.6900\n",
            "Epoch 157/300\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3618 - acc: 0.8355 - val_loss: 0.7259 - val_acc: 0.7200\n",
            "Epoch 158/300\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3621 - acc: 0.8355 - val_loss: 0.7496 - val_acc: 0.7100\n",
            "Epoch 159/300\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3562 - acc: 0.8484 - val_loss: 0.7771 - val_acc: 0.7100\n",
            "Epoch 160/300\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3556 - acc: 0.8290 - val_loss: 0.8176 - val_acc: 0.7300\n",
            "Epoch 161/300\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3573 - acc: 0.8452 - val_loss: 0.6135 - val_acc: 0.7400\n",
            "Epoch 162/300\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3447 - acc: 0.8645 - val_loss: 0.6419 - val_acc: 0.7600\n",
            "Epoch 163/300\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3256 - acc: 0.8629 - val_loss: 0.7148 - val_acc: 0.7100\n",
            "Epoch 164/300\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3764 - acc: 0.8242 - val_loss: 0.7437 - val_acc: 0.6900\n",
            "Epoch 165/300\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3596 - acc: 0.8355 - val_loss: 0.7544 - val_acc: 0.7300\n",
            "Epoch 166/300\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3707 - acc: 0.8387 - val_loss: 0.8886 - val_acc: 0.6600\n",
            "Epoch 167/300\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3661 - acc: 0.8419 - val_loss: 0.8591 - val_acc: 0.6600\n",
            "Epoch 168/300\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3517 - acc: 0.8403 - val_loss: 0.7478 - val_acc: 0.6900\n",
            "Epoch 169/300\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3634 - acc: 0.8500 - val_loss: 0.7596 - val_acc: 0.6900\n",
            "Epoch 170/300\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3591 - acc: 0.8435 - val_loss: 0.7376 - val_acc: 0.7000\n",
            "Epoch 171/300\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3742 - acc: 0.8419 - val_loss: 0.6212 - val_acc: 0.7000\n",
            "Epoch 172/300\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3771 - acc: 0.8194 - val_loss: 0.7096 - val_acc: 0.7400\n",
            "Epoch 173/300\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3478 - acc: 0.8516 - val_loss: 0.8388 - val_acc: 0.7000\n",
            "Epoch 174/300\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3441 - acc: 0.8435 - val_loss: 0.7460 - val_acc: 0.7400\n",
            "Epoch 175/300\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3401 - acc: 0.8484 - val_loss: 0.6709 - val_acc: 0.7600\n",
            "Epoch 176/300\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3891 - acc: 0.8242 - val_loss: 0.7462 - val_acc: 0.7000\n",
            "Epoch 177/300\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3650 - acc: 0.8484 - val_loss: 0.8088 - val_acc: 0.6800\n",
            "Epoch 178/300\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3360 - acc: 0.8597 - val_loss: 0.7177 - val_acc: 0.7400\n",
            "Epoch 179/300\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3283 - acc: 0.8516 - val_loss: 0.7262 - val_acc: 0.7500\n",
            "Epoch 180/300\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3282 - acc: 0.8581 - val_loss: 0.6773 - val_acc: 0.7300\n",
            "Epoch 181/300\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3635 - acc: 0.8306 - val_loss: 0.6885 - val_acc: 0.7500\n",
            "Epoch 182/300\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3607 - acc: 0.8468 - val_loss: 0.7667 - val_acc: 0.7400\n",
            "Epoch 183/300\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3559 - acc: 0.8306 - val_loss: 0.7586 - val_acc: 0.7000\n",
            "Epoch 184/300\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3597 - acc: 0.8419 - val_loss: 0.7540 - val_acc: 0.7200\n",
            "Epoch 185/300\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3808 - acc: 0.8403 - val_loss: 0.7999 - val_acc: 0.7000\n",
            "Epoch 186/300\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3397 - acc: 0.8452 - val_loss: 0.7945 - val_acc: 0.7000\n",
            "Epoch 187/300\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3337 - acc: 0.8565 - val_loss: 0.8978 - val_acc: 0.6600\n",
            "Epoch 188/300\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3442 - acc: 0.8581 - val_loss: 0.8276 - val_acc: 0.6900\n",
            "Epoch 189/300\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3364 - acc: 0.8500 - val_loss: 0.7424 - val_acc: 0.7200\n",
            "Epoch 190/300\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3687 - acc: 0.8452 - val_loss: 0.7780 - val_acc: 0.7200\n",
            "Epoch 191/300\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3567 - acc: 0.8371 - val_loss: 0.7911 - val_acc: 0.6900\n",
            "Epoch 192/300\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3583 - acc: 0.8387 - val_loss: 0.8513 - val_acc: 0.6900\n",
            "Epoch 193/300\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3461 - acc: 0.8484 - val_loss: 0.6913 - val_acc: 0.7500\n",
            "Epoch 194/300\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3325 - acc: 0.8484 - val_loss: 0.8472 - val_acc: 0.6800\n",
            "Epoch 195/300\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3475 - acc: 0.8548 - val_loss: 0.7359 - val_acc: 0.7100\n",
            "Epoch 196/300\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3368 - acc: 0.8532 - val_loss: 0.7835 - val_acc: 0.7100\n",
            "Epoch 197/300\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3595 - acc: 0.8484 - val_loss: 0.6989 - val_acc: 0.7300\n",
            "Epoch 198/300\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3558 - acc: 0.8613 - val_loss: 0.7529 - val_acc: 0.7200\n",
            "Epoch 199/300\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3440 - acc: 0.8565 - val_loss: 0.7133 - val_acc: 0.7800\n",
            "Epoch 200/300\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3772 - acc: 0.8274 - val_loss: 0.7881 - val_acc: 0.7300\n",
            "Epoch 201/300\n",
            "\n",
            "Epoch 00201: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3196 - acc: 0.8597 - val_loss: 0.8019 - val_acc: 0.7300\n",
            "Epoch 202/300\n",
            "\n",
            "Epoch 00202: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3644 - acc: 0.8548 - val_loss: 0.8842 - val_acc: 0.6600\n",
            "Epoch 203/300\n",
            "\n",
            "Epoch 00203: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3621 - acc: 0.8339 - val_loss: 0.8398 - val_acc: 0.6500\n",
            "Epoch 204/300\n",
            "\n",
            "Epoch 00204: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3576 - acc: 0.8581 - val_loss: 0.7411 - val_acc: 0.7600\n",
            "Epoch 205/300\n",
            "\n",
            "Epoch 00205: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3483 - acc: 0.8452 - val_loss: 0.6476 - val_acc: 0.7500\n",
            "Epoch 206/300\n",
            "\n",
            "Epoch 00206: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3251 - acc: 0.8661 - val_loss: 0.6376 - val_acc: 0.7600\n",
            "Epoch 207/300\n",
            "\n",
            "Epoch 00207: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3372 - acc: 0.8565 - val_loss: 0.6843 - val_acc: 0.7400\n",
            "Epoch 208/300\n",
            "\n",
            "Epoch 00208: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3599 - acc: 0.8403 - val_loss: 0.7042 - val_acc: 0.7200\n",
            "Epoch 209/300\n",
            "\n",
            "Epoch 00209: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3456 - acc: 0.8548 - val_loss: 0.6955 - val_acc: 0.7300\n",
            "Epoch 210/300\n",
            "\n",
            "Epoch 00210: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3286 - acc: 0.8629 - val_loss: 0.7266 - val_acc: 0.7400\n",
            "Epoch 211/300\n",
            "\n",
            "Epoch 00211: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3146 - acc: 0.8565 - val_loss: 0.9282 - val_acc: 0.6700\n",
            "Epoch 212/300\n",
            "\n",
            "Epoch 00212: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3289 - acc: 0.8581 - val_loss: 0.7623 - val_acc: 0.7300\n",
            "Epoch 213/300\n",
            "\n",
            "Epoch 00213: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3433 - acc: 0.8403 - val_loss: 0.6877 - val_acc: 0.7300\n",
            "Epoch 214/300\n",
            "\n",
            "Epoch 00214: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3369 - acc: 0.8500 - val_loss: 0.7849 - val_acc: 0.6700\n",
            "Epoch 215/300\n",
            "\n",
            "Epoch 00215: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3568 - acc: 0.8452 - val_loss: 0.6741 - val_acc: 0.7300\n",
            "Epoch 216/300\n",
            "\n",
            "Epoch 00216: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3156 - acc: 0.8629 - val_loss: 0.7089 - val_acc: 0.7100\n",
            "Epoch 217/300\n",
            "\n",
            "Epoch 00217: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3327 - acc: 0.8629 - val_loss: 0.7580 - val_acc: 0.6900\n",
            "Epoch 218/300\n",
            "\n",
            "Epoch 00218: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3515 - acc: 0.8452 - val_loss: 0.9416 - val_acc: 0.6500\n",
            "Epoch 219/300\n",
            "\n",
            "Epoch 00219: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3433 - acc: 0.8516 - val_loss: 0.8050 - val_acc: 0.7200\n",
            "Epoch 220/300\n",
            "\n",
            "Epoch 00220: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3568 - acc: 0.8371 - val_loss: 0.8350 - val_acc: 0.7200\n",
            "Epoch 221/300\n",
            "\n",
            "Epoch 00221: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3496 - acc: 0.8435 - val_loss: 0.8709 - val_acc: 0.6600\n",
            "Epoch 222/300\n",
            "\n",
            "Epoch 00222: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3476 - acc: 0.8468 - val_loss: 0.7087 - val_acc: 0.7300\n",
            "Epoch 223/300\n",
            "\n",
            "Epoch 00223: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3535 - acc: 0.8452 - val_loss: 0.7332 - val_acc: 0.7100\n",
            "Epoch 224/300\n",
            "\n",
            "Epoch 00224: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3441 - acc: 0.8565 - val_loss: 0.7072 - val_acc: 0.7100\n",
            "Epoch 225/300\n",
            "\n",
            "Epoch 00225: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3493 - acc: 0.8387 - val_loss: 0.7729 - val_acc: 0.7000\n",
            "Epoch 226/300\n",
            "\n",
            "Epoch 00226: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3451 - acc: 0.8597 - val_loss: 0.6216 - val_acc: 0.7700\n",
            "Epoch 227/300\n",
            "\n",
            "Epoch 00227: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3450 - acc: 0.8500 - val_loss: 0.7653 - val_acc: 0.7000\n",
            "Epoch 228/300\n",
            "\n",
            "Epoch 00228: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3170 - acc: 0.8597 - val_loss: 0.8370 - val_acc: 0.7200\n",
            "Epoch 229/300\n",
            "\n",
            "Epoch 00229: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3647 - acc: 0.8387 - val_loss: 0.7793 - val_acc: 0.7500\n",
            "Epoch 230/300\n",
            "\n",
            "Epoch 00230: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3376 - acc: 0.8726 - val_loss: 0.8327 - val_acc: 0.7200\n",
            "Epoch 231/300\n",
            "\n",
            "Epoch 00231: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3571 - acc: 0.8435 - val_loss: 0.7586 - val_acc: 0.7200\n",
            "Epoch 232/300\n",
            "\n",
            "Epoch 00232: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3457 - acc: 0.8403 - val_loss: 0.7848 - val_acc: 0.6700\n",
            "Epoch 233/300\n",
            "\n",
            "Epoch 00233: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3211 - acc: 0.8613 - val_loss: 0.7432 - val_acc: 0.7200\n",
            "Epoch 234/300\n",
            "\n",
            "Epoch 00234: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3367 - acc: 0.8661 - val_loss: 0.7698 - val_acc: 0.7200\n",
            "Epoch 235/300\n",
            "\n",
            "Epoch 00235: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3172 - acc: 0.8726 - val_loss: 0.7856 - val_acc: 0.6900\n",
            "Epoch 236/300\n",
            "\n",
            "Epoch 00236: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3212 - acc: 0.8629 - val_loss: 0.7015 - val_acc: 0.7000\n",
            "Epoch 237/300\n",
            "\n",
            "Epoch 00237: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3227 - acc: 0.8710 - val_loss: 0.7231 - val_acc: 0.7300\n",
            "Epoch 238/300\n",
            "\n",
            "Epoch 00238: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3515 - acc: 0.8419 - val_loss: 0.9013 - val_acc: 0.6500\n",
            "Epoch 239/300\n",
            "\n",
            "Epoch 00239: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3595 - acc: 0.8403 - val_loss: 0.7480 - val_acc: 0.7000\n",
            "Epoch 240/300\n",
            "\n",
            "Epoch 00240: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3323 - acc: 0.8629 - val_loss: 0.8787 - val_acc: 0.6500\n",
            "Epoch 241/300\n",
            "\n",
            "Epoch 00241: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3555 - acc: 0.8500 - val_loss: 0.7200 - val_acc: 0.7000\n",
            "Epoch 242/300\n",
            "\n",
            "Epoch 00242: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3515 - acc: 0.8452 - val_loss: 0.8285 - val_acc: 0.6800\n",
            "Epoch 243/300\n",
            "\n",
            "Epoch 00243: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3252 - acc: 0.8500 - val_loss: 0.8779 - val_acc: 0.6800\n",
            "Epoch 244/300\n",
            "\n",
            "Epoch 00244: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3468 - acc: 0.8548 - val_loss: 0.9449 - val_acc: 0.6500\n",
            "Epoch 245/300\n",
            "\n",
            "Epoch 00245: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3215 - acc: 0.8532 - val_loss: 0.8510 - val_acc: 0.7000\n",
            "Epoch 246/300\n",
            "\n",
            "Epoch 00246: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3296 - acc: 0.8629 - val_loss: 0.7567 - val_acc: 0.7300\n",
            "Epoch 247/300\n",
            "\n",
            "Epoch 00247: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3389 - acc: 0.8532 - val_loss: 0.8468 - val_acc: 0.6700\n",
            "Epoch 248/300\n",
            "\n",
            "Epoch 00248: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3255 - acc: 0.8548 - val_loss: 0.7340 - val_acc: 0.7300\n",
            "Epoch 249/300\n",
            "\n",
            "Epoch 00249: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3432 - acc: 0.8629 - val_loss: 0.9170 - val_acc: 0.6600\n",
            "Epoch 250/300\n",
            "\n",
            "Epoch 00250: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3215 - acc: 0.8565 - val_loss: 0.7175 - val_acc: 0.7500\n",
            "Epoch 251/300\n",
            "\n",
            "Epoch 00251: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3280 - acc: 0.8419 - val_loss: 0.7402 - val_acc: 0.7400\n",
            "Epoch 252/300\n",
            "\n",
            "Epoch 00252: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3213 - acc: 0.8661 - val_loss: 0.7792 - val_acc: 0.7100\n",
            "Epoch 253/300\n",
            "\n",
            "Epoch 00253: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3253 - acc: 0.8452 - val_loss: 0.7934 - val_acc: 0.6800\n",
            "Epoch 254/300\n",
            "\n",
            "Epoch 00254: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3467 - acc: 0.8452 - val_loss: 0.6642 - val_acc: 0.7500\n",
            "Epoch 255/300\n",
            "\n",
            "Epoch 00255: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3345 - acc: 0.8597 - val_loss: 0.6466 - val_acc: 0.7300\n",
            "Epoch 256/300\n",
            "\n",
            "Epoch 00256: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3262 - acc: 0.8565 - val_loss: 0.8202 - val_acc: 0.7000\n",
            "Epoch 257/300\n",
            "\n",
            "Epoch 00257: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3127 - acc: 0.8726 - val_loss: 0.9251 - val_acc: 0.6600\n",
            "Epoch 258/300\n",
            "\n",
            "Epoch 00258: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3036 - acc: 0.8694 - val_loss: 0.9426 - val_acc: 0.6600\n",
            "Epoch 259/300\n",
            "\n",
            "Epoch 00259: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3266 - acc: 0.8581 - val_loss: 0.7344 - val_acc: 0.7300\n",
            "Epoch 260/300\n",
            "\n",
            "Epoch 00260: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3571 - acc: 0.8435 - val_loss: 0.7507 - val_acc: 0.7500\n",
            "Epoch 261/300\n",
            "\n",
            "Epoch 00261: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3070 - acc: 0.8661 - val_loss: 0.8993 - val_acc: 0.6700\n",
            "Epoch 262/300\n",
            "\n",
            "Epoch 00262: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3340 - acc: 0.8484 - val_loss: 0.8949 - val_acc: 0.7000\n",
            "Epoch 263/300\n",
            "\n",
            "Epoch 00263: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3241 - acc: 0.8613 - val_loss: 0.8179 - val_acc: 0.7300\n",
            "Epoch 264/300\n",
            "\n",
            "Epoch 00264: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3035 - acc: 0.8806 - val_loss: 0.8356 - val_acc: 0.7200\n",
            "Epoch 265/300\n",
            "\n",
            "Epoch 00265: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3527 - acc: 0.8387 - val_loss: 0.7320 - val_acc: 0.7200\n",
            "Epoch 266/300\n",
            "\n",
            "Epoch 00266: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3393 - acc: 0.8613 - val_loss: 0.7677 - val_acc: 0.7200\n",
            "Epoch 267/300\n",
            "\n",
            "Epoch 00267: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3290 - acc: 0.8613 - val_loss: 0.6898 - val_acc: 0.7400\n",
            "Epoch 268/300\n",
            "\n",
            "Epoch 00268: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3300 - acc: 0.8500 - val_loss: 0.7565 - val_acc: 0.7100\n",
            "Epoch 269/300\n",
            "\n",
            "Epoch 00269: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3453 - acc: 0.8339 - val_loss: 0.8394 - val_acc: 0.6900\n",
            "Epoch 270/300\n",
            "\n",
            "Epoch 00270: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3215 - acc: 0.8661 - val_loss: 0.9194 - val_acc: 0.6400\n",
            "Epoch 271/300\n",
            "\n",
            "Epoch 00271: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3254 - acc: 0.8661 - val_loss: 0.8326 - val_acc: 0.7300\n",
            "Epoch 272/300\n",
            "\n",
            "Epoch 00272: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3433 - acc: 0.8387 - val_loss: 0.7567 - val_acc: 0.7400\n",
            "Epoch 273/300\n",
            "\n",
            "Epoch 00273: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3163 - acc: 0.8661 - val_loss: 0.8032 - val_acc: 0.7100\n",
            "Epoch 274/300\n",
            "\n",
            "Epoch 00274: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3298 - acc: 0.8532 - val_loss: 0.6526 - val_acc: 0.7500\n",
            "Epoch 275/300\n",
            "\n",
            "Epoch 00275: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3479 - acc: 0.8468 - val_loss: 0.6036 - val_acc: 0.7200\n",
            "Epoch 276/300\n",
            "\n",
            "Epoch 00276: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3607 - acc: 0.8468 - val_loss: 0.8414 - val_acc: 0.7200\n",
            "Epoch 277/300\n",
            "\n",
            "Epoch 00277: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3489 - acc: 0.8484 - val_loss: 0.8185 - val_acc: 0.6900\n",
            "Epoch 278/300\n",
            "\n",
            "Epoch 00278: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3082 - acc: 0.8710 - val_loss: 0.8785 - val_acc: 0.6800\n",
            "Epoch 279/300\n",
            "\n",
            "Epoch 00279: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3443 - acc: 0.8484 - val_loss: 0.8073 - val_acc: 0.6900\n",
            "Epoch 280/300\n",
            "\n",
            "Epoch 00280: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3173 - acc: 0.8645 - val_loss: 0.7219 - val_acc: 0.7400\n",
            "Epoch 281/300\n",
            "\n",
            "Epoch 00281: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3208 - acc: 0.8581 - val_loss: 0.7508 - val_acc: 0.7500\n",
            "Epoch 282/300\n",
            "\n",
            "Epoch 00282: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3751 - acc: 0.8258 - val_loss: 0.6545 - val_acc: 0.7400\n",
            "Epoch 283/300\n",
            "\n",
            "Epoch 00283: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3135 - acc: 0.8839 - val_loss: 0.8562 - val_acc: 0.7400\n",
            "Epoch 284/300\n",
            "\n",
            "Epoch 00284: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3170 - acc: 0.8500 - val_loss: 0.7288 - val_acc: 0.7400\n",
            "Epoch 285/300\n",
            "\n",
            "Epoch 00285: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3262 - acc: 0.8484 - val_loss: 0.9604 - val_acc: 0.7000\n",
            "Epoch 286/300\n",
            "\n",
            "Epoch 00286: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3543 - acc: 0.8355 - val_loss: 0.8048 - val_acc: 0.7200\n",
            "Epoch 287/300\n",
            "\n",
            "Epoch 00287: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3356 - acc: 0.8484 - val_loss: 0.7978 - val_acc: 0.7200\n",
            "Epoch 288/300\n",
            "\n",
            "Epoch 00288: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3171 - acc: 0.8677 - val_loss: 0.6553 - val_acc: 0.7100\n",
            "Epoch 289/300\n",
            "\n",
            "Epoch 00289: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3512 - acc: 0.8484 - val_loss: 0.8354 - val_acc: 0.6900\n",
            "Epoch 290/300\n",
            "\n",
            "Epoch 00290: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3578 - acc: 0.8419 - val_loss: 0.7865 - val_acc: 0.7200\n",
            "Epoch 291/300\n",
            "\n",
            "Epoch 00291: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3255 - acc: 0.8710 - val_loss: 0.6925 - val_acc: 0.7300\n",
            "Epoch 292/300\n",
            "\n",
            "Epoch 00292: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3247 - acc: 0.8694 - val_loss: 0.8603 - val_acc: 0.7000\n",
            "Epoch 293/300\n",
            "\n",
            "Epoch 00293: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3430 - acc: 0.8484 - val_loss: 0.6715 - val_acc: 0.7400\n",
            "Epoch 294/300\n",
            "\n",
            "Epoch 00294: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.2907 - acc: 0.8806 - val_loss: 0.9068 - val_acc: 0.6800\n",
            "Epoch 295/300\n",
            "\n",
            "Epoch 00295: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3168 - acc: 0.8484 - val_loss: 0.9146 - val_acc: 0.6300\n",
            "Epoch 296/300\n",
            "\n",
            "Epoch 00296: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3174 - acc: 0.8613 - val_loss: 0.8009 - val_acc: 0.7000\n",
            "Epoch 297/300\n",
            "\n",
            "Epoch 00297: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3081 - acc: 0.8758 - val_loss: 0.8436 - val_acc: 0.7100\n",
            "Epoch 298/300\n",
            "\n",
            "Epoch 00298: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3444 - acc: 0.8435 - val_loss: 0.7763 - val_acc: 0.7700\n",
            "Epoch 299/300\n",
            "\n",
            "Epoch 00299: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3612 - acc: 0.8468 - val_loss: 0.6722 - val_acc: 0.7500\n",
            "Epoch 300/300\n",
            "\n",
            "Epoch 00300: val_loss did not improve from 0.52688\n",
            "620/620 - 1s - loss: 0.3373 - acc: 0.8565 - val_loss: 0.7161 - val_acc: 0.7300\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "# Evaluate on test data\n",
            "40/40 [==============================] - 0s 3ms/sample - loss: 0.8812 - acc: 0.5750\n",
            "test loss, test acc: [0.8812487988510839, 0.575]\n",
            "1\n",
            "EEG_Deep/Data2A/parsed_P01T.mat\n",
            "(0, 12, 4096)\n",
            "(80, 12, 4096)\n",
            "(80, 1)\n",
            "2\n",
            "EEG_Deep/Data2A/parsed_P02T.mat\n",
            "(80, 12, 4096)\n",
            "(160, 12, 4096)\n",
            "(160, 1)\n",
            "3\n",
            "EEG_Deep/Data2A/parsed_P03T.mat\n",
            "(160, 12, 4096)\n",
            "(240, 12, 4096)\n",
            "(240, 1)\n",
            "4\n",
            "EEG_Deep/Data2A/parsed_P04T.mat\n",
            "(240, 12, 4096)\n",
            "(320, 12, 4096)\n",
            "(320, 1)\n",
            "5\n",
            "EEG_Deep/Data2A/parsed_P05T.mat\n",
            "(320, 12, 4096)\n",
            "(400, 12, 4096)\n",
            "(400, 1)\n",
            "6\n",
            "EEG_Deep/Data2A/parsed_P06T.mat\n",
            "(400, 12, 4096)\n",
            "(480, 12, 4096)\n",
            "(480, 1)\n",
            "7\n",
            "EEG_Deep/Data2A/parsed_P07T.mat\n",
            "(480, 12, 4096)\n",
            "(560, 12, 4096)\n",
            "(560, 1)\n",
            "8\n",
            "EEG_Deep/Data2A/parsed_P08T.mat\n",
            "(560, 12, 4096)\n",
            "(640, 12, 4096)\n",
            "(640, 1)\n",
            "10\n",
            "EEG_Deep/Data2A/parsed_P010T.mat\n",
            "(640, 12, 4096)\n",
            "(720, 12, 4096)\n",
            "(720, 1)\n",
            "9\n",
            "EEG_Deep/Data2A/parsed_P09E.mat\n",
            "(0, 12, 4096)\n",
            "(40, 12, 4096)\n",
            "(40, 1)\n",
            "Filtering of training data in progress\n",
            "(720, 12, 4096)\n",
            "Filtering of test data in progress\n",
            "(40, 12, 4096)\n",
            "[2. 2. 2. 1. 1. 2. 2. 2. 2. 1. 2. 1. 2. 1. 2. 1. 1. 2. 2. 1. 2. 2. 2. 1.\n",
            " 2. 2. 1. 2. 1. 1. 2. 2. 1. 2. 2. 2. 1. 2. 1. 1. 2. 2. 2. 2. 2. 1. 2. 2.\n",
            " 1. 1. 2. 2. 2. 2. 1. 1. 1. 1. 2. 1. 1. 1. 1. 2. 2. 1. 1. 2. 1. 1. 1. 2.\n",
            " 2. 1. 2. 2. 2. 1. 2. 1. 1. 2. 2. 2. 1. 2. 1. 1. 1. 1. 1. 2. 1. 1. 1. 2.\n",
            " 2. 1. 2. 1.]\n",
            "(620, 12, 1536)\n",
            "(620,)\n",
            "(100, 12, 1536)\n",
            "(100,)\n",
            "[[0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]]\n",
            "X_train shape: (620, 1, 12, 1536)\n",
            "620 train samples\n",
            "100 val samples\n",
            "(40, 12, 1536)\n",
            "(40, 1)\n",
            "X_train shape: (40, 1, 12, 1536)\n",
            "40 train samples\n",
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 1, 12, 1536)]     0         \n",
            "_________________________________________________________________\n",
            "conv2d (Conv2D)              (None, 8, 12, 1536)       200       \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 8, 12, 1536)       32        \n",
            "_________________________________________________________________\n",
            "depthwise_conv2d (DepthwiseC (None, 16, 1, 1536)       192       \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 16, 1, 1536)       64        \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 16, 1, 1536)       0         \n",
            "_________________________________________________________________\n",
            "average_pooling2d (AveragePo (None, 16, 1, 384)        0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 16, 1, 384)        0         \n",
            "_________________________________________________________________\n",
            "separable_conv2d (SeparableC (None, 16, 1, 384)        512       \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 16, 1, 384)        64        \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 16, 1, 384)        0         \n",
            "_________________________________________________________________\n",
            "average_pooling2d_1 (Average (None, 16, 1, 48)         0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 16, 1, 48)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 768)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 2)                 1538      \n",
            "_________________________________________________________________\n",
            "softmax (Activation)         (None, 2)                 0         \n",
            "=================================================================\n",
            "Total params: 2,602\n",
            "Trainable params: 2,522\n",
            "Non-trainable params: 80\n",
            "_________________________________________________________________\n",
            "Train on 620 samples, validate on 100 samples\n",
            "Epoch 1/300\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.68858, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.6972 - acc: 0.4952 - val_loss: 0.6886 - val_acc: 0.5700\n",
            "Epoch 2/300\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.68858 to 0.68061, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.6615 - acc: 0.6194 - val_loss: 0.6806 - val_acc: 0.5300\n",
            "Epoch 3/300\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.68061 to 0.67236, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.6356 - acc: 0.6532 - val_loss: 0.6724 - val_acc: 0.5400\n",
            "Epoch 4/300\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.67236 to 0.65959, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.5972 - acc: 0.6935 - val_loss: 0.6596 - val_acc: 0.5400\n",
            "Epoch 5/300\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.65959 to 0.62047, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.5478 - acc: 0.7194 - val_loss: 0.6205 - val_acc: 0.5800\n",
            "Epoch 6/300\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.62047 to 0.59680, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.5134 - acc: 0.7484 - val_loss: 0.5968 - val_acc: 0.6300\n",
            "Epoch 7/300\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.59680 to 0.51797, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.4965 - acc: 0.7548 - val_loss: 0.5180 - val_acc: 0.7400\n",
            "Epoch 8/300\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.51797 to 0.50117, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.4717 - acc: 0.7823 - val_loss: 0.5012 - val_acc: 0.7500\n",
            "Epoch 9/300\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.50117 to 0.45393, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.4816 - acc: 0.7677 - val_loss: 0.4539 - val_acc: 0.7800\n",
            "Epoch 10/300\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.45393 to 0.45032, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.4713 - acc: 0.7677 - val_loss: 0.4503 - val_acc: 0.7800\n",
            "Epoch 11/300\n",
            "\n",
            "Epoch 00011: val_loss improved from 0.45032 to 0.44363, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.4783 - acc: 0.7758 - val_loss: 0.4436 - val_acc: 0.7400\n",
            "Epoch 12/300\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.44363\n",
            "620/620 - 1s - loss: 0.4944 - acc: 0.7839 - val_loss: 0.4607 - val_acc: 0.7700\n",
            "Epoch 13/300\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.44363\n",
            "620/620 - 1s - loss: 0.4622 - acc: 0.7935 - val_loss: 0.4476 - val_acc: 0.7500\n",
            "Epoch 14/300\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.44363\n",
            "620/620 - 1s - loss: 0.4855 - acc: 0.7629 - val_loss: 0.4605 - val_acc: 0.7900\n",
            "Epoch 15/300\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.44363\n",
            "620/620 - 1s - loss: 0.4637 - acc: 0.8000 - val_loss: 0.4701 - val_acc: 0.7500\n",
            "Epoch 16/300\n",
            "\n",
            "Epoch 00016: val_loss improved from 0.44363 to 0.40349, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.4667 - acc: 0.7871 - val_loss: 0.4035 - val_acc: 0.8000\n",
            "Epoch 17/300\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.40349\n",
            "620/620 - 1s - loss: 0.4666 - acc: 0.7661 - val_loss: 0.4296 - val_acc: 0.7800\n",
            "Epoch 18/300\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.40349\n",
            "620/620 - 1s - loss: 0.4656 - acc: 0.7806 - val_loss: 0.4129 - val_acc: 0.8200\n",
            "Epoch 19/300\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.40349\n",
            "620/620 - 1s - loss: 0.4669 - acc: 0.7613 - val_loss: 0.4114 - val_acc: 0.7900\n",
            "Epoch 20/300\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.40349\n",
            "620/620 - 1s - loss: 0.4705 - acc: 0.7774 - val_loss: 0.4200 - val_acc: 0.8200\n",
            "Epoch 21/300\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.40349\n",
            "620/620 - 1s - loss: 0.4528 - acc: 0.7871 - val_loss: 0.4356 - val_acc: 0.7800\n",
            "Epoch 22/300\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.40349\n",
            "620/620 - 1s - loss: 0.4648 - acc: 0.7903 - val_loss: 0.4489 - val_acc: 0.7600\n",
            "Epoch 23/300\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.40349\n",
            "620/620 - 1s - loss: 0.4550 - acc: 0.7855 - val_loss: 0.4348 - val_acc: 0.7900\n",
            "Epoch 24/300\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.40349\n",
            "620/620 - 1s - loss: 0.4550 - acc: 0.7823 - val_loss: 0.4246 - val_acc: 0.7900\n",
            "Epoch 25/300\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.40349\n",
            "620/620 - 1s - loss: 0.4654 - acc: 0.7806 - val_loss: 0.4519 - val_acc: 0.7600\n",
            "Epoch 26/300\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.40349\n",
            "620/620 - 1s - loss: 0.4664 - acc: 0.7677 - val_loss: 0.4493 - val_acc: 0.7900\n",
            "Epoch 27/300\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.40349\n",
            "620/620 - 1s - loss: 0.4532 - acc: 0.7839 - val_loss: 0.4642 - val_acc: 0.7800\n",
            "Epoch 28/300\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.40349\n",
            "620/620 - 1s - loss: 0.4440 - acc: 0.8016 - val_loss: 0.4387 - val_acc: 0.8100\n",
            "Epoch 29/300\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.40349\n",
            "620/620 - 1s - loss: 0.4323 - acc: 0.8177 - val_loss: 0.4172 - val_acc: 0.7800\n",
            "Epoch 30/300\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.40349\n",
            "620/620 - 1s - loss: 0.4326 - acc: 0.8000 - val_loss: 0.4378 - val_acc: 0.8000\n",
            "Epoch 31/300\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.40349\n",
            "620/620 - 1s - loss: 0.4509 - acc: 0.7758 - val_loss: 0.4046 - val_acc: 0.8000\n",
            "Epoch 32/300\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.40349\n",
            "620/620 - 1s - loss: 0.4476 - acc: 0.8097 - val_loss: 0.4278 - val_acc: 0.8200\n",
            "Epoch 33/300\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.40349\n",
            "620/620 - 1s - loss: 0.4372 - acc: 0.7855 - val_loss: 0.4153 - val_acc: 0.8100\n",
            "Epoch 34/300\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.40349\n",
            "620/620 - 1s - loss: 0.4689 - acc: 0.7774 - val_loss: 0.4153 - val_acc: 0.8000\n",
            "Epoch 35/300\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.40349\n",
            "620/620 - 1s - loss: 0.4331 - acc: 0.7871 - val_loss: 0.4106 - val_acc: 0.7500\n",
            "Epoch 36/300\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.40349\n",
            "620/620 - 1s - loss: 0.4269 - acc: 0.8032 - val_loss: 0.4359 - val_acc: 0.7800\n",
            "Epoch 37/300\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.40349\n",
            "620/620 - 1s - loss: 0.4525 - acc: 0.7935 - val_loss: 0.4235 - val_acc: 0.7600\n",
            "Epoch 38/300\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.40349\n",
            "620/620 - 1s - loss: 0.4289 - acc: 0.7952 - val_loss: 0.4152 - val_acc: 0.7700\n",
            "Epoch 39/300\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.40349\n",
            "620/620 - 1s - loss: 0.4293 - acc: 0.7968 - val_loss: 0.4123 - val_acc: 0.7900\n",
            "Epoch 40/300\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.40349\n",
            "620/620 - 1s - loss: 0.4576 - acc: 0.7871 - val_loss: 0.4451 - val_acc: 0.7700\n",
            "Epoch 41/300\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.40349\n",
            "620/620 - 1s - loss: 0.4348 - acc: 0.7919 - val_loss: 0.4083 - val_acc: 0.7600\n",
            "Epoch 42/300\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.40349\n",
            "620/620 - 1s - loss: 0.4326 - acc: 0.8000 - val_loss: 0.4300 - val_acc: 0.8000\n",
            "Epoch 43/300\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.40349\n",
            "620/620 - 1s - loss: 0.4121 - acc: 0.8210 - val_loss: 0.4077 - val_acc: 0.8000\n",
            "Epoch 44/300\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.40349\n",
            "620/620 - 1s - loss: 0.4244 - acc: 0.8129 - val_loss: 0.4241 - val_acc: 0.8100\n",
            "Epoch 45/300\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.40349\n",
            "620/620 - 1s - loss: 0.4376 - acc: 0.7935 - val_loss: 0.4058 - val_acc: 0.7600\n",
            "Epoch 46/300\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.40349\n",
            "620/620 - 1s - loss: 0.4310 - acc: 0.7952 - val_loss: 0.4251 - val_acc: 0.7800\n",
            "Epoch 47/300\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.40349\n",
            "620/620 - 1s - loss: 0.4324 - acc: 0.8081 - val_loss: 0.4511 - val_acc: 0.7600\n",
            "Epoch 48/300\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.40349\n",
            "620/620 - 1s - loss: 0.4202 - acc: 0.8258 - val_loss: 0.4341 - val_acc: 0.8100\n",
            "Epoch 49/300\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.40349\n",
            "620/620 - 1s - loss: 0.4145 - acc: 0.8113 - val_loss: 0.4110 - val_acc: 0.7900\n",
            "Epoch 50/300\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.40349\n",
            "620/620 - 1s - loss: 0.4242 - acc: 0.8048 - val_loss: 0.4118 - val_acc: 0.7900\n",
            "Epoch 51/300\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.40349\n",
            "620/620 - 1s - loss: 0.4223 - acc: 0.8016 - val_loss: 0.4378 - val_acc: 0.7700\n",
            "Epoch 52/300\n",
            "\n",
            "Epoch 00052: val_loss improved from 0.40349 to 0.39113, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.4187 - acc: 0.8000 - val_loss: 0.3911 - val_acc: 0.7600\n",
            "Epoch 53/300\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.39113\n",
            "620/620 - 1s - loss: 0.4326 - acc: 0.7903 - val_loss: 0.4263 - val_acc: 0.7700\n",
            "Epoch 54/300\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.39113\n",
            "620/620 - 1s - loss: 0.4200 - acc: 0.8210 - val_loss: 0.4175 - val_acc: 0.7800\n",
            "Epoch 55/300\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.39113\n",
            "620/620 - 1s - loss: 0.4248 - acc: 0.8210 - val_loss: 0.4182 - val_acc: 0.7800\n",
            "Epoch 56/300\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.39113\n",
            "620/620 - 1s - loss: 0.4051 - acc: 0.8145 - val_loss: 0.4301 - val_acc: 0.7600\n",
            "Epoch 57/300\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.39113\n",
            "620/620 - 1s - loss: 0.4263 - acc: 0.7952 - val_loss: 0.4059 - val_acc: 0.7700\n",
            "Epoch 58/300\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.39113\n",
            "620/620 - 1s - loss: 0.4067 - acc: 0.7903 - val_loss: 0.4115 - val_acc: 0.8000\n",
            "Epoch 59/300\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.39113\n",
            "620/620 - 1s - loss: 0.4115 - acc: 0.8161 - val_loss: 0.4530 - val_acc: 0.7500\n",
            "Epoch 60/300\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.39113\n",
            "620/620 - 1s - loss: 0.4077 - acc: 0.8065 - val_loss: 0.4234 - val_acc: 0.7600\n",
            "Epoch 61/300\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.39113\n",
            "620/620 - 1s - loss: 0.4219 - acc: 0.8097 - val_loss: 0.4014 - val_acc: 0.8100\n",
            "Epoch 62/300\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.39113\n",
            "620/620 - 1s - loss: 0.4055 - acc: 0.8016 - val_loss: 0.4236 - val_acc: 0.7800\n",
            "Epoch 63/300\n",
            "\n",
            "Epoch 00063: val_loss improved from 0.39113 to 0.38561, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.4011 - acc: 0.8306 - val_loss: 0.3856 - val_acc: 0.8100\n",
            "Epoch 64/300\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.38561\n",
            "620/620 - 1s - loss: 0.4103 - acc: 0.8065 - val_loss: 0.4528 - val_acc: 0.7700\n",
            "Epoch 65/300\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.38561\n",
            "620/620 - 1s - loss: 0.4253 - acc: 0.7903 - val_loss: 0.4271 - val_acc: 0.7800\n",
            "Epoch 66/300\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.38561\n",
            "620/620 - 1s - loss: 0.3923 - acc: 0.8339 - val_loss: 0.4538 - val_acc: 0.7600\n",
            "Epoch 67/300\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.38561\n",
            "620/620 - 1s - loss: 0.3996 - acc: 0.8177 - val_loss: 0.3940 - val_acc: 0.7900\n",
            "Epoch 68/300\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.38561\n",
            "620/620 - 1s - loss: 0.4178 - acc: 0.8032 - val_loss: 0.4825 - val_acc: 0.7800\n",
            "Epoch 69/300\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.38561\n",
            "620/620 - 1s - loss: 0.3991 - acc: 0.8258 - val_loss: 0.3948 - val_acc: 0.7700\n",
            "Epoch 70/300\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.38561\n",
            "620/620 - 1s - loss: 0.4207 - acc: 0.8145 - val_loss: 0.4044 - val_acc: 0.8000\n",
            "Epoch 71/300\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.38561\n",
            "620/620 - 1s - loss: 0.4136 - acc: 0.8016 - val_loss: 0.4266 - val_acc: 0.8000\n",
            "Epoch 72/300\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.38561\n",
            "620/620 - 1s - loss: 0.4087 - acc: 0.8097 - val_loss: 0.4016 - val_acc: 0.7800\n",
            "Epoch 73/300\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.38561\n",
            "620/620 - 1s - loss: 0.4261 - acc: 0.8097 - val_loss: 0.4089 - val_acc: 0.8000\n",
            "Epoch 74/300\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.38561\n",
            "620/620 - 1s - loss: 0.4042 - acc: 0.8177 - val_loss: 0.3961 - val_acc: 0.8100\n",
            "Epoch 75/300\n",
            "\n",
            "Epoch 00075: val_loss improved from 0.38561 to 0.38525, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.4267 - acc: 0.8065 - val_loss: 0.3853 - val_acc: 0.7900\n",
            "Epoch 76/300\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.38525\n",
            "620/620 - 1s - loss: 0.4011 - acc: 0.8226 - val_loss: 0.3938 - val_acc: 0.7800\n",
            "Epoch 77/300\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.38525\n",
            "620/620 - 1s - loss: 0.4190 - acc: 0.7935 - val_loss: 0.4632 - val_acc: 0.7900\n",
            "Epoch 78/300\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.38525\n",
            "620/620 - 1s - loss: 0.4062 - acc: 0.8113 - val_loss: 0.4662 - val_acc: 0.7700\n",
            "Epoch 79/300\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.38525\n",
            "620/620 - 1s - loss: 0.3810 - acc: 0.8258 - val_loss: 0.4116 - val_acc: 0.8000\n",
            "Epoch 80/300\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.38525\n",
            "620/620 - 1s - loss: 0.3919 - acc: 0.8145 - val_loss: 0.3968 - val_acc: 0.7600\n",
            "Epoch 81/300\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.38525\n",
            "620/620 - 1s - loss: 0.3795 - acc: 0.8306 - val_loss: 0.3918 - val_acc: 0.8100\n",
            "Epoch 82/300\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.38525\n",
            "620/620 - 1s - loss: 0.4126 - acc: 0.8032 - val_loss: 0.4217 - val_acc: 0.7700\n",
            "Epoch 83/300\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.38525\n",
            "620/620 - 1s - loss: 0.3992 - acc: 0.8274 - val_loss: 0.4226 - val_acc: 0.7700\n",
            "Epoch 84/300\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.38525\n",
            "620/620 - 1s - loss: 0.4116 - acc: 0.8145 - val_loss: 0.4022 - val_acc: 0.7700\n",
            "Epoch 85/300\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.38525\n",
            "620/620 - 1s - loss: 0.4028 - acc: 0.8210 - val_loss: 0.3972 - val_acc: 0.7800\n",
            "Epoch 86/300\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.38525\n",
            "620/620 - 1s - loss: 0.4032 - acc: 0.8194 - val_loss: 0.4363 - val_acc: 0.7700\n",
            "Epoch 87/300\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.38525\n",
            "620/620 - 1s - loss: 0.4058 - acc: 0.8065 - val_loss: 0.3987 - val_acc: 0.7900\n",
            "Epoch 88/300\n",
            "\n",
            "Epoch 00088: val_loss improved from 0.38525 to 0.38015, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.3867 - acc: 0.8242 - val_loss: 0.3802 - val_acc: 0.8000\n",
            "Epoch 89/300\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.38015\n",
            "620/620 - 1s - loss: 0.3735 - acc: 0.8419 - val_loss: 0.4408 - val_acc: 0.7800\n",
            "Epoch 90/300\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.38015\n",
            "620/620 - 1s - loss: 0.4052 - acc: 0.8145 - val_loss: 0.4174 - val_acc: 0.7900\n",
            "Epoch 91/300\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.38015\n",
            "620/620 - 1s - loss: 0.4209 - acc: 0.8016 - val_loss: 0.4235 - val_acc: 0.7900\n",
            "Epoch 92/300\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.38015\n",
            "620/620 - 1s - loss: 0.3719 - acc: 0.8274 - val_loss: 0.5297 - val_acc: 0.7300\n",
            "Epoch 93/300\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.38015\n",
            "620/620 - 1s - loss: 0.4008 - acc: 0.8065 - val_loss: 0.4170 - val_acc: 0.7900\n",
            "Epoch 94/300\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.38015\n",
            "620/620 - 1s - loss: 0.4319 - acc: 0.8032 - val_loss: 0.3868 - val_acc: 0.7900\n",
            "Epoch 95/300\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.38015\n",
            "620/620 - 1s - loss: 0.3822 - acc: 0.8355 - val_loss: 0.4275 - val_acc: 0.7700\n",
            "Epoch 96/300\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.38015\n",
            "620/620 - 1s - loss: 0.3973 - acc: 0.8242 - val_loss: 0.4088 - val_acc: 0.8000\n",
            "Epoch 97/300\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.38015\n",
            "620/620 - 1s - loss: 0.4007 - acc: 0.8194 - val_loss: 0.3935 - val_acc: 0.7900\n",
            "Epoch 98/300\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.38015\n",
            "620/620 - 1s - loss: 0.3856 - acc: 0.8242 - val_loss: 0.3930 - val_acc: 0.7900\n",
            "Epoch 99/300\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.38015\n",
            "620/620 - 1s - loss: 0.3929 - acc: 0.8161 - val_loss: 0.3841 - val_acc: 0.8000\n",
            "Epoch 100/300\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.38015\n",
            "620/620 - 1s - loss: 0.3602 - acc: 0.8306 - val_loss: 0.3935 - val_acc: 0.8000\n",
            "Epoch 101/300\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.38015\n",
            "620/620 - 1s - loss: 0.4064 - acc: 0.8129 - val_loss: 0.4018 - val_acc: 0.8200\n",
            "Epoch 102/300\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.38015\n",
            "620/620 - 1s - loss: 0.3660 - acc: 0.8452 - val_loss: 0.3976 - val_acc: 0.7800\n",
            "Epoch 103/300\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.38015\n",
            "620/620 - 1s - loss: 0.3718 - acc: 0.8274 - val_loss: 0.4120 - val_acc: 0.7900\n",
            "Epoch 104/300\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.38015\n",
            "620/620 - 1s - loss: 0.3778 - acc: 0.8274 - val_loss: 0.3921 - val_acc: 0.7700\n",
            "Epoch 105/300\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 0.38015\n",
            "620/620 - 1s - loss: 0.3729 - acc: 0.8242 - val_loss: 0.4327 - val_acc: 0.7700\n",
            "Epoch 106/300\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 0.38015\n",
            "620/620 - 1s - loss: 0.4040 - acc: 0.8129 - val_loss: 0.3967 - val_acc: 0.7800\n",
            "Epoch 107/300\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.38015\n",
            "620/620 - 1s - loss: 0.3795 - acc: 0.8323 - val_loss: 0.4270 - val_acc: 0.7900\n",
            "Epoch 108/300\n",
            "\n",
            "Epoch 00108: val_loss improved from 0.38015 to 0.36911, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.3701 - acc: 0.8403 - val_loss: 0.3691 - val_acc: 0.8000\n",
            "Epoch 109/300\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.36911\n",
            "620/620 - 1s - loss: 0.3624 - acc: 0.8339 - val_loss: 0.4214 - val_acc: 0.7900\n",
            "Epoch 110/300\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.36911\n",
            "620/620 - 1s - loss: 0.3621 - acc: 0.8435 - val_loss: 0.3808 - val_acc: 0.8200\n",
            "Epoch 111/300\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 0.36911\n",
            "620/620 - 1s - loss: 0.3826 - acc: 0.8290 - val_loss: 0.3935 - val_acc: 0.8000\n",
            "Epoch 112/300\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 0.36911\n",
            "620/620 - 1s - loss: 0.3913 - acc: 0.8210 - val_loss: 0.4284 - val_acc: 0.7900\n",
            "Epoch 113/300\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.36911\n",
            "620/620 - 1s - loss: 0.3673 - acc: 0.8258 - val_loss: 0.4139 - val_acc: 0.7800\n",
            "Epoch 114/300\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.36911\n",
            "620/620 - 1s - loss: 0.4060 - acc: 0.8097 - val_loss: 0.4221 - val_acc: 0.8000\n",
            "Epoch 115/300\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.36911\n",
            "620/620 - 1s - loss: 0.3986 - acc: 0.8290 - val_loss: 0.4055 - val_acc: 0.8100\n",
            "Epoch 116/300\n",
            "\n",
            "Epoch 00116: val_loss improved from 0.36911 to 0.36837, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.3792 - acc: 0.8323 - val_loss: 0.3684 - val_acc: 0.8100\n",
            "Epoch 117/300\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.36837\n",
            "620/620 - 1s - loss: 0.3561 - acc: 0.8387 - val_loss: 0.4306 - val_acc: 0.7800\n",
            "Epoch 118/300\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.36837\n",
            "620/620 - 1s - loss: 0.3769 - acc: 0.8403 - val_loss: 0.4410 - val_acc: 0.7600\n",
            "Epoch 119/300\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 0.36837\n",
            "620/620 - 1s - loss: 0.3545 - acc: 0.8532 - val_loss: 0.3884 - val_acc: 0.7700\n",
            "Epoch 120/300\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.36837\n",
            "620/620 - 1s - loss: 0.3739 - acc: 0.8548 - val_loss: 0.4374 - val_acc: 0.7900\n",
            "Epoch 121/300\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.36837\n",
            "620/620 - 1s - loss: 0.3772 - acc: 0.8339 - val_loss: 0.4090 - val_acc: 0.7900\n",
            "Epoch 122/300\n",
            "\n",
            "Epoch 00122: val_loss improved from 0.36837 to 0.36202, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.3500 - acc: 0.8419 - val_loss: 0.3620 - val_acc: 0.8200\n",
            "Epoch 123/300\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3784 - acc: 0.8306 - val_loss: 0.4133 - val_acc: 0.7700\n",
            "Epoch 124/300\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3503 - acc: 0.8581 - val_loss: 0.3861 - val_acc: 0.8100\n",
            "Epoch 125/300\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3593 - acc: 0.8452 - val_loss: 0.3842 - val_acc: 0.8000\n",
            "Epoch 126/300\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3811 - acc: 0.8210 - val_loss: 0.4473 - val_acc: 0.7800\n",
            "Epoch 127/300\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3590 - acc: 0.8323 - val_loss: 0.3960 - val_acc: 0.7900\n",
            "Epoch 128/300\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3629 - acc: 0.8339 - val_loss: 0.4301 - val_acc: 0.7900\n",
            "Epoch 129/300\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3685 - acc: 0.8452 - val_loss: 0.3995 - val_acc: 0.8000\n",
            "Epoch 130/300\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3739 - acc: 0.8274 - val_loss: 0.3820 - val_acc: 0.8000\n",
            "Epoch 131/300\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3696 - acc: 0.8339 - val_loss: 0.3786 - val_acc: 0.7900\n",
            "Epoch 132/300\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3674 - acc: 0.8452 - val_loss: 0.4522 - val_acc: 0.7600\n",
            "Epoch 133/300\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3637 - acc: 0.8355 - val_loss: 0.4797 - val_acc: 0.7500\n",
            "Epoch 134/300\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3671 - acc: 0.8371 - val_loss: 0.3761 - val_acc: 0.8000\n",
            "Epoch 135/300\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3704 - acc: 0.8403 - val_loss: 0.4219 - val_acc: 0.8200\n",
            "Epoch 136/300\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.4092 - acc: 0.8194 - val_loss: 0.3814 - val_acc: 0.8100\n",
            "Epoch 137/300\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3712 - acc: 0.8306 - val_loss: 0.3932 - val_acc: 0.7700\n",
            "Epoch 138/300\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3745 - acc: 0.8306 - val_loss: 0.4289 - val_acc: 0.8100\n",
            "Epoch 139/300\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3617 - acc: 0.8339 - val_loss: 0.4383 - val_acc: 0.8000\n",
            "Epoch 140/300\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3648 - acc: 0.8435 - val_loss: 0.4608 - val_acc: 0.7600\n",
            "Epoch 141/300\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3576 - acc: 0.8548 - val_loss: 0.4308 - val_acc: 0.7800\n",
            "Epoch 142/300\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3555 - acc: 0.8371 - val_loss: 0.4033 - val_acc: 0.7900\n",
            "Epoch 143/300\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3680 - acc: 0.8323 - val_loss: 0.4876 - val_acc: 0.7800\n",
            "Epoch 144/300\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3912 - acc: 0.8355 - val_loss: 0.4299 - val_acc: 0.7900\n",
            "Epoch 145/300\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3726 - acc: 0.8194 - val_loss: 0.4306 - val_acc: 0.7700\n",
            "Epoch 146/300\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3824 - acc: 0.8323 - val_loss: 0.4607 - val_acc: 0.7900\n",
            "Epoch 147/300\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3794 - acc: 0.8323 - val_loss: 0.4055 - val_acc: 0.7600\n",
            "Epoch 148/300\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3575 - acc: 0.8565 - val_loss: 0.4281 - val_acc: 0.8100\n",
            "Epoch 149/300\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.4087 - acc: 0.8242 - val_loss: 0.4548 - val_acc: 0.7800\n",
            "Epoch 150/300\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3747 - acc: 0.8274 - val_loss: 0.4182 - val_acc: 0.7900\n",
            "Epoch 151/300\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3490 - acc: 0.8403 - val_loss: 0.4645 - val_acc: 0.7800\n",
            "Epoch 152/300\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3578 - acc: 0.8419 - val_loss: 0.3836 - val_acc: 0.8300\n",
            "Epoch 153/300\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3903 - acc: 0.8306 - val_loss: 0.3844 - val_acc: 0.8000\n",
            "Epoch 154/300\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3411 - acc: 0.8661 - val_loss: 0.3780 - val_acc: 0.7800\n",
            "Epoch 155/300\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3515 - acc: 0.8371 - val_loss: 0.3885 - val_acc: 0.7900\n",
            "Epoch 156/300\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3791 - acc: 0.8371 - val_loss: 0.4113 - val_acc: 0.7700\n",
            "Epoch 157/300\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.4086 - acc: 0.8129 - val_loss: 0.3961 - val_acc: 0.7700\n",
            "Epoch 158/300\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3840 - acc: 0.8258 - val_loss: 0.3815 - val_acc: 0.8000\n",
            "Epoch 159/300\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3745 - acc: 0.8339 - val_loss: 0.3634 - val_acc: 0.8300\n",
            "Epoch 160/300\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3607 - acc: 0.8258 - val_loss: 0.3985 - val_acc: 0.8000\n",
            "Epoch 161/300\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3476 - acc: 0.8613 - val_loss: 0.4128 - val_acc: 0.8000\n",
            "Epoch 162/300\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3700 - acc: 0.8177 - val_loss: 0.4261 - val_acc: 0.8100\n",
            "Epoch 163/300\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3694 - acc: 0.8290 - val_loss: 0.3798 - val_acc: 0.8000\n",
            "Epoch 164/300\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3450 - acc: 0.8516 - val_loss: 0.4338 - val_acc: 0.7900\n",
            "Epoch 165/300\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3840 - acc: 0.8194 - val_loss: 0.3804 - val_acc: 0.8100\n",
            "Epoch 166/300\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3810 - acc: 0.8306 - val_loss: 0.4149 - val_acc: 0.8000\n",
            "Epoch 167/300\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3549 - acc: 0.8468 - val_loss: 0.4116 - val_acc: 0.7900\n",
            "Epoch 168/300\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3811 - acc: 0.8371 - val_loss: 0.4330 - val_acc: 0.7900\n",
            "Epoch 169/300\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3449 - acc: 0.8597 - val_loss: 0.4621 - val_acc: 0.7800\n",
            "Epoch 170/300\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3594 - acc: 0.8419 - val_loss: 0.4243 - val_acc: 0.8200\n",
            "Epoch 171/300\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3699 - acc: 0.8371 - val_loss: 0.3830 - val_acc: 0.7800\n",
            "Epoch 172/300\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3651 - acc: 0.8371 - val_loss: 0.4267 - val_acc: 0.7800\n",
            "Epoch 173/300\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3398 - acc: 0.8484 - val_loss: 0.4504 - val_acc: 0.7800\n",
            "Epoch 174/300\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3493 - acc: 0.8452 - val_loss: 0.4627 - val_acc: 0.8000\n",
            "Epoch 175/300\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3484 - acc: 0.8452 - val_loss: 0.5036 - val_acc: 0.7800\n",
            "Epoch 176/300\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3561 - acc: 0.8452 - val_loss: 0.4407 - val_acc: 0.8000\n",
            "Epoch 177/300\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3590 - acc: 0.8355 - val_loss: 0.4468 - val_acc: 0.8000\n",
            "Epoch 178/300\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3582 - acc: 0.8403 - val_loss: 0.4380 - val_acc: 0.7800\n",
            "Epoch 179/300\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3589 - acc: 0.8306 - val_loss: 0.4536 - val_acc: 0.7600\n",
            "Epoch 180/300\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3879 - acc: 0.8145 - val_loss: 0.3783 - val_acc: 0.8000\n",
            "Epoch 181/300\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3482 - acc: 0.8613 - val_loss: 0.4526 - val_acc: 0.7900\n",
            "Epoch 182/300\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3901 - acc: 0.8242 - val_loss: 0.4298 - val_acc: 0.8000\n",
            "Epoch 183/300\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3727 - acc: 0.8226 - val_loss: 0.4001 - val_acc: 0.8000\n",
            "Epoch 184/300\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3459 - acc: 0.8419 - val_loss: 0.4910 - val_acc: 0.8000\n",
            "Epoch 185/300\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3629 - acc: 0.8355 - val_loss: 0.4547 - val_acc: 0.8200\n",
            "Epoch 186/300\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3573 - acc: 0.8355 - val_loss: 0.4691 - val_acc: 0.7900\n",
            "Epoch 187/300\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3683 - acc: 0.8339 - val_loss: 0.4643 - val_acc: 0.8000\n",
            "Epoch 188/300\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3399 - acc: 0.8339 - val_loss: 0.4320 - val_acc: 0.7800\n",
            "Epoch 189/300\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3803 - acc: 0.8258 - val_loss: 0.3970 - val_acc: 0.8000\n",
            "Epoch 190/300\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3554 - acc: 0.8371 - val_loss: 0.4447 - val_acc: 0.7700\n",
            "Epoch 191/300\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3453 - acc: 0.8516 - val_loss: 0.5105 - val_acc: 0.7600\n",
            "Epoch 192/300\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3605 - acc: 0.8371 - val_loss: 0.5489 - val_acc: 0.7800\n",
            "Epoch 193/300\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3533 - acc: 0.8484 - val_loss: 0.4867 - val_acc: 0.8100\n",
            "Epoch 194/300\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3468 - acc: 0.8516 - val_loss: 0.4228 - val_acc: 0.7900\n",
            "Epoch 195/300\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3166 - acc: 0.8742 - val_loss: 0.4489 - val_acc: 0.7800\n",
            "Epoch 196/300\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3551 - acc: 0.8468 - val_loss: 0.4412 - val_acc: 0.8000\n",
            "Epoch 197/300\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3326 - acc: 0.8645 - val_loss: 0.3970 - val_acc: 0.7600\n",
            "Epoch 198/300\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3594 - acc: 0.8371 - val_loss: 0.4180 - val_acc: 0.7900\n",
            "Epoch 199/300\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3217 - acc: 0.8758 - val_loss: 0.4642 - val_acc: 0.7900\n",
            "Epoch 200/300\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3609 - acc: 0.8484 - val_loss: 0.5089 - val_acc: 0.8000\n",
            "Epoch 201/300\n",
            "\n",
            "Epoch 00201: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3410 - acc: 0.8387 - val_loss: 0.5006 - val_acc: 0.7800\n",
            "Epoch 202/300\n",
            "\n",
            "Epoch 00202: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3605 - acc: 0.8371 - val_loss: 0.4150 - val_acc: 0.8000\n",
            "Epoch 203/300\n",
            "\n",
            "Epoch 00203: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3282 - acc: 0.8629 - val_loss: 0.4317 - val_acc: 0.7800\n",
            "Epoch 204/300\n",
            "\n",
            "Epoch 00204: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3600 - acc: 0.8371 - val_loss: 0.3865 - val_acc: 0.8000\n",
            "Epoch 205/300\n",
            "\n",
            "Epoch 00205: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3379 - acc: 0.8581 - val_loss: 0.4144 - val_acc: 0.8000\n",
            "Epoch 206/300\n",
            "\n",
            "Epoch 00206: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3508 - acc: 0.8306 - val_loss: 0.4322 - val_acc: 0.7700\n",
            "Epoch 207/300\n",
            "\n",
            "Epoch 00207: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3346 - acc: 0.8581 - val_loss: 0.5211 - val_acc: 0.7800\n",
            "Epoch 208/300\n",
            "\n",
            "Epoch 00208: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3652 - acc: 0.8339 - val_loss: 0.3789 - val_acc: 0.8200\n",
            "Epoch 209/300\n",
            "\n",
            "Epoch 00209: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3608 - acc: 0.8419 - val_loss: 0.3938 - val_acc: 0.8000\n",
            "Epoch 210/300\n",
            "\n",
            "Epoch 00210: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3528 - acc: 0.8452 - val_loss: 0.3993 - val_acc: 0.7700\n",
            "Epoch 211/300\n",
            "\n",
            "Epoch 00211: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3393 - acc: 0.8532 - val_loss: 0.5079 - val_acc: 0.7800\n",
            "Epoch 212/300\n",
            "\n",
            "Epoch 00212: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3807 - acc: 0.8113 - val_loss: 0.4171 - val_acc: 0.8200\n",
            "Epoch 213/300\n",
            "\n",
            "Epoch 00213: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3773 - acc: 0.8339 - val_loss: 0.4111 - val_acc: 0.8100\n",
            "Epoch 214/300\n",
            "\n",
            "Epoch 00214: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3558 - acc: 0.8468 - val_loss: 0.4248 - val_acc: 0.7900\n",
            "Epoch 215/300\n",
            "\n",
            "Epoch 00215: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3505 - acc: 0.8516 - val_loss: 0.3833 - val_acc: 0.7800\n",
            "Epoch 216/300\n",
            "\n",
            "Epoch 00216: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3328 - acc: 0.8694 - val_loss: 0.3798 - val_acc: 0.8000\n",
            "Epoch 217/300\n",
            "\n",
            "Epoch 00217: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3504 - acc: 0.8387 - val_loss: 0.4471 - val_acc: 0.7900\n",
            "Epoch 218/300\n",
            "\n",
            "Epoch 00218: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3321 - acc: 0.8629 - val_loss: 0.4044 - val_acc: 0.7700\n",
            "Epoch 219/300\n",
            "\n",
            "Epoch 00219: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3631 - acc: 0.8484 - val_loss: 0.4617 - val_acc: 0.7700\n",
            "Epoch 220/300\n",
            "\n",
            "Epoch 00220: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3536 - acc: 0.8500 - val_loss: 0.4110 - val_acc: 0.7900\n",
            "Epoch 221/300\n",
            "\n",
            "Epoch 00221: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3542 - acc: 0.8355 - val_loss: 0.4533 - val_acc: 0.8000\n",
            "Epoch 222/300\n",
            "\n",
            "Epoch 00222: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3486 - acc: 0.8548 - val_loss: 0.4270 - val_acc: 0.8100\n",
            "Epoch 223/300\n",
            "\n",
            "Epoch 00223: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3431 - acc: 0.8468 - val_loss: 0.4191 - val_acc: 0.8200\n",
            "Epoch 224/300\n",
            "\n",
            "Epoch 00224: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3468 - acc: 0.8516 - val_loss: 0.4388 - val_acc: 0.8200\n",
            "Epoch 225/300\n",
            "\n",
            "Epoch 00225: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3495 - acc: 0.8452 - val_loss: 0.3844 - val_acc: 0.7900\n",
            "Epoch 226/300\n",
            "\n",
            "Epoch 00226: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3508 - acc: 0.8548 - val_loss: 0.4405 - val_acc: 0.7800\n",
            "Epoch 227/300\n",
            "\n",
            "Epoch 00227: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3382 - acc: 0.8452 - val_loss: 0.3960 - val_acc: 0.7900\n",
            "Epoch 228/300\n",
            "\n",
            "Epoch 00228: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3618 - acc: 0.8468 - val_loss: 0.4196 - val_acc: 0.7600\n",
            "Epoch 229/300\n",
            "\n",
            "Epoch 00229: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3432 - acc: 0.8435 - val_loss: 0.4532 - val_acc: 0.7700\n",
            "Epoch 230/300\n",
            "\n",
            "Epoch 00230: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3529 - acc: 0.8484 - val_loss: 0.4110 - val_acc: 0.8000\n",
            "Epoch 231/300\n",
            "\n",
            "Epoch 00231: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3449 - acc: 0.8548 - val_loss: 0.4227 - val_acc: 0.8000\n",
            "Epoch 232/300\n",
            "\n",
            "Epoch 00232: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3259 - acc: 0.8677 - val_loss: 0.4133 - val_acc: 0.8000\n",
            "Epoch 233/300\n",
            "\n",
            "Epoch 00233: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3360 - acc: 0.8677 - val_loss: 0.4740 - val_acc: 0.7800\n",
            "Epoch 234/300\n",
            "\n",
            "Epoch 00234: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3374 - acc: 0.8339 - val_loss: 0.4850 - val_acc: 0.8000\n",
            "Epoch 235/300\n",
            "\n",
            "Epoch 00235: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3475 - acc: 0.8613 - val_loss: 0.4694 - val_acc: 0.7900\n",
            "Epoch 236/300\n",
            "\n",
            "Epoch 00236: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3585 - acc: 0.8355 - val_loss: 0.4546 - val_acc: 0.7600\n",
            "Epoch 237/300\n",
            "\n",
            "Epoch 00237: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3259 - acc: 0.8565 - val_loss: 0.4113 - val_acc: 0.7600\n",
            "Epoch 238/300\n",
            "\n",
            "Epoch 00238: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3479 - acc: 0.8484 - val_loss: 0.4704 - val_acc: 0.7900\n",
            "Epoch 239/300\n",
            "\n",
            "Epoch 00239: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3534 - acc: 0.8452 - val_loss: 0.4386 - val_acc: 0.7800\n",
            "Epoch 240/300\n",
            "\n",
            "Epoch 00240: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3441 - acc: 0.8548 - val_loss: 0.4203 - val_acc: 0.8000\n",
            "Epoch 241/300\n",
            "\n",
            "Epoch 00241: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3483 - acc: 0.8371 - val_loss: 0.4441 - val_acc: 0.8000\n",
            "Epoch 242/300\n",
            "\n",
            "Epoch 00242: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3594 - acc: 0.8355 - val_loss: 0.5142 - val_acc: 0.8000\n",
            "Epoch 243/300\n",
            "\n",
            "Epoch 00243: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3275 - acc: 0.8597 - val_loss: 0.4528 - val_acc: 0.8000\n",
            "Epoch 244/300\n",
            "\n",
            "Epoch 00244: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3530 - acc: 0.8403 - val_loss: 0.4043 - val_acc: 0.8100\n",
            "Epoch 245/300\n",
            "\n",
            "Epoch 00245: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3378 - acc: 0.8548 - val_loss: 0.4716 - val_acc: 0.7800\n",
            "Epoch 246/300\n",
            "\n",
            "Epoch 00246: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3640 - acc: 0.8371 - val_loss: 0.3908 - val_acc: 0.8100\n",
            "Epoch 247/300\n",
            "\n",
            "Epoch 00247: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3288 - acc: 0.8500 - val_loss: 0.3978 - val_acc: 0.7900\n",
            "Epoch 248/300\n",
            "\n",
            "Epoch 00248: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3326 - acc: 0.8484 - val_loss: 0.4236 - val_acc: 0.7900\n",
            "Epoch 249/300\n",
            "\n",
            "Epoch 00249: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3401 - acc: 0.8613 - val_loss: 0.4620 - val_acc: 0.7900\n",
            "Epoch 250/300\n",
            "\n",
            "Epoch 00250: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3475 - acc: 0.8242 - val_loss: 0.3856 - val_acc: 0.7900\n",
            "Epoch 251/300\n",
            "\n",
            "Epoch 00251: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3193 - acc: 0.8677 - val_loss: 0.4554 - val_acc: 0.7700\n",
            "Epoch 252/300\n",
            "\n",
            "Epoch 00252: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3605 - acc: 0.8339 - val_loss: 0.4413 - val_acc: 0.7700\n",
            "Epoch 253/300\n",
            "\n",
            "Epoch 00253: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3452 - acc: 0.8516 - val_loss: 0.4942 - val_acc: 0.7800\n",
            "Epoch 254/300\n",
            "\n",
            "Epoch 00254: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3230 - acc: 0.8581 - val_loss: 0.4215 - val_acc: 0.8000\n",
            "Epoch 255/300\n",
            "\n",
            "Epoch 00255: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3379 - acc: 0.8597 - val_loss: 0.4421 - val_acc: 0.7900\n",
            "Epoch 256/300\n",
            "\n",
            "Epoch 00256: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3234 - acc: 0.8581 - val_loss: 0.4548 - val_acc: 0.7800\n",
            "Epoch 257/300\n",
            "\n",
            "Epoch 00257: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3329 - acc: 0.8581 - val_loss: 0.4907 - val_acc: 0.7900\n",
            "Epoch 258/300\n",
            "\n",
            "Epoch 00258: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3446 - acc: 0.8484 - val_loss: 0.4748 - val_acc: 0.8000\n",
            "Epoch 259/300\n",
            "\n",
            "Epoch 00259: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3287 - acc: 0.8435 - val_loss: 0.3921 - val_acc: 0.8200\n",
            "Epoch 260/300\n",
            "\n",
            "Epoch 00260: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3483 - acc: 0.8484 - val_loss: 0.4247 - val_acc: 0.8300\n",
            "Epoch 261/300\n",
            "\n",
            "Epoch 00261: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3422 - acc: 0.8581 - val_loss: 0.4452 - val_acc: 0.8000\n",
            "Epoch 262/300\n",
            "\n",
            "Epoch 00262: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3544 - acc: 0.8484 - val_loss: 0.4098 - val_acc: 0.8000\n",
            "Epoch 263/300\n",
            "\n",
            "Epoch 00263: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3227 - acc: 0.8629 - val_loss: 0.4175 - val_acc: 0.8000\n",
            "Epoch 264/300\n",
            "\n",
            "Epoch 00264: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3347 - acc: 0.8403 - val_loss: 0.4763 - val_acc: 0.7900\n",
            "Epoch 265/300\n",
            "\n",
            "Epoch 00265: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3394 - acc: 0.8613 - val_loss: 0.4268 - val_acc: 0.7800\n",
            "Epoch 266/300\n",
            "\n",
            "Epoch 00266: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3196 - acc: 0.8645 - val_loss: 0.4105 - val_acc: 0.8000\n",
            "Epoch 267/300\n",
            "\n",
            "Epoch 00267: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3659 - acc: 0.8387 - val_loss: 0.4911 - val_acc: 0.7700\n",
            "Epoch 268/300\n",
            "\n",
            "Epoch 00268: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3311 - acc: 0.8645 - val_loss: 0.4823 - val_acc: 0.7700\n",
            "Epoch 269/300\n",
            "\n",
            "Epoch 00269: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3210 - acc: 0.8597 - val_loss: 0.4884 - val_acc: 0.7900\n",
            "Epoch 270/300\n",
            "\n",
            "Epoch 00270: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3644 - acc: 0.8306 - val_loss: 0.4033 - val_acc: 0.7800\n",
            "Epoch 271/300\n",
            "\n",
            "Epoch 00271: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3128 - acc: 0.8790 - val_loss: 0.4806 - val_acc: 0.7800\n",
            "Epoch 272/300\n",
            "\n",
            "Epoch 00272: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3349 - acc: 0.8677 - val_loss: 0.4722 - val_acc: 0.7600\n",
            "Epoch 273/300\n",
            "\n",
            "Epoch 00273: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3432 - acc: 0.8419 - val_loss: 0.4282 - val_acc: 0.8000\n",
            "Epoch 274/300\n",
            "\n",
            "Epoch 00274: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3102 - acc: 0.8581 - val_loss: 0.4950 - val_acc: 0.7800\n",
            "Epoch 275/300\n",
            "\n",
            "Epoch 00275: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3476 - acc: 0.8403 - val_loss: 0.4288 - val_acc: 0.8000\n",
            "Epoch 276/300\n",
            "\n",
            "Epoch 00276: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3455 - acc: 0.8484 - val_loss: 0.4676 - val_acc: 0.8000\n",
            "Epoch 277/300\n",
            "\n",
            "Epoch 00277: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3260 - acc: 0.8532 - val_loss: 0.4301 - val_acc: 0.7700\n",
            "Epoch 278/300\n",
            "\n",
            "Epoch 00278: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3455 - acc: 0.8565 - val_loss: 0.4144 - val_acc: 0.7800\n",
            "Epoch 279/300\n",
            "\n",
            "Epoch 00279: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3333 - acc: 0.8484 - val_loss: 0.4018 - val_acc: 0.7900\n",
            "Epoch 280/300\n",
            "\n",
            "Epoch 00280: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3217 - acc: 0.8468 - val_loss: 0.5027 - val_acc: 0.7900\n",
            "Epoch 281/300\n",
            "\n",
            "Epoch 00281: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3729 - acc: 0.8403 - val_loss: 0.4856 - val_acc: 0.7800\n",
            "Epoch 282/300\n",
            "\n",
            "Epoch 00282: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3304 - acc: 0.8597 - val_loss: 0.4123 - val_acc: 0.7900\n",
            "Epoch 283/300\n",
            "\n",
            "Epoch 00283: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3319 - acc: 0.8516 - val_loss: 0.4651 - val_acc: 0.7900\n",
            "Epoch 284/300\n",
            "\n",
            "Epoch 00284: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3469 - acc: 0.8419 - val_loss: 0.4232 - val_acc: 0.7900\n",
            "Epoch 285/300\n",
            "\n",
            "Epoch 00285: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3280 - acc: 0.8355 - val_loss: 0.4755 - val_acc: 0.7800\n",
            "Epoch 286/300\n",
            "\n",
            "Epoch 00286: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3261 - acc: 0.8452 - val_loss: 0.4912 - val_acc: 0.7900\n",
            "Epoch 287/300\n",
            "\n",
            "Epoch 00287: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3062 - acc: 0.8613 - val_loss: 0.3875 - val_acc: 0.7800\n",
            "Epoch 288/300\n",
            "\n",
            "Epoch 00288: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3112 - acc: 0.8645 - val_loss: 0.3858 - val_acc: 0.8000\n",
            "Epoch 289/300\n",
            "\n",
            "Epoch 00289: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3208 - acc: 0.8548 - val_loss: 0.4164 - val_acc: 0.7900\n",
            "Epoch 290/300\n",
            "\n",
            "Epoch 00290: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3415 - acc: 0.8581 - val_loss: 0.4574 - val_acc: 0.8000\n",
            "Epoch 291/300\n",
            "\n",
            "Epoch 00291: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3093 - acc: 0.8758 - val_loss: 0.4125 - val_acc: 0.7900\n",
            "Epoch 292/300\n",
            "\n",
            "Epoch 00292: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3204 - acc: 0.8484 - val_loss: 0.4912 - val_acc: 0.8000\n",
            "Epoch 293/300\n",
            "\n",
            "Epoch 00293: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3418 - acc: 0.8435 - val_loss: 0.4396 - val_acc: 0.8200\n",
            "Epoch 294/300\n",
            "\n",
            "Epoch 00294: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3362 - acc: 0.8419 - val_loss: 0.3931 - val_acc: 0.8100\n",
            "Epoch 295/300\n",
            "\n",
            "Epoch 00295: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3419 - acc: 0.8484 - val_loss: 0.4519 - val_acc: 0.8100\n",
            "Epoch 296/300\n",
            "\n",
            "Epoch 00296: val_loss did not improve from 0.36202\n",
            "620/620 - 1s - loss: 0.3300 - acc: 0.8548 - val_loss: 0.4629 - val_acc: 0.7900\n",
            "Epoch 297/300\n",
            "\n",
            "Epoch 00297: val_loss improved from 0.36202 to 0.36099, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.3256 - acc: 0.8597 - val_loss: 0.3610 - val_acc: 0.8300\n",
            "Epoch 298/300\n",
            "\n",
            "Epoch 00298: val_loss did not improve from 0.36099\n",
            "620/620 - 1s - loss: 0.3298 - acc: 0.8581 - val_loss: 0.4554 - val_acc: 0.8100\n",
            "Epoch 299/300\n",
            "\n",
            "Epoch 00299: val_loss did not improve from 0.36099\n",
            "620/620 - 1s - loss: 0.3282 - acc: 0.8565 - val_loss: 0.3779 - val_acc: 0.8000\n",
            "Epoch 300/300\n",
            "\n",
            "Epoch 00300: val_loss did not improve from 0.36099\n",
            "620/620 - 1s - loss: 0.3327 - acc: 0.8468 - val_loss: 0.4731 - val_acc: 0.8000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n9z8nhtTgAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "# Evaluate on test data\n",
            "40/40 [==============================] - 0s 3ms/sample - loss: 0.7436 - acc: 0.5750\n",
            "test loss, test acc: [0.743561175558716, 0.575]\n",
            "1\n",
            "EEG_Deep/Data2A/parsed_P01T.mat\n",
            "(0, 12, 4096)\n",
            "(80, 12, 4096)\n",
            "(80, 1)\n",
            "2\n",
            "EEG_Deep/Data2A/parsed_P02T.mat\n",
            "(80, 12, 4096)\n",
            "(160, 12, 4096)\n",
            "(160, 1)\n",
            "3\n",
            "EEG_Deep/Data2A/parsed_P03T.mat\n",
            "(160, 12, 4096)\n",
            "(240, 12, 4096)\n",
            "(240, 1)\n",
            "4\n",
            "EEG_Deep/Data2A/parsed_P04T.mat\n",
            "(240, 12, 4096)\n",
            "(320, 12, 4096)\n",
            "(320, 1)\n",
            "5\n",
            "EEG_Deep/Data2A/parsed_P05T.mat\n",
            "(320, 12, 4096)\n",
            "(400, 12, 4096)\n",
            "(400, 1)\n",
            "7\n",
            "EEG_Deep/Data2A/parsed_P07T.mat\n",
            "(400, 12, 4096)\n",
            "(480, 12, 4096)\n",
            "(480, 1)\n",
            "8\n",
            "EEG_Deep/Data2A/parsed_P08T.mat\n",
            "(480, 12, 4096)\n",
            "(560, 12, 4096)\n",
            "(560, 1)\n",
            "9\n",
            "EEG_Deep/Data2A/parsed_P09T.mat\n",
            "(560, 12, 4096)\n",
            "(640, 12, 4096)\n",
            "(640, 1)\n",
            "10\n",
            "EEG_Deep/Data2A/parsed_P010T.mat\n",
            "(640, 12, 4096)\n",
            "(720, 12, 4096)\n",
            "(720, 1)\n",
            "6\n",
            "EEG_Deep/Data2A/parsed_P06E.mat\n",
            "(0, 12, 4096)\n",
            "(40, 12, 4096)\n",
            "(40, 1)\n",
            "Filtering of training data in progress\n",
            "(720, 12, 4096)\n",
            "Filtering of test data in progress\n",
            "(40, 12, 4096)\n",
            "[2. 1. 1. 2. 2. 1. 2. 1. 2. 1. 2. 1. 1. 2. 2. 2. 2. 1. 2. 2. 1. 2. 1. 2.\n",
            " 1. 1. 2. 2. 2. 1. 1. 2. 2. 2. 2. 1. 2. 2. 1. 1. 1. 1. 2. 1. 1. 1. 2. 1.\n",
            " 2. 1. 2. 1. 2. 1. 1. 1. 2. 2. 1. 1. 1. 2. 2. 1. 1. 2. 1. 2. 1. 1. 1. 2.\n",
            " 2. 1. 1. 2. 1. 2. 1. 2. 2. 1. 2. 1. 1. 2. 2. 1. 1. 2. 2. 2. 1. 1. 1. 2.\n",
            " 1. 2. 1. 2.]\n",
            "(620, 12, 1536)\n",
            "(620,)\n",
            "(100, 12, 1536)\n",
            "(100,)\n",
            "[[0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]]\n",
            "X_train shape: (620, 1, 12, 1536)\n",
            "620 train samples\n",
            "100 val samples\n",
            "(40, 12, 1536)\n",
            "(40, 1)\n",
            "X_train shape: (40, 1, 12, 1536)\n",
            "40 train samples\n",
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 1, 12, 1536)]     0         \n",
            "_________________________________________________________________\n",
            "conv2d (Conv2D)              (None, 8, 12, 1536)       200       \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 8, 12, 1536)       32        \n",
            "_________________________________________________________________\n",
            "depthwise_conv2d (DepthwiseC (None, 16, 1, 1536)       192       \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 16, 1, 1536)       64        \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 16, 1, 1536)       0         \n",
            "_________________________________________________________________\n",
            "average_pooling2d (AveragePo (None, 16, 1, 384)        0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 16, 1, 384)        0         \n",
            "_________________________________________________________________\n",
            "separable_conv2d (SeparableC (None, 16, 1, 384)        512       \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 16, 1, 384)        64        \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 16, 1, 384)        0         \n",
            "_________________________________________________________________\n",
            "average_pooling2d_1 (Average (None, 16, 1, 48)         0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 16, 1, 48)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 768)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 2)                 1538      \n",
            "_________________________________________________________________\n",
            "softmax (Activation)         (None, 2)                 0         \n",
            "=================================================================\n",
            "Total params: 2,602\n",
            "Trainable params: 2,522\n",
            "Non-trainable params: 80\n",
            "_________________________________________________________________\n",
            "Train on 620 samples, validate on 100 samples\n",
            "Epoch 1/300\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.68759, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.6989 - acc: 0.4887 - val_loss: 0.6876 - val_acc: 0.5900\n",
            "Epoch 2/300\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.68759 to 0.68605, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.6785 - acc: 0.5597 - val_loss: 0.6860 - val_acc: 0.5700\n",
            "Epoch 3/300\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.68605 to 0.68340, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.6591 - acc: 0.6161 - val_loss: 0.6834 - val_acc: 0.5300\n",
            "Epoch 4/300\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.68340\n",
            "620/620 - 1s - loss: 0.6350 - acc: 0.6419 - val_loss: 0.6886 - val_acc: 0.4900\n",
            "Epoch 5/300\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.68340\n",
            "620/620 - 1s - loss: 0.6180 - acc: 0.6726 - val_loss: 0.7055 - val_acc: 0.4900\n",
            "Epoch 6/300\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.68340\n",
            "620/620 - 1s - loss: 0.5839 - acc: 0.7032 - val_loss: 0.7351 - val_acc: 0.4900\n",
            "Epoch 7/300\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.68340\n",
            "620/620 - 1s - loss: 0.5592 - acc: 0.7306 - val_loss: 0.7257 - val_acc: 0.5000\n",
            "Epoch 8/300\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.68340 to 0.64492, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.5869 - acc: 0.6661 - val_loss: 0.6449 - val_acc: 0.6100\n",
            "Epoch 9/300\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.64492\n",
            "620/620 - 1s - loss: 0.5587 - acc: 0.7274 - val_loss: 0.6788 - val_acc: 0.5500\n",
            "Epoch 10/300\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.64492 to 0.62265, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.5487 - acc: 0.7387 - val_loss: 0.6226 - val_acc: 0.6600\n",
            "Epoch 11/300\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.5235 - acc: 0.7403 - val_loss: 0.6548 - val_acc: 0.5800\n",
            "Epoch 12/300\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.5118 - acc: 0.7597 - val_loss: 0.6509 - val_acc: 0.5900\n",
            "Epoch 13/300\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.5206 - acc: 0.7419 - val_loss: 0.6966 - val_acc: 0.6000\n",
            "Epoch 14/300\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.5255 - acc: 0.7242 - val_loss: 0.6244 - val_acc: 0.6500\n",
            "Epoch 15/300\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.5180 - acc: 0.7468 - val_loss: 0.7530 - val_acc: 0.5800\n",
            "Epoch 16/300\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.5096 - acc: 0.7435 - val_loss: 0.6813 - val_acc: 0.6200\n",
            "Epoch 17/300\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.5091 - acc: 0.7532 - val_loss: 0.6885 - val_acc: 0.6300\n",
            "Epoch 18/300\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.4942 - acc: 0.7500 - val_loss: 0.6290 - val_acc: 0.6700\n",
            "Epoch 19/300\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.5062 - acc: 0.7597 - val_loss: 0.6246 - val_acc: 0.6600\n",
            "Epoch 20/300\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.4775 - acc: 0.7871 - val_loss: 0.6748 - val_acc: 0.6200\n",
            "Epoch 21/300\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.5060 - acc: 0.7645 - val_loss: 0.7395 - val_acc: 0.6300\n",
            "Epoch 22/300\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.5149 - acc: 0.7387 - val_loss: 0.6635 - val_acc: 0.6200\n",
            "Epoch 23/300\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.4816 - acc: 0.7694 - val_loss: 0.7081 - val_acc: 0.6100\n",
            "Epoch 24/300\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.4846 - acc: 0.7597 - val_loss: 0.6781 - val_acc: 0.6200\n",
            "Epoch 25/300\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.4762 - acc: 0.7790 - val_loss: 0.6833 - val_acc: 0.6400\n",
            "Epoch 26/300\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.4710 - acc: 0.7968 - val_loss: 0.6726 - val_acc: 0.6500\n",
            "Epoch 27/300\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.4647 - acc: 0.7887 - val_loss: 0.7404 - val_acc: 0.5800\n",
            "Epoch 28/300\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.4777 - acc: 0.7597 - val_loss: 0.7374 - val_acc: 0.5500\n",
            "Epoch 29/300\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.4856 - acc: 0.7548 - val_loss: 0.6473 - val_acc: 0.6400\n",
            "Epoch 30/300\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.4675 - acc: 0.7839 - val_loss: 0.6864 - val_acc: 0.6000\n",
            "Epoch 31/300\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.4758 - acc: 0.7726 - val_loss: 0.6905 - val_acc: 0.5900\n",
            "Epoch 32/300\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.4512 - acc: 0.7806 - val_loss: 0.7112 - val_acc: 0.6100\n",
            "Epoch 33/300\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.4627 - acc: 0.7613 - val_loss: 0.7083 - val_acc: 0.6100\n",
            "Epoch 34/300\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.4643 - acc: 0.7839 - val_loss: 0.6481 - val_acc: 0.6200\n",
            "Epoch 35/300\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.4628 - acc: 0.7774 - val_loss: 0.7373 - val_acc: 0.5800\n",
            "Epoch 36/300\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.4532 - acc: 0.7823 - val_loss: 0.7038 - val_acc: 0.5900\n",
            "Epoch 37/300\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.4512 - acc: 0.7806 - val_loss: 0.7268 - val_acc: 0.5700\n",
            "Epoch 38/300\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.4594 - acc: 0.7871 - val_loss: 0.7314 - val_acc: 0.6100\n",
            "Epoch 39/300\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.4471 - acc: 0.7887 - val_loss: 0.6563 - val_acc: 0.5900\n",
            "Epoch 40/300\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.4517 - acc: 0.7823 - val_loss: 0.6995 - val_acc: 0.6000\n",
            "Epoch 41/300\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.4692 - acc: 0.7839 - val_loss: 0.7480 - val_acc: 0.5800\n",
            "Epoch 42/300\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.4519 - acc: 0.7919 - val_loss: 0.7706 - val_acc: 0.5600\n",
            "Epoch 43/300\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.4575 - acc: 0.7823 - val_loss: 0.7876 - val_acc: 0.5800\n",
            "Epoch 44/300\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.4528 - acc: 0.7742 - val_loss: 0.8095 - val_acc: 0.5400\n",
            "Epoch 45/300\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.4814 - acc: 0.7500 - val_loss: 0.8454 - val_acc: 0.5400\n",
            "Epoch 46/300\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.4585 - acc: 0.7823 - val_loss: 0.7027 - val_acc: 0.6000\n",
            "Epoch 47/300\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.4690 - acc: 0.7774 - val_loss: 0.8523 - val_acc: 0.5400\n",
            "Epoch 48/300\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.4501 - acc: 0.7919 - val_loss: 0.6555 - val_acc: 0.6200\n",
            "Epoch 49/300\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.4574 - acc: 0.8032 - val_loss: 0.7935 - val_acc: 0.6000\n",
            "Epoch 50/300\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.4324 - acc: 0.8065 - val_loss: 0.7368 - val_acc: 0.5800\n",
            "Epoch 51/300\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.4321 - acc: 0.8129 - val_loss: 0.6636 - val_acc: 0.6400\n",
            "Epoch 52/300\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.4567 - acc: 0.7597 - val_loss: 0.7040 - val_acc: 0.6200\n",
            "Epoch 53/300\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.4465 - acc: 0.7823 - val_loss: 0.7141 - val_acc: 0.6000\n",
            "Epoch 54/300\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.4316 - acc: 0.8065 - val_loss: 0.7271 - val_acc: 0.6000\n",
            "Epoch 55/300\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.4291 - acc: 0.7984 - val_loss: 0.7212 - val_acc: 0.6100\n",
            "Epoch 56/300\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.4443 - acc: 0.7919 - val_loss: 0.8005 - val_acc: 0.5700\n",
            "Epoch 57/300\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.4527 - acc: 0.7952 - val_loss: 0.7422 - val_acc: 0.6000\n",
            "Epoch 58/300\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.4308 - acc: 0.8048 - val_loss: 0.7800 - val_acc: 0.5800\n",
            "Epoch 59/300\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.4300 - acc: 0.8097 - val_loss: 0.7373 - val_acc: 0.5800\n",
            "Epoch 60/300\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.4572 - acc: 0.7758 - val_loss: 0.6951 - val_acc: 0.5800\n",
            "Epoch 61/300\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.4508 - acc: 0.7823 - val_loss: 0.7150 - val_acc: 0.5700\n",
            "Epoch 62/300\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.4402 - acc: 0.7823 - val_loss: 0.7511 - val_acc: 0.5700\n",
            "Epoch 63/300\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.4353 - acc: 0.7839 - val_loss: 0.8313 - val_acc: 0.5300\n",
            "Epoch 64/300\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.4534 - acc: 0.7968 - val_loss: 0.6583 - val_acc: 0.6400\n",
            "Epoch 65/300\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.4358 - acc: 0.7935 - val_loss: 0.6692 - val_acc: 0.6300\n",
            "Epoch 66/300\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.4315 - acc: 0.8081 - val_loss: 0.7271 - val_acc: 0.6100\n",
            "Epoch 67/300\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.4135 - acc: 0.8161 - val_loss: 0.7785 - val_acc: 0.5600\n",
            "Epoch 68/300\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.4502 - acc: 0.7855 - val_loss: 0.8154 - val_acc: 0.5800\n",
            "Epoch 69/300\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.4353 - acc: 0.8065 - val_loss: 0.8106 - val_acc: 0.5300\n",
            "Epoch 70/300\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.4465 - acc: 0.7984 - val_loss: 0.7222 - val_acc: 0.5700\n",
            "Epoch 71/300\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.4231 - acc: 0.8161 - val_loss: 0.6957 - val_acc: 0.6200\n",
            "Epoch 72/300\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.4444 - acc: 0.7935 - val_loss: 0.6873 - val_acc: 0.5700\n",
            "Epoch 73/300\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.4356 - acc: 0.8000 - val_loss: 0.6756 - val_acc: 0.6300\n",
            "Epoch 74/300\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.4498 - acc: 0.7726 - val_loss: 0.6408 - val_acc: 0.6300\n",
            "Epoch 75/300\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.4361 - acc: 0.8065 - val_loss: 0.7853 - val_acc: 0.5900\n",
            "Epoch 76/300\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.4267 - acc: 0.8129 - val_loss: 0.7422 - val_acc: 0.6000\n",
            "Epoch 77/300\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.4249 - acc: 0.8032 - val_loss: 0.7262 - val_acc: 0.5900\n",
            "Epoch 78/300\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.4279 - acc: 0.8016 - val_loss: 0.7278 - val_acc: 0.5900\n",
            "Epoch 79/300\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.4261 - acc: 0.8032 - val_loss: 0.7565 - val_acc: 0.5800\n",
            "Epoch 80/300\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.4146 - acc: 0.8210 - val_loss: 0.7130 - val_acc: 0.6000\n",
            "Epoch 81/300\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.4020 - acc: 0.8242 - val_loss: 0.6636 - val_acc: 0.6400\n",
            "Epoch 82/300\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.4084 - acc: 0.8097 - val_loss: 0.6853 - val_acc: 0.6200\n",
            "Epoch 83/300\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.4148 - acc: 0.8097 - val_loss: 0.7121 - val_acc: 0.6100\n",
            "Epoch 84/300\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.4332 - acc: 0.8032 - val_loss: 0.7260 - val_acc: 0.5700\n",
            "Epoch 85/300\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.4481 - acc: 0.7823 - val_loss: 0.7062 - val_acc: 0.6000\n",
            "Epoch 86/300\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.4250 - acc: 0.8048 - val_loss: 0.6966 - val_acc: 0.5600\n",
            "Epoch 87/300\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.4256 - acc: 0.8113 - val_loss: 0.7562 - val_acc: 0.5600\n",
            "Epoch 88/300\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.4230 - acc: 0.8113 - val_loss: 0.7162 - val_acc: 0.5800\n",
            "Epoch 89/300\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.4235 - acc: 0.8081 - val_loss: 0.7661 - val_acc: 0.6000\n",
            "Epoch 90/300\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.4145 - acc: 0.8081 - val_loss: 0.7132 - val_acc: 0.6000\n",
            "Epoch 91/300\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.4045 - acc: 0.8258 - val_loss: 0.7471 - val_acc: 0.5900\n",
            "Epoch 92/300\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.4292 - acc: 0.8113 - val_loss: 0.6616 - val_acc: 0.6300\n",
            "Epoch 93/300\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.4341 - acc: 0.7968 - val_loss: 0.8056 - val_acc: 0.5400\n",
            "Epoch 94/300\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.4251 - acc: 0.8113 - val_loss: 0.7429 - val_acc: 0.5700\n",
            "Epoch 95/300\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.4125 - acc: 0.8226 - val_loss: 0.6742 - val_acc: 0.6100\n",
            "Epoch 96/300\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.4011 - acc: 0.8242 - val_loss: 0.7619 - val_acc: 0.5800\n",
            "Epoch 97/300\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.4198 - acc: 0.7984 - val_loss: 0.7769 - val_acc: 0.5900\n",
            "Epoch 98/300\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.4053 - acc: 0.8145 - val_loss: 0.7213 - val_acc: 0.6100\n",
            "Epoch 99/300\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.4243 - acc: 0.7984 - val_loss: 0.6980 - val_acc: 0.6200\n",
            "Epoch 100/300\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.4345 - acc: 0.7968 - val_loss: 0.7131 - val_acc: 0.6000\n",
            "Epoch 101/300\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.4116 - acc: 0.8177 - val_loss: 0.7878 - val_acc: 0.5800\n",
            "Epoch 102/300\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.3810 - acc: 0.8371 - val_loss: 0.7356 - val_acc: 0.6100\n",
            "Epoch 103/300\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.4111 - acc: 0.8161 - val_loss: 0.7688 - val_acc: 0.5900\n",
            "Epoch 104/300\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.4253 - acc: 0.8097 - val_loss: 0.7444 - val_acc: 0.5800\n",
            "Epoch 105/300\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.4100 - acc: 0.8065 - val_loss: 0.6767 - val_acc: 0.6200\n",
            "Epoch 106/300\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.4289 - acc: 0.8000 - val_loss: 0.7381 - val_acc: 0.5800\n",
            "Epoch 107/300\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.4206 - acc: 0.8113 - val_loss: 0.7819 - val_acc: 0.5800\n",
            "Epoch 108/300\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.4069 - acc: 0.8274 - val_loss: 0.8204 - val_acc: 0.5300\n",
            "Epoch 109/300\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.4050 - acc: 0.8306 - val_loss: 0.7644 - val_acc: 0.5400\n",
            "Epoch 110/300\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.4094 - acc: 0.8081 - val_loss: 0.7544 - val_acc: 0.5600\n",
            "Epoch 111/300\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.4163 - acc: 0.8242 - val_loss: 0.7124 - val_acc: 0.5800\n",
            "Epoch 112/300\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.4067 - acc: 0.8161 - val_loss: 0.7701 - val_acc: 0.5800\n",
            "Epoch 113/300\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.4109 - acc: 0.8242 - val_loss: 0.7806 - val_acc: 0.5700\n",
            "Epoch 114/300\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.4081 - acc: 0.8129 - val_loss: 0.7950 - val_acc: 0.5800\n",
            "Epoch 115/300\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.4073 - acc: 0.8161 - val_loss: 0.7835 - val_acc: 0.5800\n",
            "Epoch 116/300\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.3877 - acc: 0.8387 - val_loss: 0.6740 - val_acc: 0.6000\n",
            "Epoch 117/300\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.4013 - acc: 0.8242 - val_loss: 0.7833 - val_acc: 0.5700\n",
            "Epoch 118/300\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.4306 - acc: 0.7968 - val_loss: 0.8783 - val_acc: 0.5500\n",
            "Epoch 119/300\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.4014 - acc: 0.8403 - val_loss: 0.8781 - val_acc: 0.5400\n",
            "Epoch 120/300\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.3896 - acc: 0.8274 - val_loss: 0.7920 - val_acc: 0.5700\n",
            "Epoch 121/300\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.4060 - acc: 0.8306 - val_loss: 0.7993 - val_acc: 0.5500\n",
            "Epoch 122/300\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.3849 - acc: 0.8258 - val_loss: 0.8120 - val_acc: 0.5500\n",
            "Epoch 123/300\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.3920 - acc: 0.8161 - val_loss: 0.9178 - val_acc: 0.5300\n",
            "Epoch 124/300\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.3898 - acc: 0.8194 - val_loss: 0.7552 - val_acc: 0.5800\n",
            "Epoch 125/300\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.4159 - acc: 0.8145 - val_loss: 0.7928 - val_acc: 0.5300\n",
            "Epoch 126/300\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.4002 - acc: 0.8274 - val_loss: 0.6973 - val_acc: 0.6100\n",
            "Epoch 127/300\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.4064 - acc: 0.8161 - val_loss: 0.8073 - val_acc: 0.5500\n",
            "Epoch 128/300\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.3849 - acc: 0.8387 - val_loss: 0.7151 - val_acc: 0.5800\n",
            "Epoch 129/300\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.3865 - acc: 0.8129 - val_loss: 0.7568 - val_acc: 0.5700\n",
            "Epoch 130/300\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.4040 - acc: 0.8194 - val_loss: 0.6697 - val_acc: 0.6100\n",
            "Epoch 131/300\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.3800 - acc: 0.8306 - val_loss: 0.6982 - val_acc: 0.5700\n",
            "Epoch 132/300\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.3873 - acc: 0.8339 - val_loss: 0.6496 - val_acc: 0.6300\n",
            "Epoch 133/300\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.3940 - acc: 0.8452 - val_loss: 0.6740 - val_acc: 0.6200\n",
            "Epoch 134/300\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.3972 - acc: 0.8032 - val_loss: 0.7853 - val_acc: 0.5600\n",
            "Epoch 135/300\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.3993 - acc: 0.8210 - val_loss: 0.8767 - val_acc: 0.5200\n",
            "Epoch 136/300\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.3923 - acc: 0.8306 - val_loss: 0.7995 - val_acc: 0.5600\n",
            "Epoch 137/300\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.3999 - acc: 0.8242 - val_loss: 0.8196 - val_acc: 0.5600\n",
            "Epoch 138/300\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.3873 - acc: 0.8242 - val_loss: 0.8558 - val_acc: 0.5500\n",
            "Epoch 139/300\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.3855 - acc: 0.8242 - val_loss: 0.8093 - val_acc: 0.5600\n",
            "Epoch 140/300\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.3723 - acc: 0.8210 - val_loss: 0.7040 - val_acc: 0.6100\n",
            "Epoch 141/300\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.4028 - acc: 0.8113 - val_loss: 0.7008 - val_acc: 0.5800\n",
            "Epoch 142/300\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.3883 - acc: 0.8435 - val_loss: 0.9552 - val_acc: 0.5300\n",
            "Epoch 143/300\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.4213 - acc: 0.8048 - val_loss: 0.8165 - val_acc: 0.5600\n",
            "Epoch 144/300\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.3901 - acc: 0.8290 - val_loss: 0.8542 - val_acc: 0.5500\n",
            "Epoch 145/300\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.4078 - acc: 0.8210 - val_loss: 0.7898 - val_acc: 0.5600\n",
            "Epoch 146/300\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.4146 - acc: 0.7887 - val_loss: 0.7507 - val_acc: 0.5700\n",
            "Epoch 147/300\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.4062 - acc: 0.8242 - val_loss: 0.7538 - val_acc: 0.6000\n",
            "Epoch 148/300\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.4287 - acc: 0.7952 - val_loss: 0.7155 - val_acc: 0.6000\n",
            "Epoch 149/300\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.3881 - acc: 0.8161 - val_loss: 0.7583 - val_acc: 0.5900\n",
            "Epoch 150/300\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.4045 - acc: 0.8048 - val_loss: 0.7038 - val_acc: 0.6000\n",
            "Epoch 151/300\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.3744 - acc: 0.8371 - val_loss: 0.9045 - val_acc: 0.5200\n",
            "Epoch 152/300\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.3920 - acc: 0.8258 - val_loss: 0.7901 - val_acc: 0.5400\n",
            "Epoch 153/300\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.4225 - acc: 0.8000 - val_loss: 0.8209 - val_acc: 0.5400\n",
            "Epoch 154/300\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.3776 - acc: 0.8306 - val_loss: 0.8435 - val_acc: 0.5500\n",
            "Epoch 155/300\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.3683 - acc: 0.8613 - val_loss: 0.9258 - val_acc: 0.5300\n",
            "Epoch 156/300\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.3733 - acc: 0.8613 - val_loss: 0.8121 - val_acc: 0.5500\n",
            "Epoch 157/300\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.4000 - acc: 0.8177 - val_loss: 0.7484 - val_acc: 0.5800\n",
            "Epoch 158/300\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.3824 - acc: 0.8419 - val_loss: 0.9027 - val_acc: 0.5500\n",
            "Epoch 159/300\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.3986 - acc: 0.8323 - val_loss: 0.7726 - val_acc: 0.5700\n",
            "Epoch 160/300\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.3796 - acc: 0.8194 - val_loss: 0.8123 - val_acc: 0.5700\n",
            "Epoch 161/300\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.3849 - acc: 0.8242 - val_loss: 0.8338 - val_acc: 0.5900\n",
            "Epoch 162/300\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.3858 - acc: 0.8355 - val_loss: 0.8004 - val_acc: 0.5500\n",
            "Epoch 163/300\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.62265\n",
            "620/620 - 1s - loss: 0.3833 - acc: 0.8339 - val_loss: 0.6444 - val_acc: 0.6400\n",
            "Epoch 164/300\n",
            "\n",
            "Epoch 00164: val_loss improved from 0.62265 to 0.56964, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.3821 - acc: 0.8274 - val_loss: 0.5696 - val_acc: 0.6900\n",
            "Epoch 165/300\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.56964\n",
            "620/620 - 1s - loss: 0.3819 - acc: 0.8371 - val_loss: 0.7697 - val_acc: 0.5800\n",
            "Epoch 166/300\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 0.56964\n",
            "620/620 - 1s - loss: 0.3765 - acc: 0.8355 - val_loss: 0.9670 - val_acc: 0.5200\n",
            "Epoch 167/300\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.56964\n",
            "620/620 - 1s - loss: 0.3978 - acc: 0.8097 - val_loss: 0.8034 - val_acc: 0.5500\n",
            "Epoch 168/300\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.56964\n",
            "620/620 - 1s - loss: 0.3729 - acc: 0.8419 - val_loss: 0.8471 - val_acc: 0.5600\n",
            "Epoch 169/300\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 0.56964\n",
            "620/620 - 1s - loss: 0.3997 - acc: 0.8065 - val_loss: 0.7721 - val_acc: 0.5700\n",
            "Epoch 170/300\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.56964\n",
            "620/620 - 1s - loss: 0.3892 - acc: 0.8323 - val_loss: 0.8104 - val_acc: 0.5600\n",
            "Epoch 171/300\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.56964\n",
            "620/620 - 1s - loss: 0.3919 - acc: 0.8226 - val_loss: 0.7672 - val_acc: 0.5800\n",
            "Epoch 172/300\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.56964\n",
            "620/620 - 1s - loss: 0.3645 - acc: 0.8403 - val_loss: 0.7724 - val_acc: 0.6000\n",
            "Epoch 173/300\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.56964\n",
            "620/620 - 1s - loss: 0.3896 - acc: 0.8387 - val_loss: 0.7537 - val_acc: 0.6200\n",
            "Epoch 174/300\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.56964\n",
            "620/620 - 1s - loss: 0.3996 - acc: 0.8210 - val_loss: 0.8041 - val_acc: 0.5600\n",
            "Epoch 175/300\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.56964\n",
            "620/620 - 1s - loss: 0.3899 - acc: 0.8258 - val_loss: 0.7209 - val_acc: 0.6300\n",
            "Epoch 176/300\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 0.56964\n",
            "620/620 - 1s - loss: 0.3820 - acc: 0.8274 - val_loss: 0.7227 - val_acc: 0.6400\n",
            "Epoch 177/300\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.56964\n",
            "620/620 - 1s - loss: 0.3825 - acc: 0.8306 - val_loss: 0.9058 - val_acc: 0.5300\n",
            "Epoch 178/300\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.56964\n",
            "620/620 - 1s - loss: 0.4011 - acc: 0.8177 - val_loss: 0.7435 - val_acc: 0.5700\n",
            "Epoch 179/300\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.56964\n",
            "620/620 - 1s - loss: 0.3820 - acc: 0.8177 - val_loss: 0.9240 - val_acc: 0.5500\n",
            "Epoch 180/300\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.56964\n",
            "620/620 - 1s - loss: 0.3822 - acc: 0.8258 - val_loss: 0.8175 - val_acc: 0.5500\n",
            "Epoch 181/300\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.56964\n",
            "620/620 - 1s - loss: 0.3984 - acc: 0.8161 - val_loss: 0.8880 - val_acc: 0.5100\n",
            "Epoch 182/300\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.56964\n",
            "620/620 - 1s - loss: 0.4050 - acc: 0.8113 - val_loss: 0.8335 - val_acc: 0.5200\n",
            "Epoch 183/300\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.56964\n",
            "620/620 - 1s - loss: 0.3870 - acc: 0.8290 - val_loss: 0.8553 - val_acc: 0.5300\n",
            "Epoch 184/300\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 0.56964\n",
            "620/620 - 1s - loss: 0.3938 - acc: 0.8242 - val_loss: 0.7382 - val_acc: 0.6200\n",
            "Epoch 185/300\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.56964\n",
            "620/620 - 1s - loss: 0.3784 - acc: 0.8387 - val_loss: 0.8450 - val_acc: 0.5700\n",
            "Epoch 186/300\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.56964\n",
            "620/620 - 1s - loss: 0.4030 - acc: 0.8161 - val_loss: 0.8756 - val_acc: 0.5300\n",
            "Epoch 187/300\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 0.56964\n",
            "620/620 - 1s - loss: 0.3705 - acc: 0.8387 - val_loss: 1.0070 - val_acc: 0.5200\n",
            "Epoch 188/300\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.56964\n",
            "620/620 - 1s - loss: 0.3788 - acc: 0.8371 - val_loss: 0.9052 - val_acc: 0.5200\n",
            "Epoch 189/300\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.56964\n",
            "620/620 - 1s - loss: 0.3659 - acc: 0.8323 - val_loss: 0.7572 - val_acc: 0.6200\n",
            "Epoch 190/300\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.56964\n",
            "620/620 - 1s - loss: 0.3775 - acc: 0.8403 - val_loss: 0.6946 - val_acc: 0.6400\n",
            "Epoch 191/300\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.56964\n",
            "620/620 - 1s - loss: 0.3599 - acc: 0.8371 - val_loss: 0.9353 - val_acc: 0.5300\n",
            "Epoch 192/300\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 0.56964\n",
            "620/620 - 1s - loss: 0.3633 - acc: 0.8387 - val_loss: 0.9030 - val_acc: 0.5300\n",
            "Epoch 193/300\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.56964\n",
            "620/620 - 1s - loss: 0.3847 - acc: 0.8177 - val_loss: 0.8550 - val_acc: 0.5500\n",
            "Epoch 194/300\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.56964\n",
            "620/620 - 1s - loss: 0.3586 - acc: 0.8468 - val_loss: 0.9197 - val_acc: 0.5700\n",
            "Epoch 195/300\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.56964\n",
            "620/620 - 1s - loss: 0.3761 - acc: 0.8274 - val_loss: 0.7788 - val_acc: 0.6000\n",
            "Epoch 196/300\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.56964\n",
            "620/620 - 1s - loss: 0.3662 - acc: 0.8548 - val_loss: 0.7036 - val_acc: 0.6600\n",
            "Epoch 197/300\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.56964\n",
            "620/620 - 1s - loss: 0.3770 - acc: 0.8371 - val_loss: 0.8321 - val_acc: 0.5600\n",
            "Epoch 198/300\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.56964\n",
            "620/620 - 1s - loss: 0.3667 - acc: 0.8435 - val_loss: 0.8545 - val_acc: 0.5800\n",
            "Epoch 199/300\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.56964\n",
            "620/620 - 1s - loss: 0.3996 - acc: 0.8032 - val_loss: 0.7959 - val_acc: 0.5600\n",
            "Epoch 200/300\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.56964\n",
            "620/620 - 1s - loss: 0.3890 - acc: 0.8242 - val_loss: 0.7800 - val_acc: 0.5600\n",
            "Epoch 201/300\n",
            "\n",
            "Epoch 00201: val_loss did not improve from 0.56964\n",
            "620/620 - 1s - loss: 0.3891 - acc: 0.8258 - val_loss: 0.7873 - val_acc: 0.5500\n",
            "Epoch 202/300\n",
            "\n",
            "Epoch 00202: val_loss did not improve from 0.56964\n",
            "620/620 - 1s - loss: 0.3575 - acc: 0.8468 - val_loss: 0.8576 - val_acc: 0.5700\n",
            "Epoch 203/300\n",
            "\n",
            "Epoch 00203: val_loss did not improve from 0.56964\n",
            "620/620 - 1s - loss: 0.4082 - acc: 0.8129 - val_loss: 0.7446 - val_acc: 0.6100\n",
            "Epoch 204/300\n",
            "\n",
            "Epoch 00204: val_loss did not improve from 0.56964\n",
            "620/620 - 1s - loss: 0.3787 - acc: 0.8403 - val_loss: 0.8309 - val_acc: 0.5800\n",
            "Epoch 205/300\n",
            "\n",
            "Epoch 00205: val_loss did not improve from 0.56964\n",
            "620/620 - 1s - loss: 0.3788 - acc: 0.8306 - val_loss: 0.7491 - val_acc: 0.6100\n",
            "Epoch 206/300\n",
            "\n",
            "Epoch 00206: val_loss did not improve from 0.56964\n",
            "620/620 - 1s - loss: 0.3956 - acc: 0.8306 - val_loss: 0.8236 - val_acc: 0.5500\n",
            "Epoch 207/300\n",
            "\n",
            "Epoch 00207: val_loss did not improve from 0.56964\n",
            "620/620 - 1s - loss: 0.3956 - acc: 0.8323 - val_loss: 0.7554 - val_acc: 0.6000\n",
            "Epoch 208/300\n",
            "\n",
            "Epoch 00208: val_loss did not improve from 0.56964\n",
            "620/620 - 1s - loss: 0.3737 - acc: 0.8226 - val_loss: 0.8861 - val_acc: 0.5700\n",
            "Epoch 209/300\n",
            "\n",
            "Epoch 00209: val_loss did not improve from 0.56964\n",
            "620/620 - 1s - loss: 0.3826 - acc: 0.8258 - val_loss: 0.8807 - val_acc: 0.5500\n",
            "Epoch 210/300\n",
            "\n",
            "Epoch 00210: val_loss did not improve from 0.56964\n",
            "620/620 - 1s - loss: 0.3689 - acc: 0.8290 - val_loss: 0.9573 - val_acc: 0.5200\n",
            "Epoch 211/300\n",
            "\n",
            "Epoch 00211: val_loss did not improve from 0.56964\n",
            "620/620 - 1s - loss: 0.3836 - acc: 0.8290 - val_loss: 0.7750 - val_acc: 0.6100\n",
            "Epoch 212/300\n",
            "\n",
            "Epoch 00212: val_loss did not improve from 0.56964\n",
            "620/620 - 1s - loss: 0.3611 - acc: 0.8403 - val_loss: 0.8178 - val_acc: 0.5800\n",
            "Epoch 213/300\n",
            "\n",
            "Epoch 00213: val_loss did not improve from 0.56964\n",
            "620/620 - 1s - loss: 0.3947 - acc: 0.8129 - val_loss: 0.8245 - val_acc: 0.5500\n",
            "Epoch 214/300\n",
            "\n",
            "Epoch 00214: val_loss did not improve from 0.56964\n",
            "620/620 - 1s - loss: 0.3911 - acc: 0.8323 - val_loss: 0.8794 - val_acc: 0.5300\n",
            "Epoch 215/300\n",
            "\n",
            "Epoch 00215: val_loss did not improve from 0.56964\n",
            "620/620 - 1s - loss: 0.3837 - acc: 0.8161 - val_loss: 0.8916 - val_acc: 0.5200\n",
            "Epoch 216/300\n",
            "\n",
            "Epoch 00216: val_loss did not improve from 0.56964\n",
            "620/620 - 1s - loss: 0.3740 - acc: 0.8323 - val_loss: 0.7544 - val_acc: 0.6000\n",
            "Epoch 217/300\n",
            "\n",
            "Epoch 00217: val_loss did not improve from 0.56964\n",
            "620/620 - 1s - loss: 0.3786 - acc: 0.8290 - val_loss: 0.8924 - val_acc: 0.5300\n",
            "Epoch 218/300\n",
            "\n",
            "Epoch 00218: val_loss did not improve from 0.56964\n",
            "620/620 - 1s - loss: 0.3849 - acc: 0.8306 - val_loss: 0.7068 - val_acc: 0.6300\n",
            "Epoch 219/300\n",
            "\n",
            "Epoch 00219: val_loss did not improve from 0.56964\n",
            "620/620 - 1s - loss: 0.3798 - acc: 0.8403 - val_loss: 0.6816 - val_acc: 0.6400\n",
            "Epoch 220/300\n",
            "\n",
            "Epoch 00220: val_loss did not improve from 0.56964\n",
            "620/620 - 1s - loss: 0.3762 - acc: 0.8532 - val_loss: 0.8497 - val_acc: 0.5400\n",
            "Epoch 221/300\n",
            "\n",
            "Epoch 00221: val_loss did not improve from 0.56964\n",
            "620/620 - 1s - loss: 0.3692 - acc: 0.8355 - val_loss: 0.8407 - val_acc: 0.5700\n",
            "Epoch 222/300\n",
            "\n",
            "Epoch 00222: val_loss did not improve from 0.56964\n",
            "620/620 - 1s - loss: 0.3486 - acc: 0.8452 - val_loss: 0.7042 - val_acc: 0.6300\n",
            "Epoch 223/300\n",
            "\n",
            "Epoch 00223: val_loss did not improve from 0.56964\n",
            "620/620 - 1s - loss: 0.3872 - acc: 0.8290 - val_loss: 0.9316 - val_acc: 0.5300\n",
            "Epoch 224/300\n",
            "\n",
            "Epoch 00224: val_loss did not improve from 0.56964\n",
            "620/620 - 1s - loss: 0.3690 - acc: 0.8435 - val_loss: 0.7865 - val_acc: 0.5900\n",
            "Epoch 225/300\n",
            "\n",
            "Epoch 00225: val_loss did not improve from 0.56964\n",
            "620/620 - 1s - loss: 0.3733 - acc: 0.8419 - val_loss: 0.8594 - val_acc: 0.5800\n",
            "Epoch 226/300\n",
            "\n",
            "Epoch 00226: val_loss did not improve from 0.56964\n",
            "620/620 - 1s - loss: 0.3806 - acc: 0.8419 - val_loss: 0.8951 - val_acc: 0.5400\n",
            "Epoch 227/300\n",
            "\n",
            "Epoch 00227: val_loss did not improve from 0.56964\n",
            "620/620 - 1s - loss: 0.3780 - acc: 0.8371 - val_loss: 0.8268 - val_acc: 0.5300\n",
            "Epoch 228/300\n",
            "\n",
            "Epoch 00228: val_loss did not improve from 0.56964\n",
            "620/620 - 1s - loss: 0.3638 - acc: 0.8452 - val_loss: 0.8307 - val_acc: 0.5900\n",
            "Epoch 229/300\n",
            "\n",
            "Epoch 00229: val_loss did not improve from 0.56964\n",
            "620/620 - 1s - loss: 0.3566 - acc: 0.8629 - val_loss: 0.9590 - val_acc: 0.5300\n",
            "Epoch 230/300\n",
            "\n",
            "Epoch 00230: val_loss did not improve from 0.56964\n",
            "620/620 - 1s - loss: 0.3873 - acc: 0.8323 - val_loss: 0.7326 - val_acc: 0.5800\n",
            "Epoch 231/300\n",
            "\n",
            "Epoch 00231: val_loss did not improve from 0.56964\n",
            "620/620 - 1s - loss: 0.4142 - acc: 0.8161 - val_loss: 0.8016 - val_acc: 0.5900\n",
            "Epoch 232/300\n",
            "\n",
            "Epoch 00232: val_loss did not improve from 0.56964\n",
            "620/620 - 1s - loss: 0.3621 - acc: 0.8306 - val_loss: 0.8210 - val_acc: 0.5800\n",
            "Epoch 233/300\n",
            "\n",
            "Epoch 00233: val_loss did not improve from 0.56964\n",
            "620/620 - 1s - loss: 0.3789 - acc: 0.8323 - val_loss: 0.8609 - val_acc: 0.5400\n",
            "Epoch 234/300\n",
            "\n",
            "Epoch 00234: val_loss did not improve from 0.56964\n",
            "620/620 - 1s - loss: 0.3701 - acc: 0.8355 - val_loss: 0.8978 - val_acc: 0.5100\n",
            "Epoch 235/300\n",
            "\n",
            "Epoch 00235: val_loss did not improve from 0.56964\n",
            "620/620 - 1s - loss: 0.3706 - acc: 0.8210 - val_loss: 0.8259 - val_acc: 0.6000\n",
            "Epoch 236/300\n",
            "\n",
            "Epoch 00236: val_loss did not improve from 0.56964\n",
            "620/620 - 1s - loss: 0.3491 - acc: 0.8548 - val_loss: 0.8885 - val_acc: 0.5400\n",
            "Epoch 237/300\n",
            "\n",
            "Epoch 00237: val_loss did not improve from 0.56964\n",
            "620/620 - 1s - loss: 0.3606 - acc: 0.8387 - val_loss: 0.7589 - val_acc: 0.6000\n",
            "Epoch 238/300\n",
            "\n",
            "Epoch 00238: val_loss did not improve from 0.56964\n",
            "620/620 - 1s - loss: 0.3576 - acc: 0.8435 - val_loss: 0.9617 - val_acc: 0.5200\n",
            "Epoch 239/300\n",
            "\n",
            "Epoch 00239: val_loss did not improve from 0.56964\n",
            "620/620 - 1s - loss: 0.3608 - acc: 0.8484 - val_loss: 0.7666 - val_acc: 0.5900\n",
            "Epoch 240/300\n",
            "\n",
            "Epoch 00240: val_loss did not improve from 0.56964\n",
            "620/620 - 1s - loss: 0.3728 - acc: 0.8242 - val_loss: 0.7745 - val_acc: 0.5800\n",
            "Epoch 241/300\n",
            "\n",
            "Epoch 00241: val_loss did not improve from 0.56964\n",
            "620/620 - 1s - loss: 0.3712 - acc: 0.8435 - val_loss: 0.8510 - val_acc: 0.5800\n",
            "Epoch 242/300\n",
            "\n",
            "Epoch 00242: val_loss did not improve from 0.56964\n",
            "620/620 - 1s - loss: 0.3686 - acc: 0.8306 - val_loss: 0.7688 - val_acc: 0.6000\n",
            "Epoch 243/300\n",
            "\n",
            "Epoch 00243: val_loss did not improve from 0.56964\n",
            "620/620 - 1s - loss: 0.3890 - acc: 0.8242 - val_loss: 0.9371 - val_acc: 0.5200\n",
            "Epoch 244/300\n",
            "\n",
            "Epoch 00244: val_loss did not improve from 0.56964\n",
            "620/620 - 1s - loss: 0.3625 - acc: 0.8500 - val_loss: 0.9848 - val_acc: 0.5100\n",
            "Epoch 245/300\n",
            "\n",
            "Epoch 00245: val_loss did not improve from 0.56964\n",
            "620/620 - 1s - loss: 0.3702 - acc: 0.8452 - val_loss: 0.8371 - val_acc: 0.5400\n",
            "Epoch 246/300\n",
            "\n",
            "Epoch 00246: val_loss did not improve from 0.56964\n",
            "620/620 - 1s - loss: 0.3570 - acc: 0.8565 - val_loss: 0.8695 - val_acc: 0.5700\n",
            "Epoch 247/300\n",
            "\n",
            "Epoch 00247: val_loss did not improve from 0.56964\n",
            "620/620 - 1s - loss: 0.3611 - acc: 0.8468 - val_loss: 0.7665 - val_acc: 0.5800\n",
            "Epoch 248/300\n",
            "\n",
            "Epoch 00248: val_loss did not improve from 0.56964\n",
            "620/620 - 1s - loss: 0.3654 - acc: 0.8500 - val_loss: 0.8803 - val_acc: 0.5300\n",
            "Epoch 249/300\n",
            "\n",
            "Epoch 00249: val_loss did not improve from 0.56964\n",
            "620/620 - 1s - loss: 0.3783 - acc: 0.8371 - val_loss: 0.8622 - val_acc: 0.5700\n",
            "Epoch 250/300\n",
            "\n",
            "Epoch 00250: val_loss did not improve from 0.56964\n",
            "620/620 - 1s - loss: 0.3584 - acc: 0.8468 - val_loss: 0.8349 - val_acc: 0.5700\n",
            "Epoch 251/300\n",
            "\n",
            "Epoch 00251: val_loss did not improve from 0.56964\n",
            "620/620 - 1s - loss: 0.3873 - acc: 0.8355 - val_loss: 0.8303 - val_acc: 0.5900\n",
            "Epoch 252/300\n",
            "\n",
            "Epoch 00252: val_loss did not improve from 0.56964\n",
            "620/620 - 1s - loss: 0.3580 - acc: 0.8629 - val_loss: 0.7523 - val_acc: 0.6100\n",
            "Epoch 253/300\n",
            "\n",
            "Epoch 00253: val_loss did not improve from 0.56964\n",
            "620/620 - 1s - loss: 0.3917 - acc: 0.8258 - val_loss: 0.9254 - val_acc: 0.5100\n",
            "Epoch 254/300\n",
            "\n",
            "Epoch 00254: val_loss did not improve from 0.56964\n",
            "620/620 - 1s - loss: 0.3991 - acc: 0.8016 - val_loss: 0.7418 - val_acc: 0.6000\n",
            "Epoch 255/300\n",
            "\n",
            "Epoch 00255: val_loss did not improve from 0.56964\n",
            "620/620 - 1s - loss: 0.3467 - acc: 0.8355 - val_loss: 0.8188 - val_acc: 0.5700\n",
            "Epoch 256/300\n",
            "\n",
            "Epoch 00256: val_loss did not improve from 0.56964\n",
            "620/620 - 1s - loss: 0.3583 - acc: 0.8403 - val_loss: 0.9356 - val_acc: 0.5200\n",
            "Epoch 257/300\n",
            "\n",
            "Epoch 00257: val_loss did not improve from 0.56964\n",
            "620/620 - 1s - loss: 0.3589 - acc: 0.8452 - val_loss: 0.8264 - val_acc: 0.6000\n",
            "Epoch 258/300\n",
            "\n",
            "Epoch 00258: val_loss did not improve from 0.56964\n",
            "620/620 - 1s - loss: 0.3624 - acc: 0.8435 - val_loss: 0.8282 - val_acc: 0.5500\n",
            "Epoch 259/300\n",
            "\n",
            "Epoch 00259: val_loss did not improve from 0.56964\n",
            "620/620 - 1s - loss: 0.3407 - acc: 0.8532 - val_loss: 0.8710 - val_acc: 0.5500\n",
            "Epoch 260/300\n",
            "\n",
            "Epoch 00260: val_loss did not improve from 0.56964\n",
            "620/620 - 1s - loss: 0.3735 - acc: 0.8306 - val_loss: 0.8300 - val_acc: 0.6100\n",
            "Epoch 261/300\n",
            "\n",
            "Epoch 00261: val_loss did not improve from 0.56964\n",
            "620/620 - 1s - loss: 0.3746 - acc: 0.8355 - val_loss: 0.8176 - val_acc: 0.6000\n",
            "Epoch 262/300\n",
            "\n",
            "Epoch 00262: val_loss did not improve from 0.56964\n",
            "620/620 - 1s - loss: 0.3503 - acc: 0.8435 - val_loss: 0.8312 - val_acc: 0.6100\n",
            "Epoch 263/300\n",
            "\n",
            "Epoch 00263: val_loss did not improve from 0.56964\n",
            "620/620 - 1s - loss: 0.3561 - acc: 0.8516 - val_loss: 0.7240 - val_acc: 0.6200\n",
            "Epoch 264/300\n",
            "\n",
            "Epoch 00264: val_loss did not improve from 0.56964\n",
            "620/620 - 1s - loss: 0.3786 - acc: 0.8387 - val_loss: 0.8912 - val_acc: 0.5400\n",
            "Epoch 265/300\n",
            "\n",
            "Epoch 00265: val_loss did not improve from 0.56964\n",
            "620/620 - 1s - loss: 0.3681 - acc: 0.8435 - val_loss: 0.8808 - val_acc: 0.5700\n",
            "Epoch 266/300\n",
            "\n",
            "Epoch 00266: val_loss did not improve from 0.56964\n",
            "620/620 - 1s - loss: 0.3723 - acc: 0.8355 - val_loss: 0.8943 - val_acc: 0.5400\n",
            "Epoch 267/300\n",
            "\n",
            "Epoch 00267: val_loss did not improve from 0.56964\n",
            "620/620 - 1s - loss: 0.3700 - acc: 0.8419 - val_loss: 0.8036 - val_acc: 0.6200\n",
            "Epoch 268/300\n",
            "\n",
            "Epoch 00268: val_loss did not improve from 0.56964\n",
            "620/620 - 1s - loss: 0.3700 - acc: 0.8435 - val_loss: 0.8419 - val_acc: 0.5200\n",
            "Epoch 269/300\n",
            "\n",
            "Epoch 00269: val_loss did not improve from 0.56964\n",
            "620/620 - 1s - loss: 0.3640 - acc: 0.8565 - val_loss: 0.7165 - val_acc: 0.6200\n",
            "Epoch 270/300\n",
            "\n",
            "Epoch 00270: val_loss did not improve from 0.56964\n",
            "620/620 - 1s - loss: 0.3512 - acc: 0.8452 - val_loss: 0.8884 - val_acc: 0.5700\n",
            "Epoch 271/300\n",
            "\n",
            "Epoch 00271: val_loss did not improve from 0.56964\n",
            "620/620 - 1s - loss: 0.3489 - acc: 0.8468 - val_loss: 0.9169 - val_acc: 0.5700\n",
            "Epoch 272/300\n",
            "\n",
            "Epoch 00272: val_loss did not improve from 0.56964\n",
            "620/620 - 1s - loss: 0.3645 - acc: 0.8306 - val_loss: 0.8991 - val_acc: 0.5600\n",
            "Epoch 273/300\n",
            "\n",
            "Epoch 00273: val_loss did not improve from 0.56964\n",
            "620/620 - 1s - loss: 0.3523 - acc: 0.8339 - val_loss: 0.9943 - val_acc: 0.5300\n",
            "Epoch 274/300\n",
            "\n",
            "Epoch 00274: val_loss did not improve from 0.56964\n",
            "620/620 - 1s - loss: 0.3683 - acc: 0.8290 - val_loss: 0.7340 - val_acc: 0.6200\n",
            "Epoch 275/300\n",
            "\n",
            "Epoch 00275: val_loss did not improve from 0.56964\n",
            "620/620 - 1s - loss: 0.3579 - acc: 0.8484 - val_loss: 0.7915 - val_acc: 0.5800\n",
            "Epoch 276/300\n",
            "\n",
            "Epoch 00276: val_loss did not improve from 0.56964\n",
            "620/620 - 1s - loss: 0.3927 - acc: 0.8355 - val_loss: 0.9017 - val_acc: 0.5400\n",
            "Epoch 277/300\n",
            "\n",
            "Epoch 00277: val_loss did not improve from 0.56964\n",
            "620/620 - 1s - loss: 0.3536 - acc: 0.8548 - val_loss: 0.9015 - val_acc: 0.5500\n",
            "Epoch 278/300\n",
            "\n",
            "Epoch 00278: val_loss did not improve from 0.56964\n",
            "620/620 - 1s - loss: 0.3768 - acc: 0.8339 - val_loss: 0.8846 - val_acc: 0.5300\n",
            "Epoch 279/300\n",
            "\n",
            "Epoch 00279: val_loss did not improve from 0.56964\n",
            "620/620 - 1s - loss: 0.3555 - acc: 0.8419 - val_loss: 0.8791 - val_acc: 0.5600\n",
            "Epoch 280/300\n",
            "\n",
            "Epoch 00280: val_loss did not improve from 0.56964\n",
            "620/620 - 1s - loss: 0.3772 - acc: 0.8435 - val_loss: 0.8725 - val_acc: 0.5600\n",
            "Epoch 281/300\n",
            "\n",
            "Epoch 00281: val_loss did not improve from 0.56964\n",
            "620/620 - 1s - loss: 0.3361 - acc: 0.8484 - val_loss: 0.9564 - val_acc: 0.5400\n",
            "Epoch 282/300\n",
            "\n",
            "Epoch 00282: val_loss did not improve from 0.56964\n",
            "620/620 - 1s - loss: 0.3706 - acc: 0.8371 - val_loss: 0.8364 - val_acc: 0.5600\n",
            "Epoch 283/300\n",
            "\n",
            "Epoch 00283: val_loss did not improve from 0.56964\n",
            "620/620 - 1s - loss: 0.3621 - acc: 0.8435 - val_loss: 0.6800 - val_acc: 0.6900\n",
            "Epoch 284/300\n",
            "\n",
            "Epoch 00284: val_loss did not improve from 0.56964\n",
            "620/620 - 1s - loss: 0.3539 - acc: 0.8371 - val_loss: 0.8288 - val_acc: 0.5700\n",
            "Epoch 285/300\n",
            "\n",
            "Epoch 00285: val_loss did not improve from 0.56964\n",
            "620/620 - 1s - loss: 0.3729 - acc: 0.8548 - val_loss: 0.7839 - val_acc: 0.5700\n",
            "Epoch 286/300\n",
            "\n",
            "Epoch 00286: val_loss did not improve from 0.56964\n",
            "620/620 - 1s - loss: 0.3778 - acc: 0.8516 - val_loss: 0.9522 - val_acc: 0.5700\n",
            "Epoch 287/300\n",
            "\n",
            "Epoch 00287: val_loss did not improve from 0.56964\n",
            "620/620 - 1s - loss: 0.3696 - acc: 0.8452 - val_loss: 1.1198 - val_acc: 0.5000\n",
            "Epoch 288/300\n",
            "\n",
            "Epoch 00288: val_loss did not improve from 0.56964\n",
            "620/620 - 1s - loss: 0.3766 - acc: 0.8419 - val_loss: 0.9649 - val_acc: 0.5700\n",
            "Epoch 289/300\n",
            "\n",
            "Epoch 00289: val_loss did not improve from 0.56964\n",
            "620/620 - 1s - loss: 0.3529 - acc: 0.8581 - val_loss: 0.7985 - val_acc: 0.5600\n",
            "Epoch 290/300\n",
            "\n",
            "Epoch 00290: val_loss did not improve from 0.56964\n",
            "620/620 - 1s - loss: 0.3307 - acc: 0.8710 - val_loss: 0.8077 - val_acc: 0.6000\n",
            "Epoch 291/300\n",
            "\n",
            "Epoch 00291: val_loss did not improve from 0.56964\n",
            "620/620 - 1s - loss: 0.3412 - acc: 0.8516 - val_loss: 0.7574 - val_acc: 0.6100\n",
            "Epoch 292/300\n",
            "\n",
            "Epoch 00292: val_loss did not improve from 0.56964\n",
            "620/620 - 1s - loss: 0.3639 - acc: 0.8419 - val_loss: 0.8580 - val_acc: 0.5600\n",
            "Epoch 293/300\n",
            "\n",
            "Epoch 00293: val_loss did not improve from 0.56964\n",
            "620/620 - 1s - loss: 0.3530 - acc: 0.8548 - val_loss: 0.7110 - val_acc: 0.6500\n",
            "Epoch 294/300\n",
            "\n",
            "Epoch 00294: val_loss did not improve from 0.56964\n",
            "620/620 - 1s - loss: 0.3729 - acc: 0.8468 - val_loss: 0.9144 - val_acc: 0.5300\n",
            "Epoch 295/300\n",
            "\n",
            "Epoch 00295: val_loss did not improve from 0.56964\n",
            "620/620 - 1s - loss: 0.3476 - acc: 0.8581 - val_loss: 0.9353 - val_acc: 0.5500\n",
            "Epoch 296/300\n",
            "\n",
            "Epoch 00296: val_loss did not improve from 0.56964\n",
            "620/620 - 1s - loss: 0.3383 - acc: 0.8452 - val_loss: 0.9541 - val_acc: 0.5600\n",
            "Epoch 297/300\n",
            "\n",
            "Epoch 00297: val_loss did not improve from 0.56964\n",
            "620/620 - 1s - loss: 0.3577 - acc: 0.8419 - val_loss: 0.8515 - val_acc: 0.5800\n",
            "Epoch 298/300\n",
            "\n",
            "Epoch 00298: val_loss did not improve from 0.56964\n",
            "620/620 - 1s - loss: 0.3398 - acc: 0.8500 - val_loss: 0.8265 - val_acc: 0.5900\n",
            "Epoch 299/300\n",
            "\n",
            "Epoch 00299: val_loss did not improve from 0.56964\n",
            "620/620 - 1s - loss: 0.3492 - acc: 0.8565 - val_loss: 0.8267 - val_acc: 0.5900\n",
            "Epoch 300/300\n",
            "\n",
            "Epoch 00300: val_loss did not improve from 0.56964\n",
            "620/620 - 1s - loss: 0.3534 - acc: 0.8500 - val_loss: 0.8038 - val_acc: 0.5600\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "# Evaluate on test data\n",
            "40/40 [==============================] - 0s 3ms/sample - loss: 0.9724 - acc: 0.5250\n",
            "test loss, test acc: [0.9724482974968851, 0.525]\n",
            "[[0.67857946]\n",
            " [0.60459529]\n",
            " [0.61861274]\n",
            " [0.62621685]\n",
            " [0.45206004]\n",
            " [0.9724483 ]\n",
            " [0.97404507]\n",
            " [0.8812488 ]\n",
            " [0.74356118]\n",
            " [0.64741272]]\n",
            "[[0.625     ]\n",
            " [0.72500002]\n",
            " [0.67500001]\n",
            " [0.625     ]\n",
            " [0.75      ]\n",
            " [0.52499998]\n",
            " [0.5       ]\n",
            " [0.57499999]\n",
            " [0.57499999]\n",
            " [0.64999998]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kd6FTGs7vBI3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "df_accl_all = pd.DataFrame({'Class1vs2': acc_all[:, 0]})\n",
        "df_accl_all.to_csv (r'EEG_Deep/df_accl_Cross_Patient_8_24_2560:4096.csv', index = None, header=True) #Don't forget to add '.csv' at the end of the path\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}