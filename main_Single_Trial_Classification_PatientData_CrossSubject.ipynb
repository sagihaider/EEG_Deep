{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "main_Single_Trial_Classification_PatientData_CrossSubject",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sagihaider/EEG_Deep/blob/master/main_Single_Trial_Classification_PatientData_CrossSubject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X110PE9MjrlE",
        "colab_type": "code",
        "outputId": "2b53d46c-17ea-48c8-ad1a-6f50304d01e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        }
      },
      "source": [
        "!git clone https://github.com/sagihaider/EEG_Deep.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'EEG_Deep'...\n",
            "remote: Enumerating objects: 28, done.\u001b[K\n",
            "remote: Counting objects: 100% (28/28), done.\u001b[K\n",
            "remote: Compressing objects: 100% (27/27), done.\u001b[K\n",
            "remote: Total 181 (delta 13), reused 5 (delta 1), pack-reused 153\u001b[K\n",
            "Receiving objects: 100% (181/181), 856.92 MiB | 33.77 MiB/s, done.\n",
            "Resolving deltas: 100% (66/66), done.\n",
            "Checking out files: 100% (59/59), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ioLzMoiS60Qf",
        "colab_type": "code",
        "outputId": "959def0c-5ec3-489c-b784-1aea68b6176f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 65
        }
      },
      "source": [
        "import scipy.io as spio\n",
        "import numpy as np\n",
        "from importlib.machinery import SourceFileLoader\n",
        "\n",
        "# EEGNet-specific imports\n",
        "from EEG_Deep.EEGModels import EEGNet, ShallowConvNet, DeepConvNet\n",
        "from tensorflow.keras import utils as np_utils\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras import backend as K\n",
        "K.set_image_data_format('channels_first')\n",
        "\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# tools for plotting confusion matrices\n",
        "from matplotlib import pyplot as plt\n",
        "from scipy.signal import butter, lfilter\n",
        "\n",
        "from numpy import array\n",
        "from sklearn.model_selection import KFold"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mo80Jg_Pn5lP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Band-pass Filter\n",
        "def butter_bandpass_filter(data, lowcut, highcut, fs, order=4):\n",
        "    nyq = 0.5 * fs\n",
        "    low = lowcut / nyq\n",
        "    high = highcut / nyq\n",
        "    b, a = butter(order, [low, high], btype='band')\n",
        "    y = lfilter(b, a, data)\n",
        "    return y\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZjtnE_6RSpum",
        "colab_type": "code",
        "outputId": "2c797bf8-60b2-41d5-b3bf-6e5781b3cef7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from numpy import zeros\n",
        "K.clear_session()\n",
        "cols = 1\n",
        "rows = 10\n",
        "acc_all = zeros([rows, cols])\n",
        "loss_all = zeros([rows, cols])\n",
        "result=[]\n",
        "count = 0\n",
        "# data sample\n",
        "data = array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
        "# prepare cross validation\n",
        "kfold = KFold(10, True, 1)\n",
        "# enumerate splits\n",
        "for train, test in kfold.split(data):\n",
        "  count = count + 1\n",
        "  # print('train: %s, test: %s' % (data[train], data[test]))\n",
        "  r_X_tr = np.empty([0, 12, 4096])\n",
        "  Y_tr = np.empty([0,1])\n",
        "  r_X_ts = np.empty([0, 12, 4096])\n",
        "  Y_ts = np.empty([0,1])\n",
        "  X_tr = np.empty([720, 12, 4096])\n",
        "  X_ts = np.empty([40, 12, 4096])\n",
        "  \n",
        "\n",
        "  for x in data[train]:\n",
        "    print(x)\n",
        "    fName = 'EEG_Deep/Data2A/parsed_P0' + str(x) + 'T.mat'  # Load Data\n",
        "    print(fName)\n",
        "    mat = spio.loadmat(fName)\n",
        "    x_tr = mat['RawEEGData']\n",
        "    y_tr = mat['Labels']\n",
        "    print(r_X_tr.shape)\n",
        "    r_X_tr=np.append(r_X_tr, x_tr, axis=0)\n",
        "    Y_tr=np.append(Y_tr, y_tr, axis=0)\n",
        "    print(r_X_tr.shape)\n",
        "    print(Y_tr.shape)\n",
        "\n",
        "\n",
        "  for x in data[test]:\n",
        "    print(x)\n",
        "    subid = x \n",
        "    fName = 'EEG_Deep/Data2A/parsed_P0' + str(x) + 'E.mat'  # Load Data\n",
        "    print(fName)\n",
        "    mat = spio.loadmat(fName)\n",
        "    x_ts = mat['RawEEGData']\n",
        "    y_ts = mat['Labels']\n",
        "    print(r_X_ts.shape)\n",
        "    r_X_ts=np.append(r_X_ts, x_ts, axis=0)\n",
        "    Y_ts=np.append(Y_ts, y_ts, axis=0)\n",
        "    print(r_X_ts.shape)\n",
        "    print(Y_ts.shape)\n",
        "\n",
        "\n",
        "  ### Filter Training Data ###\n",
        "  print(\"Filtering of training data in progress\")\n",
        "  for t in range(r_X_tr.shape[0]):\n",
        "    tril = r_X_tr[t,:,:]\n",
        "    # tril = tril.transpose()\n",
        "    tril_filtered = butter_bandpass_filter(tril, \n",
        "                                              lowcut=8, \n",
        "                                              highcut=24, \n",
        "                                              fs=512,\n",
        "                                              order=4)\n",
        "    # tril_filtered = tril_filtered.transpose()\n",
        "    X_tr[t,:,:] = tril_filtered\n",
        "  \n",
        "  print(X_tr.shape)\n",
        "\n",
        "  ### Filter Test Data Data ###\n",
        "  print(\"Filtering of test data in progress\")\n",
        "  for t in range(r_X_ts.shape[0]):\n",
        "    tril = r_X_ts[t,:,:]\n",
        "    # tril = tril.transpose()\n",
        "    tril_filtered = butter_bandpass_filter(tril, \n",
        "                                              lowcut=8, \n",
        "                                              highcut=24, \n",
        "                                              fs=512,\n",
        "                                              order=4)\n",
        "    # tril_filtered = tril_filtered.transpose()\n",
        "    X_ts[t,:,:] = tril_filtered\n",
        "\n",
        "  print(X_ts.shape)\n",
        "\n",
        "  # split data of each subject in training and validation\n",
        "  X_train = X_tr[0:620,:,2560:4096]\n",
        "  Y_train = Y_tr[0:620].ravel()\n",
        "  X_val   = X_tr[620:,:,2560:4096]\n",
        "  Y_val   = Y_tr[620:].ravel()\n",
        "  print(Y_val)\n",
        "  print(np.shape(X_train))\n",
        "  print(np.shape(Y_train))\n",
        "  print(np.shape(X_val))\n",
        "  print(np.shape(Y_val))\n",
        "\n",
        "  # convert labels to one-hot encodings.\n",
        "  Y_train      = np_utils.to_categorical(Y_train-1, num_classes=4)\n",
        "  Y_val       = np_utils.to_categorical(Y_val-1, num_classes=4)\n",
        "  print(Y_val)\n",
        "\n",
        "  kernels, chans, samples = 1, 12, 1536\n",
        "  # convert data to NCHW (trials, kernels, channels, samples) format. Data \n",
        "  # contains 22 channels and 500 time-points. Set the number of kernels to 1.\n",
        "  X_train = X_train.reshape(X_train.shape[0], kernels, chans, samples)\n",
        "  X_val   = X_val.reshape(X_val.shape[0], kernels, chans, samples)\n",
        "   \n",
        "  print('X_train shape:', X_train.shape)\n",
        "  print(X_train.shape[0], 'train samples')\n",
        "  print(X_val.shape[0], 'val samples')\n",
        "\n",
        "  X_test      = X_ts[:,:,2560:4096]\n",
        "  Y_test      = Y_ts[:]\n",
        "  print(np.shape(X_test))\n",
        "  print(np.shape(Y_test))\n",
        "\n",
        "  #convert labels to one-hot encodings.\n",
        "  Y_test      = np_utils.to_categorical(Y_test-1, num_classes=4)\n",
        "\n",
        "  # convert data to NCHW (trials, kernels, channels, samples) format. Data \n",
        "  # contains 22 channels and 500 time-points. Set the number of kernels to 1.\n",
        "  X_test = X_test.reshape(X_test.shape[0], kernels, chans, samples)\n",
        "  print('X_train shape:', X_test.shape)\n",
        "  print(X_test.shape[0], 'train samples')\n",
        "\n",
        "  # configure the EEGNet-8,2,16 model with kernel length of 32 samples (other \n",
        "  # model configurations may do better, but this is a good starting point)\n",
        "  model = EEGNet(nb_classes = 4, Chans = 12, Samples = 1536,\n",
        "                 dropoutRate = 0.5, kernLength = 25, F1 = 8, \n",
        "                 D = 2, F2 = 16, norm_rate = 0.25, dropoutType = 'Dropout')\n",
        "      \n",
        "  # compile the model and set the optimizers\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='adam', \n",
        "                metrics = ['accuracy'])\n",
        "\n",
        "  # count number of parameters in the model\n",
        "  numParams    = model.count_params() \n",
        "\n",
        "  # set a valid path for your system to record model checkpoints\n",
        "  checkpointer = ModelCheckpoint(filepath='/tmp/checkpoint.h5', verbose=1,\n",
        "                                 save_best_only=True)\n",
        "  \n",
        "  # the syntax is {class_1:weight_1, class_2:weight_2,...}. Here just setting\n",
        "  # the weights all to be 1\n",
        "  class_weights = {0:1, 1:1, 2:1, 3:1}\n",
        "\n",
        "  history = model.fit(X_train, Y_train, batch_size = 16, epochs = 300,\n",
        "                      verbose = 2, validation_data=(X_val, Y_val),\n",
        "                      callbacks=[checkpointer], class_weight = class_weights)\n",
        "  \n",
        "  # Plot training & validation accuracy values\n",
        "  plt.plot(history.history['acc'])\n",
        "  plt.plot(history.history['val_acc'])\n",
        "  plt.title('Model accuracy')\n",
        "  plt.ylabel('Accuracy')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.legend(['Train', 'Test'], loc='upper left')\n",
        "  plt.show()\n",
        "  figName = 'Accuracy_A0' + str(x) + '.png'  \n",
        "  plt.savefig(figName)\n",
        "\n",
        "  print('\\n# Evaluate on test data')\n",
        "  results = model.evaluate(X_test, Y_test, batch_size=1)\n",
        "  print('test loss, test acc:', results)\n",
        "\n",
        "  loss_all[subid - 1, 0] = results[0]\n",
        "  acc_all[subid - 1, 0] = results[1]\n",
        "  \n",
        "  from keras import backend as K \n",
        "  # Do some code, e.g. train and save model\n",
        "  K.clear_session()\n",
        "\n",
        "print(loss_all)\n",
        "print(acc_all)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n",
            "EEG_Deep/Data2A/parsed_P01T.mat\n",
            "(0, 12, 4096)\n",
            "(80, 12, 4096)\n",
            "(80, 1)\n",
            "2\n",
            "EEG_Deep/Data2A/parsed_P02T.mat\n",
            "(80, 12, 4096)\n",
            "(160, 12, 4096)\n",
            "(160, 1)\n",
            "4\n",
            "EEG_Deep/Data2A/parsed_P04T.mat\n",
            "(160, 12, 4096)\n",
            "(240, 12, 4096)\n",
            "(240, 1)\n",
            "5\n",
            "EEG_Deep/Data2A/parsed_P05T.mat\n",
            "(240, 12, 4096)\n",
            "(320, 12, 4096)\n",
            "(320, 1)\n",
            "6\n",
            "EEG_Deep/Data2A/parsed_P06T.mat\n",
            "(320, 12, 4096)\n",
            "(400, 12, 4096)\n",
            "(400, 1)\n",
            "7\n",
            "EEG_Deep/Data2A/parsed_P07T.mat\n",
            "(400, 12, 4096)\n",
            "(480, 12, 4096)\n",
            "(480, 1)\n",
            "8\n",
            "EEG_Deep/Data2A/parsed_P08T.mat\n",
            "(480, 12, 4096)\n",
            "(560, 12, 4096)\n",
            "(560, 1)\n",
            "9\n",
            "EEG_Deep/Data2A/parsed_P09T.mat\n",
            "(560, 12, 4096)\n",
            "(640, 12, 4096)\n",
            "(640, 1)\n",
            "10\n",
            "EEG_Deep/Data2A/parsed_P010T.mat\n",
            "(640, 12, 4096)\n",
            "(720, 12, 4096)\n",
            "(720, 1)\n",
            "3\n",
            "EEG_Deep/Data2A/parsed_P03E.mat\n",
            "(0, 12, 4096)\n",
            "(40, 12, 4096)\n",
            "(40, 1)\n",
            "Filtering of training data in progress\n",
            "(720, 12, 4096)\n",
            "Filtering of test data in progress\n",
            "(40, 12, 4096)\n",
            "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
            " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1.]\n",
            "(620, 12, 1536)\n",
            "(620,)\n",
            "(100, 12, 1536)\n",
            "(100,)\n",
            "[[1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]]\n",
            "X_train shape: (620, 1, 12, 1536)\n",
            "620 train samples\n",
            "100 val samples\n",
            "(40, 12, 1536)\n",
            "(40, 1)\n",
            "X_train shape: (40, 1, 12, 1536)\n",
            "40 train samples\n",
            "Train on 620 samples, validate on 100 samples\n",
            "Epoch 1/300\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 1.13780, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.9213 - acc: 0.5452 - val_loss: 1.1378 - val_acc: 0.5400\n",
            "Epoch 2/300\n",
            "\n",
            "Epoch 00002: val_loss improved from 1.13780 to 1.00968, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.7469 - acc: 0.6016 - val_loss: 1.0097 - val_acc: 0.5000\n",
            "Epoch 3/300\n",
            "\n",
            "Epoch 00003: val_loss improved from 1.00968 to 0.91377, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.6873 - acc: 0.6823 - val_loss: 0.9138 - val_acc: 0.5000\n",
            "Epoch 4/300\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.91377 to 0.90361, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.6391 - acc: 0.7129 - val_loss: 0.9036 - val_acc: 0.5100\n",
            "Epoch 5/300\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.90361 to 0.82798, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.5977 - acc: 0.7355 - val_loss: 0.8280 - val_acc: 0.5300\n",
            "Epoch 6/300\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.82798 to 0.76113, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.5679 - acc: 0.7387 - val_loss: 0.7611 - val_acc: 0.5800\n",
            "Epoch 7/300\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.76113 to 0.70765, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.5380 - acc: 0.7661 - val_loss: 0.7076 - val_acc: 0.6800\n",
            "Epoch 8/300\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.70765 to 0.67321, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.5199 - acc: 0.7613 - val_loss: 0.6732 - val_acc: 0.6700\n",
            "Epoch 9/300\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.67321\n",
            "620/620 - 1s - loss: 0.5150 - acc: 0.7565 - val_loss: 0.7066 - val_acc: 0.7000\n",
            "Epoch 10/300\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.67321 to 0.61597, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.5056 - acc: 0.7548 - val_loss: 0.6160 - val_acc: 0.6900\n",
            "Epoch 11/300\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.61597\n",
            "620/620 - 1s - loss: 0.4943 - acc: 0.7742 - val_loss: 0.7062 - val_acc: 0.6400\n",
            "Epoch 12/300\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.61597\n",
            "620/620 - 1s - loss: 0.5063 - acc: 0.7758 - val_loss: 0.6519 - val_acc: 0.7000\n",
            "Epoch 13/300\n",
            "\n",
            "Epoch 00013: val_loss improved from 0.61597 to 0.61566, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.4978 - acc: 0.7726 - val_loss: 0.6157 - val_acc: 0.7000\n",
            "Epoch 14/300\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.61566\n",
            "620/620 - 1s - loss: 0.4989 - acc: 0.7500 - val_loss: 0.8939 - val_acc: 0.5400\n",
            "Epoch 15/300\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.61566\n",
            "620/620 - 1s - loss: 0.4923 - acc: 0.7710 - val_loss: 0.7120 - val_acc: 0.6600\n",
            "Epoch 16/300\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.61566\n",
            "620/620 - 1s - loss: 0.4720 - acc: 0.7790 - val_loss: 0.6439 - val_acc: 0.6800\n",
            "Epoch 17/300\n",
            "\n",
            "Epoch 00017: val_loss improved from 0.61566 to 0.59945, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.4760 - acc: 0.7823 - val_loss: 0.5994 - val_acc: 0.6800\n",
            "Epoch 18/300\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.59945\n",
            "620/620 - 1s - loss: 0.4677 - acc: 0.7935 - val_loss: 0.9021 - val_acc: 0.5600\n",
            "Epoch 19/300\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.59945\n",
            "620/620 - 1s - loss: 0.4732 - acc: 0.7645 - val_loss: 0.6758 - val_acc: 0.6400\n",
            "Epoch 20/300\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.59945\n",
            "620/620 - 1s - loss: 0.4944 - acc: 0.7742 - val_loss: 0.6094 - val_acc: 0.6900\n",
            "Epoch 21/300\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.59945\n",
            "620/620 - 1s - loss: 0.4845 - acc: 0.7661 - val_loss: 0.6861 - val_acc: 0.6400\n",
            "Epoch 22/300\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.59945\n",
            "620/620 - 1s - loss: 0.4585 - acc: 0.7855 - val_loss: 0.6385 - val_acc: 0.6600\n",
            "Epoch 23/300\n",
            "\n",
            "Epoch 00023: val_loss improved from 0.59945 to 0.51923, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.4425 - acc: 0.7790 - val_loss: 0.5192 - val_acc: 0.8100\n",
            "Epoch 24/300\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.4459 - acc: 0.8032 - val_loss: 0.5819 - val_acc: 0.6900\n",
            "Epoch 25/300\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.4455 - acc: 0.7935 - val_loss: 0.7389 - val_acc: 0.6200\n",
            "Epoch 26/300\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.4532 - acc: 0.7903 - val_loss: 0.6066 - val_acc: 0.6900\n",
            "Epoch 27/300\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.4366 - acc: 0.7968 - val_loss: 0.5313 - val_acc: 0.7600\n",
            "Epoch 28/300\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.5056 - acc: 0.7516 - val_loss: 0.5336 - val_acc: 0.7800\n",
            "Epoch 29/300\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.4411 - acc: 0.7984 - val_loss: 0.6300 - val_acc: 0.6500\n",
            "Epoch 30/300\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.4273 - acc: 0.8000 - val_loss: 0.5528 - val_acc: 0.7700\n",
            "Epoch 31/300\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.4686 - acc: 0.7823 - val_loss: 0.5996 - val_acc: 0.6800\n",
            "Epoch 32/300\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.4265 - acc: 0.8032 - val_loss: 0.7032 - val_acc: 0.6600\n",
            "Epoch 33/300\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.4437 - acc: 0.7839 - val_loss: 0.7155 - val_acc: 0.6600\n",
            "Epoch 34/300\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.4203 - acc: 0.8145 - val_loss: 0.6865 - val_acc: 0.6500\n",
            "Epoch 35/300\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.4363 - acc: 0.7919 - val_loss: 0.6781 - val_acc: 0.6300\n",
            "Epoch 36/300\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.4159 - acc: 0.8113 - val_loss: 0.8730 - val_acc: 0.5800\n",
            "Epoch 37/300\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.4516 - acc: 0.8000 - val_loss: 0.7583 - val_acc: 0.6300\n",
            "Epoch 38/300\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.4222 - acc: 0.7935 - val_loss: 0.7724 - val_acc: 0.6800\n",
            "Epoch 39/300\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.4501 - acc: 0.8065 - val_loss: 0.6291 - val_acc: 0.7400\n",
            "Epoch 40/300\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.4296 - acc: 0.7968 - val_loss: 0.7496 - val_acc: 0.6600\n",
            "Epoch 41/300\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.4603 - acc: 0.7806 - val_loss: 0.8599 - val_acc: 0.5800\n",
            "Epoch 42/300\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.4233 - acc: 0.7968 - val_loss: 0.7758 - val_acc: 0.6200\n",
            "Epoch 43/300\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.4605 - acc: 0.7758 - val_loss: 0.8789 - val_acc: 0.6200\n",
            "Epoch 44/300\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.4306 - acc: 0.8065 - val_loss: 0.7357 - val_acc: 0.6200\n",
            "Epoch 45/300\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.4120 - acc: 0.8081 - val_loss: 0.7584 - val_acc: 0.6300\n",
            "Epoch 46/300\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.4303 - acc: 0.8032 - val_loss: 0.9429 - val_acc: 0.5600\n",
            "Epoch 47/300\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.4208 - acc: 0.8177 - val_loss: 0.7278 - val_acc: 0.5900\n",
            "Epoch 48/300\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.4173 - acc: 0.8113 - val_loss: 0.8305 - val_acc: 0.5600\n",
            "Epoch 49/300\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.4137 - acc: 0.8177 - val_loss: 0.7837 - val_acc: 0.5700\n",
            "Epoch 50/300\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.4304 - acc: 0.7871 - val_loss: 0.6176 - val_acc: 0.6900\n",
            "Epoch 51/300\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3997 - acc: 0.8113 - val_loss: 0.5613 - val_acc: 0.8000\n",
            "Epoch 52/300\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.4429 - acc: 0.7968 - val_loss: 0.5775 - val_acc: 0.7300\n",
            "Epoch 53/300\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.4182 - acc: 0.8016 - val_loss: 0.6144 - val_acc: 0.6900\n",
            "Epoch 54/300\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.4110 - acc: 0.8048 - val_loss: 0.7013 - val_acc: 0.6200\n",
            "Epoch 55/300\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.4334 - acc: 0.7968 - val_loss: 0.6888 - val_acc: 0.6400\n",
            "Epoch 56/300\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.4221 - acc: 0.8000 - val_loss: 0.5802 - val_acc: 0.7500\n",
            "Epoch 57/300\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.4116 - acc: 0.8177 - val_loss: 0.5946 - val_acc: 0.7000\n",
            "Epoch 58/300\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.4296 - acc: 0.7968 - val_loss: 0.6246 - val_acc: 0.7000\n",
            "Epoch 59/300\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.4035 - acc: 0.8339 - val_loss: 0.6904 - val_acc: 0.6400\n",
            "Epoch 60/300\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.4285 - acc: 0.8032 - val_loss: 0.7793 - val_acc: 0.5900\n",
            "Epoch 61/300\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.4064 - acc: 0.8145 - val_loss: 0.8222 - val_acc: 0.6000\n",
            "Epoch 62/300\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3898 - acc: 0.8258 - val_loss: 0.6048 - val_acc: 0.6600\n",
            "Epoch 63/300\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3998 - acc: 0.8290 - val_loss: 0.8146 - val_acc: 0.6200\n",
            "Epoch 64/300\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.4211 - acc: 0.8145 - val_loss: 0.6639 - val_acc: 0.6700\n",
            "Epoch 65/300\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.4085 - acc: 0.8161 - val_loss: 0.6380 - val_acc: 0.7000\n",
            "Epoch 66/300\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.4134 - acc: 0.8145 - val_loss: 0.6701 - val_acc: 0.6800\n",
            "Epoch 67/300\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3998 - acc: 0.8339 - val_loss: 0.5967 - val_acc: 0.7300\n",
            "Epoch 68/300\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3895 - acc: 0.8194 - val_loss: 0.7809 - val_acc: 0.6100\n",
            "Epoch 69/300\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.4055 - acc: 0.8065 - val_loss: 0.8775 - val_acc: 0.5700\n",
            "Epoch 70/300\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.4192 - acc: 0.8081 - val_loss: 0.6314 - val_acc: 0.7600\n",
            "Epoch 71/300\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.4119 - acc: 0.8081 - val_loss: 1.1588 - val_acc: 0.5500\n",
            "Epoch 72/300\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3801 - acc: 0.8194 - val_loss: 0.7403 - val_acc: 0.6700\n",
            "Epoch 73/300\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3855 - acc: 0.8226 - val_loss: 0.6586 - val_acc: 0.7300\n",
            "Epoch 74/300\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.4179 - acc: 0.7952 - val_loss: 0.6898 - val_acc: 0.7300\n",
            "Epoch 75/300\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.4138 - acc: 0.8161 - val_loss: 0.8402 - val_acc: 0.5900\n",
            "Epoch 76/300\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.4203 - acc: 0.8097 - val_loss: 0.6423 - val_acc: 0.7100\n",
            "Epoch 77/300\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3938 - acc: 0.8210 - val_loss: 0.5874 - val_acc: 0.7100\n",
            "Epoch 78/300\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.4060 - acc: 0.7984 - val_loss: 0.5708 - val_acc: 0.7600\n",
            "Epoch 79/300\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3901 - acc: 0.8306 - val_loss: 0.5302 - val_acc: 0.7600\n",
            "Epoch 80/300\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.4102 - acc: 0.7935 - val_loss: 0.8197 - val_acc: 0.6000\n",
            "Epoch 81/300\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3984 - acc: 0.8129 - val_loss: 0.6661 - val_acc: 0.6500\n",
            "Epoch 82/300\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3945 - acc: 0.8145 - val_loss: 0.8706 - val_acc: 0.6000\n",
            "Epoch 83/300\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3671 - acc: 0.8339 - val_loss: 0.6684 - val_acc: 0.6900\n",
            "Epoch 84/300\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.4147 - acc: 0.8097 - val_loss: 0.8068 - val_acc: 0.5800\n",
            "Epoch 85/300\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3704 - acc: 0.8242 - val_loss: 0.6096 - val_acc: 0.7500\n",
            "Epoch 86/300\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3902 - acc: 0.8242 - val_loss: 0.7337 - val_acc: 0.6900\n",
            "Epoch 87/300\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.4037 - acc: 0.8113 - val_loss: 0.7827 - val_acc: 0.6700\n",
            "Epoch 88/300\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3799 - acc: 0.8258 - val_loss: 0.7955 - val_acc: 0.6300\n",
            "Epoch 89/300\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3886 - acc: 0.8371 - val_loss: 0.9090 - val_acc: 0.5500\n",
            "Epoch 90/300\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3696 - acc: 0.8468 - val_loss: 0.6759 - val_acc: 0.6700\n",
            "Epoch 91/300\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3941 - acc: 0.8194 - val_loss: 0.8548 - val_acc: 0.6000\n",
            "Epoch 92/300\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3937 - acc: 0.8081 - val_loss: 0.7447 - val_acc: 0.6100\n",
            "Epoch 93/300\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3725 - acc: 0.8226 - val_loss: 0.6083 - val_acc: 0.7100\n",
            "Epoch 94/300\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3929 - acc: 0.8323 - val_loss: 0.5656 - val_acc: 0.7500\n",
            "Epoch 95/300\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3553 - acc: 0.8242 - val_loss: 0.6043 - val_acc: 0.7400\n",
            "Epoch 96/300\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3996 - acc: 0.8226 - val_loss: 0.6264 - val_acc: 0.7100\n",
            "Epoch 97/300\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.4002 - acc: 0.8258 - val_loss: 0.6263 - val_acc: 0.7000\n",
            "Epoch 98/300\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3841 - acc: 0.8419 - val_loss: 0.7303 - val_acc: 0.6200\n",
            "Epoch 99/300\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.4014 - acc: 0.7935 - val_loss: 0.8950 - val_acc: 0.5900\n",
            "Epoch 100/300\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3770 - acc: 0.8355 - val_loss: 0.9409 - val_acc: 0.5900\n",
            "Epoch 101/300\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3690 - acc: 0.8500 - val_loss: 0.8518 - val_acc: 0.6000\n",
            "Epoch 102/300\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3919 - acc: 0.8097 - val_loss: 0.5545 - val_acc: 0.7500\n",
            "Epoch 103/300\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.4007 - acc: 0.8194 - val_loss: 0.6130 - val_acc: 0.7000\n",
            "Epoch 104/300\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3478 - acc: 0.8355 - val_loss: 0.7189 - val_acc: 0.6100\n",
            "Epoch 105/300\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3559 - acc: 0.8452 - val_loss: 0.8216 - val_acc: 0.6000\n",
            "Epoch 106/300\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3708 - acc: 0.8290 - val_loss: 0.7678 - val_acc: 0.6000\n",
            "Epoch 107/300\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3334 - acc: 0.8613 - val_loss: 0.6677 - val_acc: 0.6600\n",
            "Epoch 108/300\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3651 - acc: 0.8290 - val_loss: 0.6992 - val_acc: 0.6500\n",
            "Epoch 109/300\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3973 - acc: 0.8355 - val_loss: 0.6583 - val_acc: 0.6200\n",
            "Epoch 110/300\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3751 - acc: 0.8242 - val_loss: 1.1001 - val_acc: 0.5500\n",
            "Epoch 111/300\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3756 - acc: 0.8290 - val_loss: 0.8185 - val_acc: 0.6500\n",
            "Epoch 112/300\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3485 - acc: 0.8500 - val_loss: 0.6214 - val_acc: 0.7500\n",
            "Epoch 113/300\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3656 - acc: 0.8306 - val_loss: 0.8523 - val_acc: 0.5600\n",
            "Epoch 114/300\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3775 - acc: 0.8339 - val_loss: 0.6847 - val_acc: 0.6300\n",
            "Epoch 115/300\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3430 - acc: 0.8371 - val_loss: 0.5578 - val_acc: 0.7500\n",
            "Epoch 116/300\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3983 - acc: 0.8226 - val_loss: 0.6268 - val_acc: 0.6600\n",
            "Epoch 117/300\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3618 - acc: 0.8306 - val_loss: 0.5722 - val_acc: 0.7300\n",
            "Epoch 118/300\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3688 - acc: 0.8403 - val_loss: 0.8249 - val_acc: 0.5600\n",
            "Epoch 119/300\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3832 - acc: 0.8194 - val_loss: 0.6969 - val_acc: 0.6100\n",
            "Epoch 120/300\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3594 - acc: 0.8258 - val_loss: 0.9355 - val_acc: 0.5900\n",
            "Epoch 121/300\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3677 - acc: 0.8323 - val_loss: 1.0143 - val_acc: 0.5400\n",
            "Epoch 122/300\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.4157 - acc: 0.8081 - val_loss: 0.8066 - val_acc: 0.6200\n",
            "Epoch 123/300\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3755 - acc: 0.8355 - val_loss: 0.6131 - val_acc: 0.6500\n",
            "Epoch 124/300\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3461 - acc: 0.8452 - val_loss: 0.7462 - val_acc: 0.6000\n",
            "Epoch 125/300\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3519 - acc: 0.8419 - val_loss: 0.7912 - val_acc: 0.6000\n",
            "Epoch 126/300\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.4126 - acc: 0.8048 - val_loss: 0.8974 - val_acc: 0.5800\n",
            "Epoch 127/300\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3971 - acc: 0.8242 - val_loss: 0.6716 - val_acc: 0.6600\n",
            "Epoch 128/300\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3916 - acc: 0.8387 - val_loss: 0.6418 - val_acc: 0.6500\n",
            "Epoch 129/300\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3479 - acc: 0.8548 - val_loss: 0.6111 - val_acc: 0.6600\n",
            "Epoch 130/300\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3976 - acc: 0.8129 - val_loss: 0.5815 - val_acc: 0.7400\n",
            "Epoch 131/300\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3665 - acc: 0.8339 - val_loss: 0.5744 - val_acc: 0.7400\n",
            "Epoch 132/300\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3602 - acc: 0.8435 - val_loss: 0.7376 - val_acc: 0.5600\n",
            "Epoch 133/300\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3476 - acc: 0.8581 - val_loss: 0.6192 - val_acc: 0.6800\n",
            "Epoch 134/300\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.4298 - acc: 0.7984 - val_loss: 0.5442 - val_acc: 0.7700\n",
            "Epoch 135/300\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3610 - acc: 0.8306 - val_loss: 0.8134 - val_acc: 0.5800\n",
            "Epoch 136/300\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3557 - acc: 0.8516 - val_loss: 0.8098 - val_acc: 0.5800\n",
            "Epoch 137/300\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3774 - acc: 0.8290 - val_loss: 0.6303 - val_acc: 0.6800\n",
            "Epoch 138/300\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3615 - acc: 0.8387 - val_loss: 0.6941 - val_acc: 0.6200\n",
            "Epoch 139/300\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3795 - acc: 0.8242 - val_loss: 0.6559 - val_acc: 0.7000\n",
            "Epoch 140/300\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3429 - acc: 0.8516 - val_loss: 0.7020 - val_acc: 0.6300\n",
            "Epoch 141/300\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3648 - acc: 0.8435 - val_loss: 0.8242 - val_acc: 0.6000\n",
            "Epoch 142/300\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3673 - acc: 0.8403 - val_loss: 0.7968 - val_acc: 0.6800\n",
            "Epoch 143/300\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3669 - acc: 0.8387 - val_loss: 0.8002 - val_acc: 0.6300\n",
            "Epoch 144/300\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3960 - acc: 0.8339 - val_loss: 0.7947 - val_acc: 0.6400\n",
            "Epoch 145/300\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3640 - acc: 0.8532 - val_loss: 1.0154 - val_acc: 0.5300\n",
            "Epoch 146/300\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3778 - acc: 0.8371 - val_loss: 0.7802 - val_acc: 0.5700\n",
            "Epoch 147/300\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3420 - acc: 0.8613 - val_loss: 0.6054 - val_acc: 0.7200\n",
            "Epoch 148/300\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3699 - acc: 0.8355 - val_loss: 0.8098 - val_acc: 0.5700\n",
            "Epoch 149/300\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3721 - acc: 0.8226 - val_loss: 0.6344 - val_acc: 0.6800\n",
            "Epoch 150/300\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3731 - acc: 0.8194 - val_loss: 0.6669 - val_acc: 0.6700\n",
            "Epoch 151/300\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3632 - acc: 0.8306 - val_loss: 0.6170 - val_acc: 0.6900\n",
            "Epoch 152/300\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3461 - acc: 0.8387 - val_loss: 0.6488 - val_acc: 0.7000\n",
            "Epoch 153/300\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3547 - acc: 0.8371 - val_loss: 0.6415 - val_acc: 0.7100\n",
            "Epoch 154/300\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3271 - acc: 0.8629 - val_loss: 1.0087 - val_acc: 0.5500\n",
            "Epoch 155/300\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3390 - acc: 0.8484 - val_loss: 0.8137 - val_acc: 0.5500\n",
            "Epoch 156/300\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3591 - acc: 0.8371 - val_loss: 0.6379 - val_acc: 0.6700\n",
            "Epoch 157/300\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3463 - acc: 0.8597 - val_loss: 1.0998 - val_acc: 0.5300\n",
            "Epoch 158/300\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3627 - acc: 0.8532 - val_loss: 0.6901 - val_acc: 0.6700\n",
            "Epoch 159/300\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3444 - acc: 0.8403 - val_loss: 0.9679 - val_acc: 0.5400\n",
            "Epoch 160/300\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3336 - acc: 0.8581 - val_loss: 0.7430 - val_acc: 0.6100\n",
            "Epoch 161/300\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3726 - acc: 0.8258 - val_loss: 0.6629 - val_acc: 0.6900\n",
            "Epoch 162/300\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3771 - acc: 0.8387 - val_loss: 1.0191 - val_acc: 0.5700\n",
            "Epoch 163/300\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3463 - acc: 0.8355 - val_loss: 0.7116 - val_acc: 0.6400\n",
            "Epoch 164/300\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3524 - acc: 0.8452 - val_loss: 0.6324 - val_acc: 0.6700\n",
            "Epoch 165/300\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3306 - acc: 0.8484 - val_loss: 0.6591 - val_acc: 0.6600\n",
            "Epoch 166/300\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3516 - acc: 0.8274 - val_loss: 0.5870 - val_acc: 0.7600\n",
            "Epoch 167/300\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3285 - acc: 0.8532 - val_loss: 0.6435 - val_acc: 0.7000\n",
            "Epoch 168/300\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3534 - acc: 0.8306 - val_loss: 0.7076 - val_acc: 0.6600\n",
            "Epoch 169/300\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3251 - acc: 0.8613 - val_loss: 0.6331 - val_acc: 0.6800\n",
            "Epoch 170/300\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3503 - acc: 0.8613 - val_loss: 0.6515 - val_acc: 0.6800\n",
            "Epoch 171/300\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3592 - acc: 0.8419 - val_loss: 0.7548 - val_acc: 0.6100\n",
            "Epoch 172/300\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3858 - acc: 0.8468 - val_loss: 0.7087 - val_acc: 0.7000\n",
            "Epoch 173/300\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3278 - acc: 0.8661 - val_loss: 0.8662 - val_acc: 0.5500\n",
            "Epoch 174/300\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3563 - acc: 0.8403 - val_loss: 1.0217 - val_acc: 0.5400\n",
            "Epoch 175/300\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3565 - acc: 0.8548 - val_loss: 0.8580 - val_acc: 0.6400\n",
            "Epoch 176/300\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3498 - acc: 0.8355 - val_loss: 1.3773 - val_acc: 0.5200\n",
            "Epoch 177/300\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3533 - acc: 0.8484 - val_loss: 0.7848 - val_acc: 0.6400\n",
            "Epoch 178/300\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3839 - acc: 0.8242 - val_loss: 0.7224 - val_acc: 0.6100\n",
            "Epoch 179/300\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3414 - acc: 0.8597 - val_loss: 0.7081 - val_acc: 0.6200\n",
            "Epoch 180/300\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3238 - acc: 0.8548 - val_loss: 0.7421 - val_acc: 0.6300\n",
            "Epoch 181/300\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3219 - acc: 0.8710 - val_loss: 0.9572 - val_acc: 0.5600\n",
            "Epoch 182/300\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3557 - acc: 0.8355 - val_loss: 0.9714 - val_acc: 0.5600\n",
            "Epoch 183/300\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3270 - acc: 0.8468 - val_loss: 0.9829 - val_acc: 0.5800\n",
            "Epoch 184/300\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3285 - acc: 0.8581 - val_loss: 0.9257 - val_acc: 0.6100\n",
            "Epoch 185/300\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3239 - acc: 0.8548 - val_loss: 0.7258 - val_acc: 0.6400\n",
            "Epoch 186/300\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3084 - acc: 0.8694 - val_loss: 0.6768 - val_acc: 0.7000\n",
            "Epoch 187/300\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3557 - acc: 0.8532 - val_loss: 0.9799 - val_acc: 0.5500\n",
            "Epoch 188/300\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3511 - acc: 0.8516 - val_loss: 0.7571 - val_acc: 0.6700\n",
            "Epoch 189/300\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3272 - acc: 0.8661 - val_loss: 0.7204 - val_acc: 0.6900\n",
            "Epoch 190/300\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3333 - acc: 0.8597 - val_loss: 0.7656 - val_acc: 0.6500\n",
            "Epoch 191/300\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3503 - acc: 0.8548 - val_loss: 1.0209 - val_acc: 0.6000\n",
            "Epoch 192/300\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3277 - acc: 0.8532 - val_loss: 1.1489 - val_acc: 0.5700\n",
            "Epoch 193/300\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3622 - acc: 0.8339 - val_loss: 1.0576 - val_acc: 0.5900\n",
            "Epoch 194/300\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3360 - acc: 0.8532 - val_loss: 0.9436 - val_acc: 0.6000\n",
            "Epoch 195/300\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3585 - acc: 0.8468 - val_loss: 0.6780 - val_acc: 0.7300\n",
            "Epoch 196/300\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3808 - acc: 0.8242 - val_loss: 0.7552 - val_acc: 0.6600\n",
            "Epoch 197/300\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3302 - acc: 0.8581 - val_loss: 0.9044 - val_acc: 0.5900\n",
            "Epoch 198/300\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3324 - acc: 0.8516 - val_loss: 0.8056 - val_acc: 0.6400\n",
            "Epoch 199/300\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3664 - acc: 0.8387 - val_loss: 0.7088 - val_acc: 0.6500\n",
            "Epoch 200/300\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3303 - acc: 0.8677 - val_loss: 0.6217 - val_acc: 0.6900\n",
            "Epoch 201/300\n",
            "\n",
            "Epoch 00201: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3507 - acc: 0.8387 - val_loss: 0.5874 - val_acc: 0.7100\n",
            "Epoch 202/300\n",
            "\n",
            "Epoch 00202: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3530 - acc: 0.8306 - val_loss: 0.6351 - val_acc: 0.6800\n",
            "Epoch 203/300\n",
            "\n",
            "Epoch 00203: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3258 - acc: 0.8532 - val_loss: 0.6887 - val_acc: 0.6900\n",
            "Epoch 204/300\n",
            "\n",
            "Epoch 00204: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3357 - acc: 0.8419 - val_loss: 0.8257 - val_acc: 0.6100\n",
            "Epoch 205/300\n",
            "\n",
            "Epoch 00205: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3552 - acc: 0.8403 - val_loss: 1.1363 - val_acc: 0.5400\n",
            "Epoch 206/300\n",
            "\n",
            "Epoch 00206: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3354 - acc: 0.8597 - val_loss: 1.4101 - val_acc: 0.5300\n",
            "Epoch 207/300\n",
            "\n",
            "Epoch 00207: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3305 - acc: 0.8435 - val_loss: 0.6815 - val_acc: 0.6800\n",
            "Epoch 208/300\n",
            "\n",
            "Epoch 00208: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3238 - acc: 0.8645 - val_loss: 0.7688 - val_acc: 0.6200\n",
            "Epoch 209/300\n",
            "\n",
            "Epoch 00209: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3300 - acc: 0.8419 - val_loss: 0.7190 - val_acc: 0.6200\n",
            "Epoch 210/300\n",
            "\n",
            "Epoch 00210: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3349 - acc: 0.8597 - val_loss: 0.8029 - val_acc: 0.6400\n",
            "Epoch 211/300\n",
            "\n",
            "Epoch 00211: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3323 - acc: 0.8468 - val_loss: 0.9289 - val_acc: 0.5700\n",
            "Epoch 212/300\n",
            "\n",
            "Epoch 00212: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3326 - acc: 0.8661 - val_loss: 0.6935 - val_acc: 0.6300\n",
            "Epoch 213/300\n",
            "\n",
            "Epoch 00213: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3161 - acc: 0.8790 - val_loss: 0.8783 - val_acc: 0.6300\n",
            "Epoch 214/300\n",
            "\n",
            "Epoch 00214: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3459 - acc: 0.8516 - val_loss: 1.0539 - val_acc: 0.5400\n",
            "Epoch 215/300\n",
            "\n",
            "Epoch 00215: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3406 - acc: 0.8419 - val_loss: 0.6163 - val_acc: 0.7200\n",
            "Epoch 216/300\n",
            "\n",
            "Epoch 00216: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3438 - acc: 0.8500 - val_loss: 0.6207 - val_acc: 0.7200\n",
            "Epoch 217/300\n",
            "\n",
            "Epoch 00217: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3145 - acc: 0.8726 - val_loss: 0.6619 - val_acc: 0.7300\n",
            "Epoch 218/300\n",
            "\n",
            "Epoch 00218: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3223 - acc: 0.8613 - val_loss: 0.8074 - val_acc: 0.6600\n",
            "Epoch 219/300\n",
            "\n",
            "Epoch 00219: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3126 - acc: 0.8661 - val_loss: 0.7585 - val_acc: 0.6700\n",
            "Epoch 220/300\n",
            "\n",
            "Epoch 00220: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3299 - acc: 0.8500 - val_loss: 0.6395 - val_acc: 0.6900\n",
            "Epoch 221/300\n",
            "\n",
            "Epoch 00221: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3394 - acc: 0.8532 - val_loss: 1.4323 - val_acc: 0.5200\n",
            "Epoch 222/300\n",
            "\n",
            "Epoch 00222: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3329 - acc: 0.8355 - val_loss: 0.8285 - val_acc: 0.6000\n",
            "Epoch 223/300\n",
            "\n",
            "Epoch 00223: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3271 - acc: 0.8435 - val_loss: 0.6708 - val_acc: 0.6600\n",
            "Epoch 224/300\n",
            "\n",
            "Epoch 00224: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3242 - acc: 0.8645 - val_loss: 0.7683 - val_acc: 0.6600\n",
            "Epoch 225/300\n",
            "\n",
            "Epoch 00225: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3142 - acc: 0.8645 - val_loss: 0.9040 - val_acc: 0.6600\n",
            "Epoch 226/300\n",
            "\n",
            "Epoch 00226: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3378 - acc: 0.8323 - val_loss: 0.8763 - val_acc: 0.6000\n",
            "Epoch 227/300\n",
            "\n",
            "Epoch 00227: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3318 - acc: 0.8613 - val_loss: 0.9198 - val_acc: 0.5600\n",
            "Epoch 228/300\n",
            "\n",
            "Epoch 00228: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3413 - acc: 0.8468 - val_loss: 1.1070 - val_acc: 0.5200\n",
            "Epoch 229/300\n",
            "\n",
            "Epoch 00229: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3457 - acc: 0.8452 - val_loss: 1.0029 - val_acc: 0.6300\n",
            "Epoch 230/300\n",
            "\n",
            "Epoch 00230: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3392 - acc: 0.8371 - val_loss: 0.7593 - val_acc: 0.6500\n",
            "Epoch 231/300\n",
            "\n",
            "Epoch 00231: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3491 - acc: 0.8500 - val_loss: 0.9631 - val_acc: 0.5700\n",
            "Epoch 232/300\n",
            "\n",
            "Epoch 00232: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3128 - acc: 0.8694 - val_loss: 1.0079 - val_acc: 0.5500\n",
            "Epoch 233/300\n",
            "\n",
            "Epoch 00233: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3580 - acc: 0.8306 - val_loss: 0.6814 - val_acc: 0.6600\n",
            "Epoch 234/300\n",
            "\n",
            "Epoch 00234: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3409 - acc: 0.8452 - val_loss: 0.6441 - val_acc: 0.6600\n",
            "Epoch 235/300\n",
            "\n",
            "Epoch 00235: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3282 - acc: 0.8613 - val_loss: 0.7615 - val_acc: 0.6500\n",
            "Epoch 236/300\n",
            "\n",
            "Epoch 00236: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3163 - acc: 0.8677 - val_loss: 0.7219 - val_acc: 0.6800\n",
            "Epoch 237/300\n",
            "\n",
            "Epoch 00237: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3346 - acc: 0.8516 - val_loss: 0.9267 - val_acc: 0.5900\n",
            "Epoch 238/300\n",
            "\n",
            "Epoch 00238: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3133 - acc: 0.8710 - val_loss: 0.7833 - val_acc: 0.6300\n",
            "Epoch 239/300\n",
            "\n",
            "Epoch 00239: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3506 - acc: 0.8387 - val_loss: 1.0458 - val_acc: 0.5600\n",
            "Epoch 240/300\n",
            "\n",
            "Epoch 00240: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3416 - acc: 0.8484 - val_loss: 0.6990 - val_acc: 0.6600\n",
            "Epoch 241/300\n",
            "\n",
            "Epoch 00241: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3608 - acc: 0.8339 - val_loss: 0.8990 - val_acc: 0.5800\n",
            "Epoch 242/300\n",
            "\n",
            "Epoch 00242: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3717 - acc: 0.8339 - val_loss: 1.0123 - val_acc: 0.5400\n",
            "Epoch 243/300\n",
            "\n",
            "Epoch 00243: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3593 - acc: 0.8403 - val_loss: 0.8922 - val_acc: 0.6500\n",
            "Epoch 244/300\n",
            "\n",
            "Epoch 00244: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3522 - acc: 0.8452 - val_loss: 0.8009 - val_acc: 0.6300\n",
            "Epoch 245/300\n",
            "\n",
            "Epoch 00245: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3490 - acc: 0.8306 - val_loss: 0.6786 - val_acc: 0.6800\n",
            "Epoch 246/300\n",
            "\n",
            "Epoch 00246: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3381 - acc: 0.8435 - val_loss: 0.7650 - val_acc: 0.5800\n",
            "Epoch 247/300\n",
            "\n",
            "Epoch 00247: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3325 - acc: 0.8532 - val_loss: 0.7367 - val_acc: 0.6600\n",
            "Epoch 248/300\n",
            "\n",
            "Epoch 00248: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3227 - acc: 0.8677 - val_loss: 0.6470 - val_acc: 0.7000\n",
            "Epoch 249/300\n",
            "\n",
            "Epoch 00249: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3179 - acc: 0.8677 - val_loss: 0.7433 - val_acc: 0.6700\n",
            "Epoch 250/300\n",
            "\n",
            "Epoch 00250: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3384 - acc: 0.8226 - val_loss: 0.7587 - val_acc: 0.6100\n",
            "Epoch 251/300\n",
            "\n",
            "Epoch 00251: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3396 - acc: 0.8403 - val_loss: 1.0038 - val_acc: 0.5500\n",
            "Epoch 252/300\n",
            "\n",
            "Epoch 00252: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3540 - acc: 0.8452 - val_loss: 0.7468 - val_acc: 0.6500\n",
            "Epoch 253/300\n",
            "\n",
            "Epoch 00253: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3402 - acc: 0.8484 - val_loss: 0.6594 - val_acc: 0.7300\n",
            "Epoch 254/300\n",
            "\n",
            "Epoch 00254: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3044 - acc: 0.8694 - val_loss: 0.9996 - val_acc: 0.5500\n",
            "Epoch 255/300\n",
            "\n",
            "Epoch 00255: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3239 - acc: 0.8677 - val_loss: 0.8822 - val_acc: 0.5900\n",
            "Epoch 256/300\n",
            "\n",
            "Epoch 00256: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3226 - acc: 0.8484 - val_loss: 0.6342 - val_acc: 0.7400\n",
            "Epoch 257/300\n",
            "\n",
            "Epoch 00257: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3198 - acc: 0.8516 - val_loss: 0.8752 - val_acc: 0.6000\n",
            "Epoch 258/300\n",
            "\n",
            "Epoch 00258: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3433 - acc: 0.8468 - val_loss: 0.6051 - val_acc: 0.6900\n",
            "Epoch 259/300\n",
            "\n",
            "Epoch 00259: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3644 - acc: 0.8565 - val_loss: 0.9299 - val_acc: 0.5900\n",
            "Epoch 260/300\n",
            "\n",
            "Epoch 00260: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3216 - acc: 0.8613 - val_loss: 1.0322 - val_acc: 0.5500\n",
            "Epoch 261/300\n",
            "\n",
            "Epoch 00261: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3598 - acc: 0.8290 - val_loss: 1.3145 - val_acc: 0.5200\n",
            "Epoch 262/300\n",
            "\n",
            "Epoch 00262: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3253 - acc: 0.8613 - val_loss: 0.9871 - val_acc: 0.6200\n",
            "Epoch 263/300\n",
            "\n",
            "Epoch 00263: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3024 - acc: 0.8758 - val_loss: 1.0372 - val_acc: 0.5400\n",
            "Epoch 264/300\n",
            "\n",
            "Epoch 00264: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3528 - acc: 0.8468 - val_loss: 0.9382 - val_acc: 0.5600\n",
            "Epoch 265/300\n",
            "\n",
            "Epoch 00265: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3444 - acc: 0.8484 - val_loss: 0.8074 - val_acc: 0.6500\n",
            "Epoch 266/300\n",
            "\n",
            "Epoch 00266: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3059 - acc: 0.8677 - val_loss: 0.7516 - val_acc: 0.7100\n",
            "Epoch 267/300\n",
            "\n",
            "Epoch 00267: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3596 - acc: 0.8290 - val_loss: 0.8127 - val_acc: 0.5900\n",
            "Epoch 268/300\n",
            "\n",
            "Epoch 00268: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3411 - acc: 0.8435 - val_loss: 0.9480 - val_acc: 0.5300\n",
            "Epoch 269/300\n",
            "\n",
            "Epoch 00269: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.2946 - acc: 0.8694 - val_loss: 0.6767 - val_acc: 0.6600\n",
            "Epoch 270/300\n",
            "\n",
            "Epoch 00270: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3070 - acc: 0.8581 - val_loss: 0.6243 - val_acc: 0.6900\n",
            "Epoch 271/300\n",
            "\n",
            "Epoch 00271: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3338 - acc: 0.8661 - val_loss: 0.6981 - val_acc: 0.7000\n",
            "Epoch 272/300\n",
            "\n",
            "Epoch 00272: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3257 - acc: 0.8548 - val_loss: 0.8243 - val_acc: 0.6400\n",
            "Epoch 273/300\n",
            "\n",
            "Epoch 00273: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3297 - acc: 0.8581 - val_loss: 0.6984 - val_acc: 0.6800\n",
            "Epoch 274/300\n",
            "\n",
            "Epoch 00274: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3430 - acc: 0.8371 - val_loss: 0.7189 - val_acc: 0.6700\n",
            "Epoch 275/300\n",
            "\n",
            "Epoch 00275: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3168 - acc: 0.8694 - val_loss: 0.6556 - val_acc: 0.6800\n",
            "Epoch 276/300\n",
            "\n",
            "Epoch 00276: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3183 - acc: 0.8661 - val_loss: 0.7032 - val_acc: 0.7000\n",
            "Epoch 277/300\n",
            "\n",
            "Epoch 00277: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3241 - acc: 0.8516 - val_loss: 0.8040 - val_acc: 0.6300\n",
            "Epoch 278/300\n",
            "\n",
            "Epoch 00278: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.2918 - acc: 0.8839 - val_loss: 0.8171 - val_acc: 0.6300\n",
            "Epoch 279/300\n",
            "\n",
            "Epoch 00279: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3246 - acc: 0.8516 - val_loss: 0.9143 - val_acc: 0.5800\n",
            "Epoch 280/300\n",
            "\n",
            "Epoch 00280: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.2937 - acc: 0.8710 - val_loss: 0.7855 - val_acc: 0.6400\n",
            "Epoch 281/300\n",
            "\n",
            "Epoch 00281: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3286 - acc: 0.8532 - val_loss: 0.7490 - val_acc: 0.6400\n",
            "Epoch 282/300\n",
            "\n",
            "Epoch 00282: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3427 - acc: 0.8419 - val_loss: 0.7271 - val_acc: 0.6500\n",
            "Epoch 283/300\n",
            "\n",
            "Epoch 00283: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3221 - acc: 0.8597 - val_loss: 0.9569 - val_acc: 0.5900\n",
            "Epoch 284/300\n",
            "\n",
            "Epoch 00284: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3298 - acc: 0.8484 - val_loss: 0.7580 - val_acc: 0.6300\n",
            "Epoch 285/300\n",
            "\n",
            "Epoch 00285: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3068 - acc: 0.8677 - val_loss: 0.9479 - val_acc: 0.5700\n",
            "Epoch 286/300\n",
            "\n",
            "Epoch 00286: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3110 - acc: 0.8726 - val_loss: 1.0409 - val_acc: 0.5500\n",
            "Epoch 287/300\n",
            "\n",
            "Epoch 00287: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3355 - acc: 0.8532 - val_loss: 0.7024 - val_acc: 0.6700\n",
            "Epoch 288/300\n",
            "\n",
            "Epoch 00288: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.2931 - acc: 0.8661 - val_loss: 1.2708 - val_acc: 0.5200\n",
            "Epoch 289/300\n",
            "\n",
            "Epoch 00289: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3445 - acc: 0.8468 - val_loss: 0.7197 - val_acc: 0.6500\n",
            "Epoch 290/300\n",
            "\n",
            "Epoch 00290: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3041 - acc: 0.8839 - val_loss: 0.6991 - val_acc: 0.6400\n",
            "Epoch 291/300\n",
            "\n",
            "Epoch 00291: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3445 - acc: 0.8403 - val_loss: 0.6959 - val_acc: 0.6600\n",
            "Epoch 292/300\n",
            "\n",
            "Epoch 00292: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3454 - acc: 0.8484 - val_loss: 0.8697 - val_acc: 0.6200\n",
            "Epoch 293/300\n",
            "\n",
            "Epoch 00293: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3041 - acc: 0.8726 - val_loss: 0.8321 - val_acc: 0.6600\n",
            "Epoch 294/300\n",
            "\n",
            "Epoch 00294: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.2941 - acc: 0.8758 - val_loss: 0.8211 - val_acc: 0.6900\n",
            "Epoch 295/300\n",
            "\n",
            "Epoch 00295: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3079 - acc: 0.8887 - val_loss: 0.7913 - val_acc: 0.6800\n",
            "Epoch 296/300\n",
            "\n",
            "Epoch 00296: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3050 - acc: 0.8758 - val_loss: 0.9411 - val_acc: 0.5600\n",
            "Epoch 297/300\n",
            "\n",
            "Epoch 00297: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.2966 - acc: 0.8726 - val_loss: 0.7710 - val_acc: 0.6700\n",
            "Epoch 298/300\n",
            "\n",
            "Epoch 00298: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3287 - acc: 0.8532 - val_loss: 0.6585 - val_acc: 0.7200\n",
            "Epoch 299/300\n",
            "\n",
            "Epoch 00299: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3120 - acc: 0.8645 - val_loss: 0.7654 - val_acc: 0.6400\n",
            "Epoch 300/300\n",
            "\n",
            "Epoch 00300: val_loss did not improve from 0.51923\n",
            "620/620 - 1s - loss: 0.3031 - acc: 0.8758 - val_loss: 0.7897 - val_acc: 0.6300\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "# Evaluate on test data\n",
            "40/40 [==============================] - 0s 2ms/sample - loss: 0.8824 - acc: 0.6250\n",
            "test loss, test acc: [0.8823598147086159, 0.625]\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:107: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:111: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "1\n",
            "EEG_Deep/Data2A/parsed_P01T.mat\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(0, 12, 4096)\n",
            "(80, 12, 4096)\n",
            "(80, 1)\n",
            "2\n",
            "EEG_Deep/Data2A/parsed_P02T.mat\n",
            "(80, 12, 4096)\n",
            "(160, 12, 4096)\n",
            "(160, 1)\n",
            "3\n",
            "EEG_Deep/Data2A/parsed_P03T.mat\n",
            "(160, 12, 4096)\n",
            "(240, 12, 4096)\n",
            "(240, 1)\n",
            "4\n",
            "EEG_Deep/Data2A/parsed_P04T.mat\n",
            "(240, 12, 4096)\n",
            "(320, 12, 4096)\n",
            "(320, 1)\n",
            "5\n",
            "EEG_Deep/Data2A/parsed_P05T.mat\n",
            "(320, 12, 4096)\n",
            "(400, 12, 4096)\n",
            "(400, 1)\n",
            "6\n",
            "EEG_Deep/Data2A/parsed_P06T.mat\n",
            "(400, 12, 4096)\n",
            "(480, 12, 4096)\n",
            "(480, 1)\n",
            "7\n",
            "EEG_Deep/Data2A/parsed_P07T.mat\n",
            "(480, 12, 4096)\n",
            "(560, 12, 4096)\n",
            "(560, 1)\n",
            "8\n",
            "EEG_Deep/Data2A/parsed_P08T.mat\n",
            "(560, 12, 4096)\n",
            "(640, 12, 4096)\n",
            "(640, 1)\n",
            "9\n",
            "EEG_Deep/Data2A/parsed_P09T.mat\n",
            "(640, 12, 4096)\n",
            "(720, 12, 4096)\n",
            "(720, 1)\n",
            "10\n",
            "EEG_Deep/Data2A/parsed_P010E.mat\n",
            "(0, 12, 4096)\n",
            "(40, 12, 4096)\n",
            "(40, 1)\n",
            "Filtering of training data in progress\n",
            "(720, 12, 4096)\n",
            "Filtering of test data in progress\n",
            "(40, 12, 4096)\n",
            "[2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 2. 2.\n",
            " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2.\n",
            " 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2.\n",
            " 2. 2. 2. 2.]\n",
            "(620, 12, 1536)\n",
            "(620,)\n",
            "(100, 12, 1536)\n",
            "(100,)\n",
            "[[0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]]\n",
            "X_train shape: (620, 1, 12, 1536)\n",
            "620 train samples\n",
            "100 val samples\n",
            "(40, 12, 1536)\n",
            "(40, 1)\n",
            "X_train shape: (40, 1, 12, 1536)\n",
            "40 train samples\n",
            "Train on 620 samples, validate on 100 samples\n",
            "Epoch 1/300\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 1.21229, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.9537 - acc: 0.4968 - val_loss: 1.2123 - val_acc: 0.5000\n",
            "Epoch 2/300\n",
            "\n",
            "Epoch 00002: val_loss improved from 1.21229 to 1.07026, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.7553 - acc: 0.6161 - val_loss: 1.0703 - val_acc: 0.5500\n",
            "Epoch 3/300\n",
            "\n",
            "Epoch 00003: val_loss improved from 1.07026 to 0.96477, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.6908 - acc: 0.6710 - val_loss: 0.9648 - val_acc: 0.5100\n",
            "Epoch 4/300\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.96477 to 0.90194, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.6366 - acc: 0.6823 - val_loss: 0.9019 - val_acc: 0.5000\n",
            "Epoch 5/300\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.90194 to 0.90163, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.5732 - acc: 0.7435 - val_loss: 0.9016 - val_acc: 0.5000\n",
            "Epoch 6/300\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.90163\n",
            "620/620 - 1s - loss: 0.5527 - acc: 0.7516 - val_loss: 0.9339 - val_acc: 0.5000\n",
            "Epoch 7/300\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.90163 to 0.88387, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.5473 - acc: 0.7806 - val_loss: 0.8839 - val_acc: 0.5200\n",
            "Epoch 8/300\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.88387\n",
            "620/620 - 1s - loss: 0.5465 - acc: 0.7548 - val_loss: 0.9555 - val_acc: 0.5000\n",
            "Epoch 9/300\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.88387 to 0.83613, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.5122 - acc: 0.7645 - val_loss: 0.8361 - val_acc: 0.5400\n",
            "Epoch 10/300\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.83613\n",
            "620/620 - 1s - loss: 0.5227 - acc: 0.7645 - val_loss: 0.9035 - val_acc: 0.5200\n",
            "Epoch 11/300\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.83613\n",
            "620/620 - 1s - loss: 0.5137 - acc: 0.7613 - val_loss: 0.8513 - val_acc: 0.5400\n",
            "Epoch 12/300\n",
            "\n",
            "Epoch 00012: val_loss improved from 0.83613 to 0.79346, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.5165 - acc: 0.7726 - val_loss: 0.7935 - val_acc: 0.5600\n",
            "Epoch 13/300\n",
            "\n",
            "Epoch 00013: val_loss improved from 0.79346 to 0.75401, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.5039 - acc: 0.7774 - val_loss: 0.7540 - val_acc: 0.5900\n",
            "Epoch 14/300\n",
            "\n",
            "Epoch 00014: val_loss improved from 0.75401 to 0.68165, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.5098 - acc: 0.7581 - val_loss: 0.6816 - val_acc: 0.6500\n",
            "Epoch 15/300\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.68165\n",
            "620/620 - 1s - loss: 0.5067 - acc: 0.7516 - val_loss: 0.7196 - val_acc: 0.6000\n",
            "Epoch 16/300\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.68165\n",
            "620/620 - 1s - loss: 0.5031 - acc: 0.7645 - val_loss: 0.7404 - val_acc: 0.6300\n",
            "Epoch 17/300\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.68165\n",
            "620/620 - 1s - loss: 0.5104 - acc: 0.7548 - val_loss: 0.7231 - val_acc: 0.5800\n",
            "Epoch 18/300\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.68165\n",
            "620/620 - 1s - loss: 0.4906 - acc: 0.7694 - val_loss: 0.7346 - val_acc: 0.6200\n",
            "Epoch 19/300\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.68165\n",
            "620/620 - 1s - loss: 0.4981 - acc: 0.7839 - val_loss: 0.7765 - val_acc: 0.5900\n",
            "Epoch 20/300\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.68165\n",
            "620/620 - 1s - loss: 0.4782 - acc: 0.7919 - val_loss: 0.7479 - val_acc: 0.6200\n",
            "Epoch 21/300\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.68165\n",
            "620/620 - 1s - loss: 0.4893 - acc: 0.7806 - val_loss: 0.6998 - val_acc: 0.6200\n",
            "Epoch 22/300\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.68165\n",
            "620/620 - 1s - loss: 0.5125 - acc: 0.7548 - val_loss: 0.6964 - val_acc: 0.5900\n",
            "Epoch 23/300\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.68165\n",
            "620/620 - 1s - loss: 0.4716 - acc: 0.7726 - val_loss: 0.7668 - val_acc: 0.6300\n",
            "Epoch 24/300\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.68165\n",
            "620/620 - 1s - loss: 0.4657 - acc: 0.7694 - val_loss: 0.8915 - val_acc: 0.5400\n",
            "Epoch 25/300\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.68165\n",
            "620/620 - 1s - loss: 0.4859 - acc: 0.7677 - val_loss: 0.7054 - val_acc: 0.6500\n",
            "Epoch 26/300\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.68165\n",
            "620/620 - 1s - loss: 0.4632 - acc: 0.7871 - val_loss: 0.6837 - val_acc: 0.6400\n",
            "Epoch 27/300\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.68165\n",
            "620/620 - 1s - loss: 0.4725 - acc: 0.7903 - val_loss: 0.8296 - val_acc: 0.6000\n",
            "Epoch 28/300\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.68165\n",
            "620/620 - 1s - loss: 0.4409 - acc: 0.8210 - val_loss: 0.7184 - val_acc: 0.6600\n",
            "Epoch 29/300\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.68165\n",
            "620/620 - 1s - loss: 0.4419 - acc: 0.7952 - val_loss: 0.7418 - val_acc: 0.6300\n",
            "Epoch 30/300\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.68165\n",
            "620/620 - 1s - loss: 0.4483 - acc: 0.7887 - val_loss: 0.6989 - val_acc: 0.6300\n",
            "Epoch 31/300\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.68165\n",
            "620/620 - 1s - loss: 0.4637 - acc: 0.7790 - val_loss: 0.7100 - val_acc: 0.6400\n",
            "Epoch 32/300\n",
            "\n",
            "Epoch 00032: val_loss improved from 0.68165 to 0.67246, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.4458 - acc: 0.7919 - val_loss: 0.6725 - val_acc: 0.6400\n",
            "Epoch 33/300\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.67246\n",
            "620/620 - 1s - loss: 0.4624 - acc: 0.7855 - val_loss: 0.7310 - val_acc: 0.6500\n",
            "Epoch 34/300\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.67246\n",
            "620/620 - 1s - loss: 0.4469 - acc: 0.7935 - val_loss: 0.7628 - val_acc: 0.6400\n",
            "Epoch 35/300\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.67246\n",
            "620/620 - 1s - loss: 0.4365 - acc: 0.7952 - val_loss: 0.7590 - val_acc: 0.6200\n",
            "Epoch 36/300\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.67246\n",
            "620/620 - 1s - loss: 0.4446 - acc: 0.8113 - val_loss: 0.7885 - val_acc: 0.6100\n",
            "Epoch 37/300\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.67246\n",
            "620/620 - 1s - loss: 0.4506 - acc: 0.8016 - val_loss: 0.6966 - val_acc: 0.6600\n",
            "Epoch 38/300\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.67246\n",
            "620/620 - 1s - loss: 0.4359 - acc: 0.7903 - val_loss: 0.7525 - val_acc: 0.6400\n",
            "Epoch 39/300\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.67246\n",
            "620/620 - 1s - loss: 0.4488 - acc: 0.7952 - val_loss: 0.6977 - val_acc: 0.5800\n",
            "Epoch 40/300\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.67246\n",
            "620/620 - 1s - loss: 0.4446 - acc: 0.8048 - val_loss: 0.6980 - val_acc: 0.6000\n",
            "Epoch 41/300\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.67246\n",
            "620/620 - 1s - loss: 0.4472 - acc: 0.7710 - val_loss: 0.6967 - val_acc: 0.6100\n",
            "Epoch 42/300\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.67246\n",
            "620/620 - 1s - loss: 0.4634 - acc: 0.8032 - val_loss: 0.7213 - val_acc: 0.6300\n",
            "Epoch 43/300\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.67246\n",
            "620/620 - 1s - loss: 0.4218 - acc: 0.8161 - val_loss: 0.7105 - val_acc: 0.6200\n",
            "Epoch 44/300\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.67246\n",
            "620/620 - 1s - loss: 0.4502 - acc: 0.7903 - val_loss: 0.7238 - val_acc: 0.6400\n",
            "Epoch 45/300\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.67246\n",
            "620/620 - 1s - loss: 0.4226 - acc: 0.8129 - val_loss: 0.7668 - val_acc: 0.6500\n",
            "Epoch 46/300\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.67246\n",
            "620/620 - 1s - loss: 0.4543 - acc: 0.7935 - val_loss: 0.7281 - val_acc: 0.6200\n",
            "Epoch 47/300\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.67246\n",
            "620/620 - 1s - loss: 0.4103 - acc: 0.8177 - val_loss: 0.7864 - val_acc: 0.6300\n",
            "Epoch 48/300\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.67246\n",
            "620/620 - 1s - loss: 0.4096 - acc: 0.8145 - val_loss: 0.7232 - val_acc: 0.6000\n",
            "Epoch 49/300\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.67246\n",
            "620/620 - 1s - loss: 0.4251 - acc: 0.8000 - val_loss: 0.7134 - val_acc: 0.6200\n",
            "Epoch 50/300\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.67246\n",
            "620/620 - 1s - loss: 0.4671 - acc: 0.7806 - val_loss: 0.7000 - val_acc: 0.6300\n",
            "Epoch 51/300\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.67246\n",
            "620/620 - 1s - loss: 0.4118 - acc: 0.8242 - val_loss: 0.7241 - val_acc: 0.6300\n",
            "Epoch 52/300\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.67246\n",
            "620/620 - 1s - loss: 0.4320 - acc: 0.8081 - val_loss: 0.7092 - val_acc: 0.5900\n",
            "Epoch 53/300\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.67246\n",
            "620/620 - 1s - loss: 0.4076 - acc: 0.8000 - val_loss: 0.7056 - val_acc: 0.6400\n",
            "Epoch 54/300\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.67246\n",
            "620/620 - 1s - loss: 0.4337 - acc: 0.8065 - val_loss: 0.7285 - val_acc: 0.6600\n",
            "Epoch 55/300\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.67246\n",
            "620/620 - 1s - loss: 0.4442 - acc: 0.7935 - val_loss: 0.6731 - val_acc: 0.6000\n",
            "Epoch 56/300\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.67246\n",
            "620/620 - 1s - loss: 0.4039 - acc: 0.8145 - val_loss: 0.7390 - val_acc: 0.6000\n",
            "Epoch 57/300\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.67246\n",
            "620/620 - 1s - loss: 0.4357 - acc: 0.7968 - val_loss: 0.6929 - val_acc: 0.6100\n",
            "Epoch 58/300\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.67246\n",
            "620/620 - 1s - loss: 0.4118 - acc: 0.8258 - val_loss: 0.6837 - val_acc: 0.6400\n",
            "Epoch 59/300\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.67246\n",
            "620/620 - 1s - loss: 0.4290 - acc: 0.8048 - val_loss: 0.6733 - val_acc: 0.6400\n",
            "Epoch 60/300\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.67246\n",
            "620/620 - 1s - loss: 0.4168 - acc: 0.8065 - val_loss: 0.6903 - val_acc: 0.6200\n",
            "Epoch 61/300\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.67246\n",
            "620/620 - 1s - loss: 0.4488 - acc: 0.7952 - val_loss: 0.8096 - val_acc: 0.6100\n",
            "Epoch 62/300\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.67246\n",
            "620/620 - 1s - loss: 0.4061 - acc: 0.8048 - val_loss: 0.7021 - val_acc: 0.6100\n",
            "Epoch 63/300\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.67246\n",
            "620/620 - 1s - loss: 0.4014 - acc: 0.8242 - val_loss: 0.7464 - val_acc: 0.6200\n",
            "Epoch 64/300\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.67246\n",
            "620/620 - 1s - loss: 0.4395 - acc: 0.7903 - val_loss: 0.7536 - val_acc: 0.6000\n",
            "Epoch 65/300\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.67246\n",
            "620/620 - 1s - loss: 0.4008 - acc: 0.8210 - val_loss: 0.7276 - val_acc: 0.6300\n",
            "Epoch 66/300\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.67246\n",
            "620/620 - 1s - loss: 0.4088 - acc: 0.8129 - val_loss: 0.7006 - val_acc: 0.6500\n",
            "Epoch 67/300\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.67246\n",
            "620/620 - 1s - loss: 0.4160 - acc: 0.8161 - val_loss: 0.8345 - val_acc: 0.6200\n",
            "Epoch 68/300\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.67246\n",
            "620/620 - 1s - loss: 0.4156 - acc: 0.8097 - val_loss: 0.7063 - val_acc: 0.6000\n",
            "Epoch 69/300\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.67246\n",
            "620/620 - 1s - loss: 0.4251 - acc: 0.7919 - val_loss: 0.7186 - val_acc: 0.6000\n",
            "Epoch 70/300\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.67246\n",
            "620/620 - 1s - loss: 0.4250 - acc: 0.7952 - val_loss: 0.7560 - val_acc: 0.6100\n",
            "Epoch 71/300\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.67246\n",
            "620/620 - 1s - loss: 0.4193 - acc: 0.7952 - val_loss: 0.6957 - val_acc: 0.6300\n",
            "Epoch 72/300\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.67246\n",
            "620/620 - 1s - loss: 0.3946 - acc: 0.8113 - val_loss: 0.9573 - val_acc: 0.5600\n",
            "Epoch 73/300\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.67246\n",
            "620/620 - 1s - loss: 0.4260 - acc: 0.8032 - val_loss: 0.7127 - val_acc: 0.6000\n",
            "Epoch 74/300\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.67246\n",
            "620/620 - 1s - loss: 0.4094 - acc: 0.8113 - val_loss: 0.8287 - val_acc: 0.6100\n",
            "Epoch 75/300\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.67246\n",
            "620/620 - 1s - loss: 0.4171 - acc: 0.8097 - val_loss: 0.7166 - val_acc: 0.6400\n",
            "Epoch 76/300\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.67246\n",
            "620/620 - 1s - loss: 0.3953 - acc: 0.8242 - val_loss: 0.6742 - val_acc: 0.6000\n",
            "Epoch 77/300\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.67246\n",
            "620/620 - 1s - loss: 0.4109 - acc: 0.7952 - val_loss: 0.7332 - val_acc: 0.6500\n",
            "Epoch 78/300\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.67246\n",
            "620/620 - 1s - loss: 0.4065 - acc: 0.8097 - val_loss: 0.6818 - val_acc: 0.6100\n",
            "Epoch 79/300\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.67246\n",
            "620/620 - 1s - loss: 0.3741 - acc: 0.8339 - val_loss: 0.7125 - val_acc: 0.6500\n",
            "Epoch 80/300\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.67246\n",
            "620/620 - 1s - loss: 0.4094 - acc: 0.8371 - val_loss: 0.6934 - val_acc: 0.6300\n",
            "Epoch 81/300\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.67246\n",
            "620/620 - 1s - loss: 0.3849 - acc: 0.8306 - val_loss: 0.6739 - val_acc: 0.6600\n",
            "Epoch 82/300\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.67246\n",
            "620/620 - 1s - loss: 0.4099 - acc: 0.8145 - val_loss: 0.7234 - val_acc: 0.6100\n",
            "Epoch 83/300\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.67246\n",
            "620/620 - 1s - loss: 0.3966 - acc: 0.8242 - val_loss: 0.8356 - val_acc: 0.6100\n",
            "Epoch 84/300\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.67246\n",
            "620/620 - 1s - loss: 0.4032 - acc: 0.8161 - val_loss: 0.7732 - val_acc: 0.6400\n",
            "Epoch 85/300\n",
            "\n",
            "Epoch 00085: val_loss improved from 0.67246 to 0.66562, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.4142 - acc: 0.8000 - val_loss: 0.6656 - val_acc: 0.6400\n",
            "Epoch 86/300\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.66562\n",
            "620/620 - 1s - loss: 0.3796 - acc: 0.8242 - val_loss: 0.6818 - val_acc: 0.6600\n",
            "Epoch 87/300\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.66562\n",
            "620/620 - 1s - loss: 0.3832 - acc: 0.8290 - val_loss: 0.6899 - val_acc: 0.6800\n",
            "Epoch 88/300\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.66562\n",
            "620/620 - 1s - loss: 0.3844 - acc: 0.8242 - val_loss: 0.6928 - val_acc: 0.6500\n",
            "Epoch 89/300\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.66562\n",
            "620/620 - 1s - loss: 0.3842 - acc: 0.8145 - val_loss: 0.8296 - val_acc: 0.6300\n",
            "Epoch 90/300\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.66562\n",
            "620/620 - 1s - loss: 0.4302 - acc: 0.8016 - val_loss: 0.7088 - val_acc: 0.6400\n",
            "Epoch 91/300\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.66562\n",
            "620/620 - 1s - loss: 0.3940 - acc: 0.8371 - val_loss: 0.7266 - val_acc: 0.6200\n",
            "Epoch 92/300\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.66562\n",
            "620/620 - 1s - loss: 0.4004 - acc: 0.8097 - val_loss: 0.7209 - val_acc: 0.6400\n",
            "Epoch 93/300\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.66562\n",
            "620/620 - 1s - loss: 0.3698 - acc: 0.8403 - val_loss: 0.6812 - val_acc: 0.6500\n",
            "Epoch 94/300\n",
            "\n",
            "Epoch 00094: val_loss improved from 0.66562 to 0.64534, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.3899 - acc: 0.8274 - val_loss: 0.6453 - val_acc: 0.6500\n",
            "Epoch 95/300\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3714 - acc: 0.8403 - val_loss: 0.6997 - val_acc: 0.6500\n",
            "Epoch 96/300\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3801 - acc: 0.8452 - val_loss: 0.6655 - val_acc: 0.6400\n",
            "Epoch 97/300\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.4004 - acc: 0.8113 - val_loss: 0.7415 - val_acc: 0.6300\n",
            "Epoch 98/300\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3664 - acc: 0.8468 - val_loss: 0.7162 - val_acc: 0.6100\n",
            "Epoch 99/300\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.4168 - acc: 0.8016 - val_loss: 0.8021 - val_acc: 0.6200\n",
            "Epoch 100/300\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3939 - acc: 0.8242 - val_loss: 0.6889 - val_acc: 0.6400\n",
            "Epoch 101/300\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3717 - acc: 0.8242 - val_loss: 0.7320 - val_acc: 0.6200\n",
            "Epoch 102/300\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3977 - acc: 0.8177 - val_loss: 0.7114 - val_acc: 0.6300\n",
            "Epoch 103/300\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3865 - acc: 0.8161 - val_loss: 0.7719 - val_acc: 0.6500\n",
            "Epoch 104/300\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.4065 - acc: 0.8226 - val_loss: 0.7532 - val_acc: 0.6300\n",
            "Epoch 105/300\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3749 - acc: 0.8177 - val_loss: 0.8319 - val_acc: 0.6300\n",
            "Epoch 106/300\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.4079 - acc: 0.8081 - val_loss: 0.7947 - val_acc: 0.6300\n",
            "Epoch 107/300\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3838 - acc: 0.8145 - val_loss: 0.7635 - val_acc: 0.6300\n",
            "Epoch 108/300\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.4092 - acc: 0.8161 - val_loss: 0.7616 - val_acc: 0.6100\n",
            "Epoch 109/300\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3710 - acc: 0.8468 - val_loss: 0.8037 - val_acc: 0.6600\n",
            "Epoch 110/300\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3861 - acc: 0.8306 - val_loss: 0.9199 - val_acc: 0.6300\n",
            "Epoch 111/300\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.4019 - acc: 0.8210 - val_loss: 0.7575 - val_acc: 0.5300\n",
            "Epoch 112/300\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3816 - acc: 0.8387 - val_loss: 0.6856 - val_acc: 0.6300\n",
            "Epoch 113/300\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3748 - acc: 0.8355 - val_loss: 0.7118 - val_acc: 0.6300\n",
            "Epoch 114/300\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3948 - acc: 0.8113 - val_loss: 0.8020 - val_acc: 0.6500\n",
            "Epoch 115/300\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3815 - acc: 0.8371 - val_loss: 0.7197 - val_acc: 0.6400\n",
            "Epoch 116/300\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3719 - acc: 0.8274 - val_loss: 0.9250 - val_acc: 0.5800\n",
            "Epoch 117/300\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3894 - acc: 0.8290 - val_loss: 0.6840 - val_acc: 0.6400\n",
            "Epoch 118/300\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3701 - acc: 0.8274 - val_loss: 0.7040 - val_acc: 0.6000\n",
            "Epoch 119/300\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3894 - acc: 0.8145 - val_loss: 0.7014 - val_acc: 0.6300\n",
            "Epoch 120/300\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3802 - acc: 0.8403 - val_loss: 0.7295 - val_acc: 0.6000\n",
            "Epoch 121/300\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3977 - acc: 0.8242 - val_loss: 0.7536 - val_acc: 0.6600\n",
            "Epoch 122/300\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3728 - acc: 0.8290 - val_loss: 0.8236 - val_acc: 0.6300\n",
            "Epoch 123/300\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3991 - acc: 0.8210 - val_loss: 0.7228 - val_acc: 0.6000\n",
            "Epoch 124/300\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3558 - acc: 0.8419 - val_loss: 0.7086 - val_acc: 0.6200\n",
            "Epoch 125/300\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3835 - acc: 0.8403 - val_loss: 0.8511 - val_acc: 0.6300\n",
            "Epoch 126/300\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3742 - acc: 0.8306 - val_loss: 0.6956 - val_acc: 0.6500\n",
            "Epoch 127/300\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.4041 - acc: 0.7984 - val_loss: 0.8408 - val_acc: 0.5900\n",
            "Epoch 128/300\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3648 - acc: 0.8387 - val_loss: 0.8132 - val_acc: 0.6200\n",
            "Epoch 129/300\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.4005 - acc: 0.8113 - val_loss: 0.9143 - val_acc: 0.5600\n",
            "Epoch 130/300\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.4018 - acc: 0.8210 - val_loss: 0.8049 - val_acc: 0.6200\n",
            "Epoch 131/300\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3556 - acc: 0.8565 - val_loss: 0.7000 - val_acc: 0.6500\n",
            "Epoch 132/300\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3811 - acc: 0.8339 - val_loss: 0.7848 - val_acc: 0.6500\n",
            "Epoch 133/300\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3837 - acc: 0.8258 - val_loss: 0.7414 - val_acc: 0.6400\n",
            "Epoch 134/300\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3941 - acc: 0.8242 - val_loss: 0.6980 - val_acc: 0.6400\n",
            "Epoch 135/300\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3971 - acc: 0.8097 - val_loss: 0.8927 - val_acc: 0.5600\n",
            "Epoch 136/300\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3530 - acc: 0.8323 - val_loss: 0.9053 - val_acc: 0.5900\n",
            "Epoch 137/300\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3586 - acc: 0.8468 - val_loss: 0.8324 - val_acc: 0.6000\n",
            "Epoch 138/300\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3894 - acc: 0.8306 - val_loss: 0.8266 - val_acc: 0.6300\n",
            "Epoch 139/300\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3639 - acc: 0.8371 - val_loss: 0.7187 - val_acc: 0.6100\n",
            "Epoch 140/300\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3661 - acc: 0.8484 - val_loss: 0.8174 - val_acc: 0.6100\n",
            "Epoch 141/300\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3696 - acc: 0.8274 - val_loss: 0.7112 - val_acc: 0.6200\n",
            "Epoch 142/300\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3741 - acc: 0.8403 - val_loss: 0.6986 - val_acc: 0.6200\n",
            "Epoch 143/300\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3477 - acc: 0.8532 - val_loss: 0.7688 - val_acc: 0.6000\n",
            "Epoch 144/300\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.4072 - acc: 0.8226 - val_loss: 0.8096 - val_acc: 0.5900\n",
            "Epoch 145/300\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3683 - acc: 0.8274 - val_loss: 0.7171 - val_acc: 0.6700\n",
            "Epoch 146/300\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3728 - acc: 0.8258 - val_loss: 0.8316 - val_acc: 0.5800\n",
            "Epoch 147/300\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3643 - acc: 0.8387 - val_loss: 0.8866 - val_acc: 0.5400\n",
            "Epoch 148/300\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.4015 - acc: 0.8242 - val_loss: 0.7110 - val_acc: 0.6300\n",
            "Epoch 149/300\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3854 - acc: 0.8403 - val_loss: 0.8123 - val_acc: 0.6500\n",
            "Epoch 150/300\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3652 - acc: 0.8210 - val_loss: 0.7340 - val_acc: 0.6000\n",
            "Epoch 151/300\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3715 - acc: 0.8355 - val_loss: 0.7829 - val_acc: 0.6300\n",
            "Epoch 152/300\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3845 - acc: 0.8387 - val_loss: 0.7233 - val_acc: 0.5900\n",
            "Epoch 153/300\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3397 - acc: 0.8532 - val_loss: 0.7394 - val_acc: 0.6400\n",
            "Epoch 154/300\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3329 - acc: 0.8613 - val_loss: 0.7394 - val_acc: 0.6200\n",
            "Epoch 155/300\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3561 - acc: 0.8355 - val_loss: 0.7127 - val_acc: 0.6300\n",
            "Epoch 156/300\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3489 - acc: 0.8419 - val_loss: 0.9300 - val_acc: 0.5700\n",
            "Epoch 157/300\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3954 - acc: 0.8177 - val_loss: 0.9572 - val_acc: 0.5700\n",
            "Epoch 158/300\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3815 - acc: 0.8323 - val_loss: 0.9123 - val_acc: 0.5900\n",
            "Epoch 159/300\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3463 - acc: 0.8661 - val_loss: 0.7280 - val_acc: 0.6600\n",
            "Epoch 160/300\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3530 - acc: 0.8516 - val_loss: 0.6920 - val_acc: 0.6400\n",
            "Epoch 161/300\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3577 - acc: 0.8258 - val_loss: 0.9738 - val_acc: 0.5400\n",
            "Epoch 162/300\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3853 - acc: 0.8129 - val_loss: 0.6828 - val_acc: 0.6100\n",
            "Epoch 163/300\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.4060 - acc: 0.8145 - val_loss: 0.6738 - val_acc: 0.6300\n",
            "Epoch 164/300\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3594 - acc: 0.8242 - val_loss: 0.6721 - val_acc: 0.6500\n",
            "Epoch 165/300\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3751 - acc: 0.8339 - val_loss: 0.6720 - val_acc: 0.6500\n",
            "Epoch 166/300\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3792 - acc: 0.8242 - val_loss: 0.7642 - val_acc: 0.6400\n",
            "Epoch 167/300\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3352 - acc: 0.8581 - val_loss: 0.7518 - val_acc: 0.6800\n",
            "Epoch 168/300\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3502 - acc: 0.8565 - val_loss: 0.7327 - val_acc: 0.6100\n",
            "Epoch 169/300\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3538 - acc: 0.8339 - val_loss: 0.7411 - val_acc: 0.6300\n",
            "Epoch 170/300\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.4021 - acc: 0.8048 - val_loss: 0.6818 - val_acc: 0.6200\n",
            "Epoch 171/300\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3682 - acc: 0.8371 - val_loss: 0.7200 - val_acc: 0.5900\n",
            "Epoch 172/300\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3812 - acc: 0.8290 - val_loss: 0.7221 - val_acc: 0.6200\n",
            "Epoch 173/300\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3523 - acc: 0.8371 - val_loss: 0.7844 - val_acc: 0.5900\n",
            "Epoch 174/300\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3786 - acc: 0.8339 - val_loss: 0.8959 - val_acc: 0.5700\n",
            "Epoch 175/300\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3284 - acc: 0.8629 - val_loss: 0.7963 - val_acc: 0.5600\n",
            "Epoch 176/300\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3571 - acc: 0.8371 - val_loss: 0.7559 - val_acc: 0.6100\n",
            "Epoch 177/300\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3410 - acc: 0.8468 - val_loss: 0.7025 - val_acc: 0.6500\n",
            "Epoch 178/300\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3641 - acc: 0.8387 - val_loss: 0.7397 - val_acc: 0.6200\n",
            "Epoch 179/300\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3491 - acc: 0.8548 - val_loss: 0.7212 - val_acc: 0.6300\n",
            "Epoch 180/300\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3785 - acc: 0.8226 - val_loss: 0.8195 - val_acc: 0.5800\n",
            "Epoch 181/300\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3658 - acc: 0.8274 - val_loss: 0.8489 - val_acc: 0.5700\n",
            "Epoch 182/300\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3645 - acc: 0.8452 - val_loss: 0.7687 - val_acc: 0.6100\n",
            "Epoch 183/300\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3465 - acc: 0.8419 - val_loss: 0.8825 - val_acc: 0.5700\n",
            "Epoch 184/300\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3731 - acc: 0.8371 - val_loss: 0.7641 - val_acc: 0.5800\n",
            "Epoch 185/300\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3639 - acc: 0.8242 - val_loss: 0.8177 - val_acc: 0.6200\n",
            "Epoch 186/300\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3279 - acc: 0.8677 - val_loss: 0.7989 - val_acc: 0.6000\n",
            "Epoch 187/300\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3401 - acc: 0.8629 - val_loss: 0.8184 - val_acc: 0.6000\n",
            "Epoch 188/300\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3392 - acc: 0.8581 - val_loss: 0.7315 - val_acc: 0.6200\n",
            "Epoch 189/300\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3398 - acc: 0.8419 - val_loss: 0.7398 - val_acc: 0.6200\n",
            "Epoch 190/300\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3942 - acc: 0.8145 - val_loss: 0.8193 - val_acc: 0.5900\n",
            "Epoch 191/300\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3266 - acc: 0.8661 - val_loss: 0.7631 - val_acc: 0.6000\n",
            "Epoch 192/300\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3364 - acc: 0.8452 - val_loss: 0.7355 - val_acc: 0.6000\n",
            "Epoch 193/300\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3485 - acc: 0.8468 - val_loss: 0.7361 - val_acc: 0.6300\n",
            "Epoch 194/300\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3337 - acc: 0.8532 - val_loss: 0.8563 - val_acc: 0.5800\n",
            "Epoch 195/300\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3725 - acc: 0.8290 - val_loss: 0.7898 - val_acc: 0.5900\n",
            "Epoch 196/300\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3657 - acc: 0.8387 - val_loss: 0.7405 - val_acc: 0.6400\n",
            "Epoch 197/300\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3311 - acc: 0.8565 - val_loss: 0.8206 - val_acc: 0.5700\n",
            "Epoch 198/300\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3642 - acc: 0.8339 - val_loss: 0.7749 - val_acc: 0.6100\n",
            "Epoch 199/300\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3482 - acc: 0.8435 - val_loss: 0.8308 - val_acc: 0.5800\n",
            "Epoch 200/300\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3528 - acc: 0.8468 - val_loss: 0.8621 - val_acc: 0.5600\n",
            "Epoch 201/300\n",
            "\n",
            "Epoch 00201: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3442 - acc: 0.8403 - val_loss: 0.8162 - val_acc: 0.5700\n",
            "Epoch 202/300\n",
            "\n",
            "Epoch 00202: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3900 - acc: 0.8194 - val_loss: 0.8378 - val_acc: 0.5600\n",
            "Epoch 203/300\n",
            "\n",
            "Epoch 00203: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3822 - acc: 0.8274 - val_loss: 0.7286 - val_acc: 0.6300\n",
            "Epoch 204/300\n",
            "\n",
            "Epoch 00204: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3290 - acc: 0.8677 - val_loss: 0.7149 - val_acc: 0.6400\n",
            "Epoch 205/300\n",
            "\n",
            "Epoch 00205: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3335 - acc: 0.8484 - val_loss: 0.7477 - val_acc: 0.5700\n",
            "Epoch 206/300\n",
            "\n",
            "Epoch 00206: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3444 - acc: 0.8516 - val_loss: 0.7387 - val_acc: 0.6600\n",
            "Epoch 207/300\n",
            "\n",
            "Epoch 00207: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3520 - acc: 0.8226 - val_loss: 0.7654 - val_acc: 0.6000\n",
            "Epoch 208/300\n",
            "\n",
            "Epoch 00208: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3676 - acc: 0.8484 - val_loss: 0.9210 - val_acc: 0.5500\n",
            "Epoch 209/300\n",
            "\n",
            "Epoch 00209: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3482 - acc: 0.8500 - val_loss: 0.8821 - val_acc: 0.5800\n",
            "Epoch 210/300\n",
            "\n",
            "Epoch 00210: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3398 - acc: 0.8677 - val_loss: 0.7905 - val_acc: 0.5600\n",
            "Epoch 211/300\n",
            "\n",
            "Epoch 00211: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3360 - acc: 0.8565 - val_loss: 0.8070 - val_acc: 0.5300\n",
            "Epoch 212/300\n",
            "\n",
            "Epoch 00212: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3666 - acc: 0.8403 - val_loss: 0.8639 - val_acc: 0.5500\n",
            "Epoch 213/300\n",
            "\n",
            "Epoch 00213: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3256 - acc: 0.8629 - val_loss: 0.7457 - val_acc: 0.6400\n",
            "Epoch 214/300\n",
            "\n",
            "Epoch 00214: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3179 - acc: 0.8694 - val_loss: 0.7623 - val_acc: 0.5700\n",
            "Epoch 215/300\n",
            "\n",
            "Epoch 00215: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3370 - acc: 0.8435 - val_loss: 1.1389 - val_acc: 0.5300\n",
            "Epoch 216/300\n",
            "\n",
            "Epoch 00216: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3762 - acc: 0.8306 - val_loss: 0.7855 - val_acc: 0.5700\n",
            "Epoch 217/300\n",
            "\n",
            "Epoch 00217: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3356 - acc: 0.8435 - val_loss: 0.8613 - val_acc: 0.5300\n",
            "Epoch 218/300\n",
            "\n",
            "Epoch 00218: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3370 - acc: 0.8581 - val_loss: 0.7362 - val_acc: 0.5800\n",
            "Epoch 219/300\n",
            "\n",
            "Epoch 00219: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3576 - acc: 0.8339 - val_loss: 0.7782 - val_acc: 0.5800\n",
            "Epoch 220/300\n",
            "\n",
            "Epoch 00220: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3332 - acc: 0.8581 - val_loss: 0.7409 - val_acc: 0.6200\n",
            "Epoch 221/300\n",
            "\n",
            "Epoch 00221: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3503 - acc: 0.8484 - val_loss: 0.8153 - val_acc: 0.5600\n",
            "Epoch 222/300\n",
            "\n",
            "Epoch 00222: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3665 - acc: 0.8387 - val_loss: 0.7476 - val_acc: 0.6100\n",
            "Epoch 223/300\n",
            "\n",
            "Epoch 00223: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3349 - acc: 0.8597 - val_loss: 0.7969 - val_acc: 0.5900\n",
            "Epoch 224/300\n",
            "\n",
            "Epoch 00224: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3432 - acc: 0.8532 - val_loss: 0.7727 - val_acc: 0.5900\n",
            "Epoch 225/300\n",
            "\n",
            "Epoch 00225: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3389 - acc: 0.8645 - val_loss: 0.7389 - val_acc: 0.6100\n",
            "Epoch 226/300\n",
            "\n",
            "Epoch 00226: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3172 - acc: 0.8710 - val_loss: 0.9265 - val_acc: 0.5300\n",
            "Epoch 227/300\n",
            "\n",
            "Epoch 00227: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3328 - acc: 0.8484 - val_loss: 0.7446 - val_acc: 0.6400\n",
            "Epoch 228/300\n",
            "\n",
            "Epoch 00228: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3534 - acc: 0.8371 - val_loss: 0.8600 - val_acc: 0.5700\n",
            "Epoch 229/300\n",
            "\n",
            "Epoch 00229: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3280 - acc: 0.8645 - val_loss: 0.8612 - val_acc: 0.5600\n",
            "Epoch 230/300\n",
            "\n",
            "Epoch 00230: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3310 - acc: 0.8613 - val_loss: 0.8205 - val_acc: 0.5300\n",
            "Epoch 231/300\n",
            "\n",
            "Epoch 00231: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3677 - acc: 0.8355 - val_loss: 0.7357 - val_acc: 0.6200\n",
            "Epoch 232/300\n",
            "\n",
            "Epoch 00232: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3653 - acc: 0.8387 - val_loss: 0.9071 - val_acc: 0.5400\n",
            "Epoch 233/300\n",
            "\n",
            "Epoch 00233: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3113 - acc: 0.8645 - val_loss: 0.8638 - val_acc: 0.5500\n",
            "Epoch 234/300\n",
            "\n",
            "Epoch 00234: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3262 - acc: 0.8790 - val_loss: 0.9822 - val_acc: 0.5400\n",
            "Epoch 235/300\n",
            "\n",
            "Epoch 00235: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3339 - acc: 0.8565 - val_loss: 0.7502 - val_acc: 0.5900\n",
            "Epoch 236/300\n",
            "\n",
            "Epoch 00236: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3319 - acc: 0.8645 - val_loss: 1.0190 - val_acc: 0.5500\n",
            "Epoch 237/300\n",
            "\n",
            "Epoch 00237: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3529 - acc: 0.8323 - val_loss: 0.7644 - val_acc: 0.6500\n",
            "Epoch 238/300\n",
            "\n",
            "Epoch 00238: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3203 - acc: 0.8710 - val_loss: 0.8521 - val_acc: 0.5200\n",
            "Epoch 239/300\n",
            "\n",
            "Epoch 00239: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3119 - acc: 0.8790 - val_loss: 0.8213 - val_acc: 0.5600\n",
            "Epoch 240/300\n",
            "\n",
            "Epoch 00240: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3430 - acc: 0.8516 - val_loss: 0.7977 - val_acc: 0.5800\n",
            "Epoch 241/300\n",
            "\n",
            "Epoch 00241: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3370 - acc: 0.8500 - val_loss: 0.8703 - val_acc: 0.5600\n",
            "Epoch 242/300\n",
            "\n",
            "Epoch 00242: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3094 - acc: 0.8726 - val_loss: 0.8791 - val_acc: 0.5800\n",
            "Epoch 243/300\n",
            "\n",
            "Epoch 00243: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3496 - acc: 0.8355 - val_loss: 0.7976 - val_acc: 0.6000\n",
            "Epoch 244/300\n",
            "\n",
            "Epoch 00244: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3625 - acc: 0.8355 - val_loss: 0.9115 - val_acc: 0.5300\n",
            "Epoch 245/300\n",
            "\n",
            "Epoch 00245: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3237 - acc: 0.8597 - val_loss: 0.8254 - val_acc: 0.5700\n",
            "Epoch 246/300\n",
            "\n",
            "Epoch 00246: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3296 - acc: 0.8629 - val_loss: 0.8241 - val_acc: 0.5700\n",
            "Epoch 247/300\n",
            "\n",
            "Epoch 00247: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3513 - acc: 0.8613 - val_loss: 0.9239 - val_acc: 0.5600\n",
            "Epoch 248/300\n",
            "\n",
            "Epoch 00248: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3480 - acc: 0.8290 - val_loss: 0.8953 - val_acc: 0.5800\n",
            "Epoch 249/300\n",
            "\n",
            "Epoch 00249: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3394 - acc: 0.8484 - val_loss: 0.9539 - val_acc: 0.5900\n",
            "Epoch 250/300\n",
            "\n",
            "Epoch 00250: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3595 - acc: 0.8435 - val_loss: 0.7870 - val_acc: 0.5800\n",
            "Epoch 251/300\n",
            "\n",
            "Epoch 00251: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3138 - acc: 0.8597 - val_loss: 0.8610 - val_acc: 0.5300\n",
            "Epoch 252/300\n",
            "\n",
            "Epoch 00252: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3502 - acc: 0.8500 - val_loss: 0.8344 - val_acc: 0.5500\n",
            "Epoch 253/300\n",
            "\n",
            "Epoch 00253: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3364 - acc: 0.8613 - val_loss: 0.7918 - val_acc: 0.6100\n",
            "Epoch 254/300\n",
            "\n",
            "Epoch 00254: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3273 - acc: 0.8532 - val_loss: 0.9055 - val_acc: 0.5400\n",
            "Epoch 255/300\n",
            "\n",
            "Epoch 00255: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3279 - acc: 0.8645 - val_loss: 0.8627 - val_acc: 0.5700\n",
            "Epoch 256/300\n",
            "\n",
            "Epoch 00256: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3468 - acc: 0.8355 - val_loss: 0.8665 - val_acc: 0.5500\n",
            "Epoch 257/300\n",
            "\n",
            "Epoch 00257: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3455 - acc: 0.8581 - val_loss: 0.8222 - val_acc: 0.5800\n",
            "Epoch 258/300\n",
            "\n",
            "Epoch 00258: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3102 - acc: 0.8726 - val_loss: 0.8628 - val_acc: 0.5500\n",
            "Epoch 259/300\n",
            "\n",
            "Epoch 00259: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3577 - acc: 0.8339 - val_loss: 0.8032 - val_acc: 0.5800\n",
            "Epoch 260/300\n",
            "\n",
            "Epoch 00260: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3079 - acc: 0.8710 - val_loss: 0.7847 - val_acc: 0.5800\n",
            "Epoch 261/300\n",
            "\n",
            "Epoch 00261: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3338 - acc: 0.8387 - val_loss: 0.7464 - val_acc: 0.5900\n",
            "Epoch 262/300\n",
            "\n",
            "Epoch 00262: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.2920 - acc: 0.8952 - val_loss: 0.8203 - val_acc: 0.5900\n",
            "Epoch 263/300\n",
            "\n",
            "Epoch 00263: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3251 - acc: 0.8694 - val_loss: 0.8672 - val_acc: 0.5700\n",
            "Epoch 264/300\n",
            "\n",
            "Epoch 00264: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3324 - acc: 0.8661 - val_loss: 0.9382 - val_acc: 0.5300\n",
            "Epoch 265/300\n",
            "\n",
            "Epoch 00265: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3155 - acc: 0.8629 - val_loss: 0.8158 - val_acc: 0.5700\n",
            "Epoch 266/300\n",
            "\n",
            "Epoch 00266: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3613 - acc: 0.8323 - val_loss: 0.9075 - val_acc: 0.5600\n",
            "Epoch 267/300\n",
            "\n",
            "Epoch 00267: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3206 - acc: 0.8629 - val_loss: 0.8229 - val_acc: 0.5500\n",
            "Epoch 268/300\n",
            "\n",
            "Epoch 00268: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3326 - acc: 0.8661 - val_loss: 0.7816 - val_acc: 0.5800\n",
            "Epoch 269/300\n",
            "\n",
            "Epoch 00269: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3121 - acc: 0.8710 - val_loss: 0.8927 - val_acc: 0.5600\n",
            "Epoch 270/300\n",
            "\n",
            "Epoch 00270: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3114 - acc: 0.8484 - val_loss: 0.9351 - val_acc: 0.5300\n",
            "Epoch 271/300\n",
            "\n",
            "Epoch 00271: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3446 - acc: 0.8435 - val_loss: 0.7718 - val_acc: 0.5600\n",
            "Epoch 272/300\n",
            "\n",
            "Epoch 00272: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3411 - acc: 0.8581 - val_loss: 0.8781 - val_acc: 0.5500\n",
            "Epoch 273/300\n",
            "\n",
            "Epoch 00273: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3330 - acc: 0.8532 - val_loss: 1.0416 - val_acc: 0.5100\n",
            "Epoch 274/300\n",
            "\n",
            "Epoch 00274: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3321 - acc: 0.8468 - val_loss: 0.8389 - val_acc: 0.5500\n",
            "Epoch 275/300\n",
            "\n",
            "Epoch 00275: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3474 - acc: 0.8435 - val_loss: 0.9090 - val_acc: 0.5400\n",
            "Epoch 276/300\n",
            "\n",
            "Epoch 00276: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3038 - acc: 0.8645 - val_loss: 0.9982 - val_acc: 0.5400\n",
            "Epoch 277/300\n",
            "\n",
            "Epoch 00277: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3270 - acc: 0.8565 - val_loss: 0.7916 - val_acc: 0.5800\n",
            "Epoch 278/300\n",
            "\n",
            "Epoch 00278: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3335 - acc: 0.8548 - val_loss: 0.7702 - val_acc: 0.5500\n",
            "Epoch 279/300\n",
            "\n",
            "Epoch 00279: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.2984 - acc: 0.8726 - val_loss: 0.9925 - val_acc: 0.5500\n",
            "Epoch 280/300\n",
            "\n",
            "Epoch 00280: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3033 - acc: 0.8806 - val_loss: 0.7998 - val_acc: 0.5900\n",
            "Epoch 281/300\n",
            "\n",
            "Epoch 00281: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.2901 - acc: 0.8742 - val_loss: 0.7494 - val_acc: 0.5800\n",
            "Epoch 282/300\n",
            "\n",
            "Epoch 00282: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3267 - acc: 0.8726 - val_loss: 0.8847 - val_acc: 0.5300\n",
            "Epoch 283/300\n",
            "\n",
            "Epoch 00283: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3372 - acc: 0.8500 - val_loss: 0.8869 - val_acc: 0.5300\n",
            "Epoch 284/300\n",
            "\n",
            "Epoch 00284: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.2766 - acc: 0.8919 - val_loss: 0.9566 - val_acc: 0.5100\n",
            "Epoch 285/300\n",
            "\n",
            "Epoch 00285: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3172 - acc: 0.8645 - val_loss: 0.7990 - val_acc: 0.5800\n",
            "Epoch 286/300\n",
            "\n",
            "Epoch 00286: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3673 - acc: 0.8371 - val_loss: 0.8750 - val_acc: 0.5500\n",
            "Epoch 287/300\n",
            "\n",
            "Epoch 00287: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3124 - acc: 0.8661 - val_loss: 0.9977 - val_acc: 0.5300\n",
            "Epoch 288/300\n",
            "\n",
            "Epoch 00288: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.2697 - acc: 0.8935 - val_loss: 0.9971 - val_acc: 0.5100\n",
            "Epoch 289/300\n",
            "\n",
            "Epoch 00289: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3207 - acc: 0.8597 - val_loss: 1.0129 - val_acc: 0.5300\n",
            "Epoch 290/300\n",
            "\n",
            "Epoch 00290: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3085 - acc: 0.8597 - val_loss: 0.8731 - val_acc: 0.5300\n",
            "Epoch 291/300\n",
            "\n",
            "Epoch 00291: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.2962 - acc: 0.8758 - val_loss: 0.8278 - val_acc: 0.5400\n",
            "Epoch 292/300\n",
            "\n",
            "Epoch 00292: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3171 - acc: 0.8839 - val_loss: 0.8081 - val_acc: 0.5600\n",
            "Epoch 293/300\n",
            "\n",
            "Epoch 00293: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3071 - acc: 0.8661 - val_loss: 0.8395 - val_acc: 0.5300\n",
            "Epoch 294/300\n",
            "\n",
            "Epoch 00294: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3122 - acc: 0.8823 - val_loss: 0.7502 - val_acc: 0.5600\n",
            "Epoch 295/300\n",
            "\n",
            "Epoch 00295: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3173 - acc: 0.8516 - val_loss: 1.0609 - val_acc: 0.5200\n",
            "Epoch 296/300\n",
            "\n",
            "Epoch 00296: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3064 - acc: 0.8661 - val_loss: 0.8033 - val_acc: 0.5700\n",
            "Epoch 297/300\n",
            "\n",
            "Epoch 00297: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3040 - acc: 0.8613 - val_loss: 0.9186 - val_acc: 0.5500\n",
            "Epoch 298/300\n",
            "\n",
            "Epoch 00298: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3074 - acc: 0.8677 - val_loss: 0.9662 - val_acc: 0.5500\n",
            "Epoch 299/300\n",
            "\n",
            "Epoch 00299: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3408 - acc: 0.8419 - val_loss: 0.8417 - val_acc: 0.5800\n",
            "Epoch 300/300\n",
            "\n",
            "Epoch 00300: val_loss did not improve from 0.64534\n",
            "620/620 - 1s - loss: 0.3036 - acc: 0.8629 - val_loss: 0.8522 - val_acc: 0.5400\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ncFBCvgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "# Evaluate on test data\n",
            "40/40 [==============================] - 0s 3ms/sample - loss: 0.7936 - acc: 0.5000\n",
            "test loss, test acc: [0.7936480707954615, 0.5]\n",
            "1\n",
            "EEG_Deep/Data2A/parsed_P01T.mat\n",
            "(0, 12, 4096)\n",
            "(80, 12, 4096)\n",
            "(80, 1)\n",
            "2\n",
            "EEG_Deep/Data2A/parsed_P02T.mat\n",
            "(80, 12, 4096)\n",
            "(160, 12, 4096)\n",
            "(160, 1)\n",
            "3\n",
            "EEG_Deep/Data2A/parsed_P03T.mat\n",
            "(160, 12, 4096)\n",
            "(240, 12, 4096)\n",
            "(240, 1)\n",
            "4\n",
            "EEG_Deep/Data2A/parsed_P04T.mat\n",
            "(240, 12, 4096)\n",
            "(320, 12, 4096)\n",
            "(320, 1)\n",
            "5\n",
            "EEG_Deep/Data2A/parsed_P05T.mat\n",
            "(320, 12, 4096)\n",
            "(400, 12, 4096)\n",
            "(400, 1)\n",
            "6\n",
            "EEG_Deep/Data2A/parsed_P06T.mat\n",
            "(400, 12, 4096)\n",
            "(480, 12, 4096)\n",
            "(480, 1)\n",
            "8\n",
            "EEG_Deep/Data2A/parsed_P08T.mat\n",
            "(480, 12, 4096)\n",
            "(560, 12, 4096)\n",
            "(560, 1)\n",
            "9\n",
            "EEG_Deep/Data2A/parsed_P09T.mat\n",
            "(560, 12, 4096)\n",
            "(640, 12, 4096)\n",
            "(640, 1)\n",
            "10\n",
            "EEG_Deep/Data2A/parsed_P010T.mat\n",
            "(640, 12, 4096)\n",
            "(720, 12, 4096)\n",
            "(720, 1)\n",
            "7\n",
            "EEG_Deep/Data2A/parsed_P07E.mat\n",
            "(0, 12, 4096)\n",
            "(40, 12, 4096)\n",
            "(40, 1)\n",
            "Filtering of training data in progress\n",
            "(720, 12, 4096)\n",
            "Filtering of test data in progress\n",
            "(40, 12, 4096)\n",
            "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
            " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1.]\n",
            "(620, 12, 1536)\n",
            "(620,)\n",
            "(100, 12, 1536)\n",
            "(100,)\n",
            "[[1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]]\n",
            "X_train shape: (620, 1, 12, 1536)\n",
            "620 train samples\n",
            "100 val samples\n",
            "(40, 12, 1536)\n",
            "(40, 1)\n",
            "X_train shape: (40, 1, 12, 1536)\n",
            "40 train samples\n",
            "Train on 620 samples, validate on 100 samples\n",
            "Epoch 1/300\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 1.17628, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.9378 - acc: 0.4823 - val_loss: 1.1763 - val_acc: 0.5100\n",
            "Epoch 2/300\n",
            "\n",
            "Epoch 00002: val_loss improved from 1.17628 to 1.03308, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.7632 - acc: 0.5935 - val_loss: 1.0331 - val_acc: 0.5100\n",
            "Epoch 3/300\n",
            "\n",
            "Epoch 00003: val_loss improved from 1.03308 to 0.93733, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.7242 - acc: 0.6210 - val_loss: 0.9373 - val_acc: 0.6000\n",
            "Epoch 4/300\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.93733 to 0.84170, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.6870 - acc: 0.6613 - val_loss: 0.8417 - val_acc: 0.5200\n",
            "Epoch 5/300\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.84170 to 0.75619, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.6176 - acc: 0.7226 - val_loss: 0.7562 - val_acc: 0.5600\n",
            "Epoch 6/300\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.75619 to 0.72528, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.5668 - acc: 0.7210 - val_loss: 0.7253 - val_acc: 0.5500\n",
            "Epoch 7/300\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.72528\n",
            "620/620 - 1s - loss: 0.5444 - acc: 0.7597 - val_loss: 0.7392 - val_acc: 0.5700\n",
            "Epoch 8/300\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.72528\n",
            "620/620 - 1s - loss: 0.4996 - acc: 0.7694 - val_loss: 0.7442 - val_acc: 0.5600\n",
            "Epoch 9/300\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.72528\n",
            "620/620 - 1s - loss: 0.5110 - acc: 0.7677 - val_loss: 0.7783 - val_acc: 0.5500\n",
            "Epoch 10/300\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.72528\n",
            "620/620 - 1s - loss: 0.5060 - acc: 0.7629 - val_loss: 0.9948 - val_acc: 0.5100\n",
            "Epoch 11/300\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.72528\n",
            "620/620 - 1s - loss: 0.4964 - acc: 0.7661 - val_loss: 1.1578 - val_acc: 0.5200\n",
            "Epoch 12/300\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.72528\n",
            "620/620 - 1s - loss: 0.5130 - acc: 0.7565 - val_loss: 0.7390 - val_acc: 0.5700\n",
            "Epoch 13/300\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.72528\n",
            "620/620 - 1s - loss: 0.4905 - acc: 0.7661 - val_loss: 0.8032 - val_acc: 0.5200\n",
            "Epoch 14/300\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.72528\n",
            "620/620 - 1s - loss: 0.4944 - acc: 0.7710 - val_loss: 0.7570 - val_acc: 0.5700\n",
            "Epoch 15/300\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.72528\n",
            "620/620 - 1s - loss: 0.4881 - acc: 0.7694 - val_loss: 0.9651 - val_acc: 0.5500\n",
            "Epoch 16/300\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.72528\n",
            "620/620 - 1s - loss: 0.4735 - acc: 0.7774 - val_loss: 0.7373 - val_acc: 0.5600\n",
            "Epoch 17/300\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.72528\n",
            "620/620 - 1s - loss: 0.4653 - acc: 0.7871 - val_loss: 0.9406 - val_acc: 0.5300\n",
            "Epoch 18/300\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.72528\n",
            "620/620 - 1s - loss: 0.4769 - acc: 0.7935 - val_loss: 0.9420 - val_acc: 0.5200\n",
            "Epoch 19/300\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.72528\n",
            "620/620 - 1s - loss: 0.4403 - acc: 0.8097 - val_loss: 1.0644 - val_acc: 0.5100\n",
            "Epoch 20/300\n",
            "\n",
            "Epoch 00020: val_loss improved from 0.72528 to 0.69022, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.4600 - acc: 0.7919 - val_loss: 0.6902 - val_acc: 0.5700\n",
            "Epoch 21/300\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.69022\n",
            "620/620 - 1s - loss: 0.4582 - acc: 0.7839 - val_loss: 0.7974 - val_acc: 0.5700\n",
            "Epoch 22/300\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.69022\n",
            "620/620 - 1s - loss: 0.4200 - acc: 0.8129 - val_loss: 1.0083 - val_acc: 0.5300\n",
            "Epoch 23/300\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.69022\n",
            "620/620 - 1s - loss: 0.4379 - acc: 0.7968 - val_loss: 1.1120 - val_acc: 0.5000\n",
            "Epoch 24/300\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.69022\n",
            "620/620 - 1s - loss: 0.4355 - acc: 0.7984 - val_loss: 0.9287 - val_acc: 0.5300\n",
            "Epoch 25/300\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.69022\n",
            "620/620 - 1s - loss: 0.4289 - acc: 0.8129 - val_loss: 0.9257 - val_acc: 0.5100\n",
            "Epoch 26/300\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.69022\n",
            "620/620 - 1s - loss: 0.4994 - acc: 0.7581 - val_loss: 0.8913 - val_acc: 0.5400\n",
            "Epoch 27/300\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.69022\n",
            "620/620 - 1s - loss: 0.4498 - acc: 0.8016 - val_loss: 0.8927 - val_acc: 0.5200\n",
            "Epoch 28/300\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.69022\n",
            "620/620 - 1s - loss: 0.4327 - acc: 0.7984 - val_loss: 0.8919 - val_acc: 0.5400\n",
            "Epoch 29/300\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.69022\n",
            "620/620 - 1s - loss: 0.4365 - acc: 0.8016 - val_loss: 0.8081 - val_acc: 0.5500\n",
            "Epoch 30/300\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.69022\n",
            "620/620 - 1s - loss: 0.4359 - acc: 0.7968 - val_loss: 1.1199 - val_acc: 0.5100\n",
            "Epoch 31/300\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.69022\n",
            "620/620 - 1s - loss: 0.4132 - acc: 0.8339 - val_loss: 1.0492 - val_acc: 0.5400\n",
            "Epoch 32/300\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.69022\n",
            "620/620 - 1s - loss: 0.4335 - acc: 0.8097 - val_loss: 1.0063 - val_acc: 0.5300\n",
            "Epoch 33/300\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.69022\n",
            "620/620 - 1s - loss: 0.4193 - acc: 0.8177 - val_loss: 0.9004 - val_acc: 0.5400\n",
            "Epoch 34/300\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.69022\n",
            "620/620 - 1s - loss: 0.4271 - acc: 0.8113 - val_loss: 0.7919 - val_acc: 0.5700\n",
            "Epoch 35/300\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.69022\n",
            "620/620 - 1s - loss: 0.4494 - acc: 0.7952 - val_loss: 0.7239 - val_acc: 0.5700\n",
            "Epoch 36/300\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.69022\n",
            "620/620 - 1s - loss: 0.4351 - acc: 0.7935 - val_loss: 1.6189 - val_acc: 0.5000\n",
            "Epoch 37/300\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.69022\n",
            "620/620 - 1s - loss: 0.4191 - acc: 0.8016 - val_loss: 0.8397 - val_acc: 0.5500\n",
            "Epoch 38/300\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.69022\n",
            "620/620 - 1s - loss: 0.4209 - acc: 0.8097 - val_loss: 0.7602 - val_acc: 0.5600\n",
            "Epoch 39/300\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.69022\n",
            "620/620 - 1s - loss: 0.4238 - acc: 0.8145 - val_loss: 0.9397 - val_acc: 0.5400\n",
            "Epoch 40/300\n",
            "\n",
            "Epoch 00040: val_loss improved from 0.69022 to 0.68361, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.4242 - acc: 0.8081 - val_loss: 0.6836 - val_acc: 0.5900\n",
            "Epoch 41/300\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.4175 - acc: 0.8048 - val_loss: 0.8505 - val_acc: 0.5300\n",
            "Epoch 42/300\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.4050 - acc: 0.8194 - val_loss: 0.7616 - val_acc: 0.5500\n",
            "Epoch 43/300\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.4295 - acc: 0.7742 - val_loss: 0.9680 - val_acc: 0.5200\n",
            "Epoch 44/300\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.4189 - acc: 0.8065 - val_loss: 1.0855 - val_acc: 0.5100\n",
            "Epoch 45/300\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.4056 - acc: 0.8177 - val_loss: 0.9221 - val_acc: 0.5200\n",
            "Epoch 46/300\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3939 - acc: 0.8161 - val_loss: 1.0788 - val_acc: 0.5400\n",
            "Epoch 47/300\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.4123 - acc: 0.7935 - val_loss: 0.8392 - val_acc: 0.5600\n",
            "Epoch 48/300\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3962 - acc: 0.8242 - val_loss: 1.0290 - val_acc: 0.5300\n",
            "Epoch 49/300\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.4296 - acc: 0.8097 - val_loss: 1.1949 - val_acc: 0.5000\n",
            "Epoch 50/300\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.4162 - acc: 0.8065 - val_loss: 0.7574 - val_acc: 0.5900\n",
            "Epoch 51/300\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.4230 - acc: 0.7984 - val_loss: 1.1347 - val_acc: 0.5100\n",
            "Epoch 52/300\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.4115 - acc: 0.8048 - val_loss: 0.9859 - val_acc: 0.5200\n",
            "Epoch 53/300\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.4228 - acc: 0.8242 - val_loss: 1.1103 - val_acc: 0.5200\n",
            "Epoch 54/300\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.4107 - acc: 0.8081 - val_loss: 0.8452 - val_acc: 0.5400\n",
            "Epoch 55/300\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.4145 - acc: 0.7952 - val_loss: 1.0675 - val_acc: 0.4800\n",
            "Epoch 56/300\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.4147 - acc: 0.8145 - val_loss: 0.7895 - val_acc: 0.5800\n",
            "Epoch 57/300\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.4124 - acc: 0.8210 - val_loss: 1.0607 - val_acc: 0.5200\n",
            "Epoch 58/300\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3765 - acc: 0.8274 - val_loss: 0.9893 - val_acc: 0.5200\n",
            "Epoch 59/300\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3804 - acc: 0.8403 - val_loss: 1.1657 - val_acc: 0.5100\n",
            "Epoch 60/300\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3788 - acc: 0.8290 - val_loss: 1.0344 - val_acc: 0.5200\n",
            "Epoch 61/300\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.4017 - acc: 0.8081 - val_loss: 0.8446 - val_acc: 0.5200\n",
            "Epoch 62/300\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3733 - acc: 0.8323 - val_loss: 1.2556 - val_acc: 0.5000\n",
            "Epoch 63/300\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.4109 - acc: 0.8194 - val_loss: 0.8970 - val_acc: 0.5000\n",
            "Epoch 64/300\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3675 - acc: 0.8274 - val_loss: 1.1030 - val_acc: 0.5100\n",
            "Epoch 65/300\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3847 - acc: 0.8129 - val_loss: 1.0831 - val_acc: 0.5000\n",
            "Epoch 66/300\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3839 - acc: 0.8258 - val_loss: 1.0100 - val_acc: 0.5200\n",
            "Epoch 67/300\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3937 - acc: 0.8177 - val_loss: 1.0515 - val_acc: 0.4900\n",
            "Epoch 68/300\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3724 - acc: 0.8290 - val_loss: 1.0938 - val_acc: 0.5000\n",
            "Epoch 69/300\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3946 - acc: 0.8016 - val_loss: 1.0026 - val_acc: 0.4900\n",
            "Epoch 70/300\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3730 - acc: 0.8306 - val_loss: 1.2866 - val_acc: 0.5000\n",
            "Epoch 71/300\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3862 - acc: 0.8387 - val_loss: 1.0501 - val_acc: 0.5000\n",
            "Epoch 72/300\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.4445 - acc: 0.7855 - val_loss: 1.0216 - val_acc: 0.5100\n",
            "Epoch 73/300\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3614 - acc: 0.8516 - val_loss: 1.1931 - val_acc: 0.5000\n",
            "Epoch 74/300\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3951 - acc: 0.8210 - val_loss: 0.9194 - val_acc: 0.5300\n",
            "Epoch 75/300\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3713 - acc: 0.8339 - val_loss: 1.1490 - val_acc: 0.4900\n",
            "Epoch 76/300\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3791 - acc: 0.8290 - val_loss: 0.8400 - val_acc: 0.5400\n",
            "Epoch 77/300\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3729 - acc: 0.8339 - val_loss: 0.9563 - val_acc: 0.5300\n",
            "Epoch 78/300\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3656 - acc: 0.8387 - val_loss: 0.9851 - val_acc: 0.5100\n",
            "Epoch 79/300\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3735 - acc: 0.8145 - val_loss: 1.0779 - val_acc: 0.5000\n",
            "Epoch 80/300\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3553 - acc: 0.8371 - val_loss: 1.1801 - val_acc: 0.4900\n",
            "Epoch 81/300\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3593 - acc: 0.8339 - val_loss: 0.9709 - val_acc: 0.5100\n",
            "Epoch 82/300\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3496 - acc: 0.8435 - val_loss: 1.2703 - val_acc: 0.4900\n",
            "Epoch 83/300\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3730 - acc: 0.8339 - val_loss: 1.4931 - val_acc: 0.5000\n",
            "Epoch 84/300\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3837 - acc: 0.8210 - val_loss: 1.2556 - val_acc: 0.5000\n",
            "Epoch 85/300\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3397 - acc: 0.8597 - val_loss: 1.0494 - val_acc: 0.5300\n",
            "Epoch 86/300\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3822 - acc: 0.8242 - val_loss: 0.9927 - val_acc: 0.5000\n",
            "Epoch 87/300\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3862 - acc: 0.8306 - val_loss: 0.7992 - val_acc: 0.5700\n",
            "Epoch 88/300\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3961 - acc: 0.8323 - val_loss: 0.7908 - val_acc: 0.5800\n",
            "Epoch 89/300\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3774 - acc: 0.8290 - val_loss: 1.0965 - val_acc: 0.5000\n",
            "Epoch 90/300\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3483 - acc: 0.8516 - val_loss: 1.1220 - val_acc: 0.4800\n",
            "Epoch 91/300\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3699 - acc: 0.8226 - val_loss: 1.1578 - val_acc: 0.4900\n",
            "Epoch 92/300\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3556 - acc: 0.8371 - val_loss: 1.2681 - val_acc: 0.5100\n",
            "Epoch 93/300\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3885 - acc: 0.8177 - val_loss: 1.0255 - val_acc: 0.5100\n",
            "Epoch 94/300\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3579 - acc: 0.8258 - val_loss: 1.0629 - val_acc: 0.5100\n",
            "Epoch 95/300\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3960 - acc: 0.8177 - val_loss: 0.8985 - val_acc: 0.5000\n",
            "Epoch 96/300\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3794 - acc: 0.8323 - val_loss: 0.9061 - val_acc: 0.5100\n",
            "Epoch 97/300\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3721 - acc: 0.8065 - val_loss: 1.0110 - val_acc: 0.4900\n",
            "Epoch 98/300\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3522 - acc: 0.8339 - val_loss: 1.4339 - val_acc: 0.5000\n",
            "Epoch 99/300\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3432 - acc: 0.8323 - val_loss: 1.0590 - val_acc: 0.5000\n",
            "Epoch 100/300\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3713 - acc: 0.8113 - val_loss: 0.7692 - val_acc: 0.5500\n",
            "Epoch 101/300\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3603 - acc: 0.8339 - val_loss: 1.2539 - val_acc: 0.5100\n",
            "Epoch 102/300\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3481 - acc: 0.8468 - val_loss: 1.2142 - val_acc: 0.4900\n",
            "Epoch 103/300\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3691 - acc: 0.8355 - val_loss: 1.2848 - val_acc: 0.5100\n",
            "Epoch 104/300\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3562 - acc: 0.8290 - val_loss: 1.0090 - val_acc: 0.4800\n",
            "Epoch 105/300\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3560 - acc: 0.8355 - val_loss: 1.0177 - val_acc: 0.4800\n",
            "Epoch 106/300\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3497 - acc: 0.8452 - val_loss: 0.7720 - val_acc: 0.5000\n",
            "Epoch 107/300\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3566 - acc: 0.8403 - val_loss: 1.0788 - val_acc: 0.4900\n",
            "Epoch 108/300\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3516 - acc: 0.8548 - val_loss: 1.2234 - val_acc: 0.5000\n",
            "Epoch 109/300\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3368 - acc: 0.8468 - val_loss: 1.2977 - val_acc: 0.5000\n",
            "Epoch 110/300\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3507 - acc: 0.8419 - val_loss: 1.1404 - val_acc: 0.5200\n",
            "Epoch 111/300\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3595 - acc: 0.8403 - val_loss: 1.0176 - val_acc: 0.5000\n",
            "Epoch 112/300\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3705 - acc: 0.8403 - val_loss: 1.1053 - val_acc: 0.5000\n",
            "Epoch 113/300\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3414 - acc: 0.8419 - val_loss: 1.2582 - val_acc: 0.5000\n",
            "Epoch 114/300\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3434 - acc: 0.8452 - val_loss: 0.7741 - val_acc: 0.5700\n",
            "Epoch 115/300\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3589 - acc: 0.8419 - val_loss: 0.9351 - val_acc: 0.5100\n",
            "Epoch 116/300\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.4081 - acc: 0.8113 - val_loss: 0.8724 - val_acc: 0.5200\n",
            "Epoch 117/300\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3338 - acc: 0.8645 - val_loss: 1.0593 - val_acc: 0.5000\n",
            "Epoch 118/300\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3313 - acc: 0.8452 - val_loss: 1.1173 - val_acc: 0.5000\n",
            "Epoch 119/300\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3411 - acc: 0.8387 - val_loss: 0.9887 - val_acc: 0.4800\n",
            "Epoch 120/300\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3325 - acc: 0.8516 - val_loss: 0.9044 - val_acc: 0.5200\n",
            "Epoch 121/300\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3590 - acc: 0.8468 - val_loss: 0.8750 - val_acc: 0.5200\n",
            "Epoch 122/300\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3523 - acc: 0.8452 - val_loss: 1.0166 - val_acc: 0.5000\n",
            "Epoch 123/300\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3468 - acc: 0.8500 - val_loss: 1.1409 - val_acc: 0.4900\n",
            "Epoch 124/300\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3439 - acc: 0.8435 - val_loss: 1.0513 - val_acc: 0.4800\n",
            "Epoch 125/300\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3594 - acc: 0.8452 - val_loss: 1.2679 - val_acc: 0.4900\n",
            "Epoch 126/300\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3212 - acc: 0.8581 - val_loss: 1.2344 - val_acc: 0.5200\n",
            "Epoch 127/300\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3421 - acc: 0.8435 - val_loss: 1.1669 - val_acc: 0.4900\n",
            "Epoch 128/300\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3492 - acc: 0.8629 - val_loss: 0.8658 - val_acc: 0.5200\n",
            "Epoch 129/300\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3702 - acc: 0.8306 - val_loss: 1.0628 - val_acc: 0.4700\n",
            "Epoch 130/300\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3338 - acc: 0.8565 - val_loss: 1.1164 - val_acc: 0.4800\n",
            "Epoch 131/300\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3037 - acc: 0.8903 - val_loss: 1.0113 - val_acc: 0.4800\n",
            "Epoch 132/300\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3163 - acc: 0.8484 - val_loss: 1.0107 - val_acc: 0.4900\n",
            "Epoch 133/300\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3323 - acc: 0.8435 - val_loss: 0.8264 - val_acc: 0.5300\n",
            "Epoch 134/300\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3264 - acc: 0.8694 - val_loss: 1.2884 - val_acc: 0.4800\n",
            "Epoch 135/300\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3504 - acc: 0.8371 - val_loss: 1.2954 - val_acc: 0.4900\n",
            "Epoch 136/300\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3315 - acc: 0.8597 - val_loss: 1.1495 - val_acc: 0.4900\n",
            "Epoch 137/300\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3537 - acc: 0.8484 - val_loss: 1.0917 - val_acc: 0.5000\n",
            "Epoch 138/300\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3535 - acc: 0.8387 - val_loss: 1.0599 - val_acc: 0.5000\n",
            "Epoch 139/300\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3512 - acc: 0.8419 - val_loss: 0.7351 - val_acc: 0.5800\n",
            "Epoch 140/300\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3537 - acc: 0.8403 - val_loss: 1.5843 - val_acc: 0.5100\n",
            "Epoch 141/300\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3761 - acc: 0.8403 - val_loss: 1.6604 - val_acc: 0.5100\n",
            "Epoch 142/300\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3779 - acc: 0.8210 - val_loss: 1.0995 - val_acc: 0.4700\n",
            "Epoch 143/300\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3567 - acc: 0.8419 - val_loss: 0.9764 - val_acc: 0.4800\n",
            "Epoch 144/300\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3467 - acc: 0.8339 - val_loss: 0.9622 - val_acc: 0.5000\n",
            "Epoch 145/300\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3389 - acc: 0.8452 - val_loss: 0.8519 - val_acc: 0.5300\n",
            "Epoch 146/300\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3484 - acc: 0.8323 - val_loss: 0.9129 - val_acc: 0.5200\n",
            "Epoch 147/300\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3176 - acc: 0.8677 - val_loss: 1.1063 - val_acc: 0.4700\n",
            "Epoch 148/300\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3236 - acc: 0.8516 - val_loss: 1.0205 - val_acc: 0.4700\n",
            "Epoch 149/300\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3502 - acc: 0.8435 - val_loss: 0.9849 - val_acc: 0.4500\n",
            "Epoch 150/300\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3234 - acc: 0.8500 - val_loss: 0.9999 - val_acc: 0.4900\n",
            "Epoch 151/300\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3351 - acc: 0.8597 - val_loss: 1.4364 - val_acc: 0.4800\n",
            "Epoch 152/300\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3027 - acc: 0.8694 - val_loss: 1.0380 - val_acc: 0.4900\n",
            "Epoch 153/300\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3194 - acc: 0.8742 - val_loss: 1.3121 - val_acc: 0.4800\n",
            "Epoch 154/300\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3404 - acc: 0.8597 - val_loss: 1.0657 - val_acc: 0.4700\n",
            "Epoch 155/300\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3471 - acc: 0.8597 - val_loss: 0.9749 - val_acc: 0.4700\n",
            "Epoch 156/300\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3267 - acc: 0.8565 - val_loss: 1.0050 - val_acc: 0.4600\n",
            "Epoch 157/300\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3265 - acc: 0.8516 - val_loss: 1.4801 - val_acc: 0.5000\n",
            "Epoch 158/300\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3215 - acc: 0.8726 - val_loss: 1.1782 - val_acc: 0.5000\n",
            "Epoch 159/300\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3219 - acc: 0.8548 - val_loss: 1.0756 - val_acc: 0.4700\n",
            "Epoch 160/300\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3574 - acc: 0.8403 - val_loss: 0.9631 - val_acc: 0.5000\n",
            "Epoch 161/300\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3416 - acc: 0.8516 - val_loss: 1.0376 - val_acc: 0.4600\n",
            "Epoch 162/300\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3350 - acc: 0.8387 - val_loss: 1.3127 - val_acc: 0.4900\n",
            "Epoch 163/300\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3167 - acc: 0.8565 - val_loss: 1.4107 - val_acc: 0.4800\n",
            "Epoch 164/300\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3215 - acc: 0.8565 - val_loss: 1.2542 - val_acc: 0.4900\n",
            "Epoch 165/300\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3381 - acc: 0.8419 - val_loss: 1.2922 - val_acc: 0.4900\n",
            "Epoch 166/300\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3239 - acc: 0.8613 - val_loss: 1.2650 - val_acc: 0.4900\n",
            "Epoch 167/300\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3373 - acc: 0.8581 - val_loss: 0.9450 - val_acc: 0.4700\n",
            "Epoch 168/300\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3205 - acc: 0.8661 - val_loss: 1.1341 - val_acc: 0.4700\n",
            "Epoch 169/300\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3261 - acc: 0.8581 - val_loss: 1.0508 - val_acc: 0.4400\n",
            "Epoch 170/300\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3238 - acc: 0.8435 - val_loss: 1.1496 - val_acc: 0.4600\n",
            "Epoch 171/300\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3191 - acc: 0.8452 - val_loss: 1.4383 - val_acc: 0.4900\n",
            "Epoch 172/300\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3201 - acc: 0.8742 - val_loss: 1.2595 - val_acc: 0.4800\n",
            "Epoch 173/300\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3480 - acc: 0.8355 - val_loss: 1.3020 - val_acc: 0.4900\n",
            "Epoch 174/300\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3096 - acc: 0.8726 - val_loss: 1.2592 - val_acc: 0.5000\n",
            "Epoch 175/300\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3355 - acc: 0.8532 - val_loss: 0.9682 - val_acc: 0.4600\n",
            "Epoch 176/300\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3431 - acc: 0.8387 - val_loss: 1.1279 - val_acc: 0.4500\n",
            "Epoch 177/300\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3578 - acc: 0.8387 - val_loss: 1.1609 - val_acc: 0.4500\n",
            "Epoch 178/300\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3262 - acc: 0.8629 - val_loss: 1.1264 - val_acc: 0.4600\n",
            "Epoch 179/300\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3266 - acc: 0.8726 - val_loss: 1.4243 - val_acc: 0.5000\n",
            "Epoch 180/300\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3232 - acc: 0.8581 - val_loss: 1.1042 - val_acc: 0.4900\n",
            "Epoch 181/300\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3439 - acc: 0.8387 - val_loss: 1.0227 - val_acc: 0.4700\n",
            "Epoch 182/300\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3448 - acc: 0.8435 - val_loss: 1.1293 - val_acc: 0.4800\n",
            "Epoch 183/300\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3157 - acc: 0.8645 - val_loss: 1.4677 - val_acc: 0.5100\n",
            "Epoch 184/300\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3399 - acc: 0.8532 - val_loss: 1.2582 - val_acc: 0.4900\n",
            "Epoch 185/300\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3096 - acc: 0.8694 - val_loss: 1.2717 - val_acc: 0.4800\n",
            "Epoch 186/300\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3424 - acc: 0.8435 - val_loss: 0.8415 - val_acc: 0.5000\n",
            "Epoch 187/300\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3451 - acc: 0.8355 - val_loss: 1.4045 - val_acc: 0.4800\n",
            "Epoch 188/300\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3373 - acc: 0.8597 - val_loss: 1.2871 - val_acc: 0.5000\n",
            "Epoch 189/300\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3482 - acc: 0.8468 - val_loss: 1.0101 - val_acc: 0.4600\n",
            "Epoch 190/300\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3168 - acc: 0.8532 - val_loss: 1.5068 - val_acc: 0.5000\n",
            "Epoch 191/300\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3105 - acc: 0.8742 - val_loss: 1.4349 - val_acc: 0.4800\n",
            "Epoch 192/300\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3478 - acc: 0.8484 - val_loss: 0.8925 - val_acc: 0.5000\n",
            "Epoch 193/300\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3371 - acc: 0.8387 - val_loss: 1.1723 - val_acc: 0.4600\n",
            "Epoch 194/300\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3244 - acc: 0.8726 - val_loss: 1.0165 - val_acc: 0.4800\n",
            "Epoch 195/300\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.2823 - acc: 0.8839 - val_loss: 1.3827 - val_acc: 0.4700\n",
            "Epoch 196/300\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3219 - acc: 0.8484 - val_loss: 1.1178 - val_acc: 0.4800\n",
            "Epoch 197/300\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3087 - acc: 0.8710 - val_loss: 1.1981 - val_acc: 0.4800\n",
            "Epoch 198/300\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3382 - acc: 0.8435 - val_loss: 1.0977 - val_acc: 0.4500\n",
            "Epoch 199/300\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3231 - acc: 0.8581 - val_loss: 1.3803 - val_acc: 0.5000\n",
            "Epoch 200/300\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.2972 - acc: 0.8645 - val_loss: 1.1562 - val_acc: 0.4600\n",
            "Epoch 201/300\n",
            "\n",
            "Epoch 00201: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.2880 - acc: 0.8839 - val_loss: 1.1147 - val_acc: 0.4500\n",
            "Epoch 202/300\n",
            "\n",
            "Epoch 00202: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3362 - acc: 0.8468 - val_loss: 1.2496 - val_acc: 0.4700\n",
            "Epoch 203/300\n",
            "\n",
            "Epoch 00203: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3444 - acc: 0.8435 - val_loss: 1.0886 - val_acc: 0.4800\n",
            "Epoch 204/300\n",
            "\n",
            "Epoch 00204: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3369 - acc: 0.8613 - val_loss: 1.0065 - val_acc: 0.4600\n",
            "Epoch 205/300\n",
            "\n",
            "Epoch 00205: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3052 - acc: 0.8839 - val_loss: 1.5525 - val_acc: 0.4700\n",
            "Epoch 206/300\n",
            "\n",
            "Epoch 00206: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.2968 - acc: 0.8710 - val_loss: 0.9788 - val_acc: 0.4600\n",
            "Epoch 207/300\n",
            "\n",
            "Epoch 00207: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3241 - acc: 0.8613 - val_loss: 1.0435 - val_acc: 0.4800\n",
            "Epoch 208/300\n",
            "\n",
            "Epoch 00208: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3107 - acc: 0.8516 - val_loss: 1.2907 - val_acc: 0.5100\n",
            "Epoch 209/300\n",
            "\n",
            "Epoch 00209: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3128 - acc: 0.8597 - val_loss: 1.2960 - val_acc: 0.4800\n",
            "Epoch 210/300\n",
            "\n",
            "Epoch 00210: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3381 - acc: 0.8548 - val_loss: 1.2628 - val_acc: 0.4800\n",
            "Epoch 211/300\n",
            "\n",
            "Epoch 00211: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3180 - acc: 0.8661 - val_loss: 1.4052 - val_acc: 0.5000\n",
            "Epoch 212/300\n",
            "\n",
            "Epoch 00212: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3402 - acc: 0.8468 - val_loss: 1.7465 - val_acc: 0.5100\n",
            "Epoch 213/300\n",
            "\n",
            "Epoch 00213: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3052 - acc: 0.8726 - val_loss: 1.2189 - val_acc: 0.4700\n",
            "Epoch 214/300\n",
            "\n",
            "Epoch 00214: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3108 - acc: 0.8597 - val_loss: 1.0931 - val_acc: 0.4700\n",
            "Epoch 215/300\n",
            "\n",
            "Epoch 00215: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3116 - acc: 0.8645 - val_loss: 1.4593 - val_acc: 0.4600\n",
            "Epoch 216/300\n",
            "\n",
            "Epoch 00216: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3356 - acc: 0.8548 - val_loss: 0.9349 - val_acc: 0.5000\n",
            "Epoch 217/300\n",
            "\n",
            "Epoch 00217: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3188 - acc: 0.8516 - val_loss: 1.6375 - val_acc: 0.5000\n",
            "Epoch 218/300\n",
            "\n",
            "Epoch 00218: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3281 - acc: 0.8532 - val_loss: 1.2519 - val_acc: 0.4700\n",
            "Epoch 219/300\n",
            "\n",
            "Epoch 00219: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3156 - acc: 0.8548 - val_loss: 1.1429 - val_acc: 0.4800\n",
            "Epoch 220/300\n",
            "\n",
            "Epoch 00220: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3113 - acc: 0.8548 - val_loss: 1.1088 - val_acc: 0.4800\n",
            "Epoch 221/300\n",
            "\n",
            "Epoch 00221: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3240 - acc: 0.8597 - val_loss: 1.7939 - val_acc: 0.5000\n",
            "Epoch 222/300\n",
            "\n",
            "Epoch 00222: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3102 - acc: 0.8694 - val_loss: 1.3686 - val_acc: 0.5000\n",
            "Epoch 223/300\n",
            "\n",
            "Epoch 00223: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3041 - acc: 0.8806 - val_loss: 1.1434 - val_acc: 0.4400\n",
            "Epoch 224/300\n",
            "\n",
            "Epoch 00224: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3325 - acc: 0.8403 - val_loss: 1.2937 - val_acc: 0.4500\n",
            "Epoch 225/300\n",
            "\n",
            "Epoch 00225: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3354 - acc: 0.8565 - val_loss: 1.2108 - val_acc: 0.4800\n",
            "Epoch 226/300\n",
            "\n",
            "Epoch 00226: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3120 - acc: 0.8565 - val_loss: 1.4751 - val_acc: 0.4600\n",
            "Epoch 227/300\n",
            "\n",
            "Epoch 00227: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3282 - acc: 0.8581 - val_loss: 1.4411 - val_acc: 0.4600\n",
            "Epoch 228/300\n",
            "\n",
            "Epoch 00228: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3395 - acc: 0.8565 - val_loss: 1.2490 - val_acc: 0.4800\n",
            "Epoch 229/300\n",
            "\n",
            "Epoch 00229: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.2959 - acc: 0.8758 - val_loss: 1.3773 - val_acc: 0.4900\n",
            "Epoch 230/300\n",
            "\n",
            "Epoch 00230: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.2941 - acc: 0.8774 - val_loss: 1.4555 - val_acc: 0.4900\n",
            "Epoch 231/300\n",
            "\n",
            "Epoch 00231: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3058 - acc: 0.8629 - val_loss: 1.4340 - val_acc: 0.4900\n",
            "Epoch 232/300\n",
            "\n",
            "Epoch 00232: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3221 - acc: 0.8726 - val_loss: 1.3404 - val_acc: 0.4500\n",
            "Epoch 233/300\n",
            "\n",
            "Epoch 00233: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3343 - acc: 0.8435 - val_loss: 1.4224 - val_acc: 0.5000\n",
            "Epoch 234/300\n",
            "\n",
            "Epoch 00234: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3122 - acc: 0.8694 - val_loss: 1.1960 - val_acc: 0.5000\n",
            "Epoch 235/300\n",
            "\n",
            "Epoch 00235: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.2774 - acc: 0.8919 - val_loss: 1.6424 - val_acc: 0.5100\n",
            "Epoch 236/300\n",
            "\n",
            "Epoch 00236: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3276 - acc: 0.8548 - val_loss: 1.0245 - val_acc: 0.5000\n",
            "Epoch 237/300\n",
            "\n",
            "Epoch 00237: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3331 - acc: 0.8532 - val_loss: 1.2373 - val_acc: 0.4900\n",
            "Epoch 238/300\n",
            "\n",
            "Epoch 00238: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3197 - acc: 0.8581 - val_loss: 1.3122 - val_acc: 0.4800\n",
            "Epoch 239/300\n",
            "\n",
            "Epoch 00239: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3135 - acc: 0.8677 - val_loss: 1.4511 - val_acc: 0.4900\n",
            "Epoch 240/300\n",
            "\n",
            "Epoch 00240: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3081 - acc: 0.8677 - val_loss: 1.4520 - val_acc: 0.4900\n",
            "Epoch 241/300\n",
            "\n",
            "Epoch 00241: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.2929 - acc: 0.8661 - val_loss: 1.3780 - val_acc: 0.4800\n",
            "Epoch 242/300\n",
            "\n",
            "Epoch 00242: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.2786 - acc: 0.8694 - val_loss: 1.1835 - val_acc: 0.4500\n",
            "Epoch 243/300\n",
            "\n",
            "Epoch 00243: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3125 - acc: 0.8516 - val_loss: 1.2934 - val_acc: 0.4800\n",
            "Epoch 244/300\n",
            "\n",
            "Epoch 00244: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3229 - acc: 0.8613 - val_loss: 1.4431 - val_acc: 0.4900\n",
            "Epoch 245/300\n",
            "\n",
            "Epoch 00245: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3282 - acc: 0.8435 - val_loss: 1.4331 - val_acc: 0.4700\n",
            "Epoch 246/300\n",
            "\n",
            "Epoch 00246: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3358 - acc: 0.8548 - val_loss: 0.9565 - val_acc: 0.4900\n",
            "Epoch 247/300\n",
            "\n",
            "Epoch 00247: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.2972 - acc: 0.8806 - val_loss: 1.3976 - val_acc: 0.4700\n",
            "Epoch 248/300\n",
            "\n",
            "Epoch 00248: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3063 - acc: 0.8726 - val_loss: 1.3894 - val_acc: 0.4800\n",
            "Epoch 249/300\n",
            "\n",
            "Epoch 00249: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3307 - acc: 0.8548 - val_loss: 1.2060 - val_acc: 0.4500\n",
            "Epoch 250/300\n",
            "\n",
            "Epoch 00250: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3039 - acc: 0.8645 - val_loss: 1.2452 - val_acc: 0.4800\n",
            "Epoch 251/300\n",
            "\n",
            "Epoch 00251: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.2964 - acc: 0.8677 - val_loss: 1.2223 - val_acc: 0.4500\n",
            "Epoch 252/300\n",
            "\n",
            "Epoch 00252: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3465 - acc: 0.8452 - val_loss: 1.1518 - val_acc: 0.5100\n",
            "Epoch 253/300\n",
            "\n",
            "Epoch 00253: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3252 - acc: 0.8581 - val_loss: 1.3703 - val_acc: 0.4900\n",
            "Epoch 254/300\n",
            "\n",
            "Epoch 00254: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3254 - acc: 0.8661 - val_loss: 1.4203 - val_acc: 0.5000\n",
            "Epoch 255/300\n",
            "\n",
            "Epoch 00255: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3344 - acc: 0.8565 - val_loss: 1.5824 - val_acc: 0.5000\n",
            "Epoch 256/300\n",
            "\n",
            "Epoch 00256: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3250 - acc: 0.8613 - val_loss: 1.5041 - val_acc: 0.4900\n",
            "Epoch 257/300\n",
            "\n",
            "Epoch 00257: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.2961 - acc: 0.8597 - val_loss: 1.1205 - val_acc: 0.4900\n",
            "Epoch 258/300\n",
            "\n",
            "Epoch 00258: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3008 - acc: 0.8661 - val_loss: 1.4013 - val_acc: 0.5000\n",
            "Epoch 259/300\n",
            "\n",
            "Epoch 00259: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3011 - acc: 0.8742 - val_loss: 1.1616 - val_acc: 0.4700\n",
            "Epoch 260/300\n",
            "\n",
            "Epoch 00260: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3030 - acc: 0.8790 - val_loss: 1.3556 - val_acc: 0.4700\n",
            "Epoch 261/300\n",
            "\n",
            "Epoch 00261: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.2894 - acc: 0.8806 - val_loss: 1.1501 - val_acc: 0.4700\n",
            "Epoch 262/300\n",
            "\n",
            "Epoch 00262: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3180 - acc: 0.8548 - val_loss: 1.2774 - val_acc: 0.4900\n",
            "Epoch 263/300\n",
            "\n",
            "Epoch 00263: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3092 - acc: 0.8742 - val_loss: 1.1980 - val_acc: 0.5000\n",
            "Epoch 264/300\n",
            "\n",
            "Epoch 00264: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3070 - acc: 0.8742 - val_loss: 1.1588 - val_acc: 0.4700\n",
            "Epoch 265/300\n",
            "\n",
            "Epoch 00265: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3031 - acc: 0.8823 - val_loss: 1.6150 - val_acc: 0.5100\n",
            "Epoch 266/300\n",
            "\n",
            "Epoch 00266: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3079 - acc: 0.8661 - val_loss: 1.1449 - val_acc: 0.5000\n",
            "Epoch 267/300\n",
            "\n",
            "Epoch 00267: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.2780 - acc: 0.8887 - val_loss: 1.4592 - val_acc: 0.5000\n",
            "Epoch 268/300\n",
            "\n",
            "Epoch 00268: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3051 - acc: 0.8742 - val_loss: 1.1776 - val_acc: 0.4600\n",
            "Epoch 269/300\n",
            "\n",
            "Epoch 00269: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3233 - acc: 0.8516 - val_loss: 1.0325 - val_acc: 0.4800\n",
            "Epoch 270/300\n",
            "\n",
            "Epoch 00270: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3017 - acc: 0.8887 - val_loss: 1.3146 - val_acc: 0.4800\n",
            "Epoch 271/300\n",
            "\n",
            "Epoch 00271: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3008 - acc: 0.8694 - val_loss: 1.3119 - val_acc: 0.4800\n",
            "Epoch 272/300\n",
            "\n",
            "Epoch 00272: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3069 - acc: 0.8677 - val_loss: 1.5282 - val_acc: 0.5100\n",
            "Epoch 273/300\n",
            "\n",
            "Epoch 00273: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3229 - acc: 0.8645 - val_loss: 1.2622 - val_acc: 0.5000\n",
            "Epoch 274/300\n",
            "\n",
            "Epoch 00274: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3202 - acc: 0.8677 - val_loss: 1.2447 - val_acc: 0.4800\n",
            "Epoch 275/300\n",
            "\n",
            "Epoch 00275: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3176 - acc: 0.8629 - val_loss: 1.3300 - val_acc: 0.4900\n",
            "Epoch 276/300\n",
            "\n",
            "Epoch 00276: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3158 - acc: 0.8645 - val_loss: 1.2048 - val_acc: 0.5000\n",
            "Epoch 277/300\n",
            "\n",
            "Epoch 00277: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.2768 - acc: 0.8758 - val_loss: 1.3889 - val_acc: 0.4900\n",
            "Epoch 278/300\n",
            "\n",
            "Epoch 00278: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.2897 - acc: 0.8823 - val_loss: 1.6489 - val_acc: 0.5100\n",
            "Epoch 279/300\n",
            "\n",
            "Epoch 00279: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.2971 - acc: 0.8677 - val_loss: 1.5849 - val_acc: 0.5200\n",
            "Epoch 280/300\n",
            "\n",
            "Epoch 00280: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.2823 - acc: 0.8935 - val_loss: 1.2950 - val_acc: 0.4700\n",
            "Epoch 281/300\n",
            "\n",
            "Epoch 00281: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.2885 - acc: 0.8790 - val_loss: 1.3736 - val_acc: 0.4600\n",
            "Epoch 282/300\n",
            "\n",
            "Epoch 00282: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.2838 - acc: 0.8935 - val_loss: 1.4299 - val_acc: 0.4900\n",
            "Epoch 283/300\n",
            "\n",
            "Epoch 00283: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3289 - acc: 0.8597 - val_loss: 1.1272 - val_acc: 0.4800\n",
            "Epoch 284/300\n",
            "\n",
            "Epoch 00284: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3345 - acc: 0.8548 - val_loss: 1.2877 - val_acc: 0.4800\n",
            "Epoch 285/300\n",
            "\n",
            "Epoch 00285: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.2832 - acc: 0.8694 - val_loss: 1.3405 - val_acc: 0.4800\n",
            "Epoch 286/300\n",
            "\n",
            "Epoch 00286: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.2950 - acc: 0.8726 - val_loss: 1.0713 - val_acc: 0.4900\n",
            "Epoch 287/300\n",
            "\n",
            "Epoch 00287: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3021 - acc: 0.8806 - val_loss: 1.2079 - val_acc: 0.5100\n",
            "Epoch 288/300\n",
            "\n",
            "Epoch 00288: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.2966 - acc: 0.8758 - val_loss: 1.2591 - val_acc: 0.5000\n",
            "Epoch 289/300\n",
            "\n",
            "Epoch 00289: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.2905 - acc: 0.8694 - val_loss: 1.1037 - val_acc: 0.4600\n",
            "Epoch 290/300\n",
            "\n",
            "Epoch 00290: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3201 - acc: 0.8645 - val_loss: 1.1342 - val_acc: 0.4900\n",
            "Epoch 291/300\n",
            "\n",
            "Epoch 00291: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3032 - acc: 0.8742 - val_loss: 1.4572 - val_acc: 0.4900\n",
            "Epoch 292/300\n",
            "\n",
            "Epoch 00292: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3115 - acc: 0.8597 - val_loss: 1.3469 - val_acc: 0.4800\n",
            "Epoch 293/300\n",
            "\n",
            "Epoch 00293: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3102 - acc: 0.8629 - val_loss: 1.2448 - val_acc: 0.5100\n",
            "Epoch 294/300\n",
            "\n",
            "Epoch 00294: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3024 - acc: 0.8806 - val_loss: 1.3899 - val_acc: 0.5000\n",
            "Epoch 295/300\n",
            "\n",
            "Epoch 00295: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.2920 - acc: 0.8645 - val_loss: 1.4130 - val_acc: 0.4700\n",
            "Epoch 296/300\n",
            "\n",
            "Epoch 00296: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3230 - acc: 0.8613 - val_loss: 1.1511 - val_acc: 0.4500\n",
            "Epoch 297/300\n",
            "\n",
            "Epoch 00297: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3001 - acc: 0.8629 - val_loss: 1.5251 - val_acc: 0.4800\n",
            "Epoch 298/300\n",
            "\n",
            "Epoch 00298: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3280 - acc: 0.8435 - val_loss: 1.2521 - val_acc: 0.4700\n",
            "Epoch 299/300\n",
            "\n",
            "Epoch 00299: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.3113 - acc: 0.8694 - val_loss: 1.1746 - val_acc: 0.4400\n",
            "Epoch 300/300\n",
            "\n",
            "Epoch 00300: val_loss did not improve from 0.68361\n",
            "620/620 - 1s - loss: 0.2933 - acc: 0.8855 - val_loss: 1.1267 - val_acc: 0.4800\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nOy9d3zkVb3//zwzmZbek91key+whWWp\n0jsIiFJFpMlVVERFf3wtqHD1qle4eoUrAiJFASmCiBQpS98K23t2N5uyyaa3mUymnd8fnzKfmUw2\nk91Mkt05z8cjj5n51POZJOd13uW8j5BSolAoFIr0xTbaDVAoFArF6KKEQKFQKNIcJQQKhUKR5igh\nUCgUijRHCYFCoVCkOUoIFAqFIs1RQqBIC4QQk4UQUgiRkcSx1wshPhyJdikUYwElBIoxhxCiWggR\nEEIUx21fq3fmk0enZQrFkYkSAsVYZQ9wtfFBCHEUkDl6zRkbJGPRKBRDRQmBYqzyJHCd5fOXgSes\nBwgh8oQQTwghmoUQe4UQPxJC2PR9diHEb4QQLUKI3cCFCc79kxCiQQhRL4T4TyGEPZmGCSGeE0I0\nCiE6hRDvCyHmWfZ5hBD36u3pFEJ8KITw6PtOFkJ8LIToEELUCiGu17e/K4S42XKNGNeUbgV9XQix\nE9ipb/udfo0uIcQnQojPWI63CyF+IITYJYTo1vdPEEI8IIS4N+5ZXhZCfDuZ51YcuSghUIxVVgC5\nQog5egd9FfCXuGN+D+QBU4FT0YTjBn3fV4CLgEXAEuALcec+BoSA6fox5wA3kxyvATOAUuBT4K+W\nfb8BjgFOBAqB7wMRIcQk/bzfAyXAQmBdkvcDuBQ4Dpirf16tX6MQeAp4Tgjh1vd9B82augDIBW4E\nfMDjwNUWsSwGztLPV6QzUkr1o37G1A9QjdZB/Qj4L+A84E0gA5DAZMAOBIC5lvP+A3hXf/8O8FXL\nvnP0czOAMqAP8Fj2Xw0s099fD3yYZFvz9evmoQ2seoEFCY77f8CLA1zjXeBmy+eY++vXP2OQdrQb\n9wW2A5cMcNxW4Gz9/TeAV0f7961+Rv9H+RsVY5kngfeBKcS5hYBiwAHstWzbC1To78cDtXH7DCbp\n5zYIIYxttrjjE6JbJz8HLkcb2Ucs7XEBbmBXglMnDLA9WWLaJoS4A7gJ7Tkl2sjfCK4f6F6PA9ei\nCeu1wO8OoU2KIwTlGlKMWaSUe9GCxhcAf4/b3QIE0Tp1g4lAvf6+Aa1DtO4zqEWzCIqllPn6T66U\nch6Dcw1wCZrFkodmnQAIvU1+YFqC82oH2A7gJTYQXp7gGLNMsB4P+D5wBVAgpcwHOvU2DHavvwCX\nCCEWAHOAlwY4TpFGKCFQjHVuQnOLeK0bpZRh4Fng50KIHN0H/x2icYRngduEEJVCiALgTsu5DcC/\ngXuFELlCCJsQYpoQ4tQk2pODJiKtaJ33LyzXjQCPAvcJIcbrQdsThBAutDjCWUKIK4QQGUKIIiHE\nQv3UdcBlQohMIcR0/ZkHa0MIaAYyhBB3oVkEBo8A9wghZgiNo4UQRXob69DiC08CL0gpe5N4ZsUR\njhICxZhGSrlLSrlmgN3fRBtN7wY+RAt6Pqrvexh4A1iPFtCNtyiuA5zAFjT/+vPAuCSa9ASam6le\nP3dF3P47gI1onW0b8CvAJqWsQbNsvqtvXwcs0M/5H7R4x340181fOTBvAK8DO/S2+Il1Hd2HJoT/\nBrqAPwEey/7HgaPQxEChQEipFqZRKNIJIcQpaJbTJKk6AAXKIlAo0gohhAP4FvCIEgGFgRIChSJN\nEELMATrQXGC/HeXmKMYQyjWkUCgUaY6yCBQKhSLNOewmlBUXF8vJkyePdjMUCoXisOKTTz5pkVKW\nJNp32AnB5MmTWbNmoGxChUKhUCRCCLF3oH3KNaRQKBRpjhIChUKhSHNSKgRCiPOEENuFEFVCiDsT\n7J8khHhbCLFBr8lemcr2KBQKhaI/KYsR6FUaHwDOBuqA1UKIl6WUWyyH/QZ4Qkr5uBDiDLSSw18a\n6r2CwSB1dXX4/f7haPphgdvtprKyEofDMdpNUSgUhzmpDBYvBaqklLsBhBDPoFVttArBXLRCYQDL\nOMhKiHV1deTk5DB58mQsZYWPWKSUtLa2UldXx5QpU0a7OQqF4jAnla6hCmILYdURrRVvsB64TH//\nOSDHqJJoRQhxixBijRBiTXNzc78b+f1+ioqK0kIEAIQQFBUVpZUFpFAoUsdoB4vvAE4VQqxFW2qw\nHgjHHySlfEhKuURKuaSkJGEabNqIgEG6Pa9CoUgdqRSCemIXBqkkumgIAFLKfVLKy6SUi4Af6ts6\nUtgmhUKhSDktPX28smEfnb4g/1hXP/gJo0wqhWA1MEMIMUUI4URbfPxl6wFCiGJjIW20NV0f5TCk\ntbWVhQsXsnDhQsrLy6moqDA/BwKBpK5xww03sH379hS3VGElElF1tsY6rT193PLEGjp8yf0fjRX+\ntrqWbzy1li88+DHfemYd+zrG9vo/KRMCKWUIbXHsN9AWzH5WSrlZCHG3EOJi/bDTgO1CiB1oC4r/\nPFXtSSVFRUWsW7eOdevW8dWvfpVvf/vb5men0wloAd5IJDLgNf785z8za9askWpy2tPhC3DUT9/g\nw50to90UxQFYXd3Gv7fsZ0Nd56i1obbNx8793QA0dvrZ2tA16DlNXVr8bmdTDwDd/lDqGjgMpDRG\nIKV8VUo5U0o5TUr5c33bXVLKl/X3z0spZ+jH3Cyl7Etle0aaqqoq5s6dyxe/+EXmzZtHQ0MDt9xy\nC0uWLGHevHncfffd5rEnn3wy69atIxQKkZ+fz5133smCBQs44YQTaGpqGsWnODKp7+jFGwizq7ln\ntJuiOACNnVqH2uUPjlob7n5lC7f+9VMA7v33dr7yxOAlblp6Yi2Yzt5Db78/2C98OmwcdrWGBuNn\n/9zMln2DK/ZQmDs+l598Npl1zfuzbds2nnjiCZYsWQLAL3/5SwoLCwmFQpx++ul84QtfYO7cuTHn\ndHZ2cuqpp/LLX/6S73znOzz66KPceWe/+XiKQ8D4x+wexQ5mLFDT6qM4x0mmM7Yr2NfRS5YrgzyP\nNk+lqctPht1GYZZzRNu3v1sbGw5HR3qw1Lf3srvFSyAUoaWnj/qOXgKhCM6MgcfRzT2xY9pDdW11\n+oIsvOff3H3JfL50/KRDulYiRjtr6Ihn2rRppggAPP300yxevJjFixezdetWtmzZ0u8cj8fD+eef\nD8AxxxxDdXX1SDU3bej06ULQN7ZN9lTSFwpzyn8v47vPru+379o/reS/39hmfr71r5/y/ec3jGTz\nANhvWAS9I/97+qiqhXe3N7G/y084Iqlu9dLlDyFl1FKJp7rFy5Mr9tISLwSHKGR7Wr1ICWU5rkO6\nzkAccRbBwY7cU0VWVpb5fufOnfzud79j1apV5Ofnc+211yacC2DEFQDsdjuhUPp2VqnC+MfsGeO+\n21SytUHze6/a0xazPRiOUN3iZWpx9G+3utWLaPMd8j3f3LKfJ1fs5bHrj8VmGzwFen/30FxD4Yjk\nhsdWc+NJkzltVmnS7Wro7OXuf27hmEkF3PyZqQD86KVNRKSk1auN5nfu76FL/7up6/AxsSiz33Uu\n/N8P8AbCOOyxz9ZlEYJtjV384O8b+cO1x1CW6445rs0b4JYn1nDvFQuYVGT5/lu8AEyx/E6GE2UR\njCBdXV3k5OSQm5tLQ0MDb7zxxmg3KW0xXA09aWwRbKjTMrUnxXVojZ1+IjI6CtdcIgGau/v6jXSH\nyleeWMP7O5oTWmJNXX5e2bAP66qJZowgyRF1c3cf7+9o5vo/r066TeGI5NpHVvLapkb+urIGgD0t\nXva0eNnbGhW/qqYeU5Dq2/tnAXX5g3gDmh8/GJYUZ7uw62LX4Yu2/++f1vNpTQd/fG93v2usq21n\nzd52Pqpqjdm+p8WLEDChsL/4DAdKCEaQxYsXM3fuXGbPns11113HSSedNNpNSluMf8xUWASRiORw\nWAJ2fa2WiZPlinUM1OupjkanZ/V3b2/sHpZ7x8dmdjX3sPQXb/ONp9ay2RLj29/Vp7clud+TVaiC\n4YGz9Kz8a2MDu5q9OOxCz+6TvLG5sd9xVc095gCiPkE66OubYs/55hnTeeWbJ1OY5aSjNxojWLZN\nS/54atVeWvX2Gn8vNbrwVLd6Y65V3eqlIt+D22FP6pmGyhHnGhptfvrTn5rvp0+fzrp168zPQgie\nfPLJhOd9+OGH5vuOjuicuquuuoqrrrpq+Bua5nTq/5ipSOu79P8+ojjbxaPXH2tua+ry0+UPMr00\nZ0jX+mBnM7c9vZZld5zGzqYejp1caO6TUrJ8VysnTEuuvIo/GObU/17GXRfN48Kjx5kWQfxo28h5\nN7Zb/eFbG7o4aXpxv2vv7/LT7Q8xvTR7wPt7LVZA/PdudI6giU23P8RRlXmmxTYUi8BgTXU7J0zr\nV7GmHw+/v5vppdmcPL2Y59bUctsza3llQwMZNkFIn2tSnutmW0MX/qAmLonmBazY1UpxtpOevhD+\nYITx+R7mjMslz+OgU7euatt87Gzq4YollTy7po6X1+/juClFXPaHj3jlmydT06Zdd09LnBC0eFPm\nFgJlESjSFDNraJhdQ2tr2tlQ18k722JTfv/rtW3c9PjQV9bbvK+Ldl+Qxz/ey+UPLo/pIN7a2sQ1\nj6xkZZyPfyAaOv3s7+pj075OwhFpps529gZ57KM9rK/VhMFwexijcCMnHmDLADn0v3h1Kzc+dmB3\nzKb66FyAeCGo7+jFabdhtwm++9x6rn54Ba9tbDD3JxsjsFovK/e0HuBIjcZOPxvrO/n84kpKclx4\nA2FW7mljwYR8/vilY8zjFk/KZ7flu09kEayv62DhhHymlWhiWJytxfryPA4za+j+d6oQAm49bTpz\nx+Xy4tp6lm1vwh+M8MnedmrbdYvAci8pJXtavEwuUkKgUAwrpmuoL0hfKMzF93/Ix1VDm1zW0tPX\nr4N6+IOo37fdG3UH7G31UtPmIxBKzl1h0KZfw+i0rS6D5bu0jm5bXOfc7g1w9n3vxXS8AA1659Xc\n3UdXbxBjYnW7L8g9/9rKM6u1GpFGJ9fTFyIUjrBfF4KFE/JNsYhnX0cvNW2+A6Z5rq+LntvT198K\nmVSUGROvqNInYxVmOWMsgo11nayubmNbYxen/vcyPtkbFULDNZSf6aBGD27XtfsGzMF/d7sm2KfP\nLqFEz8hp7u7jxGlFnDG7FI/DjtNuY3Z5LmH9C8uwCTbv62JjXSeBUES3hoLsbvFydGW+aRUZ18vP\ndNDZG+TjXS38bU0tt542jcnFWVy2uIINdZ08/0md+by1epv3tvnMme/tviBd/hCTlUWgUAwv1hhB\nQ4efDXWdfFrTPqRrLPnPtzjtv981P4cjkg92tjAuT8sEqbJMVmvs9CNlYpeClfqOXp5aWWP6jA0h\nqNNHitYg5erqtn73Adi0r5OdTT2sjeu09+kunpaePjNrqiLfQ2dvkHBE0ubtM9tg0O0P0djVh8Mu\nOHdeObuavTF++E9r2nl/R7M5gSpelKysro5+v4ksgooCDzMsriVjVu70kmzTtbKpvpPP3v8hlz+4\nnPN++wF7W328tz1akbilO0CW087Mshzq2nrZvK+Tk3+1jP95cwcA/1y/LybOsWx7E+Pz3Mwqy6Ek\nO5qaWZHvQQjBpKJMSnNdlFuye247cwbuDDtXPbScKx9azln3vscHO1uQEo6uzOPoynyynHaK9etp\nrqEga/Tn//rp0wG4bHElHofdtPKqmnqoafOR48ogEIqwr1P7PRhCPD4vNsNoOFFCoDis2drQxRUP\nLscXSM7FY/ipoxPKQmbH1u4beq53m2XUb/i2r1k6EYiOaMMRaU6MqhkkBfPBd3fxgxc3sqvZG3P9\nOl0AjE662x9k877OmPt0+oJ88ZEVZsfY0h2b4dOodywtPX2mq2KiJQulVe/Ma9t8GCGHLn+Qpi4/\npTlujpuqxSdWW1xR//nKFn7y8mbzXnf9YzM/fHFjv+eKRCRrqts4fVaJft04IWjvpSLfExNjMDrs\nqSVZpuX1kW61/fiiuWZ6a7NlFm9LTx/FOS4mFGSyt83L7c9oMbrt+7vp7A1y+9/Wcd+bWk2vnr4Q\n7+1o5ow5pQghzI4boKLAA8B588s5a04ZZZZO+KTpRfz91hPJsNtYW9NBd1+In7y8GYCjK/O57oRJ\nvPXdU83Abr7HQYcvSFVTD5UFHnPyXmGWky8ep/2tZDntrNrThi8Q5sTpWlxj874uegNhM+5RnKI5\nBKCEQHGY88vXtrGqus10kxyIunYfC+/+N6ur20whCEWk2cl2DEEIrFlBxvtVuk/60kUVuB02s4Nu\n7u4z3QoDCUFzdx/vbNtvxhYMl4UhBE16Z2BYBBvqOolIbeRa1aSJxsb6Tj6qauUvK/cC9Ev1NC2C\n7oD5/FZXTJs3QFVTD9WtPo6bonX6Xb0hGrv8lOe5mT8+D4/DbsYkguEIm/d1UdfuM2Mt2/d389Sq\nmn4us6rmHtp9Qc6YUwbEZg35AiHafUEzuGpQ39GLwy6YUJhJIBTBHwyzurqNqcVZ3HTyFN654zQW\nTMinrt3Hm1v209MXorm7j5JsFxMLM9nf1WdaFYFQhA93thCOSNZUt7NydysPvrsLfzDC5xZpy6QU\n50Tn71Tma0Jw+1kz+enF8yjLjXbCuW4H4/M9PHzdEn504RzOm1dOc3cfXz5hEoVZThx2G+PyPObx\neZlOuvxBduzv7hdM//rp07nllKlce8IkM/X00oUVTCj0cNvTa5lz1+u8pFcvtQrVcKOEQHFI3Pfm\nDlbuHrwTThVGyYNkRvM79ncTDEu2N3bT0xcyfbhGELDdF+CnL2/u51tPRK/F52ykOK6qbqMi38OE\nwkymFmfz0tp6/u/dKtPEB0wfcDz3/ns7Nz62xhzxL4sTAgNjv+FiOn12iTnCN+5jZLY0d/cRCkf4\nf3/fyK7mHjP7p9XbZ4qeNS+9paePF9fWYRNwrV7GoMsfpKHTT3muG2eGjUUT81m1p42/ra7hrn9s\noi8UIRiOTZWVEj6pjnWzrdD/Rk6ZUUyGTcS4hmr1TJnKAg/nzx/HC187wez0irJc5OplLjp7g6za\n08bSKdHMqQkFHlbtaeMrT6zhhU/qNIsg28XEomhHPHdcLo1dfvM7bfUGuPrhFdy/rIqJhZksnlhg\n3stgfH70fCDGNWS0Z+mUQm7+zFR+ffnRvPT1k/jZJfNJRJ7HgZSwrbGb6SWxQlCQ5eQHF8xh/vg8\nAHJcGZw1t4zfXbXITH99V7fwjOBzKlBCMAwMRxlqgEcffZTGxv75y2MVKSUPLKvilQ0Ngx+cIvIz\ntX9Kw7+diJpWzRJ4a6vWERij8krd/K+2+Ggf+7iav62uTXwhC9aSB3tavEgpWbWnzRxJf/nESZTk\nuPj169t5RA8g220ixiLo8AV4ZcM+NtR1xARZL14wnlV72ugNhGMCzhC1CAwL4YSpWiqntaM3aOnp\no7rVx9Oranh53T5TPIJhabbDahF0+UO8tHYfJ00vNjNf2n0B6tt7TcFYOqWQrY1d/Or17Ty9KvZ7\nuvyYSv7j1Kk47IJV1VH3USQi+cuKvcwqy2FiYSY57gz2d/l5bWMD3312Pef+9n1As27sNsExkwop\nzNJ+r8U5TrPe0Qc7W+jyh2JSaCcWZtKnB+CrW726a8hpurxKclwsnVJIU5c20eyoijzz3CuXTODO\n82ebqbfODBt5Hgf5mY5+cyvyPA6ztlCuO3ad8Fy3g4UT8hmIoyuj95xRlji99uTpxVx09Dj+ddtn\ncNhtLJ5YQNXPLyA/00GbN4Arw0a2K3XZ/moewTBglKEGbR5BdnY2d9xxx5Cv8+ijj7J48WLKy8uH\nu4kpwRcIE47IlBUE27yvk1V72rjhpIHXZXbatX/OhgFqv4A2Uu/wBfnnun0A7Nb975UFmayt6TAz\ncYzOcUPd4GsjWZ+5utVLaa6Llp4Ax+pCcOWxE7lscSWXP7icVzdq4n5URZ6ZHghaKuEjH+7B7bAx\nqzyX6aXZfO/cWYQjkpfX72NrY1e/9Nb93X6C4QjN3X3kuDKYVa51LLVtvTR0xgaiW3oCNOklGrY1\ndtHY5TcDl4bbakJB7EzV+o5erjluojnq3bm/h0A4woRCTTSXTilEyv6WCsCXTpjE0ZX5rKlu52Pd\nVffmlv38bXUtO/b38LurFiKEIMft4O+f1vP3T+uxCW203dLTx1TLaNmw9EqyXeS6tW7qd2/vIMed\nwdnzyszjrDGO3c1e2n1BirNd5nMtnVJIeZ6bnr4QPX0hvnrqNLr8QZZOLuRXXzi63zMUZzsTTtoS\nQlCe66ahsxe3Y2jj5yWTCsz300oSC0FBlpP7r1kcs81mE1Tke+jQnymVqxIqIUgxjz/+OA888ACB\nQIATTzyR+++/n0gkwg033MC6deuQUnLLLbdQVlbGunXruPLKK/F4PKxatSqm5tBIsa+jl3F57qT+\n6AzzPl4I2r0BXA5bv4qWQ+Xxj6t5dk0d580vj/G5WvHpftX69l6k1Pz98dPwjfrxRqdqpGIaE3T2\nNHvjju8etLpkjBC0eDG+LavbwmG38cML53D5g8sBLf3yuTW19AbCeAMh0yXlD0bYVN/J5xZVcO68\ncnbote9Xx80PsAmISGjo8Gu+8FyX6cKo7+ilodOP3SYIRyT5mQ5aevrMQOPamg46fEFOnl7Mh1Ut\n7GzqIduVkbCa6LSSbLPzNQLSRoe7aEIBDrsgGJZcf+JkIlLyxHItJmG4c86bV87PX93KVQ8tZ8Xu\nNoqznZw6s4QLjxoHQI47Gix9/fbPUJLtwhcIx4zCjXYVZ7uYNz6PgkwHtW29fPOM6TEjcqsQGFlU\nlQWZlOS4OG9eOVcsmRBjVc0Zl8u/bvsMrgF+t5csrMAzwOzdslwX3r7QkDtkIQQvfO1E7v33duaN\nzxv8BAsV+R427+sy3Zip4sgTgtfuhMb+WQuHRPlRcP4vh3zapk2bePHFF/n444/JyMjglltu4Zln\nnmHatGm0tLSwcaPWzo6ODvLz8/n973/P/fffz8KFC4e3/UmyrbGL8377AT+7eB5fPnHyoMcbAb94\nIfjiIys5ZlIB91ya2GeaLMao9d3tzVytZ+LEYwpBRy+vbmzk6099yp+vP5bjphby7Oparj5uItsa\nY1Ma97YaBby0TiR+1B0IR9jW2MXRlQOb+50xRcS6ae7uozjbGVOoDeDYyYXmDNVppdl4A2G++9w6\n1td24nbYGJfnpqFTq25ZoXfqk4uysNuE2bEZzCrPZWtDF7tbemjq9lOS7SLTqXXm9R29NHT4OXVm\nCSdOK6LDF+T+ZVVmrRzDlXT+UeV8WNXCruYeSrJd5OmutYJMhxlnmV6aTZYzA5vALPdgdLgep51j\nJhUQCEX46cVagccX19bT7Q9RpPuwbzhpMv/e0sjq6nZuO2M63zhjRoyoGi6OqcVZlOZovvd4V4wp\nBDkuSnJcvHH7KbzwaT3XHh/7dzC1JBu7TVCQ6TSD40dX5iGE4EF9QtjHu6LzQ+aMyzmgi+W2M2cM\nuG9qcXZMbGgoHDOpgKe+cvyQzzOEPpWBYjgShWAM8dZbb7F69WqzDHVvby8TJkzg3HPPZfv27dx2\n221ceOGFnHPOOaPcUg3D/7xse1NSQmBkhlgn+0gpqWruGVKqWzgiCUdkTGchpTSFYNm2pgGFoDeo\ndeL1Hb2sq9UClH94dxcb6zu5780dZLoyzCqbBsZEqgPN1Fxf25GUEJw+q4Tlu1vJdGZw0vTihKPF\nT358Nv5g2HRJvbF5P+GIxCY0F9LTq7RCZ4YQODNsTCrKNLNznHYbgXCExRPz2drQRVVTD03dfWb7\nKvI91LdrrqHjpmoBzOfWaP5769ocRjD2hy9uIhCKkOdxkK13+HPH5/JRVSsOu5Y7b7MJcj0OGjr9\n2ERs8PSBOBdGRb6HfR29uDK0kXSG3cZjNyylpacvpoKmQY4+oj/QBKlCPXBrdICluW6+dtq0fseV\n57l589un8N6OZn72zy1kOu393C9GoHdcnpv8zIO3sn940ZyULg6TCCOOVZKTWu/AkScEBzFyTxVS\nSm688Ubuueeefvs2bNjAa6+9xgMPPMALL7zAQw89NAotjMXwjfYGkvtjN3LBramCnb1BAqHIAYO3\n8dzzyhbW1Xbw0te1InyNnX7W1rTT5Q+R5bTzYVULnb1BM2jY5Q+yrqaDU2aWmBZBhy/Ipnqt01tV\n3WbOYn3kg920eQNaZ2mZKGW3CeaOz6U8101jl5/8TC3XuzTHhQQ+2dvOl06YnLC9G+o6THfTpYsq\nWLa9GX8wwCULxic8Ps/jIM/jwKaLhJFKGpEwd1yOaRUYueugTaIyhGNCoYddzVqJgcIsJ1VNPTR3\n91Gqi21FvocNdR10+UOmC80Q4i0NXWQ67fgCYS5bVEFBpoOiLCet3gD5mQ5sNsEPLpjDnHGaEEwu\nysKhx10KM510+IKMy/OY2wCK4kaniQQ1y5XRb5RvkKFX5DxQ7ZxC3VJJJlNmakm2GeeZX5FnVvw0\nMEo9zy4fWp2neHLdjn6B4lRTMUIWgcoaSiFnnXUWzz77LC0tmmna2tpKTU0Nzc3NSCm5/PLLufvu\nu/n0U20ZvJycHLq7h6e648FgdFAHMn+llPzytW2sr+0wLYHO3qCZS9+oz4Js9yYfQF6xu5V1tR2m\noPz69W18TV8a8FtnzcAXCPPk8mpAy0D5+l8/5bpHV/H3T+vw9UXbuqq6jfPnl3PdCZMQAs6eW8aO\n/ZpVcdWxEwBMq6Moy4krw86XTtDSJA2/cHmem6VTClm5py1hBVEpJdf/eTV/+nAPoN3D7dBW7jpV\nnyw1EMXZ0QwYg8rCTDO3vMIy6p6mb8t02pmld2C5ngyml2Szvq4TXyAcFYICjzlHYHy+1ukZs2Rr\n2nzMLs/hmVuO59bTpyOEMOMYRsbVzZ+ZyonTinDYRUyeu1GXv3UQUf/JxXP7BToPhBFoPpBFVqi3\nvyTJDtBwXS2o7O+Dz3JlMLUkK2GxvLGOMThQrqHDmKOOOoqf/OQnnHXWWUQiERwOBw8++CB2u52b\nbroJKSVCCH71q18BcMMNN8kUrUwAACAASURBVHDzzTcfcrA4EIqwu6WH2eW5gx9swTB7D2QR7G31\n8eB7u2jq9pv518GwpDcYJtOZYebUD9Z5WNtqBG831XVy4vTimCJqFy+oYOXuNv743m4Ks1ysqW7j\ng50tlOW6+PFLmyjLc3PC1CI21HXgDYQ5qjKPW0+bzl0XzaXNG8AuBF86YRJHVeaxrbGbDLvgH+v2\nmf9YN540he2N3SydUsiPXtqkzaCdUsi/NjQw80evcc68ci5dWMFxUwu54sHlTCzMNDuyHFcGmc4M\nbjtzBvkeZ8yoORFCaB3tJ3vbyXZl0NMXYmJhJjNKc/ioqoVx+dFc9YsXjKemzce3zpzBM3qaZp7H\nwfSybJ7Sa+aX5kYtAgPDXTSpKNOMTZTmuDl+arQK57GTC3ltU6NpTRltu+nkqWb6K8DVSydQ3eod\ndCQ9UCB/IAxffvw6CFaOn1LIhUeN46gEHXsiJhdlcdmiCj63qDLh/re/c+qQ2jhWmFmWwyULx3PK\nzAMPMg4VJQTDjLUMNcA111zDNddc0++4tWvX9tt2xRVXcMUVVxxyG55ZXcPP/rmFj+88o98KSIno\nC4V56L3dpjvB2kGEI5KH3t/NxQvHU5HvMVezWrWnjRmWksqdvUFNCPSRqT8YwRcIDZo5tKu5x5yQ\ntL6uk4lFmTEunLJcFz+9eB5f/csn/ODFjdgEfOvMGUwuzuTbf1tPTauPOeNyOW/+OF74tI45uvhl\n2G2U5rrNgCHAA19czP+9WwVEXScep53/vXqRGY8oz3OZI+ZgWPLvzY38a0MDN540hW2N3Wyz1Kkx\nrItbT5s+6HdssHBCPm3eAFOKs3hnWxMV+R5uOWUqJ88oMn3soGW3GL54I8sm1+2ImZBUku2OeZaJ\nhZmmuyXH7WDJ5AJW7G7rl3FiPN/O/bE1iu48f3bMZyE0t9Fwc868ch58b9cBhaA0180DX0zeysiw\n27jvyoGTLFKZeplK3A47v7tqUcrvo4TgCGR9rVZmeG1NB+fNH3xOwkPv7ebeN3cwV5/ebw2Ivb6p\nkV+9vo2IlLy/o5lP9moB2br2XrZbsnE6ezVf8n5LyeJvPbOO4/TZlwNhZPS4MmxsqOugPE/rtF7+\nxklMKMhECK3EwEtfP4md+3vIz9Sm9xuzREMRSabDzpdPnMzulh7TShmIgkwjLTHW2jKyVMpy3Mws\nzeHk6cVcsnA8p84s4cqHVrCquv/s6dYEufSD8f3zZnHbmTNYtq2JHHcGboed8jw75QcoKGYKgcfB\nSdOLmViYiTPDxuxxmhAfP6WQBRPy+fXnY/PiT5xWzIrdbYTjXFxzxuWydHIht57eP/g6Enzv3Fl8\n9dSpZtBYMfooIRgBuv1BnBm2mBFfKjE61w11yQnBuzu0KewOfYTrC4R5bk0t58wt5/fv7AS0qo+G\nyybLaccbCPP21mjNfWMx+EaLELy5ZT8dvkA/Idiyr4uWnj5OmVnC1oZunHYbZ84pZdWedlwZNnLd\nGcwbHxv0c9htzB0fdXUVWrI/slwZzK/I48VbB1/xLV/30cf7nguznNx9yTzOmlOGzSb4y83Hmfsm\nF2Xy/s5oCmJpjstMxxwqrgw7rgw7ly6q4FK9xs1gjMvzYBPaLNmyXDfvf//0mP2luW7+8fX+z37a\nrBLue3MHk+LmVdhtgme/esJBtX84sNvEIWXvKIafI0YIDH/7WKSmzUeex0FlwfCtNzrQUoihcMQ0\n+TfUdbKnxcu9/97Oby5fkHDGZCQizVG+EfztDYb53vMbuNO20Qwgb7TU37n7kvnc9Y9NMfn3O5t6\neGZ1LS+urY+5vrVs8uMfV2MT8PSqWrY0dHHradPY3dzDlOIsLjxqPK9ubOQf6/dxxTET+mV+xGOd\nCOVxJi+wRgeUaILOdQNkCVUUeMzvoSjLyfUnTebXr29P+p6Hynnzy3n99lOScvNZOboynzduP4Vp\nJamrY684MjgihMDtdtPa2kpRUXJL9o0kUmo58vGFuQ71mq2trbjd/TuGPS1eAmEtR3xDXQfvbW/i\nlQ0NXHv8pJiAoZRa7XxrNkJ82YBwRDKh0ENxtou1NVo65r9uO5l54/P468q9fFrTYaZd/uilTQnb\n2tjlJxSOaLnlH1eT5bLjD2mup39u2Ee+x0l5npsz55SS48qguy+U1EjZyHgByBzCOq6GABzIFRNP\nRX5UwN+54zTyPA6ynBlMPICPezix2wQzyw4u9XHWIaZMKtKDI0IIKisrqauro7m5efCDR5iIlOzv\n8NNuF/ibh29hCbfbTWVl/wwJYynBixeM58kVe1lfp43k39/RzOubGrnz/Nm4HXaeXLGXu/6xmSuW\nRK8RP0PYabdx2xkzeHd7M2vRhMDMyR6Xy6c1HVQWeMxKlvdcOp8fv7SJq5dOMAuSRaQmBsXZLva2\neinNcZtVFfd39tEbiDB3XC5uh53PH1PJBzubYzJXBiLblWGWOhiKRTC9NJvHb1zKiUmsZWtgpPBl\nOu1m6YVkJtwpFIcLR4QQOBwOpkwZuDDZaFLb5uPCJ5ZRmuNi1Q/POujrdPuD9PSFYlL1egNhGtt8\nMbV1lm1rIs/j4NSZJZoQ6KtUPfT+bkIRyZTiLB7+YLc5oq5uTVwW2ZVhY91d5+Bx2s0yAw67MH3z\nc/SRpiBqgV1+TCVXLKnEYbPx3Jo6c+Hvqx9eweSiLCJSSx0MS2kWP2vp6TNrvf/4ornajNtB3EKg\nZYHkZzpp7u4bcOLSQJw6xFQ8Iz0z2RpMCsXhhppQlmKMUXZLT3RxkoPhN29s58o/rojZ9sf3d3HB\n/35ASB9he/tCvLF5PxcePc50WxiFzYxO+bGPq6lr7zVn4dbpFTcd9tgOrjzPbY60jXz10hy32Ukb\nC4gYFS8XVObhdmiBUJtNUJDlNP34tW29fKAHW0MRiZQwvyIa+DVWf7LbxAELvcVjiFLmECyCg8GY\n5h9fo16hOFJQQpBijNmyEZncJCt/MJywnklde//Fwauaeuj2h9jXoWXqvL6pkd6gVkpgIB+4sT6q\nwb5OP067rV8WjdUnbeyzrtI0U7cIbEKw5kdn9ctCOWduGTedPLCVNt9SF74s5+BcZoZVM1C1yOGi\nJNuF026LWZxEoTiSUEKQYqwLmPxzfUO/jjiek375Dsf+/C1Am2z1wU4t7tGurzFrTHyC6GpVe/Q6\nKy+urWdCoYdjJhWQ63aYVRYNv3Z8eQODgiyHOfqfUpzF7WfN4FeWnPRSvQO0Zq3kuh3cc+l8nrzp\nOIqzXf1SY3/+uaPMRbohWl/GYL6lHO9QArdWDIvjUMtdD4bNJrjn0uQqsioUhyNHRIxgLGMtyHbP\nK1t4c0sjz9wSO3qORCRCaH5v6ySlO55bz9qaDr7ymSlmieBdTT0coy90YaRmVrdoZQA+2tXCN/V6\nMqD5tHc29fClEyZx9txynly+lxc+reOk6UVs2deFLxCmLxShMMtluoY8Dju3nzUzpn1GTZv49MUv\n6csZHogTpxWR5dImTu3v9JsrV80oyzarapbmHlwdFSMVNNOV+vkZVx6buPqpQnEkoCyCQ2RtTbvp\nytlU34k3rrZ9V1wmzso9beaSgaClcX75z6u47Zl1MedKKdmhlzN4ce2+qEWg1+XpC4XNSU17Wry8\nvqkRKYlJvTRG2uW5bhZOyDcLin1+cSVr7zrHFJTCLIfpXkmUgVOe60aI2Jo2yfLUV47n4euW8Pur\nF/FHS7mHkmwXZXku7DZBcdbBCYGxnGGqYwQKxZGOEoJDoM0b4PN/+Ji/ra6l0xfkot9/yPdf2BBz\nTJc/VhikhJfWRSddrditFVF7dWNDzDqv1a0+vIEw4/LctHr7zNiA4Rpq6PBbjvWyq7mHHHdGzHJ/\n4/OMWuaaIBwzSVtdylysW/f9F2a5zM400TJ8BVlOHr9hKVcunTCUr6cfeR4HGTaBTWij+XF5Hkpz\nXEllCSXCKBeR6VCGrUJxKCghGCJd/iDn/fZ91tV20NTtJyJhx/5udrVoHXRVXCGvrt6g6aMHmFmW\nzZrqdvPzH9/fRY4rg3BE8ts3d5jbjXVzz55bhpSagIA2w7c3EDbjA8XZTqpbvNS39/YbsRsWgeF6\nWTqlkHV3nWMuCFKk+9gLMx2mn909QBmMU2aWHHItdptNUJStZRPZbYJLFo7n8iUHLy4nTS/mrDll\nBx1jUCgUGkoIhsiWfV1sa+xm+a5WcyZudavXHKlbFxcBTThyPQ7++KVjeOP2U5hanE21HjDu9gf5\nqKqFa46fyPyKXHPyF2jVPd0OGydbaqhfsaSS5u4+fvHqVjM+cNL0Ymrbe9nT6jXTHA1mlGWTYRMx\n260590bhtcIsl+kSSlSGYjgpznZRpLuCvnjcJL5z9sxBzhiYOeNyeeTLS4aUcqpQKPqT0v8gIcR5\nQojtQogqIcSdCfZPFEIsE0KsFUJsEEJckMr2DAdGJ17b7osKQYuPXboQFMUtBt7VGyLX7eDceeXM\nKs9hSkkWNW0+QuEIH1W1EAxLzphVyk8/Oy/mvI31nUwvzY7JXb/w6PF88ThtacMtDV0IoU2OCkck\nu5u9/fLcL5g/jmV3nGauCxuPsRxgYbaTLF0IXAlcQ8PJabNKOGXm4bdAiEJxJJOy/3ohhB14ADgf\nmAtcLYSYG3fYj4BnpZSLgKuA/0tVe4YLI1Wzts1Huy4E+zp7zdm3faEI7+1opqnbz/Jdrexp6SHX\nEx2FTynKIhSR1Hf0smxbMznuDBZPKmDJ5EL+58oFnDG7FNDmDRRkOmPcHoWZTi5dVEEoInlqZQ1H\nV+RxtGXhjnjXkM0mYmYdx2MsNl6Y6cRjuIZSbBF879zZ/PDC+D8DhUIxmqQyyrYUqJJS7gYQQjwD\nXAJssRwjAWOKaR6wL4XtGRYMi6CmzWemekoJH1ZpM2e9fSG+/OiqmHOsi2kb/vndLV6WbW/ilBkl\n5spWn1tUiceRwTvbmmjzBijIdFKY6TRr6uRnOhiXl0OuO4Muv1acbXJRFq4MG32hSD+31GDMLs8h\nx5XBrPIcc5WwgWIECoXiyCWVfoAKoNbyuU7fZuWnwLVCiDrgVeCbiS4khLhFCLFGCLFmtAvLVbdo\nJRnq23tpTlCT3liGD6IrWO211POZXKyN0F/b2EBTdx+nxa1zm23x4RuLixuuncIsJxl2G6fMLMFu\nE3x2wXgy7DZzFvBQ0zsnFWWx8WfnMr00+4BZQwqF4shmtP/rrwYek1JWAhcATwoh+rVJSvmQlHKJ\nlHJJSUlq1+48EJGIpLrVS57HQSgi2dLQRUmOC7fDxpTiLGaX55i5/efMLePV204GiFk0uyTbRZbT\nzrNr6gA4bVZpzD2y3VYh0Fw3pblaiQOjs77z/Nk8ev2xZglpY03ZoVoEVkYqWKxQKMYeqRSCesCa\nG1ipb7NyE/AsgJRyOeAGxmwksb6jl75QxMzkWVfbwcTCTFb/8Cze/s6pTCjMNK2Ezy4Yz/TSHDb+\n9Bx+cEF0LVghBOfM01YNm1yU2W+BlGzLLNkCvZZOea6bgiyHOWO4siAzpoLmRQvGc87csoOemAWQ\nZcYIRntsoFAoRppUxghWAzOEEFPQBOAqIH4V9xrgTOAxIcQcNCEYe4sK6LyxuRGAK4+dwL82NiCl\n5q4x1l7NdNrNKp/GyD7Ruqz3Xr6AE6YVJVw5KivONQTw9dOnxyzoHs+pM0uGXFo5HmURKBTpS8qE\nQEoZEkJ8A3gDsAOPSik3CyHuBtZIKV8Gvgs8LIT4Nlrg+Ho50BqMY4C/f1rPgso8PjOjmAmFHmrb\nemPWzrWWOsg5QI18m01wxQATqWKFQLv2/Iq8mGqdqcCMEahgsUKRdqR0br6U8lW0ILB1212W91uA\nwVccHwPsbu5hS0MXd100FyEEp80s5ckVe2N8+h5LqQPr9qGQZamkmT9AtdBUkDlC8wgUCsXYQ/3X\nJ8ny3a0AnK7n+R+rL6fY2Bmt+WO1CLIOsjSy3SbM6xRkOgc5evioLMg0g94KhSK9UNW6kmTVnjZK\nclxM1lf+OmduGRcvGM+tp08zj7FW7sw5SIsANPeQLxCOWaA91ZTlutl2z/kjdj+FQjF2UEKQBFJK\nVu1pY+mUQjNzx+2w879XL4o5LsYiGOI6ulayXRm09vQdcpE3hUKhSAblGkqCuvZeGjr9LJ1ceMDj\nTD97hs2cLXwwZLns5HkcB12eWaFQKIaCEoIkeG+HltF60vSiAx5n1Os5FLcQaBZB/gjGBxQKRXqj\nXEODEI5I3t3exIRCT0zNoERk6jn42YfgFgJYOKGA1p7BF7pXKBSK4UAJAVpnv3xXK8dPLSTD4tJZ\nW9POZX/4GCnhuhMmmfGBgTCCxYcSHwCthIRCoVCMFMo1BLy4tp5r/7SSz97/ET2WdYN/9/ZO7Hrn\nf+FR4wa9jiEEh2oRKBQKxUiieixg+S5tjsDWhi5uf2YdWxu6uPuSeby7vZnvnTuLmz8zBVcSM26N\nYPGhxggUCoViJFE9FrCqupXz5pXjD4V5a+t+AP7zX1sBuGxxRVIiANFF1JVFoFAoDifSvsfa19FL\nbVsvN5w4haVTCmnzBthQ18meFi8TCzMZl5d8aefhihEoFArFSJL2MYJ1tR0ALJlcwPyKPF7+xslc\nsnA8AEunHHjeQDyGa+hg6wwpFArFaJD2QmAsN2ldG9gQgKEKgcdhZ0ZpNnPH5Q5+sEKhUIwR0n7o\n2u0PAsSUczhvXjmfVLdzztyyIV3LZhO8+Z1Th7V9CoVCkWrSXgi6ekM47AJXRtQ4Ksp2cd+VC0ex\nVQqFQjFypL1rqNsfJNftGHSymEKhUBypKCHwh1Tev0KhSGvSXgi6/EFyR3AlMIVCoRhrpL0QKItA\noVCkO2kvBF29QXJcyiJQKBTpS9oLQbc/RK5HWQQKhSJ9SXsh6PIHyVFLQioUijQmrYUgFI7gC4TV\n2sAKhSKtSWsh6PZraw+oYLFCoUhnlBCghEChUKQ3aS0EXUadITWPQKFQpDFKCFAWgUKhSG/SVgg+\n2dvONQ+vBFDBYoVCkdakrRBs3tdpvi/LdR/gSIVCoTiySVsh8AfDALz/vdMpyXGNcmsUCoVi9Ehb\nIegNRACoKEh+TWKFQqE4EklbIfCHwjgzbNhtah0ChUKR3gwqBEKIbwohCkaiMSNJbyCMOyNtdVCh\nUChMkukJy4DVQohnhRDniSNkKS9/MIzHaR/tZigUCsWoM6gQSCl/BMwA/gRcD+wUQvxCCDFtsHN1\n4dguhKgSQtyZYP//CCHW6T87hBAdB/EMB0VvMIzHoYRAoVAokppJJaWUQohGoBEIAQXA80KIN6WU\n3090jhDCDjwAnA3UoVkVL0spt1iu+23L8d8EFh30kwyR3kAYtxIChUKhSCpG8C0hxCfAr4GPgKOk\nlF8DjgE+f4BTlwJVUsrdUsoA8AxwyQGOvxp4OumWHyL+UEQJgUKhUJCcRVAIXCal3GvdKKWMCCEu\nOsB5FUCt5XMdcFyiA4UQk4ApwDtJtGdY8AeUa0ihUCgguWDxa0Cb8UEIkSuEOA5ASrl1mNpxFfC8\nlDKcaKcQ4hYhxBohxJrm5uZhuWGvChYrFAoFkJwQ/AHosXzu0bcNRj0wwfK5Ut+WiKs4gFtISvmQ\nlHKJlHJJSUlJErcenN5gGLdDpY8qFApFMj2hkFJK44OUMkJyLqXVwAwhxBQhhBOts3+538WFmI0W\nfF6eXJOHB39QBYsVCoUCkhOC3UKI24QQDv3nW8DuwU6SUoaAbwBvAFuBZ6WUm4UQdwshLrYcehXw\njFVsRgK/Sh9VKBQKILmR/VeB/wV+BEjgbeCWZC4upXwVeDVu211xn3+azLWGm14VLFYoFAogCSGQ\nUjahjdqPGKSUeoxACYFCoVAMKgRCCDdwEzAPMAv3SylvTGG7UkogHCEiUVlDCoVCQXIxgieBcuBc\n4D207J/uVDYq1fiDWglqZREoFApFckIwXUr5Y8ArpXwcuJABJoYdLhiL0qj0UYVCoUhOCIL6a4cQ\nYj6QB5SmrkmppzegCYEKFisUCkVyWUMP6esR/AhtHkA28OOUtirF9AaVECgUCoXBAYVACGEDuqSU\n7cD7wNQRaVWKMV1DKlisUCgUB3YN6bOIE5aZPpwxLAJ3hhIChUKhSCZG8JYQ4g4hxAQhRKHxk/KW\npRDDIlDpowqFQpFcjOBK/fXrlm2Sw9hN1BvQ0kdVjEChUCiSm1k8ZSQaMpL4VbBYoVAoTJKZWXxd\nou1SyieGvzkjg0/NI1AoFAqTZFxDx1reu4EzgU+Bw1YIuv3a1Igct2OUW6JQKBSjTzKuoW9aPwsh\n8tHWHz5s6faHyLAJZREoFAoFyWUNxeNFW1/4sKXHHyLbnYEQYrSbolAoFKNOMjGCf6JlCYEmHHOB\nZ1PZqFTT7Q+S407GK6ZQKBRHPsn0hr+xvA8Be6WUdSlqz4jQ7Q+R41LxAYVCoYDkhKAGaJBS+gGE\nEB4hxGQpZXVKW5ZCuv0hZREoFAqFTjIxgueAiOVzWN922NLlD6qMIYVCodBJRggypJQB44P+3pm6\nJqWenr4QucoiUCgUCiA5IWgWQlxsfBBCXAK0pK5JqUe5hhQKhSJKMr3hV4G/CiHu1z/XAQlnGx8O\nSCnp6dPSRxUKhUKR3ISyXcDxQohs/XNPyluVQnyBMOGIVDEChUKh0BnUNSSE+IUQIl9K2SOl7BFC\nFAgh/nMkGpcKuv0hAOUaUigUCp1kYgTnSyk7jA/6amUXpK5JqaWnT9UZUigUCivJCIFdCOEyPggh\nPIDrAMePabqURaBQKBQxJNMb/hV4WwjxZ0AA1wOPp7JRqcRwDan0UYVCodBIJlj8KyHEeuAstJpD\nbwCTUt2wVGGUoM5WJSYUCoUCSL766H40EbgcOAPYmrIWpRgVLFYoFIpYBuwNhRAzgav1nxbgb4CQ\nUp4+Qm1LCR0+zSLI8yiLQKFQKODArqFtwAfARVLKKgAhxLdHpFUppM3bh8dhJ8ulLAKFQqGAA7uG\nLgMagGVCiIeFEGeiBYsPa1p7AhRlH9alkhQKhWJYGVAIpJQvSSmvAmYDy4DbgVIhxB+EEOeMVAOH\nmxZvgKLswzb7VaFQKIadQYPFUkqvlPIpKeVngUpgLfD/pbxlKaK1p4/iLGURKBQKhcGQ1iyWUrZL\nKR+SUp6ZqgalGuUaUigUilgOZvH6pBFCnCeE2C6EqBJC3DnAMVcIIbYIITYLIZ5KZXuklLR6+yjM\nUq4hhUKhMEhZ6owQwg48AJyNVrp6tRDiZSnlFssxM4D/B5wkpWwXQpSmqj0A3X0hgmFJsbIIFAqF\nwiSVFsFSoEpKuVtf1ewZ4JK4Y74CPKAXskNK2ZTC9mhuITo5puWlVN5GoVAoDitSKQQVQK3lc52+\nzcpMYKYQ4iMhxAohxHmJLiSEuEUIsUYIsaa5ufmgG9Ta08dF9hUsWv8z8B7Wi6wpFArFsJHSGEES\nZAAzgNPQZjA/LITIjz9ID1AvkVIuKSkpOeibtfQEcKMvvxzsPejrKBQKxZFEKoWgHphg+Vypb7NS\nB7wspQxKKfcAO9CEISW0evtwoNUaIhxI1W0UCoXisCKVQrAamCGEmCKEcAJXAS/HHfMSmjWAEKIY\nzVW0O1UN8vaFcAqt1hAhf6puc3D0dUNfEquAhvqgtz317VEoFGlDyoRAShkCvoFWtnor8KyUcrMQ\n4m4hxMX6YW8ArUKILWizl78npWxNVZuCYYnTsAhCfam6zcHx9/+Af9w6+HEf3AuPnJ369igUirQh\npZXXpJSvAq/GbbvL8l4C39F/Uk4wHMEzVoWgsxYykpjf0N2g/SgUCsUwkVYlOENhiUsYMYIxJgQB\nL8jI4MeFAmPPraVQKA5r0koIgpFIVAjGmkUQ8KKt/TMI4T6IhCASBps95c1SKBRHPqOdPjqiBEMS\n91gWgmRSWsNGsDuu/XuXw3M3QCQJqyJV/OMbsPH50bv/cPPSrVD19mi3QqFIOWklBKGxahFICYEe\nCPoGP9Zod7xrq/pD2Px3CHqHv33J0NsOa5+EF24anfsPN1LCur/C7ndHuyUKRcpJKyEIhiVOEdY+\njKUYQbAXkElaBPr8h3ghC4/yRLnaVdprdtno3H+4Mb5fFY9RpAFpJgRWi2AM/YMH9FF8OADh0IGP\nHVQIkrAqUkHNcu21bP7o3H+4CSshUKQPaSUEoXAEF4aPXe84A15Y/4zmCjAIh2DdU5bXYOyFpIQ1\nj8IH94GvbWiN2PkWvPsr2L85ui1gmUgWGmREb7qG4mZGG20cLYugZqX2KkbgT6q3Hbb8I7X3ML7n\noBICxZFPemUNhWW0xIQx0tv8kjaRq2AKTDxO27b3I3jpa1D/Cax+BDrr4dTvRS/UWQuvfFt7786F\nY29OvhH/vA266qFlO3zhUW1bwOLXD/aCK2fg88MDzIwebYugeZvejhFwuW14Fl77Pnx/D2QWpuYe\nyjWkSCPSyiIIhiM4RVytoY4a7dVwbQD4O7VXr17ptGVH7IWsHXdvR/INkDJa9TRg6bBjhGCQjtx0\nWcRbBKMcIzDuOxJB+L5u7TWQwsC4EgJFGpFWQhCKJLAIOuu015oV0QONDiaiB5bja/tYO+u+ruQb\nEPQl9j1bXUODdeRmjCDeIhhF15CU0faMhBCYbpsUPquKESjSiLQSgmA4ghNLHn7Ap7l5AGpXRHPw\njY7ZsAh622NH8NYOyD8EIfBZyiiZHWcgaoFAYosgHIpaAMZrvAtmNF1D4QDmZLgREQL9+0/lsxq/\nHxUjUKQBaScEDkMIPnkMfjEO6taAzaF19q07tX2GRdCzX3utX6Mdu/NN/UIWIRiKRZBICJ66HJ6/\n0dLIBKPct34Cj12gvTdHqmMofdQ6ah6JGMFIWAShASwvheIIJK2EIBSWOKTuGjJGk0EvzNCreRpx\nAlMI4lbOrFsde67NcXAWQXaZNtIMeGHPB8SUlkjUuTWsh2Y9TjHQzGLTNTQKFoF11DwSFkFwBC0C\nJQSKNCCthCDGIrAytSfIAQAAIABJREFU5VTILI7GCQwhiO9oHJn6dr0jyikfokWgp5rmjtc6mLo1\nIMNxjUzQuXXWQV+n1tmHxrBFkOE+gmIEyiJQpA9pJgSSDJlACPInwMTjLRbBAAvE9OodudFZZ5cd\nnEWQW6F1MLUrEzQyrnOLRKBrX/R8w/UylmIERmfpzhvhGEEqXUMqRqBIH9JMCCJkyAQzd/MqYeIJ\n0F4N3Y0DpyUaHXmyFsEbP4ytVeNr1SZc5ZRr17CmrBqseghWPWw5pyXa6RsxCziAa2gULQJ33tBi\nBBufh+UPHMT9DIsgla6hAWZwKxRHIGklBOFwmAwsQuDIhGNugJI5UDxT29ZRGysEuRWw6Eva6N+X\nwCIwctrj6WqA5ffD09dEt/lawVMIDo/WwXTWQ3Z57Hn1n8CaP0c/G1lNxjUNxpJrKGgVgkDsLO0D\nseFvsPpPB3G/EbQIBpvprVAcAaSVEPQry1A2Hz77W8hwajOEQRvhW11DWSVwyf1QMjvWIhB2yCrW\nhCBR6edaPd6QOz66zdcGmUWQ4dE6mEAPlM7uf641u8iY5wCxK5MNOI9glF1DkPwoOuAdWozFvN8I\nWASmCy4wuqW9FYoRIL2FwOj8AVwWIbB2MM5s7TWzKFYIHJn6ORICCawCI/BcMDm6zdeqC4G+JGVv\nOxRO63+urzU6qh5ICPrVGhoDwWJDCJJ1DwV6hhZjMe83EhaB5RlUwFhxhJNWQiDiO0+XRQgMUfB3\nxbqGnFnaa4wQ+DT3jvUcg70fayN/w/8f9GnZQd2NukWgu4aMfUbnaSUSjLqcOuu0bByIBo0h2jn5\n2rRFaVIhBDvfSi5YagpBvv55CBZBuG/ofvgRiREoIVCkD+klBJG4jKGBLIKBhKC3Q5vlG+zVOnPr\nOaB1Ho9/VguANm7Stvm74JEz4XcLNH9/TnnsIvXOLMibABXHxLbNEJ3OWsifBM4cTUwMjGDmYxfC\nn88b/pTKjhr46+fhxVsGPzZ4CK4hGDjOMtj9UtlBKyFQpBHpJQSmRSC0F6tF4MzWtvvjYgRWIUCC\nv0O3CDL7WwS+Vm094ZoV0fkBRsppyK8JRuWxWozAet/bN8JX3oltrBGY7qzTspoyCxPHCJq2aK9G\nZzpcQmB00smUez6UGAHElthIBjO1cwRqDVnvp1AcoaSVENgjuhAYZZ6tFoHNpglDP4vAiBHo5Y59\nrQNbBMYovv4T7bVwaqw7B7T5CvEWgRD9G2taBIYQFMVeK97NZcQphstdYv0OBssCOpgYgbE8Jww9\nYGwKwQi5htRcAsURTtoIgZQy6hoyhMAV55935+qTtgJg05dqiLEI0IXAbwkWE2sRQDSYWTqXmPIR\nOeM0N4/DahFkJW5wb5t2H2+z5jrKLIpaFzDwKHXYLAKLVdS2+8DH9rMIkowrSD0bZ6gB45GwCGJc\nQyqFVHFkkzZCEIrIaOXRRBYBaB27kaufVaq9xgtBz35LsFjv+HwtcN88bdUyK2XzYj9PPF4b/RvB\nX4haHPH4WrUFbCBqEcQ8UF/iZS0PdZTsbYVfTYGqt6PbEs2ANnjwZPj499p74/uMXyshEVaLYygW\nQSQ8vLOoA174zSwtMG4lRgjiLJz75sFzNxz6vRWKMUL6CEFY4jQmk5kWQdxKYO5c6NbdL9kl2qvR\nURfPBLsT6j+NuoY8Bdq+5m3QVRetTgravhzLZLEL74Wz79HexwiBxSK4dSV8bbk2R8HXGk0dHUgI\nrDEDg0MdJbfv0SyPujWWbdWJjw0FoHGjZrXYHNFaTMlYBFaLYygWgfXaw2ERdDdCTyM0b43dbnVv\nxd+nqw42//3Q761QjBHSZqnKQDiCw1idzHDpuBJYBPvWae+zy7RXo6N2uGH8Ii0QbFgEGU7tnBa9\nfLV1hJpXGXv9WRdC7rjotQysQmBMLjNSVWOEwLIkY4ZH66iscwwMDrVzNNxbhiAKe+L7QKyryuGJ\nxj7i4xeJOFiLIHSADvpgMO4dX1bEKjgx1oHl2Yx0YIXiMCeNLILI4K4hd27UH5xtuIYyo/snHg/7\n1mqZQ4af31MQFQIreRNi5wjEdOSDuIZihEBos5OtFoErRy9REddB212aGCVb4iERphDoqaols2LL\nXCQ6FjQRsOtCkJRFYOl4h2IRWDv/4XANGfeOLzQYCkR/T9YYgVX8DuQyUygOI9JHCCKJXEMJLAID\nM0Zg6agnnqBN9vJ3Rt0gmUXgtaxbYHTYuRXR6zlzYjOFBnINWa/hbdU64OxS7dx4IahdCR/+T+x5\n7jwtbfXlb0DTNnjmi/CPr0fLT4A2iv3XHbErrlkxA956Z148Y2CLwGfpFDM8lo4zGYvA0vEOZhE0\nrId3fxnbLrtLC2K/dmc0VtK1D579MrzwFeizXD/g0565ez+8+v3outHWeyeyCBKlw1rFL1HRQIVi\n/2Z4++5DG5CNMGkjBIFQJCoE08+Co67QXC5WrBbCnM9qP+MXR7dNOC763rAI4n33BVO0QnZzL4le\nL959ECMEmfSjYLIWd2jcGC2GZ72Pcd32PVCxJLp9+llQPAvW/gXe+AFse0V737IjekzV27D6Ydj3\naf/7Qtwo36NlOXXWJ663E28RZDi198mkj8ZYBIPMI1j3NLz7X9pxhhAY3+nKP0Brlfa++iPY8hJs\nfDb2+erXaM+8/Pew6o+xFWH9AwhBOBAVAqsVYhW/gWInivTmDyfCB/cOXM5+DJI2QhCTNTTuaPj8\nw2B3xB5kxg7ytHjAlX+BLEsHnFmodbQwsBBkFWuF7KZ8Jnq9eCGwpo86ElgEE4/TXBAN6zR3VPw1\nDItm9kVwxePR7ZNOhGuf197vsmT9WEf0hpvH2olbsW43Zj2H+7TMqAMda3ccnGvI5hjcIjDa3Fkf\nvbbH8n0Yz9fX2X+btZ0tumBYXUoHaxFkuAe2qhQKOKz+PtJGCILhCA6hz/a1OxMfZIy0y+YmnuQF\n0Y7Z6hqyYu2gTIsg7hjDTZThBnuCeP3EE/rfz3oNb2t0n8NiUdidWsedW6F9nv8F7TWmlLWekjqg\nEFhGvM6sqNWUyD1kPTbYG32uobiGcsYNHiMw7t1ZF53cZXWpGc9nvU5nvaWd+rPGr0ltPaefEFgs\nAmuMwLhW/sSB161QpC/W/wllEYw9RMt2Pm9/X/tgdyU+yPjHLp078IWMTtq0CApjr2kduTuztYVo\n+gmBfu5Ak8mKpmvnCBtULtW2GamqAE2bo22xWhd2hyZghnjMvUQbcXfWa/n3NSujnWoiIahZqaWC\nWtsfLwSNG6Pfk/UaQZ9FCPTOunmHtvjMxue12kUdtdHrGNfIHaeNyr0t0Lor8fdhiFdXXfTaHTX9\n9/d1ac+bWRwrfsY/Z9se/d4J4hP9XEN9Uctrw7PRaxiveRPG5j967erUl83urI+rirt/8EmHB4uU\nWlHFQ/W3N20beimTzvrYvzMrvv+/vTOPj6o6+/jvyb6QmSwsAkkgCSiKLAIqFEQFrFsr1qVo626r\ndau1ta1WW/V9209bW1uXYq1WW2utW60WX1esa90oyibIVkQIsmMSICGE5Lx/POeZc+bOvckMzDCJ\nc76fz3xm5t6Ze8+5597znGc5z9lm1hG3WTvHfA4aKHS0czt5UYqfwTT4FjJGEBSvfhlHZC1De3Yh\nkB8wiUsmgI04M/hANUfxSD5czd+lk+97MGsDYtMHuFMur2WHq01WFo/egwQBETDkOPZJiFaRnWtG\nqKO/zu99hkX7G0TTGXIcm5wGTeSIo8Z64MMngQe+CHyshaE9cgGA+vd5vx0Jk1fMI1+AR9O7m4H7\npvAqakC0INjdbIShhI8+cT7w5MX8evpy4IkL+B2wBMEALsvsm4C/fTX2WrS1GOHUaAmCodP4PafA\ndEq7mvh6lVb5m4Yk/5Nf6GqMRtDKArukP/tr3rrdHCs/xIK5u2kEG5cA908DVryU2vPMupKDEITb\nDgTuPCw151r5MidVXPXqvh3n7iOB+49P7D+/HQ7cPsJ/319OAWYezp26je2bCro/lj3H7fTpvOjt\nH7/Oz+C7dydWziSQMYJgY92ZmNr6K8yZ/lr0KNpmyDTgurXAoAn++wEeIV+7Ahgylb+LICjpz8nj\nDjs3+veXvglMvCb2ODkFwbOKAeCUO4FzPJOWREs45S7gR5+yQCEy5iERBKPOAr67hP0b4UruFFe/\nyfvEPu4VBLLfJq8YKCwFKobySKexnjt5Gbk3bzWdf9tOLk9Wrk4foXgEPvocviZr5/CNLyPH3TtZ\nWIUrOdpn68rYvExA9DZbEEy4Cri+Hug/2vIRNHEnLXUWvNqPr2nIGz7ayhrO5ToySCKNmrey1pdX\n1P0EgVyrbQGaVTLPI+fydoTJRu7L1W/t/TFEQ/JOGuwSPTJvaYjdtWERv2/yHLPB0kSD7g95frx1\nkvMsfS6xYiaBlAoCIjqBiJYR0Uoius5n/wVEtJmI5uvXN1JVll25pfivGghV3LvzH3rnFgT9RnwI\nIgiKKljTyPJc0rwifz9ATkGwRgBwJ+SNKCqq4I43Kzv6vyLYxPlNxB04YDpFWShH8HaO3v2AOUf1\neN4varJtXqrQC+tI3qCcAravN29j2/oBhwJDv8imFtXOHUj7Hu54bWf0hkUspLwONjHxZOdF+why\nC9h0E66M9hEUhPiYjfVGxe5MEHSqEeTzyL/PwcasIIsL5fXqfoLATlSY6vPIuST7baqQ+9Lv/oyX\nfc0eW+9jxinRKw96Q4gb13K4OBBsOpT28f5XMghv8TE5pZiUCQIiygYwE8CJAA4BcDYR+RnfH1NK\njdavP6aqPHv0qCAvO8lVjgiCBGeYdiUIgs6V4+Po9moENuFKoHEN31xZlkCyO8eODrO0JmD5MLTG\nUj2BJ9H9V6fKjgiCbezPsMnJ4wevyZoVLT4LgIXBjg3cidrO6DbdqbZ4NBU514DD+CGTh1rKGK40\n4a22RtC2k1eAk3La+KW38PMRiM+jIBSdYbaogsu+e0f3ihXfH4Kgo4OvZ8tn2u9k3Td+ua/2hbYW\nTumSlcMZfeMJQgg6zt4gHbrffBF5dr2TChvreRImEDxQiAiCd6PvH2m/nZv2+yAjlRrBEQBWKqVW\nKaV2A3gUwPQUnq9T2tpZEOQkWxCIhlHcJ7H/5RXH5jqK51w5PmatiEbgJwiqzGfxfZTVsC3ztyPY\nBnvrYH6w66bwftGKbI0AYD8DwDfyzPEsYMSHIEIjp4A7azs9Rq++7CuRdSAeOxdY9ARrUN65HM1b\neUH7B7/M77O+zf+rOpI1kme/F13n0iqe5HfbQbxfBAFgNJgYQeCjEexp0Qnt2oC7xnId5Frnh6Iz\nzBaW62uj4utkXvslcHMYeORs4L0/8ES/RFk7h+3wtpliywo+7sbFpmwAX/uWBq7Hqte5rf77KvD7\nicDyF4E/HB1sfnjka8Cc+4LL0drIwlx1sJZkd4RyLd+5m8v15y+ZfQse5W0zx8cnPDcuBn41hNt2\nxJncPhsWdv0/P+xw4XuPAW4pB966Q38/lv1TNn+bwb4syVb85m3Akx5jhQxY1uj6z/sr+8+aPmXf\nHdCFICAOybad7PbgbF3APJ8UkcpcQwMB2LkJ6gEc6fO704loMoDlAK5RSsXkMyCiSwBcAgDV1dV7\nVZi2dr75crMDwkL3luLewOn3A7XHJPa/k28z5pt4mfRdYPhpsdu9piGbQ6azszW3CBh3IXeom5cC\n793DHfnz1/EDfdS1wKTvAIuf4v/NusoIgvJaFnQ7dNqJPS1sbx1+GnD4xUDN0cYhXtwH2LHJCIKQ\n7pSn3822639ewUJowGHAtJujBRXAD8PHb/CraT37Xqb+hOdI5BZyZ11abYTV8NPY9zDvr/y9ZjJw\ngHbwffoBMGB0fD4C2b5lBfsrRs4ADtMddkGIy97Rzqk3Qv2N4Nu9039SoM06ncBvzbssrMVhnwhL\n/smdRsMn5r4RDe2dmcCpd0cLgs1LuR7zHuK2WjEb2PghR0Ctn88j7GEnxZ5n9Zvc7kd8078ctlBt\n3moisQAWBEXlZj2OT97iTp+Il3AFuCx7dgX76YQty1njGjkDmHIjsOARHplXjuv8f37YwloctIuf\nBsZewPfIpx8Ax93C2zvageUvmN8feAJ/t81DHe1G22xcy5rKh/8w9Y5oBEGmobVAeQ23546Nxrxq\nX1u/hJIpJN3O4mcADFZKjQQwG8CDfj9SSt2rlBqnlBrXp0+CI2+NaAS5ydYIAGDEGUYziJfBE2PT\nVHdFRZ1xUtt0ZhoqKgeO/gHwhSv5AR93YXRZt67gznbKjayhjDkP6KWzpkpnZ4ek2ky7iYXEgV/k\nGxswPonGtezPkHMNmsBCSRhzPgvPwrJoLad5mxEiW1cAB50IjJrBI/9jfwRM/TEw9vzo+k24ynzP\nD7HG06ufThDYwmYiW+B4w0cjNt2dxgww7Raj7eSH2H67YxOPEsOVRkjGE0IqwmZXAwvl1u2Jh3iK\nCcYWXKJRejWCnZvMbGv5n/hRZATvN4lPFgvqbIKfLVQlH5asVS02bvm/6jDXxzZXxaNFibCeciNf\n77KavfcTeHNSDZnGaUv8BLLX51E3FRh3UfRyqrsauW79RgBQfG3tsNGKOk7W6KcRtG7n+0BC1O3j\nNm8192nQPJ8UkUpBsA6APdyr1NsiKKW2KqVk2uYfAXgW7k0ee7RGkJOVZI2gO9CZaciPQo8/Q9ZJ\nELymIcDMnyiv5XdZZMeLOG8b1vJn+7j5JSYEVo5HFG0esrOuStm6IjwwuuwiuNa8Y0ZZdgivPKDt\nbdxJSFbY3TvZV1I22GyTY+5qstaHqLIEQRy2XLtj3LYKgIpPgETK28yjePtYgBEKm5ZwJ26PKKXD\nj8zKro/+7jeJTxYL6myCn91BNX3KWqJ0ahGB1xT7+yhBEMeMW7mutp/Ka1OPF1vw5PUCDv8Gm7fe\nmcnb7Hk+XmFTEDKmQW/wQf9R/L5itlkhEND3R0AwgUx09F4zOW55DUcGfo4EwX8ADCWiGiLKA3AW\ngFn2D4jIetpwCoBE47viJqUaQbqJaAQ+piE/pDPJ0r+v9oTL5vsJAt0hV4033/1mX4cruZPbtCTW\n/g/wQ1JYFj3fIpKym1glFhOUX9n8yC8xo1Ipe9V49hFImJ+cL7fYPKAyGivRt+GL17NNPeZ6lLDz\nWML+ojQCfaylz/LiNsuej7W/72oy11pU/tX/Bub+idOeP30F8NRlbIu2aVwHvPFrNkt07DHHEqQd\n23ezyah5qxG0n3ijWTwOZL9Rf+S6xCkINn7I730Pjv5fq1Xf5q3cgTbWm0SOfhpB4zr2Bz11GfDC\nj4wvxPZTNW9h7ef5HwLPXM0T2QDWrt74NQumt+6InQRmC57Kw3X7ktH+bCe01zGcX8LCoKPNBCp4\nBcGiJ/T30fwu98fuHVzvN28zGqC0Qz8tCBrXst/r2e9x7qriPjxQ28+CIGU+AqXUHiK6EsCLALIB\nPKCUWkxE/wNgrlJqFoBvE9EpAPYA2AbgglSVx/gIPo+CIEGNYMSZ3Fmd8HPgxRvY/GJTNhioPTY6\nyd4BI4Ghx/MchdYmYNTX/I8tnf+W5cDgo2L3H3o6P1B2mO3wU9nBtvBRYIPuXIYcp0NBB8Yew/e8\nVaxyizYjgmveQ/x+0Il87MJSk3ROOvbeQ3kyz4qX2Cw24ozoY8uSpjKjO1xphIhEOz2qr4doTLb9\nvbWJR3p2WOCjZ+tyncQjyrxiYEEDcMQlxuTz4o84iV7tMdHHEuyZshuXcOdRdSTXQ9JpCHaGXMB/\n1C9aSrwawfoF/N7PRyOQ+kqEUdtO9gvt3OSvESx4BPjgQZ4V3ryFQ46zcsw9Le352s85mSLAeb8m\nXA6snwe88r9sdnvvHg79PfoH5tgiePoOB8acy/fAyBnst2jfzQMPCRf2zgvID0UvSZtbaAmCkfy+\nbi6Hkx5zPTD/rzzQydMDjheuAz56hid3Vo8395Aks1z2PFBvmZWKKkwa+v1ISntFpdRzSqkDlVJ1\nSqmf6W0/0UIASqnrlVLDlVKjlFLHKqWWpqosEj6ak2xncXcgUUEQrgS+MZsdbxe/aGzhQl4RcN7T\n5gEHWNv4+uNA7dHAWQ+zX8D32JY10M+sc9R32dZvM/YC4MRfcCcgncsXrgLOeCDm753WCTAP7QEj\nefS/9P/0LOtJwIXPskq+e2d0yKxESwHAhc+xDdlGhMvGJXz8gnCwaWjbKn7Zo9XW7Sxc/Vj2PDDs\nZA4eAKJzJEm7rnrNhOnagqB1OyKRWI31HMnS71Az670zWn3SLcSrEWTnsV9H2qrv8Oj/2fW1TX1i\nnvPTCNa8y/M1pM2b1nO7idbZ+0AeKS99lrWN4r5m9O6da+DVfuR8X32QByIAcNofgGsWGYEhZrXG\nehPgAHDbi5ZlhxADnNOrSPvAqscDB53AiSqJjCCQmf9yrda8B5TX8TOXlcMZhG0igsAT6ZZiPofD\nY38+1xpBTidRQ/sb2xwUj33fxl7bwc+sFM95Iyk5ckyESdXhZlKfhH3uaeGOo6zGjOKB6M+CCBfb\n3NWVj0CEzO4dfL6ymoCCKzZViAC1OzF7DYuaydypeE1DFXXcMW9awuajogrOXgsYc5kfvhqBZTIL\nssXLPIqiCpP6Q0xDuxrNfA6pb5Qg0OY5r0bQ0cHO1morpcr2T6NNk5GABcWaRd2xxmcgAkHMgDGC\nQJ/PL1JJ/APNW7n8rU1mpA/EagTyW/mv3A9ec6L4COScdlnFrJofMtewvM4cs2j/m4Y+h72iP8ZH\n4DSClFLcl0dsoYGxoaFdYTvtQnGahAQxIYkZBzAPp/2QSufyydv6oZwQ2+F4kc6paZ0pVyR81DOp\nTBbokdGpjCLLgwQBuAOMJPezk+VZnUHVeB295HEWF4S5TDLiLKqw/Dl+0dqa1u1sk27bxem5V73G\nUVEAO1JFKGzfGD13ofkz01kBPCIuLOV7r7XJCL5wJUfONKwxaSJsjaC9zaznsPkj1lCqJ5hOt3lr\n7IRLqVf1kfx55ybWvuRaSy6pxnrgs0/Y3NOw1oR65vqE+co912JFq4ntH+DrK+3fuIbnY6xfyO2c\nV2QJAs+gR3wEMrL/5G0O3W3ZZn4rx80pMOfMyo4WBFtW8v3V3sY+KL80LEkgY9YsPmXUAIysDKMg\nJzvdRUk+oYF8w3YHjSAri+Oo+48KTuUdhDxU4erodZ3joe8hHG1hR/vUHgO8/gue5yBIB/6w9gPU\nTDYj53EX+R/bXrkuMoHO0gjsEe7gidy5yihVRpElB5jJdjZFFRyGSBS7PrR0Ilk5fFyJXhJkJnVu\nkYkSKu4DlI3jazHsJJ5rUBBmu3tOoclN1fIZ8PtJPF9gzn0c9VIx1HPsXpxcrf9oNqUAPIKVFBsb\nFprOXSJrRFAVhPl3791jvsvAoK0FmP8wO32vXW7i7ysPj17e1SsIpB1rjzFrii99lsuUnW8WRPrs\nY+COkdyei540/qiuNAIxIdmCIL/EtP+sq41JTa5Vn2F87b2h4GIakg59xwbgqUsAEPsL5JpJGQ49\nDVj8D47Ea1jD/9uyEvjdWPY9jL0AePh04OTf8NydJJMxgmBAaSEGlHYxiaWnMvYCnXK6mwi582ZF\nmzXiZdpN/EAkqkkA7Fy8emG0SWnQBN5WZoW52p3L2Y8BQ4/j63bNEhM95MXOPyXmppxCAMQPu3TO\nU2/izuftu3gZ0VYrJj8/xA98k+UD+MJVwIQrjdkqNDA2Wd6wL7FTXyKrvBpBaADb0mXUPeAwTjZ4\n9QK+jnVTeVW2t+/iGd4XvQDMexh49afc+c+934Q+2snqdjWxMNm8NHok3bSOO+LjfwZsusoIAknD\nIdeiIGRMfZWH86RL0h1yW7Mese/S6cnX8L7SQSZnFRCblHHAaNOesiaHxP0fMMJM3JPst8tfivaF\n5PgMLmxBAH3MfocCIBYc2bmm/Vsb+foe/3PjA5n8feDIS2OfPREERNyGk67hUX1hmZlAJkKvqJxX\nQ/zOIh5oiJlP2uODh8xEUltQJpGMEQSfa3LygJJ+6S6FwV7VLRHyihP3KwhEPOnMS5lnroMtCA48\n3mgtnUUn2RqBlC8ryzzs0jmXVrOZpHoCoH6t8+O0mmMUlUcLgtBA1hQEv6yp1eOtiW0lPhpBiUmA\n1vsgc+3lP6VVpvMoCLHgKLLWtpDIo1ClyQ8lx96sYzekzO1tHP4aruQOzc7S69UI8kNGQ62bGt15\nt7VEzy9orOc6iEAUzckvF5e0Z2EpADKRWP1HsiDIyjGhtnYKl9wifw1VTFzN21hAiVmzsNRk1rXb\nv+8h0fXOLfDXXsVH0N7K2ovfjGhbIwBMm8l3mRTYVB99XVNAxvgIHA4AZpSZ1yt+05X98NlOX7ED\n77LMIQA7p0Fsu271jJCzcowpyrtgkZ1JVSaIedeq9moE+WHLTh3gExC/ifddKK81Dl/72GJu2rGR\nBdr29Txi9/PfxGgEYZN+Qsolphk/QWBrcn7zWLxkZbMw2qLDZMWcY5t1mnwisLxk5/L1EKd2aAAL\n+aIKownYAiXeIIa8Yr4e3ja0CVrBUL7b4cZyLePJjrwXOI3AkVmIeaJmcvz/sdOI28Ijrxh4/89m\nPoJ0YAVhNi+8eZuxW4tpqKhCR/80xGasDVcCix4H/jgN+Nrj7Py0O4n8sOlo2/dwbH5BKDhyRZDO\nw/suVE+IXVegtTF6lu3j55skbH6dYUGY4+VlcaH8EJsHd29n0xBgTDNtLdHhmo31wEArqUBBiE1K\nXWXnLaow8yVEAFRPMD4HW2j6OYojxynnyX2qA6g6whxbzFRZ2ZyGZPf2xASBrGcQJAi8GkGkPCII\nrLkgK2dH/yfJOI3AkVkMHMvJ7k5NcBWor9wLfMuzkMiUG3l0LPZ5u4M97uboeRgFIfYJnPQr8ztv\nqo/DzuEOtf4/ZjQYpBHYpoLBk4ApP47O5WQjnYf3vbAM+PIdwMSrYzujXU0s4CST5vLnOVMt4O/D\nGX+FjlJSpqz70EFjAAALWUlEQVTnPwOceo8ZUWdlsW+lrdloBA1reORum+YiGkEnCzcBpsyUxfNG\nTvglX+MTfxUrFDtLcjf1J5xgcMy5bPMHgGNv4JcgbRavILDTswelqA+6D8RcKJMrAZ7DYv8nyThB\n4MgssrLYcWevAR0Po2bwIjs2h54ePXvaHq0NmQZM/I75nlvEjsZDphvTjLfzragDvqSXxPQTBPkh\nNkV1tEebnHLygcnXBo+ggzSCcBUHGvQ5yHRWMnJuWqdDOn18Nn7+lEETgGOstafyQywIR58d/bvc\nQtYIJI3zp/PYuWsLF79cV37ItSks41H7+G9xJ3rkJSbdg33eIA49DfjSb/lVdyxvqz3afJb6APEH\nMtjXLUgQBGkEvQ7gCLLWRh0dRWbimdMIHI5uiD1CjDG5WJ2BX1I/P5OB1z5sdyKR6BXLFh9PxxCk\nEdidmpy3V18eYcsI1DsXIbIWgw9iAgKCO95cvcSnmIakg0vURwBYcxk6uY72efcFufbxzm/p1Te4\nLN5jegVFdg77KuQ4haV6fgZ1rSXtJU4QOBz7gnRglBX7kNoRQTb5IY5I8evovPZhr0YAeOL14xAE\ngRqB1fnKefJK2JQjYZnldTwqDVeZNaaDsJ2qQY743EKOqVft0ULMT6DGaxryFQSezrWr9Q+6Qnw8\nXa094YfX9GMfE/Avv1wP8SsBfH29S+EmCecsdjj2hUiOoxL/zu+IS2OX3xw8kVNc+IYz6odeOmJ7\n5TuZ5/DZahNmGjT3waakP/tGZMSeV8K5l2qPiT1vXjH/XkJHw5WcGK+kP0/c6moxpaOuNake/Mgt\nNGUfdjLnWiqqiE7tIaazeE1D+0MjqDkqerJiPHz1L8DLNwe3Uf9RHI7qty6JLQjaWjiUNEVmIcAJ\nAodj34gIgoCJPifdGrttzHn88kNGsg1rTBZLoXIsAOKwzoY1HIZqzwYOIicf+OYr5ntWFifg8ztv\nXjHQ50AWBJTNWs2Xb+/6HMLUH3e+P7fIxMcfegbwlXtifxO3j6A8+j1qn6Xh7N6+7xrBxKsT/88h\n04Md+AD7hC73WQ8ZMCYoEQRAyhzFgDMNORz7hjywyXpIc/LNimleM0xhGY8g17zDoZ3V45NnKoh0\nnEUm4iY0MPmz1XMLTUqOogCHfdw+gjg0Apn5vK+CYH8TZRrSgi6FGoETBA7HviBpipP5kMqD7xeh\nUq0Xot+6Yu9nYfshUVR5vcxxE80AGw+2iaYrJ2pSBIEsSLSPpqH9jbS97SNwGoHD0Y2pGJrcFB8R\nQeDTEQ+eiEis/qBJyTtndi6neSjuw7OnQ5UmJ04ysUfmksvfi9jUi/v67xdCAwCQ/3UqKufOv+/B\nbLYLOld3ReYhhCuNs9n5CByObsyMh5KbAlxGgH4d3MHTgXOe5PP55a/ZFy56njUDIl6gx44CShYi\nCCqGcnZTP+qmAN/6N9B7iP9+IVzJv/OmxwBYsF36Bpu3Dv6yyVbaU+g9RNdtOLBzC29LoUbgBIHD\nsa9IzHey6EwQZOfErqCWLOxV1LzJ+pKFpJnozKxFxNlE48E7yc9G/AOp0Gz2B3INIuGjzjTkcGQO\n8uCHUmCjTzcSOhqUF8kRi/MROBwZSGc+gp5OszZzSHI3R9fsB43AmYYcju7G8NN4Hd9km5y6A9Nn\nAktmRSdlc3ROeS1P1Bt2cspOQSpokepuyrhx49TcuXPTXQyHw+HoURDR+0op3wgDZxpyOByODMcJ\nAofD4chwnCBwOByODMcJAofD4chwnCBwOByODMcJAofD4chwnCBwOByODMcJAofD4chwetyEMiLa\nDOCTvfx7bwBbklicdOLq0j1xdemeuLoAg5RSffx29DhBsC8Q0dygmXU9DVeX7omrS/fE1aVznGnI\n4XA4MhwnCBwOhyPDyTRBcG+6C5BEXF26J64u3RNXl07IKB+Bw+FwOGLJNI3A4XA4HB6cIHA4HI4M\nJ2MEARGdQETLiGglEV2X7vIkChGtJqJFRDSfiObqbeVENJuIVuj3snSX0w8ieoCINhHRh9Y237IT\nc6dup4VENCZ9JY8loC43E9E63Tbziegka9/1ui7LiOj49JQ6FiKqIqJXiWgJES0moqv19h7XLp3U\npSe2SwERzSGiBbout+jtNUT0ni7zY0SUp7fn6+8r9f7Be3VipdTn/gUgG8B/AdQCyAOwAMAh6S5X\ngnVYDaC3Z9utAK7Tn68D8Mt0lzOg7JMBjAHwYVdlB3ASgOcBEIDxAN5Ld/njqMvNAK71+e0h+l7L\nB1Cj78HsdNdBl60/gDH6cwmA5bq8Pa5dOqlLT2wXAtBLf84F8J6+3o8DOEtvvwfAZfrz5QDu0Z/P\nAvDY3pw3UzSCIwCsVEqtUkrtBvAogOlpLlMymA7gQf35QQCnprEsgSil3gCwzbM5qOzTAfxFMe8C\nKCWi/vunpF0TUJcgpgN4VCnVqpT6GMBK8L2YdpRS65VSH+jP2wF8BGAgemC7dFKXILpzuyil1A79\nNVe/FIApAP6ut3vbRdrr7wCmEhElet5MEQQDAay1vtej8xulO6IAvERE7xPRJXpbP6XUev15A4B+\n6SnaXhFU9p7aVldqk8kDlomuR9RFmxMOA48+e3S7eOoC9MB2IaJsIpoPYBOA2WCNpUEptUf/xC5v\npC56fyOAikTPmSmC4PPAJKXUGAAnAriCiCbbOxXrhj0yFrgnl13zewB1AEYDWA/gtvQWJ36IqBeA\nJwF8RynVZO/rae3iU5ce2S5KqXal1GgAlWBNZViqz5kpgmAdgCrre6Xe1mNQSq3T75sAPAW+QTaK\neq7fN6WvhAkTVPYe11ZKqY364e0AcB+MmaFb14WIcsEd58NKqX/ozT2yXfzq0lPbRVBKNQB4FcAE\nsCkuR++yyxupi94fBrA10XNliiD4D4Ch2vOeB3aqzEpzmeKGiIqJqEQ+A/gigA/BdThf/+x8AP9M\nTwn3iqCyzwJwno5SGQ+g0TJVdEs8tvKvgNsG4LqcpSM7agAMBTBnf5fPD21Hvh/AR0qp31i7ely7\nBNWlh7ZLHyIq1Z8LARwH9nm8CuAM/TNvu0h7nQHgFa3JJUa6veT76wWOelgOtrfdkO7yJFj2WnCU\nwwIAi6X8YFvgvwCsAPAygPJ0lzWg/I+AVfM2sH3z4qCyg6MmZup2WgRgXLrLH0ddHtJlXagfzP7W\n72/QdVkG4MR0l98q1ySw2WchgPn6dVJPbJdO6tIT22UkgHm6zB8C+IneXgsWVisBPAEgX28v0N9X\n6v21e3Nel2LC4XA4MpxMMQ05HA6HIwAnCBwOhyPDcYLA4XA4MhwnCBwOhyPDcYLA4XA4MhwnCBwO\nD0TUbmWsnE9JzFZLRIPtzKUOR3cgp+ufOBwZR4viKf4OR0bgNAKHI06I14S4lXhdiDlENERvH0xE\nr+jkZv8iomq9vR8RPaVzyy8goi/oQ2UT0X063/xLegapw5E2nCBwOGIp9JiGZlj7GpVSIwD8DsDt\nettdAB5USo0E8DCAO/X2OwG8rpQaBV7DYLHePhTATKXUcAANAE5PcX0cjk5xM4sdDg9EtEMp1ctn\n+2oAU5RSq3SSsw1KqQoi2gJOX9Cmt69XSvUmos0AKpVSrdYxBgOYrZQaqr//EECuUuqnqa+Zw+GP\n0wgcjsRQAZ8TodX63A7nq3OkGScIHI7EmGG9v6M/vw3OaAsAXwfwpv78LwCXAZHFRsL7q5AORyK4\nkYjDEUuhXiFKeEEpJSGkZUS0EDyqP1tvuwrAn4jo+wA2A7hQb78awL1EdDF45H8ZOHOpw9GtcD4C\nhyNOtI9gnFJqS7rL4nAkE2cacjgcjgzHaQQOh8OR4TiNwOFwODIcJwgcDocjw3GCwOFwODIcJwgc\nDocjw3GCwOFwODKc/wfc8u4Q41QVmwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "# Evaluate on test data\n",
            "40/40 [==============================] - 0s 2ms/sample - loss: 0.8327 - acc: 0.6000\n",
            "test loss, test acc: [0.8327391588361934, 0.6]\n",
            "1\n",
            "EEG_Deep/Data2A/parsed_P01T.mat\n",
            "(0, 12, 4096)\n",
            "(80, 12, 4096)\n",
            "(80, 1)\n",
            "2\n",
            "EEG_Deep/Data2A/parsed_P02T.mat\n",
            "(80, 12, 4096)\n",
            "(160, 12, 4096)\n",
            "(160, 1)\n",
            "3\n",
            "EEG_Deep/Data2A/parsed_P03T.mat\n",
            "(160, 12, 4096)\n",
            "(240, 12, 4096)\n",
            "(240, 1)\n",
            "4\n",
            "EEG_Deep/Data2A/parsed_P04T.mat\n",
            "(240, 12, 4096)\n",
            "(320, 12, 4096)\n",
            "(320, 1)\n",
            "6\n",
            "EEG_Deep/Data2A/parsed_P06T.mat\n",
            "(320, 12, 4096)\n",
            "(400, 12, 4096)\n",
            "(400, 1)\n",
            "7\n",
            "EEG_Deep/Data2A/parsed_P07T.mat\n",
            "(400, 12, 4096)\n",
            "(480, 12, 4096)\n",
            "(480, 1)\n",
            "8\n",
            "EEG_Deep/Data2A/parsed_P08T.mat\n",
            "(480, 12, 4096)\n",
            "(560, 12, 4096)\n",
            "(560, 1)\n",
            "9\n",
            "EEG_Deep/Data2A/parsed_P09T.mat\n",
            "(560, 12, 4096)\n",
            "(640, 12, 4096)\n",
            "(640, 1)\n",
            "10\n",
            "EEG_Deep/Data2A/parsed_P010T.mat\n",
            "(640, 12, 4096)\n",
            "(720, 12, 4096)\n",
            "(720, 1)\n",
            "5\n",
            "EEG_Deep/Data2A/parsed_P05E.mat\n",
            "(0, 12, 4096)\n",
            "(40, 12, 4096)\n",
            "(40, 1)\n",
            "Filtering of training data in progress\n",
            "(720, 12, 4096)\n",
            "Filtering of test data in progress\n",
            "(40, 12, 4096)\n",
            "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
            " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1.]\n",
            "(620, 12, 1536)\n",
            "(620,)\n",
            "(100, 12, 1536)\n",
            "(100,)\n",
            "[[1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]]\n",
            "X_train shape: (620, 1, 12, 1536)\n",
            "620 train samples\n",
            "100 val samples\n",
            "(40, 12, 1536)\n",
            "(40, 1)\n",
            "X_train shape: (40, 1, 12, 1536)\n",
            "40 train samples\n",
            "Train on 620 samples, validate on 100 samples\n",
            "Epoch 1/300\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 1.13981, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.9582 - acc: 0.5048 - val_loss: 1.1398 - val_acc: 0.4400\n",
            "Epoch 2/300\n",
            "\n",
            "Epoch 00002: val_loss improved from 1.13981 to 1.01324, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.7692 - acc: 0.5581 - val_loss: 1.0132 - val_acc: 0.5100\n",
            "Epoch 3/300\n",
            "\n",
            "Epoch 00003: val_loss improved from 1.01324 to 0.92515, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.7211 - acc: 0.6419 - val_loss: 0.9252 - val_acc: 0.5000\n",
            "Epoch 4/300\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.92515 to 0.86904, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.6826 - acc: 0.6581 - val_loss: 0.8690 - val_acc: 0.5000\n",
            "Epoch 5/300\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.86904\n",
            "620/620 - 1s - loss: 0.6080 - acc: 0.7226 - val_loss: 1.0035 - val_acc: 0.5000\n",
            "Epoch 6/300\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.86904\n",
            "620/620 - 1s - loss: 0.5687 - acc: 0.7194 - val_loss: 1.0379 - val_acc: 0.5000\n",
            "Epoch 7/300\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.86904\n",
            "620/620 - 1s - loss: 0.5408 - acc: 0.7452 - val_loss: 1.0246 - val_acc: 0.5000\n",
            "Epoch 8/300\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.86904\n",
            "620/620 - 1s - loss: 0.5501 - acc: 0.7613 - val_loss: 1.0730 - val_acc: 0.5000\n",
            "Epoch 9/300\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.86904\n",
            "620/620 - 1s - loss: 0.5560 - acc: 0.7113 - val_loss: 0.8801 - val_acc: 0.5200\n",
            "Epoch 10/300\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.86904 to 0.84593, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.5221 - acc: 0.7516 - val_loss: 0.8459 - val_acc: 0.5200\n",
            "Epoch 11/300\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.84593\n",
            "620/620 - 1s - loss: 0.5189 - acc: 0.7419 - val_loss: 0.8947 - val_acc: 0.5200\n",
            "Epoch 12/300\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.84593\n",
            "620/620 - 1s - loss: 0.4919 - acc: 0.7661 - val_loss: 0.9134 - val_acc: 0.5400\n",
            "Epoch 13/300\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.84593\n",
            "620/620 - 1s - loss: 0.4954 - acc: 0.7710 - val_loss: 0.9722 - val_acc: 0.5300\n",
            "Epoch 14/300\n",
            "\n",
            "Epoch 00014: val_loss improved from 0.84593 to 0.79456, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.5095 - acc: 0.7613 - val_loss: 0.7946 - val_acc: 0.5600\n",
            "Epoch 15/300\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.79456\n",
            "620/620 - 1s - loss: 0.4784 - acc: 0.7581 - val_loss: 1.0950 - val_acc: 0.5100\n",
            "Epoch 16/300\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.79456\n",
            "620/620 - 1s - loss: 0.5017 - acc: 0.7484 - val_loss: 0.8727 - val_acc: 0.5200\n",
            "Epoch 17/300\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.79456\n",
            "620/620 - 1s - loss: 0.4850 - acc: 0.7613 - val_loss: 0.8109 - val_acc: 0.5400\n",
            "Epoch 18/300\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.79456\n",
            "620/620 - 1s - loss: 0.4746 - acc: 0.7694 - val_loss: 0.9241 - val_acc: 0.5200\n",
            "Epoch 19/300\n",
            "\n",
            "Epoch 00019: val_loss improved from 0.79456 to 0.75690, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.4782 - acc: 0.7935 - val_loss: 0.7569 - val_acc: 0.5600\n",
            "Epoch 20/300\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.75690\n",
            "620/620 - 1s - loss: 0.5085 - acc: 0.7613 - val_loss: 1.1318 - val_acc: 0.5400\n",
            "Epoch 21/300\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.75690\n",
            "620/620 - 1s - loss: 0.4662 - acc: 0.7565 - val_loss: 0.9810 - val_acc: 0.5400\n",
            "Epoch 22/300\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.75690\n",
            "620/620 - 1s - loss: 0.4737 - acc: 0.7887 - val_loss: 1.0945 - val_acc: 0.5300\n",
            "Epoch 23/300\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.75690\n",
            "620/620 - 1s - loss: 0.4929 - acc: 0.7597 - val_loss: 0.7993 - val_acc: 0.5400\n",
            "Epoch 24/300\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.75690\n",
            "620/620 - 1s - loss: 0.5182 - acc: 0.7500 - val_loss: 1.1889 - val_acc: 0.5100\n",
            "Epoch 25/300\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.75690\n",
            "620/620 - 1s - loss: 0.4658 - acc: 0.7919 - val_loss: 0.9673 - val_acc: 0.5500\n",
            "Epoch 26/300\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.75690\n",
            "620/620 - 1s - loss: 0.4740 - acc: 0.7677 - val_loss: 0.9538 - val_acc: 0.5500\n",
            "Epoch 27/300\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.75690\n",
            "620/620 - 1s - loss: 0.4523 - acc: 0.7952 - val_loss: 1.2135 - val_acc: 0.5300\n",
            "Epoch 28/300\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.75690\n",
            "620/620 - 1s - loss: 0.4568 - acc: 0.7710 - val_loss: 1.0581 - val_acc: 0.5300\n",
            "Epoch 29/300\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.75690\n",
            "620/620 - 1s - loss: 0.4578 - acc: 0.7952 - val_loss: 1.2120 - val_acc: 0.5100\n",
            "Epoch 30/300\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.75690\n",
            "620/620 - 1s - loss: 0.4533 - acc: 0.7871 - val_loss: 1.0853 - val_acc: 0.5100\n",
            "Epoch 31/300\n",
            "\n",
            "Epoch 00031: val_loss improved from 0.75690 to 0.75540, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.4566 - acc: 0.7952 - val_loss: 0.7554 - val_acc: 0.5500\n",
            "Epoch 32/300\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.75540\n",
            "620/620 - 1s - loss: 0.4703 - acc: 0.7903 - val_loss: 1.0462 - val_acc: 0.5500\n",
            "Epoch 33/300\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.75540\n",
            "620/620 - 1s - loss: 0.4653 - acc: 0.7742 - val_loss: 1.3294 - val_acc: 0.5100\n",
            "Epoch 34/300\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.75540\n",
            "620/620 - 1s - loss: 0.4345 - acc: 0.8194 - val_loss: 1.1341 - val_acc: 0.5200\n",
            "Epoch 35/300\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.75540\n",
            "620/620 - 1s - loss: 0.4628 - acc: 0.7839 - val_loss: 0.8248 - val_acc: 0.5500\n",
            "Epoch 36/300\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.75540\n",
            "620/620 - 1s - loss: 0.4813 - acc: 0.7484 - val_loss: 0.9187 - val_acc: 0.5100\n",
            "Epoch 37/300\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.75540\n",
            "620/620 - 1s - loss: 0.4526 - acc: 0.8000 - val_loss: 0.8728 - val_acc: 0.5200\n",
            "Epoch 38/300\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.75540\n",
            "620/620 - 1s - loss: 0.4525 - acc: 0.7952 - val_loss: 1.0047 - val_acc: 0.5200\n",
            "Epoch 39/300\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.75540\n",
            "620/620 - 1s - loss: 0.4512 - acc: 0.7952 - val_loss: 0.7576 - val_acc: 0.5600\n",
            "Epoch 40/300\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.75540\n",
            "620/620 - 1s - loss: 0.4429 - acc: 0.7935 - val_loss: 1.1800 - val_acc: 0.5200\n",
            "Epoch 41/300\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.75540\n",
            "620/620 - 1s - loss: 0.4375 - acc: 0.7855 - val_loss: 1.1806 - val_acc: 0.5200\n",
            "Epoch 42/300\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.75540\n",
            "620/620 - 1s - loss: 0.4625 - acc: 0.7726 - val_loss: 1.0970 - val_acc: 0.5400\n",
            "Epoch 43/300\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.75540\n",
            "620/620 - 1s - loss: 0.4393 - acc: 0.8113 - val_loss: 0.8733 - val_acc: 0.5100\n",
            "Epoch 44/300\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.75540\n",
            "620/620 - 1s - loss: 0.4312 - acc: 0.8161 - val_loss: 0.9216 - val_acc: 0.5300\n",
            "Epoch 45/300\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.75540\n",
            "620/620 - 1s - loss: 0.4576 - acc: 0.7677 - val_loss: 0.9034 - val_acc: 0.5500\n",
            "Epoch 46/300\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.75540\n",
            "620/620 - 1s - loss: 0.4300 - acc: 0.8048 - val_loss: 0.9713 - val_acc: 0.5300\n",
            "Epoch 47/300\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.75540\n",
            "620/620 - 1s - loss: 0.4554 - acc: 0.7710 - val_loss: 1.1984 - val_acc: 0.5200\n",
            "Epoch 48/300\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.75540\n",
            "620/620 - 1s - loss: 0.4381 - acc: 0.7855 - val_loss: 1.0848 - val_acc: 0.5200\n",
            "Epoch 49/300\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.75540\n",
            "620/620 - 1s - loss: 0.4173 - acc: 0.8113 - val_loss: 1.2260 - val_acc: 0.5200\n",
            "Epoch 50/300\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.75540\n",
            "620/620 - 1s - loss: 0.4548 - acc: 0.7774 - val_loss: 1.0502 - val_acc: 0.5100\n",
            "Epoch 51/300\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.75540\n",
            "620/620 - 1s - loss: 0.4390 - acc: 0.7984 - val_loss: 1.0355 - val_acc: 0.5100\n",
            "Epoch 52/300\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.75540\n",
            "620/620 - 1s - loss: 0.4450 - acc: 0.7855 - val_loss: 1.0224 - val_acc: 0.5300\n",
            "Epoch 53/300\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.75540\n",
            "620/620 - 1s - loss: 0.4382 - acc: 0.8016 - val_loss: 0.8758 - val_acc: 0.5300\n",
            "Epoch 54/300\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.75540\n",
            "620/620 - 1s - loss: 0.4655 - acc: 0.7758 - val_loss: 0.8258 - val_acc: 0.5400\n",
            "Epoch 55/300\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.75540\n",
            "620/620 - 1s - loss: 0.4537 - acc: 0.7903 - val_loss: 0.7663 - val_acc: 0.5600\n",
            "Epoch 56/300\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.75540\n",
            "620/620 - 1s - loss: 0.4488 - acc: 0.7919 - val_loss: 1.1630 - val_acc: 0.5000\n",
            "Epoch 57/300\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.75540\n",
            "620/620 - 1s - loss: 0.4508 - acc: 0.7919 - val_loss: 1.0785 - val_acc: 0.5100\n",
            "Epoch 58/300\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.75540\n",
            "620/620 - 1s - loss: 0.4460 - acc: 0.8097 - val_loss: 0.7759 - val_acc: 0.5400\n",
            "Epoch 59/300\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.75540\n",
            "620/620 - 1s - loss: 0.4269 - acc: 0.8000 - val_loss: 1.0018 - val_acc: 0.5200\n",
            "Epoch 60/300\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.75540\n",
            "620/620 - 1s - loss: 0.4452 - acc: 0.7758 - val_loss: 1.2352 - val_acc: 0.5100\n",
            "Epoch 61/300\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.75540\n",
            "620/620 - 1s - loss: 0.4387 - acc: 0.8081 - val_loss: 0.9832 - val_acc: 0.5300\n",
            "Epoch 62/300\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.75540\n",
            "620/620 - 1s - loss: 0.4468 - acc: 0.7984 - val_loss: 1.0810 - val_acc: 0.5200\n",
            "Epoch 63/300\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.75540\n",
            "620/620 - 1s - loss: 0.4472 - acc: 0.7984 - val_loss: 1.2006 - val_acc: 0.5100\n",
            "Epoch 64/300\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.75540\n",
            "620/620 - 1s - loss: 0.4281 - acc: 0.7903 - val_loss: 0.9589 - val_acc: 0.5200\n",
            "Epoch 65/300\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.75540\n",
            "620/620 - 1s - loss: 0.4252 - acc: 0.7935 - val_loss: 0.8587 - val_acc: 0.5400\n",
            "Epoch 66/300\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.75540\n",
            "620/620 - 1s - loss: 0.4465 - acc: 0.7968 - val_loss: 0.8205 - val_acc: 0.5200\n",
            "Epoch 67/300\n",
            "\n",
            "Epoch 00067: val_loss improved from 0.75540 to 0.73435, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.4235 - acc: 0.7952 - val_loss: 0.7343 - val_acc: 0.5700\n",
            "Epoch 68/300\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.73435\n",
            "620/620 - 1s - loss: 0.4097 - acc: 0.8194 - val_loss: 1.0160 - val_acc: 0.5300\n",
            "Epoch 69/300\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.73435\n",
            "620/620 - 1s - loss: 0.4350 - acc: 0.8177 - val_loss: 0.9309 - val_acc: 0.5600\n",
            "Epoch 70/300\n",
            "\n",
            "Epoch 00070: val_loss improved from 0.73435 to 0.62008, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.4182 - acc: 0.8081 - val_loss: 0.6201 - val_acc: 0.6600\n",
            "Epoch 71/300\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.4447 - acc: 0.7855 - val_loss: 0.8102 - val_acc: 0.5100\n",
            "Epoch 72/300\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.4391 - acc: 0.7871 - val_loss: 1.0078 - val_acc: 0.5300\n",
            "Epoch 73/300\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.4359 - acc: 0.7919 - val_loss: 1.0694 - val_acc: 0.5400\n",
            "Epoch 74/300\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.4395 - acc: 0.7823 - val_loss: 1.2050 - val_acc: 0.5100\n",
            "Epoch 75/300\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.4132 - acc: 0.8194 - val_loss: 1.0071 - val_acc: 0.5100\n",
            "Epoch 76/300\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.4182 - acc: 0.7952 - val_loss: 1.0777 - val_acc: 0.5700\n",
            "Epoch 77/300\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.4212 - acc: 0.7984 - val_loss: 1.2190 - val_acc: 0.5100\n",
            "Epoch 78/300\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.4258 - acc: 0.7968 - val_loss: 1.1027 - val_acc: 0.5200\n",
            "Epoch 79/300\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.4200 - acc: 0.8145 - val_loss: 1.4003 - val_acc: 0.5200\n",
            "Epoch 80/300\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.4272 - acc: 0.7871 - val_loss: 1.3961 - val_acc: 0.5100\n",
            "Epoch 81/300\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.4251 - acc: 0.8210 - val_loss: 1.4408 - val_acc: 0.5100\n",
            "Epoch 82/300\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.4316 - acc: 0.8081 - val_loss: 1.2968 - val_acc: 0.5100\n",
            "Epoch 83/300\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.4169 - acc: 0.8242 - val_loss: 0.8874 - val_acc: 0.5100\n",
            "Epoch 84/300\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3818 - acc: 0.8274 - val_loss: 0.9363 - val_acc: 0.5000\n",
            "Epoch 85/300\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3952 - acc: 0.8081 - val_loss: 1.0880 - val_acc: 0.5100\n",
            "Epoch 86/300\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.4285 - acc: 0.7984 - val_loss: 1.0197 - val_acc: 0.5300\n",
            "Epoch 87/300\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.4052 - acc: 0.8000 - val_loss: 1.1927 - val_acc: 0.5100\n",
            "Epoch 88/300\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.4000 - acc: 0.8161 - val_loss: 1.2463 - val_acc: 0.5200\n",
            "Epoch 89/300\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3940 - acc: 0.8306 - val_loss: 0.6946 - val_acc: 0.5700\n",
            "Epoch 90/300\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.4426 - acc: 0.7984 - val_loss: 1.0413 - val_acc: 0.5300\n",
            "Epoch 91/300\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.4179 - acc: 0.8016 - val_loss: 0.9197 - val_acc: 0.5200\n",
            "Epoch 92/300\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.4245 - acc: 0.8113 - val_loss: 0.6751 - val_acc: 0.5800\n",
            "Epoch 93/300\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.4081 - acc: 0.8194 - val_loss: 0.8180 - val_acc: 0.5500\n",
            "Epoch 94/300\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.4284 - acc: 0.7919 - val_loss: 1.1862 - val_acc: 0.5200\n",
            "Epoch 95/300\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.4120 - acc: 0.8129 - val_loss: 0.8792 - val_acc: 0.5400\n",
            "Epoch 96/300\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.4183 - acc: 0.7903 - val_loss: 0.9796 - val_acc: 0.5100\n",
            "Epoch 97/300\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3961 - acc: 0.8065 - val_loss: 0.8064 - val_acc: 0.5200\n",
            "Epoch 98/300\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.4107 - acc: 0.8129 - val_loss: 0.9635 - val_acc: 0.5000\n",
            "Epoch 99/300\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.4101 - acc: 0.8032 - val_loss: 1.3796 - val_acc: 0.5100\n",
            "Epoch 100/300\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.4197 - acc: 0.7984 - val_loss: 1.5219 - val_acc: 0.5100\n",
            "Epoch 101/300\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.4355 - acc: 0.8000 - val_loss: 1.0535 - val_acc: 0.5300\n",
            "Epoch 102/300\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3799 - acc: 0.8306 - val_loss: 0.7538 - val_acc: 0.5400\n",
            "Epoch 103/300\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3973 - acc: 0.8145 - val_loss: 0.9221 - val_acc: 0.5000\n",
            "Epoch 104/300\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3862 - acc: 0.8242 - val_loss: 1.2542 - val_acc: 0.5200\n",
            "Epoch 105/300\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.4063 - acc: 0.8226 - val_loss: 1.2278 - val_acc: 0.5200\n",
            "Epoch 106/300\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3774 - acc: 0.8339 - val_loss: 0.9989 - val_acc: 0.5200\n",
            "Epoch 107/300\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3819 - acc: 0.8242 - val_loss: 0.9467 - val_acc: 0.5400\n",
            "Epoch 108/300\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.4201 - acc: 0.8016 - val_loss: 1.0269 - val_acc: 0.5500\n",
            "Epoch 109/300\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.4036 - acc: 0.8097 - val_loss: 0.8873 - val_acc: 0.5300\n",
            "Epoch 110/300\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.4132 - acc: 0.8032 - val_loss: 1.2557 - val_acc: 0.5100\n",
            "Epoch 111/300\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.4028 - acc: 0.8161 - val_loss: 1.1086 - val_acc: 0.5200\n",
            "Epoch 112/300\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3870 - acc: 0.8194 - val_loss: 0.8459 - val_acc: 0.5600\n",
            "Epoch 113/300\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3985 - acc: 0.8226 - val_loss: 0.8335 - val_acc: 0.5300\n",
            "Epoch 114/300\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3948 - acc: 0.8177 - val_loss: 0.7831 - val_acc: 0.5700\n",
            "Epoch 115/300\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.4378 - acc: 0.8081 - val_loss: 0.9234 - val_acc: 0.5300\n",
            "Epoch 116/300\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.4383 - acc: 0.7839 - val_loss: 1.3084 - val_acc: 0.5200\n",
            "Epoch 117/300\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.4009 - acc: 0.8016 - val_loss: 1.0114 - val_acc: 0.5200\n",
            "Epoch 118/300\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3776 - acc: 0.8306 - val_loss: 1.0957 - val_acc: 0.5500\n",
            "Epoch 119/300\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3858 - acc: 0.8323 - val_loss: 0.9262 - val_acc: 0.5200\n",
            "Epoch 120/300\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3846 - acc: 0.8468 - val_loss: 1.1088 - val_acc: 0.5000\n",
            "Epoch 121/300\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3643 - acc: 0.8371 - val_loss: 1.1421 - val_acc: 0.5200\n",
            "Epoch 122/300\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3950 - acc: 0.8290 - val_loss: 1.0443 - val_acc: 0.5100\n",
            "Epoch 123/300\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3678 - acc: 0.8387 - val_loss: 1.0950 - val_acc: 0.5100\n",
            "Epoch 124/300\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.4071 - acc: 0.8113 - val_loss: 0.9756 - val_acc: 0.5100\n",
            "Epoch 125/300\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.4141 - acc: 0.8177 - val_loss: 0.8844 - val_acc: 0.5500\n",
            "Epoch 126/300\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.4414 - acc: 0.7919 - val_loss: 1.1904 - val_acc: 0.5000\n",
            "Epoch 127/300\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3773 - acc: 0.8371 - val_loss: 1.2203 - val_acc: 0.5100\n",
            "Epoch 128/300\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3725 - acc: 0.8355 - val_loss: 0.9552 - val_acc: 0.5100\n",
            "Epoch 129/300\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3811 - acc: 0.8194 - val_loss: 0.9132 - val_acc: 0.5200\n",
            "Epoch 130/300\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3627 - acc: 0.8306 - val_loss: 1.1349 - val_acc: 0.5100\n",
            "Epoch 131/300\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3773 - acc: 0.8371 - val_loss: 1.3381 - val_acc: 0.5200\n",
            "Epoch 132/300\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3929 - acc: 0.8032 - val_loss: 0.9576 - val_acc: 0.4800\n",
            "Epoch 133/300\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3828 - acc: 0.8323 - val_loss: 1.0575 - val_acc: 0.4900\n",
            "Epoch 134/300\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.4091 - acc: 0.8016 - val_loss: 0.9935 - val_acc: 0.5300\n",
            "Epoch 135/300\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3595 - acc: 0.8548 - val_loss: 0.9973 - val_acc: 0.5200\n",
            "Epoch 136/300\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3944 - acc: 0.8032 - val_loss: 0.9617 - val_acc: 0.5200\n",
            "Epoch 137/300\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3898 - acc: 0.8226 - val_loss: 0.7760 - val_acc: 0.5400\n",
            "Epoch 138/300\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3470 - acc: 0.8597 - val_loss: 0.9207 - val_acc: 0.5200\n",
            "Epoch 139/300\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.4167 - acc: 0.7984 - val_loss: 0.8619 - val_acc: 0.5200\n",
            "Epoch 140/300\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.4137 - acc: 0.7903 - val_loss: 1.2575 - val_acc: 0.5200\n",
            "Epoch 141/300\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3946 - acc: 0.8177 - val_loss: 0.9886 - val_acc: 0.5500\n",
            "Epoch 142/300\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3733 - acc: 0.8387 - val_loss: 0.7502 - val_acc: 0.5700\n",
            "Epoch 143/300\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3695 - acc: 0.8597 - val_loss: 1.0680 - val_acc: 0.5100\n",
            "Epoch 144/300\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.4240 - acc: 0.7887 - val_loss: 1.1544 - val_acc: 0.5100\n",
            "Epoch 145/300\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.4069 - acc: 0.8145 - val_loss: 1.2201 - val_acc: 0.5200\n",
            "Epoch 146/300\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3804 - acc: 0.8161 - val_loss: 0.9680 - val_acc: 0.5100\n",
            "Epoch 147/300\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3705 - acc: 0.8387 - val_loss: 1.1628 - val_acc: 0.5100\n",
            "Epoch 148/300\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.4216 - acc: 0.8065 - val_loss: 0.8227 - val_acc: 0.5500\n",
            "Epoch 149/300\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3811 - acc: 0.8290 - val_loss: 0.8134 - val_acc: 0.5400\n",
            "Epoch 150/300\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3807 - acc: 0.8355 - val_loss: 0.9695 - val_acc: 0.5100\n",
            "Epoch 151/300\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3772 - acc: 0.8323 - val_loss: 0.8651 - val_acc: 0.5300\n",
            "Epoch 152/300\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3832 - acc: 0.8145 - val_loss: 0.7957 - val_acc: 0.6000\n",
            "Epoch 153/300\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.4032 - acc: 0.8194 - val_loss: 0.8312 - val_acc: 0.5500\n",
            "Epoch 154/300\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.4062 - acc: 0.8161 - val_loss: 0.9738 - val_acc: 0.5100\n",
            "Epoch 155/300\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.4085 - acc: 0.8161 - val_loss: 1.0863 - val_acc: 0.5000\n",
            "Epoch 156/300\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3985 - acc: 0.8065 - val_loss: 0.8613 - val_acc: 0.5100\n",
            "Epoch 157/300\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3896 - acc: 0.8177 - val_loss: 1.3868 - val_acc: 0.5100\n",
            "Epoch 158/300\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.4108 - acc: 0.8161 - val_loss: 1.0888 - val_acc: 0.4800\n",
            "Epoch 159/300\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3950 - acc: 0.8113 - val_loss: 1.1103 - val_acc: 0.5000\n",
            "Epoch 160/300\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3897 - acc: 0.8387 - val_loss: 0.7595 - val_acc: 0.5800\n",
            "Epoch 161/300\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3835 - acc: 0.8290 - val_loss: 1.3052 - val_acc: 0.5100\n",
            "Epoch 162/300\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3778 - acc: 0.8145 - val_loss: 1.2648 - val_acc: 0.5100\n",
            "Epoch 163/300\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3668 - acc: 0.8468 - val_loss: 1.0003 - val_acc: 0.5000\n",
            "Epoch 164/300\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3881 - acc: 0.8258 - val_loss: 0.9274 - val_acc: 0.5100\n",
            "Epoch 165/300\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3881 - acc: 0.8194 - val_loss: 1.3736 - val_acc: 0.5200\n",
            "Epoch 166/300\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3753 - acc: 0.8323 - val_loss: 0.8905 - val_acc: 0.5000\n",
            "Epoch 167/300\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3774 - acc: 0.8210 - val_loss: 1.1433 - val_acc: 0.4900\n",
            "Epoch 168/300\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3784 - acc: 0.8290 - val_loss: 1.1572 - val_acc: 0.5100\n",
            "Epoch 169/300\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.4154 - acc: 0.8065 - val_loss: 1.0646 - val_acc: 0.5000\n",
            "Epoch 170/300\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.4005 - acc: 0.8290 - val_loss: 1.0899 - val_acc: 0.4500\n",
            "Epoch 171/300\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3808 - acc: 0.8355 - val_loss: 1.1575 - val_acc: 0.4800\n",
            "Epoch 172/300\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.4059 - acc: 0.8274 - val_loss: 1.1104 - val_acc: 0.5100\n",
            "Epoch 173/300\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3560 - acc: 0.8274 - val_loss: 1.2779 - val_acc: 0.5200\n",
            "Epoch 174/300\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.4158 - acc: 0.8032 - val_loss: 1.0465 - val_acc: 0.4900\n",
            "Epoch 175/300\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3608 - acc: 0.8387 - val_loss: 1.2615 - val_acc: 0.5100\n",
            "Epoch 176/300\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.4104 - acc: 0.8016 - val_loss: 0.8549 - val_acc: 0.5400\n",
            "Epoch 177/300\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3921 - acc: 0.8306 - val_loss: 1.1619 - val_acc: 0.5100\n",
            "Epoch 178/300\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3716 - acc: 0.8323 - val_loss: 1.2555 - val_acc: 0.5100\n",
            "Epoch 179/300\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3692 - acc: 0.8468 - val_loss: 1.0664 - val_acc: 0.4900\n",
            "Epoch 180/300\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3954 - acc: 0.8113 - val_loss: 0.9664 - val_acc: 0.5400\n",
            "Epoch 181/300\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3416 - acc: 0.8597 - val_loss: 1.0057 - val_acc: 0.5000\n",
            "Epoch 182/300\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3713 - acc: 0.8226 - val_loss: 1.0246 - val_acc: 0.4600\n",
            "Epoch 183/300\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3508 - acc: 0.8468 - val_loss: 1.3678 - val_acc: 0.5000\n",
            "Epoch 184/300\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3748 - acc: 0.8306 - val_loss: 1.3825 - val_acc: 0.5100\n",
            "Epoch 185/300\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3745 - acc: 0.8274 - val_loss: 1.1859 - val_acc: 0.5100\n",
            "Epoch 186/300\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3610 - acc: 0.8484 - val_loss: 1.2202 - val_acc: 0.5200\n",
            "Epoch 187/300\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3689 - acc: 0.8435 - val_loss: 0.7301 - val_acc: 0.7100\n",
            "Epoch 188/300\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.4030 - acc: 0.7968 - val_loss: 1.0900 - val_acc: 0.4900\n",
            "Epoch 189/300\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3681 - acc: 0.8435 - val_loss: 1.0329 - val_acc: 0.5200\n",
            "Epoch 190/300\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3738 - acc: 0.8226 - val_loss: 0.7984 - val_acc: 0.5700\n",
            "Epoch 191/300\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3894 - acc: 0.8210 - val_loss: 1.0049 - val_acc: 0.5200\n",
            "Epoch 192/300\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3740 - acc: 0.8274 - val_loss: 0.8669 - val_acc: 0.5200\n",
            "Epoch 193/300\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3828 - acc: 0.8323 - val_loss: 0.9592 - val_acc: 0.5000\n",
            "Epoch 194/300\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3784 - acc: 0.8339 - val_loss: 1.2807 - val_acc: 0.5100\n",
            "Epoch 195/300\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3857 - acc: 0.8258 - val_loss: 1.1693 - val_acc: 0.5100\n",
            "Epoch 196/300\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3877 - acc: 0.8113 - val_loss: 1.1347 - val_acc: 0.5100\n",
            "Epoch 197/300\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3655 - acc: 0.8371 - val_loss: 1.1337 - val_acc: 0.4900\n",
            "Epoch 198/300\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3845 - acc: 0.8355 - val_loss: 0.9528 - val_acc: 0.5400\n",
            "Epoch 199/300\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3441 - acc: 0.8452 - val_loss: 1.2492 - val_acc: 0.5000\n",
            "Epoch 200/300\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3685 - acc: 0.8419 - val_loss: 1.0770 - val_acc: 0.4900\n",
            "Epoch 201/300\n",
            "\n",
            "Epoch 00201: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3638 - acc: 0.8371 - val_loss: 1.0440 - val_acc: 0.5000\n",
            "Epoch 202/300\n",
            "\n",
            "Epoch 00202: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3634 - acc: 0.8274 - val_loss: 1.3387 - val_acc: 0.5100\n",
            "Epoch 203/300\n",
            "\n",
            "Epoch 00203: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3990 - acc: 0.8113 - val_loss: 1.1323 - val_acc: 0.5100\n",
            "Epoch 204/300\n",
            "\n",
            "Epoch 00204: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.4039 - acc: 0.7935 - val_loss: 1.0003 - val_acc: 0.4900\n",
            "Epoch 205/300\n",
            "\n",
            "Epoch 00205: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3782 - acc: 0.8226 - val_loss: 1.3257 - val_acc: 0.5100\n",
            "Epoch 206/300\n",
            "\n",
            "Epoch 00206: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3509 - acc: 0.8468 - val_loss: 1.3645 - val_acc: 0.4900\n",
            "Epoch 207/300\n",
            "\n",
            "Epoch 00207: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3591 - acc: 0.8387 - val_loss: 1.2612 - val_acc: 0.5000\n",
            "Epoch 208/300\n",
            "\n",
            "Epoch 00208: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3669 - acc: 0.8194 - val_loss: 1.6167 - val_acc: 0.5100\n",
            "Epoch 209/300\n",
            "\n",
            "Epoch 00209: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3719 - acc: 0.8274 - val_loss: 1.3335 - val_acc: 0.5100\n",
            "Epoch 210/300\n",
            "\n",
            "Epoch 00210: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3735 - acc: 0.8258 - val_loss: 1.0083 - val_acc: 0.5100\n",
            "Epoch 211/300\n",
            "\n",
            "Epoch 00211: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3576 - acc: 0.8371 - val_loss: 1.0548 - val_acc: 0.4800\n",
            "Epoch 212/300\n",
            "\n",
            "Epoch 00212: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3623 - acc: 0.8435 - val_loss: 0.9937 - val_acc: 0.5100\n",
            "Epoch 213/300\n",
            "\n",
            "Epoch 00213: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3540 - acc: 0.8371 - val_loss: 1.1632 - val_acc: 0.5200\n",
            "Epoch 214/300\n",
            "\n",
            "Epoch 00214: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3644 - acc: 0.8371 - val_loss: 0.9056 - val_acc: 0.5300\n",
            "Epoch 215/300\n",
            "\n",
            "Epoch 00215: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3468 - acc: 0.8484 - val_loss: 1.1723 - val_acc: 0.5100\n",
            "Epoch 216/300\n",
            "\n",
            "Epoch 00216: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3113 - acc: 0.8661 - val_loss: 1.0691 - val_acc: 0.5000\n",
            "Epoch 217/300\n",
            "\n",
            "Epoch 00217: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3420 - acc: 0.8484 - val_loss: 1.1765 - val_acc: 0.5200\n",
            "Epoch 218/300\n",
            "\n",
            "Epoch 00218: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3581 - acc: 0.8500 - val_loss: 1.2655 - val_acc: 0.5100\n",
            "Epoch 219/300\n",
            "\n",
            "Epoch 00219: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3485 - acc: 0.8403 - val_loss: 1.1747 - val_acc: 0.5300\n",
            "Epoch 220/300\n",
            "\n",
            "Epoch 00220: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3514 - acc: 0.8403 - val_loss: 1.0849 - val_acc: 0.5100\n",
            "Epoch 221/300\n",
            "\n",
            "Epoch 00221: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3617 - acc: 0.8500 - val_loss: 0.7840 - val_acc: 0.5800\n",
            "Epoch 222/300\n",
            "\n",
            "Epoch 00222: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3632 - acc: 0.8339 - val_loss: 1.1707 - val_acc: 0.5100\n",
            "Epoch 223/300\n",
            "\n",
            "Epoch 00223: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3673 - acc: 0.8323 - val_loss: 0.9324 - val_acc: 0.5300\n",
            "Epoch 224/300\n",
            "\n",
            "Epoch 00224: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3568 - acc: 0.8516 - val_loss: 0.8065 - val_acc: 0.5500\n",
            "Epoch 225/300\n",
            "\n",
            "Epoch 00225: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.4119 - acc: 0.7871 - val_loss: 0.7978 - val_acc: 0.5800\n",
            "Epoch 226/300\n",
            "\n",
            "Epoch 00226: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3644 - acc: 0.8403 - val_loss: 1.1568 - val_acc: 0.5400\n",
            "Epoch 227/300\n",
            "\n",
            "Epoch 00227: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3547 - acc: 0.8516 - val_loss: 1.0498 - val_acc: 0.5400\n",
            "Epoch 228/300\n",
            "\n",
            "Epoch 00228: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3800 - acc: 0.8210 - val_loss: 0.9184 - val_acc: 0.5100\n",
            "Epoch 229/300\n",
            "\n",
            "Epoch 00229: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3692 - acc: 0.8306 - val_loss: 1.0669 - val_acc: 0.5400\n",
            "Epoch 230/300\n",
            "\n",
            "Epoch 00230: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3714 - acc: 0.8532 - val_loss: 0.8699 - val_acc: 0.5300\n",
            "Epoch 231/300\n",
            "\n",
            "Epoch 00231: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3675 - acc: 0.8435 - val_loss: 1.1637 - val_acc: 0.5100\n",
            "Epoch 232/300\n",
            "\n",
            "Epoch 00232: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3377 - acc: 0.8516 - val_loss: 1.0398 - val_acc: 0.5100\n",
            "Epoch 233/300\n",
            "\n",
            "Epoch 00233: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3526 - acc: 0.8677 - val_loss: 1.0871 - val_acc: 0.5100\n",
            "Epoch 234/300\n",
            "\n",
            "Epoch 00234: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3434 - acc: 0.8468 - val_loss: 1.0114 - val_acc: 0.4800\n",
            "Epoch 235/300\n",
            "\n",
            "Epoch 00235: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3650 - acc: 0.8435 - val_loss: 0.9929 - val_acc: 0.4900\n",
            "Epoch 236/300\n",
            "\n",
            "Epoch 00236: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3612 - acc: 0.8306 - val_loss: 0.9290 - val_acc: 0.5500\n",
            "Epoch 237/300\n",
            "\n",
            "Epoch 00237: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3771 - acc: 0.8145 - val_loss: 0.8337 - val_acc: 0.6200\n",
            "Epoch 238/300\n",
            "\n",
            "Epoch 00238: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3478 - acc: 0.8452 - val_loss: 1.3189 - val_acc: 0.5100\n",
            "Epoch 239/300\n",
            "\n",
            "Epoch 00239: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3611 - acc: 0.8371 - val_loss: 1.0226 - val_acc: 0.5100\n",
            "Epoch 240/300\n",
            "\n",
            "Epoch 00240: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3485 - acc: 0.8306 - val_loss: 1.0022 - val_acc: 0.5100\n",
            "Epoch 241/300\n",
            "\n",
            "Epoch 00241: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3693 - acc: 0.8145 - val_loss: 1.2482 - val_acc: 0.4900\n",
            "Epoch 242/300\n",
            "\n",
            "Epoch 00242: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3315 - acc: 0.8597 - val_loss: 1.0597 - val_acc: 0.5000\n",
            "Epoch 243/300\n",
            "\n",
            "Epoch 00243: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3783 - acc: 0.8323 - val_loss: 0.9990 - val_acc: 0.4700\n",
            "Epoch 244/300\n",
            "\n",
            "Epoch 00244: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3632 - acc: 0.8435 - val_loss: 1.0284 - val_acc: 0.5300\n",
            "Epoch 245/300\n",
            "\n",
            "Epoch 00245: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3835 - acc: 0.8145 - val_loss: 1.0252 - val_acc: 0.4900\n",
            "Epoch 246/300\n",
            "\n",
            "Epoch 00246: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3689 - acc: 0.8290 - val_loss: 1.1223 - val_acc: 0.5000\n",
            "Epoch 247/300\n",
            "\n",
            "Epoch 00247: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3562 - acc: 0.8468 - val_loss: 1.1860 - val_acc: 0.4800\n",
            "Epoch 248/300\n",
            "\n",
            "Epoch 00248: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3394 - acc: 0.8565 - val_loss: 1.0869 - val_acc: 0.4800\n",
            "Epoch 249/300\n",
            "\n",
            "Epoch 00249: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3488 - acc: 0.8387 - val_loss: 0.9792 - val_acc: 0.5000\n",
            "Epoch 250/300\n",
            "\n",
            "Epoch 00250: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3401 - acc: 0.8645 - val_loss: 0.9039 - val_acc: 0.5100\n",
            "Epoch 251/300\n",
            "\n",
            "Epoch 00251: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3349 - acc: 0.8677 - val_loss: 1.2082 - val_acc: 0.5000\n",
            "Epoch 252/300\n",
            "\n",
            "Epoch 00252: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3413 - acc: 0.8581 - val_loss: 0.9200 - val_acc: 0.5200\n",
            "Epoch 253/300\n",
            "\n",
            "Epoch 00253: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3566 - acc: 0.8452 - val_loss: 1.0816 - val_acc: 0.5300\n",
            "Epoch 254/300\n",
            "\n",
            "Epoch 00254: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3684 - acc: 0.8419 - val_loss: 1.1195 - val_acc: 0.5100\n",
            "Epoch 255/300\n",
            "\n",
            "Epoch 00255: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3611 - acc: 0.8468 - val_loss: 1.3715 - val_acc: 0.5100\n",
            "Epoch 256/300\n",
            "\n",
            "Epoch 00256: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3618 - acc: 0.8323 - val_loss: 1.0613 - val_acc: 0.4700\n",
            "Epoch 257/300\n",
            "\n",
            "Epoch 00257: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3489 - acc: 0.8452 - val_loss: 1.1712 - val_acc: 0.4800\n",
            "Epoch 258/300\n",
            "\n",
            "Epoch 00258: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3612 - acc: 0.8387 - val_loss: 0.9679 - val_acc: 0.4800\n",
            "Epoch 259/300\n",
            "\n",
            "Epoch 00259: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3428 - acc: 0.8565 - val_loss: 1.3034 - val_acc: 0.5000\n",
            "Epoch 260/300\n",
            "\n",
            "Epoch 00260: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3894 - acc: 0.8306 - val_loss: 0.9062 - val_acc: 0.5200\n",
            "Epoch 261/300\n",
            "\n",
            "Epoch 00261: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3538 - acc: 0.8323 - val_loss: 1.2376 - val_acc: 0.5000\n",
            "Epoch 262/300\n",
            "\n",
            "Epoch 00262: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3440 - acc: 0.8484 - val_loss: 1.0761 - val_acc: 0.5200\n",
            "Epoch 263/300\n",
            "\n",
            "Epoch 00263: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3246 - acc: 0.8468 - val_loss: 1.1940 - val_acc: 0.4900\n",
            "Epoch 264/300\n",
            "\n",
            "Epoch 00264: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3524 - acc: 0.8371 - val_loss: 1.1711 - val_acc: 0.4900\n",
            "Epoch 265/300\n",
            "\n",
            "Epoch 00265: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3306 - acc: 0.8532 - val_loss: 1.1279 - val_acc: 0.4800\n",
            "Epoch 266/300\n",
            "\n",
            "Epoch 00266: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3481 - acc: 0.8565 - val_loss: 1.1377 - val_acc: 0.4900\n",
            "Epoch 267/300\n",
            "\n",
            "Epoch 00267: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3607 - acc: 0.8323 - val_loss: 1.0810 - val_acc: 0.5000\n",
            "Epoch 268/300\n",
            "\n",
            "Epoch 00268: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3284 - acc: 0.8629 - val_loss: 1.1938 - val_acc: 0.5000\n",
            "Epoch 269/300\n",
            "\n",
            "Epoch 00269: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3523 - acc: 0.8387 - val_loss: 1.0247 - val_acc: 0.4600\n",
            "Epoch 270/300\n",
            "\n",
            "Epoch 00270: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3481 - acc: 0.8484 - val_loss: 1.0600 - val_acc: 0.4900\n",
            "Epoch 271/300\n",
            "\n",
            "Epoch 00271: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3408 - acc: 0.8371 - val_loss: 1.1445 - val_acc: 0.4700\n",
            "Epoch 272/300\n",
            "\n",
            "Epoch 00272: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3452 - acc: 0.8258 - val_loss: 1.0988 - val_acc: 0.5000\n",
            "Epoch 273/300\n",
            "\n",
            "Epoch 00273: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3770 - acc: 0.8339 - val_loss: 1.1168 - val_acc: 0.5000\n",
            "Epoch 274/300\n",
            "\n",
            "Epoch 00274: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3451 - acc: 0.8548 - val_loss: 1.1307 - val_acc: 0.4900\n",
            "Epoch 275/300\n",
            "\n",
            "Epoch 00275: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3593 - acc: 0.8419 - val_loss: 1.1632 - val_acc: 0.4900\n",
            "Epoch 276/300\n",
            "\n",
            "Epoch 00276: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3706 - acc: 0.8306 - val_loss: 0.9219 - val_acc: 0.4900\n",
            "Epoch 277/300\n",
            "\n",
            "Epoch 00277: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3651 - acc: 0.8371 - val_loss: 1.0294 - val_acc: 0.4800\n",
            "Epoch 278/300\n",
            "\n",
            "Epoch 00278: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3234 - acc: 0.8484 - val_loss: 0.9810 - val_acc: 0.4900\n",
            "Epoch 279/300\n",
            "\n",
            "Epoch 00279: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3584 - acc: 0.8355 - val_loss: 1.1416 - val_acc: 0.4900\n",
            "Epoch 280/300\n",
            "\n",
            "Epoch 00280: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3520 - acc: 0.8403 - val_loss: 1.0630 - val_acc: 0.4900\n",
            "Epoch 281/300\n",
            "\n",
            "Epoch 00281: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3166 - acc: 0.8629 - val_loss: 1.2663 - val_acc: 0.4900\n",
            "Epoch 282/300\n",
            "\n",
            "Epoch 00282: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3693 - acc: 0.8242 - val_loss: 1.0964 - val_acc: 0.4600\n",
            "Epoch 283/300\n",
            "\n",
            "Epoch 00283: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3395 - acc: 0.8629 - val_loss: 1.1241 - val_acc: 0.4700\n",
            "Epoch 284/300\n",
            "\n",
            "Epoch 00284: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3589 - acc: 0.8387 - val_loss: 1.3706 - val_acc: 0.4800\n",
            "Epoch 285/300\n",
            "\n",
            "Epoch 00285: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3552 - acc: 0.8548 - val_loss: 0.9618 - val_acc: 0.4900\n",
            "Epoch 286/300\n",
            "\n",
            "Epoch 00286: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3725 - acc: 0.8258 - val_loss: 1.2333 - val_acc: 0.4800\n",
            "Epoch 287/300\n",
            "\n",
            "Epoch 00287: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3549 - acc: 0.8403 - val_loss: 1.1943 - val_acc: 0.5100\n",
            "Epoch 288/300\n",
            "\n",
            "Epoch 00288: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3501 - acc: 0.8419 - val_loss: 0.9273 - val_acc: 0.5100\n",
            "Epoch 289/300\n",
            "\n",
            "Epoch 00289: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3451 - acc: 0.8355 - val_loss: 0.8511 - val_acc: 0.5300\n",
            "Epoch 290/300\n",
            "\n",
            "Epoch 00290: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3458 - acc: 0.8516 - val_loss: 0.8868 - val_acc: 0.4700\n",
            "Epoch 291/300\n",
            "\n",
            "Epoch 00291: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3573 - acc: 0.8339 - val_loss: 1.1421 - val_acc: 0.5100\n",
            "Epoch 292/300\n",
            "\n",
            "Epoch 00292: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3171 - acc: 0.8742 - val_loss: 1.0150 - val_acc: 0.5000\n",
            "Epoch 293/300\n",
            "\n",
            "Epoch 00293: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3593 - acc: 0.8403 - val_loss: 0.9874 - val_acc: 0.4900\n",
            "Epoch 294/300\n",
            "\n",
            "Epoch 00294: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3343 - acc: 0.8371 - val_loss: 1.1243 - val_acc: 0.5200\n",
            "Epoch 295/300\n",
            "\n",
            "Epoch 00295: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3516 - acc: 0.8581 - val_loss: 0.8591 - val_acc: 0.5600\n",
            "Epoch 296/300\n",
            "\n",
            "Epoch 00296: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3726 - acc: 0.8194 - val_loss: 1.0687 - val_acc: 0.4800\n",
            "Epoch 297/300\n",
            "\n",
            "Epoch 00297: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3512 - acc: 0.8435 - val_loss: 1.0210 - val_acc: 0.5000\n",
            "Epoch 298/300\n",
            "\n",
            "Epoch 00298: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3341 - acc: 0.8548 - val_loss: 1.2210 - val_acc: 0.4900\n",
            "Epoch 299/300\n",
            "\n",
            "Epoch 00299: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3347 - acc: 0.8532 - val_loss: 1.2393 - val_acc: 0.5100\n",
            "Epoch 300/300\n",
            "\n",
            "Epoch 00300: val_loss did not improve from 0.62008\n",
            "620/620 - 1s - loss: 0.3648 - acc: 0.8419 - val_loss: 1.0510 - val_acc: 0.5100\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nq+MiccwAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "# Evaluate on test data\n",
            "40/40 [==============================] - 0s 2ms/sample - loss: 0.5051 - acc: 0.7500\n",
            "test loss, test acc: [0.5051267422270029, 0.75]\n",
            "2\n",
            "EEG_Deep/Data2A/parsed_P02T.mat\n",
            "(0, 12, 4096)\n",
            "(80, 12, 4096)\n",
            "(80, 1)\n",
            "3\n",
            "EEG_Deep/Data2A/parsed_P03T.mat\n",
            "(80, 12, 4096)\n",
            "(160, 12, 4096)\n",
            "(160, 1)\n",
            "4\n",
            "EEG_Deep/Data2A/parsed_P04T.mat\n",
            "(160, 12, 4096)\n",
            "(240, 12, 4096)\n",
            "(240, 1)\n",
            "5\n",
            "EEG_Deep/Data2A/parsed_P05T.mat\n",
            "(240, 12, 4096)\n",
            "(320, 12, 4096)\n",
            "(320, 1)\n",
            "6\n",
            "EEG_Deep/Data2A/parsed_P06T.mat\n",
            "(320, 12, 4096)\n",
            "(400, 12, 4096)\n",
            "(400, 1)\n",
            "7\n",
            "EEG_Deep/Data2A/parsed_P07T.mat\n",
            "(400, 12, 4096)\n",
            "(480, 12, 4096)\n",
            "(480, 1)\n",
            "8\n",
            "EEG_Deep/Data2A/parsed_P08T.mat\n",
            "(480, 12, 4096)\n",
            "(560, 12, 4096)\n",
            "(560, 1)\n",
            "9\n",
            "EEG_Deep/Data2A/parsed_P09T.mat\n",
            "(560, 12, 4096)\n",
            "(640, 12, 4096)\n",
            "(640, 1)\n",
            "10\n",
            "EEG_Deep/Data2A/parsed_P010T.mat\n",
            "(640, 12, 4096)\n",
            "(720, 12, 4096)\n",
            "(720, 1)\n",
            "1\n",
            "EEG_Deep/Data2A/parsed_P01E.mat\n",
            "(0, 12, 4096)\n",
            "(40, 12, 4096)\n",
            "(40, 1)\n",
            "Filtering of training data in progress\n",
            "(720, 12, 4096)\n",
            "Filtering of test data in progress\n",
            "(40, 12, 4096)\n",
            "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
            " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1.]\n",
            "(620, 12, 1536)\n",
            "(620,)\n",
            "(100, 12, 1536)\n",
            "(100,)\n",
            "[[1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]]\n",
            "X_train shape: (620, 1, 12, 1536)\n",
            "620 train samples\n",
            "100 val samples\n",
            "(40, 12, 1536)\n",
            "(40, 1)\n",
            "X_train shape: (40, 1, 12, 1536)\n",
            "40 train samples\n",
            "Train on 620 samples, validate on 100 samples\n",
            "Epoch 1/300\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 1.17214, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.9405 - acc: 0.5161 - val_loss: 1.1721 - val_acc: 0.5400\n",
            "Epoch 2/300\n",
            "\n",
            "Epoch 00002: val_loss improved from 1.17214 to 1.01912, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.7546 - acc: 0.6113 - val_loss: 1.0191 - val_acc: 0.5100\n",
            "Epoch 3/300\n",
            "\n",
            "Epoch 00003: val_loss improved from 1.01912 to 0.88478, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.7005 - acc: 0.6710 - val_loss: 0.8848 - val_acc: 0.5000\n",
            "Epoch 4/300\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.88478 to 0.82630, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.6718 - acc: 0.6790 - val_loss: 0.8263 - val_acc: 0.5000\n",
            "Epoch 5/300\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.82630 to 0.78449, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.6310 - acc: 0.7000 - val_loss: 0.7845 - val_acc: 0.5000\n",
            "Epoch 6/300\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.78449\n",
            "620/620 - 1s - loss: 0.5962 - acc: 0.7097 - val_loss: 0.8808 - val_acc: 0.5000\n",
            "Epoch 7/300\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.78449\n",
            "620/620 - 1s - loss: 0.5717 - acc: 0.7274 - val_loss: 0.9085 - val_acc: 0.5200\n",
            "Epoch 8/300\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.78449\n",
            "620/620 - 1s - loss: 0.5660 - acc: 0.7323 - val_loss: 0.9989 - val_acc: 0.5100\n",
            "Epoch 9/300\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.78449\n",
            "620/620 - 1s - loss: 0.5649 - acc: 0.7210 - val_loss: 0.8959 - val_acc: 0.5200\n",
            "Epoch 10/300\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.78449 to 0.77970, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.5522 - acc: 0.7306 - val_loss: 0.7797 - val_acc: 0.5400\n",
            "Epoch 11/300\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.77970\n",
            "620/620 - 1s - loss: 0.5359 - acc: 0.7581 - val_loss: 0.8959 - val_acc: 0.5200\n",
            "Epoch 12/300\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.77970\n",
            "620/620 - 1s - loss: 0.5305 - acc: 0.7516 - val_loss: 1.0509 - val_acc: 0.5300\n",
            "Epoch 13/300\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.77970\n",
            "620/620 - 1s - loss: 0.5113 - acc: 0.7645 - val_loss: 0.8196 - val_acc: 0.5700\n",
            "Epoch 14/300\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.77970\n",
            "620/620 - 1s - loss: 0.5041 - acc: 0.7710 - val_loss: 0.8558 - val_acc: 0.5600\n",
            "Epoch 15/300\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.77970\n",
            "620/620 - 1s - loss: 0.5144 - acc: 0.7645 - val_loss: 0.9183 - val_acc: 0.5300\n",
            "Epoch 16/300\n",
            "\n",
            "Epoch 00016: val_loss improved from 0.77970 to 0.74949, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.5034 - acc: 0.7532 - val_loss: 0.7495 - val_acc: 0.5600\n",
            "Epoch 17/300\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.74949\n",
            "620/620 - 1s - loss: 0.5044 - acc: 0.7677 - val_loss: 0.8224 - val_acc: 0.5500\n",
            "Epoch 18/300\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.74949\n",
            "620/620 - 1s - loss: 0.4864 - acc: 0.7871 - val_loss: 0.9193 - val_acc: 0.5600\n",
            "Epoch 19/300\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.74949\n",
            "620/620 - 1s - loss: 0.4756 - acc: 0.7871 - val_loss: 0.8160 - val_acc: 0.5600\n",
            "Epoch 20/300\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.74949\n",
            "620/620 - 1s - loss: 0.4870 - acc: 0.7645 - val_loss: 1.0458 - val_acc: 0.5300\n",
            "Epoch 21/300\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.74949\n",
            "620/620 - 1s - loss: 0.4755 - acc: 0.7694 - val_loss: 0.8051 - val_acc: 0.5700\n",
            "Epoch 22/300\n",
            "\n",
            "Epoch 00022: val_loss improved from 0.74949 to 0.66096, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.4554 - acc: 0.7935 - val_loss: 0.6610 - val_acc: 0.5800\n",
            "Epoch 23/300\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.66096\n",
            "620/620 - 1s - loss: 0.4909 - acc: 0.7839 - val_loss: 0.7672 - val_acc: 0.5700\n",
            "Epoch 24/300\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.66096\n",
            "620/620 - 1s - loss: 0.4731 - acc: 0.7661 - val_loss: 0.7837 - val_acc: 0.5500\n",
            "Epoch 25/300\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.66096\n",
            "620/620 - 1s - loss: 0.4811 - acc: 0.7823 - val_loss: 0.8248 - val_acc: 0.5500\n",
            "Epoch 26/300\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.66096\n",
            "620/620 - 1s - loss: 0.5010 - acc: 0.7565 - val_loss: 0.9346 - val_acc: 0.5500\n",
            "Epoch 27/300\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.66096\n",
            "620/620 - 1s - loss: 0.4737 - acc: 0.7823 - val_loss: 0.9816 - val_acc: 0.5400\n",
            "Epoch 28/300\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.66096\n",
            "620/620 - 1s - loss: 0.4741 - acc: 0.7774 - val_loss: 0.8541 - val_acc: 0.5500\n",
            "Epoch 29/300\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.66096\n",
            "620/620 - 1s - loss: 0.4710 - acc: 0.7887 - val_loss: 0.8926 - val_acc: 0.5500\n",
            "Epoch 30/300\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.66096\n",
            "620/620 - 1s - loss: 0.4804 - acc: 0.7839 - val_loss: 1.0636 - val_acc: 0.5300\n",
            "Epoch 31/300\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.66096\n",
            "620/620 - 1s - loss: 0.4715 - acc: 0.7774 - val_loss: 0.9031 - val_acc: 0.5500\n",
            "Epoch 32/300\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.66096\n",
            "620/620 - 1s - loss: 0.4589 - acc: 0.7726 - val_loss: 0.8162 - val_acc: 0.5800\n",
            "Epoch 33/300\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.66096\n",
            "620/620 - 1s - loss: 0.4729 - acc: 0.7774 - val_loss: 0.9112 - val_acc: 0.5400\n",
            "Epoch 34/300\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.66096\n",
            "620/620 - 1s - loss: 0.4452 - acc: 0.8048 - val_loss: 0.8416 - val_acc: 0.5500\n",
            "Epoch 35/300\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.66096\n",
            "620/620 - 1s - loss: 0.4642 - acc: 0.7742 - val_loss: 0.8450 - val_acc: 0.5400\n",
            "Epoch 36/300\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.66096\n",
            "620/620 - 1s - loss: 0.4549 - acc: 0.7839 - val_loss: 0.9388 - val_acc: 0.5500\n",
            "Epoch 37/300\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.66096\n",
            "620/620 - 1s - loss: 0.4500 - acc: 0.7887 - val_loss: 1.0381 - val_acc: 0.5400\n",
            "Epoch 38/300\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.66096\n",
            "620/620 - 1s - loss: 0.4637 - acc: 0.7839 - val_loss: 0.9095 - val_acc: 0.5500\n",
            "Epoch 39/300\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.66096\n",
            "620/620 - 1s - loss: 0.4411 - acc: 0.7887 - val_loss: 0.7876 - val_acc: 0.5600\n",
            "Epoch 40/300\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.66096\n",
            "620/620 - 1s - loss: 0.4444 - acc: 0.7887 - val_loss: 0.7331 - val_acc: 0.5700\n",
            "Epoch 41/300\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.66096\n",
            "620/620 - 1s - loss: 0.4591 - acc: 0.7887 - val_loss: 0.8915 - val_acc: 0.5400\n",
            "Epoch 42/300\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.66096\n",
            "620/620 - 1s - loss: 0.4272 - acc: 0.8000 - val_loss: 0.7385 - val_acc: 0.5800\n",
            "Epoch 43/300\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.66096\n",
            "620/620 - 1s - loss: 0.4288 - acc: 0.8097 - val_loss: 1.0729 - val_acc: 0.5500\n",
            "Epoch 44/300\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.66096\n",
            "620/620 - 1s - loss: 0.4513 - acc: 0.7855 - val_loss: 0.9426 - val_acc: 0.5500\n",
            "Epoch 45/300\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.66096\n",
            "620/620 - 1s - loss: 0.4580 - acc: 0.7806 - val_loss: 0.7899 - val_acc: 0.5700\n",
            "Epoch 46/300\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.66096\n",
            "620/620 - 1s - loss: 0.4347 - acc: 0.8113 - val_loss: 1.0754 - val_acc: 0.5400\n",
            "Epoch 47/300\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.66096\n",
            "620/620 - 1s - loss: 0.4547 - acc: 0.7774 - val_loss: 0.9770 - val_acc: 0.5500\n",
            "Epoch 48/300\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.66096\n",
            "620/620 - 1s - loss: 0.4563 - acc: 0.7742 - val_loss: 0.8661 - val_acc: 0.5400\n",
            "Epoch 49/300\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.66096\n",
            "620/620 - 1s - loss: 0.4188 - acc: 0.8129 - val_loss: 1.2601 - val_acc: 0.5100\n",
            "Epoch 50/300\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.66096\n",
            "620/620 - 1s - loss: 0.4377 - acc: 0.7968 - val_loss: 0.7495 - val_acc: 0.5500\n",
            "Epoch 51/300\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.66096\n",
            "620/620 - 1s - loss: 0.4332 - acc: 0.8065 - val_loss: 1.1729 - val_acc: 0.5400\n",
            "Epoch 52/300\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.66096\n",
            "620/620 - 1s - loss: 0.4484 - acc: 0.8016 - val_loss: 0.8784 - val_acc: 0.5600\n",
            "Epoch 53/300\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.66096\n",
            "620/620 - 1s - loss: 0.4640 - acc: 0.7919 - val_loss: 0.8801 - val_acc: 0.5500\n",
            "Epoch 54/300\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.66096\n",
            "620/620 - 1s - loss: 0.4399 - acc: 0.8000 - val_loss: 0.8250 - val_acc: 0.5700\n",
            "Epoch 55/300\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.66096\n",
            "620/620 - 1s - loss: 0.4415 - acc: 0.7984 - val_loss: 1.0030 - val_acc: 0.5400\n",
            "Epoch 56/300\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.66096\n",
            "620/620 - 1s - loss: 0.4418 - acc: 0.8065 - val_loss: 1.0240 - val_acc: 0.5200\n",
            "Epoch 57/300\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.66096\n",
            "620/620 - 1s - loss: 0.4226 - acc: 0.8048 - val_loss: 0.9307 - val_acc: 0.5300\n",
            "Epoch 58/300\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.66096\n",
            "620/620 - 1s - loss: 0.4396 - acc: 0.7919 - val_loss: 0.9695 - val_acc: 0.5600\n",
            "Epoch 59/300\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.66096\n",
            "620/620 - 1s - loss: 0.4084 - acc: 0.8210 - val_loss: 0.9114 - val_acc: 0.5500\n",
            "Epoch 60/300\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.66096\n",
            "620/620 - 1s - loss: 0.4262 - acc: 0.8177 - val_loss: 0.9939 - val_acc: 0.5300\n",
            "Epoch 61/300\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.66096\n",
            "620/620 - 1s - loss: 0.4223 - acc: 0.8113 - val_loss: 1.0406 - val_acc: 0.5400\n",
            "Epoch 62/300\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.66096\n",
            "620/620 - 1s - loss: 0.4229 - acc: 0.8048 - val_loss: 1.0325 - val_acc: 0.5200\n",
            "Epoch 63/300\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.66096\n",
            "620/620 - 1s - loss: 0.4133 - acc: 0.8065 - val_loss: 0.8144 - val_acc: 0.5600\n",
            "Epoch 64/300\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.66096\n",
            "620/620 - 1s - loss: 0.4155 - acc: 0.8194 - val_loss: 1.1354 - val_acc: 0.5200\n",
            "Epoch 65/300\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.66096\n",
            "620/620 - 1s - loss: 0.4233 - acc: 0.8113 - val_loss: 0.9405 - val_acc: 0.5500\n",
            "Epoch 66/300\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.66096\n",
            "620/620 - 1s - loss: 0.4522 - acc: 0.7871 - val_loss: 0.9311 - val_acc: 0.5400\n",
            "Epoch 67/300\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.66096\n",
            "620/620 - 1s - loss: 0.4399 - acc: 0.8081 - val_loss: 0.8437 - val_acc: 0.5400\n",
            "Epoch 68/300\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.66096\n",
            "620/620 - 1s - loss: 0.4170 - acc: 0.8113 - val_loss: 1.0339 - val_acc: 0.5200\n",
            "Epoch 69/300\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.66096\n",
            "620/620 - 1s - loss: 0.4133 - acc: 0.8065 - val_loss: 0.8631 - val_acc: 0.5500\n",
            "Epoch 70/300\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.66096\n",
            "620/620 - 1s - loss: 0.4328 - acc: 0.8081 - val_loss: 1.1124 - val_acc: 0.5200\n",
            "Epoch 71/300\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.66096\n",
            "620/620 - 1s - loss: 0.4094 - acc: 0.8177 - val_loss: 0.8438 - val_acc: 0.5300\n",
            "Epoch 72/300\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.66096\n",
            "620/620 - 1s - loss: 0.4317 - acc: 0.8016 - val_loss: 1.2820 - val_acc: 0.5200\n",
            "Epoch 73/300\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.66096\n",
            "620/620 - 1s - loss: 0.4267 - acc: 0.8016 - val_loss: 0.8456 - val_acc: 0.5500\n",
            "Epoch 74/300\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.66096\n",
            "620/620 - 1s - loss: 0.4137 - acc: 0.8016 - val_loss: 0.9173 - val_acc: 0.5300\n",
            "Epoch 75/300\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.66096\n",
            "620/620 - 1s - loss: 0.3946 - acc: 0.8323 - val_loss: 0.8908 - val_acc: 0.5500\n",
            "Epoch 76/300\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.66096\n",
            "620/620 - 1s - loss: 0.4150 - acc: 0.8081 - val_loss: 0.7803 - val_acc: 0.5200\n",
            "Epoch 77/300\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.66096\n",
            "620/620 - 1s - loss: 0.4065 - acc: 0.8226 - val_loss: 1.3009 - val_acc: 0.5200\n",
            "Epoch 78/300\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.66096\n",
            "620/620 - 1s - loss: 0.4143 - acc: 0.8081 - val_loss: 0.8540 - val_acc: 0.5400\n",
            "Epoch 79/300\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.66096\n",
            "620/620 - 1s - loss: 0.3979 - acc: 0.8177 - val_loss: 0.8383 - val_acc: 0.5400\n",
            "Epoch 80/300\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.66096\n",
            "620/620 - 1s - loss: 0.4139 - acc: 0.8032 - val_loss: 0.8587 - val_acc: 0.5600\n",
            "Epoch 81/300\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.66096\n",
            "620/620 - 1s - loss: 0.4106 - acc: 0.8145 - val_loss: 1.1660 - val_acc: 0.5200\n",
            "Epoch 82/300\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.66096\n",
            "620/620 - 1s - loss: 0.3949 - acc: 0.8210 - val_loss: 1.0246 - val_acc: 0.5200\n",
            "Epoch 83/300\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.66096\n",
            "620/620 - 1s - loss: 0.3876 - acc: 0.8306 - val_loss: 0.9946 - val_acc: 0.5300\n",
            "Epoch 84/300\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.66096\n",
            "620/620 - 1s - loss: 0.4306 - acc: 0.8032 - val_loss: 0.8259 - val_acc: 0.5500\n",
            "Epoch 85/300\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.66096\n",
            "620/620 - 1s - loss: 0.3938 - acc: 0.8161 - val_loss: 0.9863 - val_acc: 0.5300\n",
            "Epoch 86/300\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.66096\n",
            "620/620 - 1s - loss: 0.3856 - acc: 0.8226 - val_loss: 1.0734 - val_acc: 0.5200\n",
            "Epoch 87/300\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.66096\n",
            "620/620 - 1s - loss: 0.4201 - acc: 0.8065 - val_loss: 0.7101 - val_acc: 0.5900\n",
            "Epoch 88/300\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.66096\n",
            "620/620 - 1s - loss: 0.4439 - acc: 0.7839 - val_loss: 0.8700 - val_acc: 0.5300\n",
            "Epoch 89/300\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.66096\n",
            "620/620 - 1s - loss: 0.4121 - acc: 0.8081 - val_loss: 1.0854 - val_acc: 0.5200\n",
            "Epoch 90/300\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.66096\n",
            "620/620 - 1s - loss: 0.3936 - acc: 0.8403 - val_loss: 0.9901 - val_acc: 0.5400\n",
            "Epoch 91/300\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.66096\n",
            "620/620 - 1s - loss: 0.4057 - acc: 0.8081 - val_loss: 0.9460 - val_acc: 0.5400\n",
            "Epoch 92/300\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.66096\n",
            "620/620 - 1s - loss: 0.3864 - acc: 0.8258 - val_loss: 1.2634 - val_acc: 0.5200\n",
            "Epoch 93/300\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.66096\n",
            "620/620 - 1s - loss: 0.4065 - acc: 0.8065 - val_loss: 0.8061 - val_acc: 0.5500\n",
            "Epoch 94/300\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.66096\n",
            "620/620 - 1s - loss: 0.3971 - acc: 0.8097 - val_loss: 0.7970 - val_acc: 0.5500\n",
            "Epoch 95/300\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.66096\n",
            "620/620 - 1s - loss: 0.3894 - acc: 0.8113 - val_loss: 0.9851 - val_acc: 0.5400\n",
            "Epoch 96/300\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.66096\n",
            "620/620 - 1s - loss: 0.3859 - acc: 0.8306 - val_loss: 0.8782 - val_acc: 0.5300\n",
            "Epoch 97/300\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.66096\n",
            "620/620 - 1s - loss: 0.4019 - acc: 0.8371 - val_loss: 0.8615 - val_acc: 0.5400\n",
            "Epoch 98/300\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.66096\n",
            "620/620 - 1s - loss: 0.4129 - acc: 0.8000 - val_loss: 0.7415 - val_acc: 0.5700\n",
            "Epoch 99/300\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.66096\n",
            "620/620 - 1s - loss: 0.4027 - acc: 0.8210 - val_loss: 0.7959 - val_acc: 0.5400\n",
            "Epoch 100/300\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.66096\n",
            "620/620 - 1s - loss: 0.4132 - acc: 0.8210 - val_loss: 0.7153 - val_acc: 0.5700\n",
            "Epoch 101/300\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.66096\n",
            "620/620 - 1s - loss: 0.3949 - acc: 0.8323 - val_loss: 1.1595 - val_acc: 0.5200\n",
            "Epoch 102/300\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.66096\n",
            "620/620 - 1s - loss: 0.4139 - acc: 0.8065 - val_loss: 0.9295 - val_acc: 0.5500\n",
            "Epoch 103/300\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.66096\n",
            "620/620 - 1s - loss: 0.4013 - acc: 0.8323 - val_loss: 0.9294 - val_acc: 0.5600\n",
            "Epoch 104/300\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.66096\n",
            "620/620 - 1s - loss: 0.3693 - acc: 0.8306 - val_loss: 0.8692 - val_acc: 0.5500\n",
            "Epoch 105/300\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 0.66096\n",
            "620/620 - 1s - loss: 0.3965 - acc: 0.8274 - val_loss: 0.8654 - val_acc: 0.5600\n",
            "Epoch 106/300\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 0.66096\n",
            "620/620 - 1s - loss: 0.4108 - acc: 0.8145 - val_loss: 0.9109 - val_acc: 0.5300\n",
            "Epoch 107/300\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.66096\n",
            "620/620 - 1s - loss: 0.3673 - acc: 0.8419 - val_loss: 0.9363 - val_acc: 0.5400\n",
            "Epoch 108/300\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 0.66096\n",
            "620/620 - 1s - loss: 0.3987 - acc: 0.8065 - val_loss: 0.8948 - val_acc: 0.5300\n",
            "Epoch 109/300\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.66096\n",
            "620/620 - 1s - loss: 0.3809 - acc: 0.8468 - val_loss: 0.8438 - val_acc: 0.5500\n",
            "Epoch 110/300\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.66096\n",
            "620/620 - 1s - loss: 0.3644 - acc: 0.8339 - val_loss: 0.8935 - val_acc: 0.5400\n",
            "Epoch 111/300\n",
            "\n",
            "Epoch 00111: val_loss improved from 0.66096 to 0.61758, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.3998 - acc: 0.8081 - val_loss: 0.6176 - val_acc: 0.6900\n",
            "Epoch 112/300\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.4238 - acc: 0.7919 - val_loss: 1.0003 - val_acc: 0.5300\n",
            "Epoch 113/300\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3675 - acc: 0.8403 - val_loss: 0.7677 - val_acc: 0.5600\n",
            "Epoch 114/300\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3820 - acc: 0.8258 - val_loss: 0.8211 - val_acc: 0.5500\n",
            "Epoch 115/300\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.4089 - acc: 0.8177 - val_loss: 0.8528 - val_acc: 0.5400\n",
            "Epoch 116/300\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3872 - acc: 0.8274 - val_loss: 1.0944 - val_acc: 0.5300\n",
            "Epoch 117/300\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3720 - acc: 0.8226 - val_loss: 0.9727 - val_acc: 0.5400\n",
            "Epoch 118/300\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3856 - acc: 0.8129 - val_loss: 1.1962 - val_acc: 0.5200\n",
            "Epoch 119/300\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.4061 - acc: 0.8194 - val_loss: 0.8060 - val_acc: 0.5800\n",
            "Epoch 120/300\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3876 - acc: 0.8242 - val_loss: 0.9199 - val_acc: 0.5400\n",
            "Epoch 121/300\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3984 - acc: 0.8161 - val_loss: 0.9797 - val_acc: 0.5600\n",
            "Epoch 122/300\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.4008 - acc: 0.8065 - val_loss: 0.8711 - val_acc: 0.5500\n",
            "Epoch 123/300\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3980 - acc: 0.8113 - val_loss: 0.8771 - val_acc: 0.5400\n",
            "Epoch 124/300\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3936 - acc: 0.8113 - val_loss: 0.6785 - val_acc: 0.6200\n",
            "Epoch 125/300\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3718 - acc: 0.8306 - val_loss: 1.0392 - val_acc: 0.5500\n",
            "Epoch 126/300\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3921 - acc: 0.8210 - val_loss: 0.8081 - val_acc: 0.5400\n",
            "Epoch 127/300\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3896 - acc: 0.8274 - val_loss: 0.9065 - val_acc: 0.5400\n",
            "Epoch 128/300\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3710 - acc: 0.8339 - val_loss: 0.8655 - val_acc: 0.5500\n",
            "Epoch 129/300\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3860 - acc: 0.8419 - val_loss: 0.8129 - val_acc: 0.5400\n",
            "Epoch 130/300\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3814 - acc: 0.8355 - val_loss: 1.2427 - val_acc: 0.5200\n",
            "Epoch 131/300\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3746 - acc: 0.8387 - val_loss: 1.0712 - val_acc: 0.5300\n",
            "Epoch 132/300\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3829 - acc: 0.8177 - val_loss: 0.6977 - val_acc: 0.5800\n",
            "Epoch 133/300\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3763 - acc: 0.8290 - val_loss: 0.9776 - val_acc: 0.5400\n",
            "Epoch 134/300\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3587 - acc: 0.8355 - val_loss: 0.8931 - val_acc: 0.5500\n",
            "Epoch 135/300\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3910 - acc: 0.8306 - val_loss: 0.7141 - val_acc: 0.5600\n",
            "Epoch 136/300\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3792 - acc: 0.8226 - val_loss: 0.9726 - val_acc: 0.5300\n",
            "Epoch 137/300\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3695 - acc: 0.8355 - val_loss: 1.0387 - val_acc: 0.5400\n",
            "Epoch 138/300\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3933 - acc: 0.8194 - val_loss: 0.7068 - val_acc: 0.5800\n",
            "Epoch 139/300\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3753 - acc: 0.8371 - val_loss: 0.7984 - val_acc: 0.5400\n",
            "Epoch 140/300\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3759 - acc: 0.8403 - val_loss: 0.8281 - val_acc: 0.5500\n",
            "Epoch 141/300\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3668 - acc: 0.8403 - val_loss: 1.0205 - val_acc: 0.5400\n",
            "Epoch 142/300\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3787 - acc: 0.8161 - val_loss: 0.8555 - val_acc: 0.5700\n",
            "Epoch 143/300\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3644 - acc: 0.8371 - val_loss: 0.9160 - val_acc: 0.5500\n",
            "Epoch 144/300\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3712 - acc: 0.8468 - val_loss: 0.9046 - val_acc: 0.5500\n",
            "Epoch 145/300\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3838 - acc: 0.8210 - val_loss: 0.9544 - val_acc: 0.5500\n",
            "Epoch 146/300\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3815 - acc: 0.8194 - val_loss: 0.9403 - val_acc: 0.5400\n",
            "Epoch 147/300\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3796 - acc: 0.8339 - val_loss: 0.8268 - val_acc: 0.5700\n",
            "Epoch 148/300\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3717 - acc: 0.8387 - val_loss: 0.6992 - val_acc: 0.5600\n",
            "Epoch 149/300\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3811 - acc: 0.8274 - val_loss: 0.7212 - val_acc: 0.5800\n",
            "Epoch 150/300\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3769 - acc: 0.8274 - val_loss: 0.7567 - val_acc: 0.5500\n",
            "Epoch 151/300\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3758 - acc: 0.8323 - val_loss: 0.8709 - val_acc: 0.5700\n",
            "Epoch 152/300\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3606 - acc: 0.8565 - val_loss: 0.8927 - val_acc: 0.5700\n",
            "Epoch 153/300\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3514 - acc: 0.8419 - val_loss: 0.9665 - val_acc: 0.5400\n",
            "Epoch 154/300\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3960 - acc: 0.8177 - val_loss: 0.9261 - val_acc: 0.5500\n",
            "Epoch 155/300\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3753 - acc: 0.8210 - val_loss: 0.7976 - val_acc: 0.5700\n",
            "Epoch 156/300\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3533 - acc: 0.8532 - val_loss: 0.8433 - val_acc: 0.5800\n",
            "Epoch 157/300\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3571 - acc: 0.8419 - val_loss: 0.9714 - val_acc: 0.5400\n",
            "Epoch 158/300\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3705 - acc: 0.8306 - val_loss: 1.0711 - val_acc: 0.5400\n",
            "Epoch 159/300\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3516 - acc: 0.8403 - val_loss: 0.8520 - val_acc: 0.5600\n",
            "Epoch 160/300\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3828 - acc: 0.8274 - val_loss: 1.0033 - val_acc: 0.5400\n",
            "Epoch 161/300\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3704 - acc: 0.8452 - val_loss: 1.1159 - val_acc: 0.5300\n",
            "Epoch 162/300\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3671 - acc: 0.8355 - val_loss: 0.9858 - val_acc: 0.5400\n",
            "Epoch 163/300\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3709 - acc: 0.8452 - val_loss: 0.8007 - val_acc: 0.5500\n",
            "Epoch 164/300\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3418 - acc: 0.8468 - val_loss: 0.7126 - val_acc: 0.5700\n",
            "Epoch 165/300\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3939 - acc: 0.8355 - val_loss: 1.0686 - val_acc: 0.5400\n",
            "Epoch 166/300\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3866 - acc: 0.8371 - val_loss: 0.9357 - val_acc: 0.5400\n",
            "Epoch 167/300\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3593 - acc: 0.8435 - val_loss: 0.8954 - val_acc: 0.5500\n",
            "Epoch 168/300\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3497 - acc: 0.8565 - val_loss: 1.1055 - val_acc: 0.5300\n",
            "Epoch 169/300\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.4164 - acc: 0.8097 - val_loss: 0.8812 - val_acc: 0.5600\n",
            "Epoch 170/300\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3694 - acc: 0.8290 - val_loss: 0.7859 - val_acc: 0.6000\n",
            "Epoch 171/300\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3549 - acc: 0.8500 - val_loss: 0.9129 - val_acc: 0.5800\n",
            "Epoch 172/300\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3582 - acc: 0.8290 - val_loss: 0.8290 - val_acc: 0.5700\n",
            "Epoch 173/300\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3700 - acc: 0.8484 - val_loss: 0.9589 - val_acc: 0.5500\n",
            "Epoch 174/300\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3853 - acc: 0.8145 - val_loss: 0.7529 - val_acc: 0.5800\n",
            "Epoch 175/300\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3552 - acc: 0.8323 - val_loss: 0.8613 - val_acc: 0.5500\n",
            "Epoch 176/300\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3544 - acc: 0.8532 - val_loss: 0.8185 - val_acc: 0.5700\n",
            "Epoch 177/300\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3612 - acc: 0.8403 - val_loss: 0.9884 - val_acc: 0.5500\n",
            "Epoch 178/300\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3841 - acc: 0.8290 - val_loss: 0.7130 - val_acc: 0.5900\n",
            "Epoch 179/300\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3911 - acc: 0.8113 - val_loss: 0.8941 - val_acc: 0.5500\n",
            "Epoch 180/300\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3536 - acc: 0.8597 - val_loss: 0.9142 - val_acc: 0.5500\n",
            "Epoch 181/300\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3491 - acc: 0.8403 - val_loss: 0.9681 - val_acc: 0.5300\n",
            "Epoch 182/300\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3696 - acc: 0.8468 - val_loss: 1.0434 - val_acc: 0.5300\n",
            "Epoch 183/300\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3674 - acc: 0.8339 - val_loss: 0.9774 - val_acc: 0.5400\n",
            "Epoch 184/300\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3953 - acc: 0.8097 - val_loss: 0.6893 - val_acc: 0.5900\n",
            "Epoch 185/300\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3751 - acc: 0.8371 - val_loss: 0.9445 - val_acc: 0.5400\n",
            "Epoch 186/300\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3590 - acc: 0.8274 - val_loss: 0.7338 - val_acc: 0.5700\n",
            "Epoch 187/300\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3643 - acc: 0.8403 - val_loss: 0.7067 - val_acc: 0.5700\n",
            "Epoch 188/300\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3727 - acc: 0.8210 - val_loss: 0.8737 - val_acc: 0.5400\n",
            "Epoch 189/300\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3602 - acc: 0.8403 - val_loss: 0.9113 - val_acc: 0.5600\n",
            "Epoch 190/300\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3670 - acc: 0.8387 - val_loss: 1.1280 - val_acc: 0.5300\n",
            "Epoch 191/300\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3211 - acc: 0.8500 - val_loss: 0.8478 - val_acc: 0.5600\n",
            "Epoch 192/300\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3729 - acc: 0.8226 - val_loss: 1.1100 - val_acc: 0.5300\n",
            "Epoch 193/300\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3515 - acc: 0.8435 - val_loss: 1.1783 - val_acc: 0.5200\n",
            "Epoch 194/300\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3551 - acc: 0.8403 - val_loss: 0.8440 - val_acc: 0.5600\n",
            "Epoch 195/300\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3653 - acc: 0.8403 - val_loss: 0.7213 - val_acc: 0.6100\n",
            "Epoch 196/300\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3605 - acc: 0.8290 - val_loss: 0.8162 - val_acc: 0.5700\n",
            "Epoch 197/300\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3570 - acc: 0.8371 - val_loss: 0.7284 - val_acc: 0.6100\n",
            "Epoch 198/300\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3722 - acc: 0.8290 - val_loss: 0.8044 - val_acc: 0.5700\n",
            "Epoch 199/300\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3589 - acc: 0.8323 - val_loss: 0.7851 - val_acc: 0.5400\n",
            "Epoch 200/300\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3296 - acc: 0.8532 - val_loss: 0.8551 - val_acc: 0.5700\n",
            "Epoch 201/300\n",
            "\n",
            "Epoch 00201: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3668 - acc: 0.8371 - val_loss: 1.0578 - val_acc: 0.5300\n",
            "Epoch 202/300\n",
            "\n",
            "Epoch 00202: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3796 - acc: 0.8306 - val_loss: 1.0408 - val_acc: 0.5300\n",
            "Epoch 203/300\n",
            "\n",
            "Epoch 00203: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3412 - acc: 0.8484 - val_loss: 0.9703 - val_acc: 0.5400\n",
            "Epoch 204/300\n",
            "\n",
            "Epoch 00204: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3407 - acc: 0.8629 - val_loss: 0.8578 - val_acc: 0.5900\n",
            "Epoch 205/300\n",
            "\n",
            "Epoch 00205: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3373 - acc: 0.8629 - val_loss: 1.0593 - val_acc: 0.5400\n",
            "Epoch 206/300\n",
            "\n",
            "Epoch 00206: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3218 - acc: 0.8677 - val_loss: 0.9268 - val_acc: 0.5500\n",
            "Epoch 207/300\n",
            "\n",
            "Epoch 00207: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3468 - acc: 0.8435 - val_loss: 0.8676 - val_acc: 0.5500\n",
            "Epoch 208/300\n",
            "\n",
            "Epoch 00208: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3424 - acc: 0.8500 - val_loss: 0.8083 - val_acc: 0.5600\n",
            "Epoch 209/300\n",
            "\n",
            "Epoch 00209: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3604 - acc: 0.8452 - val_loss: 1.1938 - val_acc: 0.5300\n",
            "Epoch 210/300\n",
            "\n",
            "Epoch 00210: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3694 - acc: 0.8323 - val_loss: 1.1769 - val_acc: 0.5300\n",
            "Epoch 211/300\n",
            "\n",
            "Epoch 00211: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3912 - acc: 0.8242 - val_loss: 0.8862 - val_acc: 0.5400\n",
            "Epoch 212/300\n",
            "\n",
            "Epoch 00212: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3454 - acc: 0.8435 - val_loss: 0.7400 - val_acc: 0.5900\n",
            "Epoch 213/300\n",
            "\n",
            "Epoch 00213: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3355 - acc: 0.8500 - val_loss: 0.9383 - val_acc: 0.5600\n",
            "Epoch 214/300\n",
            "\n",
            "Epoch 00214: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3823 - acc: 0.8145 - val_loss: 0.9193 - val_acc: 0.5600\n",
            "Epoch 215/300\n",
            "\n",
            "Epoch 00215: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3513 - acc: 0.8500 - val_loss: 0.9255 - val_acc: 0.5400\n",
            "Epoch 216/300\n",
            "\n",
            "Epoch 00216: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3470 - acc: 0.8435 - val_loss: 0.9583 - val_acc: 0.5500\n",
            "Epoch 217/300\n",
            "\n",
            "Epoch 00217: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3782 - acc: 0.8242 - val_loss: 0.8073 - val_acc: 0.6000\n",
            "Epoch 218/300\n",
            "\n",
            "Epoch 00218: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3506 - acc: 0.8597 - val_loss: 0.7213 - val_acc: 0.6100\n",
            "Epoch 219/300\n",
            "\n",
            "Epoch 00219: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3524 - acc: 0.8290 - val_loss: 0.8830 - val_acc: 0.5500\n",
            "Epoch 220/300\n",
            "\n",
            "Epoch 00220: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3626 - acc: 0.8387 - val_loss: 1.0351 - val_acc: 0.5400\n",
            "Epoch 221/300\n",
            "\n",
            "Epoch 00221: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3566 - acc: 0.8468 - val_loss: 0.8832 - val_acc: 0.5500\n",
            "Epoch 222/300\n",
            "\n",
            "Epoch 00222: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3392 - acc: 0.8645 - val_loss: 1.3022 - val_acc: 0.5400\n",
            "Epoch 223/300\n",
            "\n",
            "Epoch 00223: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3580 - acc: 0.8500 - val_loss: 1.0907 - val_acc: 0.5400\n",
            "Epoch 224/300\n",
            "\n",
            "Epoch 00224: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3318 - acc: 0.8548 - val_loss: 0.7448 - val_acc: 0.5600\n",
            "Epoch 225/300\n",
            "\n",
            "Epoch 00225: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3587 - acc: 0.8242 - val_loss: 0.9129 - val_acc: 0.5500\n",
            "Epoch 226/300\n",
            "\n",
            "Epoch 00226: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3293 - acc: 0.8629 - val_loss: 0.6461 - val_acc: 0.6400\n",
            "Epoch 227/300\n",
            "\n",
            "Epoch 00227: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3432 - acc: 0.8484 - val_loss: 0.8515 - val_acc: 0.5600\n",
            "Epoch 228/300\n",
            "\n",
            "Epoch 00228: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3462 - acc: 0.8500 - val_loss: 0.7309 - val_acc: 0.5600\n",
            "Epoch 229/300\n",
            "\n",
            "Epoch 00229: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3695 - acc: 0.8532 - val_loss: 0.6676 - val_acc: 0.6400\n",
            "Epoch 230/300\n",
            "\n",
            "Epoch 00230: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3439 - acc: 0.8419 - val_loss: 0.9240 - val_acc: 0.5500\n",
            "Epoch 231/300\n",
            "\n",
            "Epoch 00231: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3318 - acc: 0.8548 - val_loss: 0.9541 - val_acc: 0.5400\n",
            "Epoch 232/300\n",
            "\n",
            "Epoch 00232: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3364 - acc: 0.8548 - val_loss: 1.0238 - val_acc: 0.5500\n",
            "Epoch 233/300\n",
            "\n",
            "Epoch 00233: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3387 - acc: 0.8484 - val_loss: 1.3401 - val_acc: 0.5400\n",
            "Epoch 234/300\n",
            "\n",
            "Epoch 00234: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3468 - acc: 0.8435 - val_loss: 0.9511 - val_acc: 0.5400\n",
            "Epoch 235/300\n",
            "\n",
            "Epoch 00235: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3708 - acc: 0.8210 - val_loss: 0.9382 - val_acc: 0.5600\n",
            "Epoch 236/300\n",
            "\n",
            "Epoch 00236: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3236 - acc: 0.8742 - val_loss: 0.9995 - val_acc: 0.5400\n",
            "Epoch 237/300\n",
            "\n",
            "Epoch 00237: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3679 - acc: 0.8355 - val_loss: 0.9972 - val_acc: 0.5700\n",
            "Epoch 238/300\n",
            "\n",
            "Epoch 00238: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3454 - acc: 0.8597 - val_loss: 0.9263 - val_acc: 0.5800\n",
            "Epoch 239/300\n",
            "\n",
            "Epoch 00239: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3517 - acc: 0.8468 - val_loss: 1.1019 - val_acc: 0.5600\n",
            "Epoch 240/300\n",
            "\n",
            "Epoch 00240: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3504 - acc: 0.8500 - val_loss: 0.9415 - val_acc: 0.5500\n",
            "Epoch 241/300\n",
            "\n",
            "Epoch 00241: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3600 - acc: 0.8419 - val_loss: 1.0991 - val_acc: 0.5600\n",
            "Epoch 242/300\n",
            "\n",
            "Epoch 00242: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3464 - acc: 0.8452 - val_loss: 1.0239 - val_acc: 0.5400\n",
            "Epoch 243/300\n",
            "\n",
            "Epoch 00243: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3443 - acc: 0.8435 - val_loss: 0.9380 - val_acc: 0.5800\n",
            "Epoch 244/300\n",
            "\n",
            "Epoch 00244: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3473 - acc: 0.8500 - val_loss: 1.0885 - val_acc: 0.5500\n",
            "Epoch 245/300\n",
            "\n",
            "Epoch 00245: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3581 - acc: 0.8258 - val_loss: 0.8563 - val_acc: 0.5700\n",
            "Epoch 246/300\n",
            "\n",
            "Epoch 00246: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3354 - acc: 0.8597 - val_loss: 0.8280 - val_acc: 0.5800\n",
            "Epoch 247/300\n",
            "\n",
            "Epoch 00247: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3228 - acc: 0.8677 - val_loss: 0.8205 - val_acc: 0.5700\n",
            "Epoch 248/300\n",
            "\n",
            "Epoch 00248: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3565 - acc: 0.8387 - val_loss: 1.1561 - val_acc: 0.5600\n",
            "Epoch 249/300\n",
            "\n",
            "Epoch 00249: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3409 - acc: 0.8468 - val_loss: 1.0523 - val_acc: 0.5500\n",
            "Epoch 250/300\n",
            "\n",
            "Epoch 00250: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3166 - acc: 0.8903 - val_loss: 0.8736 - val_acc: 0.5600\n",
            "Epoch 251/300\n",
            "\n",
            "Epoch 00251: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3391 - acc: 0.8435 - val_loss: 0.9071 - val_acc: 0.5700\n",
            "Epoch 252/300\n",
            "\n",
            "Epoch 00252: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3394 - acc: 0.8468 - val_loss: 0.7416 - val_acc: 0.6300\n",
            "Epoch 253/300\n",
            "\n",
            "Epoch 00253: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3620 - acc: 0.8484 - val_loss: 1.0154 - val_acc: 0.5600\n",
            "Epoch 254/300\n",
            "\n",
            "Epoch 00254: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3300 - acc: 0.8565 - val_loss: 0.9168 - val_acc: 0.5600\n",
            "Epoch 255/300\n",
            "\n",
            "Epoch 00255: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3139 - acc: 0.8758 - val_loss: 1.0208 - val_acc: 0.5500\n",
            "Epoch 256/300\n",
            "\n",
            "Epoch 00256: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3266 - acc: 0.8565 - val_loss: 0.9804 - val_acc: 0.5400\n",
            "Epoch 257/300\n",
            "\n",
            "Epoch 00257: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3310 - acc: 0.8516 - val_loss: 0.9097 - val_acc: 0.5500\n",
            "Epoch 258/300\n",
            "\n",
            "Epoch 00258: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3819 - acc: 0.8258 - val_loss: 0.8765 - val_acc: 0.5500\n",
            "Epoch 259/300\n",
            "\n",
            "Epoch 00259: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3583 - acc: 0.8387 - val_loss: 0.8372 - val_acc: 0.5300\n",
            "Epoch 260/300\n",
            "\n",
            "Epoch 00260: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3468 - acc: 0.8468 - val_loss: 1.1994 - val_acc: 0.5600\n",
            "Epoch 261/300\n",
            "\n",
            "Epoch 00261: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3523 - acc: 0.8435 - val_loss: 1.1921 - val_acc: 0.5500\n",
            "Epoch 262/300\n",
            "\n",
            "Epoch 00262: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3620 - acc: 0.8500 - val_loss: 0.9646 - val_acc: 0.5400\n",
            "Epoch 263/300\n",
            "\n",
            "Epoch 00263: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3490 - acc: 0.8355 - val_loss: 0.7131 - val_acc: 0.6100\n",
            "Epoch 264/300\n",
            "\n",
            "Epoch 00264: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3413 - acc: 0.8419 - val_loss: 0.9234 - val_acc: 0.5700\n",
            "Epoch 265/300\n",
            "\n",
            "Epoch 00265: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3198 - acc: 0.8629 - val_loss: 0.9123 - val_acc: 0.5600\n",
            "Epoch 266/300\n",
            "\n",
            "Epoch 00266: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3517 - acc: 0.8484 - val_loss: 0.6718 - val_acc: 0.6200\n",
            "Epoch 267/300\n",
            "\n",
            "Epoch 00267: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3372 - acc: 0.8516 - val_loss: 0.7891 - val_acc: 0.6000\n",
            "Epoch 268/300\n",
            "\n",
            "Epoch 00268: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3325 - acc: 0.8306 - val_loss: 0.7741 - val_acc: 0.6000\n",
            "Epoch 269/300\n",
            "\n",
            "Epoch 00269: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3530 - acc: 0.8371 - val_loss: 0.7505 - val_acc: 0.6000\n",
            "Epoch 270/300\n",
            "\n",
            "Epoch 00270: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3469 - acc: 0.8484 - val_loss: 1.0293 - val_acc: 0.5600\n",
            "Epoch 271/300\n",
            "\n",
            "Epoch 00271: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3676 - acc: 0.8371 - val_loss: 0.7472 - val_acc: 0.6200\n",
            "Epoch 272/300\n",
            "\n",
            "Epoch 00272: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3099 - acc: 0.8694 - val_loss: 0.7703 - val_acc: 0.6200\n",
            "Epoch 273/300\n",
            "\n",
            "Epoch 00273: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3465 - acc: 0.8403 - val_loss: 0.7548 - val_acc: 0.5800\n",
            "Epoch 274/300\n",
            "\n",
            "Epoch 00274: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3161 - acc: 0.8613 - val_loss: 0.7311 - val_acc: 0.6000\n",
            "Epoch 275/300\n",
            "\n",
            "Epoch 00275: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3409 - acc: 0.8468 - val_loss: 0.9094 - val_acc: 0.5800\n",
            "Epoch 276/300\n",
            "\n",
            "Epoch 00276: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3422 - acc: 0.8645 - val_loss: 0.9187 - val_acc: 0.5400\n",
            "Epoch 277/300\n",
            "\n",
            "Epoch 00277: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3643 - acc: 0.8339 - val_loss: 1.1475 - val_acc: 0.5500\n",
            "Epoch 278/300\n",
            "\n",
            "Epoch 00278: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3330 - acc: 0.8500 - val_loss: 0.9326 - val_acc: 0.5500\n",
            "Epoch 279/300\n",
            "\n",
            "Epoch 00279: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3184 - acc: 0.8661 - val_loss: 1.1433 - val_acc: 0.5500\n",
            "Epoch 280/300\n",
            "\n",
            "Epoch 00280: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3292 - acc: 0.8597 - val_loss: 0.8335 - val_acc: 0.5700\n",
            "Epoch 281/300\n",
            "\n",
            "Epoch 00281: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3619 - acc: 0.8435 - val_loss: 1.1998 - val_acc: 0.5500\n",
            "Epoch 282/300\n",
            "\n",
            "Epoch 00282: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3589 - acc: 0.8306 - val_loss: 1.0983 - val_acc: 0.5600\n",
            "Epoch 283/300\n",
            "\n",
            "Epoch 00283: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3231 - acc: 0.8629 - val_loss: 0.7693 - val_acc: 0.5700\n",
            "Epoch 284/300\n",
            "\n",
            "Epoch 00284: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3666 - acc: 0.8290 - val_loss: 0.7009 - val_acc: 0.6100\n",
            "Epoch 285/300\n",
            "\n",
            "Epoch 00285: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3254 - acc: 0.8484 - val_loss: 0.8213 - val_acc: 0.5700\n",
            "Epoch 286/300\n",
            "\n",
            "Epoch 00286: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3476 - acc: 0.8355 - val_loss: 0.9571 - val_acc: 0.5600\n",
            "Epoch 287/300\n",
            "\n",
            "Epoch 00287: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3268 - acc: 0.8500 - val_loss: 0.8190 - val_acc: 0.5900\n",
            "Epoch 288/300\n",
            "\n",
            "Epoch 00288: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3296 - acc: 0.8597 - val_loss: 1.0139 - val_acc: 0.5600\n",
            "Epoch 289/300\n",
            "\n",
            "Epoch 00289: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3412 - acc: 0.8629 - val_loss: 0.7425 - val_acc: 0.5900\n",
            "Epoch 290/300\n",
            "\n",
            "Epoch 00290: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3410 - acc: 0.8516 - val_loss: 1.0694 - val_acc: 0.5700\n",
            "Epoch 291/300\n",
            "\n",
            "Epoch 00291: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3348 - acc: 0.8581 - val_loss: 0.8803 - val_acc: 0.5800\n",
            "Epoch 292/300\n",
            "\n",
            "Epoch 00292: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3318 - acc: 0.8532 - val_loss: 0.8814 - val_acc: 0.5800\n",
            "Epoch 293/300\n",
            "\n",
            "Epoch 00293: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3329 - acc: 0.8581 - val_loss: 1.0232 - val_acc: 0.5500\n",
            "Epoch 294/300\n",
            "\n",
            "Epoch 00294: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3254 - acc: 0.8661 - val_loss: 1.1998 - val_acc: 0.5400\n",
            "Epoch 295/300\n",
            "\n",
            "Epoch 00295: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3375 - acc: 0.8565 - val_loss: 0.9524 - val_acc: 0.5600\n",
            "Epoch 296/300\n",
            "\n",
            "Epoch 00296: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3484 - acc: 0.8532 - val_loss: 0.8778 - val_acc: 0.5700\n",
            "Epoch 297/300\n",
            "\n",
            "Epoch 00297: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3356 - acc: 0.8516 - val_loss: 1.0610 - val_acc: 0.5700\n",
            "Epoch 298/300\n",
            "\n",
            "Epoch 00298: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3183 - acc: 0.8629 - val_loss: 0.9874 - val_acc: 0.5600\n",
            "Epoch 299/300\n",
            "\n",
            "Epoch 00299: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3125 - acc: 0.8565 - val_loss: 0.8064 - val_acc: 0.6000\n",
            "Epoch 300/300\n",
            "\n",
            "Epoch 00300: val_loss did not improve from 0.61758\n",
            "620/620 - 1s - loss: 0.3311 - acc: 0.8500 - val_loss: 0.8873 - val_acc: 0.6100\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDMLBwcHBIRD/Hwcq+I9sxTBcAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "# Evaluate on test data\n",
            "40/40 [==============================] - 0s 2ms/sample - loss: 0.6140 - acc: 0.6500\n",
            "test loss, test acc: [0.6140446150209755, 0.65]\n",
            "1\n",
            "EEG_Deep/Data2A/parsed_P01T.mat\n",
            "(0, 12, 4096)\n",
            "(80, 12, 4096)\n",
            "(80, 1)\n",
            "2\n",
            "EEG_Deep/Data2A/parsed_P02T.mat\n",
            "(80, 12, 4096)\n",
            "(160, 12, 4096)\n",
            "(160, 1)\n",
            "3\n",
            "EEG_Deep/Data2A/parsed_P03T.mat\n",
            "(160, 12, 4096)\n",
            "(240, 12, 4096)\n",
            "(240, 1)\n",
            "5\n",
            "EEG_Deep/Data2A/parsed_P05T.mat\n",
            "(240, 12, 4096)\n",
            "(320, 12, 4096)\n",
            "(320, 1)\n",
            "6\n",
            "EEG_Deep/Data2A/parsed_P06T.mat\n",
            "(320, 12, 4096)\n",
            "(400, 12, 4096)\n",
            "(400, 1)\n",
            "7\n",
            "EEG_Deep/Data2A/parsed_P07T.mat\n",
            "(400, 12, 4096)\n",
            "(480, 12, 4096)\n",
            "(480, 1)\n",
            "8\n",
            "EEG_Deep/Data2A/parsed_P08T.mat\n",
            "(480, 12, 4096)\n",
            "(560, 12, 4096)\n",
            "(560, 1)\n",
            "9\n",
            "EEG_Deep/Data2A/parsed_P09T.mat\n",
            "(560, 12, 4096)\n",
            "(640, 12, 4096)\n",
            "(640, 1)\n",
            "10\n",
            "EEG_Deep/Data2A/parsed_P010T.mat\n",
            "(640, 12, 4096)\n",
            "(720, 12, 4096)\n",
            "(720, 1)\n",
            "4\n",
            "EEG_Deep/Data2A/parsed_P04E.mat\n",
            "(0, 12, 4096)\n",
            "(40, 12, 4096)\n",
            "(40, 1)\n",
            "Filtering of training data in progress\n",
            "(720, 12, 4096)\n",
            "Filtering of test data in progress\n",
            "(40, 12, 4096)\n",
            "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
            " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1.]\n",
            "(620, 12, 1536)\n",
            "(620,)\n",
            "(100, 12, 1536)\n",
            "(100,)\n",
            "[[1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]]\n",
            "X_train shape: (620, 1, 12, 1536)\n",
            "620 train samples\n",
            "100 val samples\n",
            "(40, 12, 1536)\n",
            "(40, 1)\n",
            "X_train shape: (40, 1, 12, 1536)\n",
            "40 train samples\n",
            "Train on 620 samples, validate on 100 samples\n",
            "Epoch 1/300\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 1.17949, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.9569 - acc: 0.4903 - val_loss: 1.1795 - val_acc: 0.5000\n",
            "Epoch 2/300\n",
            "\n",
            "Epoch 00002: val_loss improved from 1.17949 to 1.03505, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.7655 - acc: 0.6016 - val_loss: 1.0351 - val_acc: 0.5900\n",
            "Epoch 3/300\n",
            "\n",
            "Epoch 00003: val_loss improved from 1.03505 to 0.91412, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.7172 - acc: 0.6419 - val_loss: 0.9141 - val_acc: 0.5000\n",
            "Epoch 4/300\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.91412 to 0.83773, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.6789 - acc: 0.6629 - val_loss: 0.8377 - val_acc: 0.5000\n",
            "Epoch 5/300\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.83773 to 0.79694, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.6162 - acc: 0.7065 - val_loss: 0.7969 - val_acc: 0.5000\n",
            "Epoch 6/300\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.79694\n",
            "620/620 - 1s - loss: 0.6017 - acc: 0.7016 - val_loss: 0.9786 - val_acc: 0.5000\n",
            "Epoch 7/300\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.79694\n",
            "620/620 - 1s - loss: 0.5793 - acc: 0.7242 - val_loss: 0.8975 - val_acc: 0.5000\n",
            "Epoch 8/300\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.79694\n",
            "620/620 - 1s - loss: 0.5988 - acc: 0.6903 - val_loss: 0.8162 - val_acc: 0.5000\n",
            "Epoch 9/300\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.79694\n",
            "620/620 - 1s - loss: 0.5640 - acc: 0.7290 - val_loss: 0.8135 - val_acc: 0.5100\n",
            "Epoch 10/300\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.79694 to 0.70302, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.5364 - acc: 0.7468 - val_loss: 0.7030 - val_acc: 0.5700\n",
            "Epoch 11/300\n",
            "\n",
            "Epoch 00011: val_loss improved from 0.70302 to 0.69619, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.5426 - acc: 0.7323 - val_loss: 0.6962 - val_acc: 0.5600\n",
            "Epoch 12/300\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.69619\n",
            "620/620 - 1s - loss: 0.5261 - acc: 0.7387 - val_loss: 0.6979 - val_acc: 0.5800\n",
            "Epoch 13/300\n",
            "\n",
            "Epoch 00013: val_loss improved from 0.69619 to 0.68602, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.5340 - acc: 0.7484 - val_loss: 0.6860 - val_acc: 0.5500\n",
            "Epoch 14/300\n",
            "\n",
            "Epoch 00014: val_loss improved from 0.68602 to 0.59384, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.5254 - acc: 0.7371 - val_loss: 0.5938 - val_acc: 0.6800\n",
            "Epoch 15/300\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.5201 - acc: 0.7548 - val_loss: 0.6922 - val_acc: 0.5800\n",
            "Epoch 16/300\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.5063 - acc: 0.7565 - val_loss: 0.7952 - val_acc: 0.5500\n",
            "Epoch 17/300\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.5100 - acc: 0.7613 - val_loss: 0.8055 - val_acc: 0.5500\n",
            "Epoch 18/300\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.5081 - acc: 0.7565 - val_loss: 0.7621 - val_acc: 0.5400\n",
            "Epoch 19/300\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.5072 - acc: 0.7613 - val_loss: 0.7223 - val_acc: 0.5700\n",
            "Epoch 20/300\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.4926 - acc: 0.7613 - val_loss: 0.7009 - val_acc: 0.6100\n",
            "Epoch 21/300\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.5132 - acc: 0.7468 - val_loss: 0.8436 - val_acc: 0.5500\n",
            "Epoch 22/300\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.5049 - acc: 0.7468 - val_loss: 0.7956 - val_acc: 0.5400\n",
            "Epoch 23/300\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.4780 - acc: 0.7839 - val_loss: 0.7730 - val_acc: 0.5600\n",
            "Epoch 24/300\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.4809 - acc: 0.7726 - val_loss: 0.7695 - val_acc: 0.5300\n",
            "Epoch 25/300\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.4887 - acc: 0.7500 - val_loss: 0.7443 - val_acc: 0.5400\n",
            "Epoch 26/300\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.5072 - acc: 0.7468 - val_loss: 0.7298 - val_acc: 0.5400\n",
            "Epoch 27/300\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.4383 - acc: 0.8129 - val_loss: 0.9838 - val_acc: 0.5100\n",
            "Epoch 28/300\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.4697 - acc: 0.7774 - val_loss: 1.0033 - val_acc: 0.5200\n",
            "Epoch 29/300\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.5022 - acc: 0.7548 - val_loss: 0.8964 - val_acc: 0.5400\n",
            "Epoch 30/300\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.5102 - acc: 0.7468 - val_loss: 0.8994 - val_acc: 0.5300\n",
            "Epoch 31/300\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.4834 - acc: 0.7677 - val_loss: 0.6768 - val_acc: 0.6300\n",
            "Epoch 32/300\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.5012 - acc: 0.7500 - val_loss: 0.7244 - val_acc: 0.5900\n",
            "Epoch 33/300\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.4823 - acc: 0.7629 - val_loss: 0.8080 - val_acc: 0.5100\n",
            "Epoch 34/300\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.4863 - acc: 0.7677 - val_loss: 1.0321 - val_acc: 0.5300\n",
            "Epoch 35/300\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.4656 - acc: 0.7742 - val_loss: 0.8802 - val_acc: 0.5300\n",
            "Epoch 36/300\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.4688 - acc: 0.7823 - val_loss: 0.6749 - val_acc: 0.5900\n",
            "Epoch 37/300\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.4818 - acc: 0.7629 - val_loss: 0.8852 - val_acc: 0.5300\n",
            "Epoch 38/300\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.4968 - acc: 0.7677 - val_loss: 0.8063 - val_acc: 0.5400\n",
            "Epoch 39/300\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.4420 - acc: 0.7758 - val_loss: 1.1297 - val_acc: 0.5200\n",
            "Epoch 40/300\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.4809 - acc: 0.7774 - val_loss: 0.6129 - val_acc: 0.6700\n",
            "Epoch 41/300\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.4806 - acc: 0.7887 - val_loss: 0.7178 - val_acc: 0.5400\n",
            "Epoch 42/300\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.4415 - acc: 0.7903 - val_loss: 0.6613 - val_acc: 0.5800\n",
            "Epoch 43/300\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.4606 - acc: 0.7968 - val_loss: 0.6515 - val_acc: 0.5700\n",
            "Epoch 44/300\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.4521 - acc: 0.7903 - val_loss: 1.0314 - val_acc: 0.5300\n",
            "Epoch 45/300\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.4505 - acc: 0.7952 - val_loss: 0.8225 - val_acc: 0.5300\n",
            "Epoch 46/300\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.4403 - acc: 0.7903 - val_loss: 1.0224 - val_acc: 0.5200\n",
            "Epoch 47/300\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.4458 - acc: 0.7839 - val_loss: 0.8894 - val_acc: 0.5500\n",
            "Epoch 48/300\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.4766 - acc: 0.7581 - val_loss: 0.8691 - val_acc: 0.5300\n",
            "Epoch 49/300\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.4370 - acc: 0.7774 - val_loss: 0.7402 - val_acc: 0.5500\n",
            "Epoch 50/300\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.4597 - acc: 0.7855 - val_loss: 0.7109 - val_acc: 0.5600\n",
            "Epoch 51/300\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.4718 - acc: 0.7677 - val_loss: 1.1410 - val_acc: 0.5200\n",
            "Epoch 52/300\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.4429 - acc: 0.7903 - val_loss: 1.0573 - val_acc: 0.5300\n",
            "Epoch 53/300\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.4470 - acc: 0.7919 - val_loss: 0.6966 - val_acc: 0.5600\n",
            "Epoch 54/300\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.4319 - acc: 0.7984 - val_loss: 0.7652 - val_acc: 0.5600\n",
            "Epoch 55/300\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.4246 - acc: 0.8194 - val_loss: 1.1393 - val_acc: 0.5200\n",
            "Epoch 56/300\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.4669 - acc: 0.7629 - val_loss: 0.8920 - val_acc: 0.5300\n",
            "Epoch 57/300\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.4514 - acc: 0.7855 - val_loss: 0.8040 - val_acc: 0.5300\n",
            "Epoch 58/300\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.4404 - acc: 0.7839 - val_loss: 0.7704 - val_acc: 0.5400\n",
            "Epoch 59/300\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.4469 - acc: 0.7903 - val_loss: 0.7763 - val_acc: 0.5000\n",
            "Epoch 60/300\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.4383 - acc: 0.7871 - val_loss: 0.9653 - val_acc: 0.5300\n",
            "Epoch 61/300\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.4595 - acc: 0.7823 - val_loss: 0.8260 - val_acc: 0.5300\n",
            "Epoch 62/300\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.4534 - acc: 0.7919 - val_loss: 0.9635 - val_acc: 0.5200\n",
            "Epoch 63/300\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.4533 - acc: 0.7887 - val_loss: 1.0009 - val_acc: 0.5300\n",
            "Epoch 64/300\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.4289 - acc: 0.7968 - val_loss: 0.6146 - val_acc: 0.6700\n",
            "Epoch 65/300\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.4308 - acc: 0.8145 - val_loss: 0.7499 - val_acc: 0.5200\n",
            "Epoch 66/300\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.4391 - acc: 0.7903 - val_loss: 0.7050 - val_acc: 0.5500\n",
            "Epoch 67/300\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.4213 - acc: 0.8081 - val_loss: 1.0633 - val_acc: 0.5300\n",
            "Epoch 68/300\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.4268 - acc: 0.8016 - val_loss: 0.9174 - val_acc: 0.5100\n",
            "Epoch 69/300\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.4475 - acc: 0.7871 - val_loss: 0.7501 - val_acc: 0.5500\n",
            "Epoch 70/300\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.4230 - acc: 0.7935 - val_loss: 0.7906 - val_acc: 0.5100\n",
            "Epoch 71/300\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3886 - acc: 0.8242 - val_loss: 0.8403 - val_acc: 0.5300\n",
            "Epoch 72/300\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.4396 - acc: 0.7919 - val_loss: 0.8088 - val_acc: 0.5400\n",
            "Epoch 73/300\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.4397 - acc: 0.7871 - val_loss: 0.7667 - val_acc: 0.5000\n",
            "Epoch 74/300\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3952 - acc: 0.8306 - val_loss: 0.7136 - val_acc: 0.6000\n",
            "Epoch 75/300\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.4290 - acc: 0.7758 - val_loss: 0.7687 - val_acc: 0.5700\n",
            "Epoch 76/300\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.4468 - acc: 0.7968 - val_loss: 0.8069 - val_acc: 0.5700\n",
            "Epoch 77/300\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.4455 - acc: 0.7806 - val_loss: 1.1622 - val_acc: 0.5200\n",
            "Epoch 78/300\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.4141 - acc: 0.7984 - val_loss: 1.1628 - val_acc: 0.5400\n",
            "Epoch 79/300\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.4108 - acc: 0.8177 - val_loss: 0.9800 - val_acc: 0.5500\n",
            "Epoch 80/300\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.4266 - acc: 0.8032 - val_loss: 1.1115 - val_acc: 0.5000\n",
            "Epoch 81/300\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.4271 - acc: 0.7935 - val_loss: 0.9265 - val_acc: 0.5400\n",
            "Epoch 82/300\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.4055 - acc: 0.8242 - val_loss: 0.6903 - val_acc: 0.6400\n",
            "Epoch 83/300\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.4331 - acc: 0.8032 - val_loss: 0.8610 - val_acc: 0.5000\n",
            "Epoch 84/300\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3970 - acc: 0.8065 - val_loss: 0.9734 - val_acc: 0.5200\n",
            "Epoch 85/300\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.4039 - acc: 0.8194 - val_loss: 1.1350 - val_acc: 0.5200\n",
            "Epoch 86/300\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.4339 - acc: 0.7952 - val_loss: 0.7443 - val_acc: 0.5200\n",
            "Epoch 87/300\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3977 - acc: 0.8129 - val_loss: 0.8915 - val_acc: 0.5000\n",
            "Epoch 88/300\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.4127 - acc: 0.7968 - val_loss: 0.8859 - val_acc: 0.4900\n",
            "Epoch 89/300\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.4260 - acc: 0.8081 - val_loss: 0.8973 - val_acc: 0.5100\n",
            "Epoch 90/300\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3904 - acc: 0.8081 - val_loss: 0.8445 - val_acc: 0.5500\n",
            "Epoch 91/300\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.4006 - acc: 0.8032 - val_loss: 0.8090 - val_acc: 0.5000\n",
            "Epoch 92/300\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.4312 - acc: 0.7871 - val_loss: 0.7664 - val_acc: 0.5100\n",
            "Epoch 93/300\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.4127 - acc: 0.8113 - val_loss: 0.8312 - val_acc: 0.5400\n",
            "Epoch 94/300\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3758 - acc: 0.8226 - val_loss: 0.7486 - val_acc: 0.6000\n",
            "Epoch 95/300\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.4456 - acc: 0.7726 - val_loss: 1.1180 - val_acc: 0.5200\n",
            "Epoch 96/300\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3841 - acc: 0.8274 - val_loss: 0.8874 - val_acc: 0.5200\n",
            "Epoch 97/300\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3898 - acc: 0.8419 - val_loss: 0.8511 - val_acc: 0.5100\n",
            "Epoch 98/300\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.4102 - acc: 0.8000 - val_loss: 0.6742 - val_acc: 0.6200\n",
            "Epoch 99/300\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.4087 - acc: 0.8161 - val_loss: 0.7552 - val_acc: 0.5000\n",
            "Epoch 100/300\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3970 - acc: 0.8371 - val_loss: 0.7917 - val_acc: 0.5300\n",
            "Epoch 101/300\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.4355 - acc: 0.8177 - val_loss: 0.8543 - val_acc: 0.5100\n",
            "Epoch 102/300\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.4056 - acc: 0.8113 - val_loss: 0.7211 - val_acc: 0.6100\n",
            "Epoch 103/300\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.4012 - acc: 0.8210 - val_loss: 1.0059 - val_acc: 0.5100\n",
            "Epoch 104/300\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3963 - acc: 0.8258 - val_loss: 1.0214 - val_acc: 0.5300\n",
            "Epoch 105/300\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3918 - acc: 0.8210 - val_loss: 0.9481 - val_acc: 0.5200\n",
            "Epoch 106/300\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.4113 - acc: 0.8065 - val_loss: 0.6792 - val_acc: 0.6100\n",
            "Epoch 107/300\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3988 - acc: 0.7984 - val_loss: 1.0306 - val_acc: 0.4900\n",
            "Epoch 108/300\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3998 - acc: 0.8065 - val_loss: 1.0583 - val_acc: 0.5100\n",
            "Epoch 109/300\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.4005 - acc: 0.8242 - val_loss: 0.7157 - val_acc: 0.6600\n",
            "Epoch 110/300\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3923 - acc: 0.8194 - val_loss: 1.1285 - val_acc: 0.4800\n",
            "Epoch 111/300\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3932 - acc: 0.8048 - val_loss: 1.0453 - val_acc: 0.4600\n",
            "Epoch 112/300\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.4342 - acc: 0.7935 - val_loss: 1.1760 - val_acc: 0.5200\n",
            "Epoch 113/300\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.4286 - acc: 0.8113 - val_loss: 0.9645 - val_acc: 0.5100\n",
            "Epoch 114/300\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.4045 - acc: 0.7968 - val_loss: 0.9253 - val_acc: 0.5000\n",
            "Epoch 115/300\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3789 - acc: 0.8290 - val_loss: 0.8215 - val_acc: 0.5600\n",
            "Epoch 116/300\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.4254 - acc: 0.7823 - val_loss: 0.7561 - val_acc: 0.5800\n",
            "Epoch 117/300\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.4309 - acc: 0.7823 - val_loss: 1.1115 - val_acc: 0.4800\n",
            "Epoch 118/300\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3964 - acc: 0.8161 - val_loss: 0.7739 - val_acc: 0.5400\n",
            "Epoch 119/300\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3954 - acc: 0.8081 - val_loss: 0.7653 - val_acc: 0.5200\n",
            "Epoch 120/300\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3871 - acc: 0.8226 - val_loss: 0.9194 - val_acc: 0.4900\n",
            "Epoch 121/300\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.4012 - acc: 0.8113 - val_loss: 0.8047 - val_acc: 0.5300\n",
            "Epoch 122/300\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3905 - acc: 0.8387 - val_loss: 0.8543 - val_acc: 0.5300\n",
            "Epoch 123/300\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3932 - acc: 0.8177 - val_loss: 0.9039 - val_acc: 0.5000\n",
            "Epoch 124/300\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3615 - acc: 0.8452 - val_loss: 0.8371 - val_acc: 0.5300\n",
            "Epoch 125/300\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3677 - acc: 0.8403 - val_loss: 0.9367 - val_acc: 0.5100\n",
            "Epoch 126/300\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3719 - acc: 0.8113 - val_loss: 1.0778 - val_acc: 0.4700\n",
            "Epoch 127/300\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3799 - acc: 0.8242 - val_loss: 0.7370 - val_acc: 0.6300\n",
            "Epoch 128/300\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3673 - acc: 0.8306 - val_loss: 0.7855 - val_acc: 0.5500\n",
            "Epoch 129/300\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.4061 - acc: 0.8242 - val_loss: 0.8943 - val_acc: 0.5100\n",
            "Epoch 130/300\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3924 - acc: 0.8226 - val_loss: 1.1047 - val_acc: 0.4700\n",
            "Epoch 131/300\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3859 - acc: 0.8161 - val_loss: 1.0158 - val_acc: 0.4800\n",
            "Epoch 132/300\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3985 - acc: 0.8129 - val_loss: 0.9365 - val_acc: 0.4900\n",
            "Epoch 133/300\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3761 - acc: 0.8355 - val_loss: 1.0378 - val_acc: 0.4900\n",
            "Epoch 134/300\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3751 - acc: 0.8274 - val_loss: 0.9692 - val_acc: 0.5200\n",
            "Epoch 135/300\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3746 - acc: 0.8113 - val_loss: 0.9416 - val_acc: 0.5000\n",
            "Epoch 136/300\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3940 - acc: 0.8000 - val_loss: 1.1504 - val_acc: 0.4600\n",
            "Epoch 137/300\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3673 - acc: 0.8387 - val_loss: 1.1464 - val_acc: 0.4900\n",
            "Epoch 138/300\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3596 - acc: 0.8387 - val_loss: 0.8281 - val_acc: 0.5700\n",
            "Epoch 139/300\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3866 - acc: 0.8145 - val_loss: 0.9049 - val_acc: 0.5300\n",
            "Epoch 140/300\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3760 - acc: 0.8226 - val_loss: 1.1045 - val_acc: 0.4700\n",
            "Epoch 141/300\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.4210 - acc: 0.8097 - val_loss: 1.0868 - val_acc: 0.4700\n",
            "Epoch 142/300\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3632 - acc: 0.8210 - val_loss: 0.9898 - val_acc: 0.4700\n",
            "Epoch 143/300\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3705 - acc: 0.8194 - val_loss: 1.0575 - val_acc: 0.4900\n",
            "Epoch 144/300\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3853 - acc: 0.8242 - val_loss: 0.8983 - val_acc: 0.5100\n",
            "Epoch 145/300\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3550 - acc: 0.8435 - val_loss: 0.9226 - val_acc: 0.5000\n",
            "Epoch 146/300\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3612 - acc: 0.8403 - val_loss: 1.0530 - val_acc: 0.4700\n",
            "Epoch 147/300\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3779 - acc: 0.8290 - val_loss: 1.2380 - val_acc: 0.4800\n",
            "Epoch 148/300\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3674 - acc: 0.8290 - val_loss: 1.0878 - val_acc: 0.4800\n",
            "Epoch 149/300\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3585 - acc: 0.8468 - val_loss: 1.0206 - val_acc: 0.4900\n",
            "Epoch 150/300\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3639 - acc: 0.8226 - val_loss: 1.0469 - val_acc: 0.4900\n",
            "Epoch 151/300\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.4019 - acc: 0.8065 - val_loss: 0.7989 - val_acc: 0.5900\n",
            "Epoch 152/300\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3813 - acc: 0.8242 - val_loss: 0.8022 - val_acc: 0.6500\n",
            "Epoch 153/300\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3334 - acc: 0.8548 - val_loss: 0.9353 - val_acc: 0.5700\n",
            "Epoch 154/300\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3632 - acc: 0.8355 - val_loss: 1.0074 - val_acc: 0.4900\n",
            "Epoch 155/300\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3907 - acc: 0.8032 - val_loss: 0.7810 - val_acc: 0.5900\n",
            "Epoch 156/300\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3718 - acc: 0.8371 - val_loss: 1.1756 - val_acc: 0.5000\n",
            "Epoch 157/300\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3777 - acc: 0.8194 - val_loss: 0.8221 - val_acc: 0.5900\n",
            "Epoch 158/300\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3563 - acc: 0.8565 - val_loss: 1.1933 - val_acc: 0.4900\n",
            "Epoch 159/300\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3586 - acc: 0.8339 - val_loss: 0.9383 - val_acc: 0.5300\n",
            "Epoch 160/300\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3638 - acc: 0.8387 - val_loss: 0.7851 - val_acc: 0.6200\n",
            "Epoch 161/300\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3601 - acc: 0.8339 - val_loss: 1.0118 - val_acc: 0.5100\n",
            "Epoch 162/300\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3990 - acc: 0.8177 - val_loss: 0.9236 - val_acc: 0.4900\n",
            "Epoch 163/300\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3649 - acc: 0.8355 - val_loss: 1.0283 - val_acc: 0.4800\n",
            "Epoch 164/300\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3584 - acc: 0.8371 - val_loss: 0.8107 - val_acc: 0.5600\n",
            "Epoch 165/300\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3464 - acc: 0.8500 - val_loss: 0.8342 - val_acc: 0.5900\n",
            "Epoch 166/300\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3823 - acc: 0.8290 - val_loss: 0.8531 - val_acc: 0.5500\n",
            "Epoch 167/300\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3578 - acc: 0.8419 - val_loss: 0.9023 - val_acc: 0.4900\n",
            "Epoch 168/300\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3423 - acc: 0.8565 - val_loss: 1.3728 - val_acc: 0.4800\n",
            "Epoch 169/300\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3827 - acc: 0.8339 - val_loss: 1.1174 - val_acc: 0.4900\n",
            "Epoch 170/300\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3765 - acc: 0.8306 - val_loss: 1.0304 - val_acc: 0.5200\n",
            "Epoch 171/300\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3507 - acc: 0.8452 - val_loss: 0.8764 - val_acc: 0.5800\n",
            "Epoch 172/300\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3407 - acc: 0.8468 - val_loss: 1.1137 - val_acc: 0.4800\n",
            "Epoch 173/300\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3480 - acc: 0.8484 - val_loss: 1.2426 - val_acc: 0.4700\n",
            "Epoch 174/300\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3731 - acc: 0.8210 - val_loss: 1.0955 - val_acc: 0.4800\n",
            "Epoch 175/300\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3436 - acc: 0.8452 - val_loss: 1.0790 - val_acc: 0.4900\n",
            "Epoch 176/300\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3850 - acc: 0.8371 - val_loss: 0.8398 - val_acc: 0.6200\n",
            "Epoch 177/300\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3594 - acc: 0.8403 - val_loss: 0.7960 - val_acc: 0.6100\n",
            "Epoch 178/300\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3786 - acc: 0.8371 - val_loss: 0.9799 - val_acc: 0.5100\n",
            "Epoch 179/300\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3515 - acc: 0.8565 - val_loss: 0.9395 - val_acc: 0.5200\n",
            "Epoch 180/300\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3404 - acc: 0.8419 - val_loss: 1.3284 - val_acc: 0.4900\n",
            "Epoch 181/300\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3520 - acc: 0.8274 - val_loss: 0.9617 - val_acc: 0.4900\n",
            "Epoch 182/300\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3967 - acc: 0.8145 - val_loss: 1.0310 - val_acc: 0.4800\n",
            "Epoch 183/300\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3559 - acc: 0.8339 - val_loss: 1.1911 - val_acc: 0.4800\n",
            "Epoch 184/300\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3507 - acc: 0.8371 - val_loss: 0.8546 - val_acc: 0.5600\n",
            "Epoch 185/300\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3948 - acc: 0.8113 - val_loss: 1.0558 - val_acc: 0.4800\n",
            "Epoch 186/300\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3735 - acc: 0.8371 - val_loss: 1.0764 - val_acc: 0.5000\n",
            "Epoch 187/300\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3568 - acc: 0.8290 - val_loss: 0.8630 - val_acc: 0.5600\n",
            "Epoch 188/300\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3677 - acc: 0.8452 - val_loss: 1.1773 - val_acc: 0.4700\n",
            "Epoch 189/300\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3275 - acc: 0.8565 - val_loss: 0.8744 - val_acc: 0.5500\n",
            "Epoch 190/300\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3290 - acc: 0.8645 - val_loss: 0.9622 - val_acc: 0.5100\n",
            "Epoch 191/300\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3379 - acc: 0.8565 - val_loss: 1.0984 - val_acc: 0.4800\n",
            "Epoch 192/300\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3506 - acc: 0.8306 - val_loss: 1.0758 - val_acc: 0.4700\n",
            "Epoch 193/300\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3501 - acc: 0.8565 - val_loss: 1.1720 - val_acc: 0.4700\n",
            "Epoch 194/300\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3262 - acc: 0.8677 - val_loss: 1.2525 - val_acc: 0.4800\n",
            "Epoch 195/300\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3566 - acc: 0.8403 - val_loss: 1.1152 - val_acc: 0.5100\n",
            "Epoch 196/300\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3418 - acc: 0.8435 - val_loss: 0.9510 - val_acc: 0.5100\n",
            "Epoch 197/300\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3749 - acc: 0.8306 - val_loss: 1.1012 - val_acc: 0.5100\n",
            "Epoch 198/300\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3462 - acc: 0.8532 - val_loss: 0.9289 - val_acc: 0.5900\n",
            "Epoch 199/300\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3994 - acc: 0.8226 - val_loss: 1.1376 - val_acc: 0.4600\n",
            "Epoch 200/300\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3673 - acc: 0.8435 - val_loss: 1.1606 - val_acc: 0.5000\n",
            "Epoch 201/300\n",
            "\n",
            "Epoch 00201: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3536 - acc: 0.8387 - val_loss: 1.2427 - val_acc: 0.5000\n",
            "Epoch 202/300\n",
            "\n",
            "Epoch 00202: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3428 - acc: 0.8565 - val_loss: 1.2703 - val_acc: 0.4800\n",
            "Epoch 203/300\n",
            "\n",
            "Epoch 00203: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3550 - acc: 0.8290 - val_loss: 0.9783 - val_acc: 0.5100\n",
            "Epoch 204/300\n",
            "\n",
            "Epoch 00204: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3248 - acc: 0.8645 - val_loss: 1.1269 - val_acc: 0.4800\n",
            "Epoch 205/300\n",
            "\n",
            "Epoch 00205: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3520 - acc: 0.8210 - val_loss: 0.9198 - val_acc: 0.5300\n",
            "Epoch 206/300\n",
            "\n",
            "Epoch 00206: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3460 - acc: 0.8452 - val_loss: 1.1987 - val_acc: 0.4900\n",
            "Epoch 207/300\n",
            "\n",
            "Epoch 00207: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3467 - acc: 0.8403 - val_loss: 1.3673 - val_acc: 0.4600\n",
            "Epoch 208/300\n",
            "\n",
            "Epoch 00208: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3672 - acc: 0.8484 - val_loss: 1.1704 - val_acc: 0.4700\n",
            "Epoch 209/300\n",
            "\n",
            "Epoch 00209: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3503 - acc: 0.8484 - val_loss: 1.1729 - val_acc: 0.5100\n",
            "Epoch 210/300\n",
            "\n",
            "Epoch 00210: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3610 - acc: 0.8177 - val_loss: 1.0243 - val_acc: 0.5100\n",
            "Epoch 211/300\n",
            "\n",
            "Epoch 00211: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3598 - acc: 0.8290 - val_loss: 1.1964 - val_acc: 0.4700\n",
            "Epoch 212/300\n",
            "\n",
            "Epoch 00212: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3488 - acc: 0.8323 - val_loss: 1.1435 - val_acc: 0.4900\n",
            "Epoch 213/300\n",
            "\n",
            "Epoch 00213: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3403 - acc: 0.8565 - val_loss: 1.0769 - val_acc: 0.5000\n",
            "Epoch 214/300\n",
            "\n",
            "Epoch 00214: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3481 - acc: 0.8516 - val_loss: 1.1565 - val_acc: 0.5100\n",
            "Epoch 215/300\n",
            "\n",
            "Epoch 00215: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3737 - acc: 0.8274 - val_loss: 0.9746 - val_acc: 0.5700\n",
            "Epoch 216/300\n",
            "\n",
            "Epoch 00216: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3441 - acc: 0.8403 - val_loss: 1.0843 - val_acc: 0.5000\n",
            "Epoch 217/300\n",
            "\n",
            "Epoch 00217: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3401 - acc: 0.8597 - val_loss: 1.0779 - val_acc: 0.5100\n",
            "Epoch 218/300\n",
            "\n",
            "Epoch 00218: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3462 - acc: 0.8339 - val_loss: 1.3646 - val_acc: 0.4800\n",
            "Epoch 219/300\n",
            "\n",
            "Epoch 00219: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3340 - acc: 0.8484 - val_loss: 1.1531 - val_acc: 0.5100\n",
            "Epoch 220/300\n",
            "\n",
            "Epoch 00220: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3403 - acc: 0.8419 - val_loss: 0.8691 - val_acc: 0.6300\n",
            "Epoch 221/300\n",
            "\n",
            "Epoch 00221: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3874 - acc: 0.8290 - val_loss: 1.1685 - val_acc: 0.5000\n",
            "Epoch 222/300\n",
            "\n",
            "Epoch 00222: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3738 - acc: 0.8306 - val_loss: 1.3020 - val_acc: 0.4800\n",
            "Epoch 223/300\n",
            "\n",
            "Epoch 00223: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3420 - acc: 0.8484 - val_loss: 1.2442 - val_acc: 0.4800\n",
            "Epoch 224/300\n",
            "\n",
            "Epoch 00224: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3312 - acc: 0.8613 - val_loss: 1.2081 - val_acc: 0.4900\n",
            "Epoch 225/300\n",
            "\n",
            "Epoch 00225: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3272 - acc: 0.8661 - val_loss: 1.2276 - val_acc: 0.5000\n",
            "Epoch 226/300\n",
            "\n",
            "Epoch 00226: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3574 - acc: 0.8210 - val_loss: 0.9066 - val_acc: 0.5700\n",
            "Epoch 227/300\n",
            "\n",
            "Epoch 00227: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3342 - acc: 0.8548 - val_loss: 0.9072 - val_acc: 0.5700\n",
            "Epoch 228/300\n",
            "\n",
            "Epoch 00228: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3432 - acc: 0.8516 - val_loss: 0.9315 - val_acc: 0.5500\n",
            "Epoch 229/300\n",
            "\n",
            "Epoch 00229: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3431 - acc: 0.8339 - val_loss: 0.9328 - val_acc: 0.5400\n",
            "Epoch 230/300\n",
            "\n",
            "Epoch 00230: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3345 - acc: 0.8629 - val_loss: 1.1640 - val_acc: 0.5100\n",
            "Epoch 231/300\n",
            "\n",
            "Epoch 00231: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3685 - acc: 0.8323 - val_loss: 1.0056 - val_acc: 0.5000\n",
            "Epoch 232/300\n",
            "\n",
            "Epoch 00232: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3406 - acc: 0.8484 - val_loss: 1.1912 - val_acc: 0.5100\n",
            "Epoch 233/300\n",
            "\n",
            "Epoch 00233: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3257 - acc: 0.8565 - val_loss: 1.3566 - val_acc: 0.4700\n",
            "Epoch 234/300\n",
            "\n",
            "Epoch 00234: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3497 - acc: 0.8500 - val_loss: 1.1439 - val_acc: 0.4700\n",
            "Epoch 235/300\n",
            "\n",
            "Epoch 00235: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3516 - acc: 0.8403 - val_loss: 1.1282 - val_acc: 0.5000\n",
            "Epoch 236/300\n",
            "\n",
            "Epoch 00236: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3511 - acc: 0.8403 - val_loss: 1.1559 - val_acc: 0.4800\n",
            "Epoch 237/300\n",
            "\n",
            "Epoch 00237: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3531 - acc: 0.8484 - val_loss: 1.0599 - val_acc: 0.5100\n",
            "Epoch 238/300\n",
            "\n",
            "Epoch 00238: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3445 - acc: 0.8548 - val_loss: 1.0178 - val_acc: 0.6100\n",
            "Epoch 239/300\n",
            "\n",
            "Epoch 00239: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3500 - acc: 0.8435 - val_loss: 1.2788 - val_acc: 0.4900\n",
            "Epoch 240/300\n",
            "\n",
            "Epoch 00240: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3878 - acc: 0.8194 - val_loss: 1.3022 - val_acc: 0.4800\n",
            "Epoch 241/300\n",
            "\n",
            "Epoch 00241: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3457 - acc: 0.8387 - val_loss: 1.0897 - val_acc: 0.4900\n",
            "Epoch 242/300\n",
            "\n",
            "Epoch 00242: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3605 - acc: 0.8194 - val_loss: 1.1075 - val_acc: 0.5100\n",
            "Epoch 243/300\n",
            "\n",
            "Epoch 00243: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3170 - acc: 0.8581 - val_loss: 1.1213 - val_acc: 0.5200\n",
            "Epoch 244/300\n",
            "\n",
            "Epoch 00244: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3358 - acc: 0.8516 - val_loss: 0.8713 - val_acc: 0.5300\n",
            "Epoch 245/300\n",
            "\n",
            "Epoch 00245: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3098 - acc: 0.8581 - val_loss: 1.3403 - val_acc: 0.4900\n",
            "Epoch 246/300\n",
            "\n",
            "Epoch 00246: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3234 - acc: 0.8581 - val_loss: 1.0851 - val_acc: 0.4900\n",
            "Epoch 247/300\n",
            "\n",
            "Epoch 00247: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3016 - acc: 0.8694 - val_loss: 1.4048 - val_acc: 0.4800\n",
            "Epoch 248/300\n",
            "\n",
            "Epoch 00248: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3443 - acc: 0.8532 - val_loss: 0.9351 - val_acc: 0.6200\n",
            "Epoch 249/300\n",
            "\n",
            "Epoch 00249: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3706 - acc: 0.8403 - val_loss: 1.0393 - val_acc: 0.5400\n",
            "Epoch 250/300\n",
            "\n",
            "Epoch 00250: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3694 - acc: 0.8435 - val_loss: 0.9942 - val_acc: 0.5300\n",
            "Epoch 251/300\n",
            "\n",
            "Epoch 00251: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3586 - acc: 0.8355 - val_loss: 1.1295 - val_acc: 0.5000\n",
            "Epoch 252/300\n",
            "\n",
            "Epoch 00252: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3293 - acc: 0.8629 - val_loss: 1.1197 - val_acc: 0.5000\n",
            "Epoch 253/300\n",
            "\n",
            "Epoch 00253: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.2972 - acc: 0.8774 - val_loss: 1.2714 - val_acc: 0.4800\n",
            "Epoch 254/300\n",
            "\n",
            "Epoch 00254: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3469 - acc: 0.8516 - val_loss: 0.9852 - val_acc: 0.5300\n",
            "Epoch 255/300\n",
            "\n",
            "Epoch 00255: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3539 - acc: 0.8403 - val_loss: 1.1334 - val_acc: 0.5400\n",
            "Epoch 256/300\n",
            "\n",
            "Epoch 00256: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3272 - acc: 0.8710 - val_loss: 1.0852 - val_acc: 0.5000\n",
            "Epoch 257/300\n",
            "\n",
            "Epoch 00257: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3168 - acc: 0.8629 - val_loss: 1.2933 - val_acc: 0.4800\n",
            "Epoch 258/300\n",
            "\n",
            "Epoch 00258: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3522 - acc: 0.8548 - val_loss: 1.1122 - val_acc: 0.5200\n",
            "Epoch 259/300\n",
            "\n",
            "Epoch 00259: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3315 - acc: 0.8532 - val_loss: 1.1346 - val_acc: 0.5000\n",
            "Epoch 260/300\n",
            "\n",
            "Epoch 00260: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3209 - acc: 0.8597 - val_loss: 1.1301 - val_acc: 0.5100\n",
            "Epoch 261/300\n",
            "\n",
            "Epoch 00261: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3032 - acc: 0.8790 - val_loss: 1.1461 - val_acc: 0.5100\n",
            "Epoch 262/300\n",
            "\n",
            "Epoch 00262: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3276 - acc: 0.8629 - val_loss: 1.4538 - val_acc: 0.4700\n",
            "Epoch 263/300\n",
            "\n",
            "Epoch 00263: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3506 - acc: 0.8355 - val_loss: 1.3333 - val_acc: 0.4900\n",
            "Epoch 264/300\n",
            "\n",
            "Epoch 00264: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3207 - acc: 0.8710 - val_loss: 1.3087 - val_acc: 0.4900\n",
            "Epoch 265/300\n",
            "\n",
            "Epoch 00265: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3307 - acc: 0.8758 - val_loss: 0.9664 - val_acc: 0.5700\n",
            "Epoch 266/300\n",
            "\n",
            "Epoch 00266: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3355 - acc: 0.8629 - val_loss: 1.2252 - val_acc: 0.4900\n",
            "Epoch 267/300\n",
            "\n",
            "Epoch 00267: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3365 - acc: 0.8500 - val_loss: 1.0647 - val_acc: 0.5500\n",
            "Epoch 268/300\n",
            "\n",
            "Epoch 00268: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3265 - acc: 0.8581 - val_loss: 1.3008 - val_acc: 0.5100\n",
            "Epoch 269/300\n",
            "\n",
            "Epoch 00269: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3132 - acc: 0.8645 - val_loss: 1.1014 - val_acc: 0.5100\n",
            "Epoch 270/300\n",
            "\n",
            "Epoch 00270: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3711 - acc: 0.8419 - val_loss: 0.9720 - val_acc: 0.5400\n",
            "Epoch 271/300\n",
            "\n",
            "Epoch 00271: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3159 - acc: 0.8710 - val_loss: 1.1458 - val_acc: 0.5000\n",
            "Epoch 272/300\n",
            "\n",
            "Epoch 00272: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3370 - acc: 0.8468 - val_loss: 1.3182 - val_acc: 0.4800\n",
            "Epoch 273/300\n",
            "\n",
            "Epoch 00273: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3330 - acc: 0.8500 - val_loss: 1.0935 - val_acc: 0.4800\n",
            "Epoch 274/300\n",
            "\n",
            "Epoch 00274: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3320 - acc: 0.8661 - val_loss: 1.1520 - val_acc: 0.5000\n",
            "Epoch 275/300\n",
            "\n",
            "Epoch 00275: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3441 - acc: 0.8710 - val_loss: 0.9721 - val_acc: 0.5500\n",
            "Epoch 276/300\n",
            "\n",
            "Epoch 00276: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3326 - acc: 0.8500 - val_loss: 1.1447 - val_acc: 0.4700\n",
            "Epoch 277/300\n",
            "\n",
            "Epoch 00277: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3278 - acc: 0.8597 - val_loss: 1.1096 - val_acc: 0.4800\n",
            "Epoch 278/300\n",
            "\n",
            "Epoch 00278: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3069 - acc: 0.8694 - val_loss: 1.2263 - val_acc: 0.5200\n",
            "Epoch 279/300\n",
            "\n",
            "Epoch 00279: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3493 - acc: 0.8452 - val_loss: 0.9650 - val_acc: 0.5800\n",
            "Epoch 280/300\n",
            "\n",
            "Epoch 00280: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3268 - acc: 0.8613 - val_loss: 1.3140 - val_acc: 0.4900\n",
            "Epoch 281/300\n",
            "\n",
            "Epoch 00281: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3267 - acc: 0.8484 - val_loss: 1.3108 - val_acc: 0.4800\n",
            "Epoch 282/300\n",
            "\n",
            "Epoch 00282: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3251 - acc: 0.8500 - val_loss: 1.2010 - val_acc: 0.5000\n",
            "Epoch 283/300\n",
            "\n",
            "Epoch 00283: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3156 - acc: 0.8677 - val_loss: 1.0946 - val_acc: 0.5100\n",
            "Epoch 284/300\n",
            "\n",
            "Epoch 00284: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3257 - acc: 0.8516 - val_loss: 1.1019 - val_acc: 0.5000\n",
            "Epoch 285/300\n",
            "\n",
            "Epoch 00285: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3095 - acc: 0.8629 - val_loss: 1.0557 - val_acc: 0.5300\n",
            "Epoch 286/300\n",
            "\n",
            "Epoch 00286: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3461 - acc: 0.8435 - val_loss: 1.2010 - val_acc: 0.5200\n",
            "Epoch 287/300\n",
            "\n",
            "Epoch 00287: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3319 - acc: 0.8532 - val_loss: 1.0121 - val_acc: 0.5000\n",
            "Epoch 288/300\n",
            "\n",
            "Epoch 00288: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3238 - acc: 0.8581 - val_loss: 1.0018 - val_acc: 0.5600\n",
            "Epoch 289/300\n",
            "\n",
            "Epoch 00289: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3290 - acc: 0.8597 - val_loss: 1.5114 - val_acc: 0.5000\n",
            "Epoch 290/300\n",
            "\n",
            "Epoch 00290: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3017 - acc: 0.8694 - val_loss: 1.2763 - val_acc: 0.4900\n",
            "Epoch 291/300\n",
            "\n",
            "Epoch 00291: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3247 - acc: 0.8694 - val_loss: 1.3244 - val_acc: 0.4900\n",
            "Epoch 292/300\n",
            "\n",
            "Epoch 00292: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3306 - acc: 0.8419 - val_loss: 1.5123 - val_acc: 0.4800\n",
            "Epoch 293/300\n",
            "\n",
            "Epoch 00293: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3389 - acc: 0.8565 - val_loss: 0.9657 - val_acc: 0.5400\n",
            "Epoch 294/300\n",
            "\n",
            "Epoch 00294: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3195 - acc: 0.8661 - val_loss: 1.0916 - val_acc: 0.5100\n",
            "Epoch 295/300\n",
            "\n",
            "Epoch 00295: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3120 - acc: 0.8710 - val_loss: 1.1134 - val_acc: 0.5000\n",
            "Epoch 296/300\n",
            "\n",
            "Epoch 00296: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3209 - acc: 0.8565 - val_loss: 1.0806 - val_acc: 0.5200\n",
            "Epoch 297/300\n",
            "\n",
            "Epoch 00297: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3203 - acc: 0.8694 - val_loss: 1.3046 - val_acc: 0.4900\n",
            "Epoch 298/300\n",
            "\n",
            "Epoch 00298: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3221 - acc: 0.8581 - val_loss: 1.0815 - val_acc: 0.5100\n",
            "Epoch 299/300\n",
            "\n",
            "Epoch 00299: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3263 - acc: 0.8694 - val_loss: 0.9923 - val_acc: 0.5700\n",
            "Epoch 300/300\n",
            "\n",
            "Epoch 00300: val_loss did not improve from 0.59384\n",
            "620/620 - 1s - loss: 0.3287 - acc: 0.8500 - val_loss: 1.2405 - val_acc: 0.5100\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "# Evaluate on test data\n",
            "40/40 [==============================] - 0s 3ms/sample - loss: 0.4685 - acc: 0.7250\n",
            "test loss, test acc: [0.46849945541471244, 0.725]\n",
            "1\n",
            "EEG_Deep/Data2A/parsed_P01T.mat\n",
            "(0, 12, 4096)\n",
            "(80, 12, 4096)\n",
            "(80, 1)\n",
            "3\n",
            "EEG_Deep/Data2A/parsed_P03T.mat\n",
            "(80, 12, 4096)\n",
            "(160, 12, 4096)\n",
            "(160, 1)\n",
            "4\n",
            "EEG_Deep/Data2A/parsed_P04T.mat\n",
            "(160, 12, 4096)\n",
            "(240, 12, 4096)\n",
            "(240, 1)\n",
            "5\n",
            "EEG_Deep/Data2A/parsed_P05T.mat\n",
            "(240, 12, 4096)\n",
            "(320, 12, 4096)\n",
            "(320, 1)\n",
            "6\n",
            "EEG_Deep/Data2A/parsed_P06T.mat\n",
            "(320, 12, 4096)\n",
            "(400, 12, 4096)\n",
            "(400, 1)\n",
            "7\n",
            "EEG_Deep/Data2A/parsed_P07T.mat\n",
            "(400, 12, 4096)\n",
            "(480, 12, 4096)\n",
            "(480, 1)\n",
            "8\n",
            "EEG_Deep/Data2A/parsed_P08T.mat\n",
            "(480, 12, 4096)\n",
            "(560, 12, 4096)\n",
            "(560, 1)\n",
            "9\n",
            "EEG_Deep/Data2A/parsed_P09T.mat\n",
            "(560, 12, 4096)\n",
            "(640, 12, 4096)\n",
            "(640, 1)\n",
            "10\n",
            "EEG_Deep/Data2A/parsed_P010T.mat\n",
            "(640, 12, 4096)\n",
            "(720, 12, 4096)\n",
            "(720, 1)\n",
            "2\n",
            "EEG_Deep/Data2A/parsed_P02E.mat\n",
            "(0, 12, 4096)\n",
            "(40, 12, 4096)\n",
            "(40, 1)\n",
            "Filtering of training data in progress\n",
            "(720, 12, 4096)\n",
            "Filtering of test data in progress\n",
            "(40, 12, 4096)\n",
            "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
            " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1.]\n",
            "(620, 12, 1536)\n",
            "(620,)\n",
            "(100, 12, 1536)\n",
            "(100,)\n",
            "[[1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]]\n",
            "X_train shape: (620, 1, 12, 1536)\n",
            "620 train samples\n",
            "100 val samples\n",
            "(40, 12, 1536)\n",
            "(40, 1)\n",
            "X_train shape: (40, 1, 12, 1536)\n",
            "40 train samples\n",
            "Train on 620 samples, validate on 100 samples\n",
            "Epoch 1/300\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 1.20770, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.9679 - acc: 0.4774 - val_loss: 1.2077 - val_acc: 0.5800\n",
            "Epoch 2/300\n",
            "\n",
            "Epoch 00002: val_loss improved from 1.20770 to 1.07346, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.7703 - acc: 0.5645 - val_loss: 1.0735 - val_acc: 0.5500\n",
            "Epoch 3/300\n",
            "\n",
            "Epoch 00003: val_loss improved from 1.07346 to 0.96066, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.7329 - acc: 0.6210 - val_loss: 0.9607 - val_acc: 0.5200\n",
            "Epoch 4/300\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.96066 to 0.86633, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.6900 - acc: 0.6565 - val_loss: 0.8663 - val_acc: 0.5100\n",
            "Epoch 5/300\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.86633 to 0.79971, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.6251 - acc: 0.7000 - val_loss: 0.7997 - val_acc: 0.5100\n",
            "Epoch 6/300\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.79971\n",
            "620/620 - 1s - loss: 0.5823 - acc: 0.7371 - val_loss: 0.9079 - val_acc: 0.5000\n",
            "Epoch 7/300\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.79971 to 0.69266, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.5641 - acc: 0.7258 - val_loss: 0.6927 - val_acc: 0.5900\n",
            "Epoch 8/300\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.69266\n",
            "620/620 - 1s - loss: 0.5435 - acc: 0.7613 - val_loss: 0.7762 - val_acc: 0.5200\n",
            "Epoch 9/300\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.69266 to 0.68671, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.5456 - acc: 0.7452 - val_loss: 0.6867 - val_acc: 0.5500\n",
            "Epoch 10/300\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.68671\n",
            "620/620 - 1s - loss: 0.5335 - acc: 0.7516 - val_loss: 0.6930 - val_acc: 0.5800\n",
            "Epoch 11/300\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.68671\n",
            "620/620 - 1s - loss: 0.5354 - acc: 0.7339 - val_loss: 0.7248 - val_acc: 0.5300\n",
            "Epoch 12/300\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.68671\n",
            "620/620 - 1s - loss: 0.5092 - acc: 0.7581 - val_loss: 0.8117 - val_acc: 0.5400\n",
            "Epoch 13/300\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.68671\n",
            "620/620 - 1s - loss: 0.5071 - acc: 0.7548 - val_loss: 0.8453 - val_acc: 0.5400\n",
            "Epoch 14/300\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.68671\n",
            "620/620 - 1s - loss: 0.5216 - acc: 0.7452 - val_loss: 0.7896 - val_acc: 0.5400\n",
            "Epoch 15/300\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.68671\n",
            "620/620 - 1s - loss: 0.5264 - acc: 0.7419 - val_loss: 0.7215 - val_acc: 0.5200\n",
            "Epoch 16/300\n",
            "\n",
            "Epoch 00016: val_loss improved from 0.68671 to 0.65813, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.4953 - acc: 0.7419 - val_loss: 0.6581 - val_acc: 0.5600\n",
            "Epoch 17/300\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.5117 - acc: 0.7435 - val_loss: 0.8347 - val_acc: 0.5500\n",
            "Epoch 18/300\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.4978 - acc: 0.7468 - val_loss: 0.9846 - val_acc: 0.5200\n",
            "Epoch 19/300\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.5119 - acc: 0.7532 - val_loss: 0.8135 - val_acc: 0.5600\n",
            "Epoch 20/300\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.5105 - acc: 0.7468 - val_loss: 0.7303 - val_acc: 0.5600\n",
            "Epoch 21/300\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.4831 - acc: 0.7694 - val_loss: 0.7628 - val_acc: 0.5500\n",
            "Epoch 22/300\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.5127 - acc: 0.7613 - val_loss: 0.8191 - val_acc: 0.5600\n",
            "Epoch 23/300\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.5028 - acc: 0.7484 - val_loss: 0.9631 - val_acc: 0.5100\n",
            "Epoch 24/300\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.4848 - acc: 0.7774 - val_loss: 1.1711 - val_acc: 0.5000\n",
            "Epoch 25/300\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.5130 - acc: 0.7339 - val_loss: 0.7732 - val_acc: 0.5400\n",
            "Epoch 26/300\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.4719 - acc: 0.7806 - val_loss: 0.7126 - val_acc: 0.5800\n",
            "Epoch 27/300\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.4782 - acc: 0.7710 - val_loss: 0.8987 - val_acc: 0.5400\n",
            "Epoch 28/300\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.4741 - acc: 0.7758 - val_loss: 0.7755 - val_acc: 0.5300\n",
            "Epoch 29/300\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.4563 - acc: 0.7823 - val_loss: 0.8675 - val_acc: 0.5400\n",
            "Epoch 30/300\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.4667 - acc: 0.7903 - val_loss: 0.8255 - val_acc: 0.5100\n",
            "Epoch 31/300\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.4472 - acc: 0.7968 - val_loss: 1.0053 - val_acc: 0.5100\n",
            "Epoch 32/300\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.4763 - acc: 0.7774 - val_loss: 1.2084 - val_acc: 0.5100\n",
            "Epoch 33/300\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.4587 - acc: 0.7984 - val_loss: 0.9112 - val_acc: 0.5200\n",
            "Epoch 34/300\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.4419 - acc: 0.8097 - val_loss: 1.0610 - val_acc: 0.5100\n",
            "Epoch 35/300\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.4701 - acc: 0.7774 - val_loss: 1.0089 - val_acc: 0.5400\n",
            "Epoch 36/300\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.4661 - acc: 0.7710 - val_loss: 0.8226 - val_acc: 0.5400\n",
            "Epoch 37/300\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.4690 - acc: 0.7710 - val_loss: 1.4273 - val_acc: 0.5000\n",
            "Epoch 38/300\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.4699 - acc: 0.7774 - val_loss: 1.1021 - val_acc: 0.5000\n",
            "Epoch 39/300\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.4528 - acc: 0.7903 - val_loss: 1.3823 - val_acc: 0.5000\n",
            "Epoch 40/300\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.4551 - acc: 0.7855 - val_loss: 1.0075 - val_acc: 0.5200\n",
            "Epoch 41/300\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.4375 - acc: 0.7758 - val_loss: 1.0119 - val_acc: 0.5200\n",
            "Epoch 42/300\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.4540 - acc: 0.7919 - val_loss: 1.1443 - val_acc: 0.5000\n",
            "Epoch 43/300\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.4290 - acc: 0.7919 - val_loss: 1.2440 - val_acc: 0.5100\n",
            "Epoch 44/300\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.4517 - acc: 0.7823 - val_loss: 1.0848 - val_acc: 0.5200\n",
            "Epoch 45/300\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.4648 - acc: 0.7919 - val_loss: 0.8289 - val_acc: 0.5400\n",
            "Epoch 46/300\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.4574 - acc: 0.7694 - val_loss: 1.2556 - val_acc: 0.5000\n",
            "Epoch 47/300\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.4521 - acc: 0.7903 - val_loss: 0.8322 - val_acc: 0.5300\n",
            "Epoch 48/300\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.4775 - acc: 0.7645 - val_loss: 0.9151 - val_acc: 0.5200\n",
            "Epoch 49/300\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.4312 - acc: 0.8048 - val_loss: 1.0593 - val_acc: 0.5100\n",
            "Epoch 50/300\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.4556 - acc: 0.7839 - val_loss: 1.1048 - val_acc: 0.5000\n",
            "Epoch 51/300\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.4318 - acc: 0.7935 - val_loss: 1.2810 - val_acc: 0.5000\n",
            "Epoch 52/300\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.4743 - acc: 0.7710 - val_loss: 0.9525 - val_acc: 0.5000\n",
            "Epoch 53/300\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.4478 - acc: 0.7758 - val_loss: 1.1466 - val_acc: 0.5000\n",
            "Epoch 54/300\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.4435 - acc: 0.7919 - val_loss: 0.8884 - val_acc: 0.5300\n",
            "Epoch 55/300\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.4216 - acc: 0.8161 - val_loss: 0.8671 - val_acc: 0.5200\n",
            "Epoch 56/300\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.4285 - acc: 0.8113 - val_loss: 1.0448 - val_acc: 0.5100\n",
            "Epoch 57/300\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.4427 - acc: 0.7919 - val_loss: 1.1274 - val_acc: 0.5000\n",
            "Epoch 58/300\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.4017 - acc: 0.8145 - val_loss: 0.9092 - val_acc: 0.5000\n",
            "Epoch 59/300\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.4317 - acc: 0.7903 - val_loss: 1.0998 - val_acc: 0.5000\n",
            "Epoch 60/300\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.4385 - acc: 0.8226 - val_loss: 0.9555 - val_acc: 0.5000\n",
            "Epoch 61/300\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.4376 - acc: 0.7806 - val_loss: 1.5607 - val_acc: 0.5000\n",
            "Epoch 62/300\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.4495 - acc: 0.7855 - val_loss: 1.0465 - val_acc: 0.5200\n",
            "Epoch 63/300\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.4491 - acc: 0.7903 - val_loss: 1.4978 - val_acc: 0.5000\n",
            "Epoch 64/300\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.4132 - acc: 0.8081 - val_loss: 1.0978 - val_acc: 0.5100\n",
            "Epoch 65/300\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.4431 - acc: 0.7855 - val_loss: 1.0732 - val_acc: 0.5100\n",
            "Epoch 66/300\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.4310 - acc: 0.7903 - val_loss: 0.8777 - val_acc: 0.5200\n",
            "Epoch 67/300\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.4242 - acc: 0.8161 - val_loss: 0.6843 - val_acc: 0.6400\n",
            "Epoch 68/300\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.4051 - acc: 0.8065 - val_loss: 0.7364 - val_acc: 0.5600\n",
            "Epoch 69/300\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.4283 - acc: 0.8048 - val_loss: 0.7617 - val_acc: 0.5900\n",
            "Epoch 70/300\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.4161 - acc: 0.8081 - val_loss: 1.1291 - val_acc: 0.4900\n",
            "Epoch 71/300\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.4392 - acc: 0.7952 - val_loss: 1.2834 - val_acc: 0.5100\n",
            "Epoch 72/300\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.4061 - acc: 0.7984 - val_loss: 0.8207 - val_acc: 0.5200\n",
            "Epoch 73/300\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3847 - acc: 0.8339 - val_loss: 1.2855 - val_acc: 0.5100\n",
            "Epoch 74/300\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.4135 - acc: 0.8113 - val_loss: 0.9731 - val_acc: 0.5100\n",
            "Epoch 75/300\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.4579 - acc: 0.7935 - val_loss: 1.1262 - val_acc: 0.5000\n",
            "Epoch 76/300\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.4101 - acc: 0.8210 - val_loss: 0.9139 - val_acc: 0.5400\n",
            "Epoch 77/300\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.4609 - acc: 0.7742 - val_loss: 0.9487 - val_acc: 0.5100\n",
            "Epoch 78/300\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.4073 - acc: 0.8065 - val_loss: 1.2394 - val_acc: 0.5100\n",
            "Epoch 79/300\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.4050 - acc: 0.8194 - val_loss: 1.3817 - val_acc: 0.5100\n",
            "Epoch 80/300\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.4296 - acc: 0.8000 - val_loss: 0.9450 - val_acc: 0.5600\n",
            "Epoch 81/300\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.4149 - acc: 0.7968 - val_loss: 1.1526 - val_acc: 0.4900\n",
            "Epoch 82/300\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.4183 - acc: 0.7855 - val_loss: 1.1644 - val_acc: 0.5100\n",
            "Epoch 83/300\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.4200 - acc: 0.7984 - val_loss: 1.2686 - val_acc: 0.5200\n",
            "Epoch 84/300\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3954 - acc: 0.8210 - val_loss: 0.8629 - val_acc: 0.5200\n",
            "Epoch 85/300\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.4471 - acc: 0.7887 - val_loss: 1.3639 - val_acc: 0.5100\n",
            "Epoch 86/300\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.4364 - acc: 0.7855 - val_loss: 1.3446 - val_acc: 0.5100\n",
            "Epoch 87/300\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.4180 - acc: 0.8177 - val_loss: 1.1251 - val_acc: 0.5200\n",
            "Epoch 88/300\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.4214 - acc: 0.8048 - val_loss: 1.0310 - val_acc: 0.5000\n",
            "Epoch 89/300\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3820 - acc: 0.8242 - val_loss: 1.0676 - val_acc: 0.5000\n",
            "Epoch 90/300\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.4043 - acc: 0.8097 - val_loss: 1.0996 - val_acc: 0.5200\n",
            "Epoch 91/300\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3831 - acc: 0.8274 - val_loss: 1.0772 - val_acc: 0.4900\n",
            "Epoch 92/300\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.4040 - acc: 0.8258 - val_loss: 1.1594 - val_acc: 0.5100\n",
            "Epoch 93/300\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3780 - acc: 0.8242 - val_loss: 0.9819 - val_acc: 0.5200\n",
            "Epoch 94/300\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.4043 - acc: 0.8032 - val_loss: 0.8161 - val_acc: 0.5500\n",
            "Epoch 95/300\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3991 - acc: 0.8161 - val_loss: 1.3819 - val_acc: 0.5000\n",
            "Epoch 96/300\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3971 - acc: 0.8145 - val_loss: 0.9918 - val_acc: 0.5200\n",
            "Epoch 97/300\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3928 - acc: 0.8242 - val_loss: 1.0281 - val_acc: 0.5300\n",
            "Epoch 98/300\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.4239 - acc: 0.8000 - val_loss: 0.9975 - val_acc: 0.5100\n",
            "Epoch 99/300\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3791 - acc: 0.8355 - val_loss: 1.3288 - val_acc: 0.5100\n",
            "Epoch 100/300\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.4085 - acc: 0.8032 - val_loss: 1.1876 - val_acc: 0.5200\n",
            "Epoch 101/300\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3891 - acc: 0.8194 - val_loss: 1.3316 - val_acc: 0.5100\n",
            "Epoch 102/300\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3908 - acc: 0.8258 - val_loss: 1.0391 - val_acc: 0.5200\n",
            "Epoch 103/300\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3896 - acc: 0.8323 - val_loss: 1.0820 - val_acc: 0.5200\n",
            "Epoch 104/300\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.4056 - acc: 0.7952 - val_loss: 0.9849 - val_acc: 0.5300\n",
            "Epoch 105/300\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.4333 - acc: 0.7871 - val_loss: 1.1685 - val_acc: 0.5100\n",
            "Epoch 106/300\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3951 - acc: 0.8194 - val_loss: 1.3068 - val_acc: 0.5200\n",
            "Epoch 107/300\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3920 - acc: 0.8032 - val_loss: 1.0188 - val_acc: 0.5500\n",
            "Epoch 108/300\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3878 - acc: 0.8242 - val_loss: 1.0835 - val_acc: 0.5400\n",
            "Epoch 109/300\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3977 - acc: 0.8161 - val_loss: 1.1847 - val_acc: 0.4900\n",
            "Epoch 110/300\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3875 - acc: 0.8226 - val_loss: 1.1642 - val_acc: 0.5000\n",
            "Epoch 111/300\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.4042 - acc: 0.8016 - val_loss: 1.1035 - val_acc: 0.4900\n",
            "Epoch 112/300\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3872 - acc: 0.8161 - val_loss: 1.0404 - val_acc: 0.5000\n",
            "Epoch 113/300\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3701 - acc: 0.8306 - val_loss: 0.9195 - val_acc: 0.5100\n",
            "Epoch 114/300\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3931 - acc: 0.8242 - val_loss: 1.0600 - val_acc: 0.5000\n",
            "Epoch 115/300\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3885 - acc: 0.8355 - val_loss: 1.1834 - val_acc: 0.5100\n",
            "Epoch 116/300\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3720 - acc: 0.8339 - val_loss: 1.2417 - val_acc: 0.5100\n",
            "Epoch 117/300\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.4154 - acc: 0.8032 - val_loss: 1.0401 - val_acc: 0.5100\n",
            "Epoch 118/300\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3588 - acc: 0.8532 - val_loss: 1.2379 - val_acc: 0.5100\n",
            "Epoch 119/300\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3665 - acc: 0.8339 - val_loss: 1.2953 - val_acc: 0.5100\n",
            "Epoch 120/300\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3952 - acc: 0.8032 - val_loss: 1.1401 - val_acc: 0.5000\n",
            "Epoch 121/300\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3857 - acc: 0.8194 - val_loss: 1.0749 - val_acc: 0.5100\n",
            "Epoch 122/300\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3804 - acc: 0.8306 - val_loss: 1.1175 - val_acc: 0.5100\n",
            "Epoch 123/300\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.4070 - acc: 0.8097 - val_loss: 1.5153 - val_acc: 0.5100\n",
            "Epoch 124/300\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3877 - acc: 0.8306 - val_loss: 1.3433 - val_acc: 0.5000\n",
            "Epoch 125/300\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3780 - acc: 0.8290 - val_loss: 1.1033 - val_acc: 0.5100\n",
            "Epoch 126/300\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3862 - acc: 0.8242 - val_loss: 0.8451 - val_acc: 0.5500\n",
            "Epoch 127/300\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3751 - acc: 0.8306 - val_loss: 1.1625 - val_acc: 0.5100\n",
            "Epoch 128/300\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3937 - acc: 0.8242 - val_loss: 1.1912 - val_acc: 0.5000\n",
            "Epoch 129/300\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3934 - acc: 0.8177 - val_loss: 1.1216 - val_acc: 0.4900\n",
            "Epoch 130/300\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3548 - acc: 0.8323 - val_loss: 1.3111 - val_acc: 0.5100\n",
            "Epoch 131/300\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3814 - acc: 0.8258 - val_loss: 0.7481 - val_acc: 0.5900\n",
            "Epoch 132/300\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3762 - acc: 0.8387 - val_loss: 1.0566 - val_acc: 0.5100\n",
            "Epoch 133/300\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3886 - acc: 0.8065 - val_loss: 1.1770 - val_acc: 0.4900\n",
            "Epoch 134/300\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3921 - acc: 0.8081 - val_loss: 1.1969 - val_acc: 0.5100\n",
            "Epoch 135/300\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3949 - acc: 0.8194 - val_loss: 1.0458 - val_acc: 0.5100\n",
            "Epoch 136/300\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.4212 - acc: 0.8016 - val_loss: 1.2439 - val_acc: 0.5100\n",
            "Epoch 137/300\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3960 - acc: 0.8129 - val_loss: 0.9944 - val_acc: 0.5100\n",
            "Epoch 138/300\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.4049 - acc: 0.8048 - val_loss: 1.3051 - val_acc: 0.5000\n",
            "Epoch 139/300\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3596 - acc: 0.8419 - val_loss: 1.0679 - val_acc: 0.5100\n",
            "Epoch 140/300\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.4252 - acc: 0.8016 - val_loss: 0.9228 - val_acc: 0.5200\n",
            "Epoch 141/300\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3694 - acc: 0.8210 - val_loss: 0.9846 - val_acc: 0.5200\n",
            "Epoch 142/300\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.4298 - acc: 0.7790 - val_loss: 0.9462 - val_acc: 0.5100\n",
            "Epoch 143/300\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3584 - acc: 0.8452 - val_loss: 1.0235 - val_acc: 0.5200\n",
            "Epoch 144/300\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3709 - acc: 0.8339 - val_loss: 1.1848 - val_acc: 0.5000\n",
            "Epoch 145/300\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3829 - acc: 0.8210 - val_loss: 1.0005 - val_acc: 0.5000\n",
            "Epoch 146/300\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3891 - acc: 0.8306 - val_loss: 1.2831 - val_acc: 0.5100\n",
            "Epoch 147/300\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3781 - acc: 0.8323 - val_loss: 1.1161 - val_acc: 0.4900\n",
            "Epoch 148/300\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3743 - acc: 0.8145 - val_loss: 1.3801 - val_acc: 0.5100\n",
            "Epoch 149/300\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3942 - acc: 0.8258 - val_loss: 1.2311 - val_acc: 0.5100\n",
            "Epoch 150/300\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3631 - acc: 0.8484 - val_loss: 1.5132 - val_acc: 0.5000\n",
            "Epoch 151/300\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3768 - acc: 0.8161 - val_loss: 1.0212 - val_acc: 0.4800\n",
            "Epoch 152/300\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3569 - acc: 0.8323 - val_loss: 1.1252 - val_acc: 0.4900\n",
            "Epoch 153/300\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3676 - acc: 0.8387 - val_loss: 1.5285 - val_acc: 0.5100\n",
            "Epoch 154/300\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3993 - acc: 0.8129 - val_loss: 0.9464 - val_acc: 0.5300\n",
            "Epoch 155/300\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3730 - acc: 0.8452 - val_loss: 1.2501 - val_acc: 0.4900\n",
            "Epoch 156/300\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3604 - acc: 0.8355 - val_loss: 1.2733 - val_acc: 0.5100\n",
            "Epoch 157/300\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3541 - acc: 0.8306 - val_loss: 1.0635 - val_acc: 0.5100\n",
            "Epoch 158/300\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3788 - acc: 0.8323 - val_loss: 1.0397 - val_acc: 0.5100\n",
            "Epoch 159/300\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3772 - acc: 0.8242 - val_loss: 1.1167 - val_acc: 0.4900\n",
            "Epoch 160/300\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3598 - acc: 0.8403 - val_loss: 1.0959 - val_acc: 0.5100\n",
            "Epoch 161/300\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3635 - acc: 0.8194 - val_loss: 1.4892 - val_acc: 0.5100\n",
            "Epoch 162/300\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3721 - acc: 0.8452 - val_loss: 1.4734 - val_acc: 0.5100\n",
            "Epoch 163/300\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3551 - acc: 0.8484 - val_loss: 1.1225 - val_acc: 0.5000\n",
            "Epoch 164/300\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3819 - acc: 0.8129 - val_loss: 1.1416 - val_acc: 0.5100\n",
            "Epoch 165/300\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3724 - acc: 0.8387 - val_loss: 1.3412 - val_acc: 0.5100\n",
            "Epoch 166/300\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3577 - acc: 0.8419 - val_loss: 1.1124 - val_acc: 0.5100\n",
            "Epoch 167/300\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3468 - acc: 0.8661 - val_loss: 1.3841 - val_acc: 0.5200\n",
            "Epoch 168/300\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3588 - acc: 0.8387 - val_loss: 1.0201 - val_acc: 0.5000\n",
            "Epoch 169/300\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3702 - acc: 0.8323 - val_loss: 1.1017 - val_acc: 0.5000\n",
            "Epoch 170/300\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3711 - acc: 0.8403 - val_loss: 1.4363 - val_acc: 0.5100\n",
            "Epoch 171/300\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3828 - acc: 0.8306 - val_loss: 1.1272 - val_acc: 0.5000\n",
            "Epoch 172/300\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3873 - acc: 0.8097 - val_loss: 1.2586 - val_acc: 0.5200\n",
            "Epoch 173/300\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3563 - acc: 0.8371 - val_loss: 1.0651 - val_acc: 0.4900\n",
            "Epoch 174/300\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3633 - acc: 0.8435 - val_loss: 1.0614 - val_acc: 0.5100\n",
            "Epoch 175/300\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3604 - acc: 0.8226 - val_loss: 1.0957 - val_acc: 0.4800\n",
            "Epoch 176/300\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3625 - acc: 0.8323 - val_loss: 1.2723 - val_acc: 0.5100\n",
            "Epoch 177/300\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3597 - acc: 0.8306 - val_loss: 0.9925 - val_acc: 0.5400\n",
            "Epoch 178/300\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3527 - acc: 0.8613 - val_loss: 1.0514 - val_acc: 0.5100\n",
            "Epoch 179/300\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3505 - acc: 0.8387 - val_loss: 1.0799 - val_acc: 0.5000\n",
            "Epoch 180/300\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3805 - acc: 0.8274 - val_loss: 1.3546 - val_acc: 0.5000\n",
            "Epoch 181/300\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3276 - acc: 0.8742 - val_loss: 1.4152 - val_acc: 0.5100\n",
            "Epoch 182/300\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3674 - acc: 0.8435 - val_loss: 1.9139 - val_acc: 0.5100\n",
            "Epoch 183/300\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3888 - acc: 0.8129 - val_loss: 1.1471 - val_acc: 0.4900\n",
            "Epoch 184/300\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3589 - acc: 0.8419 - val_loss: 1.1118 - val_acc: 0.5000\n",
            "Epoch 185/300\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3909 - acc: 0.8097 - val_loss: 1.2447 - val_acc: 0.5000\n",
            "Epoch 186/300\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3765 - acc: 0.8306 - val_loss: 1.3594 - val_acc: 0.5100\n",
            "Epoch 187/300\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3798 - acc: 0.8210 - val_loss: 1.1139 - val_acc: 0.5000\n",
            "Epoch 188/300\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3608 - acc: 0.8339 - val_loss: 1.2774 - val_acc: 0.4900\n",
            "Epoch 189/300\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3310 - acc: 0.8500 - val_loss: 1.5165 - val_acc: 0.5000\n",
            "Epoch 190/300\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3463 - acc: 0.8339 - val_loss: 1.2697 - val_acc: 0.5000\n",
            "Epoch 191/300\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3622 - acc: 0.8387 - val_loss: 1.6373 - val_acc: 0.5100\n",
            "Epoch 192/300\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3767 - acc: 0.8129 - val_loss: 1.0276 - val_acc: 0.5500\n",
            "Epoch 193/300\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3730 - acc: 0.8274 - val_loss: 1.1390 - val_acc: 0.5100\n",
            "Epoch 194/300\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3755 - acc: 0.8226 - val_loss: 1.2450 - val_acc: 0.5000\n",
            "Epoch 195/300\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3986 - acc: 0.8129 - val_loss: 1.0944 - val_acc: 0.5200\n",
            "Epoch 196/300\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3676 - acc: 0.8274 - val_loss: 1.1653 - val_acc: 0.5000\n",
            "Epoch 197/300\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3632 - acc: 0.8323 - val_loss: 1.6538 - val_acc: 0.5000\n",
            "Epoch 198/300\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3446 - acc: 0.8452 - val_loss: 1.5224 - val_acc: 0.5000\n",
            "Epoch 199/300\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3906 - acc: 0.8323 - val_loss: 0.9555 - val_acc: 0.5400\n",
            "Epoch 200/300\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3702 - acc: 0.8226 - val_loss: 1.6021 - val_acc: 0.5100\n",
            "Epoch 201/300\n",
            "\n",
            "Epoch 00201: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3776 - acc: 0.8274 - val_loss: 1.0631 - val_acc: 0.4800\n",
            "Epoch 202/300\n",
            "\n",
            "Epoch 00202: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3707 - acc: 0.8355 - val_loss: 1.3031 - val_acc: 0.5100\n",
            "Epoch 203/300\n",
            "\n",
            "Epoch 00203: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3573 - acc: 0.8339 - val_loss: 1.0546 - val_acc: 0.5100\n",
            "Epoch 204/300\n",
            "\n",
            "Epoch 00204: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3602 - acc: 0.8306 - val_loss: 1.3991 - val_acc: 0.5100\n",
            "Epoch 205/300\n",
            "\n",
            "Epoch 00205: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3700 - acc: 0.8242 - val_loss: 0.9788 - val_acc: 0.5000\n",
            "Epoch 206/300\n",
            "\n",
            "Epoch 00206: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3687 - acc: 0.8452 - val_loss: 1.3708 - val_acc: 0.5100\n",
            "Epoch 207/300\n",
            "\n",
            "Epoch 00207: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3545 - acc: 0.8323 - val_loss: 1.3580 - val_acc: 0.5000\n",
            "Epoch 208/300\n",
            "\n",
            "Epoch 00208: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3532 - acc: 0.8323 - val_loss: 1.2779 - val_acc: 0.5000\n",
            "Epoch 209/300\n",
            "\n",
            "Epoch 00209: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3544 - acc: 0.8468 - val_loss: 1.2660 - val_acc: 0.5000\n",
            "Epoch 210/300\n",
            "\n",
            "Epoch 00210: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3755 - acc: 0.8290 - val_loss: 1.4938 - val_acc: 0.5000\n",
            "Epoch 211/300\n",
            "\n",
            "Epoch 00211: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3565 - acc: 0.8452 - val_loss: 1.1900 - val_acc: 0.4900\n",
            "Epoch 212/300\n",
            "\n",
            "Epoch 00212: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3533 - acc: 0.8323 - val_loss: 1.5150 - val_acc: 0.5000\n",
            "Epoch 213/300\n",
            "\n",
            "Epoch 00213: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3313 - acc: 0.8516 - val_loss: 1.1741 - val_acc: 0.4900\n",
            "Epoch 214/300\n",
            "\n",
            "Epoch 00214: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3820 - acc: 0.8194 - val_loss: 1.3364 - val_acc: 0.5100\n",
            "Epoch 215/300\n",
            "\n",
            "Epoch 00215: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3514 - acc: 0.8355 - val_loss: 1.1396 - val_acc: 0.5000\n",
            "Epoch 216/300\n",
            "\n",
            "Epoch 00216: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3716 - acc: 0.8339 - val_loss: 1.5661 - val_acc: 0.5000\n",
            "Epoch 217/300\n",
            "\n",
            "Epoch 00217: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3766 - acc: 0.8177 - val_loss: 1.1077 - val_acc: 0.5000\n",
            "Epoch 218/300\n",
            "\n",
            "Epoch 00218: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3643 - acc: 0.8548 - val_loss: 1.2228 - val_acc: 0.5000\n",
            "Epoch 219/300\n",
            "\n",
            "Epoch 00219: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3774 - acc: 0.8242 - val_loss: 1.1782 - val_acc: 0.4800\n",
            "Epoch 220/300\n",
            "\n",
            "Epoch 00220: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3759 - acc: 0.8226 - val_loss: 1.1567 - val_acc: 0.5000\n",
            "Epoch 221/300\n",
            "\n",
            "Epoch 00221: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3349 - acc: 0.8661 - val_loss: 1.3639 - val_acc: 0.5100\n",
            "Epoch 222/300\n",
            "\n",
            "Epoch 00222: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3642 - acc: 0.8419 - val_loss: 1.4064 - val_acc: 0.5100\n",
            "Epoch 223/300\n",
            "\n",
            "Epoch 00223: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3638 - acc: 0.8403 - val_loss: 1.0949 - val_acc: 0.4700\n",
            "Epoch 224/300\n",
            "\n",
            "Epoch 00224: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3488 - acc: 0.8500 - val_loss: 1.3641 - val_acc: 0.5100\n",
            "Epoch 225/300\n",
            "\n",
            "Epoch 00225: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3557 - acc: 0.8323 - val_loss: 1.3689 - val_acc: 0.5100\n",
            "Epoch 226/300\n",
            "\n",
            "Epoch 00226: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3504 - acc: 0.8629 - val_loss: 1.4194 - val_acc: 0.4800\n",
            "Epoch 227/300\n",
            "\n",
            "Epoch 00227: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3421 - acc: 0.8694 - val_loss: 1.3155 - val_acc: 0.5000\n",
            "Epoch 228/300\n",
            "\n",
            "Epoch 00228: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3595 - acc: 0.8274 - val_loss: 1.0318 - val_acc: 0.5000\n",
            "Epoch 229/300\n",
            "\n",
            "Epoch 00229: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3578 - acc: 0.8484 - val_loss: 1.2555 - val_acc: 0.5000\n",
            "Epoch 230/300\n",
            "\n",
            "Epoch 00230: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3285 - acc: 0.8500 - val_loss: 1.4340 - val_acc: 0.5100\n",
            "Epoch 231/300\n",
            "\n",
            "Epoch 00231: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3211 - acc: 0.8661 - val_loss: 1.7004 - val_acc: 0.5000\n",
            "Epoch 232/300\n",
            "\n",
            "Epoch 00232: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3412 - acc: 0.8371 - val_loss: 1.3479 - val_acc: 0.5000\n",
            "Epoch 233/300\n",
            "\n",
            "Epoch 00233: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3341 - acc: 0.8532 - val_loss: 1.4953 - val_acc: 0.5000\n",
            "Epoch 234/300\n",
            "\n",
            "Epoch 00234: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3465 - acc: 0.8403 - val_loss: 1.5725 - val_acc: 0.5100\n",
            "Epoch 235/300\n",
            "\n",
            "Epoch 00235: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3711 - acc: 0.8339 - val_loss: 1.4432 - val_acc: 0.5100\n",
            "Epoch 236/300\n",
            "\n",
            "Epoch 00236: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3634 - acc: 0.8258 - val_loss: 1.3069 - val_acc: 0.5000\n",
            "Epoch 237/300\n",
            "\n",
            "Epoch 00237: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3483 - acc: 0.8435 - val_loss: 1.5146 - val_acc: 0.5000\n",
            "Epoch 238/300\n",
            "\n",
            "Epoch 00238: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3819 - acc: 0.8323 - val_loss: 1.4978 - val_acc: 0.5000\n",
            "Epoch 239/300\n",
            "\n",
            "Epoch 00239: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3729 - acc: 0.8306 - val_loss: 1.0020 - val_acc: 0.4900\n",
            "Epoch 240/300\n",
            "\n",
            "Epoch 00240: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3586 - acc: 0.8484 - val_loss: 1.6715 - val_acc: 0.5100\n",
            "Epoch 241/300\n",
            "\n",
            "Epoch 00241: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3385 - acc: 0.8468 - val_loss: 1.0998 - val_acc: 0.5000\n",
            "Epoch 242/300\n",
            "\n",
            "Epoch 00242: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3636 - acc: 0.8290 - val_loss: 0.8208 - val_acc: 0.5400\n",
            "Epoch 243/300\n",
            "\n",
            "Epoch 00243: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3459 - acc: 0.8468 - val_loss: 1.2742 - val_acc: 0.5000\n",
            "Epoch 244/300\n",
            "\n",
            "Epoch 00244: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3399 - acc: 0.8435 - val_loss: 1.4344 - val_acc: 0.5100\n",
            "Epoch 245/300\n",
            "\n",
            "Epoch 00245: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3491 - acc: 0.8355 - val_loss: 1.2107 - val_acc: 0.4900\n",
            "Epoch 246/300\n",
            "\n",
            "Epoch 00246: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3291 - acc: 0.8565 - val_loss: 1.3480 - val_acc: 0.5000\n",
            "Epoch 247/300\n",
            "\n",
            "Epoch 00247: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3552 - acc: 0.8500 - val_loss: 1.4164 - val_acc: 0.5100\n",
            "Epoch 248/300\n",
            "\n",
            "Epoch 00248: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3492 - acc: 0.8452 - val_loss: 1.4680 - val_acc: 0.5100\n",
            "Epoch 249/300\n",
            "\n",
            "Epoch 00249: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3473 - acc: 0.8419 - val_loss: 1.3315 - val_acc: 0.5100\n",
            "Epoch 250/300\n",
            "\n",
            "Epoch 00250: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3527 - acc: 0.8355 - val_loss: 1.1500 - val_acc: 0.5000\n",
            "Epoch 251/300\n",
            "\n",
            "Epoch 00251: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3507 - acc: 0.8419 - val_loss: 1.3552 - val_acc: 0.5000\n",
            "Epoch 252/300\n",
            "\n",
            "Epoch 00252: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3619 - acc: 0.8371 - val_loss: 1.3358 - val_acc: 0.4900\n",
            "Epoch 253/300\n",
            "\n",
            "Epoch 00253: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3728 - acc: 0.8145 - val_loss: 1.3385 - val_acc: 0.4900\n",
            "Epoch 254/300\n",
            "\n",
            "Epoch 00254: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3344 - acc: 0.8484 - val_loss: 1.3014 - val_acc: 0.5000\n",
            "Epoch 255/300\n",
            "\n",
            "Epoch 00255: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3345 - acc: 0.8452 - val_loss: 0.9279 - val_acc: 0.5700\n",
            "Epoch 256/300\n",
            "\n",
            "Epoch 00256: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3538 - acc: 0.8419 - val_loss: 1.6031 - val_acc: 0.5000\n",
            "Epoch 257/300\n",
            "\n",
            "Epoch 00257: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3673 - acc: 0.8323 - val_loss: 1.0380 - val_acc: 0.5000\n",
            "Epoch 258/300\n",
            "\n",
            "Epoch 00258: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3268 - acc: 0.8613 - val_loss: 1.5245 - val_acc: 0.5000\n",
            "Epoch 259/300\n",
            "\n",
            "Epoch 00259: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3749 - acc: 0.8403 - val_loss: 1.2272 - val_acc: 0.5000\n",
            "Epoch 260/300\n",
            "\n",
            "Epoch 00260: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3128 - acc: 0.8726 - val_loss: 1.1975 - val_acc: 0.5000\n",
            "Epoch 261/300\n",
            "\n",
            "Epoch 00261: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3391 - acc: 0.8516 - val_loss: 1.2108 - val_acc: 0.5100\n",
            "Epoch 262/300\n",
            "\n",
            "Epoch 00262: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3162 - acc: 0.8758 - val_loss: 1.4520 - val_acc: 0.5100\n",
            "Epoch 263/300\n",
            "\n",
            "Epoch 00263: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3568 - acc: 0.8548 - val_loss: 1.2876 - val_acc: 0.4900\n",
            "Epoch 264/300\n",
            "\n",
            "Epoch 00264: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3537 - acc: 0.8403 - val_loss: 1.4032 - val_acc: 0.5100\n",
            "Epoch 265/300\n",
            "\n",
            "Epoch 00265: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3477 - acc: 0.8548 - val_loss: 1.3488 - val_acc: 0.5100\n",
            "Epoch 266/300\n",
            "\n",
            "Epoch 00266: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3232 - acc: 0.8742 - val_loss: 1.6161 - val_acc: 0.5000\n",
            "Epoch 267/300\n",
            "\n",
            "Epoch 00267: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3614 - acc: 0.8226 - val_loss: 1.6380 - val_acc: 0.5000\n",
            "Epoch 268/300\n",
            "\n",
            "Epoch 00268: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3882 - acc: 0.8177 - val_loss: 1.3813 - val_acc: 0.5100\n",
            "Epoch 269/300\n",
            "\n",
            "Epoch 00269: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3287 - acc: 0.8403 - val_loss: 1.6215 - val_acc: 0.5100\n",
            "Epoch 270/300\n",
            "\n",
            "Epoch 00270: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3339 - acc: 0.8532 - val_loss: 1.4607 - val_acc: 0.5000\n",
            "Epoch 271/300\n",
            "\n",
            "Epoch 00271: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3816 - acc: 0.8226 - val_loss: 1.4434 - val_acc: 0.5000\n",
            "Epoch 272/300\n",
            "\n",
            "Epoch 00272: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3394 - acc: 0.8419 - val_loss: 1.5111 - val_acc: 0.5000\n",
            "Epoch 273/300\n",
            "\n",
            "Epoch 00273: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3476 - acc: 0.8452 - val_loss: 1.3700 - val_acc: 0.5100\n",
            "Epoch 274/300\n",
            "\n",
            "Epoch 00274: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3550 - acc: 0.8306 - val_loss: 1.0953 - val_acc: 0.4800\n",
            "Epoch 275/300\n",
            "\n",
            "Epoch 00275: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3346 - acc: 0.8645 - val_loss: 1.0430 - val_acc: 0.5000\n",
            "Epoch 276/300\n",
            "\n",
            "Epoch 00276: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3518 - acc: 0.8452 - val_loss: 1.1226 - val_acc: 0.5100\n",
            "Epoch 277/300\n",
            "\n",
            "Epoch 00277: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3468 - acc: 0.8452 - val_loss: 1.1618 - val_acc: 0.5000\n",
            "Epoch 278/300\n",
            "\n",
            "Epoch 00278: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3167 - acc: 0.8726 - val_loss: 1.3007 - val_acc: 0.5100\n",
            "Epoch 279/300\n",
            "\n",
            "Epoch 00279: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3179 - acc: 0.8758 - val_loss: 1.2520 - val_acc: 0.5100\n",
            "Epoch 280/300\n",
            "\n",
            "Epoch 00280: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3113 - acc: 0.8806 - val_loss: 1.0994 - val_acc: 0.4700\n",
            "Epoch 281/300\n",
            "\n",
            "Epoch 00281: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3225 - acc: 0.8694 - val_loss: 1.1666 - val_acc: 0.5200\n",
            "Epoch 282/300\n",
            "\n",
            "Epoch 00282: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3412 - acc: 0.8468 - val_loss: 1.6753 - val_acc: 0.5100\n",
            "Epoch 283/300\n",
            "\n",
            "Epoch 00283: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3582 - acc: 0.8323 - val_loss: 1.2392 - val_acc: 0.4900\n",
            "Epoch 284/300\n",
            "\n",
            "Epoch 00284: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3147 - acc: 0.8500 - val_loss: 1.1460 - val_acc: 0.5000\n",
            "Epoch 285/300\n",
            "\n",
            "Epoch 00285: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3265 - acc: 0.8645 - val_loss: 1.4074 - val_acc: 0.5100\n",
            "Epoch 286/300\n",
            "\n",
            "Epoch 00286: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.2997 - acc: 0.8677 - val_loss: 0.8801 - val_acc: 0.5300\n",
            "Epoch 287/300\n",
            "\n",
            "Epoch 00287: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3349 - acc: 0.8500 - val_loss: 1.3154 - val_acc: 0.5100\n",
            "Epoch 288/300\n",
            "\n",
            "Epoch 00288: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3561 - acc: 0.8403 - val_loss: 1.3136 - val_acc: 0.4900\n",
            "Epoch 289/300\n",
            "\n",
            "Epoch 00289: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3314 - acc: 0.8516 - val_loss: 1.3673 - val_acc: 0.4900\n",
            "Epoch 290/300\n",
            "\n",
            "Epoch 00290: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3507 - acc: 0.8581 - val_loss: 1.4296 - val_acc: 0.5000\n",
            "Epoch 291/300\n",
            "\n",
            "Epoch 00291: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3551 - acc: 0.8452 - val_loss: 1.3387 - val_acc: 0.5000\n",
            "Epoch 292/300\n",
            "\n",
            "Epoch 00292: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3560 - acc: 0.8532 - val_loss: 1.6931 - val_acc: 0.5100\n",
            "Epoch 293/300\n",
            "\n",
            "Epoch 00293: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3485 - acc: 0.8500 - val_loss: 1.6379 - val_acc: 0.5000\n",
            "Epoch 294/300\n",
            "\n",
            "Epoch 00294: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3771 - acc: 0.8177 - val_loss: 1.3052 - val_acc: 0.5000\n",
            "Epoch 295/300\n",
            "\n",
            "Epoch 00295: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3255 - acc: 0.8661 - val_loss: 1.4814 - val_acc: 0.5000\n",
            "Epoch 296/300\n",
            "\n",
            "Epoch 00296: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3289 - acc: 0.8452 - val_loss: 1.3778 - val_acc: 0.5000\n",
            "Epoch 297/300\n",
            "\n",
            "Epoch 00297: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3687 - acc: 0.8468 - val_loss: 1.2743 - val_acc: 0.5000\n",
            "Epoch 298/300\n",
            "\n",
            "Epoch 00298: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3141 - acc: 0.8839 - val_loss: 1.1131 - val_acc: 0.4900\n",
            "Epoch 299/300\n",
            "\n",
            "Epoch 00299: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3498 - acc: 0.8435 - val_loss: 1.2697 - val_acc: 0.4800\n",
            "Epoch 300/300\n",
            "\n",
            "Epoch 00300: val_loss did not improve from 0.65813\n",
            "620/620 - 1s - loss: 0.3302 - acc: 0.8581 - val_loss: 1.3178 - val_acc: 0.4900\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nOydd3ib1b34P0dbHvJOnNjZg0zIgpYN\nZVNGB7RAgZZCKb8uLi1tobeDS293byfc9tKWNtBSCoUW2oZRNmGEhJBBnL0cJ3a8Ldmy9vn98Q69\nkuVYdqx46Hyex4+kd+m8lvT9nu88QkqJQqFQKPIX20gPQKFQKBQji1IECoVCkecoRaBQKBR5jlIE\nCoVCkecoRaBQKBR5jlIECoVCkecoRaDIC4QQ04UQUgjhyOLYTwghVh+LcSkUowGlCBSjDiHEPiFE\nRAhRmbb9HV2YTx+ZkSkU4xOlCBSjlb3A1cYLIcRioGDkhjM6yMaiUSgGi1IEitHKg8D1ltcfBx6w\nHiCEKBFCPCCEaBFC7BdCfF0IYdP32YUQPxZCtAoh9gDvz3Du74QQjUKIg0KI/xZC2LMZmBDiUSFE\nkxCiSwjxihBioWWfVwjxP/p4uoQQq4UQXn3faUKI14UQnUKIA0KIT+jbXxJC3GS5RoprSreCPiuE\n2Ans1Lf9XL+GXwjxthDidMvxdiHE14QQu4UQAX3/FCHEvUKI/0m7lyeFELdlc9+K8YtSBIrRypuA\nTwgxXxfQVwF/TDvml0AJMBM4E01x3KDv+xRwCbAUWAFckXbuH4AYMFs/5nzgJrLjKWAOMAFYD/zJ\nsu/HwHLgFKAc+AqQEEJM08/7JVAFLAE2ZPl+AB8A3gMs0F+v1a9RDjwEPCqE8Oj7vohmTV0M+IBP\nAkFgJXC1RVlWAufq5yvyGSml+lN/o+oP2IcmoL4OfA+4EPg34AAkMB2wAxFggeW8TwMv6c9fAG6x\n7DtfP9cBTATCgNey/2rgRf35J4DVWY61VL9uCdrEqhc4IcNxdwJ/6+caLwE3WV6nvL9+/fcNMI4O\n432B7cDl/Ry3FThPf/45YNVIf97qb+T/lL9RMZp5EHgFmEGaWwioBJzAfsu2/UCN/nwycCBtn8E0\n/dxGIYSxzZZ2fEZ06+Q7wJVoM/uEZTxuwAPsznDqlH62Z0vK2IQQtwM3ot2nRJv5G8H1I73XSuBa\nNMV6LfDzoxiTYpygXEOKUYuUcj9a0Phi4PG03a1AFE2oG0wFDurPG9EEonWfwQE0i6BSSlmq//mk\nlAsZmGuAy9EslhI06wRA6GMKAbMynHegn+0APaQGwqszHGO2CdbjAV8BPgKUSSlLgS59DAO91x+B\ny4UQJwDzgb/3c5wij1CKQDHauRHNLdJj3SiljAOPAN8RQhTrPvgvkowjPAJ8QQhRK4QoA+6wnNsI\nPAv8jxDCJ4SwCSFmCSHOzGI8xWhKpA1NeH/Xct0EcD/wEyHEZD1oe7IQwo0WRzhXCPERIYRDCFEh\nhFiin7oB+JAQokAIMVu/54HGEANaAIcQ4ptoFoHBb4FvCyHmCI3jhRAV+hgb0OILDwKPSSl7s7hn\nxThHKQLFqEZKuVtKua6f3Z9Hm03vAVajBT3v1/f9BngG2IgW0E23KK4HXEAdmn/9r8CkLIb0AJqb\n6aB+7ptp+28HNqMJ23bgB4BNSlmPZtl8Sd++AThBP+enaPGOw2iumz9xZJ4BngZ26GMJkeo6+gma\nInwW8AO/A7yW/SuBxWjKQKFASKkWplEo8gkhxBloltM0qQSAAmURKBR5hRDCCdwK/FYpAYWBUgQK\nRZ4ghJgPdKK5wH42wsNRjCKUa0ihUCjynJxaBEKIC4UQ24UQu4QQd2TYP00I8bwQYpNeZl+by/Eo\nFAqFoi85swj0wpsdwHmAkbJ2tZSyznLMo8A/pZQrhRDvA26QUl53pOtWVlbK6dOn52TMCoVCMV55\n++23W6WUVZn25bKy+CRgl5RyD4AQ4mG0Qpw6yzEL0HK/AV4ki+KW6dOns25df9mECoVCociEEGJ/\nf/ty6RqqITW3uYFk+b/BRuBD+vMPAsVG4YtCoVAojg0jnTV0O3CmEOIdtO6RB4F4+kFCiJuFEOuE\nEOtaWlqO9RgVCoViXJNLRXCQ1F4vtST7wAAgpTwkpfyQlHIp8J/6ts70C0kp75NSrpBSrqiqyuji\nUigUCsUQyWWMYC0wRwgxA00BXIXWsMtE74fervdouZNke4BBEY1GaWhoIBQKHeWQxw4ej4fa2lqc\nTudID0WhUIxxcqYIpJQxIcTn0Pqi2IH7pZRbhBB3A+uklE8CZwHfE0JItHbDnx3KezU0NFBcXMz0\n6dOxtBUet0gpaWtro6GhgRkzZoz0cBQKxRgnp+sRSClXAavStn3T8vyvaM2+jopQKJQ3SgBACEFF\nRQUqXqJQKIaDkQ4WDxv5ogQM8u1+FQpF7hg3ikChUCjGG1JKHl/fQE84ltP3UYpgGGhra2PJkiUs\nWbKE6upqampqzNeRSCSra9xwww1s3749xyNVKBRjifr2IF98ZCOrNjfm9H3UmsXDQEVFBRs2bADg\nrrvuoqioiNtvvz3lGGORaJsts+79/e9/n/NxKhSKY8MTGw7yq5d288TnTsXtsA/5Op3BKAD+kLII\nxiy7du1iwYIFfOxjH2PhwoU0NjZy8803s2LFChYuXMjdd99tHnvaaaexYcMGYrEYpaWl3HHHHZxw\nwgmcfPLJNDc3j+BdKBSKwSCl5NaHN7CtKcDOw91Hda2ArgC6c6wIxp1F8F//2ELdIf+wXnPBZB/f\nujSbdc37sm3bNh544AFWrFgBwPe//33Ky8uJxWKcffbZXHHFFSxYsCDlnK6uLs4880y+//3v88Uv\nfpH777+fO+7o07xVoVAMM09tbsTttPG+eROHfI2XdiSz+eoO+VlUUzLka3WHNYugJ6IsgjHNrFmz\nTCUA8Oc//5lly5axbNkytm7dSl1dXZ9zvF4vF110EQDLly9n3759x2q4ijFIW3eYK3/9Oo1dah36\noRJPSOIJyc+f38k9L+wyt//k2e3c9eSWrK/TG4lz9z/qqC3z4rLb2HKo66jGZbiEAsoiGBxDnbnn\nisLCQvP5zp07+fnPf85bb71FaWkp1157bcZqaJfLZT632+3EYrn9EijGNtubAqzd18HGA11MKvEO\nfIKiDzeuXEtNqZfW7ggeZ3J+/AtdKdx1WXZy5eG19ext7eGhm97DT/69g7rGo/NOGApAZQ2NI/x+\nP8XFxfh8PhobG3nmmWdGekiKcUBPROvT2NWbXYbaaKIlEOb+1XsZ6ZUSdx7u5t1Dftp7wjT7w0Me\nz4H2XorcDk6ZXcnCyT7qDvlJJIZ+b0ZsoFspgvHDsmXLWLBgAfPmzeP666/n1FNPHekhKcYBQd1/\n3NUbHeGRDJ6r7nuDu/9Zx6Guke0T5g9F2Xk4QEJCJJ6gMxglFk8M+jodwQhlhVr/r9kTi+mJxGnp\nDpv73z3YxU0r15mf2UAEQtpnmmtFMO5cQyPNXXfdZT6fPXu2mVYKWjXwgw8+mPG81atXm887O5MN\nWK+66iquuuqq4R+oYtTxj42H+NlzO3j2tjOx27KvHO8JaxaBkWo4Gnl8fQP3vbKHp2493ayKl1Ky\nu6UHSLo+Nh7o5KYH1vHUradTWeQ+JmOLJ2QfH/zhQIhQLLuGjr2ROJ29ESaVeGnviVBeoLl2JxRr\n428JhJno8xCKxrnkl9rvfGujn+XTyjNe70B7kNN/+CIP3njSMcsaUhaBQjFK2HCgk90tPXQGB+fi\nMWaXnaPYIqg75GdbUyAlH37DgeSEx5jxrt3XTksgzP62nmM2NmPWbeWwP0yjxUo5kqvovlf2cMkv\nViOlpCMYoVRXBFWGItAtgqfeTRaFHezs3wLa1KAFmP/45n7z/6KyhhSKPKFNFxgdWSiCzmCEOx/f\nRE84RtCIEYxii8AQZG0WN8n6+qQiCOpWzYH2IAAdPcfmXh54Yx+Prz/YZ/thf4hGi7AOx/p3Ex3q\n7KWtJ0JXb1SzCAp1RaBbNC1+7Z6tNQUHO/rP8CpwawVogVAMv+EaUllDCkV+0NqtKYD2LITgm3va\n+PNbB3j/4smmkO0cxcFiw33V2h1hpr62lNXyMWa+9YYiGKRVNFR+9dJuovG+s/1mfwiPM1kRHIrG\nU15bMYT1oc4QHT0RyvqxCPa19TCzspD2YISDnUHz/LbuMBUWN1hEVzqBUMx0EQbCMTYc6GRWVSHF\nnuFfg0RZBArFKKFVFxjtPdlYBJrwaekO0RsZ/TECIwbQarEIrMHtnjRFMNR7Wb2zdVCplh3BSMqY\n7DaBz+PgsD9Mk8U1ZFhdmTAUQX17Dz2ROOV6sNjjtFPscdAS0K6/p6WH6ZWF1JR6TYvgtV2trPjO\ncylFsMbnGQhFTQUZiSX4wL2v8djbDVnf22BQikChGCUYFkE2s2FDiLYEwuZse7RlDb17sIuH1tQD\nmV1DncEoxR6HuT+RkBzQBeRQrJv2ngjX/m4Nn/zD2qyOD0XjhKKpLp+KQheTSryaa8ifVAS90SMo\ngl7t3uoaAwCUFSbrgCYUu2kJhEkkJPvaephRWcjkUi8HO7X7fHz9QaSETQ0WN5muCLrDsT7xixlV\nRVnd22BRikAx5tna6OfSX642Z2ZjkURC0t4zCIvAogjM9NERtAiMpopW/vxWPf/1D60q11BWLd3J\ne+vsjVJTqhXAdYdjtHSHTbfIpoYuPvp/b2T1vzAwKqvX7G3Pqg4g3eqwCagscjOp1ENDRy8N7Un3\njTFLf+CNfXzziXdTzjO+d8as3sgaAs091BIIa1lI0USKRRCJJfh3XRMAr+xs4cO/ep1tTX7z8/SH\nYgRCMUq8SVfQjIpkgepwohTBMDAcbagB7r//fpqamnI40vHJz57bweaDXaze2TrSQxkyHcEIRt1R\nx2BcQ4GwOYMMhGNE+8l9f/dgF+FY/7Pao+U7/9rKx367JmVbIBQjHEsQisZTLIJQNE7dIT9dwQgT\nfB7sNkFPOGa6hQBe3dnKmr3tvLW3PesxNPuT1saWLPqNpVtesycUManEw4zKQva19bC3VZvBQ9Ii\n+OYTW3jgjf0p5/l7DUWgZftYLYKqYg8t3WH26mmyMysLqS3z0hOJ89zWw/hDMRw2warNTby9v4Mv\nPbLRzKyKxBIEI3EmlXjM69WU5aZyXCmCYcBoQ71hwwZuueUWbrvtNvO1tV3EQChFMDSMGdNIukZe\n39XK2/s7BnXO9qYAj6/XfL5tFuHfnpVrSDum2WIRQFIoWWkJhLnsntU8ui43/mWATQe72Nmc2mnT\ncGt09UZTYgS/W72Xy+9dTUNHL2UFTgpddg609/KtJ7ZgtwnTSgDY1Rzo816JhOSBN/aZGUYGhy2u\nnMfWZ77XJzYcNPv/WBWBy27jfz+2nLsuW8jMykKCkTj+UIwFk31A3xiBYXFIKU3BbRTFlVsVQZFm\nEexr08Y6raKAKeUF5lgAzpirRc+F0BTYC9sOp7xXtUURDKa+ZDAoRZBjVq5cyUknncSSJUv4zGc+\nQyKRIBaLcd1117F48WIWLVrEL37xC/7yl7+wYcMGPvrRjw7aksh3DEUwksHSu/9Zx0/+nf3CQsFI\njAt+9gpffGQjAK2B5Gw2G4sgU4wAMtcS7GvrISFhd8vRtUS28tCaen7/2l7zdVNXqI8iNgKdncGo\nmR7a1h3hrb3tROOStp4IpV4nRW4H/9x0iLpGP/dcvZT5k3zmNXZkaOO86WAX33xiC6f/8EW2NwX4\nzJ/eZsOBTpr1/+H7F0/i0XUNGV2FX//7u3xq5ToCoWjK98XndTJ7QhFTyguYXpl0vyzQx9IbiaW4\nqYy2Hj2ROPG0FhJlaa6h7nCMnc0B7DZBtc/D8bVaN9LntjZTW+Zl+bQyAN4zQyswa0hLLTX6R7ns\nuRPX4y999Kk7oGnz8F6zejFc9P1Bn/buu+/yt7/9jddffx2Hw8HNN9/Mww8/zKxZs2htbWXzZm2c\nnZ2dlJaW8stf/pJ77rmHJUuWDO/4xznGwh+Gjz1XnPr9Fzhn/gTuvnxRn30tgXCf2drtj26k7pCf\nVbee3uf4+1cnhWgommxDMLnEQ3sGhSalpL49yDTdR5zMGgpTVuDC53HgD8UyWkX1+mw0fQadzpW/\nfp0LF03ixtNmZNz/5Uc3Uuh2cNdlC3ngjX1E4gmaukIc7OylqStEJJ5ISbM0qmI7ghHTNdTSHU6x\nHEoKXBS6HSSkNiM+b8FEXtiWXH/Demx9W5CpFQU0dCTv4w+v72XV5iZWbW7iQ8tqqCh0ccuZs/jX\n5kb+8No+vnDOHPNYo4I4EIrxvae2sXByUuGUeJOicIZVEejH9Ebj7DyctE46gxG+/9RW/l2nzd6L\nPQ7zfksLkj79yaXabH7NnnaqfR4cdhuTSrxMKvHQ2BViwSQfx00sBjQF9uae9j4TmvMXTOTPb9Wb\n6ai5QFkEOeS5555j7dq1rFixgiVLlvDyyy+ze/duZs+ezfbt2/nCF77AM888Q0nJ0PuVK5Jme5N/\neBXByztaaLa4Gw529vbxDwNE4wnag5E+QvivbzdQ1+jP6Js3WiuAJjDb9CDqnInFGSuLf/rcTs78\n0Utmxa0hLDqDUTqCESbr7pTOYIQnNhxMeU/D915/BEXQFYyydl8Hr+5s6feYNXvbeWl7s5kBc6iz\nl9W7Wnn63SYiemzC+j8wBONhf8iMf+xvC6YcU+J1UujWhHBlkRuH3ZbiY9/d0k08IdlyqIszfvQi\nb+xuSynGWmOJITy+/iBVxW4W15Zw8eJq7nlhF7uaAzxXd5iDnb1mUVZVsZuH1tTz1GbNDSsEKQHZ\nySVeXA4bDptgri6kg5F4ilL6d91h/vhmPYf179yt58zhyxccx73XLMNpmbnPrNSyfOoa/Sn+/aVT\nSwFN0Zx1XBU/vvIErlwxJXleVSH3XLOUdV8/l3mTtDFctmRyv5/N0TL+LIIhzNxzhZSST37yk3z7\n29/us2/Tpk089dRT3HvvvTz22GPcd999IzDC8UFvVPuBNw1jP/5EQnLTyrXcfMZMvnzBvJR9Ukqz\nXw5oWT5S9h+j2NoYYMkU7Yf/2q5WtjUFUnLXO4MRnth4iMoiF9MrClhf38Gu5gB/WlPP19+/ALtN\ncO+LWjvkg529TKsopKs3SqHLTk8kTmcwyrKpZWxrCrBmTzv/98oe/vdjy7h48SQgaQkcaO/tM3aD\nXS3abLe/FbWklBz2a7P+/e1BM+1yx+EAMYtrpKs3ykSfNgs2XDNGquSSKaUpbSUA0zUEMNGnzXgN\noTyzspA9rT3UtwfZpQvh9fUdHPaHKPY4qCh0sUdXqEVuB93hmPned1++iJe3v8h3V23jxe3NnDNv\ngtmi/gvnzOHeF3axelcrHqeNUq8rRRHYbILpFQXE4tIcW28knmJR/eaVPSn3Ma/ax2lzKvv836ZX\nFpjPrbGPZVPLWLW5iQWTfDjsNq5YXgtAgctOMBKn0OXgkuOTgv/Vr5ydcv5woyyCHHLuuefyyCOP\n0NqqZbO0tbVRX19PS0sLUkquvPJK7r77btavXw9AcXExgUDf4JjiyBgWQeMgO1h29UY56TvP8eae\ntj77eiIxonFpCndrK+F0H66RrRIIxYgnJHc+vplv/7MOw1P0Tn0yiPzHN/fzs3/vSPE3/271XjYe\n6ORbly6koshNIBTjN6/s5fev7WNro5+Onojph24JhInGE3SHY8yekMwpNzJLDIFpDT4blkBvNM6J\n33mOl3f0nfUbCuBgZ2/GTpf+Xi0DSEp4fmsymJlelWv8v6SU5nWMGfx1753G9z+0mKtPmsJUPWBa\nWuCkUG+pMLFYuwfDx37pCZogXL2zxfyf1x3yc6izl5pSrznD9jhtXHqCpvQqirRzK4vcnKu7maSE\n57c1m0poQrGbU2ZVmO91/SnT+MDSmpT7uO7k6Vx38jQKXNrYeiNxGjp6zc/0UFfI3Afg82aeUxd7\nnGbzPKsgv2jxJC5cWM179XEYGArJ60qtYp5SXoAtR4FiUIogpyxevJhvfetbnHvuuRx//PGcf/75\nHD58mAMHDnDGGWewZMkSbrjhBr773e8CcMMNN3DTTTflRbD4gGWWNxRC0Tiv7mwhkZCmIjjsDw2q\n9/vave00B8LmbNtKckEQ7doRS1pmempiS3dSATUHQvz5rXp+t3ovXt1Xbu2pU98eJBCO0dDRy7QK\nTRi+ta8dn8fBJcdPMi0HI+vlnfoOXt2VTIs97A+ZmUFLp5aZ2w1FsE93HVmzhw50BE1h1Nod4cEM\n7i1rUHa3/rlsa/KbrqjDgeQ9PluXmtViZVdzN5saOumJxDFS+Q/pFkGh28FVJ03lex86nlpdiGuK\nQBOiE/TZvHEvFy2uZvaEIlZtbjKtirpGPw0dmiKYrAdRJ5d6ee9MTaBaU0gvWqQph9oyLzYh+MPr\n+wDweZyma6Y7HOMzZ83m8iVpiuC907jh1Bk47TacdkGvHscxXEWA+Z7GNftjhm4VWF1DNaVefn3d\n8j7nGYqgwDX0Be+HwvhzDY0w1jbUANdccw3XXHNNn+PeeeedPts+8pGP8JGPfCRXQxuQeELyyxd2\ncsMpMygpGP5+Jlbu/mcdzf4QT3zutCGd/+Ab+/nOqq2cO3+CmeMdjUtae8JMKPYMcLZGk+7/z3R8\nIG2JwLClArXuUBcXLqo2X7dYMn7+/s4h87mRWfLqzhZ6I3E8TpsZuO3qjbJkSin724IcaA8ypawA\nIQQnz6qgxOs0Z9bv1HeaAUvQumIamUFG9gloM0+fx8GB9l7z+qApzMP+MJcvmcwTG7SxTfD1DTru\nbA6Y77vjcIDFNSW8/xeriSckv752mSmsAd7a247dJvpkywDc+biWAHH1SVPNbYYQL7Jcw5gdl3hd\nfVxDZ86t4onPnsq8ah8XL6rmnhd3EdDX7t3b2oNNwEkzys00zZpSL6fM0twylxw/yXyPs46roqrY\nzfUnT2Pl6/vZqq8W5vM6TCWazRKQHqfmrmkJhFk+TXPBgea6ekE/xupaSmdGZSFr93Vk5doxFEOh\n69iKZmURKEy2Nvr52XM7ebYu97UMbd1hs6UCwMOWKtSBiCckmw9queDPbW1Omc0aQtma521gLDTS\nHY5x7W/XsGqz1ha4yN139mUuGq67N6zB1+2HNR//rQ+/w8Nv1acogr+srU+5zsWLq+kMRvnE79/i\n9kc3EbC4XYzslGhcmkLNabdx/gJt4fRpFQW8c6CT9p4IDt1v/dbedt7/i1cBLV/d6HnvddkpKXAm\ng7bBqBnUBThjThWn6z7s9u5Ua1NKyfamAGfOrcLlsLGzuZttTQFT0N/+6Cbe0a0aY6Y6u6qIQv35\nwsm+FCEPWlWxwSG9i2eB5f9szI6twWLDv2+zCU7QLaPzF1aTkPDuQb/5fgmpCX9DsNaWeakqdrPz\nOxdxlUUBeZx2Xvvq+/jU6TOpKnablmOJ18m86uTMfiA0v32M1u4wU8sLcDk0sWnN7zdaZWTCSEed\nnI0i6Mc1lGuURaAwMYJ7rd25d0sFLKmOUkrueXEXoWhiwDWnH1pTz9f+tjml2rK1O2ym73X1Rtnc\n0MUVv36d/7tuOTeuXMd1753GNe+ZyqW/XM3KT55EPCFZbXG3+DPMCv1pSwRa2xDvbO7mxpXr2Hig\nk+1NATP/GzALhwzOWzCR+vYga/a2p2S4AKZrCFKLkD5z9mxqywpw2AU/emY7+9p6KCt0MdHnSblG\nZZGb6RWFNAfCeJ12Sr0uDqDNvne1dDPza6s4Z94EQMtO+fDy2oxtG17e0UJzIMxZx1Wxv62HTQ2d\npoL56y0nc/39b/GTf+8A4L8uW8jqXa2ct2Aiv3h+Jy2BMHdeNJ+W7hC3/WVjn/+j9X9oVRYfXlaL\n026jsqivRWBlwSQf5YUu2nsiXLakhjd2t7KvLci0igJTaBoKwZkhz94Q2tbUS5/XicNu40dXHG+m\n4x4Jr9NOY1eIaFxSVeym1OukORBmUomXyiI3rd1hHEfI8f/g0hoisQQzKwd+r5FyDY0bi2Ck1zw9\n1uTifg0zuTlw9MsGvrqzhTd29w3CGvj1zoqxeIJ3D2p+30wLhKRjzDQbu0JmwE9KTH9xVzDKa7tb\nCccS/P2dg8QTkj+8vo/P/mk94ViCx95u6JNPnynbJ33R8JDufppRWci+1h426oFHw3dsjeNV+5JK\nqsTr5FcfW85XL0zNPAJ0d5D23NqGeEZlIbeeO8e0GLYc8lNW4DRnzNU+Dys/eRKLakrMrBRtQZSk\ne8JoYvb8tmZsIml9VBS5aO0J8+i6A+b/4f7X9jGh2M0lx09m6dQyNjV08fKOFmZWFbJiejkfXqZl\ntDjtgitXTOHnVy3lkuMnM3+Sj4WTSzhtTiUfXFpLsS7QV0xLxi58lpmyVbhNKS/gs2fPRghhzvQz\nuehsNmEWdc2qKuTZ287k/k+s4LwF1cyuKsJltzGv2tfnvHQMxSYEFOlulytXTOGkGZlXCbPidTnY\nryv4qmK3+X+uLvHw1K2n87fPnHLE8yeVePmPc+dmFexNKgLlGho0Ho+Htra2vFEGUkra2trweLLz\nhWeLIfysro6h8tW/buLq37zJD57elnHtV6sP/l+6iyYcS5hNx/rDmp1xiiXjwjDTO3ujZvMvY6Wn\nqmK3mQP+bN3hPq0QMisCbVsgzSJYVFNi5sSfUFtCY2eIw/5wyszSCPiC9sOeUl7AJ0+bbgaPnXZN\nIFQWu83ZcEVh31Ykxgx5f1uQsoKkG+iU2RWcqbcluOn0mRS47Jw5tyrFT23N5plWUWgWeZUXumjo\n6OXLf93EuT95mXAszuqdLXxoWS0uh42lU0sJRuK8urOVs4/TrAkjDTU9Q+gHHz6e+65fbr42Zuin\nzk6mURrtFIA+7iODE6aUsmCSL6WQy8pxuhvHYRO4HDbeN28idptggs/Dmq+dwznzJ2Q8z4phERS7\nHYPOvilw2c3Mq6oiN6Ve7bOaVOKhqtidErQ/WlSw+Ciora2loaGBlpb+i2HGGx6Ph9ra2n73JxKS\n5kA4xY85EH5L24KhctPKtSyZUkqTP8TkEg+/emk3oWjcdPnc+Ie1CJFM+ezqjaYs4RcIRc3Z8e9f\n28vPn9/Jmq+dQ0dPlOoSDynsm1EAACAASURBVPtak7P5k2dVAprLwqjg7AxGzV4ye1p7cDts3Hbu\nXL72t82cfVwVL25v4fH1DUwp93L6nCo2HujMqAi60ywCI0awuMbHPzYewia0FMCNDV1sbujigkXV\n7G3V/PFLppby9BYtzmL8sN0OOyfNKGfLoS6K3A72tQWpKHTh8zgJhGIpriED6wy5vNBlNpSzpo3O\nnVhM3d0XprxXOnMsx5cXuk1lG44leG1XKwkJx1Vrxyydogk1IbTMGaDfWXP6Qi0lXicHO3s5ZVYF\nP39+J6C5v4wsq/5muUunlmWsvjb4wvvmEAhF+eDSvt/3sgz/t0wYisB3hKBuf3gt91lV7MbndWIT\nSStjODEqnJUiGAJOp5MZMzKXxecr/9rcyJce3chbXzvHXEN1IEyLQC92emZLE6fNrkzJGLHy2q5W\n5kwoMtP+pJS8srOVHYe7SUi49dw5PL7+oOlGkVLyvKV9AMAbe9rY3xbkpBnlvLW3ne5wzFQE3/5n\nHQkJ7/vxyxzs7GXLf13Aoa5ePnHKdJZOLWVRTdIlUF7owuWw0djVy57WZNXuRJ+HK1fUYhNw8fGT\nOOV7L9ARjLJiejnf/eBi7nhsU58xWf8XwUic1TtbTXfZvGofNqEJ4LkTNeEZiSdYPrWUp99tJBqX\nKRaBVfB845IFHPaH+MXzOzVFUOQyg4xG/rsVa3ZPWaGLufrM+D39CObSfjK95kxMKoLKtPf54dNa\nf6QZegXslHKt/cHimhIzyGm3CR695eQBhZOhiOZYUiw/c9ZsVukVvIa/frCUFDj54RUnDOlcA2PZ\nyCOlefb7/pbPsKrYTW2Zl+kVhUeMCwwVI1vvWAeLx4VrSNGXAx1BIrFESmHRQBjukJZAmIaOIJ9+\n8G3+tbmRRELyw6e3pTQtiyckN/x+Lfe/ts/c1hGMEoklTDO6prSAmlKvWYa/v61vi4OH36rHbhNc\nofuhA6EY6/a189tX95gzXyP9cH19B1Jq5fmXL6nB7bCbwqnA5aDU6+Stve1YPYQTfW6cdhtXnTQV\nn8fJubobwXBDGCmTxj0atQ3WeMW1v1tjNofzeZ2cPKuC8xdWp2SBLKwpocTrRAhYXJNM67QKntkT\nijh1diUTfR48ThsFLoe5P5NF4HbYKdMFQ3mBi6tPnMoLXzqT5dP6UQTe1GsUuuxUFbvN1Err+9gE\nzKsuNlMhjT73Qgge/8wp/M9HUgXvidPLWTj5yK1QSvQq4TKLQlowyUfd3Rfw8pfPOuK5uSZpEQx+\n7mtNSS1yO7j9guN4+Ob3DtvYrBhK51inj44LiyDfSSQkwWg8xQdruDv6W7avOxzr47M1soYCoZhZ\nyenvjbK/Pcj/vrQbt8POredqTbzaeyJE4gmz0dvH73+rz6y2pszLBJ+HloDWg37tvr695Tc2dHFC\nbYnpS/aHojy54RB/fbshpQskYFYAW33JJV4nwUgcr9NOidfJ9sPJHO89rT1mgNXg4sWT+PuGQ0zX\nBZ/P6yQSS7CtKcD/vrQbh93GF8+bm5LmacXjtPGnm96r/5+SymL+JB8+rxOX3Uah20Gxx0Eklsi4\nzu2lJ0w2s54MwZRJEYBm0XQEo5QVurDZBDOPsEKVMZssdjsIhGPMm+Tjsf+XGsg03qe2rICTZpSz\nrSlAeaErpW7E6HY5WN5//CTmTixKaWFhswkKXA6mVYysqDGs1iPl+/fHeXo6L2iKssjt6DfecbSU\n6MpcuYYUg+bXr+zmh09vZ/03zsPjtLG1MWD6+zO1C3h9dyvX/GYNf/7UeznZEnC1FtcYnRaDkWTX\nxUZLLx8jjmAonExtCyaVeJjocxOJJ/jYb9f0269/akWh6SIxUkBjCdmnbfKaPZoimZ6mCBr1cv/S\nAidS72K5bFpZRkVw9rwJ3HnRPN6vz/IMwfD6bi2d1GiH0F+hkdHpFJJFXOWFWgrkxGIPDj0QXFHo\n6ned2/MWTDSFi7EQeUVhZn/zBJ9HF9YDCzDjXqZVFvDuQX/GdEzjfaZXFrJ0aikPvLG/3yDtYDFa\nQgA88x9nsDPDWgIjheESG4pryGG38cinT87YDHC4WTKllK9dPI/T51Tl/L2sKEUwDjBW5vrzW/UU\nuuzc/c86U8Bbe9UbGFk1/9h0qF9FYMyseyIxM8vG2svHiCN09UbN1EorlUVuPE67KYgNJWATkF6Q\nOrXcayqC7pT6Avj0GTN576wKbvj9WjY2dFJZ5E75MZdafKrGbGpyiddsYZAuDJ12G58+c5b52hCe\nhrVxsDPI/av3slnPOErHnebnXlxbwtRyTZD+6MrjzVbU5YUunPaBq1aN9MqyfgT9RN2lUZZFnKfU\nUAQVhbx70J8xHdOwCGZWFpqB4ek5WP7wuOpiM9tnNOB22JlVVcisCUNb8zebNNPhwG4T3HzGrIEP\nHGaUIjjG9Nf98WgwMidWvr6PS0+YTEImm4hlcg3Z9PevT/PZ+0NRc43V7brvuDcS57CuAJqsisC0\nCGIpfW0MQV+TQRB/69IFzKv2cfVv3kx536nlBebMOBCKpmTxzJpQxOmzK7EJLX1xhqWbIyT94l6n\n3VQKU8q9pgJKtwjSMRSBUfOw5aCfN3XLw+jxbyVdEay84STz86wtS47twkXVWbUvOHV2JW09kRRL\nw4ox/v5cR1bmVfs4cXoZZ82t4l+bGjPee3mhi/fNm8C58ycyraKA8xdM5PyFEzNcbfzx9H+cgX2Y\nf3vjBaUIjjHzvvE07188iZ98dPgWnzFWt2oOhM1+KsZqTYZrSErJ9sMB5lX7aNP9+ruau5FSsuWQ\nn4WTfQRCMeZOLKIlEGZro24RhOMWi6Cva8jfmyq4J5V4SUhpdpe0zkqPm1icks7qctiIxBJMKS8w\nfa7WimPQBKHDbqOyyE1zINzHjWEI/wKXwxTqU8sLzEDuQGX9xjk9lnV/DSaXevE3pbo30n3+/WWO\nZDurO39hNecvrO53vzH+yqKBUxVLCpw8essppkvNSKm1YrcJ7v/Eiebr+65fkdU4xwOZKo8VGuo/\ncwzpCWutfB9/5+CwXretJ2Km5hk9eKzvCfD81mYu/Nmr7GnpNlsMNPlD/Off3+WSX67mxe3NBEJR\nppYX4nbYTAXSHY6yq7kbp13gD8XM61ljBFbBXV3i4f+uW85XLjgOSE2BnD2xKCWgXKsLOaN/i9th\nIxBOVwRu/VETaukBZGvb3lKLIjh9diX3Xbc8pco1E9bgoXXFKkjtOGqQbhHkmg8ureH3N5yYVZ8a\ng1lVRfzu4yvM7psKxUDk9FsthLhQCLFdCLFLCHFHhv1ThRAvCiHeEUJsEkJcnMvxjDTbmo4uePbS\n9mZ+oRfqWGntDrNIF2Lp7ghDcO/SZ4kHOnpp647gtAtcdhsPrdFaNmxv6sbfG6PE60zpkljf3ks4\nluD4Wi033ogTGHn13eHUtVyrSzwcX1tqZgEZKZAlXidVRW6K3Q5cDhsFLjtlhS4cNmFmqRR7nHQF\noyn3YPSoNxRCer+WEtMisLqGtN7t5y+sHtANp423hBtPm8Ft5841rwV9F4K320ROcsePhNdlNyt8\nB8M58ycOOW9fkX/k7JsihLAD9wIXAQuAq4UQC9IO+zrwiJRyKXAV8L+5Gs9ooE6veD1Sp8Ij8Y+N\njfzqpd0prTSi8QSdwWhK7rqVbj1YbGTDtATCtPdEWDGtnMf+3yl8QF/+bsuhLiLxBMUeR0rfdKOX\nvNGt0YgTWKuPjVTTK5bXcuXyvtWf1SVeM61QCEGlXlFbVuCitsxrBliLPQ4O6e4nu03gdthM4T6h\nH4ugXA+iFrkdZqxkanlqHOFIeJx2nvzcaXzjkgVM1ZvAnTGnik+dPoNfXbs89VglWBXjlFzGCE4C\ndkkp9wAIIR4GLgfqLMdIwLDHS4BDjGPqdP/9UItFusNReqNxAuGYmTljzMZnTyzGZbf1cWcYFoEh\n0A1FMH+yj8W1JfzsqqW0dkdYr2f1+DyOFIvAcNPM0xt/GXGClu4wQmiZPUYB2Z0XzUtpnmbw7csX\npvjWK4vd9EbifOn8uSmz/2KPw1RYnzt7NidMKTFn9DMqCvE67X0yXC45YTJOu40p5QVUFbv54YeP\nT6nsHQw1pV7cDhvLppWaPv7yQhd+PZ3VnaEmQKEYD+RyilMDHLC8btC3WbkLuFYI0QCsAj6f6UJC\niJuFEOuEEOvGcj8hI23Tn0WXzUwYqaDWBdWNtW+ritwZ+wp1R/TlAi2KoK0nktLkbM7EIg7pM31f\nmmvIYF51McVuB4+vP4iUkpZA2DyuoSNonpuJFdPLWWSxWJZMKWVRTQnzJ/lS0vKKPQ7TulhUU8L7\n5iWzWa47eRrP/McZfYK1RW4HH9atEI/TzkdOnDLkrKxCt4Nn/uMMPn7KdHPbk587lS+er7mMjnV8\nQKE4Voz0N/tq4A9SylrgYuBBIUSfMUkp75NSrpBSrqiqOraFFkeLlNoi6I+93WD2qg9G4mYDscFg\nZLQctizHZ6wdUFXsMhWBtbliT9q6sY1dvXT1RlMKmOZMSOZ7L5zsyxiYrCxyc+fF83ljTxsPvLGf\nQChmtgeubw9S6LJnnZVx9+WL+GmGrKkit8O0aNIrQD1Ou+m6ySXTKwtTUjlrywrMjB2lCBTjlVx+\nsw8CUyyva/VtVm4EHgGQUr4BeIBKxhHbDwd4bmszX3p0I129UbNXfaaOlwNhCPVtTQHTumjTLYKK\nQjeTdUVgpFg6bIIePQvHUCJGoVi5JXvHKPz54NIaZk8oNmMEdotGKfE6ufqkKVQUulipr/26YrqW\nkVPfHhxS6X461msMx/WGCyO1NVO7CIViPJBLRbAWmCOEmCGEcKEFg59MO6YeOAdACDEfTRGMXd9P\nBlZt0losG0LV6ATZnyJ4aXszdz6+mVWbG1m7r51ntiSXjTRaI3/7n3VcrC9XaLiGKovdVOvZN/P1\nmfqkUg/d4bgZH3DZbexp0TpzWl1Dy6aW8tBN7+FHVxwPaM3RPE6bmYkEWuxACMGCyT6zu6fR/CwU\nTQypvW86J6T18R8tGN1XlUWgGK/kLFgspYwJIT4HPAPYgfullFuEEHcD66SUTwJfAn4jhLgNLXD8\nCTnOVpdZ9a4myI31X+dOLObVna0piuDeF3fhsAk+feYsfv78Tt6p7+TFbc3m4uo/v2oJ25sCfaqE\ne8IxmrrCFLjsFLrsnDGnki2Hupg/ycc/NzUyqcRLU1fIDObOn+wzW0JbK1WFEJxiWUykssjNhm+e\nz+9W72Vjg9Y/30ibXDDZx6s7Wylw2U3XEAyP4D7V0iVzNCkCY03j/qp/FYqxTk4ri6WUq9CCwNZt\n37Q8rwNOzeUYRpKGjiC7mrvxOu306v145mawCH70jNYT/orltebygU2WgPCTGw6xvr7DDPwatATC\n1LcHmVpeYArzU2ZXcqA9SDgap7Unwu7mbv7+zkFKvE5On13JxgOd2ATMH2B5P4/TbrpErELZEP5z\nJhThddnNexsOwW1dw9fjHD2zb9MiGEVjUiiGE/XNziGv79L611xhya2frQdmjWIlayvj37y6l9bu\nCBek9X7Z09pDRzBKuq3U0h3mQHswZTlA0Aqqvnj+cRS7HXT2RnlmSxPXvGeqWb9w5tyqlLbD/WEs\njuHLUH1r3MdHT9TCQFbFNVSEEGY9xHD3YzoajHRfZREoxitKEeSQ13a3Ulnk5gx9fVm3w2YWOxkW\nwS7L+rkPvLEPgAsWVqf4o/daVtyyctgfMi2CTBS6HcQTEpsQXH/yNM6YW0VpgZM7L56f1fgNAWhd\ngHxGZRGLa0o46zjtnr5y4XFMrygwFcLR8ugtJ/PON84blmsNF0XKIlCMc1TTuWGmrTtMideJTQhe\n29XGqbMrzEVIJvo8pgulK6gpAqOh2weWTObvG7R6uplVmrBdl6F//xXLtUXGH1pTz7bGAL3R+BEV\nAWiLsUwq8TKpxMuGb56f9b0U6L5xq9vHbhP84/OnJY9xOXjpy2dnfc2B8Djtoy47RwWLFeMd9c0e\nRuIJyfL/fo4vPbqRt+s7aO0O8755EyyKwI3LYcPrtKdYBC6HjcuXJGvtppYX8K1LF3LXpekdOeCi\nRdX89+WLsNuE2eN/SnnmhmTG8nw3nja09ZwLnH0VQT7icthw2W3KNaQYtyiLYBiQUnLvi7s4c67W\nHOyJDYcoK9AWUz9n/kQKXXZcDlvKcnmGIthxOMCsqiKWTdVy8o01X8sLtT48d/2jLuW9itwObDZB\nZZGL9fWaIujPIrhoUTXP3nYGcycObYGQwgzB4nxl/qRicw1lhWK8oRTBMNDeE+HHz+4w1wAAePrd\nJs6aW2X6l699zzSW6y2RZ1QWsrGhk1A0zrp9HVx6wiRKCpzMqirEabeZgdLSAidOuyAaT0aJDeFc\nVew2K4ytC6JYcdptQ1YCkOzCqRQBPPG50wY+SKEYoyhFMAx06rN7o7gLtCyau5YtNF9/0+LmuXBR\nNd96cgt/eH0f3eEYF+p9479xyQKsiUFCCKqK3GYfIEh2LjVaRJw4vSxnPvUSrxMhUtcUUCgU4w8V\nIxgGjEWtra2Zp5YXmAuUp3PhomqEgJ88u4MSr5NT9HWDzzpuQp/e81XFbmwCinVLwLAIjGyjT546\nNP9/NlQUuXn00yenxC8UCsX4QymCIRKJJfjMn95my6EuOno0i8DqGvrS+XNTevVYmejz8IElNRR5\nHHz85GlHbNZWVeymsshNlT4rN1xNX71oHu+ZUX7EZQ6HgxXTy0ddFo9CoRhelGtoiOxr62HV5ibe\n3NPOnRfNA6BZ99k/9v9OMeMB/ZGp+2Ymrj95Ogc7e3l8fQP1bUEzhfGyEyZz2QmTj+IOFAqFQkMp\ngiHSprd/bu+JmBlARhuJQvfwzaCNYrSXtjdTpDd+UygUiuFEKYIh0mIJDG9tTF2LeKgrkB2JE6eX\nM4QlDBQKhWJAVIwAeKe+g7n/+ZS5Hm82WFcJW7W5MWWfkXY5nNx0+kx++/EVw35dhUKhUIoA+NOa\neiLxBC9sawYgFk/w9LtNR1xFrKU7jMMmcDlspkvIoCAHFoFCoVDkCqUIgFK9YKqzV/P7r97Vyi1/\nfJs7H9/c7zktgTATfR5mV6VWmwoxulooKxQKxUAoiUXSldOpN4IzAsF/fbuBfRk6f4aicVoCYaqK\n3eaKYwaFLhXQVSgUYwulCIDusObaOdAe5O397SkVwq/uTF0580B7kCV3P8urO1upKnb3aeHgzUF8\nQKFQKHKJcmaTXBzmqXebeOrdJpZOLcXtsFHscbK+vpPrTk4e+6/NjYSiWuygqtjdpxFZoVIECoVi\njKEUARAIpS4BubXRT2WRm4WTfbxTn7omwFOWDCGv056yvCKoQLFCoRh7KNcQ4LcsFwkQiiaoLHKx\ndGoZ+9qCtPdoMYO6Q342NnTx3pnlgN7dc0Ixnz5zJreeMwfITeqoQqFQ5BI1faWvRQBQWeTmxOla\nm4jH1zfwsfdM4yuPbaSyyMWvPrac13a3ctZxE7DZBHdeNJ/ntx4GoMCt/qUKhWJsoSwCNIvgshMm\n88qXz2b+JG1x9soiN8unlXHWcVX8+NntXPjzV9hyyM9/f2AxZYUuLjl+stkADpJdQVWMQKFQjDWU\nIkCzCHxeB1MrCqgp1VYRqyhyIYTg+x86npNnVjDR52HlDSdx4aLM3T6NthIqa0ihUIw18tqPYawQ\nFghFKfZoRWU1pdr6v5VFWtvn6hIPv7/hpAGvZTSay0WfIYVCocgleS21fvnCTu59cTcAPkMRlGmK\noKLINahrGa6hgmHsPKpQKBTHgrx2DRkVxJBcArKmVEsHrSoa3PKMpiJw5rVuVSgUY5C8lloTipPC\n3qf3GzrzuCo+e/Yslk8/8sIy6RS5HXz9/fM5d37m5SkVCoVitJLXiqAnkuwaalgERW4HX75g3pCu\nd9PpM4dlXAqFQnEsyWvXULelfsDnyWudqFAo8pi8VgSBcLKi+EgLyCsUCsV4Jq+nwYFQjHnVxdxy\n5iwW15SM9HAUCoViRMh7RVBV7OYDS2tGeigKhUIxYuS1PyQQipr1AwqFQpGvDKgIhBCfF0IMLpdy\njBAIxVL6BSkUCkU+ko1FMBFYK4R4RAhxoRhH6zB2h2Nm2qhCoVDkKwMqAinl14E5wO+ATwA7hRDf\nFULMyvHYckosniAYiZs9hhQKhSJfySpGIKWUQJP+FwPKgL8KIX6Yw7HllO6wVkOgLAKFQpHvDCgF\nhRC3AtcDrcBvgS9LKaNCCBuwE/hKboeYG4zFaIqUIlAoFHlONlKwHPiQlHK/daOUMiGEuCQ3w8o9\nhiJQFcUKhSLfycY19BTQbrwQQviEEO8BkFJuPdKJenB5uxBilxDijgz7fyqE2KD/7RBCdA72BoZK\nQF+nWMUIFApFvpPNdPhXwDLL6+4M2/oghLAD9wLnAQ1omUdPSinrjGOklLdZjv88sDT7oR8dRoxA\npY8qFIp8JxuLQOjBYkBzCZGdAjkJ2CWl3COljAAPA5cf4firgT9ncd1hwXANqWCxQqHId7JRBHuE\nEF8QQjj1v1uBPVmcVwMcsLxu0Lf1QQgxDZgBvNDP/puFEOuEEOtaWlqyeOuBCYRVsFihUCggO0Vw\nC3AKcBBNmL8HuHmYx3EV8FcpZTzTTinlfVLKFVLKFVVVVcPyhkHlGlIoFAogCxePlLIZTVAPloPA\nFMvrWn1bJq4CPjuE9xgyPeEYQoDXqdYYVigU+U02dQQe4EZgIeAxtkspPznAqWuBOUKIGWgK4Crg\nmgzXn4dWoPZG9sM+enoicQpdDsZRxwyFQqEYEtm4hh4EqoELgJfRZvaBgU6SUsaAzwHPAFuBR6SU\nW4QQdwshLrMcehXwsDUgfSzoCccocClrQKFQKLJxkM+WUl4phLhcSrlSCPEQ8Go2F5dSrgJWpW37\nZtrru7Id7HDSE4mr+IBCoVCQnUVgrOfYKYRYBJQAE3I3pGNDTzhGgVtZBAqFQpHNlPg+fT2CrwNP\nAkXAN3I6qmNAdzhGoUtZBAqFQnFESag3lvNLKTuAV4CZx2RUx4BgJMaEYs/AByoUCsU454iuIb2K\neEx2Fx2InnCcQhUjUCgUiqxiBM8JIW4XQkwRQpQbfzkfWY7pCccoVFlDCoVCkVWM4KP6o7XgSzLG\n3UQ94ZiyCBQKhYLsKotnHIuBHEsSCUkwGlcWgUKhUJBdZfH1mbZLKR8Y/uEcG3qjcaREWQQKhUJB\ndq6hEy3PPcA5wHpgzCqCnojWcK5AKQKFQqHIyjX0eetrIUQp2toCY5aesNbktEgVlCkUCkVWWUPp\n9KCtHTBm6dFbUBeogjKFQqHIKkbwD7QsIdAUxwLgkVwOKtf0qLUIFAqFwiQbSfhjy/MYsF9K2ZCj\n8RwTzBiByhpSKBSKrBRBPdAopQwBCCG8QojpUsp9OR1ZDknGCJRFoFAoFNnECB4FEpbXcX3bmMWM\nEShFoFAoFFkpAoeUMmK80J+7cjek3NOtYgQKhUJhko0iaLGuKCaEuBxozd2Qco+/N4oQUKwUgUKh\nUGQVI7gF+JMQ4h79dQOQsdp4rOAPxShyObDZ1HrFCoVCkU1B2W7gvUKIIv11d85HlWMCoRg+r3Ok\nh6FQKBSjggFdQ0KI7wohSqWU3VLKbiFEmRDiv4/F4HKFPxSl2KPcQgqFQgHZxQguklJ2Gi/01cou\nzt2Qck9AKQKFQqEwyUYR2IUQbuOFEMILuI9w/KjH3xvD51GuIYVCoYDsFMGfgOeFEDcKIW4C/g2s\nzO2wcksgnOcWQdtu+PFc6BrTBeIKhWKYyCZY/AMhxEbgXLSeQ88A03I9sFyS98Hitt3QfRg69kFJ\n7UiPRqFQjDDZdh89jKYErgTeB2zN2YhyjJSSQCiW3xZBIqo9xsIjOw6FQjEq6FcaCiHmAlfrf63A\nXwAhpTz7GI0tJwQjceIJSXE+xwgSWmU18ciRj1MoFHnBkabF24BXgUuklLsAhBC3HZNR5RB/SJsN\n53WwOK4sAoVCkeRIrqEPAY3Ai0KI3wghzgHGfCluIKTNhvPbNaQsAoVCkaRfRSCl/LuU8ipgHvAi\n8B/ABCHEr4QQ5x+rAQ43AcMiyOdgsaEIlEWgUCjIIlgspeyRUj4kpbwUqAXeAb6a85HlCH+vsghM\n15CyCBQKBYNcs1hK2SGlvE9KeU6uBpRrkjGCPFYEyjWkUCgsDGXx+jGN34wRKNeQcg0pFArIQ0XQ\nq69XXJjPaxEoi0ChUFjIO0UQimqrbnoceXfrSVT6qEKhsJB30rA3GsdpFzjseXfrSRJx7TGuFIFC\nochHRRCJ43HaR3oYI4vRYsKwDBQKRV6Td4ogHFOKQAWLFQqFlbxTBL2RON58VwSqjkChUFjIP0UQ\nVYpAWQQKhcJKThWBEOJCIcR2IcQuIcQd/RzzESFEnRBiixDioVyOB7SsIY8z7/RfKmb6qFIECoUi\ni4VphooQwg7cC5wHNABrhRBPSinrLMfMAe4ETpVSdgghJuRqPAa9URUjSKaPKteQQqHIrUVwErBL\nSrlHShkBHgYuTzvmU8C9UsoOACllcw7HA0BYKQJVUKZQKFLIpSKoAQ5YXjfo26zMBeYKIV4TQrwp\nhLgw04WEEDcLIdYJIda1tLQc1aBUjAClCBQKRQoj7Sx3AHOAs9BWQvuNEKI0/SC90d0KKeWKqqqq\no3rD3mgcr0spAkAFixUKBZBbRXAQmGJ5Xatvs9IAPCmljEop9wI70BRDzlDBYizpo0oRKBSK3CqC\ntcAcIcQMIYQLuAp4Mu2Yv6NZAwghKtFcRXtyOCZCmSqLI8FcvuXow7QIIhDpGdmxjCSRIEg50qNQ\nKEacnCkCKWUM+BzwDLAVeERKuUUIcbcQ4jL9sGeANiFEHdoqaF+WUrblakyQIUaw91X4wXToPrrY\nw5jCUASt2+G7k2HPyyM7npEg2A4/nAm7nx/pkSgUI05OezFLKVcBq9K2fdPyXAJf1P9yTjSeIJaQ\nqRZB1wHNRdJ9GIqOhWl9EwAAIABJREFULv4wZjAUgUHDWzDzzJEZy0gRbINYL3QeGPhYhWKck1fO\n8lBU67qZYhEYmTP5FDhNbzbnKh6ZcYwkxuetMqcUivxSBL26IvBYs4byMXBqtKE2cBWOzDhGEuPz\nzqcJgELRD/mjCJq34njnQUCmLkpjWgQhSCS0mMHRcOgdCPmP7hq5JpFmETi9IzOOkcSoqs6nCYBC\n0Q/5owh2/pvyF26nkFBqHYHVNbTnBVh5CRzeMrT3iEfhdxfA2t8e/XhzSbprKB/XJTA+93y8d4Ui\njfxZuNdbBkCZ6E6LERh9d0JaABGSj4MlHNBmmEM9/1iRHixOtxDygXyMDSkU/ZA/FkFBOQAldKdm\nDRmCIBaB3g7tebR3aO8RDuiPo901lKYI8jFgqoLFCoVJ/igC3SIoFWmKwBojMBTBUIusTEUQGOIg\njxF9FEEs83HjGRUsVihM8k8R0NOPayisFRkBRIdYaRzp1h7D3UMc5DEi3S+ej64hFSzOX959HJo2\nj/QoRhX5pwhEd2qvoYwWwRAVwZixCNLSR/PRPaKCxfnLU1+Ft+4b6VGMKvJPEdDdf9aQGSMYqmtI\njw1ERrlFkIiCzZl8nZeuIRUszltiIfW5p5E/isDhJmr3ahaBo5+CsqO2CAzX0BgIFlvdQfloEahg\ncf4SC2l/CpP8UQRAyFFCKd24+3UNdWrPhxojGCuuIRUjUMHifCWR0H7z6nNPIc8UgY9S0YPTnkkR\nhPvPGgr54aGPQvteLbV05aXwf2fAgbXwyPXQuFE7zlQE3aO7vXGfGEEWiiCRgMc/DQ3rcjOmY40Z\nLFYWQV5hTgCURWAlfwrKgF57MSWiG4dNJDcaQjDcDRFdkKdbBPtfhx1Pa1+eC78Pe1/Rtq/6kqYE\nQl1w/RPJ2EAiqikWpye3NzRUElE48Sawu2Dd/dkpglAnbHoYKudA7YrcjzHXxJUiyEusdUMKk7yy\nCIJ2H2WiByGsikD/QnQ3JbelWwR2XV+27Eh1+xiWQPks7dEaGxjNAeN4FNw+uPB74CnJThgaRXbj\nxaRWweL8JKYsgkzklSLosfsoI81/b8yGAxZFkG4RGEHgwKGkIpi4KLnf49OPs1x7tAaMpQQZB7ue\nNWRz9i0wy4TxPxkvPyAVLM5PjO+vmgCkkF+KwKa5hlL894YgCDTqG0TfrCGrgG/XV9KceVZym3G8\ntZBstAaMDaFvpI/andkJQ8NKGi8/IBUszk+URZCRvFIE3bZinMRTXT+GEDQCxUUT+9YRWN08Rnxg\n1tnJbcbx4QAI/V86WquLTUWgp9DandnFCAyLYCiVuI/fDE/fObhzXvwu3FUyPEH3rgZtWcrmbclt\n+RosjsfgF8tgy99GeiQjg7IIMpJXiiAg9JW4DKEPfYVgSc2RLYK23dpjzXL46B+hqDp5fKRbe51+\nzmjCuF/DNWR3ZacIjHscyg/o4NuDL+l/+QfaY+f+wb9fOm27tI6wLRZFkK8WQagL2ncPvdX6WCeu\nWotkIq8UQRdF2pMURZD2hfDVZIgRWIR6Z7326CqG+ZdC0YTk8WE/+CZpz0drsNi0CBzJx2zqCAyr\nZygmdW/H4Bv5+Wq1x+FIV81U32EWEuaZRWDErkbrRCXXKIsgI3mlCPymRdCe3GgVBMIOxdWZFUFh\nleZXjwTA4U1mErkKk0IuHADfZP35KA0WpysCuyvLGMEQLYJEQlMEgy3SMxTqwfWDOy8ThpvOqpzz\nNVg8Vhoj5gpTEagYgZW8UgSZLQLLbNhbqgv2INSvgd0vao9hP7iLzTUNcBclz3EWaEJOSu3HVWwo\ngoDmm87UrqKrQbt2sL3vPiu9ndDdPPgbPRJ9FIGz/15D7XuSxWfRIQaLw36QicG37TAE1cFcWQRj\n2DUkJbTuGtq5A62Z4W8cW0qibbc22cgW6wRgMOeNc/JKEXRmVASWGaG3DJyFmqvk/vPhwQ9ojzuf\n0xSB3rgOd3HyHFeBJuRiYe28ogma5RBogp8uhMc/1XcgD31Uu/bTdxx5wE/fAX++emg32x99YgTO\nzK6hnla450So+7v2eqgWwVAb+RlWW9O7gzsvE5EMws8IFst430rr0c6+V+Ge5Vpdy2Ax+2H14xr6\nw8Xw0veGPrZjSaBJ+45u/1f251gtARUnMMkrRdCeKNCe9GsRlGuC3eDDv9Mew11aTCCTInAWakLO\n+GF5SqB6EWz+q/Z61/N9B9LVoD0ONNtv3WFJax0m+sQI+kkf7WnRju08oL0eah3BUBr5SZmqQI52\n5mZt/WFgFQJjzSowPpOuA4M/d6AOuf7GoV13JOg+rCly/yB+I9aK4rH2ueeQvFIEwYSTsHAPYBF4\nteeeUph/WXKf1SJwZbAIjFmnqwhqVkCPLuQrZqUOIpHIPmDnbxz+oF7GGEEG15Dxvun9l4ZqEcR6\nsxfo0aD2uRRUJF8fDUcKFsPYixMY/1Pr9zhbjtQYMR7TPqehXHckGMrSsNaJjFIEJnmlCKLxBD02\nHwQHcA0BTJgPDpcWJIb+XUNGjMD4UrqLtdRSA+uxoFkXUheIR8osSsS1GU84MLwN7PooAkdmQZiu\nCI7WIgBNyAzmHF9N6nsPlUzuEKsQGHOKQHebDUVgR47gGjImM8GxoggyJAEMhPVzVwFjk7xSBJFY\ngqC9OPkDSsQ1oWwUgXnLkjP5CfO1x2I9e8VdZFEEGYLF/SmC9B+r8VrYjzzb727WzF4kNG6Ava9m\nPs7fCNuf6v866WSqI8gUIzAVgS50IkMsKLPefyQIh97R6goy0bYb9ryUPKdETyEd6hrSBsa9RI6h\na6hpMzT0c59Hi2kRdA7+3ExusvR9Y84iGITVPNIWQSQIG/8y6roT55UiiMYlQbsv+UU3ZoKGq8db\nBjP1iuEVN2qPRjrokYLFkPT3u4ugYjZUHw+I/hVB6ZQjZ2f4DyWf//ub8JdrM3951v5WCyhn++NN\nbzFh66ey2BQKaWs0DNU1BJq//x+3wr9uz3zsc9+Cv944/BZBf8Fih+4GzIVF8Mx/wqp+7vNoMRXB\nAFlnmbAunpT+fRpzimAINREjbRHUPQF/u3nUFfTllSKIxBKEHBkUgduiCKoXwV1d2iMMrAgMV5Kp\nCHxgs8Etr8Kpt2rvZf3BmYpgauYfo0HAogja92ltoI2qZivBVkBmn2+fbYsJY/Y8nK6hYJuWBeQ/\nmPnYhnXa/RgNAEt0RTDUFeMM+ksfNT7HXMwMuxq0Kt5ccFQxAkMZyr6WlqEkoj1jw38+lJqI+Ai7\nBI1Ekf5+AyNEXimCaDxByGlVBLoAtCqCdIy6ANcRgsWQbGNtVRLeMu3LZp3RGjPs0qlk/DEaWDMh\njC9Nppx64176c7ekkyl9NJsYwdGmjwLsf0Nzd3U391U+/kPJDKnmOu2xZIr2ONQ1pA0yZg1Fk5/V\ncAsEKbV7yVX17nAEi6Gvb926byhup2PNkILFI2wRGBM8q8U/CsgrRRCJJwg7SjSTWkqLRaD7/Asy\nKAKjwvVIwWKAwGHt0WWJHxjHp8yKdXO+dJo+qH5mM1aLQOp57pmEvXG9bBVBpvTRTG2ojR9Xeh1A\nLDw4/6a1aG6fEeeQWiDcirWVhGE2+4bLIugnWGx87sOtCEJdqXGj4SZ4FIrA+n1LH59VoA7F7XSs\nyRT7GYiUGMEIKAJDAQx3WvhRkleKIBpLEHaWaj/8SE9m11A61mBxpspil+EaOgyI5GtIHm8Iw9d+\nAZsf0Z4bs13jy7z+Afjjh+HtldrrTDOGTH13jJlbw7rMArqrQfPJGzPw/tpQR3T/veGWMd0EQYiG\nLMJYatd6+k7405Ww7zVt86s/SXZmXXOfVoQHmrByl2jPjf3G/dU9ocU4tv4Dnr87ue/wFnB4kv+/\n4UofjYe12EAioQXIXVm4hp79BvzxCtjzMqz+qfY4EMZnF+vtv2o7E40b4d/fSv0cO/bBU19NLXob\nLosg7IcXv5f8XlkF6liIEwxUHJeJYxksbtmhxYqsn53x3VCuoZEjGpf0eHXB3rYrKRxnnAmLPgxV\n8/ueVHsiLPwgTHkvVB4Hi66A6acn9xsWQff/b+/Lo+Mq7nS/n/bdli1b1uIVb7GNV2HMMgTMEuxh\nGQJDYEggJIEMMwnJnDPJwORMHjPJvMkw5A0xkOGxvZAcJpBAAoYBAmZxICxeAGMb78LGWrzIiyRj\nqyXZ9f74qlR1r7qlbkntluj6zulzb9fd6ldV97dX3b2MD7hfP3Mtgo42Mru61TzPHDOD+N3/C2xf\nAax6gP/3bbJWA0ANfs/67oP32CEeO9oUfaXOzc8Dqx+07hZzTtFobk2MoHYlsPYXdnnigJsgtFbQ\noY+Bd34ObHuJn7qMHAFe/RHw3q94fOVPgPceJUM7sB0YPZ3lHUftYnItDcAbPwVe+RHw1j3M1lpw\noz5WT0Fp2rY/WUMnjtOaMe3dfqS7AhArE+rTJuCtZcD2l7l95V+A93/V+zNda649ASa15hHgT3cH\nmfCmZ4F376dAANhX7f0I6kaO2Lb4tIl9te5xfSzU54MdfQoWuxPKkmwRPP/3wNv3BhU4YwkkMgnu\nJCBtBIFSCu3HT2D/MB0Erl9rmeqIicBVjwRnFRvklQB/+QuguJzfIL7qYaDUYdDmmtY9QUsBCAqC\nvRtsmmbeMHuuGcQtju+w4xi14knn2nuNO4PXh5dzPnaIxwxNYXT5JPXAq3+PcyOGj+P/zBy6nupW\nB+8RZgrtnwLQQm6XtgKKK3l+4wdMw21tZJsePcBrDtbSxeAKzmqdWnvoY9LYdhjYvQqYex1wyX9a\n11p1jbWu+mMRGC3XuJkiLZbxdwWLY7iGTFsUV3KGuDoR3wvsnpMIkzLppq41aO4VThnNLemeiBAP\nIq22LZr0EhWGOQ05QdBDKmwsdLZZBSPZ3y0uKud2n3Z1dkY4Yx/wrqFUoeM4X5i2girOWK1fazXD\nzJy+39hkDR1t6j55zBUErlZw9IA9N9JKa+HYQbpDjh0Edr9L5jzpXHvN5Au4dZl9Z4Ta7vizeG20\nvHXDSIxAqFvDeQ7GcjGxgt3v2uOAZqBi699xlIvyAcCut+haWvBVMvRtL+lnOQHfY4dsJtNERxCU\nzwIyc4EtLzqxCWXrZFxxVQsGxiIwzMLcN3LEMoDeLIK6NZxjctrXWEcgqO3HgsvI42VS7Z9aq81l\nEuZ5YXfQiElsv0TjEJEW2xb7twTrG2lFoM8HO9qdLKd414vqjFCIAsm3CIzV3fA+t8btml3oXUOp\nQsdxzubNyc7kEhD1a7tn0PQFZkkKoGdBUL/WCpyOo/bc9iP2xTcT0TY9x+34s6wvv3wmX2BXoHR9\nVW0UUDGnF4uggUHMpq3BCW+mTp+8w/1DHzOmEWmxqbNHm/jSGHp2/onptePP5P9VD+pnNVqmcuwQ\ns5yyC4LPK65gAH73O8Hnm3NKHEGQlUtG3B+LwDBic99IaxTXUIzvMdSvAUbPACacY8taGnvXwl1h\nES+jblxnkwJcJtESFgQ63mSWLkmEYSvF8VYSEgSuRVA4ipMde1sZdzCgpwyoWOhso0UOJD9G0KFn\n0hsFzbRz5Ty+i/1NghhAZKW6AicLRhBkZ2bQ7bDtD1xZFOifReBmCeWEXEPZ+WSEr/8btbepS+xK\niTmORWBe9qr5dLts/h/6yIvL6UI6dohMuGpBkNkbJpBfSuG25mEyNVewmXtvfZE+aCg+x8Ccq44D\ns64G1v0auGcBGc74s8iUfnczzykYSXdPawMwfSkHtMuoO44ytmHqVv8eBZTbLiUVdE0c2slt6URg\n/yagdII+XkWLoXwWLYTswuALs20F/drnfB94+q+Dwdj84cBly4CnbuJLnlMIXPQjHjNpwG3Njmuo\nyJbdt8i2VUYGcMndbOsZfwFUzNYf8HHW4nnxdpbv/Yh+/IU3Aef/k27zRlCzVpwMaNpkygWMNb32\nYxoYVfOA2ddwlVmXKbU0cob1in+2QuHjlYylnPlt/h8xidujB7h95GK6rm5YDoyaxjr+4lLgsp9Z\nIdtxlDSYtmjSguDIXrZjpJWuUKW/IfHUN4Ct2tpbdAvdeDlFls4w3vkvar9ffKD7sfVPAs/9HVA2\nBfj8P7CPhlUBN6/kUi4Ar33mW8ANzzJRYP8W4NHLOMZufJ4uXIBC6tFL9RfndDvfUwNc/UvS+uB5\nXJhPBLj0bsb4DDojpBHobhEs/zaw8Rmg5ka+D2t/CZx6JV2WK+8E3roXmLyYrmIA+NPPgB2vAtc/\nQ/re+yWvXf4dYMQEYJSOje3fRIvPzCGomg/sepOCwQj0w58Aj10NXPvftm9PItJGELR3OoLg1K+Q\noa37NQ/2RxDkl1LDaGvubhEAwNK76NcXAWZ/CZj3Zb4ArmuoyyKo4ba1gQwI4HmuINj8HF+EghGO\nIBjBwfXOffS7V85luVLWNbRnPQAhA3U1XFdozPsKGfLKO/l/1DRgykU0aTOzyLRNLKFsKhnpJXfz\nhcwuAN64y7qDOo6S0cy4PBhAL6kCzr2dwm7iOfSjHm2y55x5KzBtiWUOOQVBi+Cj37MOr/9valzz\nb2B522H254o7GHiefz1fzLfv5fGpF1GA7PvIZiONnMxtwwd8WadcBIw4BfjwcQZt25qpNGTnk7nt\n38p7HKwF1v+Wz9m7ketHbXjKMsjWBsZgDu+i5TNsLPtv2wq6ATOyaVHVvs5nZOYAc64lU1h5J5n/\nxqeBBmeS4KbnSKNRBKoXcrvvIzJ3Y4XUv8d+2/U2sHc9rzOCwAik8pnsByNk1AkKg/YjHG+SwbH1\nydtc5uNEB7PdPtUuzViCYNvLQO1rOtZTGDz20dO0MuvXMo040gzsa6bP3Ewc3PQcY2m73gI+dwnv\nZ+bn1L5mBcHejTwPoPvlyF72xa432ZZ7NwDTL+F9PloeFATHI3xfgKDwPXEc2PA7tsGOV/k/0gys\nfwpY+lMy+kgzhX7HMY6Jl3+o20+x/z9eSRojzbTw8obb9m2u4zuYkQ1MOJvJBy0NVhBsX8ExuO1l\n4PRvRm/fJCKpriERuVhEtojIdhHptvi+iHxVRPaLyAf6941k1aXduIYyM6iVLrnTHuyPIMjIsC+a\n8T26mHcdsOQnwMX/RgY9fSm15KxcDgrXIqiusdeF75lfao8bBuFaBOaYazFEWoKTsUZNAxb/wDJZ\nwLqeAC57ce5tQPkM/exi4Ozvsv4X/Ti4kqrxMy+4gbRNubD789uarfbpXjfxz3jP6UsZPJ76BXu8\nfAaFh0F2SBAYM7vhfbbRkp/wd+nP2I8N71OjunQZ26Xhfb74lfNZXr/WtvfIKRTiZqLembfyXuPP\nsn5d0w+zrgROWcz9LS9QYzz8CSfHZebSpfap1s5bGtjWBuMWkRlFmplFVTGHwtDQMfkCPnfhTXTH\ntTZ2d/O16SDxwVpuq2uYllu3hudm5bG8SyDo691JiF1CpCborgPsBLgcvabWsYMcX1MupAJzsJb1\nb6mzvu4wWhvJ9BrXdT/mxq/cYLo7XyFc5/q1FKIFI4PXuzGUEmd8tTjttuTfOc7C7dgZoXKRkRWM\nDe3fQiGQlcf7tzZwP9LMPmrayn470Qk0fhiMSbR/al22ZtwAwbkyLQ2sy5hZVuN3Y0mGvnjnAw0w\nkiYIRCQTwH0AlgCYAeBaEZkR5dQnlFJz9e+hZNXHBItzsjTJeQ7T7o8gAKwm72q+vUGEGrWxCHKK\nqKWZ4LNh7DlFAIQMq2Iu982gcQXB8PE2CG5gXjiznHOVI2gMXNqLxnA7Zo7zbAeG2QDBFxBwApCb\nguXGH21g/LPxIsdxDUVagx+gd5lZVi4w5lRdXsP2NcdNINq41gwjKamkoDK+ckOTafvsQmveu7Rs\nepbbAzsAKH67GqCANllTZVPtdcUV9t5NW3mfijn0xQM2k8rU4UAPH5c/WEuNPW84rcD6NRwPE85m\nWVd2mGGm79vlv+vW2LoYGs3YaGlgPMWkNrc06KXARwQVFCA2szKMLTzfpaWBjNX0h8vIuxaAPGGt\nSXN9vU5sqFoQFGguAy12xldrI88rGsN3qaqGVtmnTfaczjaO46y8oEVg7v+5S2mlHDtk+3X1QwAU\nsPCb9lwzZgBaIGaxSsChc49Nl26us8qLqbMbSzLPH4hvdPcBybQIFgLYrpSqVUq1A3gcwOW9XJM0\nBGIEBoax9SdYDFif+8GPE7sut5iDumkrB4cImYRkklGYc/KHc22gvBIypp1vAPs22/S/ghGa0dUw\n+2ffZv7qVun6GYY4v3sdXNqNpTBKM7Gw5ucKjViCALCZSIBNVTRIRFgCtAgiLUBbi9a2lA1ah7Va\n898wrm7/a8gs6tZQi88v1cxdBWkw11XOs2syAVZQGt+6uW76UlBAr7FMzhUEJZVOeykKn+x8umjC\ndBRX0LpQxy2dbnse2kmGbyzRPeuZnli1gM9oaQCa6ykA8ks556BpK2e+160KCkfTJgC1+GMH7Qx6\nM28hv5TtAKFikJHFVFozxiJHdFrkAWu1mPG5b7NNlABsFpzLyI8e1KvOvkrtO284+3n3KlpcRhDs\n38LyQzuD17uzwlvq+azqkCKw+X+sBt8ZodKQlcu+MvWsfZ1KyoSz7f0mnUua1/237eeSak6M3Oqs\n+Lt9hW0rwC5c2XYYGKm1/9rXaXFU1VABzC2xQnv/VtKXNxw4uIOC3dTrYO1JWak0mYKgCoD7qaM6\nXRbGlSLyoYg8KSJjk1UZGyNwGNHc67jtr0VQOY/bssmJXVdQxuDx9hV2bkLpRAYhjY+1qDzoXhl7\nGl+0n5/OYFV2gdXcx55G18PPT+dvuQ4sTvw8t+MWda+DEQSui2j8WdyGtXnXIigcHTqWY+s52jH8\nikP3SBQ5BaT3Z3Po9waAmq9RK64+LXiuoW/s6aH/2p9uGMO2l7TAFVvnvGF2TkjlPI6Jcad3p9EI\nNpPaCpDpj5rO+RTm5R5Wbc8prgj2oWnXcWew78pn2WNmHEgGcNo3uHUnOp7otDGOsU79xi7kc3a/\ny0+kRprtBL1tfwDuPpVM1LRFxVwKw6oFrMMbd5GRFowkQzOpvSYGVj6T146ZzaQEM8Ye+0sGge8/\n27bLtpfs8fv/jDEYybRZZq2NNsd+9UPAPfM5qx6ge6z9CPDwhZbGsQsBKOCBczkOzKRH0/YGB3Zw\n/BuFp3Iu+/HZW4E/3qW/K97KOuaW0K9v6rnhKcZdXMWlpMoqEWXT2BZjFzLxwp0Jv+NVvj9zr2Ob\nusIkfwR/JsXajMHiCrb3rreA+04jfQv1Z20fWmzrtWwe44JJRqqDxc8C+LVSKiIi3wTwKIDF4ZNE\n5GYANwPAuHHj+vSgrhhBliP7lv4HffgmCNVXFI8B/vpNBhoTwRcfBPZ8yH3D1C5bFlz754I7gqlx\n599BX7XREkonWC379Fs4YN3rC8vI2MedYTVQF0YAmA/wABz8N71mXS0GWbl2PzPK0Pnyk9RsSscD\nD+puNJrw3220331IBMZVduwgZ/WOmMSA9+cu7S6oZlwBfH2cDZZPOg/42kuW+Y05VcdlWixtpn4u\nA8gtJv3uxEGD635LGtUJ4Cm9VHlxJfuhebcNwJZU8j4dR3lvt67mWef9I7NMXKus5utUBgpHkWlM\nW8qA915nIqHRPCdfAFz7BPt/0mIGmHfoT6Nedi8w+2pg9cNM7z0eAS78EXCarnNuEXDzawxqT72I\n1qwIlYbVD3V/1jWPkcl1HiNjBxgA3vw8tVjjD7/sHtvPW15gkPnANrZHQRnLO9uAYbN4Td1qAMKJ\nmgVlFBaV82y+/9iFHOt/9Rsy8d/dRDfMpPMYtB4zBzj1Kj7rj//B+xsrJ6cQuPFFZpftehM4dDX7\nfvTnKCRdNyNAoWOsGtNPl99H68SMl4t/Yl1GxzvsktLDqhhfm3NtcJznFJL2vRsY0zEJCiYWtPNN\n0n/NY8wqHHu6kxarmEW180/2mUlCMgVBPQBXw6/WZV1QSh1w/j4E4E5EgVLqAQAPAEBNTU2f7KSO\nTidYbJCR2d290FeEmWY8KJvc3YoIu1yKRgFwmHThyGAWhIvcImDGZdGPVceg01hDhSOD5dHcSK5F\nEA3lM/kzuftZeZaRmI/MJIps55nNu4FTr2aZscJcZGTQKjIQCWr12Xnsp4b3rKViGHTYchkzC1Fh\naGzazv+ZudTQS7Q27sYfcovJ7EoqyBBMdpl5Vv5wO0nPINyHVfNtlotBl8soA5h2sS0vcaybeV/W\n7pF5dEtIJoWAm81jFIPKecH2dNfcMvsmvRewwc6MLK4X5QZFx8y2rsXOCAVBvW5vN6uuaLQWLG10\nuc260h6b/udBekVsQsGb/0mmWlIZdHPt2WBODtJSvYDZaeuesBlvVQuYlFAeJWTpzgsq0XV2x25x\nOTDri9zvWlOqjcpAbjHHzWHHEZJdQNr3bmBfZmj+U1IJ7NhCd2LZVEuzSbowWPVQ9FWHBxjJdA2t\nBjBFRCaKSA6AawAsd08QEfftuwxAKNI4cDAWQXZWMkkegjCfzXQtgljoTRAY5BRS8zZxj/7gyL7g\n//4K7vDENeOyCVsXvaHEESQifLGPHaRmnV1IbdYE+o0bpOtZlVFvGRPhxRCjLY4IBGMcpt0NvaNn\ndE/pjOd5YSHkIhxEBkKWj95v3q0Fo5OgkVtiXVyJtL1RUMKC27TpqGnBRBCAbdDeyvTirPyg6zKM\n/FKO85zi6Ong4XO7nu/UJ/zhKneSpEFxBVNj61ZFb8euus9nllKSl8NIGldUSnUC+BaAP4AM/jdK\nqY0i8i8iYlSeW0Vko4isA3ArgK8mqz5Rg8Uedu0TY7b3hKw4YykiOhCbIMOLBrPsgtFIe3pp4oG5\nPiwAwmmuvSGnkKa+uc5s69da4ZBbrDXfbPuszBybqRMvDMMxMYdYgsC0t5sdZvZjWYQ9PQ/obrGE\nn1dcQcugpLo78wzERSqDa3G5n34NJxT0BENPWHh0CcEo48OU7XiVSRg9JYeYZU7iEU7Z+VY5cgWT\nm22XXWjbwR3wkzwwAAAKnElEQVS7JRV24l4069uguoZuvb3ruTBltI9TDQCSyhWVUs8rpaYqpU5R\nSv2rLvuhUmq53r9dKTVTKTVHKXWeUmpzz3fsO9o7dfqoFwRBGJfW7Kt7P9d82vGs7/Z+7qhpNvOp\nP5h/PbeL/pZWS3kMl028GH8mX96K2fxfOoHaaV/qWjHb3scwjr0b7BLjZVOC962YQ9dJolbSqGlk\nKJWaYcQSBKOmU9Cc4oTZxi4kYzrl/PifZ+6flR90lUTD5PMZg5q+1LaFgctMiyvY7iYDyv2+RyIJ\nBRPOpktpTOhZpeMpmCd3CzHSL2+Y8aTP9/6Mijnxj4dowiwzywrtnAKd/FFsJwECwGjtlpOM4KKM\nYRgrYv2TwAvfd77pMbBIdbD4pKFrraGsfroqPmuomA3804Howd8wMrN4rptSGQvXP4OuBcz6g/P/\nF3DeD8hAFtwQDFj3BcPHAbd9Yu+TNwz4fm0wRTNefOVpGxg0jOBEp/VRL/0pulJMAWDxD60rLhGc\nshi4bReXNd71ZmxBUDoeuL0u2EaFZaQvkcw4c/9Yz3Fx6TLSJBnd0xxziylkzbpVxko6digoCBJx\nDY08Bbh9d/dxkFsMfG9bdDozMoBvr+HKrfFYqVc9En998ksZFwrTYBIFsgsY8P9+bdCiHn8G8L0d\nbLeCHtxvw8YyQ+/9x/g/msUzAEgb9TiwxIRHEPEIAffceDTajEwbGOsPRGjKi/RfCBiE72Punygy\nsyyNrlZrtLiMjKDQzMhIrK0NTBt0MegeGEe0NsrKTYw+w5jiEQQZmaxbRmZ02rqC8s4nX83WuJ0S\ncQ0BscdBT3TmFDKzJ+6xG4eyA9i+CLsWjXsop1CP3SgCqrCsZyEA8NrqGqYDZxcEJzgOINKGK/oY\ngUdSkVdiX/6BykQLIxFNvT/ILWGWUW9MKh6Es7JMDCG32GGi/Zxrkkp0CbMoFgEQnG/SV3TNi5jX\nN0UiDqSha8gLAo8koaSS684kmoEUL/IT0NT7A5Hoqa19gdH2owqCPgSLBxvyS8HMsDHB8gEVBKGZ\n8klA2giCiHcNeSQbM6/oW6whXoxbBIw7k0HoZGPmF7sHf/uCqV/gfAIzH8RkDuUWMfA76Tz7tbyh\niMnn62UrQq4fIwiiffUwUVSfxoC8WZE4CUgbQdC16JwXBB7Jwnn/mNz7l00BvvZC7+cNBP78roG5\nz4zLg6vJuhZB1QLg+qcH5jmpwswrok/wHEiLILeI32NIItJGEHx50ThcMrsCedleEHh4pAxdgiDK\nku2fJXRZBHFO4ksx0kYQFOdlozivn6uMenh49A9dWUNFPZ831GHoGwiL4CTAq8ceHh4nD10WwWdc\nEAxkjOAkIG0sAg8Pj0GAWVcysNrbOj5DHTOvAKDs5yoHOUSdhI8eDCRqamrUmjWp+YqPh4eHx1CF\niKxVSkWdmuxdQx4eHh5pDi8IPDw8PNIcXhB4eHh4pDm8IPDw8PBIc3hB4OHh4ZHm8ILAw8PDI83h\nBYGHh4dHmsMLAg8PD480x5CbUCYi+wHs6uPlZQCaBrA6qYSnZXDC0zI44WkBxiulRkU7MOQEQX8g\nImtizawbavC0DE54WgYnPC09w7uGPDw8PNIcXhB4eHh4pDnSTRA8kOoKDCA8LYMTnpbBCU9LD0ir\nGIGHh4eHR3ekm0Xg4eHh4RGCFwQeHh4eaY60EQQicrGIbBGR7SJyW6rrkyhEZKeIrBeRD0RkjS4b\nISIvi8g2vS1NdT2jQUQeEZF9IrLBKYtadyGW6X76UETmp67m3RGDljtEpF73zQcistQ5drumZYuI\nfCE1te4OERkrIq+JyEcislFEvqPLh1y/9EDLUOyXPBFZJSLrNC3/rMsnisi7us5PiEiOLs/V/7fr\n4xP69GCl1Gf+ByATwA4AkwDkAFgHYEaq65UgDTsBlIXK7gRwm96/DcC/p7qeMep+DoD5ADb0VncA\nSwG8AEAALALwbqrrHwctdwD4+yjnztBjLRfARD0GM1NNg65bBYD5er8YwFZd3yHXLz3QMhT7RQAU\n6f1sAO/q9v4NgGt0+f0AbtH7fwPgfr1/DYAn+vLcdLEIFgLYrpSqVUq1A3gcwOUprtNA4HIAj+r9\nRwH8RQrrEhNKqT8COBgqjlX3ywH8UhHvABguIhUnp6a9IwYtsXA5gMeVUhGl1McAtoNjMeVQSjUq\npd7T+60ANgGowhDslx5oiYXB3C9KKXVE/83WPwVgMYAndXm4X0x/PQngfBGRRJ+bLoKgCsBu538d\neh4ogxEKwEsislZEbtZl5UqpRr2/B0B5aqrWJ8Sq+1Dtq29pl8kjjotuSNCi3QnzQO1zSPdLiBZg\nCPaLiGSKyAcA9gF4GbRYDiulOvUpbn27aNHHmwGMTPSZ6SIIPgs4Wyk1H8ASAH8rIue4BxVtwyGZ\nCzyU667xXwBOATAXQCOAn6a2OvFDRIoAPAXgu0qpFvfYUOuXKLQMyX5RSh1XSs0FUA1aKtOT/cx0\nEQT1AMY6/6t12ZCBUqpeb/cB+D04QPYa81xv96WuhgkjVt2HXF8ppfbql/cEgAdh3QyDmhYRyQYZ\n52NKqd/p4iHZL9FoGar9YqCUOgzgNQBngK64LH3IrW8XLfr4MAAHEn1WugiC1QCm6Mh7DhhUWZ7i\nOsUNESkUkWKzD+AiABtAGm7Qp90A4JnU1LBPiFX35QCu11kqiwA0O66KQYmQr/wKsG8A0nKNzuyY\nCGAKgFUnu37RoP3IDwPYpJT6P86hIdcvsWgZov0ySkSG6/18ABeCMY/XAFylTwv3i+mvqwC8qi25\nxJDqKPnJ+oFZD1tBf9sPUl2fBOs+CcxyWAdgo6k/6At8BcA2ACsAjEh1XWPU/9egad4B+je/Hqvu\nYNbEfbqf1gOoSXX946DlV7quH+oXs8I5/weali0AlqS6/k69zgbdPh8C+ED/lg7FfumBlqHYL7MB\nvK/rvAHAD3X5JFBYbQfwWwC5ujxP/9+uj0/qy3P9EhMeHh4eaY50cQ15eHh4eMSAFwQeHh4eaQ4v\nCDw8PDzSHF4QeHh4eKQ5vCDw8PDwSHN4QeDhEYKIHHdWrPxABnC1WhGZ4K5c6uExGJDV+ykeHmmH\nY4pT/D080gLeIvDwiBPCb0LcKfwuxCoRmazLJ4jIq3pxs1dEZJwuLxeR3+u15deJyJn6Vpki8qBe\nb/4lPYPUwyNl8ILAw6M78kOuoS85x5qVUqcCuBfA3brsHgCPKqVmA3gMwDJdvgzASqXUHPAbBht1\n+RQA9ymlZgI4DODKJNPj4dEj/MxiD48QROSIUqooSvlOAIuVUrV6kbM9SqmRItIELl/QocsblVJl\nIrIfQLVSKuLcYwKAl5VSU/T/fwCQrZT6cfIp8/CIDm8ReHgkBhVjPxFEnP3j8LE6jxTDCwIPj8Tw\nJWf7tt5/C1zRFgCuA/CG3n8FwC1A18dGhp2sSnp4JAKviXh4dEe+/kKUwYtKKZNCWioiH4Ja/bW6\n7NsA/p+IfA/AfgA36vLvAHhARL4Oav63gCuXengMKvgYgYdHnNAxghqlVFOq6+LhMZDwriEPDw+P\nNIe3CDw8PDzSHN4i8PDw8EhzeEHg4eHhkebwgsDDw8MjzeEFgYeHh0eawwsCDw8PjzTH/wf6aWBp\n4/NVigAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "# Evaluate on test data\n",
            "40/40 [==============================] - 0s 3ms/sample - loss: 0.4401 - acc: 0.8500\n",
            "test loss, test acc: [0.44008426542859524, 0.85]\n",
            "1\n",
            "EEG_Deep/Data2A/parsed_P01T.mat\n",
            "(0, 12, 4096)\n",
            "(80, 12, 4096)\n",
            "(80, 1)\n",
            "2\n",
            "EEG_Deep/Data2A/parsed_P02T.mat\n",
            "(80, 12, 4096)\n",
            "(160, 12, 4096)\n",
            "(160, 1)\n",
            "3\n",
            "EEG_Deep/Data2A/parsed_P03T.mat\n",
            "(160, 12, 4096)\n",
            "(240, 12, 4096)\n",
            "(240, 1)\n",
            "4\n",
            "EEG_Deep/Data2A/parsed_P04T.mat\n",
            "(240, 12, 4096)\n",
            "(320, 12, 4096)\n",
            "(320, 1)\n",
            "5\n",
            "EEG_Deep/Data2A/parsed_P05T.mat\n",
            "(320, 12, 4096)\n",
            "(400, 12, 4096)\n",
            "(400, 1)\n",
            "6\n",
            "EEG_Deep/Data2A/parsed_P06T.mat\n",
            "(400, 12, 4096)\n",
            "(480, 12, 4096)\n",
            "(480, 1)\n",
            "7\n",
            "EEG_Deep/Data2A/parsed_P07T.mat\n",
            "(480, 12, 4096)\n",
            "(560, 12, 4096)\n",
            "(560, 1)\n",
            "9\n",
            "EEG_Deep/Data2A/parsed_P09T.mat\n",
            "(560, 12, 4096)\n",
            "(640, 12, 4096)\n",
            "(640, 1)\n",
            "10\n",
            "EEG_Deep/Data2A/parsed_P010T.mat\n",
            "(640, 12, 4096)\n",
            "(720, 12, 4096)\n",
            "(720, 1)\n",
            "8\n",
            "EEG_Deep/Data2A/parsed_P08E.mat\n",
            "(0, 12, 4096)\n",
            "(40, 12, 4096)\n",
            "(40, 1)\n",
            "Filtering of training data in progress\n",
            "(720, 12, 4096)\n",
            "Filtering of test data in progress\n",
            "(40, 12, 4096)\n",
            "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
            " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1.]\n",
            "(620, 12, 1536)\n",
            "(620,)\n",
            "(100, 12, 1536)\n",
            "(100,)\n",
            "[[1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]]\n",
            "X_train shape: (620, 1, 12, 1536)\n",
            "620 train samples\n",
            "100 val samples\n",
            "(40, 12, 1536)\n",
            "(40, 1)\n",
            "X_train shape: (40, 1, 12, 1536)\n",
            "40 train samples\n",
            "Train on 620 samples, validate on 100 samples\n",
            "Epoch 1/300\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 1.16257, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.9404 - acc: 0.5177 - val_loss: 1.1626 - val_acc: 0.5000\n",
            "Epoch 2/300\n",
            "\n",
            "Epoch 00002: val_loss improved from 1.16257 to 1.01347, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.7642 - acc: 0.5694 - val_loss: 1.0135 - val_acc: 0.5400\n",
            "Epoch 3/300\n",
            "\n",
            "Epoch 00003: val_loss improved from 1.01347 to 0.92139, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.7265 - acc: 0.6000 - val_loss: 0.9214 - val_acc: 0.5200\n",
            "Epoch 4/300\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.92139 to 0.83670, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.6906 - acc: 0.6516 - val_loss: 0.8367 - val_acc: 0.5700\n",
            "Epoch 5/300\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.83670 to 0.81369, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.6516 - acc: 0.6839 - val_loss: 0.8137 - val_acc: 0.5400\n",
            "Epoch 6/300\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.81369 to 0.77150, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.5940 - acc: 0.7290 - val_loss: 0.7715 - val_acc: 0.5500\n",
            "Epoch 7/300\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.77150\n",
            "620/620 - 1s - loss: 0.5587 - acc: 0.7613 - val_loss: 0.7719 - val_acc: 0.5100\n",
            "Epoch 8/300\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.77150\n",
            "620/620 - 1s - loss: 0.5411 - acc: 0.7435 - val_loss: 0.8873 - val_acc: 0.5000\n",
            "Epoch 9/300\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.77150\n",
            "620/620 - 1s - loss: 0.5308 - acc: 0.7710 - val_loss: 0.8305 - val_acc: 0.5200\n",
            "Epoch 10/300\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.77150\n",
            "620/620 - 1s - loss: 0.5230 - acc: 0.7597 - val_loss: 0.7833 - val_acc: 0.5300\n",
            "Epoch 11/300\n",
            "\n",
            "Epoch 00011: val_loss improved from 0.77150 to 0.67994, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.5017 - acc: 0.7581 - val_loss: 0.6799 - val_acc: 0.5700\n",
            "Epoch 12/300\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.5249 - acc: 0.7484 - val_loss: 0.7759 - val_acc: 0.5000\n",
            "Epoch 13/300\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.5034 - acc: 0.7645 - val_loss: 1.0957 - val_acc: 0.5100\n",
            "Epoch 14/300\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.5081 - acc: 0.7726 - val_loss: 1.0461 - val_acc: 0.5200\n",
            "Epoch 15/300\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.4864 - acc: 0.7677 - val_loss: 0.7964 - val_acc: 0.5300\n",
            "Epoch 16/300\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.4850 - acc: 0.7613 - val_loss: 0.9626 - val_acc: 0.5400\n",
            "Epoch 17/300\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.4827 - acc: 0.7726 - val_loss: 1.2414 - val_acc: 0.5000\n",
            "Epoch 18/300\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.4808 - acc: 0.7774 - val_loss: 1.2168 - val_acc: 0.5000\n",
            "Epoch 19/300\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.4605 - acc: 0.7839 - val_loss: 0.9318 - val_acc: 0.5200\n",
            "Epoch 20/300\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.4659 - acc: 0.7726 - val_loss: 0.9727 - val_acc: 0.5200\n",
            "Epoch 21/300\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.4931 - acc: 0.7645 - val_loss: 0.8314 - val_acc: 0.5400\n",
            "Epoch 22/300\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.4594 - acc: 0.7871 - val_loss: 1.2227 - val_acc: 0.5200\n",
            "Epoch 23/300\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.4578 - acc: 0.7935 - val_loss: 1.1298 - val_acc: 0.5200\n",
            "Epoch 24/300\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.4686 - acc: 0.7613 - val_loss: 1.1753 - val_acc: 0.5100\n",
            "Epoch 25/300\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.4569 - acc: 0.7710 - val_loss: 1.1544 - val_acc: 0.5100\n",
            "Epoch 26/300\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.4330 - acc: 0.7968 - val_loss: 0.8115 - val_acc: 0.5400\n",
            "Epoch 27/300\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.4698 - acc: 0.8065 - val_loss: 1.0221 - val_acc: 0.5200\n",
            "Epoch 28/300\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.4420 - acc: 0.7871 - val_loss: 0.9498 - val_acc: 0.5300\n",
            "Epoch 29/300\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.4369 - acc: 0.8097 - val_loss: 0.8267 - val_acc: 0.5200\n",
            "Epoch 30/300\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.4543 - acc: 0.7935 - val_loss: 0.9540 - val_acc: 0.5200\n",
            "Epoch 31/300\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.4573 - acc: 0.7839 - val_loss: 1.1577 - val_acc: 0.5200\n",
            "Epoch 32/300\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.4286 - acc: 0.7935 - val_loss: 0.7479 - val_acc: 0.5400\n",
            "Epoch 33/300\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.4485 - acc: 0.7806 - val_loss: 0.8343 - val_acc: 0.5100\n",
            "Epoch 34/300\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.4363 - acc: 0.8097 - val_loss: 1.0662 - val_acc: 0.5300\n",
            "Epoch 35/300\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.4141 - acc: 0.8113 - val_loss: 1.0713 - val_acc: 0.5300\n",
            "Epoch 36/300\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.4221 - acc: 0.7984 - val_loss: 0.7106 - val_acc: 0.5500\n",
            "Epoch 37/300\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.4076 - acc: 0.8177 - val_loss: 1.1187 - val_acc: 0.5100\n",
            "Epoch 38/300\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.4346 - acc: 0.7919 - val_loss: 1.0536 - val_acc: 0.5100\n",
            "Epoch 39/300\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.4380 - acc: 0.7823 - val_loss: 1.0653 - val_acc: 0.5200\n",
            "Epoch 40/300\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3841 - acc: 0.8242 - val_loss: 1.1782 - val_acc: 0.5200\n",
            "Epoch 41/300\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.4472 - acc: 0.7790 - val_loss: 0.8514 - val_acc: 0.5300\n",
            "Epoch 42/300\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.4173 - acc: 0.8000 - val_loss: 0.8536 - val_acc: 0.5500\n",
            "Epoch 43/300\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.4456 - acc: 0.7935 - val_loss: 1.0765 - val_acc: 0.5300\n",
            "Epoch 44/300\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.4538 - acc: 0.7855 - val_loss: 1.1160 - val_acc: 0.5100\n",
            "Epoch 45/300\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3910 - acc: 0.8258 - val_loss: 1.2581 - val_acc: 0.5100\n",
            "Epoch 46/300\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.4283 - acc: 0.8048 - val_loss: 0.7235 - val_acc: 0.5400\n",
            "Epoch 47/300\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.4126 - acc: 0.8161 - val_loss: 1.1114 - val_acc: 0.5300\n",
            "Epoch 48/300\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.4040 - acc: 0.8000 - val_loss: 0.8794 - val_acc: 0.5300\n",
            "Epoch 49/300\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.4502 - acc: 0.7806 - val_loss: 0.7526 - val_acc: 0.5200\n",
            "Epoch 50/300\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.4074 - acc: 0.8290 - val_loss: 1.2581 - val_acc: 0.5300\n",
            "Epoch 51/300\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.4065 - acc: 0.8113 - val_loss: 1.1151 - val_acc: 0.5300\n",
            "Epoch 52/300\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3905 - acc: 0.8145 - val_loss: 0.9212 - val_acc: 0.5300\n",
            "Epoch 53/300\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.4080 - acc: 0.8145 - val_loss: 0.6945 - val_acc: 0.5500\n",
            "Epoch 54/300\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3991 - acc: 0.8274 - val_loss: 0.7932 - val_acc: 0.5400\n",
            "Epoch 55/300\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.4127 - acc: 0.8129 - val_loss: 0.8435 - val_acc: 0.5400\n",
            "Epoch 56/300\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.4327 - acc: 0.7968 - val_loss: 1.1144 - val_acc: 0.5300\n",
            "Epoch 57/300\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.4383 - acc: 0.8161 - val_loss: 0.8279 - val_acc: 0.5500\n",
            "Epoch 58/300\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.4091 - acc: 0.8097 - val_loss: 1.0840 - val_acc: 0.5200\n",
            "Epoch 59/300\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.4047 - acc: 0.8129 - val_loss: 1.4937 - val_acc: 0.5000\n",
            "Epoch 60/300\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.4120 - acc: 0.8048 - val_loss: 1.2236 - val_acc: 0.5200\n",
            "Epoch 61/300\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.4070 - acc: 0.8145 - val_loss: 1.2477 - val_acc: 0.5100\n",
            "Epoch 62/300\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3836 - acc: 0.8290 - val_loss: 0.8430 - val_acc: 0.5600\n",
            "Epoch 63/300\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.4297 - acc: 0.7903 - val_loss: 1.2589 - val_acc: 0.5300\n",
            "Epoch 64/300\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.4326 - acc: 0.7968 - val_loss: 1.0618 - val_acc: 0.5300\n",
            "Epoch 65/300\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3854 - acc: 0.8274 - val_loss: 1.1823 - val_acc: 0.5200\n",
            "Epoch 66/300\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.4251 - acc: 0.8145 - val_loss: 1.3570 - val_acc: 0.5100\n",
            "Epoch 67/300\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.4047 - acc: 0.8226 - val_loss: 0.9027 - val_acc: 0.5200\n",
            "Epoch 68/300\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3695 - acc: 0.8290 - val_loss: 1.2287 - val_acc: 0.5100\n",
            "Epoch 69/300\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3808 - acc: 0.8226 - val_loss: 0.7231 - val_acc: 0.5400\n",
            "Epoch 70/300\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3919 - acc: 0.8274 - val_loss: 1.1753 - val_acc: 0.5200\n",
            "Epoch 71/300\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3886 - acc: 0.8177 - val_loss: 1.0214 - val_acc: 0.5300\n",
            "Epoch 72/300\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3785 - acc: 0.8371 - val_loss: 0.9112 - val_acc: 0.5500\n",
            "Epoch 73/300\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3830 - acc: 0.8355 - val_loss: 0.8769 - val_acc: 0.5500\n",
            "Epoch 74/300\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3706 - acc: 0.8419 - val_loss: 1.2054 - val_acc: 0.5300\n",
            "Epoch 75/300\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3876 - acc: 0.8258 - val_loss: 1.1935 - val_acc: 0.5200\n",
            "Epoch 76/300\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3804 - acc: 0.8113 - val_loss: 1.0164 - val_acc: 0.5300\n",
            "Epoch 77/300\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3861 - acc: 0.8274 - val_loss: 1.2261 - val_acc: 0.5200\n",
            "Epoch 78/300\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3810 - acc: 0.8177 - val_loss: 0.8404 - val_acc: 0.5900\n",
            "Epoch 79/300\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3966 - acc: 0.8145 - val_loss: 0.9176 - val_acc: 0.5400\n",
            "Epoch 80/300\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3943 - acc: 0.8032 - val_loss: 1.0321 - val_acc: 0.5400\n",
            "Epoch 81/300\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3892 - acc: 0.8355 - val_loss: 0.7861 - val_acc: 0.5900\n",
            "Epoch 82/300\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.4356 - acc: 0.8097 - val_loss: 0.9081 - val_acc: 0.5500\n",
            "Epoch 83/300\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3470 - acc: 0.8419 - val_loss: 0.8528 - val_acc: 0.5400\n",
            "Epoch 84/300\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3900 - acc: 0.8161 - val_loss: 1.0665 - val_acc: 0.5300\n",
            "Epoch 85/300\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3622 - acc: 0.8516 - val_loss: 0.9653 - val_acc: 0.5500\n",
            "Epoch 86/300\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.4007 - acc: 0.8032 - val_loss: 0.8123 - val_acc: 0.5400\n",
            "Epoch 87/300\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.4012 - acc: 0.8129 - val_loss: 0.8855 - val_acc: 0.5700\n",
            "Epoch 88/300\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3821 - acc: 0.8306 - val_loss: 0.8564 - val_acc: 0.5700\n",
            "Epoch 89/300\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3693 - acc: 0.8306 - val_loss: 1.0333 - val_acc: 0.5300\n",
            "Epoch 90/300\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3726 - acc: 0.8435 - val_loss: 1.2640 - val_acc: 0.5100\n",
            "Epoch 91/300\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.4010 - acc: 0.8355 - val_loss: 1.2916 - val_acc: 0.5100\n",
            "Epoch 92/300\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3861 - acc: 0.8145 - val_loss: 1.1212 - val_acc: 0.5200\n",
            "Epoch 93/300\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3762 - acc: 0.8258 - val_loss: 1.0925 - val_acc: 0.5200\n",
            "Epoch 94/300\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3751 - acc: 0.8339 - val_loss: 0.8516 - val_acc: 0.5700\n",
            "Epoch 95/300\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3740 - acc: 0.8210 - val_loss: 1.2625 - val_acc: 0.5300\n",
            "Epoch 96/300\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3549 - acc: 0.8355 - val_loss: 1.3258 - val_acc: 0.5200\n",
            "Epoch 97/300\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3623 - acc: 0.8274 - val_loss: 0.9539 - val_acc: 0.5200\n",
            "Epoch 98/300\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3597 - acc: 0.8500 - val_loss: 0.8662 - val_acc: 0.5500\n",
            "Epoch 99/300\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3437 - acc: 0.8500 - val_loss: 1.0399 - val_acc: 0.5200\n",
            "Epoch 100/300\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3461 - acc: 0.8435 - val_loss: 1.2358 - val_acc: 0.5100\n",
            "Epoch 101/300\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3721 - acc: 0.8258 - val_loss: 1.1555 - val_acc: 0.5100\n",
            "Epoch 102/300\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3772 - acc: 0.8403 - val_loss: 0.8546 - val_acc: 0.5200\n",
            "Epoch 103/300\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3801 - acc: 0.8194 - val_loss: 1.4549 - val_acc: 0.5000\n",
            "Epoch 104/300\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3575 - acc: 0.8355 - val_loss: 0.7886 - val_acc: 0.5600\n",
            "Epoch 105/300\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3587 - acc: 0.8419 - val_loss: 1.1437 - val_acc: 0.5400\n",
            "Epoch 106/300\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3765 - acc: 0.8194 - val_loss: 1.3401 - val_acc: 0.5200\n",
            "Epoch 107/300\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3694 - acc: 0.8419 - val_loss: 1.2011 - val_acc: 0.5200\n",
            "Epoch 108/300\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3529 - acc: 0.8387 - val_loss: 0.9852 - val_acc: 0.5300\n",
            "Epoch 109/300\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3444 - acc: 0.8403 - val_loss: 1.0587 - val_acc: 0.5300\n",
            "Epoch 110/300\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3910 - acc: 0.8258 - val_loss: 1.0107 - val_acc: 0.5200\n",
            "Epoch 111/300\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3571 - acc: 0.8435 - val_loss: 1.1207 - val_acc: 0.5300\n",
            "Epoch 112/300\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3833 - acc: 0.8242 - val_loss: 0.7900 - val_acc: 0.5800\n",
            "Epoch 113/300\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.4131 - acc: 0.8145 - val_loss: 1.1714 - val_acc: 0.5300\n",
            "Epoch 114/300\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3543 - acc: 0.8452 - val_loss: 1.0417 - val_acc: 0.5300\n",
            "Epoch 115/300\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3611 - acc: 0.8387 - val_loss: 0.9003 - val_acc: 0.5600\n",
            "Epoch 116/300\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3649 - acc: 0.8403 - val_loss: 0.9645 - val_acc: 0.5500\n",
            "Epoch 117/300\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3697 - acc: 0.8371 - val_loss: 1.0728 - val_acc: 0.5400\n",
            "Epoch 118/300\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3612 - acc: 0.8258 - val_loss: 1.0218 - val_acc: 0.5400\n",
            "Epoch 119/300\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3723 - acc: 0.8339 - val_loss: 1.1263 - val_acc: 0.5300\n",
            "Epoch 120/300\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3583 - acc: 0.8419 - val_loss: 1.0578 - val_acc: 0.5400\n",
            "Epoch 121/300\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3525 - acc: 0.8435 - val_loss: 1.3146 - val_acc: 0.5200\n",
            "Epoch 122/300\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3576 - acc: 0.8258 - val_loss: 1.2305 - val_acc: 0.5400\n",
            "Epoch 123/300\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3591 - acc: 0.8419 - val_loss: 1.2845 - val_acc: 0.5200\n",
            "Epoch 124/300\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3662 - acc: 0.8339 - val_loss: 0.9903 - val_acc: 0.5200\n",
            "Epoch 125/300\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3488 - acc: 0.8290 - val_loss: 0.8641 - val_acc: 0.5900\n",
            "Epoch 126/300\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3268 - acc: 0.8661 - val_loss: 1.3964 - val_acc: 0.5100\n",
            "Epoch 127/300\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3924 - acc: 0.8242 - val_loss: 0.9775 - val_acc: 0.5500\n",
            "Epoch 128/300\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3839 - acc: 0.8177 - val_loss: 0.8885 - val_acc: 0.5500\n",
            "Epoch 129/300\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3421 - acc: 0.8629 - val_loss: 1.1601 - val_acc: 0.5300\n",
            "Epoch 130/300\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3396 - acc: 0.8565 - val_loss: 0.8648 - val_acc: 0.5400\n",
            "Epoch 131/300\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3650 - acc: 0.8371 - val_loss: 1.2490 - val_acc: 0.5200\n",
            "Epoch 132/300\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3478 - acc: 0.8532 - val_loss: 1.0492 - val_acc: 0.4900\n",
            "Epoch 133/300\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3429 - acc: 0.8468 - val_loss: 0.9044 - val_acc: 0.5500\n",
            "Epoch 134/300\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3870 - acc: 0.7952 - val_loss: 1.2374 - val_acc: 0.5200\n",
            "Epoch 135/300\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3755 - acc: 0.8113 - val_loss: 0.9774 - val_acc: 0.5200\n",
            "Epoch 136/300\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3496 - acc: 0.8484 - val_loss: 1.1489 - val_acc: 0.5300\n",
            "Epoch 137/300\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3828 - acc: 0.8242 - val_loss: 0.9123 - val_acc: 0.5800\n",
            "Epoch 138/300\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3963 - acc: 0.8323 - val_loss: 1.0634 - val_acc: 0.5300\n",
            "Epoch 139/300\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3631 - acc: 0.8387 - val_loss: 0.8078 - val_acc: 0.5600\n",
            "Epoch 140/300\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3623 - acc: 0.8323 - val_loss: 1.1345 - val_acc: 0.5200\n",
            "Epoch 141/300\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3588 - acc: 0.8419 - val_loss: 1.1349 - val_acc: 0.5200\n",
            "Epoch 142/300\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3495 - acc: 0.8516 - val_loss: 0.8607 - val_acc: 0.5900\n",
            "Epoch 143/300\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3560 - acc: 0.8403 - val_loss: 0.9070 - val_acc: 0.5800\n",
            "Epoch 144/300\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3453 - acc: 0.8403 - val_loss: 1.2299 - val_acc: 0.5100\n",
            "Epoch 145/300\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3582 - acc: 0.8371 - val_loss: 1.0537 - val_acc: 0.5200\n",
            "Epoch 146/300\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3393 - acc: 0.8500 - val_loss: 1.3056 - val_acc: 0.5100\n",
            "Epoch 147/300\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3542 - acc: 0.8339 - val_loss: 1.2830 - val_acc: 0.5100\n",
            "Epoch 148/300\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3456 - acc: 0.8484 - val_loss: 1.1131 - val_acc: 0.5100\n",
            "Epoch 149/300\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3427 - acc: 0.8710 - val_loss: 1.4062 - val_acc: 0.5100\n",
            "Epoch 150/300\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3517 - acc: 0.8565 - val_loss: 1.0784 - val_acc: 0.5300\n",
            "Epoch 151/300\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3297 - acc: 0.8581 - val_loss: 1.3074 - val_acc: 0.5200\n",
            "Epoch 152/300\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3173 - acc: 0.8500 - val_loss: 0.9303 - val_acc: 0.5600\n",
            "Epoch 153/300\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3402 - acc: 0.8629 - val_loss: 1.1569 - val_acc: 0.5200\n",
            "Epoch 154/300\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3429 - acc: 0.8516 - val_loss: 1.2404 - val_acc: 0.5100\n",
            "Epoch 155/300\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3455 - acc: 0.8403 - val_loss: 0.9704 - val_acc: 0.5200\n",
            "Epoch 156/300\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3669 - acc: 0.8323 - val_loss: 0.9270 - val_acc: 0.5500\n",
            "Epoch 157/300\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3495 - acc: 0.8516 - val_loss: 1.2972 - val_acc: 0.5100\n",
            "Epoch 158/300\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3621 - acc: 0.8242 - val_loss: 1.0234 - val_acc: 0.5300\n",
            "Epoch 159/300\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3894 - acc: 0.8145 - val_loss: 1.1238 - val_acc: 0.5200\n",
            "Epoch 160/300\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3482 - acc: 0.8452 - val_loss: 0.8628 - val_acc: 0.5600\n",
            "Epoch 161/300\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3665 - acc: 0.8371 - val_loss: 0.8949 - val_acc: 0.5800\n",
            "Epoch 162/300\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3393 - acc: 0.8532 - val_loss: 0.8869 - val_acc: 0.5700\n",
            "Epoch 163/300\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3644 - acc: 0.8419 - val_loss: 1.1791 - val_acc: 0.5300\n",
            "Epoch 164/300\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3515 - acc: 0.8403 - val_loss: 1.2982 - val_acc: 0.5200\n",
            "Epoch 165/300\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3517 - acc: 0.8371 - val_loss: 0.9835 - val_acc: 0.5200\n",
            "Epoch 166/300\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3561 - acc: 0.8355 - val_loss: 1.0744 - val_acc: 0.5300\n",
            "Epoch 167/300\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3259 - acc: 0.8597 - val_loss: 1.0769 - val_acc: 0.5300\n",
            "Epoch 168/300\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3585 - acc: 0.8274 - val_loss: 1.0199 - val_acc: 0.5600\n",
            "Epoch 169/300\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3443 - acc: 0.8435 - val_loss: 1.0467 - val_acc: 0.5500\n",
            "Epoch 170/300\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3443 - acc: 0.8306 - val_loss: 0.7237 - val_acc: 0.6000\n",
            "Epoch 171/300\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3639 - acc: 0.8452 - val_loss: 0.9870 - val_acc: 0.5500\n",
            "Epoch 172/300\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3425 - acc: 0.8452 - val_loss: 1.0506 - val_acc: 0.5700\n",
            "Epoch 173/300\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3603 - acc: 0.8161 - val_loss: 1.1747 - val_acc: 0.5400\n",
            "Epoch 174/300\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3578 - acc: 0.8500 - val_loss: 0.9218 - val_acc: 0.5600\n",
            "Epoch 175/300\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3584 - acc: 0.8484 - val_loss: 0.8874 - val_acc: 0.5600\n",
            "Epoch 176/300\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3804 - acc: 0.8274 - val_loss: 0.8412 - val_acc: 0.5700\n",
            "Epoch 177/300\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3590 - acc: 0.8484 - val_loss: 0.9620 - val_acc: 0.5000\n",
            "Epoch 178/300\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3232 - acc: 0.8532 - val_loss: 1.3623 - val_acc: 0.5100\n",
            "Epoch 179/300\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.2997 - acc: 0.8919 - val_loss: 0.9365 - val_acc: 0.5300\n",
            "Epoch 180/300\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3450 - acc: 0.8516 - val_loss: 0.8740 - val_acc: 0.5600\n",
            "Epoch 181/300\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3661 - acc: 0.8274 - val_loss: 1.1670 - val_acc: 0.5300\n",
            "Epoch 182/300\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3338 - acc: 0.8532 - val_loss: 1.0991 - val_acc: 0.5300\n",
            "Epoch 183/300\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3610 - acc: 0.8355 - val_loss: 1.0668 - val_acc: 0.5400\n",
            "Epoch 184/300\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3469 - acc: 0.8435 - val_loss: 1.2517 - val_acc: 0.5200\n",
            "Epoch 185/300\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3250 - acc: 0.8484 - val_loss: 1.1585 - val_acc: 0.5300\n",
            "Epoch 186/300\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3382 - acc: 0.8548 - val_loss: 0.9017 - val_acc: 0.5300\n",
            "Epoch 187/300\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3453 - acc: 0.8565 - val_loss: 0.8917 - val_acc: 0.5700\n",
            "Epoch 188/300\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3382 - acc: 0.8452 - val_loss: 0.8444 - val_acc: 0.5800\n",
            "Epoch 189/300\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3186 - acc: 0.8532 - val_loss: 0.9816 - val_acc: 0.5400\n",
            "Epoch 190/300\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3381 - acc: 0.8565 - val_loss: 1.4312 - val_acc: 0.5000\n",
            "Epoch 191/300\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3517 - acc: 0.8532 - val_loss: 1.4352 - val_acc: 0.5000\n",
            "Epoch 192/300\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3412 - acc: 0.8532 - val_loss: 1.0853 - val_acc: 0.5400\n",
            "Epoch 193/300\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3253 - acc: 0.8629 - val_loss: 1.5068 - val_acc: 0.5100\n",
            "Epoch 194/300\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3277 - acc: 0.8468 - val_loss: 1.1128 - val_acc: 0.5200\n",
            "Epoch 195/300\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3345 - acc: 0.8565 - val_loss: 1.2743 - val_acc: 0.5200\n",
            "Epoch 196/300\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3254 - acc: 0.8581 - val_loss: 1.0467 - val_acc: 0.5300\n",
            "Epoch 197/300\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3292 - acc: 0.8597 - val_loss: 0.9369 - val_acc: 0.5500\n",
            "Epoch 198/300\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3532 - acc: 0.8516 - val_loss: 0.8881 - val_acc: 0.5600\n",
            "Epoch 199/300\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3370 - acc: 0.8452 - val_loss: 1.3894 - val_acc: 0.5200\n",
            "Epoch 200/300\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3348 - acc: 0.8435 - val_loss: 1.1471 - val_acc: 0.5300\n",
            "Epoch 201/300\n",
            "\n",
            "Epoch 00201: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3210 - acc: 0.8629 - val_loss: 0.9917 - val_acc: 0.5500\n",
            "Epoch 202/300\n",
            "\n",
            "Epoch 00202: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3025 - acc: 0.8871 - val_loss: 0.9614 - val_acc: 0.5700\n",
            "Epoch 203/300\n",
            "\n",
            "Epoch 00203: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3701 - acc: 0.8274 - val_loss: 0.9650 - val_acc: 0.5400\n",
            "Epoch 204/300\n",
            "\n",
            "Epoch 00204: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3471 - acc: 0.8435 - val_loss: 1.6988 - val_acc: 0.5000\n",
            "Epoch 205/300\n",
            "\n",
            "Epoch 00205: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3349 - acc: 0.8403 - val_loss: 1.0149 - val_acc: 0.5100\n",
            "Epoch 206/300\n",
            "\n",
            "Epoch 00206: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3658 - acc: 0.8323 - val_loss: 0.9638 - val_acc: 0.5300\n",
            "Epoch 207/300\n",
            "\n",
            "Epoch 00207: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3220 - acc: 0.8613 - val_loss: 1.1763 - val_acc: 0.5200\n",
            "Epoch 208/300\n",
            "\n",
            "Epoch 00208: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3186 - acc: 0.8645 - val_loss: 1.2412 - val_acc: 0.5100\n",
            "Epoch 209/300\n",
            "\n",
            "Epoch 00209: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3385 - acc: 0.8500 - val_loss: 1.1226 - val_acc: 0.5100\n",
            "Epoch 210/300\n",
            "\n",
            "Epoch 00210: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3224 - acc: 0.8726 - val_loss: 1.2235 - val_acc: 0.5200\n",
            "Epoch 211/300\n",
            "\n",
            "Epoch 00211: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.2873 - acc: 0.8919 - val_loss: 1.1964 - val_acc: 0.5200\n",
            "Epoch 212/300\n",
            "\n",
            "Epoch 00212: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3240 - acc: 0.8435 - val_loss: 1.0328 - val_acc: 0.5700\n",
            "Epoch 213/300\n",
            "\n",
            "Epoch 00213: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3108 - acc: 0.8758 - val_loss: 1.1166 - val_acc: 0.5300\n",
            "Epoch 214/300\n",
            "\n",
            "Epoch 00214: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3574 - acc: 0.8387 - val_loss: 1.2289 - val_acc: 0.5300\n",
            "Epoch 215/300\n",
            "\n",
            "Epoch 00215: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3247 - acc: 0.8661 - val_loss: 1.4572 - val_acc: 0.5100\n",
            "Epoch 216/300\n",
            "\n",
            "Epoch 00216: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3039 - acc: 0.8694 - val_loss: 1.3426 - val_acc: 0.5300\n",
            "Epoch 217/300\n",
            "\n",
            "Epoch 00217: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3305 - acc: 0.8548 - val_loss: 1.4536 - val_acc: 0.5200\n",
            "Epoch 218/300\n",
            "\n",
            "Epoch 00218: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3670 - acc: 0.8210 - val_loss: 1.4567 - val_acc: 0.5000\n",
            "Epoch 219/300\n",
            "\n",
            "Epoch 00219: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3100 - acc: 0.8661 - val_loss: 0.9281 - val_acc: 0.5700\n",
            "Epoch 220/300\n",
            "\n",
            "Epoch 00220: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3313 - acc: 0.8516 - val_loss: 0.8580 - val_acc: 0.5900\n",
            "Epoch 221/300\n",
            "\n",
            "Epoch 00221: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3308 - acc: 0.8597 - val_loss: 1.2950 - val_acc: 0.5200\n",
            "Epoch 222/300\n",
            "\n",
            "Epoch 00222: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3238 - acc: 0.8581 - val_loss: 1.1694 - val_acc: 0.5100\n",
            "Epoch 223/300\n",
            "\n",
            "Epoch 00223: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3186 - acc: 0.8516 - val_loss: 1.1917 - val_acc: 0.5100\n",
            "Epoch 224/300\n",
            "\n",
            "Epoch 00224: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3227 - acc: 0.8548 - val_loss: 1.4855 - val_acc: 0.5200\n",
            "Epoch 225/300\n",
            "\n",
            "Epoch 00225: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3461 - acc: 0.8516 - val_loss: 0.9994 - val_acc: 0.5300\n",
            "Epoch 226/300\n",
            "\n",
            "Epoch 00226: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3178 - acc: 0.8565 - val_loss: 1.1790 - val_acc: 0.5300\n",
            "Epoch 227/300\n",
            "\n",
            "Epoch 00227: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3458 - acc: 0.8306 - val_loss: 1.1955 - val_acc: 0.5200\n",
            "Epoch 228/300\n",
            "\n",
            "Epoch 00228: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3212 - acc: 0.8597 - val_loss: 1.2399 - val_acc: 0.5200\n",
            "Epoch 229/300\n",
            "\n",
            "Epoch 00229: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3136 - acc: 0.8645 - val_loss: 1.0466 - val_acc: 0.5200\n",
            "Epoch 230/300\n",
            "\n",
            "Epoch 00230: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.2975 - acc: 0.8548 - val_loss: 1.4297 - val_acc: 0.5100\n",
            "Epoch 231/300\n",
            "\n",
            "Epoch 00231: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3286 - acc: 0.8516 - val_loss: 1.0993 - val_acc: 0.5200\n",
            "Epoch 232/300\n",
            "\n",
            "Epoch 00232: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3307 - acc: 0.8468 - val_loss: 1.1621 - val_acc: 0.5300\n",
            "Epoch 233/300\n",
            "\n",
            "Epoch 00233: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3101 - acc: 0.8629 - val_loss: 1.1337 - val_acc: 0.5400\n",
            "Epoch 234/300\n",
            "\n",
            "Epoch 00234: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3110 - acc: 0.8581 - val_loss: 1.2534 - val_acc: 0.5300\n",
            "Epoch 235/300\n",
            "\n",
            "Epoch 00235: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.2954 - acc: 0.8742 - val_loss: 1.3348 - val_acc: 0.5100\n",
            "Epoch 236/300\n",
            "\n",
            "Epoch 00236: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3660 - acc: 0.8452 - val_loss: 1.0080 - val_acc: 0.5100\n",
            "Epoch 237/300\n",
            "\n",
            "Epoch 00237: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3413 - acc: 0.8468 - val_loss: 0.9112 - val_acc: 0.5500\n",
            "Epoch 238/300\n",
            "\n",
            "Epoch 00238: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3176 - acc: 0.8645 - val_loss: 1.3242 - val_acc: 0.5200\n",
            "Epoch 239/300\n",
            "\n",
            "Epoch 00239: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3190 - acc: 0.8565 - val_loss: 1.0789 - val_acc: 0.5300\n",
            "Epoch 240/300\n",
            "\n",
            "Epoch 00240: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3331 - acc: 0.8452 - val_loss: 1.3816 - val_acc: 0.5100\n",
            "Epoch 241/300\n",
            "\n",
            "Epoch 00241: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3299 - acc: 0.8532 - val_loss: 1.3625 - val_acc: 0.5200\n",
            "Epoch 242/300\n",
            "\n",
            "Epoch 00242: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3078 - acc: 0.8661 - val_loss: 1.3832 - val_acc: 0.5100\n",
            "Epoch 243/300\n",
            "\n",
            "Epoch 00243: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3445 - acc: 0.8403 - val_loss: 1.4155 - val_acc: 0.5100\n",
            "Epoch 244/300\n",
            "\n",
            "Epoch 00244: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3269 - acc: 0.8629 - val_loss: 1.2208 - val_acc: 0.5300\n",
            "Epoch 245/300\n",
            "\n",
            "Epoch 00245: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3023 - acc: 0.8758 - val_loss: 1.5253 - val_acc: 0.5200\n",
            "Epoch 246/300\n",
            "\n",
            "Epoch 00246: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3345 - acc: 0.8565 - val_loss: 1.7607 - val_acc: 0.5000\n",
            "Epoch 247/300\n",
            "\n",
            "Epoch 00247: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3317 - acc: 0.8484 - val_loss: 1.3024 - val_acc: 0.5200\n",
            "Epoch 248/300\n",
            "\n",
            "Epoch 00248: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.2998 - acc: 0.8726 - val_loss: 1.2729 - val_acc: 0.5300\n",
            "Epoch 249/300\n",
            "\n",
            "Epoch 00249: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3183 - acc: 0.8597 - val_loss: 1.4461 - val_acc: 0.5200\n",
            "Epoch 250/300\n",
            "\n",
            "Epoch 00250: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3286 - acc: 0.8597 - val_loss: 1.3746 - val_acc: 0.5200\n",
            "Epoch 251/300\n",
            "\n",
            "Epoch 00251: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.2752 - acc: 0.8839 - val_loss: 1.3580 - val_acc: 0.5300\n",
            "Epoch 252/300\n",
            "\n",
            "Epoch 00252: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3340 - acc: 0.8629 - val_loss: 1.0279 - val_acc: 0.5500\n",
            "Epoch 253/300\n",
            "\n",
            "Epoch 00253: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.2911 - acc: 0.8823 - val_loss: 1.1080 - val_acc: 0.5200\n",
            "Epoch 254/300\n",
            "\n",
            "Epoch 00254: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3258 - acc: 0.8565 - val_loss: 1.2355 - val_acc: 0.5200\n",
            "Epoch 255/300\n",
            "\n",
            "Epoch 00255: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3377 - acc: 0.8435 - val_loss: 0.8725 - val_acc: 0.6000\n",
            "Epoch 256/300\n",
            "\n",
            "Epoch 00256: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3148 - acc: 0.8629 - val_loss: 1.1930 - val_acc: 0.5300\n",
            "Epoch 257/300\n",
            "\n",
            "Epoch 00257: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3216 - acc: 0.8742 - val_loss: 1.2513 - val_acc: 0.5200\n",
            "Epoch 258/300\n",
            "\n",
            "Epoch 00258: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3410 - acc: 0.8452 - val_loss: 1.0969 - val_acc: 0.5500\n",
            "Epoch 259/300\n",
            "\n",
            "Epoch 00259: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3207 - acc: 0.8694 - val_loss: 0.9726 - val_acc: 0.5400\n",
            "Epoch 260/300\n",
            "\n",
            "Epoch 00260: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3194 - acc: 0.8500 - val_loss: 1.2347 - val_acc: 0.5100\n",
            "Epoch 261/300\n",
            "\n",
            "Epoch 00261: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3405 - acc: 0.8484 - val_loss: 1.5865 - val_acc: 0.5100\n",
            "Epoch 262/300\n",
            "\n",
            "Epoch 00262: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3596 - acc: 0.8419 - val_loss: 1.4889 - val_acc: 0.5200\n",
            "Epoch 263/300\n",
            "\n",
            "Epoch 00263: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3404 - acc: 0.8403 - val_loss: 1.3616 - val_acc: 0.5200\n",
            "Epoch 264/300\n",
            "\n",
            "Epoch 00264: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3197 - acc: 0.8758 - val_loss: 1.7663 - val_acc: 0.5100\n",
            "Epoch 265/300\n",
            "\n",
            "Epoch 00265: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3143 - acc: 0.8661 - val_loss: 1.2236 - val_acc: 0.5300\n",
            "Epoch 266/300\n",
            "\n",
            "Epoch 00266: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3165 - acc: 0.8758 - val_loss: 1.3070 - val_acc: 0.5300\n",
            "Epoch 267/300\n",
            "\n",
            "Epoch 00267: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3218 - acc: 0.8661 - val_loss: 1.4059 - val_acc: 0.5200\n",
            "Epoch 268/300\n",
            "\n",
            "Epoch 00268: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3132 - acc: 0.8597 - val_loss: 1.0228 - val_acc: 0.5300\n",
            "Epoch 269/300\n",
            "\n",
            "Epoch 00269: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3226 - acc: 0.8565 - val_loss: 1.4812 - val_acc: 0.5200\n",
            "Epoch 270/300\n",
            "\n",
            "Epoch 00270: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3582 - acc: 0.8339 - val_loss: 1.2172 - val_acc: 0.5100\n",
            "Epoch 271/300\n",
            "\n",
            "Epoch 00271: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3358 - acc: 0.8452 - val_loss: 1.3604 - val_acc: 0.5100\n",
            "Epoch 272/300\n",
            "\n",
            "Epoch 00272: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3100 - acc: 0.8629 - val_loss: 1.3094 - val_acc: 0.5300\n",
            "Epoch 273/300\n",
            "\n",
            "Epoch 00273: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3234 - acc: 0.8500 - val_loss: 1.1942 - val_acc: 0.5300\n",
            "Epoch 274/300\n",
            "\n",
            "Epoch 00274: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3126 - acc: 0.8613 - val_loss: 1.5995 - val_acc: 0.5100\n",
            "Epoch 275/300\n",
            "\n",
            "Epoch 00275: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3383 - acc: 0.8645 - val_loss: 1.4496 - val_acc: 0.5100\n",
            "Epoch 276/300\n",
            "\n",
            "Epoch 00276: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3333 - acc: 0.8403 - val_loss: 1.7476 - val_acc: 0.5100\n",
            "Epoch 277/300\n",
            "\n",
            "Epoch 00277: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.2920 - acc: 0.8613 - val_loss: 1.2923 - val_acc: 0.5200\n",
            "Epoch 278/300\n",
            "\n",
            "Epoch 00278: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3041 - acc: 0.8548 - val_loss: 1.1571 - val_acc: 0.5200\n",
            "Epoch 279/300\n",
            "\n",
            "Epoch 00279: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3087 - acc: 0.8677 - val_loss: 1.0883 - val_acc: 0.5200\n",
            "Epoch 280/300\n",
            "\n",
            "Epoch 00280: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3056 - acc: 0.8581 - val_loss: 1.1420 - val_acc: 0.5200\n",
            "Epoch 281/300\n",
            "\n",
            "Epoch 00281: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3315 - acc: 0.8500 - val_loss: 1.4351 - val_acc: 0.5200\n",
            "Epoch 282/300\n",
            "\n",
            "Epoch 00282: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3919 - acc: 0.8210 - val_loss: 1.2949 - val_acc: 0.5000\n",
            "Epoch 283/300\n",
            "\n",
            "Epoch 00283: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3043 - acc: 0.8677 - val_loss: 1.4015 - val_acc: 0.5200\n",
            "Epoch 284/300\n",
            "\n",
            "Epoch 00284: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3438 - acc: 0.8484 - val_loss: 1.2692 - val_acc: 0.5300\n",
            "Epoch 285/300\n",
            "\n",
            "Epoch 00285: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3133 - acc: 0.8694 - val_loss: 1.0607 - val_acc: 0.5200\n",
            "Epoch 286/300\n",
            "\n",
            "Epoch 00286: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3164 - acc: 0.8581 - val_loss: 1.1171 - val_acc: 0.5200\n",
            "Epoch 287/300\n",
            "\n",
            "Epoch 00287: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3391 - acc: 0.8403 - val_loss: 1.3880 - val_acc: 0.5100\n",
            "Epoch 288/300\n",
            "\n",
            "Epoch 00288: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3416 - acc: 0.8339 - val_loss: 1.1956 - val_acc: 0.5200\n",
            "Epoch 289/300\n",
            "\n",
            "Epoch 00289: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3012 - acc: 0.8677 - val_loss: 1.3936 - val_acc: 0.5200\n",
            "Epoch 290/300\n",
            "\n",
            "Epoch 00290: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3176 - acc: 0.8710 - val_loss: 1.3224 - val_acc: 0.5200\n",
            "Epoch 291/300\n",
            "\n",
            "Epoch 00291: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.2955 - acc: 0.8758 - val_loss: 1.3495 - val_acc: 0.5200\n",
            "Epoch 292/300\n",
            "\n",
            "Epoch 00292: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3091 - acc: 0.8694 - val_loss: 1.4409 - val_acc: 0.5200\n",
            "Epoch 293/300\n",
            "\n",
            "Epoch 00293: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3091 - acc: 0.8677 - val_loss: 1.3093 - val_acc: 0.5200\n",
            "Epoch 294/300\n",
            "\n",
            "Epoch 00294: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3376 - acc: 0.8516 - val_loss: 1.2715 - val_acc: 0.5200\n",
            "Epoch 295/300\n",
            "\n",
            "Epoch 00295: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3108 - acc: 0.8532 - val_loss: 1.3866 - val_acc: 0.5100\n",
            "Epoch 296/300\n",
            "\n",
            "Epoch 00296: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3301 - acc: 0.8758 - val_loss: 1.2937 - val_acc: 0.5100\n",
            "Epoch 297/300\n",
            "\n",
            "Epoch 00297: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3117 - acc: 0.8677 - val_loss: 1.3436 - val_acc: 0.5200\n",
            "Epoch 298/300\n",
            "\n",
            "Epoch 00298: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3170 - acc: 0.8661 - val_loss: 1.2027 - val_acc: 0.5300\n",
            "Epoch 299/300\n",
            "\n",
            "Epoch 00299: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3358 - acc: 0.8516 - val_loss: 1.3333 - val_acc: 0.5100\n",
            "Epoch 300/300\n",
            "\n",
            "Epoch 00300: val_loss did not improve from 0.67994\n",
            "620/620 - 1s - loss: 0.3221 - acc: 0.8516 - val_loss: 1.1596 - val_acc: 0.5400\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "# Evaluate on test data\n",
            "40/40 [==============================] - 0s 3ms/sample - loss: 0.7197 - acc: 0.6250\n",
            "test loss, test acc: [0.7197243425527858, 0.625]\n",
            "1\n",
            "EEG_Deep/Data2A/parsed_P01T.mat\n",
            "(0, 12, 4096)\n",
            "(80, 12, 4096)\n",
            "(80, 1)\n",
            "2\n",
            "EEG_Deep/Data2A/parsed_P02T.mat\n",
            "(80, 12, 4096)\n",
            "(160, 12, 4096)\n",
            "(160, 1)\n",
            "3\n",
            "EEG_Deep/Data2A/parsed_P03T.mat\n",
            "(160, 12, 4096)\n",
            "(240, 12, 4096)\n",
            "(240, 1)\n",
            "4\n",
            "EEG_Deep/Data2A/parsed_P04T.mat\n",
            "(240, 12, 4096)\n",
            "(320, 12, 4096)\n",
            "(320, 1)\n",
            "5\n",
            "EEG_Deep/Data2A/parsed_P05T.mat\n",
            "(320, 12, 4096)\n",
            "(400, 12, 4096)\n",
            "(400, 1)\n",
            "6\n",
            "EEG_Deep/Data2A/parsed_P06T.mat\n",
            "(400, 12, 4096)\n",
            "(480, 12, 4096)\n",
            "(480, 1)\n",
            "7\n",
            "EEG_Deep/Data2A/parsed_P07T.mat\n",
            "(480, 12, 4096)\n",
            "(560, 12, 4096)\n",
            "(560, 1)\n",
            "8\n",
            "EEG_Deep/Data2A/parsed_P08T.mat\n",
            "(560, 12, 4096)\n",
            "(640, 12, 4096)\n",
            "(640, 1)\n",
            "10\n",
            "EEG_Deep/Data2A/parsed_P010T.mat\n",
            "(640, 12, 4096)\n",
            "(720, 12, 4096)\n",
            "(720, 1)\n",
            "9\n",
            "EEG_Deep/Data2A/parsed_P09E.mat\n",
            "(0, 12, 4096)\n",
            "(40, 12, 4096)\n",
            "(40, 1)\n",
            "Filtering of training data in progress\n",
            "(720, 12, 4096)\n",
            "Filtering of test data in progress\n",
            "(40, 12, 4096)\n",
            "[2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 2. 2.\n",
            " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1.]\n",
            "(620, 12, 1536)\n",
            "(620,)\n",
            "(100, 12, 1536)\n",
            "(100,)\n",
            "[[0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]]\n",
            "X_train shape: (620, 1, 12, 1536)\n",
            "620 train samples\n",
            "100 val samples\n",
            "(40, 12, 1536)\n",
            "(40, 1)\n",
            "X_train shape: (40, 1, 12, 1536)\n",
            "40 train samples\n",
            "Train on 620 samples, validate on 100 samples\n",
            "Epoch 1/300\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 1.18613, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.9586 - acc: 0.4694 - val_loss: 1.1861 - val_acc: 0.5500\n",
            "Epoch 2/300\n",
            "\n",
            "Epoch 00002: val_loss improved from 1.18613 to 1.02059, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.7755 - acc: 0.5484 - val_loss: 1.0206 - val_acc: 0.5100\n",
            "Epoch 3/300\n",
            "\n",
            "Epoch 00003: val_loss improved from 1.02059 to 0.90483, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.7305 - acc: 0.5758 - val_loss: 0.9048 - val_acc: 0.5000\n",
            "Epoch 4/300\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.90483 to 0.80931, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.6970 - acc: 0.6306 - val_loss: 0.8093 - val_acc: 0.5000\n",
            "Epoch 5/300\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.80931 to 0.77823, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.6697 - acc: 0.6548 - val_loss: 0.7782 - val_acc: 0.5000\n",
            "Epoch 6/300\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.77823\n",
            "620/620 - 1s - loss: 0.6178 - acc: 0.6919 - val_loss: 0.8257 - val_acc: 0.5000\n",
            "Epoch 7/300\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.77823 to 0.75668, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.5648 - acc: 0.7403 - val_loss: 0.7567 - val_acc: 0.5200\n",
            "Epoch 8/300\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.75668 to 0.67827, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.5719 - acc: 0.7387 - val_loss: 0.6783 - val_acc: 0.5400\n",
            "Epoch 9/300\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.67827\n",
            "620/620 - 1s - loss: 0.5392 - acc: 0.7565 - val_loss: 0.7025 - val_acc: 0.5600\n",
            "Epoch 10/300\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.67827 to 0.57427, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.5469 - acc: 0.7387 - val_loss: 0.5743 - val_acc: 0.7100\n",
            "Epoch 11/300\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.57427\n",
            "620/620 - 1s - loss: 0.5277 - acc: 0.7516 - val_loss: 0.6530 - val_acc: 0.6200\n",
            "Epoch 12/300\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.57427\n",
            "620/620 - 1s - loss: 0.5125 - acc: 0.7565 - val_loss: 0.6267 - val_acc: 0.6600\n",
            "Epoch 13/300\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.57427\n",
            "620/620 - 1s - loss: 0.5318 - acc: 0.7484 - val_loss: 0.6570 - val_acc: 0.6000\n",
            "Epoch 14/300\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.57427\n",
            "620/620 - 1s - loss: 0.5012 - acc: 0.7500 - val_loss: 0.6462 - val_acc: 0.6100\n",
            "Epoch 15/300\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.57427\n",
            "620/620 - 1s - loss: 0.4938 - acc: 0.7694 - val_loss: 0.8077 - val_acc: 0.5800\n",
            "Epoch 16/300\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.57427\n",
            "620/620 - 1s - loss: 0.4983 - acc: 0.7694 - val_loss: 0.6773 - val_acc: 0.5900\n",
            "Epoch 17/300\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.57427\n",
            "620/620 - 1s - loss: 0.4960 - acc: 0.7565 - val_loss: 0.7759 - val_acc: 0.5700\n",
            "Epoch 18/300\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.57427\n",
            "620/620 - 1s - loss: 0.4878 - acc: 0.7645 - val_loss: 0.8311 - val_acc: 0.5600\n",
            "Epoch 19/300\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.57427\n",
            "620/620 - 1s - loss: 0.4676 - acc: 0.7984 - val_loss: 0.6097 - val_acc: 0.6400\n",
            "Epoch 20/300\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.57427\n",
            "620/620 - 1s - loss: 0.4792 - acc: 0.7548 - val_loss: 0.6933 - val_acc: 0.5800\n",
            "Epoch 21/300\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.57427\n",
            "620/620 - 1s - loss: 0.4832 - acc: 0.7935 - val_loss: 0.7773 - val_acc: 0.5800\n",
            "Epoch 22/300\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.57427\n",
            "620/620 - 1s - loss: 0.4746 - acc: 0.7613 - val_loss: 0.8769 - val_acc: 0.5400\n",
            "Epoch 23/300\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.57427\n",
            "620/620 - 1s - loss: 0.4672 - acc: 0.7839 - val_loss: 0.6814 - val_acc: 0.5900\n",
            "Epoch 24/300\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.57427\n",
            "620/620 - 1s - loss: 0.4579 - acc: 0.7855 - val_loss: 0.7415 - val_acc: 0.6000\n",
            "Epoch 25/300\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.57427\n",
            "620/620 - 1s - loss: 0.4519 - acc: 0.7984 - val_loss: 0.8587 - val_acc: 0.5700\n",
            "Epoch 26/300\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.57427\n",
            "620/620 - 1s - loss: 0.4448 - acc: 0.7903 - val_loss: 0.8351 - val_acc: 0.5800\n",
            "Epoch 27/300\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.57427\n",
            "620/620 - 1s - loss: 0.4609 - acc: 0.7774 - val_loss: 0.7979 - val_acc: 0.5800\n",
            "Epoch 28/300\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.57427\n",
            "620/620 - 1s - loss: 0.4630 - acc: 0.7790 - val_loss: 0.7791 - val_acc: 0.5900\n",
            "Epoch 29/300\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.57427\n",
            "620/620 - 1s - loss: 0.4385 - acc: 0.7935 - val_loss: 0.8299 - val_acc: 0.5700\n",
            "Epoch 30/300\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.57427\n",
            "620/620 - 1s - loss: 0.4460 - acc: 0.7839 - val_loss: 0.8813 - val_acc: 0.5700\n",
            "Epoch 31/300\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.57427\n",
            "620/620 - 1s - loss: 0.4339 - acc: 0.7952 - val_loss: 0.8106 - val_acc: 0.6000\n",
            "Epoch 32/300\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.57427\n",
            "620/620 - 1s - loss: 0.4450 - acc: 0.7855 - val_loss: 0.8740 - val_acc: 0.5400\n",
            "Epoch 33/300\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.57427\n",
            "620/620 - 1s - loss: 0.4595 - acc: 0.7806 - val_loss: 0.6776 - val_acc: 0.6100\n",
            "Epoch 34/300\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.57427\n",
            "620/620 - 1s - loss: 0.4402 - acc: 0.7823 - val_loss: 0.7420 - val_acc: 0.6100\n",
            "Epoch 35/300\n",
            "\n",
            "Epoch 00035: val_loss improved from 0.57427 to 0.57409, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.4378 - acc: 0.7887 - val_loss: 0.5741 - val_acc: 0.6500\n",
            "Epoch 36/300\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.57409\n",
            "620/620 - 1s - loss: 0.4467 - acc: 0.7839 - val_loss: 0.8701 - val_acc: 0.5700\n",
            "Epoch 37/300\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.57409\n",
            "620/620 - 1s - loss: 0.4434 - acc: 0.8081 - val_loss: 0.9229 - val_acc: 0.5300\n",
            "Epoch 38/300\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.57409\n",
            "620/620 - 1s - loss: 0.4450 - acc: 0.7887 - val_loss: 0.6616 - val_acc: 0.6300\n",
            "Epoch 39/300\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.57409\n",
            "620/620 - 1s - loss: 0.4209 - acc: 0.8032 - val_loss: 0.8347 - val_acc: 0.5700\n",
            "Epoch 40/300\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.57409\n",
            "620/620 - 1s - loss: 0.4350 - acc: 0.7774 - val_loss: 0.8182 - val_acc: 0.5900\n",
            "Epoch 41/300\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.57409\n",
            "620/620 - 1s - loss: 0.4364 - acc: 0.7839 - val_loss: 0.7195 - val_acc: 0.6000\n",
            "Epoch 42/300\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.57409\n",
            "620/620 - 1s - loss: 0.4522 - acc: 0.7823 - val_loss: 1.3081 - val_acc: 0.5000\n",
            "Epoch 43/300\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.57409\n",
            "620/620 - 1s - loss: 0.4974 - acc: 0.7677 - val_loss: 0.7187 - val_acc: 0.6200\n",
            "Epoch 44/300\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.57409\n",
            "620/620 - 1s - loss: 0.4211 - acc: 0.8097 - val_loss: 0.7014 - val_acc: 0.6000\n",
            "Epoch 45/300\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.57409\n",
            "620/620 - 1s - loss: 0.4348 - acc: 0.8000 - val_loss: 1.0002 - val_acc: 0.5300\n",
            "Epoch 46/300\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.57409\n",
            "620/620 - 1s - loss: 0.4306 - acc: 0.8000 - val_loss: 0.6016 - val_acc: 0.6300\n",
            "Epoch 47/300\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.57409\n",
            "620/620 - 1s - loss: 0.4304 - acc: 0.8000 - val_loss: 0.6450 - val_acc: 0.6300\n",
            "Epoch 48/300\n",
            "\n",
            "Epoch 00048: val_loss improved from 0.57409 to 0.57047, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.4522 - acc: 0.7758 - val_loss: 0.5705 - val_acc: 0.6500\n",
            "Epoch 49/300\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.4591 - acc: 0.7661 - val_loss: 0.8675 - val_acc: 0.5500\n",
            "Epoch 50/300\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.4376 - acc: 0.8032 - val_loss: 0.9864 - val_acc: 0.5200\n",
            "Epoch 51/300\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.4127 - acc: 0.8048 - val_loss: 0.8498 - val_acc: 0.5900\n",
            "Epoch 52/300\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.4224 - acc: 0.7935 - val_loss: 1.1104 - val_acc: 0.5100\n",
            "Epoch 53/300\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.4318 - acc: 0.7935 - val_loss: 0.9350 - val_acc: 0.5300\n",
            "Epoch 54/300\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3934 - acc: 0.8194 - val_loss: 0.7284 - val_acc: 0.5900\n",
            "Epoch 55/300\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.4004 - acc: 0.8306 - val_loss: 0.8850 - val_acc: 0.5700\n",
            "Epoch 56/300\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.4210 - acc: 0.8194 - val_loss: 0.6934 - val_acc: 0.5900\n",
            "Epoch 57/300\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.4080 - acc: 0.8016 - val_loss: 0.9594 - val_acc: 0.5500\n",
            "Epoch 58/300\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.4088 - acc: 0.8210 - val_loss: 1.0142 - val_acc: 0.5500\n",
            "Epoch 59/300\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.4287 - acc: 0.8274 - val_loss: 0.7807 - val_acc: 0.6000\n",
            "Epoch 60/300\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.4369 - acc: 0.7984 - val_loss: 0.6109 - val_acc: 0.6400\n",
            "Epoch 61/300\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.4322 - acc: 0.7855 - val_loss: 0.7512 - val_acc: 0.6000\n",
            "Epoch 62/300\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.4112 - acc: 0.7935 - val_loss: 0.8526 - val_acc: 0.5700\n",
            "Epoch 63/300\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.4290 - acc: 0.7806 - val_loss: 0.7625 - val_acc: 0.6000\n",
            "Epoch 64/300\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.4225 - acc: 0.8194 - val_loss: 1.0138 - val_acc: 0.5300\n",
            "Epoch 65/300\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.4043 - acc: 0.8129 - val_loss: 0.7537 - val_acc: 0.5900\n",
            "Epoch 66/300\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3850 - acc: 0.8210 - val_loss: 0.7132 - val_acc: 0.6200\n",
            "Epoch 67/300\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.4346 - acc: 0.7984 - val_loss: 0.8659 - val_acc: 0.5700\n",
            "Epoch 68/300\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.4094 - acc: 0.8371 - val_loss: 0.7598 - val_acc: 0.6300\n",
            "Epoch 69/300\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.4197 - acc: 0.8129 - val_loss: 0.6762 - val_acc: 0.6600\n",
            "Epoch 70/300\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.4323 - acc: 0.8032 - val_loss: 0.8593 - val_acc: 0.5700\n",
            "Epoch 71/300\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.4264 - acc: 0.7984 - val_loss: 0.8682 - val_acc: 0.5900\n",
            "Epoch 72/300\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.4244 - acc: 0.8097 - val_loss: 0.7675 - val_acc: 0.6000\n",
            "Epoch 73/300\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.4202 - acc: 0.8097 - val_loss: 1.1244 - val_acc: 0.5100\n",
            "Epoch 74/300\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.4001 - acc: 0.8129 - val_loss: 0.8775 - val_acc: 0.5600\n",
            "Epoch 75/300\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3948 - acc: 0.8242 - val_loss: 1.0673 - val_acc: 0.5300\n",
            "Epoch 76/300\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.4175 - acc: 0.8161 - val_loss: 0.6595 - val_acc: 0.6200\n",
            "Epoch 77/300\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3898 - acc: 0.8194 - val_loss: 0.9044 - val_acc: 0.5700\n",
            "Epoch 78/300\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3924 - acc: 0.8274 - val_loss: 0.7291 - val_acc: 0.6000\n",
            "Epoch 79/300\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3736 - acc: 0.8355 - val_loss: 0.8030 - val_acc: 0.5700\n",
            "Epoch 80/300\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3997 - acc: 0.8065 - val_loss: 0.7661 - val_acc: 0.5600\n",
            "Epoch 81/300\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3859 - acc: 0.8210 - val_loss: 0.8129 - val_acc: 0.5800\n",
            "Epoch 82/300\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.4292 - acc: 0.8048 - val_loss: 0.7891 - val_acc: 0.5800\n",
            "Epoch 83/300\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.4150 - acc: 0.7935 - val_loss: 0.7926 - val_acc: 0.6100\n",
            "Epoch 84/300\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3828 - acc: 0.8177 - val_loss: 0.7935 - val_acc: 0.5600\n",
            "Epoch 85/300\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3943 - acc: 0.8210 - val_loss: 1.0239 - val_acc: 0.5200\n",
            "Epoch 86/300\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.4046 - acc: 0.8194 - val_loss: 0.9353 - val_acc: 0.5400\n",
            "Epoch 87/300\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3949 - acc: 0.8161 - val_loss: 1.0467 - val_acc: 0.5300\n",
            "Epoch 88/300\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.4345 - acc: 0.7919 - val_loss: 0.8031 - val_acc: 0.5700\n",
            "Epoch 89/300\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.4084 - acc: 0.8194 - val_loss: 1.1931 - val_acc: 0.5100\n",
            "Epoch 90/300\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3923 - acc: 0.8242 - val_loss: 0.6712 - val_acc: 0.6400\n",
            "Epoch 91/300\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.4083 - acc: 0.8097 - val_loss: 0.9643 - val_acc: 0.5300\n",
            "Epoch 92/300\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3619 - acc: 0.8452 - val_loss: 0.6161 - val_acc: 0.6500\n",
            "Epoch 93/300\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3842 - acc: 0.8323 - val_loss: 0.6800 - val_acc: 0.6100\n",
            "Epoch 94/300\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.4004 - acc: 0.8048 - val_loss: 0.8760 - val_acc: 0.5600\n",
            "Epoch 95/300\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3762 - acc: 0.8371 - val_loss: 0.7411 - val_acc: 0.6300\n",
            "Epoch 96/300\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3878 - acc: 0.8355 - val_loss: 0.9219 - val_acc: 0.5600\n",
            "Epoch 97/300\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3786 - acc: 0.8306 - val_loss: 0.8044 - val_acc: 0.5900\n",
            "Epoch 98/300\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3876 - acc: 0.8258 - val_loss: 0.7897 - val_acc: 0.5900\n",
            "Epoch 99/300\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3954 - acc: 0.8145 - val_loss: 0.9610 - val_acc: 0.5600\n",
            "Epoch 100/300\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3880 - acc: 0.8274 - val_loss: 0.7638 - val_acc: 0.6200\n",
            "Epoch 101/300\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3849 - acc: 0.8258 - val_loss: 1.0525 - val_acc: 0.5700\n",
            "Epoch 102/300\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3873 - acc: 0.8129 - val_loss: 1.0300 - val_acc: 0.5600\n",
            "Epoch 103/300\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3529 - acc: 0.8387 - val_loss: 0.9602 - val_acc: 0.5900\n",
            "Epoch 104/300\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3955 - acc: 0.8177 - val_loss: 0.8297 - val_acc: 0.6100\n",
            "Epoch 105/300\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.4623 - acc: 0.7677 - val_loss: 0.8367 - val_acc: 0.5600\n",
            "Epoch 106/300\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.4067 - acc: 0.8097 - val_loss: 0.8644 - val_acc: 0.5900\n",
            "Epoch 107/300\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3582 - acc: 0.8371 - val_loss: 0.7784 - val_acc: 0.5800\n",
            "Epoch 108/300\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3786 - acc: 0.8290 - val_loss: 0.7771 - val_acc: 0.6000\n",
            "Epoch 109/300\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3962 - acc: 0.8194 - val_loss: 0.9858 - val_acc: 0.5600\n",
            "Epoch 110/300\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3789 - acc: 0.8323 - val_loss: 0.9280 - val_acc: 0.5800\n",
            "Epoch 111/300\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.4031 - acc: 0.8145 - val_loss: 0.7343 - val_acc: 0.5800\n",
            "Epoch 112/300\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3822 - acc: 0.8258 - val_loss: 0.8850 - val_acc: 0.5500\n",
            "Epoch 113/300\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3706 - acc: 0.8387 - val_loss: 0.9953 - val_acc: 0.5500\n",
            "Epoch 114/300\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.4047 - acc: 0.7968 - val_loss: 0.7397 - val_acc: 0.6400\n",
            "Epoch 115/300\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3732 - acc: 0.8371 - val_loss: 0.7952 - val_acc: 0.5800\n",
            "Epoch 116/300\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3716 - acc: 0.8371 - val_loss: 0.8889 - val_acc: 0.5700\n",
            "Epoch 117/300\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3936 - acc: 0.8048 - val_loss: 0.7348 - val_acc: 0.6300\n",
            "Epoch 118/300\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3740 - acc: 0.8113 - val_loss: 0.9815 - val_acc: 0.5800\n",
            "Epoch 119/300\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3832 - acc: 0.8306 - val_loss: 1.1591 - val_acc: 0.5300\n",
            "Epoch 120/300\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3959 - acc: 0.8113 - val_loss: 0.7631 - val_acc: 0.6300\n",
            "Epoch 121/300\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3687 - acc: 0.8516 - val_loss: 0.8049 - val_acc: 0.6000\n",
            "Epoch 122/300\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3811 - acc: 0.8339 - val_loss: 0.8989 - val_acc: 0.5600\n",
            "Epoch 123/300\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3602 - acc: 0.8452 - val_loss: 0.8136 - val_acc: 0.5700\n",
            "Epoch 124/300\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3703 - acc: 0.8242 - val_loss: 0.9595 - val_acc: 0.5600\n",
            "Epoch 125/300\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.4099 - acc: 0.8048 - val_loss: 0.7229 - val_acc: 0.6200\n",
            "Epoch 126/300\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3697 - acc: 0.8355 - val_loss: 0.9290 - val_acc: 0.5500\n",
            "Epoch 127/300\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3610 - acc: 0.8419 - val_loss: 0.8667 - val_acc: 0.5800\n",
            "Epoch 128/300\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3549 - acc: 0.8452 - val_loss: 0.9287 - val_acc: 0.5700\n",
            "Epoch 129/300\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3703 - acc: 0.8274 - val_loss: 0.7975 - val_acc: 0.6000\n",
            "Epoch 130/300\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3907 - acc: 0.8258 - val_loss: 0.9341 - val_acc: 0.5500\n",
            "Epoch 131/300\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3559 - acc: 0.8532 - val_loss: 1.1318 - val_acc: 0.5500\n",
            "Epoch 132/300\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3748 - acc: 0.8258 - val_loss: 0.8961 - val_acc: 0.5900\n",
            "Epoch 133/300\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3654 - acc: 0.8323 - val_loss: 1.0733 - val_acc: 0.5500\n",
            "Epoch 134/300\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3623 - acc: 0.8419 - val_loss: 0.8487 - val_acc: 0.6000\n",
            "Epoch 135/300\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.4025 - acc: 0.8161 - val_loss: 0.9561 - val_acc: 0.5600\n",
            "Epoch 136/300\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3854 - acc: 0.8161 - val_loss: 1.0184 - val_acc: 0.5600\n",
            "Epoch 137/300\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3718 - acc: 0.8339 - val_loss: 0.9693 - val_acc: 0.5600\n",
            "Epoch 138/300\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3792 - acc: 0.8468 - val_loss: 0.8749 - val_acc: 0.5800\n",
            "Epoch 139/300\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3736 - acc: 0.8371 - val_loss: 0.9091 - val_acc: 0.5600\n",
            "Epoch 140/300\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3691 - acc: 0.8290 - val_loss: 0.7001 - val_acc: 0.6400\n",
            "Epoch 141/300\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3595 - acc: 0.8371 - val_loss: 1.1172 - val_acc: 0.5500\n",
            "Epoch 142/300\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3798 - acc: 0.8194 - val_loss: 0.8426 - val_acc: 0.5900\n",
            "Epoch 143/300\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3245 - acc: 0.8629 - val_loss: 0.8087 - val_acc: 0.5800\n",
            "Epoch 144/300\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3562 - acc: 0.8452 - val_loss: 0.9039 - val_acc: 0.5600\n",
            "Epoch 145/300\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3821 - acc: 0.8274 - val_loss: 1.0379 - val_acc: 0.5500\n",
            "Epoch 146/300\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3677 - acc: 0.8387 - val_loss: 0.7583 - val_acc: 0.6200\n",
            "Epoch 147/300\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3416 - acc: 0.8452 - val_loss: 0.8086 - val_acc: 0.6000\n",
            "Epoch 148/300\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3471 - acc: 0.8565 - val_loss: 0.8078 - val_acc: 0.5900\n",
            "Epoch 149/300\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3717 - acc: 0.8452 - val_loss: 0.9985 - val_acc: 0.5600\n",
            "Epoch 150/300\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3438 - acc: 0.8419 - val_loss: 0.8892 - val_acc: 0.6200\n",
            "Epoch 151/300\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3708 - acc: 0.8274 - val_loss: 0.8989 - val_acc: 0.5800\n",
            "Epoch 152/300\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3474 - acc: 0.8452 - val_loss: 0.9828 - val_acc: 0.5700\n",
            "Epoch 153/300\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3787 - acc: 0.8371 - val_loss: 0.9413 - val_acc: 0.5600\n",
            "Epoch 154/300\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3565 - acc: 0.8387 - val_loss: 0.9596 - val_acc: 0.5500\n",
            "Epoch 155/300\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.4044 - acc: 0.7984 - val_loss: 0.8178 - val_acc: 0.5800\n",
            "Epoch 156/300\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3740 - acc: 0.8242 - val_loss: 0.9347 - val_acc: 0.5600\n",
            "Epoch 157/300\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3920 - acc: 0.8113 - val_loss: 0.6212 - val_acc: 0.6600\n",
            "Epoch 158/300\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3661 - acc: 0.8290 - val_loss: 0.9553 - val_acc: 0.5800\n",
            "Epoch 159/300\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3469 - acc: 0.8516 - val_loss: 1.0472 - val_acc: 0.5500\n",
            "Epoch 160/300\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3194 - acc: 0.8645 - val_loss: 0.7537 - val_acc: 0.6100\n",
            "Epoch 161/300\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3687 - acc: 0.8323 - val_loss: 0.8177 - val_acc: 0.6000\n",
            "Epoch 162/300\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3767 - acc: 0.8387 - val_loss: 1.0006 - val_acc: 0.5400\n",
            "Epoch 163/300\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3734 - acc: 0.8290 - val_loss: 0.9434 - val_acc: 0.5700\n",
            "Epoch 164/300\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3785 - acc: 0.8419 - val_loss: 0.6993 - val_acc: 0.6000\n",
            "Epoch 165/300\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3468 - acc: 0.8371 - val_loss: 0.8301 - val_acc: 0.5900\n",
            "Epoch 166/300\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3750 - acc: 0.8274 - val_loss: 0.9172 - val_acc: 0.5800\n",
            "Epoch 167/300\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3527 - acc: 0.8355 - val_loss: 1.0479 - val_acc: 0.5400\n",
            "Epoch 168/300\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3530 - acc: 0.8468 - val_loss: 0.9261 - val_acc: 0.5600\n",
            "Epoch 169/300\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3309 - acc: 0.8645 - val_loss: 0.9916 - val_acc: 0.5600\n",
            "Epoch 170/300\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3598 - acc: 0.8387 - val_loss: 0.8445 - val_acc: 0.6100\n",
            "Epoch 171/300\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3475 - acc: 0.8516 - val_loss: 0.8631 - val_acc: 0.5800\n",
            "Epoch 172/300\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3617 - acc: 0.8339 - val_loss: 0.9352 - val_acc: 0.5900\n",
            "Epoch 173/300\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3549 - acc: 0.8532 - val_loss: 1.2341 - val_acc: 0.5200\n",
            "Epoch 174/300\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3254 - acc: 0.8355 - val_loss: 0.7535 - val_acc: 0.6100\n",
            "Epoch 175/300\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3482 - acc: 0.8419 - val_loss: 0.8288 - val_acc: 0.6000\n",
            "Epoch 176/300\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3489 - acc: 0.8500 - val_loss: 0.9169 - val_acc: 0.5700\n",
            "Epoch 177/300\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3425 - acc: 0.8597 - val_loss: 0.6702 - val_acc: 0.6500\n",
            "Epoch 178/300\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3421 - acc: 0.8565 - val_loss: 1.0995 - val_acc: 0.5300\n",
            "Epoch 179/300\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3629 - acc: 0.8371 - val_loss: 0.9581 - val_acc: 0.5700\n",
            "Epoch 180/300\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3445 - acc: 0.8452 - val_loss: 0.7967 - val_acc: 0.6000\n",
            "Epoch 181/300\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3483 - acc: 0.8339 - val_loss: 0.7439 - val_acc: 0.6300\n",
            "Epoch 182/300\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3506 - acc: 0.8419 - val_loss: 0.6957 - val_acc: 0.6300\n",
            "Epoch 183/300\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3493 - acc: 0.8452 - val_loss: 0.9659 - val_acc: 0.5800\n",
            "Epoch 184/300\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3432 - acc: 0.8355 - val_loss: 0.8835 - val_acc: 0.5800\n",
            "Epoch 185/300\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3462 - acc: 0.8484 - val_loss: 0.7519 - val_acc: 0.6000\n",
            "Epoch 186/300\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3545 - acc: 0.8468 - val_loss: 0.7888 - val_acc: 0.6300\n",
            "Epoch 187/300\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3442 - acc: 0.8452 - val_loss: 0.9112 - val_acc: 0.5800\n",
            "Epoch 188/300\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3508 - acc: 0.8355 - val_loss: 1.0924 - val_acc: 0.5200\n",
            "Epoch 189/300\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3315 - acc: 0.8516 - val_loss: 0.9477 - val_acc: 0.5900\n",
            "Epoch 190/300\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3570 - acc: 0.8452 - val_loss: 1.0159 - val_acc: 0.5400\n",
            "Epoch 191/300\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3717 - acc: 0.8339 - val_loss: 0.8518 - val_acc: 0.5800\n",
            "Epoch 192/300\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3515 - acc: 0.8565 - val_loss: 0.9154 - val_acc: 0.5900\n",
            "Epoch 193/300\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3432 - acc: 0.8403 - val_loss: 0.9527 - val_acc: 0.5800\n",
            "Epoch 194/300\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3265 - acc: 0.8597 - val_loss: 1.0833 - val_acc: 0.5600\n",
            "Epoch 195/300\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3704 - acc: 0.8210 - val_loss: 0.5990 - val_acc: 0.6600\n",
            "Epoch 196/300\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3942 - acc: 0.8097 - val_loss: 0.7118 - val_acc: 0.6300\n",
            "Epoch 197/300\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3641 - acc: 0.8323 - val_loss: 0.9294 - val_acc: 0.5400\n",
            "Epoch 198/300\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3424 - acc: 0.8581 - val_loss: 1.0470 - val_acc: 0.5600\n",
            "Epoch 199/300\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3406 - acc: 0.8419 - val_loss: 0.7649 - val_acc: 0.6000\n",
            "Epoch 200/300\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3362 - acc: 0.8484 - val_loss: 1.2741 - val_acc: 0.5100\n",
            "Epoch 201/300\n",
            "\n",
            "Epoch 00201: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3225 - acc: 0.8710 - val_loss: 0.8892 - val_acc: 0.5800\n",
            "Epoch 202/300\n",
            "\n",
            "Epoch 00202: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3511 - acc: 0.8403 - val_loss: 0.5901 - val_acc: 0.6600\n",
            "Epoch 203/300\n",
            "\n",
            "Epoch 00203: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3217 - acc: 0.8565 - val_loss: 0.7735 - val_acc: 0.6100\n",
            "Epoch 204/300\n",
            "\n",
            "Epoch 00204: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3141 - acc: 0.8548 - val_loss: 1.0612 - val_acc: 0.5900\n",
            "Epoch 205/300\n",
            "\n",
            "Epoch 00205: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3340 - acc: 0.8484 - val_loss: 0.9393 - val_acc: 0.5800\n",
            "Epoch 206/300\n",
            "\n",
            "Epoch 00206: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3341 - acc: 0.8758 - val_loss: 0.8433 - val_acc: 0.5900\n",
            "Epoch 207/300\n",
            "\n",
            "Epoch 00207: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3209 - acc: 0.8548 - val_loss: 1.0782 - val_acc: 0.5400\n",
            "Epoch 208/300\n",
            "\n",
            "Epoch 00208: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3560 - acc: 0.8484 - val_loss: 0.9424 - val_acc: 0.5700\n",
            "Epoch 209/300\n",
            "\n",
            "Epoch 00209: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3759 - acc: 0.8194 - val_loss: 1.1346 - val_acc: 0.5400\n",
            "Epoch 210/300\n",
            "\n",
            "Epoch 00210: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3333 - acc: 0.8710 - val_loss: 0.9955 - val_acc: 0.5400\n",
            "Epoch 211/300\n",
            "\n",
            "Epoch 00211: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3407 - acc: 0.8597 - val_loss: 1.0079 - val_acc: 0.5400\n",
            "Epoch 212/300\n",
            "\n",
            "Epoch 00212: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3388 - acc: 0.8500 - val_loss: 0.8401 - val_acc: 0.5700\n",
            "Epoch 213/300\n",
            "\n",
            "Epoch 00213: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3516 - acc: 0.8500 - val_loss: 0.8191 - val_acc: 0.5800\n",
            "Epoch 214/300\n",
            "\n",
            "Epoch 00214: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3293 - acc: 0.8645 - val_loss: 1.0472 - val_acc: 0.5600\n",
            "Epoch 215/300\n",
            "\n",
            "Epoch 00215: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3495 - acc: 0.8532 - val_loss: 0.8632 - val_acc: 0.5900\n",
            "Epoch 216/300\n",
            "\n",
            "Epoch 00216: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3203 - acc: 0.8613 - val_loss: 0.6448 - val_acc: 0.6600\n",
            "Epoch 217/300\n",
            "\n",
            "Epoch 00217: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3108 - acc: 0.8661 - val_loss: 1.1611 - val_acc: 0.5300\n",
            "Epoch 218/300\n",
            "\n",
            "Epoch 00218: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3457 - acc: 0.8452 - val_loss: 1.0532 - val_acc: 0.5400\n",
            "Epoch 219/300\n",
            "\n",
            "Epoch 00219: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3323 - acc: 0.8629 - val_loss: 1.1077 - val_acc: 0.5400\n",
            "Epoch 220/300\n",
            "\n",
            "Epoch 00220: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3358 - acc: 0.8500 - val_loss: 0.9583 - val_acc: 0.5700\n",
            "Epoch 221/300\n",
            "\n",
            "Epoch 00221: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3279 - acc: 0.8565 - val_loss: 1.0077 - val_acc: 0.5700\n",
            "Epoch 222/300\n",
            "\n",
            "Epoch 00222: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3395 - acc: 0.8355 - val_loss: 1.1109 - val_acc: 0.5400\n",
            "Epoch 223/300\n",
            "\n",
            "Epoch 00223: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3529 - acc: 0.8468 - val_loss: 0.8493 - val_acc: 0.5800\n",
            "Epoch 224/300\n",
            "\n",
            "Epoch 00224: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3353 - acc: 0.8694 - val_loss: 1.0907 - val_acc: 0.5500\n",
            "Epoch 225/300\n",
            "\n",
            "Epoch 00225: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3334 - acc: 0.8516 - val_loss: 1.0211 - val_acc: 0.5700\n",
            "Epoch 226/300\n",
            "\n",
            "Epoch 00226: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3354 - acc: 0.8516 - val_loss: 0.8602 - val_acc: 0.5900\n",
            "Epoch 227/300\n",
            "\n",
            "Epoch 00227: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3284 - acc: 0.8565 - val_loss: 0.9484 - val_acc: 0.5700\n",
            "Epoch 228/300\n",
            "\n",
            "Epoch 00228: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3625 - acc: 0.8339 - val_loss: 1.0201 - val_acc: 0.5300\n",
            "Epoch 229/300\n",
            "\n",
            "Epoch 00229: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3458 - acc: 0.8403 - val_loss: 0.9001 - val_acc: 0.5600\n",
            "Epoch 230/300\n",
            "\n",
            "Epoch 00230: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3154 - acc: 0.8710 - val_loss: 1.2043 - val_acc: 0.5200\n",
            "Epoch 231/300\n",
            "\n",
            "Epoch 00231: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3509 - acc: 0.8371 - val_loss: 1.0666 - val_acc: 0.5300\n",
            "Epoch 232/300\n",
            "\n",
            "Epoch 00232: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3880 - acc: 0.8177 - val_loss: 0.8452 - val_acc: 0.6300\n",
            "Epoch 233/300\n",
            "\n",
            "Epoch 00233: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3541 - acc: 0.8452 - val_loss: 0.8555 - val_acc: 0.5900\n",
            "Epoch 234/300\n",
            "\n",
            "Epoch 00234: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3405 - acc: 0.8613 - val_loss: 0.9450 - val_acc: 0.5700\n",
            "Epoch 235/300\n",
            "\n",
            "Epoch 00235: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3345 - acc: 0.8452 - val_loss: 0.7639 - val_acc: 0.6400\n",
            "Epoch 236/300\n",
            "\n",
            "Epoch 00236: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3340 - acc: 0.8613 - val_loss: 0.7959 - val_acc: 0.5800\n",
            "Epoch 237/300\n",
            "\n",
            "Epoch 00237: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3659 - acc: 0.8371 - val_loss: 0.7890 - val_acc: 0.6000\n",
            "Epoch 238/300\n",
            "\n",
            "Epoch 00238: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3213 - acc: 0.8500 - val_loss: 1.0224 - val_acc: 0.5700\n",
            "Epoch 239/300\n",
            "\n",
            "Epoch 00239: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3032 - acc: 0.8581 - val_loss: 1.0311 - val_acc: 0.5400\n",
            "Epoch 240/300\n",
            "\n",
            "Epoch 00240: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3167 - acc: 0.8710 - val_loss: 0.8740 - val_acc: 0.6000\n",
            "Epoch 241/300\n",
            "\n",
            "Epoch 00241: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3309 - acc: 0.8500 - val_loss: 0.8397 - val_acc: 0.6000\n",
            "Epoch 242/300\n",
            "\n",
            "Epoch 00242: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3177 - acc: 0.8645 - val_loss: 0.9095 - val_acc: 0.5800\n",
            "Epoch 243/300\n",
            "\n",
            "Epoch 00243: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3196 - acc: 0.8629 - val_loss: 0.8313 - val_acc: 0.6000\n",
            "Epoch 244/300\n",
            "\n",
            "Epoch 00244: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.2899 - acc: 0.8742 - val_loss: 1.0584 - val_acc: 0.5800\n",
            "Epoch 245/300\n",
            "\n",
            "Epoch 00245: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3424 - acc: 0.8516 - val_loss: 1.1064 - val_acc: 0.5400\n",
            "Epoch 246/300\n",
            "\n",
            "Epoch 00246: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.2769 - acc: 0.8855 - val_loss: 0.9619 - val_acc: 0.5800\n",
            "Epoch 247/300\n",
            "\n",
            "Epoch 00247: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3072 - acc: 0.8790 - val_loss: 0.8276 - val_acc: 0.5900\n",
            "Epoch 248/300\n",
            "\n",
            "Epoch 00248: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3562 - acc: 0.8387 - val_loss: 0.9916 - val_acc: 0.5400\n",
            "Epoch 249/300\n",
            "\n",
            "Epoch 00249: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3228 - acc: 0.8548 - val_loss: 0.6893 - val_acc: 0.6500\n",
            "Epoch 250/300\n",
            "\n",
            "Epoch 00250: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3045 - acc: 0.8726 - val_loss: 1.0197 - val_acc: 0.5600\n",
            "Epoch 251/300\n",
            "\n",
            "Epoch 00251: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3428 - acc: 0.8645 - val_loss: 1.0062 - val_acc: 0.5400\n",
            "Epoch 252/300\n",
            "\n",
            "Epoch 00252: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3352 - acc: 0.8516 - val_loss: 0.9792 - val_acc: 0.5300\n",
            "Epoch 253/300\n",
            "\n",
            "Epoch 00253: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3058 - acc: 0.8694 - val_loss: 0.8602 - val_acc: 0.5900\n",
            "Epoch 254/300\n",
            "\n",
            "Epoch 00254: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3329 - acc: 0.8355 - val_loss: 0.8361 - val_acc: 0.6000\n",
            "Epoch 255/300\n",
            "\n",
            "Epoch 00255: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3141 - acc: 0.8677 - val_loss: 1.0275 - val_acc: 0.5300\n",
            "Epoch 256/300\n",
            "\n",
            "Epoch 00256: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3143 - acc: 0.8710 - val_loss: 0.7654 - val_acc: 0.6000\n",
            "Epoch 257/300\n",
            "\n",
            "Epoch 00257: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3266 - acc: 0.8548 - val_loss: 1.0821 - val_acc: 0.5300\n",
            "Epoch 258/300\n",
            "\n",
            "Epoch 00258: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3396 - acc: 0.8387 - val_loss: 1.0848 - val_acc: 0.5500\n",
            "Epoch 259/300\n",
            "\n",
            "Epoch 00259: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3392 - acc: 0.8532 - val_loss: 0.8911 - val_acc: 0.5600\n",
            "Epoch 260/300\n",
            "\n",
            "Epoch 00260: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3236 - acc: 0.8629 - val_loss: 0.8799 - val_acc: 0.6100\n",
            "Epoch 261/300\n",
            "\n",
            "Epoch 00261: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3250 - acc: 0.8532 - val_loss: 0.9456 - val_acc: 0.6000\n",
            "Epoch 262/300\n",
            "\n",
            "Epoch 00262: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3340 - acc: 0.8532 - val_loss: 0.9237 - val_acc: 0.5800\n",
            "Epoch 263/300\n",
            "\n",
            "Epoch 00263: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.2977 - acc: 0.8677 - val_loss: 0.8396 - val_acc: 0.5900\n",
            "Epoch 264/300\n",
            "\n",
            "Epoch 00264: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3128 - acc: 0.8774 - val_loss: 1.0430 - val_acc: 0.5400\n",
            "Epoch 265/300\n",
            "\n",
            "Epoch 00265: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3414 - acc: 0.8532 - val_loss: 0.9169 - val_acc: 0.5700\n",
            "Epoch 266/300\n",
            "\n",
            "Epoch 00266: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3262 - acc: 0.8629 - val_loss: 0.6987 - val_acc: 0.6400\n",
            "Epoch 267/300\n",
            "\n",
            "Epoch 00267: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3262 - acc: 0.8516 - val_loss: 1.0603 - val_acc: 0.5300\n",
            "Epoch 268/300\n",
            "\n",
            "Epoch 00268: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3399 - acc: 0.8468 - val_loss: 0.7866 - val_acc: 0.6300\n",
            "Epoch 269/300\n",
            "\n",
            "Epoch 00269: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3187 - acc: 0.8613 - val_loss: 0.7111 - val_acc: 0.6500\n",
            "Epoch 270/300\n",
            "\n",
            "Epoch 00270: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3430 - acc: 0.8516 - val_loss: 0.8106 - val_acc: 0.6200\n",
            "Epoch 271/300\n",
            "\n",
            "Epoch 00271: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3012 - acc: 0.8694 - val_loss: 0.7791 - val_acc: 0.6000\n",
            "Epoch 272/300\n",
            "\n",
            "Epoch 00272: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3252 - acc: 0.8710 - val_loss: 1.0155 - val_acc: 0.5300\n",
            "Epoch 273/300\n",
            "\n",
            "Epoch 00273: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3389 - acc: 0.8532 - val_loss: 0.8619 - val_acc: 0.5800\n",
            "Epoch 274/300\n",
            "\n",
            "Epoch 00274: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3157 - acc: 0.8484 - val_loss: 0.8861 - val_acc: 0.6000\n",
            "Epoch 275/300\n",
            "\n",
            "Epoch 00275: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3122 - acc: 0.8758 - val_loss: 0.6470 - val_acc: 0.6600\n",
            "Epoch 276/300\n",
            "\n",
            "Epoch 00276: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3240 - acc: 0.8645 - val_loss: 1.1750 - val_acc: 0.5200\n",
            "Epoch 277/300\n",
            "\n",
            "Epoch 00277: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3150 - acc: 0.8629 - val_loss: 1.0084 - val_acc: 0.5600\n",
            "Epoch 278/300\n",
            "\n",
            "Epoch 00278: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3064 - acc: 0.8581 - val_loss: 1.1341 - val_acc: 0.5400\n",
            "Epoch 279/300\n",
            "\n",
            "Epoch 00279: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3299 - acc: 0.8548 - val_loss: 1.4380 - val_acc: 0.5000\n",
            "Epoch 280/300\n",
            "\n",
            "Epoch 00280: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3565 - acc: 0.8452 - val_loss: 0.8088 - val_acc: 0.6300\n",
            "Epoch 281/300\n",
            "\n",
            "Epoch 00281: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3176 - acc: 0.8645 - val_loss: 1.1137 - val_acc: 0.5300\n",
            "Epoch 282/300\n",
            "\n",
            "Epoch 00282: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3267 - acc: 0.8790 - val_loss: 0.8919 - val_acc: 0.5500\n",
            "Epoch 283/300\n",
            "\n",
            "Epoch 00283: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3454 - acc: 0.8581 - val_loss: 0.9892 - val_acc: 0.5700\n",
            "Epoch 284/300\n",
            "\n",
            "Epoch 00284: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3457 - acc: 0.8339 - val_loss: 0.9643 - val_acc: 0.5500\n",
            "Epoch 285/300\n",
            "\n",
            "Epoch 00285: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3060 - acc: 0.8645 - val_loss: 1.0600 - val_acc: 0.5600\n",
            "Epoch 286/300\n",
            "\n",
            "Epoch 00286: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3085 - acc: 0.8758 - val_loss: 0.9812 - val_acc: 0.5800\n",
            "Epoch 287/300\n",
            "\n",
            "Epoch 00287: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3483 - acc: 0.8452 - val_loss: 0.7764 - val_acc: 0.6100\n",
            "Epoch 288/300\n",
            "\n",
            "Epoch 00288: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3348 - acc: 0.8419 - val_loss: 0.9354 - val_acc: 0.5700\n",
            "Epoch 289/300\n",
            "\n",
            "Epoch 00289: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3271 - acc: 0.8500 - val_loss: 0.8022 - val_acc: 0.6100\n",
            "Epoch 290/300\n",
            "\n",
            "Epoch 00290: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3142 - acc: 0.8710 - val_loss: 1.0206 - val_acc: 0.5800\n",
            "Epoch 291/300\n",
            "\n",
            "Epoch 00291: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.2725 - acc: 0.8823 - val_loss: 1.1685 - val_acc: 0.5300\n",
            "Epoch 292/300\n",
            "\n",
            "Epoch 00292: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3151 - acc: 0.8645 - val_loss: 1.0887 - val_acc: 0.5200\n",
            "Epoch 293/300\n",
            "\n",
            "Epoch 00293: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3270 - acc: 0.8613 - val_loss: 0.8920 - val_acc: 0.5600\n",
            "Epoch 294/300\n",
            "\n",
            "Epoch 00294: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3063 - acc: 0.8758 - val_loss: 1.2973 - val_acc: 0.5200\n",
            "Epoch 295/300\n",
            "\n",
            "Epoch 00295: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3166 - acc: 0.8726 - val_loss: 1.0906 - val_acc: 0.5300\n",
            "Epoch 296/300\n",
            "\n",
            "Epoch 00296: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.2933 - acc: 0.8742 - val_loss: 0.8215 - val_acc: 0.5600\n",
            "Epoch 297/300\n",
            "\n",
            "Epoch 00297: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3099 - acc: 0.8661 - val_loss: 0.9474 - val_acc: 0.5600\n",
            "Epoch 298/300\n",
            "\n",
            "Epoch 00298: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3268 - acc: 0.8500 - val_loss: 0.9560 - val_acc: 0.5400\n",
            "Epoch 299/300\n",
            "\n",
            "Epoch 00299: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3067 - acc: 0.8694 - val_loss: 1.1822 - val_acc: 0.5300\n",
            "Epoch 300/300\n",
            "\n",
            "Epoch 00300: val_loss did not improve from 0.57047\n",
            "620/620 - 1s - loss: 0.3003 - acc: 0.8774 - val_loss: 0.7518 - val_acc: 0.6000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBAQEBLQ4/j+yicgrcbDRFQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "# Evaluate on test data\n",
            "40/40 [==============================] - 0s 3ms/sample - loss: 0.7280 - acc: 0.5500\n",
            "test loss, test acc: [0.7279863136820495, 0.55]\n",
            "1\n",
            "EEG_Deep/Data2A/parsed_P01T.mat\n",
            "(0, 12, 4096)\n",
            "(80, 12, 4096)\n",
            "(80, 1)\n",
            "2\n",
            "EEG_Deep/Data2A/parsed_P02T.mat\n",
            "(80, 12, 4096)\n",
            "(160, 12, 4096)\n",
            "(160, 1)\n",
            "3\n",
            "EEG_Deep/Data2A/parsed_P03T.mat\n",
            "(160, 12, 4096)\n",
            "(240, 12, 4096)\n",
            "(240, 1)\n",
            "4\n",
            "EEG_Deep/Data2A/parsed_P04T.mat\n",
            "(240, 12, 4096)\n",
            "(320, 12, 4096)\n",
            "(320, 1)\n",
            "5\n",
            "EEG_Deep/Data2A/parsed_P05T.mat\n",
            "(320, 12, 4096)\n",
            "(400, 12, 4096)\n",
            "(400, 1)\n",
            "7\n",
            "EEG_Deep/Data2A/parsed_P07T.mat\n",
            "(400, 12, 4096)\n",
            "(480, 12, 4096)\n",
            "(480, 1)\n",
            "8\n",
            "EEG_Deep/Data2A/parsed_P08T.mat\n",
            "(480, 12, 4096)\n",
            "(560, 12, 4096)\n",
            "(560, 1)\n",
            "9\n",
            "EEG_Deep/Data2A/parsed_P09T.mat\n",
            "(560, 12, 4096)\n",
            "(640, 12, 4096)\n",
            "(640, 1)\n",
            "10\n",
            "EEG_Deep/Data2A/parsed_P010T.mat\n",
            "(640, 12, 4096)\n",
            "(720, 12, 4096)\n",
            "(720, 1)\n",
            "6\n",
            "EEG_Deep/Data2A/parsed_P06E.mat\n",
            "(0, 12, 4096)\n",
            "(40, 12, 4096)\n",
            "(40, 1)\n",
            "Filtering of training data in progress\n",
            "(720, 12, 4096)\n",
            "Filtering of test data in progress\n",
            "(40, 12, 4096)\n",
            "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
            " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1.]\n",
            "(620, 12, 1536)\n",
            "(620,)\n",
            "(100, 12, 1536)\n",
            "(100,)\n",
            "[[1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]]\n",
            "X_train shape: (620, 1, 12, 1536)\n",
            "620 train samples\n",
            "100 val samples\n",
            "(40, 12, 1536)\n",
            "(40, 1)\n",
            "X_train shape: (40, 1, 12, 1536)\n",
            "40 train samples\n",
            "Train on 620 samples, validate on 100 samples\n",
            "Epoch 1/300\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 1.16935, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.9306 - acc: 0.4823 - val_loss: 1.1693 - val_acc: 0.5400\n",
            "Epoch 2/300\n",
            "\n",
            "Epoch 00002: val_loss improved from 1.16935 to 1.00222, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.7715 - acc: 0.5242 - val_loss: 1.0022 - val_acc: 0.5800\n",
            "Epoch 3/300\n",
            "\n",
            "Epoch 00003: val_loss improved from 1.00222 to 0.88132, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.7284 - acc: 0.5887 - val_loss: 0.8813 - val_acc: 0.6800\n",
            "Epoch 4/300\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.88132 to 0.79355, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.6932 - acc: 0.6306 - val_loss: 0.7936 - val_acc: 0.7000\n",
            "Epoch 5/300\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.79355 to 0.76393, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.6641 - acc: 0.6532 - val_loss: 0.7639 - val_acc: 0.5400\n",
            "Epoch 6/300\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.76393\n",
            "620/620 - 1s - loss: 0.6209 - acc: 0.6919 - val_loss: 0.8060 - val_acc: 0.5200\n",
            "Epoch 7/300\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.76393\n",
            "620/620 - 1s - loss: 0.5859 - acc: 0.7177 - val_loss: 0.9892 - val_acc: 0.5100\n",
            "Epoch 8/300\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.76393\n",
            "620/620 - 1s - loss: 0.5857 - acc: 0.7145 - val_loss: 0.8216 - val_acc: 0.5200\n",
            "Epoch 9/300\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.76393\n",
            "620/620 - 1s - loss: 0.5662 - acc: 0.7468 - val_loss: 0.9357 - val_acc: 0.5200\n",
            "Epoch 10/300\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.76393\n",
            "620/620 - 1s - loss: 0.5581 - acc: 0.7274 - val_loss: 1.0186 - val_acc: 0.5100\n",
            "Epoch 11/300\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.76393\n",
            "620/620 - 1s - loss: 0.5566 - acc: 0.7403 - val_loss: 0.9730 - val_acc: 0.5100\n",
            "Epoch 12/300\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.76393\n",
            "620/620 - 1s - loss: 0.5313 - acc: 0.7516 - val_loss: 0.8823 - val_acc: 0.5300\n",
            "Epoch 13/300\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.76393\n",
            "620/620 - 1s - loss: 0.5301 - acc: 0.7419 - val_loss: 0.7924 - val_acc: 0.5400\n",
            "Epoch 14/300\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.76393\n",
            "620/620 - 1s - loss: 0.5421 - acc: 0.7339 - val_loss: 0.8887 - val_acc: 0.5400\n",
            "Epoch 15/300\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.76393\n",
            "620/620 - 1s - loss: 0.5381 - acc: 0.7532 - val_loss: 0.9209 - val_acc: 0.5300\n",
            "Epoch 16/300\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.76393\n",
            "620/620 - 1s - loss: 0.5473 - acc: 0.7194 - val_loss: 0.8569 - val_acc: 0.5400\n",
            "Epoch 17/300\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.76393\n",
            "620/620 - 1s - loss: 0.5236 - acc: 0.7645 - val_loss: 0.8212 - val_acc: 0.5300\n",
            "Epoch 18/300\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.76393\n",
            "620/620 - 1s - loss: 0.5355 - acc: 0.7548 - val_loss: 0.9436 - val_acc: 0.5400\n",
            "Epoch 19/300\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.76393\n",
            "620/620 - 1s - loss: 0.5142 - acc: 0.7452 - val_loss: 0.9034 - val_acc: 0.5200\n",
            "Epoch 20/300\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.76393\n",
            "620/620 - 1s - loss: 0.5173 - acc: 0.7661 - val_loss: 0.8586 - val_acc: 0.5200\n",
            "Epoch 21/300\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.76393\n",
            "620/620 - 1s - loss: 0.4863 - acc: 0.7806 - val_loss: 1.0275 - val_acc: 0.5300\n",
            "Epoch 22/300\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.76393\n",
            "620/620 - 1s - loss: 0.5373 - acc: 0.7258 - val_loss: 0.8358 - val_acc: 0.5600\n",
            "Epoch 23/300\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.76393\n",
            "620/620 - 1s - loss: 0.5115 - acc: 0.7532 - val_loss: 0.8511 - val_acc: 0.5600\n",
            "Epoch 24/300\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.76393\n",
            "620/620 - 1s - loss: 0.4873 - acc: 0.7694 - val_loss: 0.8260 - val_acc: 0.5400\n",
            "Epoch 25/300\n",
            "\n",
            "Epoch 00025: val_loss improved from 0.76393 to 0.74379, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.5184 - acc: 0.7435 - val_loss: 0.7438 - val_acc: 0.5300\n",
            "Epoch 26/300\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.74379\n",
            "620/620 - 1s - loss: 0.4818 - acc: 0.7758 - val_loss: 1.0779 - val_acc: 0.5200\n",
            "Epoch 27/300\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.74379\n",
            "620/620 - 1s - loss: 0.5033 - acc: 0.7565 - val_loss: 1.0512 - val_acc: 0.5300\n",
            "Epoch 28/300\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.74379\n",
            "620/620 - 1s - loss: 0.5051 - acc: 0.7516 - val_loss: 0.9223 - val_acc: 0.5100\n",
            "Epoch 29/300\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.74379\n",
            "620/620 - 1s - loss: 0.5123 - acc: 0.7419 - val_loss: 1.1678 - val_acc: 0.5100\n",
            "Epoch 30/300\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.74379\n",
            "620/620 - 1s - loss: 0.4785 - acc: 0.7871 - val_loss: 0.7682 - val_acc: 0.5400\n",
            "Epoch 31/300\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.74379\n",
            "620/620 - 1s - loss: 0.5032 - acc: 0.7516 - val_loss: 1.0069 - val_acc: 0.5200\n",
            "Epoch 32/300\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.74379\n",
            "620/620 - 1s - loss: 0.4868 - acc: 0.7629 - val_loss: 0.8899 - val_acc: 0.5300\n",
            "Epoch 33/300\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.74379\n",
            "620/620 - 1s - loss: 0.4925 - acc: 0.7516 - val_loss: 0.9179 - val_acc: 0.5100\n",
            "Epoch 34/300\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.74379\n",
            "620/620 - 1s - loss: 0.5068 - acc: 0.7548 - val_loss: 0.7915 - val_acc: 0.5000\n",
            "Epoch 35/300\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.74379\n",
            "620/620 - 1s - loss: 0.4682 - acc: 0.7855 - val_loss: 1.0391 - val_acc: 0.5100\n",
            "Epoch 36/300\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.74379\n",
            "620/620 - 1s - loss: 0.4905 - acc: 0.7581 - val_loss: 1.0710 - val_acc: 0.5200\n",
            "Epoch 37/300\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.74379\n",
            "620/620 - 1s - loss: 0.4880 - acc: 0.7565 - val_loss: 0.7570 - val_acc: 0.5100\n",
            "Epoch 38/300\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.74379\n",
            "620/620 - 1s - loss: 0.4753 - acc: 0.7613 - val_loss: 0.8578 - val_acc: 0.5300\n",
            "Epoch 39/300\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.74379\n",
            "620/620 - 1s - loss: 0.4924 - acc: 0.7629 - val_loss: 1.0168 - val_acc: 0.5200\n",
            "Epoch 40/300\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.74379\n",
            "620/620 - 1s - loss: 0.4795 - acc: 0.7694 - val_loss: 0.7820 - val_acc: 0.5200\n",
            "Epoch 41/300\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.74379\n",
            "620/620 - 1s - loss: 0.4517 - acc: 0.7968 - val_loss: 1.1866 - val_acc: 0.5200\n",
            "Epoch 42/300\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.74379\n",
            "620/620 - 1s - loss: 0.4811 - acc: 0.7839 - val_loss: 1.0939 - val_acc: 0.5300\n",
            "Epoch 43/300\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.74379\n",
            "620/620 - 1s - loss: 0.4698 - acc: 0.7758 - val_loss: 0.9382 - val_acc: 0.5500\n",
            "Epoch 44/300\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.74379\n",
            "620/620 - 1s - loss: 0.4709 - acc: 0.7710 - val_loss: 1.1006 - val_acc: 0.5200\n",
            "Epoch 45/300\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.74379\n",
            "620/620 - 1s - loss: 0.4898 - acc: 0.7694 - val_loss: 0.9026 - val_acc: 0.5100\n",
            "Epoch 46/300\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.74379\n",
            "620/620 - 1s - loss: 0.4460 - acc: 0.7774 - val_loss: 0.8277 - val_acc: 0.5300\n",
            "Epoch 47/300\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.74379\n",
            "620/620 - 1s - loss: 0.4559 - acc: 0.7919 - val_loss: 1.0816 - val_acc: 0.5100\n",
            "Epoch 48/300\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.74379\n",
            "620/620 - 1s - loss: 0.4598 - acc: 0.7758 - val_loss: 0.9735 - val_acc: 0.5200\n",
            "Epoch 49/300\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.74379\n",
            "620/620 - 1s - loss: 0.4812 - acc: 0.7500 - val_loss: 0.7490 - val_acc: 0.5400\n",
            "Epoch 50/300\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.74379\n",
            "620/620 - 1s - loss: 0.4758 - acc: 0.7774 - val_loss: 1.0273 - val_acc: 0.5100\n",
            "Epoch 51/300\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.74379\n",
            "620/620 - 1s - loss: 0.4832 - acc: 0.7500 - val_loss: 0.8251 - val_acc: 0.5600\n",
            "Epoch 52/300\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.74379\n",
            "620/620 - 1s - loss: 0.4590 - acc: 0.7823 - val_loss: 1.1695 - val_acc: 0.5100\n",
            "Epoch 53/300\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.74379\n",
            "620/620 - 1s - loss: 0.4652 - acc: 0.7661 - val_loss: 0.9675 - val_acc: 0.5000\n",
            "Epoch 54/300\n",
            "\n",
            "Epoch 00054: val_loss improved from 0.74379 to 0.69238, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.4618 - acc: 0.7806 - val_loss: 0.6924 - val_acc: 0.6000\n",
            "Epoch 55/300\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.69238\n",
            "620/620 - 1s - loss: 0.4686 - acc: 0.7677 - val_loss: 1.0464 - val_acc: 0.5200\n",
            "Epoch 56/300\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.69238\n",
            "620/620 - 1s - loss: 0.4481 - acc: 0.7839 - val_loss: 0.9110 - val_acc: 0.5400\n",
            "Epoch 57/300\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.69238\n",
            "620/620 - 1s - loss: 0.4568 - acc: 0.7774 - val_loss: 0.9631 - val_acc: 0.5200\n",
            "Epoch 58/300\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.69238\n",
            "620/620 - 1s - loss: 0.4504 - acc: 0.7710 - val_loss: 0.9186 - val_acc: 0.5100\n",
            "Epoch 59/300\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.69238\n",
            "620/620 - 1s - loss: 0.4441 - acc: 0.8016 - val_loss: 0.8968 - val_acc: 0.5300\n",
            "Epoch 60/300\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.69238\n",
            "620/620 - 1s - loss: 0.4647 - acc: 0.7742 - val_loss: 0.8268 - val_acc: 0.5300\n",
            "Epoch 61/300\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.69238\n",
            "620/620 - 1s - loss: 0.4466 - acc: 0.7855 - val_loss: 1.0382 - val_acc: 0.5100\n",
            "Epoch 62/300\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.69238\n",
            "620/620 - 1s - loss: 0.4455 - acc: 0.7919 - val_loss: 0.8254 - val_acc: 0.5200\n",
            "Epoch 63/300\n",
            "\n",
            "Epoch 00063: val_loss improved from 0.69238 to 0.65518, saving model to /tmp/checkpoint.h5\n",
            "620/620 - 1s - loss: 0.4595 - acc: 0.7871 - val_loss: 0.6552 - val_acc: 0.6400\n",
            "Epoch 64/300\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.4528 - acc: 0.7823 - val_loss: 0.9162 - val_acc: 0.5300\n",
            "Epoch 65/300\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.4187 - acc: 0.8226 - val_loss: 1.0572 - val_acc: 0.5200\n",
            "Epoch 66/300\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.4322 - acc: 0.7806 - val_loss: 0.8122 - val_acc: 0.5500\n",
            "Epoch 67/300\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.4390 - acc: 0.7806 - val_loss: 0.7986 - val_acc: 0.5000\n",
            "Epoch 68/300\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.4396 - acc: 0.7968 - val_loss: 1.0520 - val_acc: 0.5100\n",
            "Epoch 69/300\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.4395 - acc: 0.8016 - val_loss: 0.8806 - val_acc: 0.5400\n",
            "Epoch 70/300\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.4432 - acc: 0.7871 - val_loss: 0.9637 - val_acc: 0.5200\n",
            "Epoch 71/300\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.4440 - acc: 0.7887 - val_loss: 0.8994 - val_acc: 0.5400\n",
            "Epoch 72/300\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.4182 - acc: 0.8032 - val_loss: 0.9519 - val_acc: 0.5500\n",
            "Epoch 73/300\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.4249 - acc: 0.8016 - val_loss: 0.8750 - val_acc: 0.5200\n",
            "Epoch 74/300\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.4474 - acc: 0.7903 - val_loss: 0.8093 - val_acc: 0.5300\n",
            "Epoch 75/300\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.4439 - acc: 0.7919 - val_loss: 0.9624 - val_acc: 0.5300\n",
            "Epoch 76/300\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.4106 - acc: 0.8081 - val_loss: 0.6742 - val_acc: 0.6300\n",
            "Epoch 77/300\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.4319 - acc: 0.8129 - val_loss: 0.8376 - val_acc: 0.5400\n",
            "Epoch 78/300\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.4388 - acc: 0.8145 - val_loss: 0.8466 - val_acc: 0.5400\n",
            "Epoch 79/300\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.4218 - acc: 0.7984 - val_loss: 1.0640 - val_acc: 0.5300\n",
            "Epoch 80/300\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.4333 - acc: 0.8000 - val_loss: 0.9529 - val_acc: 0.5200\n",
            "Epoch 81/300\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.4197 - acc: 0.7984 - val_loss: 0.8606 - val_acc: 0.5100\n",
            "Epoch 82/300\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.4098 - acc: 0.8226 - val_loss: 1.1875 - val_acc: 0.5200\n",
            "Epoch 83/300\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.4385 - acc: 0.7871 - val_loss: 0.9578 - val_acc: 0.5400\n",
            "Epoch 84/300\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.4155 - acc: 0.7952 - val_loss: 0.8871 - val_acc: 0.5300\n",
            "Epoch 85/300\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.4224 - acc: 0.8032 - val_loss: 0.7604 - val_acc: 0.5800\n",
            "Epoch 86/300\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.4228 - acc: 0.8048 - val_loss: 1.0838 - val_acc: 0.5300\n",
            "Epoch 87/300\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.4281 - acc: 0.7919 - val_loss: 0.8198 - val_acc: 0.5300\n",
            "Epoch 88/300\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.4246 - acc: 0.7806 - val_loss: 0.8059 - val_acc: 0.5200\n",
            "Epoch 89/300\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.4131 - acc: 0.7919 - val_loss: 0.8764 - val_acc: 0.5000\n",
            "Epoch 90/300\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.4252 - acc: 0.7984 - val_loss: 1.0689 - val_acc: 0.5100\n",
            "Epoch 91/300\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3985 - acc: 0.8210 - val_loss: 0.8243 - val_acc: 0.5400\n",
            "Epoch 92/300\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.4213 - acc: 0.7903 - val_loss: 0.8266 - val_acc: 0.5100\n",
            "Epoch 93/300\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.4192 - acc: 0.7903 - val_loss: 1.1636 - val_acc: 0.5100\n",
            "Epoch 94/300\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3968 - acc: 0.8177 - val_loss: 0.8084 - val_acc: 0.5800\n",
            "Epoch 95/300\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3942 - acc: 0.8258 - val_loss: 0.8967 - val_acc: 0.5400\n",
            "Epoch 96/300\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3994 - acc: 0.8113 - val_loss: 0.8479 - val_acc: 0.5300\n",
            "Epoch 97/300\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.4002 - acc: 0.8290 - val_loss: 1.2010 - val_acc: 0.5100\n",
            "Epoch 98/300\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3913 - acc: 0.8210 - val_loss: 0.8164 - val_acc: 0.5500\n",
            "Epoch 99/300\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.4007 - acc: 0.8145 - val_loss: 1.1720 - val_acc: 0.5000\n",
            "Epoch 100/300\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.4214 - acc: 0.8048 - val_loss: 0.9085 - val_acc: 0.5300\n",
            "Epoch 101/300\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.4001 - acc: 0.8097 - val_loss: 0.7640 - val_acc: 0.5300\n",
            "Epoch 102/300\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.4129 - acc: 0.8097 - val_loss: 0.9101 - val_acc: 0.5100\n",
            "Epoch 103/300\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.4282 - acc: 0.7984 - val_loss: 0.9336 - val_acc: 0.5000\n",
            "Epoch 104/300\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3973 - acc: 0.8065 - val_loss: 1.0405 - val_acc: 0.5200\n",
            "Epoch 105/300\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.4103 - acc: 0.8242 - val_loss: 0.8628 - val_acc: 0.5600\n",
            "Epoch 106/300\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.4018 - acc: 0.8177 - val_loss: 1.1566 - val_acc: 0.5200\n",
            "Epoch 107/300\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.4413 - acc: 0.7790 - val_loss: 0.8925 - val_acc: 0.5300\n",
            "Epoch 108/300\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3952 - acc: 0.8387 - val_loss: 0.8776 - val_acc: 0.5300\n",
            "Epoch 109/300\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.4066 - acc: 0.8016 - val_loss: 0.7682 - val_acc: 0.5500\n",
            "Epoch 110/300\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.4320 - acc: 0.8081 - val_loss: 1.1082 - val_acc: 0.5200\n",
            "Epoch 111/300\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3796 - acc: 0.8161 - val_loss: 0.9658 - val_acc: 0.5200\n",
            "Epoch 112/300\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.4196 - acc: 0.8194 - val_loss: 1.3982 - val_acc: 0.5000\n",
            "Epoch 113/300\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.4148 - acc: 0.7984 - val_loss: 0.9889 - val_acc: 0.5000\n",
            "Epoch 114/300\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.4234 - acc: 0.8016 - val_loss: 1.0574 - val_acc: 0.5300\n",
            "Epoch 115/300\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3973 - acc: 0.8242 - val_loss: 1.1610 - val_acc: 0.5100\n",
            "Epoch 116/300\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3936 - acc: 0.8194 - val_loss: 0.9474 - val_acc: 0.5300\n",
            "Epoch 117/300\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3958 - acc: 0.7968 - val_loss: 0.9156 - val_acc: 0.5500\n",
            "Epoch 118/300\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3617 - acc: 0.8403 - val_loss: 0.9818 - val_acc: 0.5400\n",
            "Epoch 119/300\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.4262 - acc: 0.8210 - val_loss: 0.9600 - val_acc: 0.5600\n",
            "Epoch 120/300\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.4021 - acc: 0.8177 - val_loss: 1.1341 - val_acc: 0.5000\n",
            "Epoch 121/300\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.4091 - acc: 0.8113 - val_loss: 0.8387 - val_acc: 0.5600\n",
            "Epoch 122/300\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3921 - acc: 0.8161 - val_loss: 1.2518 - val_acc: 0.5200\n",
            "Epoch 123/300\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.4074 - acc: 0.8242 - val_loss: 1.0390 - val_acc: 0.5300\n",
            "Epoch 124/300\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3928 - acc: 0.8194 - val_loss: 0.9531 - val_acc: 0.5200\n",
            "Epoch 125/300\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.4090 - acc: 0.8161 - val_loss: 0.8487 - val_acc: 0.5100\n",
            "Epoch 126/300\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3988 - acc: 0.8145 - val_loss: 0.8671 - val_acc: 0.5300\n",
            "Epoch 127/300\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3969 - acc: 0.8274 - val_loss: 0.9311 - val_acc: 0.5100\n",
            "Epoch 128/300\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3926 - acc: 0.8161 - val_loss: 0.9123 - val_acc: 0.5200\n",
            "Epoch 129/300\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3974 - acc: 0.8306 - val_loss: 0.9406 - val_acc: 0.5000\n",
            "Epoch 130/300\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.4026 - acc: 0.8177 - val_loss: 1.0807 - val_acc: 0.5000\n",
            "Epoch 131/300\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3904 - acc: 0.8210 - val_loss: 0.9469 - val_acc: 0.5500\n",
            "Epoch 132/300\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.4046 - acc: 0.8177 - val_loss: 1.0366 - val_acc: 0.5100\n",
            "Epoch 133/300\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3736 - acc: 0.8306 - val_loss: 1.2917 - val_acc: 0.5100\n",
            "Epoch 134/300\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.4049 - acc: 0.8081 - val_loss: 1.0660 - val_acc: 0.5300\n",
            "Epoch 135/300\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.4257 - acc: 0.7855 - val_loss: 0.8212 - val_acc: 0.5800\n",
            "Epoch 136/300\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3783 - acc: 0.8355 - val_loss: 0.7126 - val_acc: 0.5900\n",
            "Epoch 137/300\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.4193 - acc: 0.8081 - val_loss: 0.8333 - val_acc: 0.5500\n",
            "Epoch 138/300\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.4132 - acc: 0.8032 - val_loss: 0.8454 - val_acc: 0.5300\n",
            "Epoch 139/300\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.4075 - acc: 0.7968 - val_loss: 0.8560 - val_acc: 0.5300\n",
            "Epoch 140/300\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3601 - acc: 0.8387 - val_loss: 0.9171 - val_acc: 0.5400\n",
            "Epoch 141/300\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.4017 - acc: 0.8177 - val_loss: 0.9434 - val_acc: 0.5000\n",
            "Epoch 142/300\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3776 - acc: 0.8452 - val_loss: 0.9980 - val_acc: 0.5000\n",
            "Epoch 143/300\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3681 - acc: 0.8274 - val_loss: 1.1423 - val_acc: 0.5100\n",
            "Epoch 144/300\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3870 - acc: 0.8274 - val_loss: 1.2450 - val_acc: 0.5100\n",
            "Epoch 145/300\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3803 - acc: 0.8210 - val_loss: 0.9550 - val_acc: 0.5200\n",
            "Epoch 146/300\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3683 - acc: 0.8452 - val_loss: 0.8860 - val_acc: 0.5300\n",
            "Epoch 147/300\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3925 - acc: 0.8210 - val_loss: 1.0482 - val_acc: 0.4800\n",
            "Epoch 148/300\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.4086 - acc: 0.8177 - val_loss: 1.1089 - val_acc: 0.5100\n",
            "Epoch 149/300\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3869 - acc: 0.8290 - val_loss: 1.0071 - val_acc: 0.5100\n",
            "Epoch 150/300\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.4030 - acc: 0.8048 - val_loss: 0.7973 - val_acc: 0.5800\n",
            "Epoch 151/300\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3690 - acc: 0.8323 - val_loss: 0.8184 - val_acc: 0.5600\n",
            "Epoch 152/300\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3837 - acc: 0.8210 - val_loss: 0.8255 - val_acc: 0.5400\n",
            "Epoch 153/300\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3674 - acc: 0.8516 - val_loss: 0.9469 - val_acc: 0.5300\n",
            "Epoch 154/300\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.4039 - acc: 0.8081 - val_loss: 0.9132 - val_acc: 0.5400\n",
            "Epoch 155/300\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3912 - acc: 0.8258 - val_loss: 1.1784 - val_acc: 0.5100\n",
            "Epoch 156/300\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.4126 - acc: 0.8081 - val_loss: 1.2278 - val_acc: 0.4900\n",
            "Epoch 157/300\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.4025 - acc: 0.8113 - val_loss: 1.2317 - val_acc: 0.5100\n",
            "Epoch 158/300\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3863 - acc: 0.8290 - val_loss: 1.2470 - val_acc: 0.5100\n",
            "Epoch 159/300\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3629 - acc: 0.8274 - val_loss: 1.1300 - val_acc: 0.5000\n",
            "Epoch 160/300\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3703 - acc: 0.8613 - val_loss: 0.8766 - val_acc: 0.5500\n",
            "Epoch 161/300\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.4042 - acc: 0.8177 - val_loss: 1.1903 - val_acc: 0.5100\n",
            "Epoch 162/300\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3856 - acc: 0.8339 - val_loss: 1.0899 - val_acc: 0.5100\n",
            "Epoch 163/300\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3793 - acc: 0.8258 - val_loss: 0.9684 - val_acc: 0.5100\n",
            "Epoch 164/300\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.4486 - acc: 0.7823 - val_loss: 1.0337 - val_acc: 0.5400\n",
            "Epoch 165/300\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3728 - acc: 0.8194 - val_loss: 0.8455 - val_acc: 0.5500\n",
            "Epoch 166/300\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3904 - acc: 0.8129 - val_loss: 0.8853 - val_acc: 0.5200\n",
            "Epoch 167/300\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.4097 - acc: 0.8113 - val_loss: 0.8554 - val_acc: 0.5400\n",
            "Epoch 168/300\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3774 - acc: 0.8274 - val_loss: 1.4521 - val_acc: 0.5100\n",
            "Epoch 169/300\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3783 - acc: 0.8258 - val_loss: 0.8493 - val_acc: 0.5500\n",
            "Epoch 170/300\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3754 - acc: 0.8419 - val_loss: 1.0665 - val_acc: 0.5300\n",
            "Epoch 171/300\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.4047 - acc: 0.8016 - val_loss: 0.9464 - val_acc: 0.4900\n",
            "Epoch 172/300\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3843 - acc: 0.8145 - val_loss: 0.9680 - val_acc: 0.4900\n",
            "Epoch 173/300\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3456 - acc: 0.8532 - val_loss: 0.9551 - val_acc: 0.5000\n",
            "Epoch 174/300\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3917 - acc: 0.8032 - val_loss: 1.1145 - val_acc: 0.5100\n",
            "Epoch 175/300\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3722 - acc: 0.8355 - val_loss: 0.9141 - val_acc: 0.5200\n",
            "Epoch 176/300\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.4167 - acc: 0.8097 - val_loss: 0.8336 - val_acc: 0.5300\n",
            "Epoch 177/300\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.4011 - acc: 0.8194 - val_loss: 0.9689 - val_acc: 0.5000\n",
            "Epoch 178/300\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3699 - acc: 0.8339 - val_loss: 1.1124 - val_acc: 0.5000\n",
            "Epoch 179/300\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3704 - acc: 0.8500 - val_loss: 0.9014 - val_acc: 0.5300\n",
            "Epoch 180/300\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3423 - acc: 0.8500 - val_loss: 1.0681 - val_acc: 0.4900\n",
            "Epoch 181/300\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3585 - acc: 0.8258 - val_loss: 1.3338 - val_acc: 0.5100\n",
            "Epoch 182/300\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3564 - acc: 0.8613 - val_loss: 0.9419 - val_acc: 0.5200\n",
            "Epoch 183/300\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3831 - acc: 0.8355 - val_loss: 0.9890 - val_acc: 0.5000\n",
            "Epoch 184/300\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3915 - acc: 0.8161 - val_loss: 0.9913 - val_acc: 0.5200\n",
            "Epoch 185/300\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3911 - acc: 0.8145 - val_loss: 1.1901 - val_acc: 0.5100\n",
            "Epoch 186/300\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3603 - acc: 0.8516 - val_loss: 1.0696 - val_acc: 0.5000\n",
            "Epoch 187/300\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3545 - acc: 0.8387 - val_loss: 1.4220 - val_acc: 0.5100\n",
            "Epoch 188/300\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3548 - acc: 0.8516 - val_loss: 0.9983 - val_acc: 0.5000\n",
            "Epoch 189/300\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3695 - acc: 0.8323 - val_loss: 0.9934 - val_acc: 0.5100\n",
            "Epoch 190/300\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3912 - acc: 0.8097 - val_loss: 0.9059 - val_acc: 0.4900\n",
            "Epoch 191/300\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3491 - acc: 0.8500 - val_loss: 1.1511 - val_acc: 0.5100\n",
            "Epoch 192/300\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3768 - acc: 0.8419 - val_loss: 0.9927 - val_acc: 0.4800\n",
            "Epoch 193/300\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3706 - acc: 0.8387 - val_loss: 1.4053 - val_acc: 0.5100\n",
            "Epoch 194/300\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3765 - acc: 0.8258 - val_loss: 0.9189 - val_acc: 0.5700\n",
            "Epoch 195/300\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3754 - acc: 0.8306 - val_loss: 0.7349 - val_acc: 0.5800\n",
            "Epoch 196/300\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3732 - acc: 0.8323 - val_loss: 1.0962 - val_acc: 0.5200\n",
            "Epoch 197/300\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3620 - acc: 0.8323 - val_loss: 1.1075 - val_acc: 0.4900\n",
            "Epoch 198/300\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3476 - acc: 0.8484 - val_loss: 1.0911 - val_acc: 0.5200\n",
            "Epoch 199/300\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3731 - acc: 0.8468 - val_loss: 1.1575 - val_acc: 0.5100\n",
            "Epoch 200/300\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3586 - acc: 0.8242 - val_loss: 1.0515 - val_acc: 0.5300\n",
            "Epoch 201/300\n",
            "\n",
            "Epoch 00201: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3726 - acc: 0.8306 - val_loss: 0.9906 - val_acc: 0.5100\n",
            "Epoch 202/300\n",
            "\n",
            "Epoch 00202: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3351 - acc: 0.8613 - val_loss: 0.9485 - val_acc: 0.5100\n",
            "Epoch 203/300\n",
            "\n",
            "Epoch 00203: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3550 - acc: 0.8403 - val_loss: 1.0914 - val_acc: 0.5000\n",
            "Epoch 204/300\n",
            "\n",
            "Epoch 00204: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3852 - acc: 0.8258 - val_loss: 1.1300 - val_acc: 0.5100\n",
            "Epoch 205/300\n",
            "\n",
            "Epoch 00205: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3610 - acc: 0.8629 - val_loss: 1.0969 - val_acc: 0.5100\n",
            "Epoch 206/300\n",
            "\n",
            "Epoch 00206: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3431 - acc: 0.8500 - val_loss: 1.2058 - val_acc: 0.5000\n",
            "Epoch 207/300\n",
            "\n",
            "Epoch 00207: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3424 - acc: 0.8548 - val_loss: 1.3460 - val_acc: 0.5200\n",
            "Epoch 208/300\n",
            "\n",
            "Epoch 00208: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3675 - acc: 0.8387 - val_loss: 1.1337 - val_acc: 0.5000\n",
            "Epoch 209/300\n",
            "\n",
            "Epoch 00209: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3531 - acc: 0.8484 - val_loss: 1.2233 - val_acc: 0.5000\n",
            "Epoch 210/300\n",
            "\n",
            "Epoch 00210: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3591 - acc: 0.8371 - val_loss: 1.1947 - val_acc: 0.5200\n",
            "Epoch 211/300\n",
            "\n",
            "Epoch 00211: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3871 - acc: 0.8210 - val_loss: 1.2126 - val_acc: 0.5100\n",
            "Epoch 212/300\n",
            "\n",
            "Epoch 00212: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3663 - acc: 0.8419 - val_loss: 1.4041 - val_acc: 0.5200\n",
            "Epoch 213/300\n",
            "\n",
            "Epoch 00213: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3594 - acc: 0.8597 - val_loss: 0.7863 - val_acc: 0.5900\n",
            "Epoch 214/300\n",
            "\n",
            "Epoch 00214: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3894 - acc: 0.8177 - val_loss: 1.1180 - val_acc: 0.5100\n",
            "Epoch 215/300\n",
            "\n",
            "Epoch 00215: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3740 - acc: 0.8177 - val_loss: 0.9921 - val_acc: 0.5200\n",
            "Epoch 216/300\n",
            "\n",
            "Epoch 00216: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3869 - acc: 0.8210 - val_loss: 0.9938 - val_acc: 0.5200\n",
            "Epoch 217/300\n",
            "\n",
            "Epoch 00217: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3674 - acc: 0.8581 - val_loss: 1.0012 - val_acc: 0.5200\n",
            "Epoch 218/300\n",
            "\n",
            "Epoch 00218: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3421 - acc: 0.8548 - val_loss: 0.9385 - val_acc: 0.5400\n",
            "Epoch 219/300\n",
            "\n",
            "Epoch 00219: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3584 - acc: 0.8355 - val_loss: 1.3591 - val_acc: 0.5200\n",
            "Epoch 220/300\n",
            "\n",
            "Epoch 00220: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3662 - acc: 0.8258 - val_loss: 1.1573 - val_acc: 0.5000\n",
            "Epoch 221/300\n",
            "\n",
            "Epoch 00221: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3496 - acc: 0.8516 - val_loss: 1.0216 - val_acc: 0.5200\n",
            "Epoch 222/300\n",
            "\n",
            "Epoch 00222: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3659 - acc: 0.8177 - val_loss: 1.2515 - val_acc: 0.5200\n",
            "Epoch 223/300\n",
            "\n",
            "Epoch 00223: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3652 - acc: 0.8484 - val_loss: 1.0504 - val_acc: 0.5300\n",
            "Epoch 224/300\n",
            "\n",
            "Epoch 00224: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3963 - acc: 0.8194 - val_loss: 0.8079 - val_acc: 0.5600\n",
            "Epoch 225/300\n",
            "\n",
            "Epoch 00225: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3842 - acc: 0.8226 - val_loss: 1.1257 - val_acc: 0.5200\n",
            "Epoch 226/300\n",
            "\n",
            "Epoch 00226: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3301 - acc: 0.8597 - val_loss: 1.0178 - val_acc: 0.5300\n",
            "Epoch 227/300\n",
            "\n",
            "Epoch 00227: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3496 - acc: 0.8403 - val_loss: 0.9156 - val_acc: 0.5200\n",
            "Epoch 228/300\n",
            "\n",
            "Epoch 00228: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3703 - acc: 0.8403 - val_loss: 1.0970 - val_acc: 0.5300\n",
            "Epoch 229/300\n",
            "\n",
            "Epoch 00229: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3761 - acc: 0.8387 - val_loss: 1.0463 - val_acc: 0.5400\n",
            "Epoch 230/300\n",
            "\n",
            "Epoch 00230: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3374 - acc: 0.8613 - val_loss: 1.5378 - val_acc: 0.5100\n",
            "Epoch 231/300\n",
            "\n",
            "Epoch 00231: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3586 - acc: 0.8290 - val_loss: 1.0906 - val_acc: 0.4900\n",
            "Epoch 232/300\n",
            "\n",
            "Epoch 00232: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3308 - acc: 0.8548 - val_loss: 1.0199 - val_acc: 0.5100\n",
            "Epoch 233/300\n",
            "\n",
            "Epoch 00233: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3837 - acc: 0.8177 - val_loss: 0.8857 - val_acc: 0.5100\n",
            "Epoch 234/300\n",
            "\n",
            "Epoch 00234: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3639 - acc: 0.8339 - val_loss: 1.2942 - val_acc: 0.5000\n",
            "Epoch 235/300\n",
            "\n",
            "Epoch 00235: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3741 - acc: 0.8387 - val_loss: 1.3191 - val_acc: 0.5100\n",
            "Epoch 236/300\n",
            "\n",
            "Epoch 00236: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3706 - acc: 0.8452 - val_loss: 1.2210 - val_acc: 0.5100\n",
            "Epoch 237/300\n",
            "\n",
            "Epoch 00237: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3565 - acc: 0.8581 - val_loss: 1.0814 - val_acc: 0.5400\n",
            "Epoch 238/300\n",
            "\n",
            "Epoch 00238: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3667 - acc: 0.8339 - val_loss: 0.9025 - val_acc: 0.4900\n",
            "Epoch 239/300\n",
            "\n",
            "Epoch 00239: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3388 - acc: 0.8452 - val_loss: 0.9371 - val_acc: 0.5000\n",
            "Epoch 240/300\n",
            "\n",
            "Epoch 00240: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3513 - acc: 0.8371 - val_loss: 1.1091 - val_acc: 0.4900\n",
            "Epoch 241/300\n",
            "\n",
            "Epoch 00241: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3678 - acc: 0.8323 - val_loss: 0.9549 - val_acc: 0.5100\n",
            "Epoch 242/300\n",
            "\n",
            "Epoch 00242: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3413 - acc: 0.8468 - val_loss: 1.1071 - val_acc: 0.5300\n",
            "Epoch 243/300\n",
            "\n",
            "Epoch 00243: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3342 - acc: 0.8452 - val_loss: 0.7598 - val_acc: 0.5800\n",
            "Epoch 244/300\n",
            "\n",
            "Epoch 00244: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3751 - acc: 0.8516 - val_loss: 1.5793 - val_acc: 0.5100\n",
            "Epoch 245/300\n",
            "\n",
            "Epoch 00245: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3689 - acc: 0.8306 - val_loss: 1.1588 - val_acc: 0.5100\n",
            "Epoch 246/300\n",
            "\n",
            "Epoch 00246: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3774 - acc: 0.8210 - val_loss: 1.2964 - val_acc: 0.5100\n",
            "Epoch 247/300\n",
            "\n",
            "Epoch 00247: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3347 - acc: 0.8468 - val_loss: 1.0154 - val_acc: 0.5200\n",
            "Epoch 248/300\n",
            "\n",
            "Epoch 00248: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3408 - acc: 0.8500 - val_loss: 1.2401 - val_acc: 0.5200\n",
            "Epoch 249/300\n",
            "\n",
            "Epoch 00249: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3668 - acc: 0.8242 - val_loss: 1.0889 - val_acc: 0.4900\n",
            "Epoch 250/300\n",
            "\n",
            "Epoch 00250: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3531 - acc: 0.8532 - val_loss: 0.9894 - val_acc: 0.5100\n",
            "Epoch 251/300\n",
            "\n",
            "Epoch 00251: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3929 - acc: 0.8258 - val_loss: 1.0426 - val_acc: 0.4900\n",
            "Epoch 252/300\n",
            "\n",
            "Epoch 00252: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3548 - acc: 0.8532 - val_loss: 1.0344 - val_acc: 0.5100\n",
            "Epoch 253/300\n",
            "\n",
            "Epoch 00253: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3521 - acc: 0.8403 - val_loss: 0.9443 - val_acc: 0.5000\n",
            "Epoch 254/300\n",
            "\n",
            "Epoch 00254: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3472 - acc: 0.8565 - val_loss: 0.9363 - val_acc: 0.5100\n",
            "Epoch 255/300\n",
            "\n",
            "Epoch 00255: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3648 - acc: 0.8500 - val_loss: 1.0853 - val_acc: 0.5300\n",
            "Epoch 256/300\n",
            "\n",
            "Epoch 00256: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3651 - acc: 0.8387 - val_loss: 1.0094 - val_acc: 0.5200\n",
            "Epoch 257/300\n",
            "\n",
            "Epoch 00257: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3619 - acc: 0.8339 - val_loss: 1.1269 - val_acc: 0.5000\n",
            "Epoch 258/300\n",
            "\n",
            "Epoch 00258: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3567 - acc: 0.8484 - val_loss: 0.8371 - val_acc: 0.5500\n",
            "Epoch 259/300\n",
            "\n",
            "Epoch 00259: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3248 - acc: 0.8548 - val_loss: 0.9483 - val_acc: 0.5600\n",
            "Epoch 260/300\n",
            "\n",
            "Epoch 00260: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3173 - acc: 0.8694 - val_loss: 1.1941 - val_acc: 0.5100\n",
            "Epoch 261/300\n",
            "\n",
            "Epoch 00261: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3553 - acc: 0.8468 - val_loss: 1.1901 - val_acc: 0.5100\n",
            "Epoch 262/300\n",
            "\n",
            "Epoch 00262: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3472 - acc: 0.8516 - val_loss: 1.2700 - val_acc: 0.5100\n",
            "Epoch 263/300\n",
            "\n",
            "Epoch 00263: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3729 - acc: 0.8403 - val_loss: 0.9711 - val_acc: 0.5100\n",
            "Epoch 264/300\n",
            "\n",
            "Epoch 00264: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3270 - acc: 0.8548 - val_loss: 1.0682 - val_acc: 0.5000\n",
            "Epoch 265/300\n",
            "\n",
            "Epoch 00265: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3569 - acc: 0.8290 - val_loss: 1.0324 - val_acc: 0.5100\n",
            "Epoch 266/300\n",
            "\n",
            "Epoch 00266: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3667 - acc: 0.8306 - val_loss: 0.8858 - val_acc: 0.5500\n",
            "Epoch 267/300\n",
            "\n",
            "Epoch 00267: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3490 - acc: 0.8516 - val_loss: 1.0496 - val_acc: 0.4800\n",
            "Epoch 268/300\n",
            "\n",
            "Epoch 00268: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3639 - acc: 0.8468 - val_loss: 1.3432 - val_acc: 0.5100\n",
            "Epoch 269/300\n",
            "\n",
            "Epoch 00269: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3348 - acc: 0.8629 - val_loss: 1.2393 - val_acc: 0.5100\n",
            "Epoch 270/300\n",
            "\n",
            "Epoch 00270: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3469 - acc: 0.8387 - val_loss: 0.9478 - val_acc: 0.5300\n",
            "Epoch 271/300\n",
            "\n",
            "Epoch 00271: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3489 - acc: 0.8548 - val_loss: 0.9518 - val_acc: 0.5100\n",
            "Epoch 272/300\n",
            "\n",
            "Epoch 00272: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3688 - acc: 0.8371 - val_loss: 0.9192 - val_acc: 0.5000\n",
            "Epoch 273/300\n",
            "\n",
            "Epoch 00273: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3392 - acc: 0.8484 - val_loss: 0.9831 - val_acc: 0.5300\n",
            "Epoch 274/300\n",
            "\n",
            "Epoch 00274: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3229 - acc: 0.8677 - val_loss: 1.1287 - val_acc: 0.5100\n",
            "Epoch 275/300\n",
            "\n",
            "Epoch 00275: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3730 - acc: 0.8226 - val_loss: 1.1874 - val_acc: 0.5100\n",
            "Epoch 276/300\n",
            "\n",
            "Epoch 00276: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3707 - acc: 0.8468 - val_loss: 0.9826 - val_acc: 0.4900\n",
            "Epoch 277/300\n",
            "\n",
            "Epoch 00277: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3483 - acc: 0.8194 - val_loss: 1.0835 - val_acc: 0.5200\n",
            "Epoch 278/300\n",
            "\n",
            "Epoch 00278: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3802 - acc: 0.8387 - val_loss: 1.4737 - val_acc: 0.5100\n",
            "Epoch 279/300\n",
            "\n",
            "Epoch 00279: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3412 - acc: 0.8532 - val_loss: 1.0199 - val_acc: 0.5000\n",
            "Epoch 280/300\n",
            "\n",
            "Epoch 00280: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3895 - acc: 0.8097 - val_loss: 1.1179 - val_acc: 0.4900\n",
            "Epoch 281/300\n",
            "\n",
            "Epoch 00281: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3872 - acc: 0.8468 - val_loss: 1.1085 - val_acc: 0.5200\n",
            "Epoch 282/300\n",
            "\n",
            "Epoch 00282: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3473 - acc: 0.8629 - val_loss: 1.1531 - val_acc: 0.5100\n",
            "Epoch 283/300\n",
            "\n",
            "Epoch 00283: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3574 - acc: 0.8581 - val_loss: 1.0729 - val_acc: 0.4700\n",
            "Epoch 284/300\n",
            "\n",
            "Epoch 00284: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3568 - acc: 0.8419 - val_loss: 1.1465 - val_acc: 0.4900\n",
            "Epoch 285/300\n",
            "\n",
            "Epoch 00285: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3549 - acc: 0.8468 - val_loss: 1.3384 - val_acc: 0.5000\n",
            "Epoch 286/300\n",
            "\n",
            "Epoch 00286: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3629 - acc: 0.8387 - val_loss: 1.2749 - val_acc: 0.5000\n",
            "Epoch 287/300\n",
            "\n",
            "Epoch 00287: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3527 - acc: 0.8323 - val_loss: 1.0134 - val_acc: 0.5000\n",
            "Epoch 288/300\n",
            "\n",
            "Epoch 00288: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3974 - acc: 0.8242 - val_loss: 1.0260 - val_acc: 0.5200\n",
            "Epoch 289/300\n",
            "\n",
            "Epoch 00289: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3444 - acc: 0.8452 - val_loss: 1.1888 - val_acc: 0.4900\n",
            "Epoch 290/300\n",
            "\n",
            "Epoch 00290: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3657 - acc: 0.8339 - val_loss: 1.3670 - val_acc: 0.5100\n",
            "Epoch 291/300\n",
            "\n",
            "Epoch 00291: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3581 - acc: 0.8355 - val_loss: 1.0250 - val_acc: 0.5300\n",
            "Epoch 292/300\n",
            "\n",
            "Epoch 00292: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3781 - acc: 0.8371 - val_loss: 0.9663 - val_acc: 0.5100\n",
            "Epoch 293/300\n",
            "\n",
            "Epoch 00293: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.4012 - acc: 0.8371 - val_loss: 1.4751 - val_acc: 0.4900\n",
            "Epoch 294/300\n",
            "\n",
            "Epoch 00294: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3640 - acc: 0.8387 - val_loss: 0.9145 - val_acc: 0.5400\n",
            "Epoch 295/300\n",
            "\n",
            "Epoch 00295: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3574 - acc: 0.8290 - val_loss: 0.9327 - val_acc: 0.4800\n",
            "Epoch 296/300\n",
            "\n",
            "Epoch 00296: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3524 - acc: 0.8355 - val_loss: 1.0324 - val_acc: 0.5000\n",
            "Epoch 297/300\n",
            "\n",
            "Epoch 00297: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3624 - acc: 0.8387 - val_loss: 1.1531 - val_acc: 0.5100\n",
            "Epoch 298/300\n",
            "\n",
            "Epoch 00298: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3436 - acc: 0.8677 - val_loss: 1.3456 - val_acc: 0.5000\n",
            "Epoch 299/300\n",
            "\n",
            "Epoch 00299: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3329 - acc: 0.8468 - val_loss: 1.4410 - val_acc: 0.4900\n",
            "Epoch 300/300\n",
            "\n",
            "Epoch 00300: val_loss did not improve from 0.65518\n",
            "620/620 - 1s - loss: 0.3308 - acc: 0.8435 - val_loss: 1.0357 - val_acc: 0.5000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nOydeXhcZb34P99Zs+9J0zZt032npZR9\nEWQrqKCICIgiilwX0B+o9+KO4IJer14X1IuKC7IqgqhAAdn3Ulrovm9ps++ZyWS29/fHWXJmMkkm\naaZpkvfzPHk655z3nHlnkr7f97uLUgqNRqPRaJJxjfYENBqNRnN0ogWERqPRaFKiBYRGo9FoUqIF\nhEaj0WhSogWERqPRaFKiBYRGo9FoUqIFhGbCIyLVIqJExJPG2I+LyEtHYl4azWijBYRmTCEie0Uk\nLCJlSefXmYt89ejMTKMZf2gBoRmL7AGusA5EZCmQM3rTOTpIRwPSaIaCFhCascjdwMccx1cDf3IO\nEJFCEfmTiDSKyD4R+bqIuMxrbhH5kYg0ichu4D0p7v2diNSKyEER+Y6IuNOZmIj8RUTqRKRdRF4Q\nkcWOa9ki8j/mfNpF5CURyTavnSYir4hIm4gcEJGPm+efE5FrHc9IMHGZWtPnRGQHsMM891PzGR0i\nslZETneMd4vIV0Vkl4h0mtenicgdIvI/SZ/lURG5MZ3PrRmfaAGhGYu8BhSIyEJz4b4c+HPSmJ8D\nhcAs4F0YAuUa89qngPcCxwIrgUuT7v0DEAXmmGPOA64lPR4H5gIVwFvAPY5rPwKOA04BSoD/BOIi\nMsO87+dAObAcWJ/m+wG8HzgRWGQerzGfUQLcC/xFRLLMazdhaF8XAgXAJ4Ag8EfgCocQLQPOMe/X\nTFSUUvpH/4yZH2AvxsL1deD7wCrgKcADKKAacANhYJHjvv8AnjNfPwN82nHtPPNeDzAJ6AGyHdev\nAJ41X38ceCnNuRaZzy3E2Ix1A8tSjPsK8HA/z3gOuNZxnPD+5vPfPcg8Wq33BbYBF/czbgtwrvn6\neuCx0f5965/R/dE2S81Y5W7gBWAmSeYloAzwAvsc5/YBU83XU4ADSdcsZpj31oqIdc6VND4lpjbz\nXeBDGJpA3DEfP5AF7Epx67R+zqdLwtxE5EvAJzE+p8LQFCyn/kDv9UfgKgyBexXw08OYk2YcoE1M\nmjGJUmofhrP6QuBvSZebgAjGYm8xHThovq7FWCid1ywOYGgQZUqpIvOnQCm1mMG5ErgYQ8MpxNBm\nAMScUwiYneK+A/2cBwiQ6ICvTDHGLsls+hv+E7gMKFZKFQHt5hwGe68/AxeLyDJgIfBIP+M0EwQt\nIDRjmU9imFcCzpNKqRjwIPBdEck3bfw30euneBD4vIhUiUgxcLPj3lrgSeB/RKRARFwiMltE3pXG\nfPIxhEszxqL+Pcdz48BdwI9FZIrpLD5ZRPwYfopzROQyEfGISKmILDdvXQ9cIiI5IjLH/MyDzSEK\nNAIeEfkmhgZh8VvgNhGZKwbHiEipOccaDP/F3cBDSqnuND6zZhyjBYRmzKKU2qWUerOfyzdg7L53\nAy9hOFvvMq/9BlgNvI3hSE7WQD4G+IDNGPb7vwKT05jSnzDMVQfNe19Luv4lYAPGItwC/ABwKaX2\nY2hCXzTPrweWmff8BMOfUo9hArqHgVkNPAFsN+cSItEE9WMMAfkk0AH8Dsh2XP8jsBRDSGgmOKKU\nbhik0WgMROQMDE1rhtKLw4RHaxAajQYAEfECXwB+q4WDBrSA0Gg0gIgsBNowTGn/O8rT0RwlaBOT\nRqPRaFKiNQiNRqPRpGTcJMqVlZWp6urq0Z6GRqPRjCnWrl3bpJQqT3Vt3AiI6upq3nyzv4hHjUaj\n0aRCRPb1d02bmDQajUaTEi0gNBqNRpMSLSA0Go1Gk5Jx44NIRSQSoaamhlAoNNpTOWJkZWVRVVWF\n1+sd7aloNJoxzrgWEDU1NeTn51NdXY2jdPO4RSlFc3MzNTU1zJw5c7Sno9Foxjjj2sQUCoUoLS2d\nEMIBQEQoLS2dUBqTRqPJHONaQAATRjhYTLTPq9FoMse4FxAajUYzXFoCYR59+9BoT2PU0AIigzQ3\nN7N8+XKWL19OZWUlU6dOtY/D4XBaz7jmmmvYtm1bhmeq0WhSce/r+/j8feto6uoZ0n2hSIxvPLKR\n5iHeNxT+vv5gxoXXuHZSjzalpaWsX78egFtuuYW8vDy+9KUvJYyxmoO7XKll9e9///uMz1Oj0aRm\nT1MQgIaOHsry/Gnf9/C6g9z92j5cAt++eElG5va7l/YAcNGyKRl5PmgNYlTYuXMnixYt4iMf+QiL\nFy+mtraW6667jpUrV7J48WJuvfVWe+xpp53G+vXriUajFBUVcfPNN7Ns2TJOPvlkGhoaRvFTaDTQ\nE42xvb5ztKeRMfY1G91sB9IglFJsPNiecK41aFgIfJ7+l9gttR30RGPDnltbMEJDR+Y0FJhAGsS3\n/7GJzYc6RvSZi6YU8K33pdPLvi9bt27lT3/6EytXrgTg9ttvp6SkhGg0yllnncWll17KokWLEu5p\nb2/nXe96F7fffjs33XQTd911FzfffHOqx2vGGLG44i9vHuCDx1XhdY/svu213c0U5XhZUFkw+OA0\naOgI8ea+Vi5cOpnbH9/K71/ey8s3v5upRdmD3zxMntxUx+KphRl9j1TsbTY0iIEExKu7mrnyt6/z\nzxtOY8nUQgA6Q1EACrJS5yN1hiJc8NMXOX1uGXd/8sRhza0tGCYQjhGPK1yuzASnaA1ilJg9e7Yt\nHADuu+8+VqxYwYoVK9iyZQubN2/uc092djYXXHABAMcddxx79+49UtPVZJi1+1q5+W8beHVX84g/\n+6sPb+AnT20fsefdv+YAn73nLbp6ovama1dDV8qxbcEwX314A8FwNO3nK6X4/uNbbM0kGovzmXve\n4k+v7j3cqQ+JzlDEFgwDCYidjcZnr2ntTrgX6HfhPtBijH1xR9OQvhuLWFzREYoSiyuaA+n5M4fD\nhNEghrvTzxS5ubn26x07dvDTn/6UN954g6KiIq666qqUuQw+n89+7Xa7iUaH/oelOTppM00SXT0j\n/zttD0Zo6hq5RaTFXJAaOkJUFmYBlimmb8XoHz+1nXtf38+yqkI+fPx0Gjt7+OjvXufbFy3mxFml\nKZ/fFozwf8/vxu92cdN582kJhInFFe3BSMrxX/nbBuZPyuPjpw4/OVQpRTgWx+9x2+f2mdoDQGNn\n/wLiQIsxrjnQO8b6jvpb/A+09j77n2/Xctnx04Y0347u3u+ioTNEeX76/pGhoDWIo4COjg7y8/Mp\nKCigtraW1atXj/aUNEeYDtMkEciAgOgMRWkdwV2mtTjVd/SQ4zMW1K11nbxT09ZnbLMpmLK8xrgd\n9Z1srevkw3e+1q/93RKSdR3GJqnR3L23d0fY0xSgI9S7OMbjiofX1fDc9sbD+kzff3wr87/+BJFY\n3D5nCQgRbAEbi/f1N1jaQHNXmHdq2lBKUdduzD0YTv0ZLaECsLl26KbvtgQBkTk/REYFhIisEpFt\nIrJTRPoYy0Vkuog8KyLrROQdEbnQPF8tIt0ist78+XUm5znarFixgkWLFrFgwQI+9rGPceqpp472\nlDRHGMskMVQB8cTGWmrbu/u93hONEY7FaQmOnIBoNxenhs4QXT3GAnjP6/u56Bcvc7CtO+VYl5nA\nGXAsmM9vS72oW/b7ug7LvGPMvSMU4UO/foVfPbfLHnuwrZtQJG7v2NNlZ0Mnz29vZHdjF89srecP\nr+wFDH+NxaZD7bhdwryKfNvE9M93DvG+X7zEIcfnrGkzFvunNtdz0S9e5o09LdSbcw/2JAqIh9bW\n0BYMU9PaTZ7fw4LKfGoc2kS6tDp+n40ZdFRnzMQkIm7gDuBcoAZYIyKPKqWcxvWvAw8qpX4lIouA\nx4Bq89oupdTyTM3vSHPLLbfYr+fMmWOHv4KR/Xz33XenvO+ll16yX7e19e7QLr/8ci6//PKRn6gm\nY9S1h/jFszv41vsW93FEd3SbGkQ/O85URGJxPnvPW1x/1hxuOm9+yjHWYtsWjBCNxfG4XTy0tga3\nSxCBnmicy1YOzbxhLfqNnT10hRLNPvuaAgmO5LZuYyGzBF9XT+/4g23dRGJxbv3HZj5z5mymmPfZ\nGoQp+CzzTmNnD01d4QRzj2X/b+7HhPa3t2qIxOJ8+PjpCed/+dwuXtrRxNkLJ/HwuhpmlOSwo6GL\nJzbWcfpcw1T25OZ6TqguIdfv4bltDXz14Q3k+T0oBbXtITxu4X9Wb7f9MBsPGZrFrsYA9ab2E3CY\nmDYebOeLf3mbi5dPIdATpao4m6rinARtIl2c5raGzsyV1smkD+IEYKdSajeAiNwPXAw4BYQCrNCK\nQmDipixqxj3PbWvgz6/t52MnVzNvUn7CNUuDGIrDMtATJa5SC5XP/Hktr+xq5gPHTrXPtXVHKMvz\n88W/vA3Agsp84kpx2cpp/Oq5Xaw/0Mr/fXRln2fF44q4UnhModarQfT08ZnUtHbTHY6RbZqerLHW\nHLtCvePrOkLsbOji7tf2sWhKAZesmIrP7bK/C8tMY+3eLZNPp0MoWc7xpq4elFIJpWZiccVNDxqf\n1RIQ339sCy2BMK2BMC2BMM1dPYQicXaZguapzfV85/1L2NUYYGdDFx89aQZb6zqJxhX3vr6faSWG\nEGsNhFm3v5UH3jxgv59Sxr8bD7UTjRsH3Y7fjaUpBHqi1LR2U1WcQ1VxNq/uauoz98GwBC+MXRPT\nVOCA47jGPOfkFuAqEanB0B5ucFybaZqenheR01O9gYhcJyJvisibjY2HZ4PUaDKNvVimMCN12Cam\n9DUIa3FOtnPH4oqnt9TT3h1h9aY6+3yyH2JPU8BehH/wxFZWb6pPObffvrSbOV97nCfNZ9kCoiNE\nZyjK8mlFfPXCBYjAn1/fx7Jbn7TNXm3BxM/caf5bluenvj1kz+lQWzfH3fY0/97SYH+ujlCU7nCM\nJnMB7InGEz43wE5TQPRE4wmCsq49xL821CZ8jjV7W/i/F3bzl7U1tAYjROOKA2bkUVzBlMIsGjp7\nqOsI8dw2I8fovMWTiDr8Epa/oSUQtoUAgDNYae3eVvt1IBzlnZo2YnFFrfldl+X5OdASZFpJNtNK\ncgiEY7y6uzlB8A2G9b2W5fkymgsx2k7qK4A/KKWqgAuBu0XEBdQC05VSxwI3AfeKSJ8gbqXUnUqp\nlUqpleXlKXtuazQZZV9zwF44wViY/tFP+QNrYU3luLRMQUPRIKyFsjvpnobOEJGYsXhZixLQJxyy\nJxq3F2GLN/e1kswrZujt/3tgPfG4sh2klgYxqyyX686YzZTCbN6paSccjfP2gTa6wzH7c9kCIhTF\n4xKqS3OobQ/Zc9pe30lXT5SdjV32PWBoGckhpk4tZKcjvPbR9YdsR/l//Hktn79vneOzxvjDy3vt\nYytqbG9TwD537qJJAGyt7WRnQxdleT4mF2azfHpRn++kJRhOWJiPm1Fsv95mhufOKM1he30XF/3i\nZZ7aXGdrQNG4IhCOMbUom2nFhkZy5W9e5+fP7LSf8dTmelurSYUlIJZMLeTNfa0JjvuRJJMC4iDg\nNG5WmeecfBJ4EEAp9SqQBZQppXqUUs3m+bXALmBeBueq0QyL37+8l+vvXUfc3E3e98Z+brhvXcqd\neFs6GsQQfBDWc7ojifdYu9yinMQkrf4imeo6QlSX5gDwyq6mPtct+34wHKOhs4ewuZNv6OyhMxQl\nP8uwVFcV9/oeNtd2csjhPLds8V3m+EmFWdR3hGxnq5VD0BoMJ2gIte3ddhSTRactGGNsOtTBzDIj\nZPyrD2/gG3/fBMDbBwxB4TPNYo2dPQm2esuZ7vzuzl44yZx7B3ubA8woNZ57xfHTefub57F8Wq+g\naAmEaegMMbMsl/XfPJeTk0J2s71uFlYW2M7zQ20hOyvb8k8UZHupKs6x73lxR+93f9MD6/np0zvo\nj/buCAVZHm46dx4tgR5+8PjWfsceDpkUEGuAuSIyU0R8wOXAo0lj9gNnA4jIQgwB0Sgi5aaTGxGZ\nBcwFdmdwrppxSigSsxfvTNDU1UM4Frd3udZiliqqZiANwnJSBweJYuqJxjjl+//myU11Dq0jWUAY\nO1XnggaGBuEM47Soaw8RMw3oz25tIJb0fR1oDVJgCgFrV+v3uGjoCNHVEyXPvDatpHex21LbkRDp\nY5nOrPGVBVnUdYRs4WPNuT0YSdAQttV19slBsK6/sKOR7kiMDztyCDYebLcX/69euIBff3QFYAgz\n5+/E0rCcLJpSQFVxNltqO9jbFKTaFBAul1CY42Xh5F6/kSEgeijP91OU46PUrNNkmZrmVebb3wsY\nGouVlW0JiHy/x/ZpWN9ZayBMoCdKZ0+ULQOEv7YFwxTl+DimqohPnDqTuCIjf+cZExBKqShwPbAa\n2IIRrbRJRG4VkYvMYV8EPiUibwP3AR9XSingDOAdEVkP/BX4tFKqJVNz1Yw93j7Q1mchS0YpxYJv\nPMHXHtkw4u/fFgyzu7HLVvWtRal5gMzbDtth21cIdKapQexrDnKoPcT3H99q77RDSRqEtRs/pipR\nQFiLj4XlE63r6CbQEyPP72F7fRd/fm1fwrzaghGOnW6YUCwBMbs8z87kzfMbmoqlQeT43Gyt67BD\nPbO97gQTU57fy+TCLEKRuL2r7nBEW3WGIvg9Ltwu4dv/2Mz2+i7y/L2LrfW5V2+sozDbywVLKu1r\nVskSgBmluVTkG4l8DR2GgHAuyBY+j4uCLA+luT4WTi5g3f62BK3KYulU4/sszvHSGjCiqSrMBLWS\nXCOJ1Sq1sWhyPrm+3qS7xq6wLQQtv09elof8LC+3X7KUH1+2DDDCbC2n8+6mQMLvdk9TwBaWbd0R\nW0P82nsW8v1Llmak3EZGfRBKqceUUvOUUrOVUt81z31TKfWo+XqzUupUpdQypdRypdST5vmHlFKL\nzXMrlFL/yOQ8M8VIlPsGuOuuu6irqxt84AThQEuQi+94mWe2DlysMGzulu9748CA44bDz5/ZyZW/\ned02kRxqM/7TW7vUATWIFI7ojjR9ENbiUpzjtRfdPhpEa5BJBX4mm1nOYCzaLcFwgn3/lNml5jMN\nX8JHTpzOSbNKuPOFXmXdEjbHmnZ4y+a/ZGqvS9DaKS+ZUojP4+KSFVM50NLNblOYVJfl2kKxMxQh\n3+9hUoExt+QksbbuMJ09Ucrz/dxz7Yl8+XwjfPe0OWX2mGA4Rnt3hCc313Puokn2s8DYwd/3xn4A\nZpbl2gt4XXs3bd0RZpfn9flO//P8+TzwHycjIiyZUmgL+xlluQnjLj2uir9++mSWTC2kORCmviNk\nC6DZ5Xm4BM4xzVQLJxeQ4xBqW2o7bKe29bu2hN7lJ0znvcdMIdfn5oUdTTSYGkYsrtjZ0MXf3qrh\naw9v4KwfPceX/vI27cEIG2raqTQ/dyabhI22k3pcY5X7Xr9+PZ/+9Ke58cYb7WNn2YzB0AIiEWtR\nHqj8AaReiIfLk5vq+MPLe+zjpi4j2sXa7VlRO5bJJFVcfns/GoRSypEoN/CcrRINJbk+e7HvjsT4\n8ZPbWLPXULJrWoNMK86h1NzVZnvdlOf7aQn02vd/evlyfnf18eT7PdS0BglH4+T6PZw2p4yDbd22\n8Ek2V1kCYpnDfJVvLnRnL6zgza+fwxlmHsGru5vJ83soy/P1NTGZwmt7faIjts00MeX5PZw0q5TP\nnTWHrbet4jNnzk4Yd/ere+nqiXLlidPJ8rrJ83soyfVx6pwyW3OZXpJDaZ4fl8D2hi6UgjkpBER1\naS4LJxsC75IVUx3nEzUIn8fFyuoSinN81LQGCYZjVBQYAmjRlALWf+s8Lls5jVnluZw2p4wcb68G\nsa3OcFy7Hbv8fIcJyudxceb8Cp7aXG9nkANcf+9b3PTg2/x1bQ1g+Ih+sHorbd0RPn/23D6fZaTR\nAmKU+OMf/8gJJ5zA8uXL+exnP0s8HicajfLRj36UpUuXsmTJEn72s5/xwAMPsH79ej784Q8PWfMY\nr4QiVrjjwJEbqUw5w+Wva2v4zYu9AsJaQC0hZe067eJujro8SimC4ahtjkre8Ycicdsmvq85wPk/\necFeUJKxdvQ5Po+96AZ6ovz82Z08ut6InjrQ0s00c3EEYyEqy/PTaDqVAUpz/WR53UwqzGJ3o2Hm\nyfN7mFNhLKC7GruMshJmEtiSqYV43WKbmJY5zFfWQiciFGR5mW4urJsOdlCR7yfP73EkyhlO6plJ\nu3OL9u6IPcYiy+umOCdxQ/V/L+zmmKpCjjUFVVmej4WT8/nIiTMS7nO7hLI8v/19Wp/PSXFurzN/\nWkkOHnMRt5zUyZTk+uzs7gpHDaSCLC+VhVk888UzmVWel6BBWM7w2eW9z8xPqvR6/pJKmrp6eGKj\nsRn0uIT9LUH+a9UCtt62iu9+YAmRmOLBNQe4dEWVbc7KJBOmWB+P3wx1I2yLrlwKF9w+5Ns2btzI\nww8/zCuvvILH4+G6667j/vvvZ/bs2TQ1NbFhgzHPtrY2ioqK+PnPf84vfvELli8fN4nlh4VVw8fp\nzExFf3Vw+uNASxCv22Xvbmvbu4nGFNNKcuiOxBLKGyTv9A+1dRM2w0YhUYO45/X9fP2RjY57E+dt\naQ9ZXhehSJxt9Z38a0Mt8ysTk+msOVqfzRKQzV1hlDJCL3uiMWrbu43ds6lB5JtO4S11HfY9llmo\nsiDLDst0CogXdzTxtYc3suFgO/mmfb4012/vbqc5om+c/gHAjswJx+KU5/tNYdYbxWRoFX6mFmX3\nKc3RGgxT3uO3525RmG0spi4xchY6Q1HOmFtum1duuWgxxTk+Fk8xNIF8x5wqCvxsNU1ZU4uz8Xtc\n9ETj9rOKkoTPM188k9d2N9vvmUyJY25O81YyTh+ExezyPFtrSv7ezppfjs/t4vGNdfg8Ln5xxbEU\n5/o4vroE6NXionHFe5dN7vd9RxKtQYwCTz/9NGvWrGHlypUsX76c559/nl27djFnzhy2bdvG5z//\neVavXk1hYeZ3CEcL7d0R/vTq3rQiMSwNonOQiB/nQtydhrC44b51/NdD79jH//XQBm58YL19fzAc\ns4VTsnZS2x5K8Ds0d/XQ1NXDA2v28+8t9QljnYIrHI3b8e+VjsXm1RThpoCd2BXoidrmIsu23dIV\n5kBLN3EF1WU5lOZZAsLLpIIs6tpDtgZhLU7l+X5bC8r1e5hRmovHJfz36m3saQrwrfct4p5rT0RE\nKMs3nuf3uBIidJyvrWcXmw7USQVZ5PndtvO90xH1lBxlBcbvtqmzp8/u2rpncmGvk9npcD5zfgXL\nphXhcbt4/stn8sSNZ9jXJhdm2+9fkuuzhY8VdZWsnUwvzRmwumqxQ0BU96MJAXY2uX3sddsCRQS7\n0KFFfpbX9vWU5/k5b3GlLRwA5k3KJ8trONRP6qcS7kgzcTSIYez0M4VSik984hPcdtttfa698847\nPP7449xxxx089NBD3HnnnaMwwyPP/z69nd+/vJfKgizOW1w54FgrsqNzCBpEXUeoX7PG717aQ3m+\nnx31neQmORYtc4P1rLZghEkF7gThU5TjZWtdJ//x57X2ueZAmD+/to//fXoH5yysSHg/573Pb2/k\nbjNqqLIwyw6FXG8mmlmLTDQW57Z/brbj+wPhqF0oz6I1GLajgmaU5pLn9+DzuAwNotBPMByzE+cs\nE46zTHSu351QI+ra02dyjaOEttVy8/S55Ym2dH/fnXZVcQ6twXYq8v34PC4CPVGjcGA0bu/uZ5va\nisclCVnJh9pDfYSO2yWcNb+c6SU5/PFV4/tyajFOkk1DS6YU8tRmQ0iX5PoozvVxqD3EjNJc9rcE\n+9UU+mPljGLmVOTxpfPmDdjAKNeX+BkqC7NswZzn96R0Lp88u5TX97SQ5e27d/e6XVy8bCpl+b4R\nbyrVH1qDGAXOOeccHnzwQZqajF1ic3Mz+/fvp7GxEaUUH/rQh7j11lt56623AMjPz6ezc3y2dXxt\ndzMX/PRFOw8guZSyE8sUYwmI/kxMnaEISqmEhKs6R0ZxMBxNyAf45bM7+eETWwmYiWAdoQjt3RGz\nOJxR48d6T8vM5DQxfeykGcwuz7MX78JsL81dYTuOvT6pFML6A22c/5MXaAmEOWg6nU+aVcKJM3t3\nhZGY4q39vVnNe5uD9sIIhi0/uVBeS6A31n5maS4iQnmen4Jsr71ztZzM1kJVlte7G7YXL3NxvvLE\nxAJ31nd4/mIjUscy5+T6+5pSrN19RYGfXL+HaFzZGpalHSwwTWiWsEoUOn33rr+/5gSuOqnXx1DV\nj4BIxpkJXZzjoyTXR57fw6R8P4XZ3oT3TYeFkwt4+qZ3sWrJwGaeHPN7mWQ6sisLsuxzqT4fwCmz\njWitvc2pC/j94NJj+PL5C4Y038Nh4mgQRxFLly7lW9/6Fueccw7xeByv18uvf/1r3G43n/zkJ+3C\nXT/4wQ8AuOaaa7j22mvJzs7mjTfeGFIEVLpsr++kwkz6GS5NXT10dEeYlSJSpD/+vv4gW2o7CJum\nm9f3pE532Xyogwt/9iJ3XLmCUIqaPBahSIyltzzJVSdNTyh/UNdhmGaUUrznZy9x0qwSvn/JMbQG\nwka5h96KC+xq6MLaz0ZiivbuiK1BWIuc08R0wdLJfPTkao7/7tMAzJ+Uz76WgD3GWQ4iy+uiobOH\nhs4eNh/qoLY9hM/j4t5rT+Jv64xCA5Zt3BmlZUUvvX/5FLp6Ymw42NbHD9IaDLOnqYuCLI8dI//d\nDyyhPN9vj93Z0IXLYd6wtALA1p7uvfYk9rcE7BBOi9nleWyt67RDOX9/zfE8t63RdoY7sXb3FflZ\ndlkLO/7ffJ9Viyu59eLFZHvdfPmv7zClKMvOAk+2z1s4TU+Ti/q3/ztZVtVrqs3yuplSmM2hgm6u\nO2PWoNrq4ZBjahBzKvKo7+ihsjDL1iqSTWgWltltRYryHqOBFhBHCGe5b4Arr7ySK6+8ss+4devW\n9Tl32WWXcdlll2VqasZ8fvM6H1wxla9cuHDYz/jvJ7axZl8Lz3zxzH7H1LWHeGlnE5ceVwVgt9jc\nZUbSrNvfRigSsxvMWFjtJ4TGC8cAACAASURBVO98YRfvWzYFSO2DsBbVP7+2PyGk0TKt7G4KsKcp\nwMHWbr543nz2OGrxWNz92j5wuEKaunrsKJS2oKGdOM1EJbm+BFPN3El5vLG3V9A5yzmU5vptx+yh\ntm4OtnUzpTALl0tsp2ZVcQ77W4IJQqjGdE5/5cKF/Pr5Xby2u7nP54/EFBsPdlBdlmubL86cb5i3\n9ps70p0NXQnmDee8rUV50ZQCFk3p27/6+x9cyvXvnmPb4Cvys/otFW4lzVXk++18FEuTsjQUl0v4\n2MnVvLzT0KRnlOTaAqI/56/T9JSumSV50/PlVfPpDEWZWZbL3El9AwFGiimFWfjcLk6oLuXlnc1M\nKsiyBXOyCc3C53Hx5I1nJERHjSbaxKRBKUVzoOewe9seau8eNDfh/jX7+dJf3qYzFOFQW3eCKl2U\n4yUci6fsy2yFj2481NEbU+9osnPTg+uNsgyODOZHzLDP8nw/D62toScas58djsW557X9Cbt7y9Tw\nt7cO2rt5gMbOsO3kbg2GCUXiOH3p1m79Bx9cyorpRZwws9exaFGY7aUi35+QYHawrZtDbd12HwSr\n3IVlnmkLRrjpwfXsbQpQ09qNz+OiPM8MGw1H6QxF8HkS/wuvP9Bml4hwYsXrGyGkvbvXVBpEfxRk\nee18gcE4aVYpCyrzWTC5wBY8d7+2FxGYleQLskxMMxx5Bxctn5LyuTnevuasdPj4KdVcYpY+L8vz\n9+uPGkkqCrJY/61zWWVmek8uzLK/4/40JDCc0YejyY8kWkBoCIZjKHX47S6buoxErIEikazEsvbu\nCG+Y5iTLEfzBFVXk+z12HLgTK1M5Fld21y/LSf2df23hb28d5KG3auzS0GAslgA/vPQYdjUG+P3L\ne3l1dzOVBVmcv3gSv3p+J89ubcDvceES7MqaydR3hOxdcFswYu/sXWL8R7f6GH/4+On87bOn8r5j\n+i5u7zlmMm987ZyEnXFteze17aGEyBzoLdewbn8bf3vrIFf97nUOtAapKs42NA2zaU1jZw/lKcw7\nc1PE+hu5BIZgcC5OyU7qkWLupHye+H9n2PZ+gJd3NnPViTP67NqLso3FcMHkAq44YRr/uP60Phqk\nxXDLSdxy0WJ+/OEjHyae4/MwqzyXK06YxrsXVAyqQRxtjI1ZHgZDbcQx1lFq6AW7LMEwlEqiYMTk\nK4WdGNXc1WMImnC0Xxtrk0NAWKaWY6oKeWt/G9WlOZy1oIKnttTzXbP7mYWz8NteM1KnqydKXXuI\nB9YYpRUisbidwGSR7XVz1vwKVs4o5qG1NTR29fDu+RV8edV8zv3xCzy5uZ5JBX5Kco24/O99YCmY\nfy5twQifveethI5fLY56Rp89cw7HVPUNRXa5hFdufjfPbWvkf5/eTkNnj7045zgiW/a3BKnvCDHV\ntKVfsGQy374ozIePn8afXtlHi5lsV9PaTVFOb+VPaxfaE41TUeDvk0uQqjw1GJnFrcH2hNLQxTk+\nXGJoT5agG2mOry7hxnPm4XELHz+lus/1aSXZ3H7JUsOX43BC98dvPraSWeWZ1wBGCq/bxfcvOQbo\n7bPdn5P6aGNcaxBZWVk0NzcPa9EciyilaG5uJisrPeedheXsHYoG8eSmOk7/4bN89K7X7fe2HLip\nnMcWdgP6YIS2YJhsr5v5lYbZYkpRNucumkRLIMyGpGimQ+3dzJtk7Iyd5adf2NFom3saOntsE5cV\nIWPtilctqWRHg1Fc75IVVUwuzOa3Vxvd01YtruTOjx7H9z6whFPmlHHKbONn1eJKO5vVojUYtk1c\nS6YW9OvknFKUzZUnTrcduFasvTN8cd3+NqNRjWlicruEq0+pJsvrJtfvToh+2niww9Zw8hw7fSt3\nwhmJc8zU1ALitvcvAUiIrXe7hJJc/6DmpcMh2+fmC+fM5XNnzUn5PiLC5SdMTzvc9NxFk1LWVBoL\n2BrEGBEQY2OWw6SqqoqamhomUre5rKwsqqqqhnSPs2RDOoSjcb5mZgbvaw6ilKKjO2rHsneGokzu\nJ8fP8iW0d0doDUYozvHaNW8mF2bbZQ/WH2izK4iCYWI6cVYJ2+u7bHMPGI1VinO8zKnIo7Gjh5Ic\nH4XZXqYUZbO1rjev4fzFlXznX1uYXZ7LqXOMcNKTZpWy63sXElcqpcPT5RJK83wJAsJpYkpnUbXC\nSC0B4UzYszqkTU4RS5/j89j1nSwsQZLnyDtYPKWAxzfWMa042/bnFOakXmiPqSpi23dWISRq1OX5\nfrvSrCazWIKhPw37aGNcCwiv18vMmTMHHzjBsTWINGsXPbGpjsbOHk6fW8aLO5poC0ZocZSh6K91\nolKKpk5jXHt3xK5pf8GSyexrCTJ3Uh5et4tJBX4eWHOAHz+1HY9L+MunT6apqyelbf2pzfVcsKQS\nt0vYeLCd0jwjoshamC2TzrSSHD531myOry5JMDm6XYKb/k2Q5fn+BBOToUFEE549EFbWriX4rHpM\ns8tz7citY1LU1Mnxue36TH/99Mn8dW2N7dtw+gosf4UzyW4gUpmRKvL9E0bLHm0sAVGQPTaW3rEx\nS01GsctGp1n99G9v1TCtJJuPnDidF3c0caA1aJe/gN5yxn3eJxyzQz7buyO0BMIU5xrF3b73gaX2\nuGVVRTy5uR63S4jFFX83o5GqinPI8bn71Fg6eXYpe5uCNHQ2UJ7fQ1mez47OcdbDGU6CUVmen01m\nwbqiHC8HWrr7lGseiJJcv3mvISgs2/m7F1Swq3EP/7lqfkLpBgundjK/Mp/bP3iMfex8XyuqqDDb\ny6+vOs42rQ2FL503f0QLG2r6pzTPz48vW2aHHx/tZNQHISKrRGSbiOwUkZtTXJ8uIs+KyDoReUdE\nLnRc+4p53zYROT+T85zo2O0g0zQx7W4MsHxasV3Lpqa1226UA73RRWv3tfDPd3r7MzsjjAwNIpIy\nnM9ysl59cjVZXhf/esdoPj+tODvlovzuBRVUFBilJPY2BynL89vROTmHaesty/Njba4/cOxUmrp6\n7KqpybV0UlGaZGK69vRZPHr9qdx8wUIe+swpfOZds1PeZwkIkb4lG5zCwwoRLcz2smpJ5YC1gfpj\naVXhEavto4FLVlQlFPw7msmYgDBbht4BXAAsAq4QkUVJw76O0WnuWIyWpL80711kHi8GVgG/tFqQ\nakYeSzD0RONEU7SkvOulPfzs3zuIxuLE4opDbd1MK862BcSBlmBCDkVnKMLjG2r54K9e5fp719k9\njJ1d1tq6I7QGw3Z0j5NzFk5iQWU+nzitmvmT8tndFMDndrFsWpEdHuiM/68qzrETixrNNpCpNIjh\n4AwDvWjZFKYUZvG0WXwvHQ3ipFklHDej2G7e43YJx1QV4XYJx80o7jfCzpp3ns/TJ7TT+b5Zpsmo\nYIzYtDVji0xqECcAO5VSu5VSYeB+4OKkMQqwMm8KAWu7eTFwv1KqRym1B9hpPk9zGERjcS76xUs8\nszWxuqjTOR0Ix4x6UL9+hV89twuAW/+5mR8/tZ1b/7mZuo4Q0biiqjiHgiwvhdledjZ0sdeRkdwZ\nivLn13vrBlkF5JxJdG3BMO3dEUpSaBDzzBj6quIc24Ry7PQiuzEMYN93xjyjOY2zLIRTQKTjJxgI\nZyJZfpaHDzkyh3PSyBs4bkYJD33mlH7j+vvDmneqeHnrO5hdnovLJXzh7Lm8d1nqxDKN5nDIpICY\nCjh7PdaY55zcAlwlIjXAY8ANQ7hXM0RagmHeqWnnrX1tCeedVUGD4Shr9rayZm8rP3hia0LUzQvb\nG22HrZXtm+V18Ze1Nfz2JaOZjtsltATCvLm3lRPMcEqryYylQUwtyrbLUg+WMWrZ1E8222Nai+Oy\naYV87wNLuePKY4HeTGEwwlatnf/hJn85i9lled12ViykdviOFNa8U2kpuX4PP7viWO677iQAbjx3\nXsrS2RrN4TLaeRBXAH9QSlUBFwJ3i0jacxKR60TkTRF5czyHsrYFw/z2xd1p9UoYCCuU0dn4BpI0\niJ4of3x1L2A4Zfe1GLv/uRV57GsJ2jWRrGJs5y0ycgVKcn2cNKuEPL+HF7Y30hON85GTjGqgVjmL\nnQ1d+DwuZpbl2hqHs5tXKk6YWYrXLXaBOMv+nuV1c+WJ0+1wQavs8jWnVjOrPM/OMD5cDcJpYsrx\neYblBB4OA2kQYJi7kovpaTQjTSajmA4CzkpeVeY5J5/E8DGglHpVRLKAsjTvRSl1J3AnwMqVK8dt\nnN4TG+v4zr+2cNKs0sNqM2iFWFr/WiQKiBgvbm+0x1mtGi9YUsnPntnJ01saEOmtpPmt9y3i2xct\nRsQog/Gu/36OrXWduMQoFDe1KJudDV0opXhycz1nzC0nx+e2C80NpkEsmlLAhlvOt000VgZqVtLu\nPdfvYettq+xxBdkezpxfzvHVxRwOzlIW2V43IsKXz59vC8pMkTvGEqo045NMahBrgLkiMlNEfBhO\n50eTxuwHzgYQkYVAFtBojrtcRPwiMhOYC7yRwbke1VgOYGdhOYuWIRTYa0/SIFoCYaKxeEL0Um17\niI5Q1C4q97wpLM43TSsvbG+ksiDLNq943C5cLkFE8LhddlTN4imFFGZ7mVWey87GLt6paae2PcSq\nJZUJGbPJ3bxS4bTfWztqf4qGKs5xIsIfrjmBs03NY7g4fRBWFvTnzprDTy8/9rCeOxhW9JV2PmtG\nk4wJCKVUFLgeWA1swYhW2iQit4rIReawLwKfEpG3gfuAjyuDTcCDwGbgCeBzSqmhFQoaR1j19JMF\nxDs1bRz3nafYdKj/JjtOLAFh1RM687+f5UdPbqerJ2qXarA0htPmGM7f57c1UpLrY5GjiudAdXCs\nBc3yGcytyGdXQ4B/b23AJXD2gookATG0BdBpYjoSFGZ78brF1h6OFFqD0BwNZNQHoZR6TCk1Tyk1\nWyn1XfPcN5VSj5qvNyulTlVKLVNKLVdKPem497vmffOVUo9ncp5HO62mSShZQGw82IFSsLU2PXOH\nJSDaghGe395IRyjKva/vo6mrtyrotnojKey0OUZnq+ZAmOrSHESE8xdPoizPx7cvWtzve1gahCUg\nFlTm0x2J8fiGWmaW5VKc67NLLWd73UO2o+fZJqYj1HLRJZTm+tPKeRhJLA1irFT91IxP9F/fKPHC\n9kZe3tXEVy4YvEGPrUE0JgoIq6qp1W1sMJwmptWb6vC5XXSEomw82MExVYXUdYTYamoQix0NY6xQ\n0juuXIHbNCf1R36WB7dL7IJwVpjqjoYu3nuM0aLxg8dVcfrcMrJ97j6N3QfDEhD+I6RBAJTl+/r4\nbTKN1iA0RwOjHcU0YVm9qY7fv7S3z7n3/vxFYknRSpYGsbcpkNBL2YoEsrpw9YfVV9la5HqicR7b\nUMv7j51im46snfzuxgB5fqNl5Xfev4TvfmAJXzh7LmD4GwYzs1x63DRuXrXAXtjmTsrDyvNyNpup\nKMgaVsGyvCNsYgKYlJ91xBdqy5SWrzUIzSiiBcQoEQzHCMfi9ER7XSvrD7Sx8WBHnzDU1kAYj0uI\nxhW7G3sT0vaZxdlqkjSIl3Y02RnRrYEwp9z+DA+sOZBQsTMSU5wws9Suz+8M56wqzkZEuOqkGXzk\nxBlDsr2fNreMT50xyz7O8rrtHtWL0uxGNhDWwuk/QiYmgC+dP98ulX2k6O1drAWEZvTQAiIDvLKr\niee2NQw4xgot7XQUtuv1ESQJiGCYU0yfwOt7jG5q8biycxRqWns1iF2NXVz1u9f514Zafv38Ltbu\nayUcjfP39Yfs51ssnJzPRcunMLUom1llufaiazWmGSms3IEFkw8/h8BaMI+kBrFwckFCD4UjweSi\nLLK8rjHb90AzPtDbkwxw5W+MJjp7b39Pv2OsqqadoagdStkbZdS7kMfjivbuCMuqCtnV0MWru5r5\n2MnV1HeGCEXilOb6qG3vJhKL43W77IJ4j2+o44lNdSwzM2xf39PMzLJcfG6X3U9hTkUefo+bZ790\nJl63sO5AK2/saeXdC0a20uR7j5lMKBKzm9scDr0mpvG9tynL87P526uG3WJToxkJtIA4QoQihknJ\nCgPt1SB6hUGqTOeOUMQuSXHSrFKe2VpPPK7YY5qaTp1TxqNvH+KdmjY+f996O3v5lV1NgBEKCxBX\nsKsxwJyKPDsaysplsArf/fIjx2Xks69aMplVSyaPyLMWTM7n46dUc8rsshF53tGMFg6a0WZ8b8NG\ngfZ+ol1uf3wrV9z5mn1s9TQYzMRkOaiLc7ycOqeU1mCEdQfaWLO3FREjwxng2a2NHGzr5tVdhgnK\n6lmglFFPyCpFYXVvG6v4PW5uuWjxmCmXrNGMZbSAGGGs0NNkDrQE2WGWnACngOgVKFaUUatDyFiZ\n0sU5Ps5bXEme38Pdr+7llV1NLJ5SwGyzy5qlFexL0VVsWkkO55u9k2eUGjkIn3/3nGF/Ro1GMzHQ\nAmKE6U9AdIQihKNxe/EPmk16OlJoEE4Tk6VNFOV4yfN7uPS4Kh5Zf4jX97Rw8qxSeydtVUw92NY3\n5LWqOIfzFxslJ0pyfez+3oXceO68w/qcGo1m/KMFxAiTvIP/8ZPbuOG+dXR0G4Kgrj0E9GoQXaaA\niMcVHaY20eZwUls9FCxBcN0Zs/C5jV/bybNLKTLLVljv68yhsEJXpxVns7K6hE+/azarllTatZM0\nGo1mILSTeoRxahDRWJwXdzZR2xayz9V3hFhQmd/HB9HZE7VbW7Y4NIgXdzRRnu+3y2tPKcrm9a+e\nzbPbGjhzXgUul1CY7e0Twgpw3qJJ3PfGfuZOysPtEm6+YOg9mTUazcRFC4gR4KnN9eT43Jw6p4wa\nR1ZzKBqnprWb9mDELoZX1xGyQ1yh1wfhdG5bZqVQJMaz2xp4/7FTEyJainN9XLKiyj4uzfUlCIiK\nfD9dPVHee8wUrjm1murSofcp1mg0Gi0gRoAfP7Udj0v4xw2nUd/Zqy20BsK9bTZNmVDbHrK1B4DH\nNtQSCEe58oQZAPjcLttPcf8b+wmGY6xa3NvFLBXFuT5wtPycWZbL/dedpM1IGo3msNA+iBEgGI6y\npbaDUCRGQ0cPRWYJ612Nffs31LeHbAc1wKH2EPe9cYDdTcbY6aU5tAXD/H39QW75x2ZOnlVqV0bt\nj+SeCgXZXi0cNBrNYaMFxAgQ6IkRjSte291MdyTGjBLDX5CqwU9dR4hAT9/WFq/tbgGMPIXWYIRf\nP7+bBZX5/OmTJ+B1D/xrKk3KCXD2W9BoNJrhogXECNBtagRPba4HYLpp809uS+lzu6jvCNEdiZLM\na7uNBLfjZpQQiyu21HZw9SnVgwoHME1MDnQXMo1GMxJk1AchIquAnwJu4LdKqduTrv8EOMs8zAEq\nlFJF5rUYsMG8tl8pdRFHIUopgqbT2RYQJUbW8o4kDWJldTGv7W5mR31fzWKP6UO4+pQZeN3C89sb\nef/yqWnNwdIgKguyqOsIUZCtXUsajebwydhKIiJu4A7gXKAGWCMijyqlNltjlFI3OsbfADgb/XYr\npZZnan7Dpbmrhxyfh2yfm8/es5Yphdl2eGqD6ZCeUWJoEDvruyjL89PUZZz/1BmzeG13M3e+sDvh\nmfl+D509UVbOKCbH5+Ha02dx7emzSBdLg5hZlktdR0ibmDQazYiQSRPTCcBOpdRupVQYuB+4eIDx\nV2D0pT6qufTXr/LdxwwZ9+quZl41TUNWrSMwHM1g5DbMKsvF6zYcxgsrCzhn4SR2m9rCHLNMxgeP\nM0JWv3fJ0mHNqSTXEAgzzV7R2sSk0WhGgkzaIqYCBxzHNcCJqQaKyAxgJvCM43SWiLwJRIHblVKP\npLjvOuA6gOnTp4/QtPunJRBmT1MAt0sIRWK0OvIbTpldyl/W1pDldSU03ynO9VKS66O+o4eCbA+n\nzyvnSdMUddfVx+N2CyU5Pq45tdqukzRU5lbkk+tzs2pxJY+uP8S8SYffd0Gj0WiOFmP15cBflVLO\n8J4ZSqmDIjILeEZENiildjlvUkrdCdwJsHLlysQ+nSPINx7ZyLvmlduN63c1dtkRSk1dRlLbibNK\n+etbNVTkZ5HtaGZTnOOjOMdHSyBMttfNsWZ/BoDSPJ/dIW24wgGMYnybbl0FwMZvnz/s52g0Go2T\nTAqIg8A0x3GVeS4VlwOfc55QSh00/90tIs9h+Cd29b01syiluO+N/QTCUbtlplLYWoBFeb6f+ZPy\nKcj2JgiIohwfJbk+GjuN3IT5lb27++wj2BVNo9FohkomBcQaYK6IzMQQDJcDVyYPEpEFQDHwquNc\nMRBUSvWISBlwKvDDDM61X7ojRo5DfUcIQSjI8tARivLkprqEcTk+Nz/60DLcLkloh1mc42V6SY5d\nc8kZtqobwmg0mqOZjAkIpVRURK4HVmOEud6llNokIrcCbyqlHjWHXg7cr6xGCQYLgf8TkTiGI/12\nZ/TTkcRZhbUtGGH59GJqWoNsrUvMccjxuVk8pRAwKrNaFOf4uPI90wlF4va5Dx1XxT/fqT0Cs9do\nNJrhk1EfhFLqMeCxpHPfTDq+JcV9rwDDC+kZYawieHXtISJxxSmzS5lWnM3uxsS+D7m+3q/S5RKy\nvC5CkTjFuT7ys7zkO9ox//DSY/jvDy07IvPXaDSa4aIzqQfB6tEQCMcIR+PMrchPWRvJcmBbWGam\n4py+Iae6TpJGoxkLHC1RTEctHUl9FmZX5KaMOMrxJ36V2V43bUQoytG9kzUazdhEaxCDkNyIZ055\nPmV5fsrz/cws6xUUyRFJ2QNoEBqNRjMW0AJiEJwaRFmen0JzwV/ztXP45UdWAJDlddkJcxZ+U0Do\nshcajWasogXEIHSEeiuvzqlINC3lZxlmJaeD2iLb66Iw24snjWqsGo1GczSifRCD0NEdIcfnpjjH\nx9KphQnXCkztIMffN+Ety+vW5iWNRjOm0QJiENq7IxRkeXn4s6eQn1QEL8/nQQRyvH2/xoWTCxJq\nMmk0Gs1YQwuIQegIRSjM9lJRkNXnmssl5Pk9KTWIb7x30ZGYnkaj0WQMbSAfhI7u6IANeAqyvH1y\nIDQajWY8oAXEADy7tYHt9Z0DRiJNLc6msiC73+sajUYzVtEmpn6IxxXX/GENgF2SOxV3fvQ4Hamk\n0WjGJVpA9EObI/+hJRDud5zOlNZoNOMVvfXtB0soLJ5SwH+tWjDKs9FoNJojjxYQ/dAaNATEzRcs\nYElS/oNGo9FMBAYVECJyg9nAZ0LRbLYSLdYmJI1GM0FJR4OYBKwRkQdFZJVMkFrVlgZRkqsFhEaj\nmZgMKiCUUl8H5gK/Az4O7BCR74nI7MHuNQXKNhHZKSI3p7j+ExFZb/5sF5E2x7WrRWSH+XP1kD7V\nCGD5ILSA0Gg0E5W0opiUUkpE6oA6IIrRQ/qvIvKUUuo/U90jIm7gDuBcoAZDC3nU2TpUKXWjY/wN\nwLHm6xLgW8BKQAFrzXtbh/EZh0VLIEyOz53QX1qj0WgmEun4IL4gImuBHwIvA0uVUp8BjgM+OMCt\nJwA7lVK7lVJh4H7g4gHGXwHcZ74+H3hKKdViCoWngFWDfprDRSl4/ofQuI2WQFj7HzQazYQmHQ2i\nBLhEKbXPeVIpFReR9w5w31TggOO4Bjgx1UARmQHMBJ4Z4N6pKe67DrgOYPr06QN/inTorINnvwvv\nPEhL7i8pzdMCQqPRTFzScVI/DrRYByJSICInAiiltozQPC4H/qqUig3lJqXUnUqplUqpleXl5Yc/\ni0CD8a/bR2tQaxAajWZik46A+BXQ5TjuMs8NxkFgmuO4yjyXisvpNS8N9d6Ro8sUEHkVNHeFtYNa\no9FMaNIREKKUUtaBUipOeqapNcBcEZkpIj4MIfBon4eLLMBwer/qOL0aOE9Eis0cjPPMc5mls9b4\nN6+C1qAWEBqNZmKTjoDYLSKfFxGv+fMFYPdgNymlosD1GAv7FuBBpdQmEblVRC5yDL0cuD9JCLUA\nt2EImTXArea5zNJhCIhYdinBcEz3k9ZoNBOadDSBTwM/A76OEXL6b0zH8GAopR4DHks6982k41v6\nufcu4K503mfEMDWISNzIBcwboIqrRqPRjHcGXQGVUg0Yu/zxT2cdAJGIUck1L0sLCI1GM3EZdAUU\nkSzgk8BiwO67qZT6RAbnNTp0HgIgEo0CWoPQaDQTm3R8EHcDlRjJa89jRBR1ZnJSo4alQUSNMhta\nQGg0molMOgJijlLqG0BAKfVH4D30k/A2polF7TDXqKVBaBOTRqOZwKQjIKzWam0isgQoBCoyN6VR\nItyF4YOHaESbmDQajSadFfBOMxfh6xh5DHnANzI6q9EgHrVfxmJaQGg0Gs2AK6CIuIAOs2DeC8Cs\nIzKr0SDW24M6FtVRTBqNRjOgicnMmk5ZznvcEQv3vjQ1iFyfFhAajWbiko4P4mkR+ZKITBOREusn\n4zM70jhNTNEYOT43bteEaJ6n0Wg0KUlni/xh89/POc4pxpu5yWFiisci5Gr/g0ajmeCkk0k980hM\nZNSJOwVEjHwtIDQazQQnnUzqj6U6r5T608hPZxRx+CDi8Sh5uVpAaDSaiU06q+DxjtdZwNnAW8A4\nExC9PggVj2kHtUajmfCkY2K6wXksIkUY/aXHFw4Tk4pFdYirRqOZ8KQTxZRMAKN/9PjC4aRW8ZhO\nktNoNBOedHwQ/8CqQWEIlEXAg5mc1KhgCQi3DxXTAkKj0WjSWQV/5HgdBfYppWrSebiIrAJ+CriB\n3yqlbk8x5jLgFgwh9LZS6krzfAzYYA7br5S6KPneEcUyMXmyIaJNTBqNRpPOKrgfqFVKhQBEJFtE\nqpVSewe6SUTcwB3AuUANsEZEHlVKbXaMmQt8BThVKdUqIs4igN1KqeVD+ziHgalBKE8WLuLkawGh\n0WgmOOn4IP4CxB3HMfPcYJwA7FRK7VZKhTEc2xcnjfkUcIdZ68nqXjc6mJnUMbcPN3HK8/yjNhWN\nRqM5GkhHQHjMBR4Ax3aJWAAAIABJREFU87UvjfumAgccxzXmOSfzgHki8rKIvGaapCyyRORN8/z7\nU72BiFxnjnmzsbExjSkNgJkHEREfLuKU5WsBodFoJjbpCIhGEbHt/yJyMdA0Qu/vAeYCZwJXAL8x\nw2gBZiilVgJXAv8rIrOTb1ZK3amUWqmUWlleXn54MzFNTGHx49EahEaj0aTlg/g0cI+I/MI8rgFS\nZlcncRCY5jiuMs85qQFeV0pFgD0ish1DYKxRSh0EUErtFpHngGOBXWm87/AwndQ9+HDRSanWIDQa\nzQRnUA1CKbVLKXUSRnjrIqXUKUqpnWk8ew0wV0RmiogPuByj4ZCTRzC0B0SkDMPktFtEikXE7zh/\nKrCZTGJmUoeUBzdxSnLTsaJpNBrN+GVQASEi3xORIqVUl1Kqy1y8vzPYfUqpKHA9sBrYAjyolNok\nIrc6TFargWYR2Qw8C3xZKdUMLATeFJG3zfO3O6OfMoLpgwjGffhccbzu4eQQjkH++D54J52YA41G\nM9FIx8R0gVLqq9aBGY56IUYL0gFRSj0GPJZ07puO1wq4yfxxjnkFWJrG3EYO08QUiHsocKlBBo8j\n9r4Mk5fBMR8a7ZloNJqjjHS2yW7L3ANGHgQw/gz0pompK+bBKxNEQCgFKpZQqFCj0Wgs0tEg7gH+\nLSK/BwT4OPDHTE5qVIhHAKErIngkPujwcYHVRc9RqFCj0Wgs0qnm+gPTF3AORjmM1cCMTE/siBML\ng9tHIAoe3wQTEDEtIDQaTV/S9cTWYwiHDwHvxnA6jy9iUZTbSzgGbiaYgIgP0cT02H/CP/7fyM9H\no9EcVfSrQYjIPIzktSswEuMeAEQpddYRmtuRJR5BuTzEcE0cAWFpDkMVEPUbEzrwaTSa8clAJqat\nwIvAe628BxG58YjMajSIRVDiIYYbl4qN9myODHHzcw7VxBSLaAGh0UwABjIxXQLUAs+KyG9E5GwM\nJ/X4JBYh7vISw4WoCaJBWM7poTqpY2Htt9BoJgD9Cgil1CNKqcuBBRjJav8PqBCRX4nIeUdqgkeM\neISYGCYmYaJoEJaTeogmpnhUaxAazQQgnVIbAaXUvUqp92HUU1oH/FfGZ3akiUWIWwJiwmgQw3RS\naxOTRjMhGFI9CaVUq1lB9exMTWjUcGoQQ10wxyqxYeZBxCPaxKTRTAAmSMGhNIhFiOIhjgtBGVnG\n453hmphiWkBoNBMBLSAsYhGiuIkp8yuJTwA/xLCd1FpAaDQTAS0gLOIRIriJ4TaPJ4CZabiZ1HHt\ng9BoJgJaQFjEokRwg8v8SiZCLoSlJWkntUajSYEWEBaxMBHlwe3xGscTwcQ03EzqWMQQoBPhO9Jo\nJjAZFRAiskpEtonIThG5uZ8xl4nIZhHZJCL3Os5fLSI7zJ+rMzlPAOIRwsqN220ml08IDeIwTEzD\nuU+j0Ywp0in3PSxExA3cAZyL0Xt6jYg86uwMJyJzga8Ap5qNiCrM8yXAt4CVGEUC15r3tmZqvsSi\nhJULj8cDUSbG7ng45b6VcgiWMHizRn5eGo3mqCCTGsQJwE6l1G6lVBi4H7g4acyngDushV8p1WCe\nPx94SinVYl57CliVwblCPEJP3I1nPJmYulsHDtcdTpirU2uYCI58jWYCk0kBMRU44DiuMc85mQfM\nE5GXReQ1EVk1hHtHlliYUNyN1zNOTEyBZvjRfNj57/7HDCeT2qltaEe1RjOuyZiJaQjvPxc4E6OM\nxwsiknYvahG5DrgOYPr06Yc3k1iUnrjLoUGM8d1xsBliPdBxsP8xsWHkQcS0gNBoJgqZ1CAOAtMc\nx1XmOSc1wKNKqYhSag+wHUNgpHMvZtmPlUqpleXl5Yc323iEUNyF1ztOTEy2I3mARXw4Jian4NRO\nao1mXJNJAbEGmCsiM0XEB1wOPJo05hEM7QERKcMwOe3GaGt6nogUi0gxcJ55LnPEInTHXPgsATHW\nC/bF0hEQVh7EUDSIcOrXGo1m3JExAaGUigLXYyzsW4AHlVKbRORWEbnIHLYaaBaRzRglxb+slGpW\nSrUAt2EImTXArea5zBGLEIy78PjGiQZhCYhoT/9j4sPIg5ioJqaeLghm9k9w3NJeM9oz0AyTjPog\nlFKPAY8lnfum47UCbjJ/ku+9C7grk/NLeD8zisk/XnwQ6eQqDCcPYqKamH52LAQa4Jb20Z7J2KJm\nLfz23fC5NVA+b7RnoxkiOpPawizW5/WOkygm28Q0gAZhL/AqfY0pwcQ0gQREoGHwMZq+dNUZ/+rv\nb0yiBQRAPI6oGFHc46fURjwdE5PjM6a72E9EE1OXXtyGTTRk/jvA36HmqEULCLAX07DyGJnUMPYF\nRGwIJiZI31GdkAcxQTSIQ+t6X0+EPiEjSdTcREyUv5VxhhYQYP/xRnH3CoiJYGJyLvbp+lycIbED\naRA9XYM7J2MRaN6V3vuOJgff6n0dDgzt3rHyGZMJdYyMc9nSIAb6O8wUgWYINB359x1HaAEB9kIX\nwTMO8yDS1CDSzYVIN8z1xf+Buy4Y+FkbH4JfnmSUBDmaadjc+zrcNbR7N/4N7jjRWKzGEr88GX6y\n+PCfY/2NREfBHPnoDfDIZ4/8+44jtIAA+z99gCw846Waq7Xgp+uDGGkTU1e98TMQgUZjATnaw0ed\nQqFniAIi0Gh8Z+37R3ZOmaZjhEJTbQ3i/7f3pWFyVeW67+ruqu7qTk8ZuztzICFkIBAiARFEVCYR\nBESCegWUCzII5yjnHtCjh4t6nK4ejsq5iogXZRZEQBAMGJmnkAkChIQQkvSQqdPp7urqGtf98a2v\n1tq7dlXX2JVK1vs89dSuXXtYa++1v3d94y4DQQz20PW3yBuWIIDkQz8oA6jx+WldpYe58gOZVRQT\ncnBSZ2liigTp3Jk0MSavXGflo43IkLE8kNu+fP0HeorXnlIjnGMfMyGWxTgsFSJDB08gRYlgCQIw\nNIiADnM92ExM2RKiw2+R4djRkPoeSr8NE0Sus/LRRnQI8I+h5Vzbyn0c6C5um0qJ7rV6OVFgRYGk\nBlEGJ3V0SJ/fIi9YggCSM6ZBWWdUc630UhvZmJjyIIhs8yCYGJgoPI/FBFHEGWspEB0CGlStr1y1\nHb7+/RVEEGbUVqEzf96/HGGu0aHy+D4OIFiCAJICKogA/P4DxMSUs5O6yCYmJohMUT+VYmKKhoAx\nk2g5VzKrRA1i5zt6ORPBZwPufzlMPdGQ1SAKRLnLfe8fUAJqEIYGUekmpmx8EAXnQWTyQWShQcQq\nRIOIBIExE2k517ZWog/CJOxCBWy5CEJKum9V1aN73gMMVoMAknbloKzTYa4HQxSTw0ldKhNTFj6I\nitAgFEHka2KqJA3CHDPF0iBG28QUCwOQNoO7QFiCAJKRKUEEdLnvStcgsjIxmWGu2RJElhpENgQR\nrwAndSJO7WyYAEAcHE7qmEEKhQpYvsej7aTmcRcL2+z3AmAJAgDCg4gLHyLwwV+sct+9m4FVfwB6\n3iy8fbng/eeUUMsmkzofE1O2PgglZCJF0iCkBDY/k19UTdcayrXoeSP3hDUWNP4GimTKWYNQJpqh\nPZUzm40aZqVYsXwQWfZ968uFay2AMTGR+ZNTJAhsfaXwtlQwLEEAQHgAkep6CAH4aopkYnriBuCR\nq4GHvlp4+7LFrg3AHWfSe6jjWdTAyaeuUnI7kd4sxfZfIMsw1/6Rz9u9Fvj9WcCW57JrJyORAG79\nKHDnucCd5wHP/yy3/ZngfAGgdkwePgiDRCvFDxELATV1tBwt1AeRQ6JccA9w+2nAuvsKOyfgnJjk\nG4l19wXA7afs3xpuiWEJAgAigwhX1aO2pgqCnVqFahBcAXQ0M2g5IznUq2f6pQpz9Tekf+jZ/guM\nQBBKeGTzAA7tdn5niyGlMXStppIeQ7lqEIrofAVqEEAFEUQYqGtRy4VqEDmU2gjuAiCLUz8pakTP\n5au58WQk1/pbBxBKShBCiNOEEBuEEJuEENd7/H+xEGKXEGKN+lxq/Bc31rtfVVpchJkgqoGqIpXa\nGO5T3/sym1mKiagxa8/mlaPxfBLl1Ha++vTHNkkhYx6E2j8bocskkusMfqCLvmub6Xy57s/tz1eD\niEWA+vGqLRXih4iGgAATRIFmsVw0CH5mihG0EC3Qj2L6LaIHL0GULMxVCFEN4BYAnwSwHcBrQohH\npJRvuTa9T0p5tcchQlLKI0vVPgciAwgpDQKCNYgC8yBCe2nWGQ1STZixswpv50gw7f6JLAgiEQOq\n/bRNLiYmUUUmiHT7mDOujHkQOWgQLJhzVfd51u4LAOF9uQv4iMsHkbOTehhonUGaT6UQRGwYaOqg\n5UL9AfEcwly5aGMxwp7NSVk+obpmJdti+EQqFKXUII4BsElKuVlKGQFwL4CzS3i+/BEeREgEUOur\n0nHThZiYEgnSHCYeTr+LlUUbjwLvPJ7+fzP3IKt3UsdIcPLy9pVAf1fqdt3rdMnqRBSo8gHVvgwa\nRMh72Q02O3B9o1gE2PCE97Y8qzRnlxueyNy/zc8AO1SQgK8udf902LWBPoDWhnwBoLYp99ltPAI0\nthERZyIIKYF3Hkt1wm94wnkNNz1d+ryR6DAQaKXlYuVBZDOLDykNohg2f1OLzScHo8ss8e5hAYhH\ngQ1/zf24FYZSEsRkANuM39vVOjfOE0KsE0I8IISYaqyvE0KsFEK8LIT4jNcJhBCXqW1W7tpVQNXG\n8AARRE211iAKKbUR7qf9J82j38WaOb79KHDvhc5MVxPJ0NKgJggZT092iShQowgiHgXuWQa88F+p\n2/36BOAXi/V21X6teXi2I+i97IZbg9jwGHDPBd7vT3CbmHZvpG3fSmN9jIXJMb3iB/S7utZ5nEx4\n/Dr6AAZB1AN1zVqIZYvYMGlbjW2ZfRDbVwL3ft7phN/8DPXx2f9Dv0N7ydG+6g+5tSFXxAyCGM1M\natYgimJiKlCD2P2u97EYG/5Kz8vujbkfu4JQbif1owBmSCmPALAcwB3Gf9OllEsAfB7AzUKIQ9w7\nSylvlVIukVIumTBhQv6tiAwiiACZmKqKYGJiW+oEpUEUyzm5T/FtuhLGZv2jbDKeE3E9s46HyTno\nFoDuUtzxKFBdQ590JqZsNQhuFwt9PtewhxBmLYOFR59y/qe7FgPddA/5OnhpIOkwvA8YUsLKDHNt\nnEQlzHMJtY1FgJpaoLHdWzszzwlQgAGDayJxlFeoD4DU46AUSCRoLCSd1KOYSc33vRgakoMg8tAg\nTK3BiyD4PmUTgVfBKCVBdAIwNYIpal0SUso9UkrWPW8DcLTxX6f63gzgHwCOKllLw0QQdb7q4piY\neCbUOp1m6MXSIJho0r1gx6x/lE3NpLihQQztASBTHwZT1QYME5M/fe6Eozx2FlFMbuHtRSpJDUJ9\nj3Qt3KSci/kiMmQQkmFiamynPodyeH9FbFgTRKaJAmtaZvv2qNlpXbNqC/e9hL4MvieBIhFEPj6I\nYmgQhfogoiMQBN+nA7wYYCkJ4jUAs4UQM4UQfgDLADjsAUKIduPnWQDeVutbhRC1ank8gOMBuJ3b\nxYGUQGQQg7LO6aQuJIqJhVGgFWhqL94DzTPQdELR4YMwBm66QZyIaQ2CQwvdD0OnmsVWqyKGWZmY\nzIcrk4mJQyCHidD4ofMilYjLxMTX1EvbMP93tyMyMHJmbTSk25KMYmogIQ9k1gTciEfIvDUiQXCA\ngSEcu1TZbTe5lTJcloVpbSMFIxScB8E+iGwIopg+CGOSkY8PIjrCJIfv0wFeDLBkBCGljAG4GsCT\nIMF/v5RyvRDiJiHEWWqza4QQ64UQawFcA+Bitf5wACvV+hUAfugR/VQcRIIAJAYkm5iyKNa36Sng\n2Z8AO9ZTpuWOt4Dtr1OmLqAFeKA1vWDo2wq8tyK3tvJx3EIxkQDW3qvV3eiQc3b/xv3ahOHYL0a2\ndUDnbfCD9cGLwO5N2swhEyRYkyYmH/D+s8CWF5zH3PMesPFJWq5tTtUGgru1c4/t8wAJbhb+mWZs\n7ll0OrJMFxggEyPb1aNBQ5vhPIiAJoiBHmDvFvIRjISkBtFGfQzuBtbdn0pSEZcGEQ3pV52G9gLr\nH9J9zoWgcgULvJo60i5LYWKSElh7X6rjOp0G8e6TmQM9tr6c6gtw5EHk0YfIkNbcPDVaNVZHswjh\nttd08MQooaTVXKWUjwN43LXuO8byDQBu8NjvRQALS9m2JNRgHEjU6TwIUZ0+wUtK4MFLaTBvfRnY\n9S6ZkgZ3UEnoi/+iB3pdC9A8Fdj8j9TjvHQLORu/2QkIkV1b0wnFrS8BD10ONHJo4pDTxPTkN6lf\nSy937peIaQHNtnwWVA99FZh8NLDrbb1tJKhNTBPmEkE8fCVwrfGCmWd/Aqy9h5YbxqfOvlbdATz9\nXeD6rXSs1unAnk0kACKZCEKRX1KDYBNTBg2iuhYYP0dFMhkCOTII+Ou99wPUm8jCRIYOE1ObPvbd\n59PyjR7Ey5CSBEhNLdCirK13ngd0rwFaZwJTP6S35T5zP3s3ay22Zx3w5gPAzBN136XMftzkAjPv\nw1dXmJM6HtN9MLOZu1YBD10G1PiB+efo9ckwV4MgQn2U0XzCN4CPf9v7PH++AuhYDHz2t3qdw8SU\nRx5EdIjyV4b3eWvBPA5HU4N49Boaz5+7Y+Rti4RyO6nLj8Y24Du9eLjqZBXmWkWCfiDN+5T3vq8H\n8gcvUab09pUU9cAzO57hB1qAjiMpD8I9AwrupoGXzkTihpTphWK/cu0wgUSGUv0D/Z1IgRnmygTB\nAmFoD+0z0EOaAED9jkdJezj9x8DJ36aZtOnINuPHG8anCvug8nXw+6qnHEPf3WuM2XMGlT5bDWKg\nh+7tFc8DS77s/C+TE5SL8/F20SHSsoRwEkQ2YMFUUwu0q5Se7jXqP5dgcZuYuF9VPiJQgK41QO1L\n1+9CkdQgapUGUUCinEkKpolpnxqLbk0omShnmAG7VgOQmTPgg3tS72m0GAQxVi17kGSkDD6I8MCo\nVz62BAEAVdUYileRiQlQIYlp1PhO5bQ9+mI9s+ByBDyzC+2lh8sXoJkN4HxLF6Af8GztyaG9+oFz\nC4ekwOLyFqFU1dfrPF4aBGdhRwZJnY0O6XyO4T5NEEIAU5ak9s0UnvXjUoW9u9+T5lMCWucqgwQy\nmJj4uz8LHwQne3Efk8fKQBAO2/OgJgiA+t0wQZt+gBHKqav/qmspUZIjg7za7TYx8XUaO1OHXJsC\ntVSOavY51ASIJAoptWFeG6+aVO4+cJ9lQt8HHlvpCDERpwRI9zgzXxObr4nJ30D33ivZM1wGH0Qk\nWLhPKEdYglAIxxIUxQSQYEknuLtW0wN/5BdS/4sGyUQQ6tNRIG0LyWTljgZiAZGtPdl8mNzCxa2d\nRFUUU5XP2MbjPHGVSS2qnATBGgqfhwkitFebmAA9Kzb7Zl63uubUB5ePyRqEL0DH6VqV6hg2YTqp\nE3G9f1oNolvP+H0ugsg0CzPPHR6k36Y5qrHNmSCVyaFqahBCAB1GIJ5bC0zRINT/rTP1NmbodakI\nggWer47uTSECKdn/gIsg1Fg0x62U1GcmUb6uPLbSTQTYt5ZCECGdy5GXkzpE5OALjKBBjKLAjoYK\nr42VIyxBKAxH4y4NIs0D2LWahH77IhKU4+dQhAtjoIeEFg9Ofz0J2K7VwM63gXf/RutZsG17hRyQ\nbgR3A6vv1Ko2P0x1zbRvLAK8+hsSlikROyoPwu9qFwC89lvg6ZvISZ6IUVivSSSRoVShywSx/s/A\ne3+nmTRAJDj2ECqnDaSqwL4AnWftvXqdW4OoqQUmH0Vl0Tl8dHAH8I8fUoIY51okZ2wh4B8/INs2\nJ66ZDt+195HDfaBHO5U5lJfxxgN0LwAKFODgAsBVJmSQfvtMgmh3CpyISxvp7wLW/VG11SAIwEUQ\nrmucDHMdcP4/diY84TWBeftRnWS4cXl+Dk0WQDUB0rxMgdS31Xuspj2WERHlqUEYfQgP0D1lX81r\nv6H/eWylmwgkHdsugogEcw/V3buFxjhA98NXr8rleGm0Lid1PErPYzxKQSOv3Vac2f7Gp2isJhJ0\nL0a5ZLwlCIVwLEFOaoAIIrTXe+aw6x2gbQE99AvOBRZ8Flh4HjDtOPp/oJsSmfgdxgAJhs5VJJgf\nvpLW8cB+5sfAA19JLZ299h7g4at0dEavevDbF9G+m56ibN8PXkgVFhFlJnITxFAv8NjXged+Sg7y\nhDIXBQzTRyyUGufPBLFSOQFZcwCAyYu12Y1JrHUG+RY6jgIgyInoNp0MMkHUAZMWkDmG+/rOX4gE\n/v5dYNuryVBk+Bvp/2d/QmUvDvk4CRUz0e6hy4CX/5u2T6dBvP474O/fo+VHr9HZ1oBLgxigGWpt\nk1434wSdlQ2kahAv/hL406UkHJIEoc4/91PAuNnO6+A+LxPscB9pns1TnNvxNeCoM0ZkCPjjxcAr\nv6bfD18FvPBz5Iyo4YPwuXwQr94K/PGS7IUUC8/aRueLe3hCY05sOPlv3KH0/exP6P7z+nTBCLw+\nRVPdp17yhOz9BK/cStdweJ/SIJSZOJNPjMnnvRX0PG55Duh8HXjsGzqarxD8+as0UWKiHuW6UJYg\nAMTiCcQT0tAglO3aLXhjYXKWNamKIefeCpz0r8BZvwDOvoXW7d1CYa8dhhDtOIqE7nsryJQTCxth\np5KE3KDLKc7Clm2wXauJdCbOA0L7nGp6igbhQRCRAWf5AM40rqoh0nGc22WOmjhPL5/8b8DpPzT6\ntpjaMtCj23HWL4FLlwOLvwRceC/ZlHvW0X/8QHMQQLVf+wrYh2Jei4HuZChyUuADFDl1yMm0zOYH\ndpZ3vk7fjWl8EADNTKVU18/or9sHwc5uxvHXAN/eCXzxT/Tb7c9gk0h0yPBBqBySKUuAr61U0TFu\nHwRHMRlEGmjVmihjzAQyCbrNZDvepPsZ6qV+De0h23yuSJqYlAZhCqT+bjgCDLI9Vm0j7ceh4/0G\nQTBp8CRjxgl6/zcepO+2hRkIwpXxzhjooee0qiZ7DaK/k9rZvVb7IPz1aXxiRv0wwHgeu/QEq9Aa\nbLEIyYuBbt2GUc67sAQB0h4AUBQTYESruAiCf5sCg8HrNi6nmTk7pwGaZQN6FrB7Y2qtp5RzqQHH\nAqdzFRFNoJUefI4W4kgjE4moctI1ONe7ncmJOJmXzLYCToKoayZnM8O9LZtNulYb18fIf+S+swDw\n0iDM7d0Y6NYPo3nd68dq4cnH5G9OMOPtvQiif7tS3aPOB9kUNGEmCI/2sVZhCup4jIQLH8fMKTAR\naPHQIJgg2MSk/FhugvCPIS3CrbmY1zcySGSRT8kKJoSaOuWkNt9nwXkY2UZxGRoEoAlzoEeHknNY\nb9cquqamdhoLARDAzI/SmPfKTWKidSTGxYDgTrpvNXXZ+yB4/HauUsEJAWViypDZz9fHdLwnzagF\n5qvwM9Lf5SyjM4qwBAEgFKWBF2AndTIhynWDkwKwAynwN1A46AaV9mHamyfO17NIQNu/Hcd2PXTm\nYA2r2X/HYu3EY/vyzre935gV7tdRHAwWIpMW0PHjUfJBmG0FnATR2O6Mt3dv234EzWg7V+nrZQry\nMROBpikkAKIhQ0ioWWiN35twq/1kAx7o0UKY70vzNPpm05jbqc4zZ96eQ3l9rtyHDY/Rd3CnNvGZ\ns8XBHXQsr/bVqmtrCuHdG5wPMgvIGr9z30Crh5Oay6S4NAgz8gkgYVvr8eIinkiE+grLSDY1CLeD\n1ss0lM2xOOEsHiFtMGxUOuZx3rWaNNm6JucxJhymzWxeyZ5JH0RQayPBnTQB4yq62c66uV/bX6OJ\ng69BXQNXFFM8pid7TD78zAz0GFpygRnvpq/GfMf2KMISBIBgmIRDQ63KG2wyMmZfuw147Doqec0D\nyEtg8H4yQbZP03Zc4yehzOAwSXN2yMfu20bOLh5w3WuBR74GQGoNwjwGawW8ngVKPKK1FM4O523b\nj3SamCa7NYhOfSz37Jljwxn+BipKyBpEbZMWnozJygdjzppNDaK2UdvWGYFWus79XVoIs3N8/Gxn\nnx+9lma17lm5W4MwCUJU6WgkmQDW3El5LaYGwfkHTV4TAtVHR2kMQ0OLBDNoEK2ZfRCJBJFdnaFB\nJIMextD1SjFtGeGg7ozkdx6jLNyNyylDPhMcmdR1JJB2baBAAy/nMiO4B3jpv0lID++jqsB8LVmD\neOIGss0DeqKxbzuw4j8oSGHy4lStt2NxqqZoIkm0UgtPfpaaOnQfAGDL83T+Nx9MPY6ZZ7Tlefr2\nBbxNTOY9d2sQ5jjkdkgJvPgLZ76Q+Yy7wUEUvH8s5FweqVRMEWEJAkAwTBpEvV8J0roW0ga6VhM5\nvPYbGvDmwPPC7E+SOWbh51KzXBctA+acTsusQSw8H5hzGglpPvart5Kzq+8DymQOtFLG8vg5wNRj\ntNDjpCku6MZ+hIbx+pwTDqPZ+7K7SSDu2Ujta51BNupElI4/ZiIw/SPAPPW6jv4u6v+C86h9AHDM\nZcBxXu91Aj3sXavIdNYyLfX/SQsowdB0rPJskDWrJhcRBVpJU+OoMAA47HRq+ydupN8t04Hxh9Gx\n337EOSs3iYqd1L4AMPdM4JTv037sqwCIZJ75oZMg2GeTUYMwhAVraIAzF8V0agM0vtzCzoyeiga1\nBtE6nRz+h39an9f96tPhfuXgF0Qsw4YGISXw8NXk9H3ym8CT30rti4moQRD+BjrPHWdRpj5fGy/T\nyZsPAE/eQFFU7zwGLP8O8Kby0/D1W3sPkXLzVLoPALB5BfDMj0hzmHM6Ta7aFgIX3En+iPnn6EmP\nlx/CvI7J9hmm4Bq/JogV/0ETvuU3eh8nHqbng6+fv54mFW6zjoMg2AdhaFfJEHa1bvdG4G//BqxW\nZdoHd9Iz/nuP1+NICfz5SpIDpimPo9NkIvuXexUBJS21USkIRkiDGMMahBBAxyL1rgFJD3jXaqB5\nMgk0t12YccoT4MHCAAAY80lEQVT36OOFpZeTkP2ukWi15Mukav9snlPVZiz4LHDclc7juB3KAPkR\nZnyESnrUj9cz37pm4OvraXnCXDpvY7tT4LEz/ZLHiIjeepg0iEALcObP9HZn/MS7XwBpCGvupJnX\nEeen/s9ayC71HouagBFOqYR3Y5sSyAKAVNpLGwnxZFLdAmdZj9oxwFWv0DXt73KFoxp95DDXmjpg\n2V20vPMtXUaE0d9tCAOho6q8fBCs8Tg0iFX6LYJRU4NwEUSgNdVJHQ3pvocHtQ/C30AO/zcfBFb9\nXmkQrjfbda9BUsPsXquzjiMDNNEI9ZLgGuihNsUiqWYvRixEk4lqn/J39Y/sLwMME0uXXl7/EBH1\nhLl6u+vepevBhMhhrMvuBqYdS8tfVTN4JsWtr9D3sIcGYV7H6BCAsYamzz6IMPkv+FxeORW8T8di\nYNNyWvYxQbhMTKb2luKDMCY07orDSS1Pnd8MGmEM9epMedOU17vZOGco/f0rMqwGAWBQmZjqa6v1\nyo7F2l6+5MsUZrprAwmefGvgCEGDtu8D+s1Ew6aUREIPYl7vBuceANp0NGm+Dqs1NQgzv4Gdy41t\nToFnOgU5n4MJIluwuSAeTnViA1rjYmJsnaH/Y+HJbWKHOJuYBowoI6/rwdd0oMf54DsIgjUIw9TD\nba4y5kgDPVpwNRiRRp4+kho6LjtZY2Eyk0xbSr8dPgg3QbSQBmU6XaNDuu/D++hjTkR4ubYxVYNg\nwTPrYyTMOYAhPKC1mt73qa3xCLBzfWp/GLEwEaoQhnnLMCua2q4JU0DycjxMExDuP2dnA9pnx+M9\nndkWSPU1mTA1CDYF9XeTE7xhgvJBhEkYR4OUeBjuTw0r59m6aW5NlyhnkjPX7AruomszuENXRuYC\nlDyO+D6Z49RtLkqWkOmj68jj03yJ1ihmU1uCADCkTExJDQLQA6V5GpmOAODdJ7wd1LnANKWw6swC\nbs8mZ+JVOlMWt40dfZMNO60ZcVRtEMTko/S5uA3jZjudguzMBdJrSV6YtECTkduJDeiHn9+EZyZ/\nuQlizER1/hbqf2yY9gu0OtvnPr4ZPQI47xMTg+kLYCIbP0eXeA/vo3dHAzqG3tfgzIMwYb6jesd6\nMtlNP55+R4YyaxCA0+kaHdIkz+GWpoOal5M+CJdpq2U6ME5NHHrfp2+ZoEKOgHNcucu+mIiG9PXi\nc5r7TpznHcVkmlhMAulYrMfGpPnOfZra9bHHZCKIbHwQcJqYxkyiAAz2QXCfk6HRLoe3qUEwfPVE\nZNEh50uizOsRC6uwX0nPo4xrsy+3hdvNdcvMfpiaAW/PfR3o0s94r0EQoxjqak1M0E7qer9LgwBI\nsJpCL9NMJxvw/lU+/SA2tpN5hiNRph9PCXDpztWxGHjjj/Td84bTkWdqENVeGkS7FsZu57RZUiIX\ngqippeTBHetThQCfE9C+l7GzjDZ6EMTOt7QGAdB1yRQK29ROx/bVE0EO7fHWIEyCaFOk1jxFzdaU\nltL7Ps102bGaSWPkaKKu1WRjBsjUBwA73tBJa14+CIB8J0/fpN58F6Mch52g5EkgjQbBPghT4K8i\nf5V5XMb7z6a2u1NdT5mgtsVCtO/QbmcJdi/B3HEUOaxX/IB8aM//J405Niu583ImL9ZBD+6x0dhG\nZsfA2NRkRq/rFeqjKsg9bwBHX0K5NVtfJAIP9xsEYZRZqamlHJEXd9J1m/IhSvjctJxm7xMOo+MM\n7tL9Y/jr9aTkocuUVjJe1yDzNZBA53vPz2PfVj0On7qRzsHoWuUktc5VmtgBPQ6H+3QdtL5t2ucI\n0D16+1HSjuacmv66FQGWIODhgwBIcMw/hx6C+rHA/HPp5hZ6Q+acRmr11KV6XesMGhDvPklC7mPf\nIqc4J+S5cdjp5Ahcejmpnod+gh6wGSeQo++5n9J2polp0gI69yEn04N/+FnkTDfhsOHnqCkt/hKZ\n4NyzZYAEQJWPKt/6G+kabnichBQLoVkn0WfifPKl1LVoU1TvZsqaTofGdkpCDIyliKpAi9b6ACPM\n1dBAamqBD32FhNak+eRveOcvdD399cDsU0jQzDsLacEz+Zd/RRnfs0/R9vY195A5Z8oxqZFfTOJv\nPEhZ3YyOxSRUBrrpOCaBN00GZp9Kk4chVb1USpqR9m0FPnSpvpa9BkHsegdomEihnwBpxF1rqFQ9\nJF2TeJSOv/d9chBzJJFpZmxsJ/I7/NM0Tp/5Ic2U3RFB7OuYulT5xk6g67DufuDE65zbMulnIn+A\n7O21TdSHZ35HmtrwPir7UtdCGepr7nISBNewOuRkuj6RQRqjrGGv+D459w87A1h3Hx2jsUNrsABd\nm6lLKbt726tKW+ih4A2AAgh61tHH3wjM/wyVsweo33s20ZjqXqeP2bXaGbHXt8V1/QwNQkpqf2O7\n018WDVHAAT9LJYQlCJgahHE5hADO/3/69/m/Q1Fw5OfpY4JnLW8/SrOTGcfTJx3GziSnMqC/AXoX\nhZT0YHIZDUaNH/j8ffr3BR4vvjcJwstUlAnuktomqqpokO/bSjbpmScC17jMHBPnAl96mMoKACTs\n2HSViGYWIo1tNIPs76R2u/vmpUEAwOk/0su7NtDD3PsemSdOvC5VoLnhbyTB0/secOjH6fpyhMnQ\nbtJELl3u3V4gtYBjyzTtoHWjxg984X5a3ryCNA7TdGJqkaYGARDRvPsELc89g0Is+T0Nopq0ElGt\nciiMOmKmBnPk54GPq1e5LLsbuO1kCuJw+0P6VVb9omV6ewD4n0+nvw7ZaOWNbeTj4TL2nJB69i00\nQ19zl/ZBDHQD0z9Myyd8nT6Mba/SN7/TfO/7dC02/JUI3tQWfQ00nr6mot36tgE3L6B+N02hSST7\n1a56mcxkHIAxdhZwzq+A77fRuK9tJg2xczXlDgEUDODOVUm+KZFzedqUpmUQRH8ntV9Ukc/MHRpc\nRJTUByGEOE0IsUEIsUkIcb3H/xcLIXYJIdaoz6XGfxcJITaqz0WlbGcwEoe/ugr+mjK5ZNoXARCp\nGdj5QAgynwBOgsgG5gzbbX4qFCwERiIec/ZaUwtMmufc3/PYStvZt83buZ6OIBzHUAQUj6Qm1KVD\n7RgSiLs36vtW7dOOxXSOfj5X91oyE7CPI9sH3Yyg6loFQNAYSnc+bpt/DJGz+TpdGSdhtHcLabFD\ne7VJx/SBmGTRZhD3wvN1e6p8ZGKR8ZG1AkDfN3eIs+e2bTpLfeqxmig6jtL3KxoiB25ob/rx4jad\n8uw+EdV+Oobb59U8hXxTiShNdMzk17oWClxgc1GghfbnaxhoofvQtZraV9tMUYbuhEe3f6exI9UX\nyX4lmXBqJyVAySSiEKIawC0ATgcwD8CFQoh5HpveJ6U8Un1uU/uOBfDvAJYCOAbAvwshcjCK54Zg\nOOaMYBpt1I7RdspiCGZOyss167LKuAZmmeliIFuC4IeSH2QWbpmEiCkMvHwnXlFMbtQ26iiudM5w\nN/xjlPNQuqJfmOTSDNn6cSRMo0PUdjZLZSIwR1uNLO6u1ZQ4WNfkPB872QEl0GvoXOkmINGgioDa\namgQHk5yQBG38idMWaJDpdsW6PDlrAiCNYgsyYTDTeeeodrUTDP1JEEEnSGuXnBnppshrO6x6Z4o\nCGH4Jhfr+1Xl0+TOZlH2YbFwD7TS8Qe6yF8WaPYumeKOEGMNwsQHL+nlTAEHRUApp8zHANgkpdws\npYwAuBeAR2aIJ04FsFxK2Sul3AtgOYDTStROBMNxNPjLbG3jgZeraccLbUqFdb+nNxdUFXlopHOM\nu8HClR9kvh4ZTUwekWEmqqrIGesu+23CfGNctjN5M2PcvG9MMF5tSZ7LsL/z5IDfyTESzCzuzlV6\n7PgCOiKrearevqmDzB8cwTamjaKezHBjhhleW+3T2oGb7Li/HUdpgjDJJyuhz9cgSxMTY/apANT7\nNYTQwRWv3gasvD3zMTOFb7vJ02uiYPab/W2BFm2a4gg9zprmdgRa9Njf+jJdz9pGrUE8dSNw7xco\nHLfB8IOYQSWMzpUUfdfYQRrkP35EiYklQCml4mQA24zf20EagRvnCSFOBPAugH+WUm5Ls2+Kx1YI\ncRmAywBg2jSPDN4sEQzHnA7qcuDIC0k1NyN88sWiZeToPe6q3Pc9/lrvZLxCcfiZZC9tmZ55u2lL\ngcM+pcP75pxGn6leQ0ehdQY56oO7qbCbF5ZcAhz6Se//GIuW0fsAOMt3JMw+lWzKbQuc0WMssDJF\ngjUq23RjG726NbhLZ7KPBCam3RvJaWqS0+L/AWx/nUw/7ONobAeWXKxDSY+7kma9ooocrGvuch7f\nFKKBFoqYcgvWRcvI7zNhLgU79G0lJ3DXaiLYiXMxIibNo3s962Mjb5sUkoLMOMdeQZUFAD3T37le\n53ikC7Ko9qX6TQ7/NAllDia46FGK1PIiiAXnkhlt6lLyGQLO+3z8P9N9OeqLznYHWmniJqood6Ku\nhaKRwgPks3n+P8mvMe4QCsh44Wa1v6FBsFkPoOsuEzRB2LE+tSx8kVBuJ/WjAO6RUoaFEJcDuAPA\nydnuLKW8FcCtALBkyZK8C5QEI2U2MQFkG+aX0heKuibgwrvz2/eTNxWnDW5k27+Wac62N05yOte9\nUOMHvuhRX8eE6ZBOh4/+L/pki8PPpI8bviwIgk1mjR3Z9dEEz+o5hNXUyj79X/Q9vI9KX3DC2In/\norc5/lq9PLzPgyDM8NoW5dtx9WXasTrzuf0I4HO/p+XLVuTQj4bsxylfr4YJJORPM97h4eVry5h4\n1+okiCVfAQ4xSCrTWJ1wmG4zhy87THvjnPeSCaKuhSYOEw4nEgu0EjmE9moz0Xm3AdOPo4CJF24m\nIqtr0mRXP1aXWm/qoEnJO38h0uGs8yKjlCamTgCGnospal0SUso9Uko2lN8G4Ohs9y0m9gsNwuLA\nQZIgMpgzcjGvuMEaxPvPEgG0LUzdhs1QnDCW9lhNqbZ2rwS9dOay0UIu16u6NjM5c1/YyZyNOcwL\nbGLKdG2SJibVHnaEB1qceTSiSkc3JRNoXVFeZka76U+SicKDW9KglATxGoDZQoiZQgg/gGUAHjE3\nEEKYd+YsABzL9SSAU4QQrco5fYpaVxIEw3FnkpyFRSHwZ0MQOTho3WAHaO97lNnsZQqpqiZ/zkgC\n1fS9MLwS9HJJnCwFcnJoj1AOh+8LB3Pkm/xq+iDStsUwMQHaHBho1U7qzlVkMnLnnziqCwhnlYTG\nDqdpsRi+Sw+UbNospYwJIa4GCfZqALdLKdcLIW4CsFJK+QiAa4QQZwGIAegFcLHat1cI8V0QyQDA\nTVLK3pSTFAnBSEyX+rawKBQ8I884s8whxNMN8z0f7tBME7Vj0pdrcbclMqRLsLt9ENX+7CO7SgX2\nn4x0verHjdznQAvNxltnUCIhv68iVzBBZAqL5vbyNeWZfl0LheWyBjHHiMGpqaVjMkFU+4gkHImL\nbWRyap1BuTeNxiuOi4iSSkUp5eMAHnet+46xfAOAG9LsezuA20vZPkYwHCt/FJPFgYNsfBCzTiJH\ncj4zv7pmYNGF5BhedGH67ZZe7qykmg5HX6TLQsSGne2efy45cPMtUFks1PjJd3LoJ7z/P+X7JDSD\nu0Y2hx2xjK77pIXkU8i3b5zvkokgJs6n8806iX63LSRn/uxTgPV/0sUezXIcAPDhr1EWPuO4q4gM\n3vkL/WYSPP6fKGmyRLBSEZQoZzUIi6IhmcuRSYOYRE7JfCAEZemOhBO+kd3xjlAlV56/mbQIU8Ae\n8jGnA7ecyBRA8eE07yrxwuFnAlDBBXNOyb89XFU1k3blqwPO/bX+Xe2jd9gDwMa/6fVuredj33T+\nNgMLAG0WW3JJ9u3NAwd9NddoPIFILIEG64OwKBb8IyTK7a/wSpCzSA9OCszX/Gbm0eTqi6ptHHmb\nIuCgJwgu9W01CIuiwZ0NXilgx2muJVoOVvB7IrItzeKGWbSv0CrRJcJBTxCiCvjC0mmY15Gm5r+F\nRa5wZ4NXCgItVnvIBfzCJ/c72LNFIRrEKOGgnzY31fnw/XM84sgtLPLF3E9RjZ9K0yCOvpiS4iyy\nw0k3UA6Cu2x+tuBotLpm57tYMuGsX4zquDroCcLCouiYNA+YdGO5W5E7SvxugQMODeOc723PFcmX\nUuWgPSz+Uv7nywMHvYnJwsLCoizIhyBGGZYgLCwsLMoBNjFZgrCwsLCwcICd1PtpBBNgfRAWFhYW\n5UFtI72WdW5pKrEWA5YgLCwsLMqFbLPdywRrYrKwsLCw8IQlCAsLCwsLT1iCsLCwsLDwhCUICwsL\nCwtPWIKwsLCwsPCEJQgLCwsLC09YgrCwsLCw8IQlCAsLCwsLTwgpZbnbUBQIIXYB+KCAQ4wHsLtI\nzSk3DpS+HCj9AGxf9lfYvgDTpZQTvP44YAiiUAghVkopl5S7HcXAgdKXA6UfgO3L/grbl8ywJiYL\nCwsLC09YgrCwsLCw8IQlCI1by92AIuJA6cuB0g/A9mV/he1LBlgfhIWFhYWFJ6wGYWFhYWHhCUsQ\nFhYWFhaeOOgJQghxmhBigxBikxDi+nK3J1cIIbYIId4QQqwRQqxU68YKIZYLITaq79Zyt9MLQojb\nhRA7hRBvGus82y4IP1f3aZ0QYnH5Wp6KNH25UQjRqe7NGiHEGcZ/N6i+bBBCnFqeVntDCDFVCLFC\nCPGWEGK9EOJatb6i7k2GflTcfRFC1AkhXhVCrFV9+d9q/UwhxCuqzfcJIfxqfa36vUn9PyOvE0sp\nD9oPgGoA7wGYBcAPYC2AeeVuV4592AJgvGvdjwFcr5avB/CjcrczTdtPBLAYwJsjtR3AGQD+CkAA\nOBbAK+VufxZ9uRHAdR7bzlNjrRbATDUGq8vdB6N97QAWq+VGAO+qNlfUvcnQj4q7L+rajlHLPgCv\nqGt9P4Blav2vAFyhlq8E8Cu1vAzAffmc92DXII4BsElKuVlKGQFwL4Czy9ymYuBsAHeo5TsAfKaM\nbUkLKeWzAHpdq9O1/WwAv5eElwG0CCHaR6elIyNNX9LhbAD3SinDUsr3AWwCjcX9AlLKbinlKrU8\nAOBtAJNRYfcmQz/SYb+9L+raDqqfPvWRAE4G8IBa774nfK8eAPBxIYTI9bwHO0FMBrDN+L0dmQfQ\n/ggJ4G9CiNeFEJepdZOklN1quQfApPI0LS+ka3ul3qurldnldsPUVzF9UaaJo0Az1oq9N65+ABV4\nX4QQ1UKINQB2AlgO0nD6pJQxtYnZ3mRf1P/7AIzL9ZwHO0EcCPiIlHIxgNMBXCWEONH8U5KOWZGx\nzJXcdoX/C+AQAEcC6Abw0/I2JzcIIcYAeBDAP0kp+83/KuneePSjIu+LlDIupTwSwBSQZjO31Oc8\n2AmiE8BU4/cUta5iIKXsVN87ATwEGjg7WMVX3zvL18Kcka7tFXevpJQ71EOdAPAbaHPFft8XIYQP\nJFTvklL+Sa2uuHvj1Y9Kvi8AIKXsA7ACwHEgc16N+stsb7Iv6v9mAHtyPdfBThCvAZitIgH8IGfO\nI2VuU9YQQjQIIRp5GcApAN4E9eEitdlFAB4uTwvzQrq2PwLgSypi5lgA+wxzx34Jlx3+HNC9Aagv\ny1SkyUwAswG8OtrtSwdlq/4tgLellD8z/qqoe5OuH5V4X4QQE4QQLWo5AOCTIJ/KCgCfVZu57wnf\nq88C+LvS+nJDub3z5f6AIjDeBdnzvlXu9uTY9lmgqIu1ANZz+0G2xqcBbATwFICx5W5rmvbfA1Lx\noyD76VfStR0UxXGLuk9vAFhS7vZn0Zc/qLauUw9su7H9t1RfNgA4vdztd/XlIyDz0ToAa9TnjEq7\nNxn6UXH3BcARAFarNr8J4Dtq/SwQiW0C8EcAtWp9nfq9Sf0/K5/z2lIbFhYWFhaeONhNTBYWFhYW\naWAJwsLCwsLCE5YgLCwsLCw8YQnCwsLCwsITliAsLCwsLDxhCcLCIgcIIeJGFdA1oogVgIUQM8xq\nsBYW5UbNyJtYWFgYCEkqd2BhccDDahAWFkWAoPdy/FjQuzleFUIcqtbPEEL8XRWGe1oIMU2tnySE\neEjV918rhPiwOlS1EOI3qub/31TWrIVFWWAJwsIiNwRcJqYLjP/2SSkXAvglgJvVul8AuENKeQSA\nuwD8XK3/OYBnpJSLQO+RWK/WzwZwi5RyPoA+AOeVuD8WFmlhM6ktLHKAEGJQSjnGY/0WACdLKTer\nAnE9UspxQojdoFIOUbW+W0o5XgixC8AUKWXYOMYMAMullLPV738F4JNSfq/0PbOwSIXVICwsigeZ\nZjkXhI3lOKyf0KKMsARhYVE8XGB8v6SWXwRVCQaALwB4Ti0/DeAKIPkimObRaqSFRbawsxMLi9wQ\nUG/1YjwhpeRQ11YhxDqQFnChWvc1AL8TQvwLgF0ALlHrrwVwqxDiKyBN4QpQNVgLi/0G1gdhYVEE\nKB/EEinl7nK3xcKiWLAmJgsLCwsLT1gNwsLCwsLCE1aDsLCwsLDwhCUICwsLCwtPWIKwsLCwsPCE\nJQgLCwsLC09YgrCwsLCw8MT/ByAgL4hl80P6AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "# Evaluate on test data\n",
            "40/40 [==============================] - 0s 3ms/sample - loss: 0.8426 - acc: 0.6750\n",
            "test loss, test acc: [0.8426251159515232, 0.675]\n",
            "[[0.61404462]\n",
            " [0.44008427]\n",
            " [0.88235981]\n",
            " [0.46849946]\n",
            " [0.50512674]\n",
            " [0.84262512]\n",
            " [0.83273916]\n",
            " [0.71972434]\n",
            " [0.72798631]\n",
            " [0.79364807]]\n",
            "[[0.64999998]\n",
            " [0.85000002]\n",
            " [0.625     ]\n",
            " [0.72500002]\n",
            " [0.75      ]\n",
            " [0.67500001]\n",
            " [0.60000002]\n",
            " [0.625     ]\n",
            " [0.55000001]\n",
            " [0.5       ]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kd6FTGs7vBI3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "df_accl_all = pd.DataFrame({'Class1vs2': acc_all[:, 0]})\n",
        "df_accl_all.to_csv (r'EEG_Deep/df_accl_Cross_Patient_8_24_2560:4096.csv', index = None, header=True) #Don't forget to add '.csv' at the end of the path\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}