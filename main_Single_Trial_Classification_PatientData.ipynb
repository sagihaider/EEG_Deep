{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "main_Single_Trial_Classification_PatientData",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sagihaider/EEG_Deep/blob/master/main_Single_Trial_Classification_PatientData.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X110PE9MjrlE",
        "colab_type": "code",
        "outputId": "53a19c25-523f-4c98-aa46-72769883acb2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "!git clone https://github.com/sagihaider/EEG_Deep.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'EEG_Deep'...\n",
            "remote: Enumerating objects: 3, done.\u001b[K\n",
            "remote: Counting objects: 100% (3/3), done.\u001b[K\n",
            "remote: Compressing objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 156 (delta 0), reused 0 (delta 0), pack-reused 153\u001b[K\n",
            "Receiving objects: 100% (156/156), 856.35 MiB | 15.86 MiB/s, done.\n",
            "Resolving deltas: 100% (53/53), done.\n",
            "Checking out files: 100% (57/57), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ioLzMoiS60Qf",
        "colab_type": "code",
        "outputId": "7346b418-04f3-40a7-c6c4-5fa155368bbb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 63
        }
      },
      "source": [
        "import scipy.io as spio\n",
        "import numpy as np\n",
        "from importlib.machinery import SourceFileLoader\n",
        "\n",
        "# EEGNet-specific imports\n",
        "from EEG_Deep.EEGModels import EEGNet, ShallowConvNet, DeepConvNet\n",
        "from tensorflow.keras import utils as np_utils\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras import backend as K\n",
        "K.set_image_data_format('channels_first')\n",
        "\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# tools for plotting confusion matrices\n",
        "from matplotlib import pyplot as plt\n",
        "from scipy.signal import butter, lfilter"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mo80Jg_Pn5lP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Band-pass Filter\n",
        "def butter_bandpass_filter(data, lowcut, highcut, fs, order=4):\n",
        "    nyq = 0.5 * fs\n",
        "    low = lowcut / nyq\n",
        "    high = highcut / nyq\n",
        "    b, a = butter(order, [low, high], btype='band')\n",
        "    y = lfilter(b, a, data)\n",
        "    return y\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZjtnE_6RSpum",
        "colab_type": "code",
        "outputId": "0eb0c326-dcb8-44e8-e004-abec4631815f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from numpy import zeros\n",
        "K.clear_session()\n",
        "cols = 6\n",
        "rows = 9\n",
        "acc_all = zeros([rows, cols])\n",
        "loss_all = zeros([rows, cols])\n",
        "X_tr_c12 = np.empty([80, 12, 4096])\n",
        "X_ts_c12 = np.empty([80, 12, 4096])\n",
        "result=[]\n",
        "\n",
        "from itertools import combinations \n",
        "comb = combinations([1, 2], 2) \n",
        "  # Print the obtained combinations \n",
        "bincomb=[]\n",
        "for i in list(comb): \n",
        "    bincomb.append(i)\n",
        "\n",
        "for x in range(1,10):\n",
        "  fName = 'EEG_Deep/Data2A/parsed_P0' + str(x) + 'T.mat'  # Load Data\n",
        "  print(fName)\n",
        "  mat = spio.loadmat(fName)\n",
        "  r_X_tr = mat['RawEEGData']\n",
        "  r_y_tr = mat['Labels']\n",
        "\n",
        "  ### Filter Data ###\n",
        "  for t in range(r_X_tr.shape[0]):\n",
        "    tril = r_X_tr[t,:,:]\n",
        "    # tril = tril.transpose()\n",
        "    tril_filtered = butter_bandpass_filter(tril, \n",
        "                                              lowcut=4, \n",
        "                                              highcut=40, \n",
        "                                              fs=512,\n",
        "                                              order=4)\n",
        "    # tril_filtered = tril_filtered.transpose()\n",
        "    X_tr_c12[t,:,:] = tril_filtered\n",
        "\n",
        "  print(\"Filtering of Training Data Finished\")\n",
        "  ## Test Data Load \n",
        "\n",
        "  fName = 'EEG_Deep/Data2A/parsed_P0' + str(x) + 'E.mat'  # Load Data\n",
        "  print(fName)\n",
        "  mat = spio.loadmat(fName)\n",
        "  r_X_ts = mat['RawEEGData']\n",
        "  r_y_ts = mat['Labels']\n",
        "  for t in range(r_X_ts.shape[0]):\n",
        "    tril = r_X_ts[t,:,:]\n",
        "    # tril = tril.transpose()\n",
        "    tril_filtered = butter_bandpass_filter(tril, \n",
        "                                              lowcut=4, \n",
        "                                              highcut=40, \n",
        "                                              fs=512,\n",
        "                                              order=4)\n",
        "    # tril_filtered = tril_filtered.transpose()\n",
        "    X_ts_c12[t,:,:] = tril_filtered\n",
        "  \n",
        "  print(\"Filtering of Testing Data Finished\")    \n",
        "\n",
        "  for k, com in enumerate(bincomb):\n",
        "      print(com)\n",
        "      # # Find labels to specific class\n",
        "      print(\"Finding labels in training data\")\n",
        "      class1indx = list(np.where(r_y_tr == com[0]))\n",
        "      class2indx = list(np.where(r_y_tr == com[1]))\n",
        "      c1=list(class1indx[0])\n",
        "      c2=list(class2indx[0])\n",
        "      y_tr_c12 = c1 + c2\n",
        "      y_tr_c12.sort()\n",
        "      # print(y_tr_c12)\n",
        "      x_tr_12 = X_tr_c12[y_tr_c12,:,:]\n",
        "      y_tr_12 = r_y_tr[y_tr_c12]\n",
        "      # print(np.shape(x_tr_12))\n",
        "      # print(np.shape(y_tr_12))\n",
        "      # # Find labels to specific class\n",
        "      print(\"Finding labels in testing data\")\n",
        "      class1indx = list(np.where(r_y_ts == com[0]))\n",
        "      class2indx = list(np.where(r_y_ts == com[1]))\n",
        "      c1=list(class1indx[0])\n",
        "      c2=list(class2indx[0])\n",
        "      y_ts_c12 = c1 + c2\n",
        "      y_ts_c12.sort()\n",
        "      # print(y_ts_c12)\n",
        "      x_ts_12 = X_ts_c12[y_ts_c12,:,:]\n",
        "      y_ts_12 = r_y_ts[y_ts_c12]\n",
        "      # print(np.shape(x_ts_12))\n",
        "      # print(np.shape(y_ts_12))\n",
        "      del class1indx, class2indx, c1, c2\n",
        "      # split data of each subject in training and validation\n",
        "      X_train = x_tr_12[0:60,:,1792:3328]\n",
        "      Y_train = y_tr_12[0:60].ravel()\n",
        "      X_val   = x_tr_12[60:,:,1792:3328]\n",
        "      Y_val   = y_tr_12[60:].ravel()\n",
        "      print(Y_val)\n",
        "      print(np.shape(X_train))\n",
        "      print(np.shape(Y_train))\n",
        "      print(np.shape(X_val))\n",
        "      print(np.shape(Y_val))\n",
        "  \n",
        "      # convert labels to one-hot encodings.\n",
        "      Y_train      = np_utils.to_categorical(Y_train-1, num_classes=4)\n",
        "      Y_val       = np_utils.to_categorical(Y_val-1, num_classes=4)\n",
        "      print(Y_val)\n",
        "\n",
        "      kernels, chans, samples = 1, 12, 1536\n",
        "      # convert data to NCHW (trials, kernels, channels, samples) format. Data \n",
        "      # contains 22 channels and 500 time-points. Set the number of kernels to 1.\n",
        "      X_train = X_train.reshape(X_train.shape[0], kernels, chans, samples)\n",
        "      X_val   = X_val.reshape(X_val.shape[0], kernels, chans, samples)\n",
        "   \n",
        "      print('X_train shape:', X_train.shape)\n",
        "      print(X_train.shape[0], 'train samples')\n",
        "      print(X_val.shape[0], 'val samples')\n",
        "\n",
        "      X_test      = x_ts_12[:,:,1792:3328]\n",
        "      Y_test      = y_ts_12[:]\n",
        "      print(np.shape(X_test))\n",
        "      print(np.shape(Y_test))\n",
        "\n",
        "      #convert labels to one-hot encodings.\n",
        "      Y_test      = np_utils.to_categorical(Y_test-1, num_classes=4)\n",
        "\n",
        "      # convert data to NCHW (trials, kernels, channels, samples) format. Data \n",
        "      # contains 22 channels and 500 time-points. Set the number of kernels to 1.\n",
        "      X_test      = X_test.reshape(X_test.shape[0], kernels, chans, samples)\n",
        "\n",
        "      print('X_train shape:', X_test.shape)\n",
        "      print(X_test.shape[0], 'train samples')\n",
        "\n",
        "      # configure the EEGNet-8,2,16 model with kernel length of 32 samples (other \n",
        "      # model configurations may do better, but this is a good starting point)\n",
        "      model = EEGNet(nb_classes = 4, Chans = 12, Samples = 1536,\n",
        "                     dropoutRate = 0.5, kernLength = 25, F1 = 8, \n",
        "                     D = 2, F2 = 16, norm_rate = 0.25, dropoutType = 'Dropout')\n",
        "      \n",
        "      # compile the model and set the optimizers\n",
        "      model.compile(loss='categorical_crossentropy', optimizer='adam', \n",
        "                    metrics = ['accuracy'])\n",
        "\n",
        "      # count number of parameters in the model\n",
        "      numParams    = model.count_params() \n",
        "\n",
        "      # set a valid path for your system to record model checkpoints\n",
        "      checkpointer = ModelCheckpoint(filepath='/tmp/checkpoint.h5', verbose=1,\n",
        "                                     save_best_only=True)\n",
        "  \n",
        "      # the syntax is {class_1:weight_1, class_2:weight_2,...}. Here just setting\n",
        "      # the weights all to be 1\n",
        "      class_weights = {0:1, 1:1, 2:1, 3:1}\n",
        "\n",
        "      history = model.fit(X_train, Y_train, batch_size = 16, epochs = 300, \n",
        "                      verbose = 2, validation_data=(X_val, Y_val),\n",
        "                      callbacks=[checkpointer], class_weight = class_weights)\n",
        "  \n",
        "      # Plot training & validation accuracy values\n",
        "      plt.plot(history.history['acc'])\n",
        "      plt.plot(history.history['val_acc'])\n",
        "      plt.title('Model accuracy')\n",
        "      plt.ylabel('Accuracy')\n",
        "      plt.xlabel('Epoch')\n",
        "      plt.legend(['Train', 'Test'], loc='upper left')\n",
        "      plt.show()\n",
        "      figName = 'Accuracy_A0' + str(x) + '.png'  \n",
        "      plt.savefig(figName)\n",
        "\n",
        "      print('\\n# Evaluate on test data')\n",
        "      results = model.evaluate(X_test, Y_test, batch_size=1)\n",
        "      print('test loss, test acc:', results)\n",
        "\n",
        "      loss_all[x - 1, k-1] = results[0]\n",
        "      acc_all[x - 1, k-1] = results[1]\n",
        "\n",
        "      from keras import backend as K \n",
        "      # Do some code, e.g. train and save model\n",
        "      K.clear_session()\n",
        "\n",
        "\n",
        "print(loss_all)\n",
        "print(acc_all)\n",
        "\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "EEG_Deep/Data2A/parsed_P01T.mat\n",
            "Filtering of Training Data Finished\n",
            "EEG_Deep/Data2A/parsed_P01E.mat\n",
            "Filtering of Testing Data Finished\n",
            "(1, 2)\n",
            "Finding labels in training data\n",
            "Finding labels in testing data\n",
            "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
            "(60, 12, 1536)\n",
            "(60,)\n",
            "(20, 12, 1536)\n",
            "(20,)\n",
            "[[1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]]\n",
            "X_train shape: (60, 1, 12, 1536)\n",
            "60 train samples\n",
            "20 val samples\n",
            "(40, 12, 1536)\n",
            "(40, 1)\n",
            "X_train shape: (40, 1, 12, 1536)\n",
            "40 train samples\n",
            "Train on 60 samples, validate on 20 samples\n",
            "Epoch 1/300\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 1.33718, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 1s - loss: 1.2301 - acc: 0.4833 - val_loss: 1.3372 - val_acc: 0.0500\n",
            "Epoch 2/300\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 1.33718\n",
            "60/60 - 0s - loss: 1.0200 - acc: 0.6500 - val_loss: 1.3396 - val_acc: 0.0000e+00\n",
            "Epoch 3/300\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 1.33718\n",
            "60/60 - 0s - loss: 0.8647 - acc: 0.6667 - val_loss: 1.3409 - val_acc: 0.0000e+00\n",
            "Epoch 4/300\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 1.33718\n",
            "60/60 - 0s - loss: 0.7785 - acc: 0.6667 - val_loss: 1.3485 - val_acc: 0.0000e+00\n",
            "Epoch 5/300\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 1.33718\n",
            "60/60 - 0s - loss: 0.7420 - acc: 0.6667 - val_loss: 1.3503 - val_acc: 0.0000e+00\n",
            "Epoch 6/300\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 1.33718\n",
            "60/60 - 0s - loss: 0.6785 - acc: 0.6667 - val_loss: 1.3469 - val_acc: 0.0000e+00\n",
            "Epoch 7/300\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 1.33718\n",
            "60/60 - 0s - loss: 0.6578 - acc: 0.6667 - val_loss: 1.3422 - val_acc: 0.0000e+00\n",
            "Epoch 8/300\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 1.33718\n",
            "60/60 - 0s - loss: 0.6361 - acc: 0.6667 - val_loss: 1.3405 - val_acc: 0.0000e+00\n",
            "Epoch 9/300\n",
            "\n",
            "Epoch 00009: val_loss improved from 1.33718 to 1.33651, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6115 - acc: 0.6833 - val_loss: 1.3365 - val_acc: 0.0000e+00\n",
            "Epoch 10/300\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.5974 - acc: 0.7000 - val_loss: 1.3445 - val_acc: 0.0000e+00\n",
            "Epoch 11/300\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.5834 - acc: 0.7500 - val_loss: 1.3524 - val_acc: 0.0000e+00\n",
            "Epoch 12/300\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.5544 - acc: 0.7333 - val_loss: 1.3607 - val_acc: 0.0000e+00\n",
            "Epoch 13/300\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.5382 - acc: 0.7500 - val_loss: 1.3620 - val_acc: 0.0000e+00\n",
            "Epoch 14/300\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.5281 - acc: 0.8167 - val_loss: 1.3840 - val_acc: 0.0000e+00\n",
            "Epoch 15/300\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.5132 - acc: 0.8333 - val_loss: 1.4069 - val_acc: 0.0000e+00\n",
            "Epoch 16/300\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.4980 - acc: 0.9000 - val_loss: 1.4328 - val_acc: 0.0000e+00\n",
            "Epoch 17/300\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.4671 - acc: 0.8833 - val_loss: 1.4588 - val_acc: 0.0000e+00\n",
            "Epoch 18/300\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.4782 - acc: 0.9167 - val_loss: 1.4891 - val_acc: 0.0000e+00\n",
            "Epoch 19/300\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.4580 - acc: 0.8333 - val_loss: 1.5345 - val_acc: 0.0000e+00\n",
            "Epoch 20/300\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.4466 - acc: 0.9167 - val_loss: 1.5899 - val_acc: 0.0000e+00\n",
            "Epoch 21/300\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.4136 - acc: 0.9333 - val_loss: 1.6508 - val_acc: 0.0000e+00\n",
            "Epoch 22/300\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.4432 - acc: 0.8667 - val_loss: 1.6948 - val_acc: 0.0000e+00\n",
            "Epoch 23/300\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.4142 - acc: 0.8833 - val_loss: 1.7528 - val_acc: 0.0000e+00\n",
            "Epoch 24/300\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.3930 - acc: 0.9167 - val_loss: 1.8004 - val_acc: 0.0000e+00\n",
            "Epoch 25/300\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.3943 - acc: 0.9167 - val_loss: 1.8493 - val_acc: 0.0500\n",
            "Epoch 26/300\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.3852 - acc: 0.9167 - val_loss: 1.9060 - val_acc: 0.1000\n",
            "Epoch 27/300\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.3929 - acc: 0.9833 - val_loss: 1.9678 - val_acc: 0.1000\n",
            "Epoch 28/300\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.3679 - acc: 0.9500 - val_loss: 2.0773 - val_acc: 0.1000\n",
            "Epoch 29/300\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.3779 - acc: 0.9167 - val_loss: 2.1943 - val_acc: 0.1000\n",
            "Epoch 30/300\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.3941 - acc: 0.9000 - val_loss: 2.2843 - val_acc: 0.1000\n",
            "Epoch 31/300\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.3591 - acc: 0.9167 - val_loss: 2.3242 - val_acc: 0.1000\n",
            "Epoch 32/300\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.3254 - acc: 0.9833 - val_loss: 2.4098 - val_acc: 0.1000\n",
            "Epoch 33/300\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.3205 - acc: 0.9500 - val_loss: 2.4771 - val_acc: 0.1000\n",
            "Epoch 34/300\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.3154 - acc: 0.9500 - val_loss: 2.5608 - val_acc: 0.1000\n",
            "Epoch 35/300\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.3140 - acc: 0.9667 - val_loss: 2.6304 - val_acc: 0.1000\n",
            "Epoch 36/300\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.3113 - acc: 0.9500 - val_loss: 2.7020 - val_acc: 0.1000\n",
            "Epoch 37/300\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.2953 - acc: 0.9833 - val_loss: 2.7800 - val_acc: 0.1000\n",
            "Epoch 38/300\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.3087 - acc: 0.9833 - val_loss: 2.8581 - val_acc: 0.1000\n",
            "Epoch 39/300\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.2872 - acc: 0.9833 - val_loss: 2.9899 - val_acc: 0.0500\n",
            "Epoch 40/300\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.2760 - acc: 0.9833 - val_loss: 3.0513 - val_acc: 0.0500\n",
            "Epoch 41/300\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.2779 - acc: 0.9667 - val_loss: 3.0719 - val_acc: 0.0500\n",
            "Epoch 42/300\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.3023 - acc: 0.9667 - val_loss: 3.0985 - val_acc: 0.1000\n",
            "Epoch 43/300\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.2832 - acc: 0.9833 - val_loss: 3.1162 - val_acc: 0.1000\n",
            "Epoch 44/300\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.2843 - acc: 0.9500 - val_loss: 3.0938 - val_acc: 0.1000\n",
            "Epoch 45/300\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.2494 - acc: 0.9833 - val_loss: 3.1050 - val_acc: 0.1000\n",
            "Epoch 46/300\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.2746 - acc: 0.9667 - val_loss: 3.1136 - val_acc: 0.1500\n",
            "Epoch 47/300\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.2757 - acc: 0.9833 - val_loss: 3.1863 - val_acc: 0.1500\n",
            "Epoch 48/300\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.2975 - acc: 0.9167 - val_loss: 3.2505 - val_acc: 0.1500\n",
            "Epoch 49/300\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.2475 - acc: 0.9833 - val_loss: 3.2327 - val_acc: 0.1500\n",
            "Epoch 50/300\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.2691 - acc: 0.9667 - val_loss: 3.2676 - val_acc: 0.1500\n",
            "Epoch 51/300\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.2627 - acc: 0.9833 - val_loss: 3.2956 - val_acc: 0.2000\n",
            "Epoch 52/300\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.2791 - acc: 0.9500 - val_loss: 3.3726 - val_acc: 0.2000\n",
            "Epoch 53/300\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.2549 - acc: 0.9667 - val_loss: 3.4465 - val_acc: 0.2000\n",
            "Epoch 54/300\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.2478 - acc: 0.9833 - val_loss: 3.5467 - val_acc: 0.2000\n",
            "Epoch 55/300\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.2351 - acc: 0.9667 - val_loss: 3.6200 - val_acc: 0.2000\n",
            "Epoch 56/300\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.2374 - acc: 0.9833 - val_loss: 3.7249 - val_acc: 0.2000\n",
            "Epoch 57/300\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.2069 - acc: 1.0000 - val_loss: 3.8206 - val_acc: 0.2000\n",
            "Epoch 58/300\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.2606 - acc: 0.9333 - val_loss: 3.8040 - val_acc: 0.2500\n",
            "Epoch 59/300\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.2712 - acc: 0.9333 - val_loss: 3.7631 - val_acc: 0.2500\n",
            "Epoch 60/300\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.2121 - acc: 1.0000 - val_loss: 3.7449 - val_acc: 0.2500\n",
            "Epoch 61/300\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.2605 - acc: 0.9833 - val_loss: 3.7478 - val_acc: 0.2500\n",
            "Epoch 62/300\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.2326 - acc: 0.9667 - val_loss: 3.7010 - val_acc: 0.2500\n",
            "Epoch 63/300\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.2307 - acc: 1.0000 - val_loss: 3.7197 - val_acc: 0.2500\n",
            "Epoch 64/300\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.2194 - acc: 0.9833 - val_loss: 3.7597 - val_acc: 0.2500\n",
            "Epoch 65/300\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.2211 - acc: 0.9833 - val_loss: 3.7932 - val_acc: 0.2500\n",
            "Epoch 66/300\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.2199 - acc: 0.9833 - val_loss: 3.7642 - val_acc: 0.3000\n",
            "Epoch 67/300\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.2214 - acc: 0.9833 - val_loss: 3.7575 - val_acc: 0.3000\n",
            "Epoch 68/300\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.2021 - acc: 0.9667 - val_loss: 3.6953 - val_acc: 0.3500\n",
            "Epoch 69/300\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.2368 - acc: 0.9667 - val_loss: 3.6307 - val_acc: 0.3500\n",
            "Epoch 70/300\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.2079 - acc: 1.0000 - val_loss: 3.6582 - val_acc: 0.3500\n",
            "Epoch 71/300\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.1999 - acc: 0.9667 - val_loss: 3.7263 - val_acc: 0.3500\n",
            "Epoch 72/300\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.1895 - acc: 0.9667 - val_loss: 3.7362 - val_acc: 0.3500\n",
            "Epoch 73/300\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.2012 - acc: 0.9667 - val_loss: 3.7472 - val_acc: 0.3500\n",
            "Epoch 74/300\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.1855 - acc: 1.0000 - val_loss: 3.7459 - val_acc: 0.3500\n",
            "Epoch 75/300\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.2189 - acc: 1.0000 - val_loss: 3.7344 - val_acc: 0.4000\n",
            "Epoch 76/300\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.1952 - acc: 1.0000 - val_loss: 3.8042 - val_acc: 0.4000\n",
            "Epoch 77/300\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.1787 - acc: 1.0000 - val_loss: 3.8622 - val_acc: 0.3500\n",
            "Epoch 78/300\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.1774 - acc: 1.0000 - val_loss: 3.8785 - val_acc: 0.3500\n",
            "Epoch 79/300\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.2064 - acc: 0.9833 - val_loss: 3.8734 - val_acc: 0.3500\n",
            "Epoch 80/300\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.2050 - acc: 0.9833 - val_loss: 3.8552 - val_acc: 0.3500\n",
            "Epoch 81/300\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.2039 - acc: 0.9667 - val_loss: 3.8069 - val_acc: 0.3500\n",
            "Epoch 82/300\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.1699 - acc: 0.9667 - val_loss: 3.7612 - val_acc: 0.4000\n",
            "Epoch 83/300\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.1922 - acc: 0.9667 - val_loss: 3.7693 - val_acc: 0.4000\n",
            "Epoch 84/300\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.1800 - acc: 0.9500 - val_loss: 3.7700 - val_acc: 0.4000\n",
            "Epoch 85/300\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.1851 - acc: 0.9667 - val_loss: 3.7500 - val_acc: 0.4000\n",
            "Epoch 86/300\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.1915 - acc: 0.9667 - val_loss: 3.7620 - val_acc: 0.4000\n",
            "Epoch 87/300\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.1693 - acc: 0.9833 - val_loss: 3.7843 - val_acc: 0.4000\n",
            "Epoch 88/300\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.1945 - acc: 1.0000 - val_loss: 3.8345 - val_acc: 0.4000\n",
            "Epoch 89/300\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.2185 - acc: 0.9833 - val_loss: 3.9267 - val_acc: 0.4000\n",
            "Epoch 90/300\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.1974 - acc: 0.9667 - val_loss: 3.9427 - val_acc: 0.4000\n",
            "Epoch 91/300\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.1710 - acc: 1.0000 - val_loss: 3.9191 - val_acc: 0.4000\n",
            "Epoch 92/300\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.1753 - acc: 1.0000 - val_loss: 3.9068 - val_acc: 0.4000\n",
            "Epoch 93/300\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.2450 - acc: 0.9167 - val_loss: 3.8335 - val_acc: 0.4000\n",
            "Epoch 94/300\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.1820 - acc: 0.9833 - val_loss: 3.7976 - val_acc: 0.4000\n",
            "Epoch 95/300\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.1597 - acc: 0.9833 - val_loss: 3.7657 - val_acc: 0.4000\n",
            "Epoch 96/300\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.1448 - acc: 1.0000 - val_loss: 3.8049 - val_acc: 0.3500\n",
            "Epoch 97/300\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.1783 - acc: 0.9833 - val_loss: 3.7706 - val_acc: 0.4000\n",
            "Epoch 98/300\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.1685 - acc: 1.0000 - val_loss: 3.7495 - val_acc: 0.4000\n",
            "Epoch 99/300\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.1645 - acc: 1.0000 - val_loss: 3.7265 - val_acc: 0.4500\n",
            "Epoch 100/300\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.1817 - acc: 1.0000 - val_loss: 3.7062 - val_acc: 0.4500\n",
            "Epoch 101/300\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.1465 - acc: 1.0000 - val_loss: 3.7365 - val_acc: 0.5000\n",
            "Epoch 102/300\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.2091 - acc: 0.9833 - val_loss: 3.7166 - val_acc: 0.4500\n",
            "Epoch 103/300\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.1575 - acc: 0.9833 - val_loss: 3.6902 - val_acc: 0.4500\n",
            "Epoch 104/300\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.1562 - acc: 1.0000 - val_loss: 3.7023 - val_acc: 0.4500\n",
            "Epoch 105/300\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.1583 - acc: 0.9833 - val_loss: 3.6942 - val_acc: 0.4500\n",
            "Epoch 106/300\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.1308 - acc: 0.9833 - val_loss: 3.7666 - val_acc: 0.4500\n",
            "Epoch 107/300\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.1419 - acc: 0.9833 - val_loss: 3.8317 - val_acc: 0.4500\n",
            "Epoch 108/300\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.1377 - acc: 0.9833 - val_loss: 3.8943 - val_acc: 0.4500\n",
            "Epoch 109/300\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.1653 - acc: 0.9833 - val_loss: 3.9493 - val_acc: 0.5000\n",
            "Epoch 110/300\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.1470 - acc: 0.9833 - val_loss: 3.9395 - val_acc: 0.5000\n",
            "Epoch 111/300\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.1565 - acc: 0.9667 - val_loss: 3.9508 - val_acc: 0.5000\n",
            "Epoch 112/300\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.1373 - acc: 0.9833 - val_loss: 3.9712 - val_acc: 0.5000\n",
            "Epoch 113/300\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.1695 - acc: 0.9667 - val_loss: 3.9937 - val_acc: 0.5000\n",
            "Epoch 114/300\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.1386 - acc: 1.0000 - val_loss: 3.9649 - val_acc: 0.5000\n",
            "Epoch 115/300\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.1792 - acc: 0.9667 - val_loss: 3.8915 - val_acc: 0.5000\n",
            "Epoch 116/300\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.1681 - acc: 0.9500 - val_loss: 3.8961 - val_acc: 0.5000\n",
            "Epoch 117/300\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.1519 - acc: 0.9833 - val_loss: 3.9326 - val_acc: 0.5000\n",
            "Epoch 118/300\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.1343 - acc: 1.0000 - val_loss: 4.0385 - val_acc: 0.4000\n",
            "Epoch 119/300\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.1279 - acc: 1.0000 - val_loss: 4.0538 - val_acc: 0.4000\n",
            "Epoch 120/300\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.1347 - acc: 1.0000 - val_loss: 4.0506 - val_acc: 0.4500\n",
            "Epoch 121/300\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.1267 - acc: 0.9833 - val_loss: 3.9829 - val_acc: 0.4500\n",
            "Epoch 122/300\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.1307 - acc: 0.9833 - val_loss: 3.9225 - val_acc: 0.4500\n",
            "Epoch 123/300\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.1332 - acc: 1.0000 - val_loss: 3.8561 - val_acc: 0.5000\n",
            "Epoch 124/300\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.1526 - acc: 0.9833 - val_loss: 3.7397 - val_acc: 0.5000\n",
            "Epoch 125/300\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.1432 - acc: 1.0000 - val_loss: 3.7374 - val_acc: 0.5000\n",
            "Epoch 126/300\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.1340 - acc: 1.0000 - val_loss: 3.8232 - val_acc: 0.5000\n",
            "Epoch 127/300\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.1555 - acc: 0.9667 - val_loss: 3.8956 - val_acc: 0.5000\n",
            "Epoch 128/300\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.1240 - acc: 1.0000 - val_loss: 3.9604 - val_acc: 0.5000\n",
            "Epoch 129/300\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.1233 - acc: 0.9833 - val_loss: 4.0063 - val_acc: 0.5000\n",
            "Epoch 130/300\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.1424 - acc: 1.0000 - val_loss: 4.0659 - val_acc: 0.5000\n",
            "Epoch 131/300\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.1136 - acc: 1.0000 - val_loss: 4.0721 - val_acc: 0.5000\n",
            "Epoch 132/300\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.1161 - acc: 1.0000 - val_loss: 4.1584 - val_acc: 0.5000\n",
            "Epoch 133/300\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.1379 - acc: 1.0000 - val_loss: 4.2222 - val_acc: 0.5000\n",
            "Epoch 134/300\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.1499 - acc: 0.9667 - val_loss: 4.1885 - val_acc: 0.5000\n",
            "Epoch 135/300\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.1286 - acc: 0.9667 - val_loss: 4.1310 - val_acc: 0.5000\n",
            "Epoch 136/300\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.1068 - acc: 1.0000 - val_loss: 4.1196 - val_acc: 0.5000\n",
            "Epoch 137/300\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.0986 - acc: 1.0000 - val_loss: 4.1629 - val_acc: 0.5000\n",
            "Epoch 138/300\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.1073 - acc: 0.9833 - val_loss: 4.2587 - val_acc: 0.5000\n",
            "Epoch 139/300\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.1202 - acc: 0.9833 - val_loss: 4.3104 - val_acc: 0.5000\n",
            "Epoch 140/300\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.1105 - acc: 1.0000 - val_loss: 4.3316 - val_acc: 0.5000\n",
            "Epoch 141/300\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.1232 - acc: 1.0000 - val_loss: 4.4182 - val_acc: 0.5000\n",
            "Epoch 142/300\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.1048 - acc: 0.9833 - val_loss: 4.5254 - val_acc: 0.5000\n",
            "Epoch 143/300\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.1551 - acc: 0.9833 - val_loss: 4.5929 - val_acc: 0.4500\n",
            "Epoch 144/300\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.1089 - acc: 1.0000 - val_loss: 4.5979 - val_acc: 0.5000\n",
            "Epoch 145/300\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.1110 - acc: 0.9833 - val_loss: 4.5708 - val_acc: 0.5000\n",
            "Epoch 146/300\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.1100 - acc: 0.9833 - val_loss: 4.5536 - val_acc: 0.5000\n",
            "Epoch 147/300\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.1023 - acc: 0.9833 - val_loss: 4.5229 - val_acc: 0.5000\n",
            "Epoch 148/300\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.1556 - acc: 0.9833 - val_loss: 4.4751 - val_acc: 0.5000\n",
            "Epoch 149/300\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.1032 - acc: 1.0000 - val_loss: 4.5245 - val_acc: 0.5000\n",
            "Epoch 150/300\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.1149 - acc: 1.0000 - val_loss: 4.4892 - val_acc: 0.5000\n",
            "Epoch 151/300\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.1237 - acc: 1.0000 - val_loss: 4.3879 - val_acc: 0.5000\n",
            "Epoch 152/300\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.1134 - acc: 1.0000 - val_loss: 4.3906 - val_acc: 0.5000\n",
            "Epoch 153/300\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.1379 - acc: 0.9667 - val_loss: 4.2858 - val_acc: 0.5000\n",
            "Epoch 154/300\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.1085 - acc: 1.0000 - val_loss: 4.0522 - val_acc: 0.5000\n",
            "Epoch 155/300\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.1518 - acc: 0.9500 - val_loss: 3.9841 - val_acc: 0.5500\n",
            "Epoch 156/300\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.1161 - acc: 1.0000 - val_loss: 3.9918 - val_acc: 0.5500\n",
            "Epoch 157/300\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.1213 - acc: 1.0000 - val_loss: 4.0590 - val_acc: 0.4500\n",
            "Epoch 158/300\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.1120 - acc: 1.0000 - val_loss: 4.1377 - val_acc: 0.4500\n",
            "Epoch 159/300\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.0805 - acc: 1.0000 - val_loss: 4.2902 - val_acc: 0.4500\n",
            "Epoch 160/300\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.1007 - acc: 1.0000 - val_loss: 4.4106 - val_acc: 0.4500\n",
            "Epoch 161/300\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.1206 - acc: 0.9833 - val_loss: 4.5088 - val_acc: 0.4500\n",
            "Epoch 162/300\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.1300 - acc: 0.9833 - val_loss: 4.4769 - val_acc: 0.4500\n",
            "Epoch 163/300\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.1008 - acc: 1.0000 - val_loss: 4.4232 - val_acc: 0.5000\n",
            "Epoch 164/300\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.1165 - acc: 0.9833 - val_loss: 4.3898 - val_acc: 0.5000\n",
            "Epoch 165/300\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.1239 - acc: 1.0000 - val_loss: 4.2709 - val_acc: 0.5000\n",
            "Epoch 166/300\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.1159 - acc: 0.9833 - val_loss: 4.2312 - val_acc: 0.5000\n",
            "Epoch 167/300\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.0950 - acc: 1.0000 - val_loss: 4.2414 - val_acc: 0.5000\n",
            "Epoch 168/300\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.1012 - acc: 1.0000 - val_loss: 4.3096 - val_acc: 0.5000\n",
            "Epoch 169/300\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.1145 - acc: 0.9833 - val_loss: 4.3530 - val_acc: 0.5000\n",
            "Epoch 170/300\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.0819 - acc: 1.0000 - val_loss: 4.4446 - val_acc: 0.5000\n",
            "Epoch 171/300\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.1135 - acc: 0.9833 - val_loss: 4.6239 - val_acc: 0.5000\n",
            "Epoch 172/300\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.1075 - acc: 0.9667 - val_loss: 4.6086 - val_acc: 0.5000\n",
            "Epoch 173/300\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.1171 - acc: 0.9833 - val_loss: 4.5323 - val_acc: 0.5000\n",
            "Epoch 174/300\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.1426 - acc: 0.9667 - val_loss: 4.4696 - val_acc: 0.5000\n",
            "Epoch 175/300\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.0869 - acc: 1.0000 - val_loss: 4.4314 - val_acc: 0.5000\n",
            "Epoch 176/300\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.1048 - acc: 0.9833 - val_loss: 4.4223 - val_acc: 0.5000\n",
            "Epoch 177/300\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.1258 - acc: 1.0000 - val_loss: 4.4008 - val_acc: 0.5000\n",
            "Epoch 178/300\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.1194 - acc: 0.9833 - val_loss: 4.4399 - val_acc: 0.4500\n",
            "Epoch 179/300\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.0998 - acc: 1.0000 - val_loss: 4.4149 - val_acc: 0.4500\n",
            "Epoch 180/300\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.1085 - acc: 1.0000 - val_loss: 4.3343 - val_acc: 0.4500\n",
            "Epoch 181/300\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.0908 - acc: 0.9833 - val_loss: 4.2936 - val_acc: 0.5000\n",
            "Epoch 182/300\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.0952 - acc: 1.0000 - val_loss: 4.2547 - val_acc: 0.5500\n",
            "Epoch 183/300\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.0943 - acc: 1.0000 - val_loss: 4.2271 - val_acc: 0.5500\n",
            "Epoch 184/300\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.1224 - acc: 0.9500 - val_loss: 4.2532 - val_acc: 0.5500\n",
            "Epoch 185/300\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.1130 - acc: 1.0000 - val_loss: 4.4302 - val_acc: 0.5500\n",
            "Epoch 186/300\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.1046 - acc: 0.9667 - val_loss: 4.5585 - val_acc: 0.5000\n",
            "Epoch 187/300\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.0766 - acc: 1.0000 - val_loss: 4.6531 - val_acc: 0.4500\n",
            "Epoch 188/300\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.1049 - acc: 1.0000 - val_loss: 4.6078 - val_acc: 0.5000\n",
            "Epoch 189/300\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.0856 - acc: 1.0000 - val_loss: 4.5396 - val_acc: 0.5000\n",
            "Epoch 190/300\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.0871 - acc: 0.9833 - val_loss: 4.3765 - val_acc: 0.5500\n",
            "Epoch 191/300\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.1001 - acc: 1.0000 - val_loss: 4.3141 - val_acc: 0.5500\n",
            "Epoch 192/300\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.0923 - acc: 1.0000 - val_loss: 4.3624 - val_acc: 0.5500\n",
            "Epoch 193/300\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.0860 - acc: 1.0000 - val_loss: 4.4377 - val_acc: 0.5500\n",
            "Epoch 194/300\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.0948 - acc: 0.9833 - val_loss: 4.4145 - val_acc: 0.5500\n",
            "Epoch 195/300\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.0753 - acc: 1.0000 - val_loss: 4.4672 - val_acc: 0.5000\n",
            "Epoch 196/300\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.0698 - acc: 1.0000 - val_loss: 4.5749 - val_acc: 0.5000\n",
            "Epoch 197/300\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.0638 - acc: 1.0000 - val_loss: 4.6968 - val_acc: 0.5000\n",
            "Epoch 198/300\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.0908 - acc: 1.0000 - val_loss: 4.8061 - val_acc: 0.5000\n",
            "Epoch 199/300\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.0849 - acc: 1.0000 - val_loss: 4.9184 - val_acc: 0.5000\n",
            "Epoch 200/300\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.1009 - acc: 1.0000 - val_loss: 4.8538 - val_acc: 0.5000\n",
            "Epoch 201/300\n",
            "\n",
            "Epoch 00201: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.0762 - acc: 1.0000 - val_loss: 4.9123 - val_acc: 0.5000\n",
            "Epoch 202/300\n",
            "\n",
            "Epoch 00202: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.0900 - acc: 0.9833 - val_loss: 4.9339 - val_acc: 0.5000\n",
            "Epoch 203/300\n",
            "\n",
            "Epoch 00203: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.0829 - acc: 1.0000 - val_loss: 4.8881 - val_acc: 0.5000\n",
            "Epoch 204/300\n",
            "\n",
            "Epoch 00204: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.0951 - acc: 1.0000 - val_loss: 4.8499 - val_acc: 0.5000\n",
            "Epoch 205/300\n",
            "\n",
            "Epoch 00205: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.0869 - acc: 1.0000 - val_loss: 4.8846 - val_acc: 0.5000\n",
            "Epoch 206/300\n",
            "\n",
            "Epoch 00206: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.0700 - acc: 1.0000 - val_loss: 5.0055 - val_acc: 0.5000\n",
            "Epoch 207/300\n",
            "\n",
            "Epoch 00207: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.1135 - acc: 0.9833 - val_loss: 5.1121 - val_acc: 0.5000\n",
            "Epoch 208/300\n",
            "\n",
            "Epoch 00208: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.1031 - acc: 0.9833 - val_loss: 5.1865 - val_acc: 0.4500\n",
            "Epoch 209/300\n",
            "\n",
            "Epoch 00209: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.0817 - acc: 0.9833 - val_loss: 5.1751 - val_acc: 0.5000\n",
            "Epoch 210/300\n",
            "\n",
            "Epoch 00210: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.0823 - acc: 1.0000 - val_loss: 5.0460 - val_acc: 0.5000\n",
            "Epoch 211/300\n",
            "\n",
            "Epoch 00211: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.0815 - acc: 1.0000 - val_loss: 4.9413 - val_acc: 0.5000\n",
            "Epoch 212/300\n",
            "\n",
            "Epoch 00212: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.1237 - acc: 0.9833 - val_loss: 4.9516 - val_acc: 0.5000\n",
            "Epoch 213/300\n",
            "\n",
            "Epoch 00213: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.1029 - acc: 0.9667 - val_loss: 4.9398 - val_acc: 0.4500\n",
            "Epoch 214/300\n",
            "\n",
            "Epoch 00214: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.1026 - acc: 1.0000 - val_loss: 4.8203 - val_acc: 0.4500\n",
            "Epoch 215/300\n",
            "\n",
            "Epoch 00215: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.0925 - acc: 1.0000 - val_loss: 4.6660 - val_acc: 0.4500\n",
            "Epoch 216/300\n",
            "\n",
            "Epoch 00216: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.0757 - acc: 1.0000 - val_loss: 4.6096 - val_acc: 0.4500\n",
            "Epoch 217/300\n",
            "\n",
            "Epoch 00217: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.0829 - acc: 0.9833 - val_loss: 4.6839 - val_acc: 0.4500\n",
            "Epoch 218/300\n",
            "\n",
            "Epoch 00218: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.0874 - acc: 1.0000 - val_loss: 4.7357 - val_acc: 0.4500\n",
            "Epoch 219/300\n",
            "\n",
            "Epoch 00219: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.0781 - acc: 1.0000 - val_loss: 4.7908 - val_acc: 0.4500\n",
            "Epoch 220/300\n",
            "\n",
            "Epoch 00220: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.0859 - acc: 1.0000 - val_loss: 4.9014 - val_acc: 0.4500\n",
            "Epoch 221/300\n",
            "\n",
            "Epoch 00221: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.0907 - acc: 1.0000 - val_loss: 4.8480 - val_acc: 0.4500\n",
            "Epoch 222/300\n",
            "\n",
            "Epoch 00222: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.0761 - acc: 1.0000 - val_loss: 4.7647 - val_acc: 0.5000\n",
            "Epoch 223/300\n",
            "\n",
            "Epoch 00223: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.0774 - acc: 1.0000 - val_loss: 4.5815 - val_acc: 0.5000\n",
            "Epoch 224/300\n",
            "\n",
            "Epoch 00224: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.0748 - acc: 1.0000 - val_loss: 4.4988 - val_acc: 0.5500\n",
            "Epoch 225/300\n",
            "\n",
            "Epoch 00225: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.0720 - acc: 1.0000 - val_loss: 4.4834 - val_acc: 0.5500\n",
            "Epoch 226/300\n",
            "\n",
            "Epoch 00226: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.0975 - acc: 0.9833 - val_loss: 4.5736 - val_acc: 0.5000\n",
            "Epoch 227/300\n",
            "\n",
            "Epoch 00227: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.0755 - acc: 0.9833 - val_loss: 4.7079 - val_acc: 0.4500\n",
            "Epoch 228/300\n",
            "\n",
            "Epoch 00228: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.0702 - acc: 1.0000 - val_loss: 4.7337 - val_acc: 0.5000\n",
            "Epoch 229/300\n",
            "\n",
            "Epoch 00229: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.0791 - acc: 1.0000 - val_loss: 4.8251 - val_acc: 0.5000\n",
            "Epoch 230/300\n",
            "\n",
            "Epoch 00230: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.0629 - acc: 1.0000 - val_loss: 5.0044 - val_acc: 0.5000\n",
            "Epoch 231/300\n",
            "\n",
            "Epoch 00231: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.0860 - acc: 0.9667 - val_loss: 5.2194 - val_acc: 0.5000\n",
            "Epoch 232/300\n",
            "\n",
            "Epoch 00232: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.0857 - acc: 1.0000 - val_loss: 5.1134 - val_acc: 0.5000\n",
            "Epoch 233/300\n",
            "\n",
            "Epoch 00233: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.0717 - acc: 1.0000 - val_loss: 5.0094 - val_acc: 0.5000\n",
            "Epoch 234/300\n",
            "\n",
            "Epoch 00234: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.0687 - acc: 1.0000 - val_loss: 4.8763 - val_acc: 0.5000\n",
            "Epoch 235/300\n",
            "\n",
            "Epoch 00235: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.0745 - acc: 0.9833 - val_loss: 4.7499 - val_acc: 0.5000\n",
            "Epoch 236/300\n",
            "\n",
            "Epoch 00236: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.0817 - acc: 1.0000 - val_loss: 4.6207 - val_acc: 0.5500\n",
            "Epoch 237/300\n",
            "\n",
            "Epoch 00237: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.0819 - acc: 1.0000 - val_loss: 4.6171 - val_acc: 0.5500\n",
            "Epoch 238/300\n",
            "\n",
            "Epoch 00238: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.0708 - acc: 1.0000 - val_loss: 4.5887 - val_acc: 0.5500\n",
            "Epoch 239/300\n",
            "\n",
            "Epoch 00239: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.0586 - acc: 1.0000 - val_loss: 4.6169 - val_acc: 0.5500\n",
            "Epoch 240/300\n",
            "\n",
            "Epoch 00240: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.0934 - acc: 0.9833 - val_loss: 4.5165 - val_acc: 0.5500\n",
            "Epoch 241/300\n",
            "\n",
            "Epoch 00241: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.0786 - acc: 0.9833 - val_loss: 4.3295 - val_acc: 0.5500\n",
            "Epoch 242/300\n",
            "\n",
            "Epoch 00242: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.0617 - acc: 1.0000 - val_loss: 4.2570 - val_acc: 0.6000\n",
            "Epoch 243/300\n",
            "\n",
            "Epoch 00243: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.0901 - acc: 1.0000 - val_loss: 4.2479 - val_acc: 0.6000\n",
            "Epoch 244/300\n",
            "\n",
            "Epoch 00244: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.0802 - acc: 1.0000 - val_loss: 4.2487 - val_acc: 0.6000\n",
            "Epoch 245/300\n",
            "\n",
            "Epoch 00245: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.0847 - acc: 1.0000 - val_loss: 4.2759 - val_acc: 0.6000\n",
            "Epoch 246/300\n",
            "\n",
            "Epoch 00246: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.0766 - acc: 1.0000 - val_loss: 4.3104 - val_acc: 0.5500\n",
            "Epoch 247/300\n",
            "\n",
            "Epoch 00247: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.0679 - acc: 1.0000 - val_loss: 4.3190 - val_acc: 0.5500\n",
            "Epoch 248/300\n",
            "\n",
            "Epoch 00248: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.0795 - acc: 0.9833 - val_loss: 4.3205 - val_acc: 0.5500\n",
            "Epoch 249/300\n",
            "\n",
            "Epoch 00249: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.0689 - acc: 1.0000 - val_loss: 4.3702 - val_acc: 0.5500\n",
            "Epoch 250/300\n",
            "\n",
            "Epoch 00250: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.0602 - acc: 1.0000 - val_loss: 4.3488 - val_acc: 0.5500\n",
            "Epoch 251/300\n",
            "\n",
            "Epoch 00251: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.0681 - acc: 1.0000 - val_loss: 4.2860 - val_acc: 0.6000\n",
            "Epoch 252/300\n",
            "\n",
            "Epoch 00252: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.0989 - acc: 0.9833 - val_loss: 4.2945 - val_acc: 0.6000\n",
            "Epoch 253/300\n",
            "\n",
            "Epoch 00253: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.0838 - acc: 0.9833 - val_loss: 4.3908 - val_acc: 0.5500\n",
            "Epoch 254/300\n",
            "\n",
            "Epoch 00254: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.0530 - acc: 1.0000 - val_loss: 4.4166 - val_acc: 0.5500\n",
            "Epoch 255/300\n",
            "\n",
            "Epoch 00255: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.0724 - acc: 1.0000 - val_loss: 4.3518 - val_acc: 0.5500\n",
            "Epoch 256/300\n",
            "\n",
            "Epoch 00256: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.0732 - acc: 1.0000 - val_loss: 4.1370 - val_acc: 0.5500\n",
            "Epoch 257/300\n",
            "\n",
            "Epoch 00257: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.0622 - acc: 1.0000 - val_loss: 3.9923 - val_acc: 0.5500\n",
            "Epoch 258/300\n",
            "\n",
            "Epoch 00258: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.0734 - acc: 1.0000 - val_loss: 3.9335 - val_acc: 0.5500\n",
            "Epoch 259/300\n",
            "\n",
            "Epoch 00259: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.0890 - acc: 1.0000 - val_loss: 3.9912 - val_acc: 0.5500\n",
            "Epoch 260/300\n",
            "\n",
            "Epoch 00260: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.0703 - acc: 1.0000 - val_loss: 3.9863 - val_acc: 0.6000\n",
            "Epoch 261/300\n",
            "\n",
            "Epoch 00261: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.0717 - acc: 1.0000 - val_loss: 3.9710 - val_acc: 0.6500\n",
            "Epoch 262/300\n",
            "\n",
            "Epoch 00262: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.0644 - acc: 1.0000 - val_loss: 3.9370 - val_acc: 0.6500\n",
            "Epoch 263/300\n",
            "\n",
            "Epoch 00263: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.0549 - acc: 1.0000 - val_loss: 3.9575 - val_acc: 0.6500\n",
            "Epoch 264/300\n",
            "\n",
            "Epoch 00264: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.0683 - acc: 1.0000 - val_loss: 4.0245 - val_acc: 0.5500\n",
            "Epoch 265/300\n",
            "\n",
            "Epoch 00265: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.0692 - acc: 1.0000 - val_loss: 4.2002 - val_acc: 0.5500\n",
            "Epoch 266/300\n",
            "\n",
            "Epoch 00266: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.0700 - acc: 1.0000 - val_loss: 4.2400 - val_acc: 0.5500\n",
            "Epoch 267/300\n",
            "\n",
            "Epoch 00267: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.0774 - acc: 1.0000 - val_loss: 4.1767 - val_acc: 0.5500\n",
            "Epoch 268/300\n",
            "\n",
            "Epoch 00268: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.0738 - acc: 1.0000 - val_loss: 3.9676 - val_acc: 0.5500\n",
            "Epoch 269/300\n",
            "\n",
            "Epoch 00269: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.0670 - acc: 1.0000 - val_loss: 3.8591 - val_acc: 0.6000\n",
            "Epoch 270/300\n",
            "\n",
            "Epoch 00270: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.0699 - acc: 1.0000 - val_loss: 3.7439 - val_acc: 0.6000\n",
            "Epoch 271/300\n",
            "\n",
            "Epoch 00271: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.0706 - acc: 1.0000 - val_loss: 3.6971 - val_acc: 0.6000\n",
            "Epoch 272/300\n",
            "\n",
            "Epoch 00272: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.0595 - acc: 1.0000 - val_loss: 3.7572 - val_acc: 0.5500\n",
            "Epoch 273/300\n",
            "\n",
            "Epoch 00273: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.0594 - acc: 1.0000 - val_loss: 3.7635 - val_acc: 0.5500\n",
            "Epoch 274/300\n",
            "\n",
            "Epoch 00274: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.0720 - acc: 1.0000 - val_loss: 3.7073 - val_acc: 0.5500\n",
            "Epoch 275/300\n",
            "\n",
            "Epoch 00275: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.0786 - acc: 0.9833 - val_loss: 3.8628 - val_acc: 0.5500\n",
            "Epoch 276/300\n",
            "\n",
            "Epoch 00276: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.0682 - acc: 1.0000 - val_loss: 3.9885 - val_acc: 0.5500\n",
            "Epoch 277/300\n",
            "\n",
            "Epoch 00277: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.0765 - acc: 1.0000 - val_loss: 4.0332 - val_acc: 0.5500\n",
            "Epoch 278/300\n",
            "\n",
            "Epoch 00278: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.0724 - acc: 0.9833 - val_loss: 3.9572 - val_acc: 0.5500\n",
            "Epoch 279/300\n",
            "\n",
            "Epoch 00279: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.0636 - acc: 1.0000 - val_loss: 4.0116 - val_acc: 0.5500\n",
            "Epoch 280/300\n",
            "\n",
            "Epoch 00280: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.0607 - acc: 1.0000 - val_loss: 4.1079 - val_acc: 0.5500\n",
            "Epoch 281/300\n",
            "\n",
            "Epoch 00281: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.0824 - acc: 1.0000 - val_loss: 4.0537 - val_acc: 0.5500\n",
            "Epoch 282/300\n",
            "\n",
            "Epoch 00282: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.0560 - acc: 1.0000 - val_loss: 3.9899 - val_acc: 0.5500\n",
            "Epoch 283/300\n",
            "\n",
            "Epoch 00283: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.0795 - acc: 1.0000 - val_loss: 3.8821 - val_acc: 0.5500\n",
            "Epoch 284/300\n",
            "\n",
            "Epoch 00284: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.0529 - acc: 1.0000 - val_loss: 3.7107 - val_acc: 0.5500\n",
            "Epoch 285/300\n",
            "\n",
            "Epoch 00285: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.0641 - acc: 0.9833 - val_loss: 3.6100 - val_acc: 0.5500\n",
            "Epoch 286/300\n",
            "\n",
            "Epoch 00286: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.0489 - acc: 1.0000 - val_loss: 3.5796 - val_acc: 0.5500\n",
            "Epoch 287/300\n",
            "\n",
            "Epoch 00287: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.0723 - acc: 1.0000 - val_loss: 3.5863 - val_acc: 0.5500\n",
            "Epoch 288/300\n",
            "\n",
            "Epoch 00288: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.0852 - acc: 0.9833 - val_loss: 3.6857 - val_acc: 0.5500\n",
            "Epoch 289/300\n",
            "\n",
            "Epoch 00289: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.0572 - acc: 1.0000 - val_loss: 3.7311 - val_acc: 0.5500\n",
            "Epoch 290/300\n",
            "\n",
            "Epoch 00290: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.0636 - acc: 0.9833 - val_loss: 3.7364 - val_acc: 0.5500\n",
            "Epoch 291/300\n",
            "\n",
            "Epoch 00291: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.0722 - acc: 0.9833 - val_loss: 3.6300 - val_acc: 0.5500\n",
            "Epoch 292/300\n",
            "\n",
            "Epoch 00292: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.0480 - acc: 1.0000 - val_loss: 3.5944 - val_acc: 0.5500\n",
            "Epoch 293/300\n",
            "\n",
            "Epoch 00293: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.0695 - acc: 1.0000 - val_loss: 3.5103 - val_acc: 0.5500\n",
            "Epoch 294/300\n",
            "\n",
            "Epoch 00294: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.0592 - acc: 1.0000 - val_loss: 3.5401 - val_acc: 0.5500\n",
            "Epoch 295/300\n",
            "\n",
            "Epoch 00295: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.0696 - acc: 0.9833 - val_loss: 3.6084 - val_acc: 0.5500\n",
            "Epoch 296/300\n",
            "\n",
            "Epoch 00296: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.0692 - acc: 1.0000 - val_loss: 3.7605 - val_acc: 0.5500\n",
            "Epoch 297/300\n",
            "\n",
            "Epoch 00297: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.0596 - acc: 1.0000 - val_loss: 4.0063 - val_acc: 0.5500\n",
            "Epoch 298/300\n",
            "\n",
            "Epoch 00298: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.0780 - acc: 1.0000 - val_loss: 4.2461 - val_acc: 0.5500\n",
            "Epoch 299/300\n",
            "\n",
            "Epoch 00299: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.0861 - acc: 0.9833 - val_loss: 4.2435 - val_acc: 0.5500\n",
            "Epoch 300/300\n",
            "\n",
            "Epoch 00300: val_loss did not improve from 1.33651\n",
            "60/60 - 0s - loss: 0.0595 - acc: 1.0000 - val_loss: 3.8757 - val_acc: 0.5500\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "# Evaluate on test data\n",
            "40/40 [==============================] - 0s 2ms/sample - loss: 0.8262 - acc: 0.5000\n",
            "test loss, test acc: [0.8261870723261382, 0.5]\n",
            "EEG_Deep/Data2A/parsed_P02T.mat\n",
            "Filtering of Training Data Finished\n",
            "EEG_Deep/Data2A/parsed_P02E.mat\n",
            "Filtering of Testing Data Finished\n",
            "(1, 2)\n",
            "Finding labels in training data\n",
            "Finding labels in testing data\n",
            "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
            "(60, 12, 1536)\n",
            "(60,)\n",
            "(20, 12, 1536)\n",
            "(20,)\n",
            "[[1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]]\n",
            "X_train shape: (60, 1, 12, 1536)\n",
            "60 train samples\n",
            "20 val samples\n",
            "(40, 12, 1536)\n",
            "(40, 1)\n",
            "X_train shape: (40, 1, 12, 1536)\n",
            "40 train samples\n",
            "Train on 60 samples, validate on 20 samples\n",
            "Epoch 1/300\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 1.37929, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 1s - loss: 1.4180 - acc: 0.3500 - val_loss: 1.3793 - val_acc: 0.0000e+00\n",
            "Epoch 2/300\n",
            "\n",
            "Epoch 00002: val_loss improved from 1.37929 to 1.36206, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 1.0606 - acc: 0.6333 - val_loss: 1.3621 - val_acc: 0.0000e+00\n",
            "Epoch 3/300\n",
            "\n",
            "Epoch 00003: val_loss improved from 1.36206 to 1.34879, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.9060 - acc: 0.6667 - val_loss: 1.3488 - val_acc: 0.0000e+00\n",
            "Epoch 4/300\n",
            "\n",
            "Epoch 00004: val_loss improved from 1.34879 to 1.33890, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.8145 - acc: 0.6667 - val_loss: 1.3389 - val_acc: 0.0000e+00\n",
            "Epoch 5/300\n",
            "\n",
            "Epoch 00005: val_loss improved from 1.33890 to 1.32849, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.7616 - acc: 0.6667 - val_loss: 1.3285 - val_acc: 0.0000e+00\n",
            "Epoch 6/300\n",
            "\n",
            "Epoch 00006: val_loss improved from 1.32849 to 1.31237, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.7366 - acc: 0.6667 - val_loss: 1.3124 - val_acc: 0.0000e+00\n",
            "Epoch 7/300\n",
            "\n",
            "Epoch 00007: val_loss improved from 1.31237 to 1.30048, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6955 - acc: 0.6667 - val_loss: 1.3005 - val_acc: 0.0000e+00\n",
            "Epoch 8/300\n",
            "\n",
            "Epoch 00008: val_loss improved from 1.30048 to 1.28460, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6892 - acc: 0.6833 - val_loss: 1.2846 - val_acc: 0.0000e+00\n",
            "Epoch 9/300\n",
            "\n",
            "Epoch 00009: val_loss improved from 1.28460 to 1.27082, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6623 - acc: 0.6667 - val_loss: 1.2708 - val_acc: 0.0000e+00\n",
            "Epoch 10/300\n",
            "\n",
            "Epoch 00010: val_loss improved from 1.27082 to 1.25945, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6349 - acc: 0.7000 - val_loss: 1.2595 - val_acc: 0.0000e+00\n",
            "Epoch 11/300\n",
            "\n",
            "Epoch 00011: val_loss improved from 1.25945 to 1.25266, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6308 - acc: 0.6833 - val_loss: 1.2527 - val_acc: 0.0000e+00\n",
            "Epoch 12/300\n",
            "\n",
            "Epoch 00012: val_loss improved from 1.25266 to 1.24698, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6237 - acc: 0.7500 - val_loss: 1.2470 - val_acc: 0.0000e+00\n",
            "Epoch 13/300\n",
            "\n",
            "Epoch 00013: val_loss improved from 1.24698 to 1.24447, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6126 - acc: 0.7333 - val_loss: 1.2445 - val_acc: 0.0000e+00\n",
            "Epoch 14/300\n",
            "\n",
            "Epoch 00014: val_loss improved from 1.24447 to 1.24152, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5889 - acc: 0.7500 - val_loss: 1.2415 - val_acc: 0.0000e+00\n",
            "Epoch 15/300\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 1.24152\n",
            "60/60 - 0s - loss: 0.5753 - acc: 0.7500 - val_loss: 1.2416 - val_acc: 0.0000e+00\n",
            "Epoch 16/300\n",
            "\n",
            "Epoch 00016: val_loss improved from 1.24152 to 1.22943, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5686 - acc: 0.7667 - val_loss: 1.2294 - val_acc: 0.0000e+00\n",
            "Epoch 17/300\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 1.22943\n",
            "60/60 - 0s - loss: 0.5468 - acc: 0.8500 - val_loss: 1.2302 - val_acc: 0.0000e+00\n",
            "Epoch 18/300\n",
            "\n",
            "Epoch 00018: val_loss improved from 1.22943 to 1.22913, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5196 - acc: 0.9000 - val_loss: 1.2291 - val_acc: 0.0000e+00\n",
            "Epoch 19/300\n",
            "\n",
            "Epoch 00019: val_loss improved from 1.22913 to 1.22692, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5326 - acc: 0.8333 - val_loss: 1.2269 - val_acc: 0.0000e+00\n",
            "Epoch 20/300\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 1.22692\n",
            "60/60 - 0s - loss: 0.5064 - acc: 0.8500 - val_loss: 1.2335 - val_acc: 0.0000e+00\n",
            "Epoch 21/300\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 1.22692\n",
            "60/60 - 0s - loss: 0.4990 - acc: 0.9000 - val_loss: 1.2369 - val_acc: 0.0000e+00\n",
            "Epoch 22/300\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 1.22692\n",
            "60/60 - 0s - loss: 0.4909 - acc: 0.8500 - val_loss: 1.2340 - val_acc: 0.0000e+00\n",
            "Epoch 23/300\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 1.22692\n",
            "60/60 - 0s - loss: 0.4854 - acc: 0.8500 - val_loss: 1.2289 - val_acc: 0.0000e+00\n",
            "Epoch 24/300\n",
            "\n",
            "Epoch 00024: val_loss improved from 1.22692 to 1.22179, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4556 - acc: 0.8667 - val_loss: 1.2218 - val_acc: 0.0000e+00\n",
            "Epoch 25/300\n",
            "\n",
            "Epoch 00025: val_loss improved from 1.22179 to 1.20976, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4566 - acc: 0.8667 - val_loss: 1.2098 - val_acc: 0.0000e+00\n",
            "Epoch 26/300\n",
            "\n",
            "Epoch 00026: val_loss improved from 1.20976 to 1.20544, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4398 - acc: 0.9500 - val_loss: 1.2054 - val_acc: 0.0000e+00\n",
            "Epoch 27/300\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 1.20544\n",
            "60/60 - 0s - loss: 0.4430 - acc: 0.9333 - val_loss: 1.2108 - val_acc: 0.0000e+00\n",
            "Epoch 28/300\n",
            "\n",
            "Epoch 00028: val_loss improved from 1.20544 to 1.19742, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4418 - acc: 0.9000 - val_loss: 1.1974 - val_acc: 0.0000e+00\n",
            "Epoch 29/300\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 1.19742\n",
            "60/60 - 0s - loss: 0.4352 - acc: 0.9167 - val_loss: 1.1994 - val_acc: 0.0000e+00\n",
            "Epoch 30/300\n",
            "\n",
            "Epoch 00030: val_loss improved from 1.19742 to 1.19384, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4185 - acc: 0.9500 - val_loss: 1.1938 - val_acc: 0.0000e+00\n",
            "Epoch 31/300\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 1.19384\n",
            "60/60 - 0s - loss: 0.4041 - acc: 0.9167 - val_loss: 1.2099 - val_acc: 0.0000e+00\n",
            "Epoch 32/300\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 1.19384\n",
            "60/60 - 0s - loss: 0.3820 - acc: 0.9500 - val_loss: 1.2266 - val_acc: 0.0000e+00\n",
            "Epoch 33/300\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 1.19384\n",
            "60/60 - 0s - loss: 0.3890 - acc: 0.9333 - val_loss: 1.2423 - val_acc: 0.0000e+00\n",
            "Epoch 34/300\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 1.19384\n",
            "60/60 - 0s - loss: 0.3948 - acc: 0.9000 - val_loss: 1.2524 - val_acc: 0.0000e+00\n",
            "Epoch 35/300\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 1.19384\n",
            "60/60 - 0s - loss: 0.3583 - acc: 0.9667 - val_loss: 1.2421 - val_acc: 0.0000e+00\n",
            "Epoch 36/300\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 1.19384\n",
            "60/60 - 0s - loss: 0.3678 - acc: 0.9667 - val_loss: 1.2349 - val_acc: 0.0000e+00\n",
            "Epoch 37/300\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 1.19384\n",
            "60/60 - 0s - loss: 0.3744 - acc: 0.9333 - val_loss: 1.2361 - val_acc: 0.0000e+00\n",
            "Epoch 38/300\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 1.19384\n",
            "60/60 - 0s - loss: 0.3739 - acc: 0.9667 - val_loss: 1.2178 - val_acc: 0.0500\n",
            "Epoch 39/300\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 1.19384\n",
            "60/60 - 0s - loss: 0.3891 - acc: 0.9167 - val_loss: 1.2426 - val_acc: 0.0000e+00\n",
            "Epoch 40/300\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 1.19384\n",
            "60/60 - 0s - loss: 0.3668 - acc: 0.9333 - val_loss: 1.2382 - val_acc: 0.0500\n",
            "Epoch 41/300\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 1.19384\n",
            "60/60 - 0s - loss: 0.3742 - acc: 0.9333 - val_loss: 1.2216 - val_acc: 0.0500\n",
            "Epoch 42/300\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 1.19384\n",
            "60/60 - 0s - loss: 0.3464 - acc: 0.9667 - val_loss: 1.2026 - val_acc: 0.0500\n",
            "Epoch 43/300\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 1.19384\n",
            "60/60 - 0s - loss: 0.3398 - acc: 0.9833 - val_loss: 1.1978 - val_acc: 0.1000\n",
            "Epoch 44/300\n",
            "\n",
            "Epoch 00044: val_loss improved from 1.19384 to 1.18997, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3387 - acc: 1.0000 - val_loss: 1.1900 - val_acc: 0.1000\n",
            "Epoch 45/300\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 1.18997\n",
            "60/60 - 0s - loss: 0.3050 - acc: 1.0000 - val_loss: 1.2201 - val_acc: 0.1000\n",
            "Epoch 46/300\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 1.18997\n",
            "60/60 - 0s - loss: 0.3376 - acc: 1.0000 - val_loss: 1.2416 - val_acc: 0.1000\n",
            "Epoch 47/300\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 1.18997\n",
            "60/60 - 0s - loss: 0.3292 - acc: 0.9833 - val_loss: 1.2603 - val_acc: 0.0500\n",
            "Epoch 48/300\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 1.18997\n",
            "60/60 - 0s - loss: 0.3289 - acc: 0.9667 - val_loss: 1.2483 - val_acc: 0.0500\n",
            "Epoch 49/300\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 1.18997\n",
            "60/60 - 0s - loss: 0.3392 - acc: 0.9333 - val_loss: 1.2141 - val_acc: 0.1000\n",
            "Epoch 50/300\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 1.18997\n",
            "60/60 - 0s - loss: 0.3056 - acc: 1.0000 - val_loss: 1.1955 - val_acc: 0.1000\n",
            "Epoch 51/300\n",
            "\n",
            "Epoch 00051: val_loss improved from 1.18997 to 1.16742, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3299 - acc: 0.9500 - val_loss: 1.1674 - val_acc: 0.2000\n",
            "Epoch 52/300\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 1.16742\n",
            "60/60 - 0s - loss: 0.3275 - acc: 0.9500 - val_loss: 1.1708 - val_acc: 0.2000\n",
            "Epoch 53/300\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 1.16742\n",
            "60/60 - 0s - loss: 0.3051 - acc: 0.9833 - val_loss: 1.1805 - val_acc: 0.1500\n",
            "Epoch 54/300\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 1.16742\n",
            "60/60 - 0s - loss: 0.3032 - acc: 0.9667 - val_loss: 1.1688 - val_acc: 0.2000\n",
            "Epoch 55/300\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 1.16742\n",
            "60/60 - 0s - loss: 0.2958 - acc: 0.9500 - val_loss: 1.1947 - val_acc: 0.2000\n",
            "Epoch 56/300\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 1.16742\n",
            "60/60 - 0s - loss: 0.3160 - acc: 0.9667 - val_loss: 1.1785 - val_acc: 0.2000\n",
            "Epoch 57/300\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 1.16742\n",
            "60/60 - 0s - loss: 0.3152 - acc: 0.9667 - val_loss: 1.1699 - val_acc: 0.2000\n",
            "Epoch 58/300\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 1.16742\n",
            "60/60 - 0s - loss: 0.3140 - acc: 0.9500 - val_loss: 1.1784 - val_acc: 0.2000\n",
            "Epoch 59/300\n",
            "\n",
            "Epoch 00059: val_loss improved from 1.16742 to 1.15876, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3028 - acc: 1.0000 - val_loss: 1.1588 - val_acc: 0.2000\n",
            "Epoch 60/300\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 1.15876\n",
            "60/60 - 0s - loss: 0.2751 - acc: 0.9833 - val_loss: 1.1626 - val_acc: 0.2000\n",
            "Epoch 61/300\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 1.15876\n",
            "60/60 - 0s - loss: 0.2850 - acc: 0.9833 - val_loss: 1.1657 - val_acc: 0.2000\n",
            "Epoch 62/300\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 1.15876\n",
            "60/60 - 0s - loss: 0.2855 - acc: 0.9833 - val_loss: 1.1668 - val_acc: 0.2000\n",
            "Epoch 63/300\n",
            "\n",
            "Epoch 00063: val_loss improved from 1.15876 to 1.12025, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3067 - acc: 0.9667 - val_loss: 1.1202 - val_acc: 0.2500\n",
            "Epoch 64/300\n",
            "\n",
            "Epoch 00064: val_loss improved from 1.12025 to 1.08937, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2818 - acc: 1.0000 - val_loss: 1.0894 - val_acc: 0.2500\n",
            "Epoch 65/300\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 1.08937\n",
            "60/60 - 0s - loss: 0.2963 - acc: 0.9833 - val_loss: 1.1302 - val_acc: 0.2500\n",
            "Epoch 66/300\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 1.08937\n",
            "60/60 - 0s - loss: 0.2792 - acc: 0.9833 - val_loss: 1.1328 - val_acc: 0.2500\n",
            "Epoch 67/300\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 1.08937\n",
            "60/60 - 0s - loss: 0.2596 - acc: 1.0000 - val_loss: 1.1372 - val_acc: 0.2500\n",
            "Epoch 68/300\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 1.08937\n",
            "60/60 - 0s - loss: 0.2881 - acc: 0.9333 - val_loss: 1.1256 - val_acc: 0.2500\n",
            "Epoch 69/300\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 1.08937\n",
            "60/60 - 0s - loss: 0.2702 - acc: 0.9833 - val_loss: 1.1043 - val_acc: 0.3000\n",
            "Epoch 70/300\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 1.08937\n",
            "60/60 - 0s - loss: 0.2676 - acc: 0.9667 - val_loss: 1.0927 - val_acc: 0.3000\n",
            "Epoch 71/300\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 1.08937\n",
            "60/60 - 0s - loss: 0.2777 - acc: 0.9667 - val_loss: 1.1101 - val_acc: 0.3000\n",
            "Epoch 72/300\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 1.08937\n",
            "60/60 - 0s - loss: 0.2613 - acc: 1.0000 - val_loss: 1.1018 - val_acc: 0.3500\n",
            "Epoch 73/300\n",
            "\n",
            "Epoch 00073: val_loss improved from 1.08937 to 1.06758, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2346 - acc: 0.9833 - val_loss: 1.0676 - val_acc: 0.4000\n",
            "Epoch 74/300\n",
            "\n",
            "Epoch 00074: val_loss improved from 1.06758 to 1.03978, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2676 - acc: 0.9667 - val_loss: 1.0398 - val_acc: 0.4000\n",
            "Epoch 75/300\n",
            "\n",
            "Epoch 00075: val_loss improved from 1.03978 to 1.00799, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2628 - acc: 0.9833 - val_loss: 1.0080 - val_acc: 0.4000\n",
            "Epoch 76/300\n",
            "\n",
            "Epoch 00076: val_loss improved from 1.00799 to 1.00322, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2435 - acc: 1.0000 - val_loss: 1.0032 - val_acc: 0.4000\n",
            "Epoch 77/300\n",
            "\n",
            "Epoch 00077: val_loss improved from 1.00322 to 0.98544, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2443 - acc: 0.9833 - val_loss: 0.9854 - val_acc: 0.4000\n",
            "Epoch 78/300\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.98544\n",
            "60/60 - 0s - loss: 0.2573 - acc: 1.0000 - val_loss: 0.9992 - val_acc: 0.4000\n",
            "Epoch 79/300\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.98544\n",
            "60/60 - 0s - loss: 0.2556 - acc: 0.9667 - val_loss: 1.0110 - val_acc: 0.4000\n",
            "Epoch 80/300\n",
            "\n",
            "Epoch 00080: val_loss improved from 0.98544 to 0.98335, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2552 - acc: 0.9667 - val_loss: 0.9833 - val_acc: 0.4000\n",
            "Epoch 81/300\n",
            "\n",
            "Epoch 00081: val_loss improved from 0.98335 to 0.95890, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2529 - acc: 1.0000 - val_loss: 0.9589 - val_acc: 0.4000\n",
            "Epoch 82/300\n",
            "\n",
            "Epoch 00082: val_loss improved from 0.95890 to 0.93929, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2201 - acc: 0.9833 - val_loss: 0.9393 - val_acc: 0.4500\n",
            "Epoch 83/300\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.93929\n",
            "60/60 - 0s - loss: 0.2196 - acc: 1.0000 - val_loss: 0.9638 - val_acc: 0.4000\n",
            "Epoch 84/300\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.93929\n",
            "60/60 - 0s - loss: 0.2464 - acc: 0.9667 - val_loss: 1.0106 - val_acc: 0.4000\n",
            "Epoch 85/300\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.93929\n",
            "60/60 - 0s - loss: 0.2322 - acc: 0.9667 - val_loss: 1.0180 - val_acc: 0.4000\n",
            "Epoch 86/300\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.93929\n",
            "60/60 - 0s - loss: 0.2462 - acc: 0.9833 - val_loss: 0.9486 - val_acc: 0.4500\n",
            "Epoch 87/300\n",
            "\n",
            "Epoch 00087: val_loss improved from 0.93929 to 0.90194, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2395 - acc: 1.0000 - val_loss: 0.9019 - val_acc: 0.4500\n",
            "Epoch 88/300\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.90194\n",
            "60/60 - 0s - loss: 0.2275 - acc: 0.9833 - val_loss: 0.9029 - val_acc: 0.4500\n",
            "Epoch 89/300\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.90194\n",
            "60/60 - 0s - loss: 0.2349 - acc: 0.9833 - val_loss: 0.9393 - val_acc: 0.4500\n",
            "Epoch 90/300\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.90194\n",
            "60/60 - 0s - loss: 0.2419 - acc: 0.9833 - val_loss: 0.9450 - val_acc: 0.4500\n",
            "Epoch 91/300\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.90194\n",
            "60/60 - 0s - loss: 0.2142 - acc: 0.9833 - val_loss: 0.9328 - val_acc: 0.4500\n",
            "Epoch 92/300\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.90194\n",
            "60/60 - 0s - loss: 0.2388 - acc: 0.9667 - val_loss: 0.9069 - val_acc: 0.4500\n",
            "Epoch 93/300\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.90194\n",
            "60/60 - 0s - loss: 0.2003 - acc: 1.0000 - val_loss: 0.9197 - val_acc: 0.4500\n",
            "Epoch 94/300\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.90194\n",
            "60/60 - 0s - loss: 0.2479 - acc: 0.9833 - val_loss: 0.9500 - val_acc: 0.4500\n",
            "Epoch 95/300\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.90194\n",
            "60/60 - 0s - loss: 0.2268 - acc: 0.9833 - val_loss: 0.9147 - val_acc: 0.5500\n",
            "Epoch 96/300\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.90194\n",
            "60/60 - 0s - loss: 0.1951 - acc: 1.0000 - val_loss: 0.9225 - val_acc: 0.5500\n",
            "Epoch 97/300\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.90194\n",
            "60/60 - 0s - loss: 0.2197 - acc: 1.0000 - val_loss: 0.9458 - val_acc: 0.4500\n",
            "Epoch 98/300\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.90194\n",
            "60/60 - 0s - loss: 0.2110 - acc: 1.0000 - val_loss: 0.9195 - val_acc: 0.5500\n",
            "Epoch 99/300\n",
            "\n",
            "Epoch 00099: val_loss improved from 0.90194 to 0.89613, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2215 - acc: 1.0000 - val_loss: 0.8961 - val_acc: 0.5500\n",
            "Epoch 100/300\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.89613\n",
            "60/60 - 0s - loss: 0.2535 - acc: 0.9833 - val_loss: 0.9048 - val_acc: 0.5500\n",
            "Epoch 101/300\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.89613\n",
            "60/60 - 0s - loss: 0.2229 - acc: 1.0000 - val_loss: 0.9185 - val_acc: 0.5500\n",
            "Epoch 102/300\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.89613\n",
            "60/60 - 0s - loss: 0.2253 - acc: 1.0000 - val_loss: 0.9060 - val_acc: 0.5500\n",
            "Epoch 103/300\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.89613\n",
            "60/60 - 0s - loss: 0.2173 - acc: 0.9833 - val_loss: 0.9271 - val_acc: 0.5000\n",
            "Epoch 104/300\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.89613\n",
            "60/60 - 0s - loss: 0.2004 - acc: 0.9667 - val_loss: 0.9092 - val_acc: 0.5000\n",
            "Epoch 105/300\n",
            "\n",
            "Epoch 00105: val_loss improved from 0.89613 to 0.88183, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2008 - acc: 1.0000 - val_loss: 0.8818 - val_acc: 0.6000\n",
            "Epoch 106/300\n",
            "\n",
            "Epoch 00106: val_loss improved from 0.88183 to 0.84038, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2140 - acc: 0.9667 - val_loss: 0.8404 - val_acc: 0.6000\n",
            "Epoch 107/300\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.84038\n",
            "60/60 - 0s - loss: 0.2505 - acc: 0.9500 - val_loss: 0.8659 - val_acc: 0.6000\n",
            "Epoch 108/300\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 0.84038\n",
            "60/60 - 0s - loss: 0.2235 - acc: 0.9833 - val_loss: 0.8556 - val_acc: 0.6000\n",
            "Epoch 109/300\n",
            "\n",
            "Epoch 00109: val_loss improved from 0.84038 to 0.83973, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2207 - acc: 0.9833 - val_loss: 0.8397 - val_acc: 0.5500\n",
            "Epoch 110/300\n",
            "\n",
            "Epoch 00110: val_loss improved from 0.83973 to 0.83794, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2007 - acc: 1.0000 - val_loss: 0.8379 - val_acc: 0.5500\n",
            "Epoch 111/300\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 0.83794\n",
            "60/60 - 0s - loss: 0.2091 - acc: 0.9833 - val_loss: 0.9107 - val_acc: 0.5000\n",
            "Epoch 112/300\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 0.83794\n",
            "60/60 - 0s - loss: 0.1809 - acc: 1.0000 - val_loss: 0.8988 - val_acc: 0.5000\n",
            "Epoch 113/300\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.83794\n",
            "60/60 - 0s - loss: 0.2022 - acc: 0.9833 - val_loss: 0.8750 - val_acc: 0.5500\n",
            "Epoch 114/300\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.83794\n",
            "60/60 - 0s - loss: 0.2074 - acc: 1.0000 - val_loss: 0.8504 - val_acc: 0.5500\n",
            "Epoch 115/300\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.83794\n",
            "60/60 - 0s - loss: 0.1730 - acc: 1.0000 - val_loss: 0.8503 - val_acc: 0.5500\n",
            "Epoch 116/300\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.83794\n",
            "60/60 - 0s - loss: 0.1949 - acc: 0.9833 - val_loss: 0.8511 - val_acc: 0.5000\n",
            "Epoch 117/300\n",
            "\n",
            "Epoch 00117: val_loss improved from 0.83794 to 0.83116, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2328 - acc: 0.9667 - val_loss: 0.8312 - val_acc: 0.6000\n",
            "Epoch 118/300\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.83116\n",
            "60/60 - 0s - loss: 0.1815 - acc: 0.9833 - val_loss: 0.8794 - val_acc: 0.5500\n",
            "Epoch 119/300\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 0.83116\n",
            "60/60 - 0s - loss: 0.1791 - acc: 1.0000 - val_loss: 0.9134 - val_acc: 0.5000\n",
            "Epoch 120/300\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.83116\n",
            "60/60 - 0s - loss: 0.1806 - acc: 1.0000 - val_loss: 0.8846 - val_acc: 0.5500\n",
            "Epoch 121/300\n",
            "\n",
            "Epoch 00121: val_loss improved from 0.83116 to 0.80399, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1680 - acc: 1.0000 - val_loss: 0.8040 - val_acc: 0.5500\n",
            "Epoch 122/300\n",
            "\n",
            "Epoch 00122: val_loss improved from 0.80399 to 0.76606, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1710 - acc: 1.0000 - val_loss: 0.7661 - val_acc: 0.6500\n",
            "Epoch 123/300\n",
            "\n",
            "Epoch 00123: val_loss improved from 0.76606 to 0.74086, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1887 - acc: 1.0000 - val_loss: 0.7409 - val_acc: 0.6500\n",
            "Epoch 124/300\n",
            "\n",
            "Epoch 00124: val_loss improved from 0.74086 to 0.74017, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1782 - acc: 1.0000 - val_loss: 0.7402 - val_acc: 0.6500\n",
            "Epoch 125/300\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 0.74017\n",
            "60/60 - 0s - loss: 0.1722 - acc: 1.0000 - val_loss: 0.7464 - val_acc: 0.6500\n",
            "Epoch 126/300\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 0.74017\n",
            "60/60 - 0s - loss: 0.2050 - acc: 0.9667 - val_loss: 0.7700 - val_acc: 0.6000\n",
            "Epoch 127/300\n",
            "\n",
            "Epoch 00127: val_loss improved from 0.74017 to 0.71695, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1531 - acc: 1.0000 - val_loss: 0.7170 - val_acc: 0.6500\n",
            "Epoch 128/300\n",
            "\n",
            "Epoch 00128: val_loss improved from 0.71695 to 0.67861, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1723 - acc: 1.0000 - val_loss: 0.6786 - val_acc: 0.7000\n",
            "Epoch 129/300\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.67861\n",
            "60/60 - 0s - loss: 0.1764 - acc: 1.0000 - val_loss: 0.7005 - val_acc: 0.6500\n",
            "Epoch 130/300\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 0.67861\n",
            "60/60 - 0s - loss: 0.1520 - acc: 0.9833 - val_loss: 0.7322 - val_acc: 0.6000\n",
            "Epoch 131/300\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 0.67861\n",
            "60/60 - 0s - loss: 0.1554 - acc: 0.9833 - val_loss: 0.7400 - val_acc: 0.6000\n",
            "Epoch 132/300\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 0.67861\n",
            "60/60 - 0s - loss: 0.1637 - acc: 1.0000 - val_loss: 0.7406 - val_acc: 0.6000\n",
            "Epoch 133/300\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.67861\n",
            "60/60 - 0s - loss: 0.1587 - acc: 1.0000 - val_loss: 0.7329 - val_acc: 0.6000\n",
            "Epoch 134/300\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.67861\n",
            "60/60 - 0s - loss: 0.1522 - acc: 1.0000 - val_loss: 0.7461 - val_acc: 0.6000\n",
            "Epoch 135/300\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 0.67861\n",
            "60/60 - 0s - loss: 0.1662 - acc: 0.9833 - val_loss: 0.7472 - val_acc: 0.6000\n",
            "Epoch 136/300\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.67861\n",
            "60/60 - 0s - loss: 0.1826 - acc: 0.9667 - val_loss: 0.7528 - val_acc: 0.6000\n",
            "Epoch 137/300\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 0.67861\n",
            "60/60 - 0s - loss: 0.1712 - acc: 1.0000 - val_loss: 0.7120 - val_acc: 0.6500\n",
            "Epoch 138/300\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.67861\n",
            "60/60 - 0s - loss: 0.1685 - acc: 1.0000 - val_loss: 0.7559 - val_acc: 0.6000\n",
            "Epoch 139/300\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.67861\n",
            "60/60 - 0s - loss: 0.1703 - acc: 0.9833 - val_loss: 0.7426 - val_acc: 0.6000\n",
            "Epoch 140/300\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 0.67861\n",
            "60/60 - 0s - loss: 0.1673 - acc: 0.9833 - val_loss: 0.7562 - val_acc: 0.6000\n",
            "Epoch 141/300\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.67861\n",
            "60/60 - 0s - loss: 0.1688 - acc: 0.9833 - val_loss: 0.7906 - val_acc: 0.5500\n",
            "Epoch 142/300\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.67861\n",
            "60/60 - 0s - loss: 0.1680 - acc: 1.0000 - val_loss: 0.7787 - val_acc: 0.5500\n",
            "Epoch 143/300\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 0.67861\n",
            "60/60 - 0s - loss: 0.1723 - acc: 1.0000 - val_loss: 0.7613 - val_acc: 0.6000\n",
            "Epoch 144/300\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 0.67861\n",
            "60/60 - 0s - loss: 0.1654 - acc: 1.0000 - val_loss: 0.7823 - val_acc: 0.5500\n",
            "Epoch 145/300\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.67861\n",
            "60/60 - 0s - loss: 0.1500 - acc: 0.9833 - val_loss: 0.7548 - val_acc: 0.5500\n",
            "Epoch 146/300\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.67861\n",
            "60/60 - 0s - loss: 0.1458 - acc: 1.0000 - val_loss: 0.7245 - val_acc: 0.6000\n",
            "Epoch 147/300\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.67861\n",
            "60/60 - 0s - loss: 0.1590 - acc: 1.0000 - val_loss: 0.7227 - val_acc: 0.6000\n",
            "Epoch 148/300\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 0.67861\n",
            "60/60 - 0s - loss: 0.1527 - acc: 1.0000 - val_loss: 0.7246 - val_acc: 0.6000\n",
            "Epoch 149/300\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.67861\n",
            "60/60 - 0s - loss: 0.1520 - acc: 0.9833 - val_loss: 0.7272 - val_acc: 0.6000\n",
            "Epoch 150/300\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.67861\n",
            "60/60 - 0s - loss: 0.1575 - acc: 0.9833 - val_loss: 0.7505 - val_acc: 0.5500\n",
            "Epoch 151/300\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.67861\n",
            "60/60 - 0s - loss: 0.1607 - acc: 1.0000 - val_loss: 0.7287 - val_acc: 0.6000\n",
            "Epoch 152/300\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.67861\n",
            "60/60 - 0s - loss: 0.1633 - acc: 1.0000 - val_loss: 0.7475 - val_acc: 0.5500\n",
            "Epoch 153/300\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 0.67861\n",
            "60/60 - 0s - loss: 0.1280 - acc: 1.0000 - val_loss: 0.7516 - val_acc: 0.6000\n",
            "Epoch 154/300\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.67861\n",
            "60/60 - 0s - loss: 0.1745 - acc: 0.9833 - val_loss: 0.7871 - val_acc: 0.6000\n",
            "Epoch 155/300\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.67861\n",
            "60/60 - 0s - loss: 0.1530 - acc: 0.9833 - val_loss: 0.7959 - val_acc: 0.6000\n",
            "Epoch 156/300\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.67861\n",
            "60/60 - 0s - loss: 0.1446 - acc: 1.0000 - val_loss: 0.7781 - val_acc: 0.6000\n",
            "Epoch 157/300\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.67861\n",
            "60/60 - 0s - loss: 0.1524 - acc: 1.0000 - val_loss: 0.7105 - val_acc: 0.6500\n",
            "Epoch 158/300\n",
            "\n",
            "Epoch 00158: val_loss improved from 0.67861 to 0.67372, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1511 - acc: 1.0000 - val_loss: 0.6737 - val_acc: 0.6500\n",
            "Epoch 159/300\n",
            "\n",
            "Epoch 00159: val_loss improved from 0.67372 to 0.67133, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1278 - acc: 1.0000 - val_loss: 0.6713 - val_acc: 0.6500\n",
            "Epoch 160/300\n",
            "\n",
            "Epoch 00160: val_loss improved from 0.67133 to 0.63703, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1409 - acc: 1.0000 - val_loss: 0.6370 - val_acc: 0.7000\n",
            "Epoch 161/300\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.63703\n",
            "60/60 - 0s - loss: 0.1318 - acc: 1.0000 - val_loss: 0.6377 - val_acc: 0.6500\n",
            "Epoch 162/300\n",
            "\n",
            "Epoch 00162: val_loss improved from 0.63703 to 0.62005, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1443 - acc: 0.9833 - val_loss: 0.6201 - val_acc: 0.6500\n",
            "Epoch 163/300\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.62005\n",
            "60/60 - 0s - loss: 0.1784 - acc: 0.9500 - val_loss: 0.6445 - val_acc: 0.6500\n",
            "Epoch 164/300\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.62005\n",
            "60/60 - 0s - loss: 0.1562 - acc: 1.0000 - val_loss: 0.6572 - val_acc: 0.7000\n",
            "Epoch 165/300\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.62005\n",
            "60/60 - 0s - loss: 0.1496 - acc: 1.0000 - val_loss: 0.6542 - val_acc: 0.6500\n",
            "Epoch 166/300\n",
            "\n",
            "Epoch 00166: val_loss improved from 0.62005 to 0.59678, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1400 - acc: 1.0000 - val_loss: 0.5968 - val_acc: 0.7500\n",
            "Epoch 167/300\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.59678\n",
            "60/60 - 0s - loss: 0.1513 - acc: 1.0000 - val_loss: 0.6303 - val_acc: 0.7500\n",
            "Epoch 168/300\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.59678\n",
            "60/60 - 0s - loss: 0.1568 - acc: 0.9833 - val_loss: 0.6800 - val_acc: 0.7000\n",
            "Epoch 169/300\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 0.59678\n",
            "60/60 - 0s - loss: 0.1491 - acc: 1.0000 - val_loss: 0.7364 - val_acc: 0.5500\n",
            "Epoch 170/300\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.59678\n",
            "60/60 - 0s - loss: 0.1235 - acc: 1.0000 - val_loss: 0.7695 - val_acc: 0.5500\n",
            "Epoch 171/300\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.59678\n",
            "60/60 - 0s - loss: 0.1209 - acc: 1.0000 - val_loss: 0.7936 - val_acc: 0.5500\n",
            "Epoch 172/300\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.59678\n",
            "60/60 - 0s - loss: 0.1387 - acc: 0.9833 - val_loss: 0.8292 - val_acc: 0.5500\n",
            "Epoch 173/300\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.59678\n",
            "60/60 - 0s - loss: 0.1254 - acc: 1.0000 - val_loss: 0.8265 - val_acc: 0.5500\n",
            "Epoch 174/300\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.59678\n",
            "60/60 - 0s - loss: 0.1362 - acc: 1.0000 - val_loss: 0.7917 - val_acc: 0.5500\n",
            "Epoch 175/300\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.59678\n",
            "60/60 - 0s - loss: 0.1549 - acc: 1.0000 - val_loss: 0.7472 - val_acc: 0.6000\n",
            "Epoch 176/300\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 0.59678\n",
            "60/60 - 0s - loss: 0.1434 - acc: 1.0000 - val_loss: 0.7036 - val_acc: 0.6500\n",
            "Epoch 177/300\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.59678\n",
            "60/60 - 0s - loss: 0.1245 - acc: 1.0000 - val_loss: 0.6882 - val_acc: 0.6500\n",
            "Epoch 178/300\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.59678\n",
            "60/60 - 0s - loss: 0.1411 - acc: 0.9833 - val_loss: 0.6687 - val_acc: 0.7000\n",
            "Epoch 179/300\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.59678\n",
            "60/60 - 0s - loss: 0.1377 - acc: 1.0000 - val_loss: 0.6941 - val_acc: 0.6500\n",
            "Epoch 180/300\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.59678\n",
            "60/60 - 0s - loss: 0.1232 - acc: 1.0000 - val_loss: 0.7858 - val_acc: 0.6000\n",
            "Epoch 181/300\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.59678\n",
            "60/60 - 0s - loss: 0.1275 - acc: 1.0000 - val_loss: 0.8536 - val_acc: 0.5000\n",
            "Epoch 182/300\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.59678\n",
            "60/60 - 0s - loss: 0.1237 - acc: 1.0000 - val_loss: 0.8753 - val_acc: 0.5500\n",
            "Epoch 183/300\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.59678\n",
            "60/60 - 0s - loss: 0.1234 - acc: 1.0000 - val_loss: 0.8522 - val_acc: 0.5500\n",
            "Epoch 184/300\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 0.59678\n",
            "60/60 - 0s - loss: 0.1374 - acc: 0.9833 - val_loss: 0.7616 - val_acc: 0.6000\n",
            "Epoch 185/300\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.59678\n",
            "60/60 - 0s - loss: 0.1259 - acc: 1.0000 - val_loss: 0.7371 - val_acc: 0.6500\n",
            "Epoch 186/300\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.59678\n",
            "60/60 - 0s - loss: 0.1287 - acc: 1.0000 - val_loss: 0.7304 - val_acc: 0.6500\n",
            "Epoch 187/300\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 0.59678\n",
            "60/60 - 0s - loss: 0.1452 - acc: 0.9833 - val_loss: 0.7121 - val_acc: 0.6500\n",
            "Epoch 188/300\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.59678\n",
            "60/60 - 0s - loss: 0.1268 - acc: 1.0000 - val_loss: 0.6619 - val_acc: 0.6500\n",
            "Epoch 189/300\n",
            "\n",
            "Epoch 00189: val_loss improved from 0.59678 to 0.59282, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1232 - acc: 1.0000 - val_loss: 0.5928 - val_acc: 0.7000\n",
            "Epoch 190/300\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.59282\n",
            "60/60 - 0s - loss: 0.1300 - acc: 1.0000 - val_loss: 0.6359 - val_acc: 0.6500\n",
            "Epoch 191/300\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.59282\n",
            "60/60 - 0s - loss: 0.1218 - acc: 1.0000 - val_loss: 0.6383 - val_acc: 0.6500\n",
            "Epoch 192/300\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 0.59282\n",
            "60/60 - 0s - loss: 0.1467 - acc: 0.9667 - val_loss: 0.7195 - val_acc: 0.6500\n",
            "Epoch 193/300\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.59282\n",
            "60/60 - 0s - loss: 0.1486 - acc: 0.9833 - val_loss: 0.7550 - val_acc: 0.6000\n",
            "Epoch 194/300\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.59282\n",
            "60/60 - 0s - loss: 0.1424 - acc: 0.9833 - val_loss: 0.7487 - val_acc: 0.5500\n",
            "Epoch 195/300\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.59282\n",
            "60/60 - 0s - loss: 0.1526 - acc: 0.9833 - val_loss: 0.7732 - val_acc: 0.6000\n",
            "Epoch 196/300\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.59282\n",
            "60/60 - 0s - loss: 0.1391 - acc: 1.0000 - val_loss: 0.7729 - val_acc: 0.6000\n",
            "Epoch 197/300\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.59282\n",
            "60/60 - 0s - loss: 0.1622 - acc: 0.9667 - val_loss: 0.8093 - val_acc: 0.5500\n",
            "Epoch 198/300\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.59282\n",
            "60/60 - 0s - loss: 0.1375 - acc: 1.0000 - val_loss: 0.8580 - val_acc: 0.5500\n",
            "Epoch 199/300\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.59282\n",
            "60/60 - 0s - loss: 0.1179 - acc: 1.0000 - val_loss: 0.9271 - val_acc: 0.5500\n",
            "Epoch 200/300\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.59282\n",
            "60/60 - 0s - loss: 0.1287 - acc: 1.0000 - val_loss: 0.8659 - val_acc: 0.5500\n",
            "Epoch 201/300\n",
            "\n",
            "Epoch 00201: val_loss did not improve from 0.59282\n",
            "60/60 - 0s - loss: 0.1278 - acc: 0.9833 - val_loss: 0.8335 - val_acc: 0.5500\n",
            "Epoch 202/300\n",
            "\n",
            "Epoch 00202: val_loss did not improve from 0.59282\n",
            "60/60 - 0s - loss: 0.1248 - acc: 1.0000 - val_loss: 0.7859 - val_acc: 0.6000\n",
            "Epoch 203/300\n",
            "\n",
            "Epoch 00203: val_loss did not improve from 0.59282\n",
            "60/60 - 0s - loss: 0.1306 - acc: 1.0000 - val_loss: 0.6748 - val_acc: 0.6000\n",
            "Epoch 204/300\n",
            "\n",
            "Epoch 00204: val_loss did not improve from 0.59282\n",
            "60/60 - 0s - loss: 0.1411 - acc: 0.9833 - val_loss: 0.6183 - val_acc: 0.7500\n",
            "Epoch 205/300\n",
            "\n",
            "Epoch 00205: val_loss did not improve from 0.59282\n",
            "60/60 - 0s - loss: 0.1218 - acc: 1.0000 - val_loss: 0.6134 - val_acc: 0.7500\n",
            "Epoch 206/300\n",
            "\n",
            "Epoch 00206: val_loss did not improve from 0.59282\n",
            "60/60 - 0s - loss: 0.1215 - acc: 1.0000 - val_loss: 0.6461 - val_acc: 0.6500\n",
            "Epoch 207/300\n",
            "\n",
            "Epoch 00207: val_loss did not improve from 0.59282\n",
            "60/60 - 0s - loss: 0.1238 - acc: 1.0000 - val_loss: 0.7276 - val_acc: 0.6000\n",
            "Epoch 208/300\n",
            "\n",
            "Epoch 00208: val_loss did not improve from 0.59282\n",
            "60/60 - 0s - loss: 0.1193 - acc: 1.0000 - val_loss: 0.7951 - val_acc: 0.5500\n",
            "Epoch 209/300\n",
            "\n",
            "Epoch 00209: val_loss did not improve from 0.59282\n",
            "60/60 - 0s - loss: 0.1207 - acc: 1.0000 - val_loss: 0.7488 - val_acc: 0.5500\n",
            "Epoch 210/300\n",
            "\n",
            "Epoch 00210: val_loss did not improve from 0.59282\n",
            "60/60 - 0s - loss: 0.1263 - acc: 1.0000 - val_loss: 0.7312 - val_acc: 0.5500\n",
            "Epoch 211/300\n",
            "\n",
            "Epoch 00211: val_loss did not improve from 0.59282\n",
            "60/60 - 0s - loss: 0.1078 - acc: 1.0000 - val_loss: 0.7003 - val_acc: 0.6500\n",
            "Epoch 212/300\n",
            "\n",
            "Epoch 00212: val_loss did not improve from 0.59282\n",
            "60/60 - 0s - loss: 0.1077 - acc: 1.0000 - val_loss: 0.7450 - val_acc: 0.5500\n",
            "Epoch 213/300\n",
            "\n",
            "Epoch 00213: val_loss did not improve from 0.59282\n",
            "60/60 - 0s - loss: 0.1200 - acc: 1.0000 - val_loss: 0.8008 - val_acc: 0.5500\n",
            "Epoch 214/300\n",
            "\n",
            "Epoch 00214: val_loss did not improve from 0.59282\n",
            "60/60 - 0s - loss: 0.1240 - acc: 1.0000 - val_loss: 0.8643 - val_acc: 0.5500\n",
            "Epoch 215/300\n",
            "\n",
            "Epoch 00215: val_loss did not improve from 0.59282\n",
            "60/60 - 0s - loss: 0.1087 - acc: 1.0000 - val_loss: 0.8012 - val_acc: 0.5500\n",
            "Epoch 216/300\n",
            "\n",
            "Epoch 00216: val_loss did not improve from 0.59282\n",
            "60/60 - 0s - loss: 0.1069 - acc: 1.0000 - val_loss: 0.7107 - val_acc: 0.5500\n",
            "Epoch 217/300\n",
            "\n",
            "Epoch 00217: val_loss did not improve from 0.59282\n",
            "60/60 - 0s - loss: 0.1415 - acc: 0.9833 - val_loss: 0.6677 - val_acc: 0.6500\n",
            "Epoch 218/300\n",
            "\n",
            "Epoch 00218: val_loss did not improve from 0.59282\n",
            "60/60 - 0s - loss: 0.1236 - acc: 1.0000 - val_loss: 0.6220 - val_acc: 0.8000\n",
            "Epoch 219/300\n",
            "\n",
            "Epoch 00219: val_loss did not improve from 0.59282\n",
            "60/60 - 0s - loss: 0.1155 - acc: 1.0000 - val_loss: 0.7240 - val_acc: 0.6000\n",
            "Epoch 220/300\n",
            "\n",
            "Epoch 00220: val_loss did not improve from 0.59282\n",
            "60/60 - 0s - loss: 0.1339 - acc: 1.0000 - val_loss: 0.8324 - val_acc: 0.5500\n",
            "Epoch 221/300\n",
            "\n",
            "Epoch 00221: val_loss did not improve from 0.59282\n",
            "60/60 - 0s - loss: 0.1186 - acc: 0.9833 - val_loss: 0.8721 - val_acc: 0.5500\n",
            "Epoch 222/300\n",
            "\n",
            "Epoch 00222: val_loss did not improve from 0.59282\n",
            "60/60 - 0s - loss: 0.1293 - acc: 0.9667 - val_loss: 0.7887 - val_acc: 0.5500\n",
            "Epoch 223/300\n",
            "\n",
            "Epoch 00223: val_loss did not improve from 0.59282\n",
            "60/60 - 0s - loss: 0.1139 - acc: 0.9833 - val_loss: 0.6746 - val_acc: 0.6500\n",
            "Epoch 224/300\n",
            "\n",
            "Epoch 00224: val_loss did not improve from 0.59282\n",
            "60/60 - 0s - loss: 0.1195 - acc: 1.0000 - val_loss: 0.7400 - val_acc: 0.5500\n",
            "Epoch 225/300\n",
            "\n",
            "Epoch 00225: val_loss did not improve from 0.59282\n",
            "60/60 - 0s - loss: 0.1108 - acc: 0.9833 - val_loss: 0.8639 - val_acc: 0.5500\n",
            "Epoch 226/300\n",
            "\n",
            "Epoch 00226: val_loss did not improve from 0.59282\n",
            "60/60 - 0s - loss: 0.1344 - acc: 1.0000 - val_loss: 0.9173 - val_acc: 0.5000\n",
            "Epoch 227/300\n",
            "\n",
            "Epoch 00227: val_loss did not improve from 0.59282\n",
            "60/60 - 0s - loss: 0.1215 - acc: 0.9833 - val_loss: 0.9229 - val_acc: 0.5000\n",
            "Epoch 228/300\n",
            "\n",
            "Epoch 00228: val_loss did not improve from 0.59282\n",
            "60/60 - 0s - loss: 0.1205 - acc: 0.9833 - val_loss: 0.9072 - val_acc: 0.5000\n",
            "Epoch 229/300\n",
            "\n",
            "Epoch 00229: val_loss did not improve from 0.59282\n",
            "60/60 - 0s - loss: 0.0981 - acc: 1.0000 - val_loss: 0.8247 - val_acc: 0.5000\n",
            "Epoch 230/300\n",
            "\n",
            "Epoch 00230: val_loss did not improve from 0.59282\n",
            "60/60 - 0s - loss: 0.0928 - acc: 1.0000 - val_loss: 0.7862 - val_acc: 0.6000\n",
            "Epoch 231/300\n",
            "\n",
            "Epoch 00231: val_loss did not improve from 0.59282\n",
            "60/60 - 0s - loss: 0.0978 - acc: 1.0000 - val_loss: 0.7997 - val_acc: 0.6000\n",
            "Epoch 232/300\n",
            "\n",
            "Epoch 00232: val_loss did not improve from 0.59282\n",
            "60/60 - 0s - loss: 0.1147 - acc: 1.0000 - val_loss: 0.8469 - val_acc: 0.5500\n",
            "Epoch 233/300\n",
            "\n",
            "Epoch 00233: val_loss did not improve from 0.59282\n",
            "60/60 - 0s - loss: 0.1177 - acc: 1.0000 - val_loss: 0.7823 - val_acc: 0.6000\n",
            "Epoch 234/300\n",
            "\n",
            "Epoch 00234: val_loss did not improve from 0.59282\n",
            "60/60 - 0s - loss: 0.1119 - acc: 1.0000 - val_loss: 0.7070 - val_acc: 0.6000\n",
            "Epoch 235/300\n",
            "\n",
            "Epoch 00235: val_loss did not improve from 0.59282\n",
            "60/60 - 0s - loss: 0.1079 - acc: 1.0000 - val_loss: 0.7932 - val_acc: 0.6000\n",
            "Epoch 236/300\n",
            "\n",
            "Epoch 00236: val_loss did not improve from 0.59282\n",
            "60/60 - 0s - loss: 0.1272 - acc: 0.9667 - val_loss: 0.8711 - val_acc: 0.5500\n",
            "Epoch 237/300\n",
            "\n",
            "Epoch 00237: val_loss did not improve from 0.59282\n",
            "60/60 - 0s - loss: 0.1078 - acc: 1.0000 - val_loss: 0.9543 - val_acc: 0.5000\n",
            "Epoch 238/300\n",
            "\n",
            "Epoch 00238: val_loss did not improve from 0.59282\n",
            "60/60 - 0s - loss: 0.0970 - acc: 1.0000 - val_loss: 0.8934 - val_acc: 0.5000\n",
            "Epoch 239/300\n",
            "\n",
            "Epoch 00239: val_loss did not improve from 0.59282\n",
            "60/60 - 0s - loss: 0.0878 - acc: 1.0000 - val_loss: 0.8186 - val_acc: 0.5500\n",
            "Epoch 240/300\n",
            "\n",
            "Epoch 00240: val_loss did not improve from 0.59282\n",
            "60/60 - 0s - loss: 0.1150 - acc: 1.0000 - val_loss: 0.7574 - val_acc: 0.6000\n",
            "Epoch 241/300\n",
            "\n",
            "Epoch 00241: val_loss did not improve from 0.59282\n",
            "60/60 - 0s - loss: 0.1288 - acc: 0.9833 - val_loss: 0.7393 - val_acc: 0.6000\n",
            "Epoch 242/300\n",
            "\n",
            "Epoch 00242: val_loss did not improve from 0.59282\n",
            "60/60 - 0s - loss: 0.0938 - acc: 1.0000 - val_loss: 0.6987 - val_acc: 0.6500\n",
            "Epoch 243/300\n",
            "\n",
            "Epoch 00243: val_loss did not improve from 0.59282\n",
            "60/60 - 0s - loss: 0.1127 - acc: 1.0000 - val_loss: 0.7159 - val_acc: 0.6500\n",
            "Epoch 244/300\n",
            "\n",
            "Epoch 00244: val_loss did not improve from 0.59282\n",
            "60/60 - 0s - loss: 0.1214 - acc: 1.0000 - val_loss: 0.7554 - val_acc: 0.6000\n",
            "Epoch 245/300\n",
            "\n",
            "Epoch 00245: val_loss did not improve from 0.59282\n",
            "60/60 - 0s - loss: 0.1121 - acc: 0.9833 - val_loss: 0.8032 - val_acc: 0.6000\n",
            "Epoch 246/300\n",
            "\n",
            "Epoch 00246: val_loss did not improve from 0.59282\n",
            "60/60 - 0s - loss: 0.1115 - acc: 1.0000 - val_loss: 0.7958 - val_acc: 0.6000\n",
            "Epoch 247/300\n",
            "\n",
            "Epoch 00247: val_loss did not improve from 0.59282\n",
            "60/60 - 0s - loss: 0.0988 - acc: 1.0000 - val_loss: 0.8018 - val_acc: 0.6000\n",
            "Epoch 248/300\n",
            "\n",
            "Epoch 00248: val_loss did not improve from 0.59282\n",
            "60/60 - 0s - loss: 0.0973 - acc: 1.0000 - val_loss: 0.8499 - val_acc: 0.6000\n",
            "Epoch 249/300\n",
            "\n",
            "Epoch 00249: val_loss did not improve from 0.59282\n",
            "60/60 - 0s - loss: 0.1093 - acc: 1.0000 - val_loss: 0.8337 - val_acc: 0.6000\n",
            "Epoch 250/300\n",
            "\n",
            "Epoch 00250: val_loss did not improve from 0.59282\n",
            "60/60 - 0s - loss: 0.1072 - acc: 0.9833 - val_loss: 0.8356 - val_acc: 0.5500\n",
            "Epoch 251/300\n",
            "\n",
            "Epoch 00251: val_loss did not improve from 0.59282\n",
            "60/60 - 0s - loss: 0.0956 - acc: 1.0000 - val_loss: 0.8154 - val_acc: 0.6000\n",
            "Epoch 252/300\n",
            "\n",
            "Epoch 00252: val_loss did not improve from 0.59282\n",
            "60/60 - 0s - loss: 0.1086 - acc: 0.9833 - val_loss: 0.8226 - val_acc: 0.6000\n",
            "Epoch 253/300\n",
            "\n",
            "Epoch 00253: val_loss did not improve from 0.59282\n",
            "60/60 - 0s - loss: 0.1106 - acc: 0.9833 - val_loss: 0.8354 - val_acc: 0.5500\n",
            "Epoch 254/300\n",
            "\n",
            "Epoch 00254: val_loss did not improve from 0.59282\n",
            "60/60 - 0s - loss: 0.0898 - acc: 1.0000 - val_loss: 0.8805 - val_acc: 0.5000\n",
            "Epoch 255/300\n",
            "\n",
            "Epoch 00255: val_loss did not improve from 0.59282\n",
            "60/60 - 0s - loss: 0.1060 - acc: 1.0000 - val_loss: 0.7897 - val_acc: 0.6000\n",
            "Epoch 256/300\n",
            "\n",
            "Epoch 00256: val_loss did not improve from 0.59282\n",
            "60/60 - 0s - loss: 0.1169 - acc: 1.0000 - val_loss: 0.7022 - val_acc: 0.7000\n",
            "Epoch 257/300\n",
            "\n",
            "Epoch 00257: val_loss did not improve from 0.59282\n",
            "60/60 - 0s - loss: 0.1067 - acc: 1.0000 - val_loss: 0.6589 - val_acc: 0.7000\n",
            "Epoch 258/300\n",
            "\n",
            "Epoch 00258: val_loss did not improve from 0.59282\n",
            "60/60 - 0s - loss: 0.1040 - acc: 1.0000 - val_loss: 0.7586 - val_acc: 0.7000\n",
            "Epoch 259/300\n",
            "\n",
            "Epoch 00259: val_loss did not improve from 0.59282\n",
            "60/60 - 0s - loss: 0.0984 - acc: 1.0000 - val_loss: 0.9182 - val_acc: 0.6000\n",
            "Epoch 260/300\n",
            "\n",
            "Epoch 00260: val_loss did not improve from 0.59282\n",
            "60/60 - 0s - loss: 0.1261 - acc: 0.9833 - val_loss: 0.9011 - val_acc: 0.6000\n",
            "Epoch 261/300\n",
            "\n",
            "Epoch 00261: val_loss did not improve from 0.59282\n",
            "60/60 - 0s - loss: 0.1107 - acc: 1.0000 - val_loss: 0.7497 - val_acc: 0.7000\n",
            "Epoch 262/300\n",
            "\n",
            "Epoch 00262: val_loss did not improve from 0.59282\n",
            "60/60 - 0s - loss: 0.1044 - acc: 0.9833 - val_loss: 0.6615 - val_acc: 0.7000\n",
            "Epoch 263/300\n",
            "\n",
            "Epoch 00263: val_loss did not improve from 0.59282\n",
            "60/60 - 0s - loss: 0.1206 - acc: 0.9833 - val_loss: 0.6797 - val_acc: 0.7500\n",
            "Epoch 264/300\n",
            "\n",
            "Epoch 00264: val_loss did not improve from 0.59282\n",
            "60/60 - 0s - loss: 0.0827 - acc: 1.0000 - val_loss: 0.7606 - val_acc: 0.6500\n",
            "Epoch 265/300\n",
            "\n",
            "Epoch 00265: val_loss did not improve from 0.59282\n",
            "60/60 - 0s - loss: 0.0948 - acc: 1.0000 - val_loss: 0.8463 - val_acc: 0.5000\n",
            "Epoch 266/300\n",
            "\n",
            "Epoch 00266: val_loss did not improve from 0.59282\n",
            "60/60 - 0s - loss: 0.0871 - acc: 1.0000 - val_loss: 0.8007 - val_acc: 0.6000\n",
            "Epoch 267/300\n",
            "\n",
            "Epoch 00267: val_loss did not improve from 0.59282\n",
            "60/60 - 0s - loss: 0.0921 - acc: 1.0000 - val_loss: 0.8080 - val_acc: 0.6000\n",
            "Epoch 268/300\n",
            "\n",
            "Epoch 00268: val_loss did not improve from 0.59282\n",
            "60/60 - 0s - loss: 0.1037 - acc: 1.0000 - val_loss: 0.8055 - val_acc: 0.6000\n",
            "Epoch 269/300\n",
            "\n",
            "Epoch 00269: val_loss did not improve from 0.59282\n",
            "60/60 - 0s - loss: 0.1169 - acc: 0.9833 - val_loss: 0.8355 - val_acc: 0.6000\n",
            "Epoch 270/300\n",
            "\n",
            "Epoch 00270: val_loss did not improve from 0.59282\n",
            "60/60 - 0s - loss: 0.0850 - acc: 1.0000 - val_loss: 0.8440 - val_acc: 0.6000\n",
            "Epoch 271/300\n",
            "\n",
            "Epoch 00271: val_loss did not improve from 0.59282\n",
            "60/60 - 0s - loss: 0.1108 - acc: 0.9667 - val_loss: 0.8896 - val_acc: 0.6000\n",
            "Epoch 272/300\n",
            "\n",
            "Epoch 00272: val_loss did not improve from 0.59282\n",
            "60/60 - 0s - loss: 0.1212 - acc: 1.0000 - val_loss: 0.8231 - val_acc: 0.6500\n",
            "Epoch 273/300\n",
            "\n",
            "Epoch 00273: val_loss did not improve from 0.59282\n",
            "60/60 - 0s - loss: 0.0940 - acc: 1.0000 - val_loss: 0.7981 - val_acc: 0.6500\n",
            "Epoch 274/300\n",
            "\n",
            "Epoch 00274: val_loss did not improve from 0.59282\n",
            "60/60 - 0s - loss: 0.1076 - acc: 0.9833 - val_loss: 0.7471 - val_acc: 0.6500\n",
            "Epoch 275/300\n",
            "\n",
            "Epoch 00275: val_loss did not improve from 0.59282\n",
            "60/60 - 0s - loss: 0.0831 - acc: 1.0000 - val_loss: 0.7612 - val_acc: 0.6500\n",
            "Epoch 276/300\n",
            "\n",
            "Epoch 00276: val_loss did not improve from 0.59282\n",
            "60/60 - 0s - loss: 0.0980 - acc: 1.0000 - val_loss: 0.8178 - val_acc: 0.6000\n",
            "Epoch 277/300\n",
            "\n",
            "Epoch 00277: val_loss did not improve from 0.59282\n",
            "60/60 - 0s - loss: 0.0847 - acc: 1.0000 - val_loss: 0.8042 - val_acc: 0.6000\n",
            "Epoch 278/300\n",
            "\n",
            "Epoch 00278: val_loss did not improve from 0.59282\n",
            "60/60 - 0s - loss: 0.0961 - acc: 0.9833 - val_loss: 0.7201 - val_acc: 0.6000\n",
            "Epoch 279/300\n",
            "\n",
            "Epoch 00279: val_loss did not improve from 0.59282\n",
            "60/60 - 0s - loss: 0.0975 - acc: 1.0000 - val_loss: 0.6783 - val_acc: 0.6500\n",
            "Epoch 280/300\n",
            "\n",
            "Epoch 00280: val_loss did not improve from 0.59282\n",
            "60/60 - 0s - loss: 0.0804 - acc: 1.0000 - val_loss: 0.6255 - val_acc: 0.7000\n",
            "Epoch 281/300\n",
            "\n",
            "Epoch 00281: val_loss did not improve from 0.59282\n",
            "60/60 - 0s - loss: 0.0857 - acc: 1.0000 - val_loss: 0.6426 - val_acc: 0.6500\n",
            "Epoch 282/300\n",
            "\n",
            "Epoch 00282: val_loss did not improve from 0.59282\n",
            "60/60 - 0s - loss: 0.1074 - acc: 0.9833 - val_loss: 0.7609 - val_acc: 0.6000\n",
            "Epoch 283/300\n",
            "\n",
            "Epoch 00283: val_loss did not improve from 0.59282\n",
            "60/60 - 0s - loss: 0.0802 - acc: 1.0000 - val_loss: 0.8095 - val_acc: 0.6000\n",
            "Epoch 284/300\n",
            "\n",
            "Epoch 00284: val_loss did not improve from 0.59282\n",
            "60/60 - 0s - loss: 0.0910 - acc: 1.0000 - val_loss: 0.8085 - val_acc: 0.6000\n",
            "Epoch 285/300\n",
            "\n",
            "Epoch 00285: val_loss did not improve from 0.59282\n",
            "60/60 - 0s - loss: 0.0912 - acc: 0.9833 - val_loss: 0.7506 - val_acc: 0.6000\n",
            "Epoch 286/300\n",
            "\n",
            "Epoch 00286: val_loss did not improve from 0.59282\n",
            "60/60 - 0s - loss: 0.1155 - acc: 0.9667 - val_loss: 0.8197 - val_acc: 0.6000\n",
            "Epoch 287/300\n",
            "\n",
            "Epoch 00287: val_loss did not improve from 0.59282\n",
            "60/60 - 0s - loss: 0.1097 - acc: 0.9833 - val_loss: 0.7453 - val_acc: 0.6500\n",
            "Epoch 288/300\n",
            "\n",
            "Epoch 00288: val_loss did not improve from 0.59282\n",
            "60/60 - 0s - loss: 0.0867 - acc: 1.0000 - val_loss: 0.7415 - val_acc: 0.6500\n",
            "Epoch 289/300\n",
            "\n",
            "Epoch 00289: val_loss did not improve from 0.59282\n",
            "60/60 - 0s - loss: 0.0827 - acc: 1.0000 - val_loss: 0.8281 - val_acc: 0.6500\n",
            "Epoch 290/300\n",
            "\n",
            "Epoch 00290: val_loss did not improve from 0.59282\n",
            "60/60 - 0s - loss: 0.0905 - acc: 1.0000 - val_loss: 0.9439 - val_acc: 0.5500\n",
            "Epoch 291/300\n",
            "\n",
            "Epoch 00291: val_loss did not improve from 0.59282\n",
            "60/60 - 0s - loss: 0.0889 - acc: 1.0000 - val_loss: 0.9099 - val_acc: 0.5500\n",
            "Epoch 292/300\n",
            "\n",
            "Epoch 00292: val_loss did not improve from 0.59282\n",
            "60/60 - 0s - loss: 0.0838 - acc: 1.0000 - val_loss: 0.8390 - val_acc: 0.6000\n",
            "Epoch 293/300\n",
            "\n",
            "Epoch 00293: val_loss did not improve from 0.59282\n",
            "60/60 - 0s - loss: 0.0713 - acc: 1.0000 - val_loss: 0.8920 - val_acc: 0.5000\n",
            "Epoch 294/300\n",
            "\n",
            "Epoch 00294: val_loss did not improve from 0.59282\n",
            "60/60 - 0s - loss: 0.0895 - acc: 1.0000 - val_loss: 0.9398 - val_acc: 0.4500\n",
            "Epoch 295/300\n",
            "\n",
            "Epoch 00295: val_loss did not improve from 0.59282\n",
            "60/60 - 0s - loss: 0.0881 - acc: 1.0000 - val_loss: 0.9244 - val_acc: 0.4500\n",
            "Epoch 296/300\n",
            "\n",
            "Epoch 00296: val_loss did not improve from 0.59282\n",
            "60/60 - 0s - loss: 0.0897 - acc: 1.0000 - val_loss: 0.8325 - val_acc: 0.6500\n",
            "Epoch 297/300\n",
            "\n",
            "Epoch 00297: val_loss did not improve from 0.59282\n",
            "60/60 - 0s - loss: 0.0814 - acc: 1.0000 - val_loss: 0.8178 - val_acc: 0.6500\n",
            "Epoch 298/300\n",
            "\n",
            "Epoch 00298: val_loss did not improve from 0.59282\n",
            "60/60 - 0s - loss: 0.0920 - acc: 1.0000 - val_loss: 0.8288 - val_acc: 0.6500\n",
            "Epoch 299/300\n",
            "\n",
            "Epoch 00299: val_loss did not improve from 0.59282\n",
            "60/60 - 0s - loss: 0.1036 - acc: 0.9833 - val_loss: 0.8962 - val_acc: 0.6000\n",
            "Epoch 300/300\n",
            "\n",
            "Epoch 00300: val_loss did not improve from 0.59282\n",
            "60/60 - 0s - loss: 0.0861 - acc: 1.0000 - val_loss: 0.9729 - val_acc: 0.5500\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "# Evaluate on test data\n",
            "40/40 [==============================] - 0s 2ms/sample - loss: 0.9016 - acc: 0.5500\n",
            "test loss, test acc: [0.9016194310970604, 0.55]\n",
            "EEG_Deep/Data2A/parsed_P03T.mat\n",
            "Filtering of Training Data Finished\n",
            "EEG_Deep/Data2A/parsed_P03E.mat\n",
            "Filtering of Testing Data Finished\n",
            "(1, 2)\n",
            "Finding labels in training data\n",
            "Finding labels in testing data\n",
            "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
            "(60, 12, 1536)\n",
            "(60,)\n",
            "(20, 12, 1536)\n",
            "(20,)\n",
            "[[1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]]\n",
            "X_train shape: (60, 1, 12, 1536)\n",
            "60 train samples\n",
            "20 val samples\n",
            "(40, 12, 1536)\n",
            "(40, 1)\n",
            "X_train shape: (40, 1, 12, 1536)\n",
            "40 train samples\n",
            "Train on 60 samples, validate on 20 samples\n",
            "Epoch 1/300\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 1.37806, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 1s - loss: 1.3187 - acc: 0.3667 - val_loss: 1.3781 - val_acc: 0.5000\n",
            "Epoch 2/300\n",
            "\n",
            "Epoch 00002: val_loss improved from 1.37806 to 1.36800, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 1.1242 - acc: 0.7167 - val_loss: 1.3680 - val_acc: 0.4500\n",
            "Epoch 3/300\n",
            "\n",
            "Epoch 00003: val_loss improved from 1.36800 to 1.35456, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 1.0188 - acc: 0.8000 - val_loss: 1.3546 - val_acc: 0.0500\n",
            "Epoch 4/300\n",
            "\n",
            "Epoch 00004: val_loss improved from 1.35456 to 1.33804, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.9330 - acc: 0.7667 - val_loss: 1.3380 - val_acc: 0.0500\n",
            "Epoch 5/300\n",
            "\n",
            "Epoch 00005: val_loss improved from 1.33804 to 1.32225, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.8866 - acc: 0.7333 - val_loss: 1.3223 - val_acc: 0.0000e+00\n",
            "Epoch 6/300\n",
            "\n",
            "Epoch 00006: val_loss improved from 1.32225 to 1.30937, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.7671 - acc: 0.8000 - val_loss: 1.3094 - val_acc: 0.0000e+00\n",
            "Epoch 7/300\n",
            "\n",
            "Epoch 00007: val_loss improved from 1.30937 to 1.30210, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.7519 - acc: 0.8333 - val_loss: 1.3021 - val_acc: 0.0000e+00\n",
            "Epoch 8/300\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.6633 - acc: 0.8167 - val_loss: 1.3036 - val_acc: 0.0000e+00\n",
            "Epoch 9/300\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.6388 - acc: 0.8333 - val_loss: 1.3078 - val_acc: 0.0000e+00\n",
            "Epoch 10/300\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.5960 - acc: 0.8333 - val_loss: 1.3198 - val_acc: 0.0000e+00\n",
            "Epoch 11/300\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.6358 - acc: 0.8000 - val_loss: 1.3355 - val_acc: 0.0000e+00\n",
            "Epoch 12/300\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.5771 - acc: 0.8167 - val_loss: 1.3471 - val_acc: 0.0000e+00\n",
            "Epoch 13/300\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.5611 - acc: 0.7667 - val_loss: 1.3600 - val_acc: 0.0000e+00\n",
            "Epoch 14/300\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.5576 - acc: 0.8167 - val_loss: 1.3642 - val_acc: 0.0000e+00\n",
            "Epoch 15/300\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.5608 - acc: 0.8000 - val_loss: 1.3651 - val_acc: 0.0000e+00\n",
            "Epoch 16/300\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.5449 - acc: 0.8167 - val_loss: 1.3752 - val_acc: 0.0000e+00\n",
            "Epoch 17/300\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.5615 - acc: 0.7833 - val_loss: 1.3758 - val_acc: 0.0000e+00\n",
            "Epoch 18/300\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.5376 - acc: 0.8167 - val_loss: 1.3846 - val_acc: 0.0000e+00\n",
            "Epoch 19/300\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.5052 - acc: 0.8000 - val_loss: 1.3908 - val_acc: 0.0000e+00\n",
            "Epoch 20/300\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.5074 - acc: 0.8333 - val_loss: 1.4030 - val_acc: 0.0000e+00\n",
            "Epoch 21/300\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.5101 - acc: 0.7833 - val_loss: 1.4092 - val_acc: 0.0000e+00\n",
            "Epoch 22/300\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.4601 - acc: 0.8333 - val_loss: 1.4285 - val_acc: 0.0000e+00\n",
            "Epoch 23/300\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.5017 - acc: 0.8333 - val_loss: 1.4356 - val_acc: 0.0000e+00\n",
            "Epoch 24/300\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.4925 - acc: 0.8333 - val_loss: 1.4445 - val_acc: 0.0000e+00\n",
            "Epoch 25/300\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.4530 - acc: 0.8500 - val_loss: 1.4788 - val_acc: 0.0000e+00\n",
            "Epoch 26/300\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.4570 - acc: 0.8167 - val_loss: 1.5034 - val_acc: 0.0000e+00\n",
            "Epoch 27/300\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.4524 - acc: 0.8500 - val_loss: 1.5301 - val_acc: 0.0000e+00\n",
            "Epoch 28/300\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.4476 - acc: 0.8667 - val_loss: 1.5446 - val_acc: 0.0000e+00\n",
            "Epoch 29/300\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.4593 - acc: 0.8000 - val_loss: 1.5720 - val_acc: 0.0000e+00\n",
            "Epoch 30/300\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.4280 - acc: 0.8667 - val_loss: 1.6173 - val_acc: 0.0000e+00\n",
            "Epoch 31/300\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.3994 - acc: 0.8333 - val_loss: 1.6776 - val_acc: 0.0000e+00\n",
            "Epoch 32/300\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.4155 - acc: 0.8500 - val_loss: 1.7366 - val_acc: 0.0000e+00\n",
            "Epoch 33/300\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.3913 - acc: 0.8667 - val_loss: 1.7540 - val_acc: 0.0000e+00\n",
            "Epoch 34/300\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.4345 - acc: 0.8167 - val_loss: 1.8090 - val_acc: 0.0000e+00\n",
            "Epoch 35/300\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.3636 - acc: 0.9000 - val_loss: 1.8210 - val_acc: 0.0000e+00\n",
            "Epoch 36/300\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.4012 - acc: 0.8833 - val_loss: 1.8823 - val_acc: 0.0000e+00\n",
            "Epoch 37/300\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.4011 - acc: 0.8500 - val_loss: 1.9112 - val_acc: 0.0000e+00\n",
            "Epoch 38/300\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.4009 - acc: 0.8833 - val_loss: 1.9662 - val_acc: 0.0000e+00\n",
            "Epoch 39/300\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.3903 - acc: 0.8667 - val_loss: 1.9824 - val_acc: 0.0000e+00\n",
            "Epoch 40/300\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.4105 - acc: 0.8500 - val_loss: 1.9839 - val_acc: 0.0000e+00\n",
            "Epoch 41/300\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.3919 - acc: 0.8667 - val_loss: 2.0333 - val_acc: 0.0000e+00\n",
            "Epoch 42/300\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.3817 - acc: 0.8667 - val_loss: 2.0460 - val_acc: 0.0000e+00\n",
            "Epoch 43/300\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.3750 - acc: 0.8833 - val_loss: 2.1464 - val_acc: 0.0000e+00\n",
            "Epoch 44/300\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.3560 - acc: 0.9167 - val_loss: 2.2661 - val_acc: 0.0000e+00\n",
            "Epoch 45/300\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.3351 - acc: 0.8667 - val_loss: 2.3108 - val_acc: 0.0000e+00\n",
            "Epoch 46/300\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.3741 - acc: 0.8833 - val_loss: 2.2939 - val_acc: 0.0000e+00\n",
            "Epoch 47/300\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.3763 - acc: 0.8500 - val_loss: 2.3628 - val_acc: 0.0000e+00\n",
            "Epoch 48/300\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.3459 - acc: 0.8833 - val_loss: 2.3833 - val_acc: 0.0000e+00\n",
            "Epoch 49/300\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.3412 - acc: 0.8833 - val_loss: 2.4073 - val_acc: 0.0000e+00\n",
            "Epoch 50/300\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.3178 - acc: 0.9167 - val_loss: 2.4808 - val_acc: 0.0000e+00\n",
            "Epoch 51/300\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.3409 - acc: 0.9000 - val_loss: 2.6063 - val_acc: 0.0000e+00\n",
            "Epoch 52/300\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.3556 - acc: 0.8500 - val_loss: 2.6548 - val_acc: 0.0000e+00\n",
            "Epoch 53/300\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.3280 - acc: 0.9000 - val_loss: 2.6634 - val_acc: 0.0000e+00\n",
            "Epoch 54/300\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.3366 - acc: 0.8833 - val_loss: 2.6591 - val_acc: 0.0000e+00\n",
            "Epoch 55/300\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.3042 - acc: 0.9000 - val_loss: 2.7076 - val_acc: 0.0000e+00\n",
            "Epoch 56/300\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.3259 - acc: 0.8833 - val_loss: 2.7504 - val_acc: 0.0000e+00\n",
            "Epoch 57/300\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.3066 - acc: 0.9000 - val_loss: 2.8137 - val_acc: 0.0000e+00\n",
            "Epoch 58/300\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.2896 - acc: 0.9333 - val_loss: 2.8631 - val_acc: 0.0000e+00\n",
            "Epoch 59/300\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.3307 - acc: 0.8500 - val_loss: 2.8828 - val_acc: 0.0000e+00\n",
            "Epoch 60/300\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.3349 - acc: 0.8833 - val_loss: 2.8817 - val_acc: 0.0000e+00\n",
            "Epoch 61/300\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.3377 - acc: 0.9000 - val_loss: 2.8846 - val_acc: 0.0000e+00\n",
            "Epoch 62/300\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.2989 - acc: 0.9333 - val_loss: 2.9457 - val_acc: 0.0000e+00\n",
            "Epoch 63/300\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.2863 - acc: 0.9167 - val_loss: 3.0555 - val_acc: 0.0000e+00\n",
            "Epoch 64/300\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.2936 - acc: 0.9000 - val_loss: 3.1529 - val_acc: 0.0000e+00\n",
            "Epoch 65/300\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.2436 - acc: 0.9500 - val_loss: 3.2039 - val_acc: 0.0000e+00\n",
            "Epoch 66/300\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.2738 - acc: 0.9333 - val_loss: 3.2733 - val_acc: 0.0000e+00\n",
            "Epoch 67/300\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.2987 - acc: 0.9000 - val_loss: 3.2990 - val_acc: 0.0000e+00\n",
            "Epoch 68/300\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.2795 - acc: 0.9000 - val_loss: 3.2880 - val_acc: 0.0000e+00\n",
            "Epoch 69/300\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.3134 - acc: 0.9167 - val_loss: 3.2896 - val_acc: 0.0000e+00\n",
            "Epoch 70/300\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.2857 - acc: 0.9167 - val_loss: 3.2477 - val_acc: 0.0000e+00\n",
            "Epoch 71/300\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.2975 - acc: 0.9333 - val_loss: 3.2803 - val_acc: 0.0000e+00\n",
            "Epoch 72/300\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.2777 - acc: 0.9333 - val_loss: 3.2971 - val_acc: 0.0000e+00\n",
            "Epoch 73/300\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.2563 - acc: 0.9333 - val_loss: 3.3180 - val_acc: 0.0000e+00\n",
            "Epoch 74/300\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.2867 - acc: 0.9500 - val_loss: 3.3585 - val_acc: 0.0000e+00\n",
            "Epoch 75/300\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.2472 - acc: 0.9333 - val_loss: 3.4720 - val_acc: 0.0000e+00\n",
            "Epoch 76/300\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.3092 - acc: 0.8667 - val_loss: 3.5822 - val_acc: 0.0000e+00\n",
            "Epoch 77/300\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.2608 - acc: 0.9000 - val_loss: 3.6094 - val_acc: 0.0000e+00\n",
            "Epoch 78/300\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.2522 - acc: 0.9333 - val_loss: 3.6910 - val_acc: 0.0000e+00\n",
            "Epoch 79/300\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.2573 - acc: 0.9333 - val_loss: 3.6506 - val_acc: 0.0000e+00\n",
            "Epoch 80/300\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.2659 - acc: 0.9167 - val_loss: 3.6105 - val_acc: 0.0000e+00\n",
            "Epoch 81/300\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.2672 - acc: 0.9333 - val_loss: 3.6168 - val_acc: 0.0000e+00\n",
            "Epoch 82/300\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.2962 - acc: 0.9000 - val_loss: 3.6329 - val_acc: 0.0000e+00\n",
            "Epoch 83/300\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.2815 - acc: 0.9500 - val_loss: 3.7716 - val_acc: 0.0000e+00\n",
            "Epoch 84/300\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.2430 - acc: 0.9333 - val_loss: 3.9463 - val_acc: 0.0000e+00\n",
            "Epoch 85/300\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.2642 - acc: 0.9000 - val_loss: 3.9167 - val_acc: 0.0000e+00\n",
            "Epoch 86/300\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.2470 - acc: 0.9333 - val_loss: 3.8067 - val_acc: 0.0000e+00\n",
            "Epoch 87/300\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.2022 - acc: 0.9833 - val_loss: 3.7575 - val_acc: 0.0000e+00\n",
            "Epoch 88/300\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.2399 - acc: 0.9333 - val_loss: 3.8492 - val_acc: 0.0000e+00\n",
            "Epoch 89/300\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.2178 - acc: 0.9500 - val_loss: 3.9111 - val_acc: 0.0000e+00\n",
            "Epoch 90/300\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.2414 - acc: 0.9333 - val_loss: 3.9389 - val_acc: 0.0000e+00\n",
            "Epoch 91/300\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.2642 - acc: 0.9333 - val_loss: 3.9208 - val_acc: 0.0000e+00\n",
            "Epoch 92/300\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.2472 - acc: 0.9333 - val_loss: 3.8923 - val_acc: 0.0000e+00\n",
            "Epoch 93/300\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.2314 - acc: 0.9500 - val_loss: 3.9302 - val_acc: 0.0000e+00\n",
            "Epoch 94/300\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.2235 - acc: 0.9333 - val_loss: 4.0467 - val_acc: 0.0000e+00\n",
            "Epoch 95/300\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.2152 - acc: 0.9500 - val_loss: 4.0571 - val_acc: 0.0000e+00\n",
            "Epoch 96/300\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.2655 - acc: 0.9000 - val_loss: 4.0156 - val_acc: 0.0000e+00\n",
            "Epoch 97/300\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.2267 - acc: 0.9333 - val_loss: 4.1013 - val_acc: 0.0000e+00\n",
            "Epoch 98/300\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.2106 - acc: 0.9500 - val_loss: 4.1589 - val_acc: 0.0000e+00\n",
            "Epoch 99/300\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.2373 - acc: 0.9333 - val_loss: 4.1917 - val_acc: 0.0000e+00\n",
            "Epoch 100/300\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.2404 - acc: 0.9333 - val_loss: 4.1708 - val_acc: 0.0000e+00\n",
            "Epoch 101/300\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.2451 - acc: 0.9333 - val_loss: 4.1543 - val_acc: 0.0000e+00\n",
            "Epoch 102/300\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.2546 - acc: 0.9167 - val_loss: 4.0012 - val_acc: 0.0000e+00\n",
            "Epoch 103/300\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.2306 - acc: 0.9667 - val_loss: 3.9888 - val_acc: 0.0000e+00\n",
            "Epoch 104/300\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.2268 - acc: 0.9500 - val_loss: 3.8770 - val_acc: 0.0000e+00\n",
            "Epoch 105/300\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.1834 - acc: 0.9500 - val_loss: 3.9098 - val_acc: 0.0000e+00\n",
            "Epoch 106/300\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.2122 - acc: 0.9500 - val_loss: 3.9601 - val_acc: 0.0000e+00\n",
            "Epoch 107/300\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.2317 - acc: 0.9167 - val_loss: 4.0005 - val_acc: 0.0000e+00\n",
            "Epoch 108/300\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.2015 - acc: 0.9500 - val_loss: 3.9546 - val_acc: 0.0000e+00\n",
            "Epoch 109/300\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.2260 - acc: 0.9333 - val_loss: 4.0028 - val_acc: 0.0000e+00\n",
            "Epoch 110/300\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.2234 - acc: 0.9500 - val_loss: 3.8514 - val_acc: 0.0000e+00\n",
            "Epoch 111/300\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.1737 - acc: 0.9500 - val_loss: 4.0141 - val_acc: 0.0000e+00\n",
            "Epoch 112/300\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.1878 - acc: 0.9667 - val_loss: 4.3730 - val_acc: 0.0000e+00\n",
            "Epoch 113/300\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.2316 - acc: 0.9500 - val_loss: 4.4181 - val_acc: 0.0000e+00\n",
            "Epoch 114/300\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.2184 - acc: 0.9500 - val_loss: 4.3261 - val_acc: 0.0000e+00\n",
            "Epoch 115/300\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.2395 - acc: 0.9333 - val_loss: 4.3106 - val_acc: 0.0000e+00\n",
            "Epoch 116/300\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.2038 - acc: 0.9500 - val_loss: 4.3679 - val_acc: 0.0000e+00\n",
            "Epoch 117/300\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.2350 - acc: 0.9500 - val_loss: 4.3808 - val_acc: 0.0000e+00\n",
            "Epoch 118/300\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.2289 - acc: 0.9333 - val_loss: 4.4490 - val_acc: 0.0000e+00\n",
            "Epoch 119/300\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.2277 - acc: 0.9333 - val_loss: 4.2481 - val_acc: 0.0000e+00\n",
            "Epoch 120/300\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.2055 - acc: 0.9333 - val_loss: 4.2085 - val_acc: 0.0000e+00\n",
            "Epoch 121/300\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.1865 - acc: 0.9833 - val_loss: 4.2401 - val_acc: 0.0000e+00\n",
            "Epoch 122/300\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.2170 - acc: 0.9333 - val_loss: 4.1740 - val_acc: 0.0000e+00\n",
            "Epoch 123/300\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.2126 - acc: 0.9833 - val_loss: 4.1483 - val_acc: 0.0000e+00\n",
            "Epoch 124/300\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.1945 - acc: 0.9500 - val_loss: 4.1754 - val_acc: 0.0000e+00\n",
            "Epoch 125/300\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.2661 - acc: 0.9000 - val_loss: 4.2562 - val_acc: 0.0000e+00\n",
            "Epoch 126/300\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.1707 - acc: 0.9667 - val_loss: 4.5077 - val_acc: 0.0000e+00\n",
            "Epoch 127/300\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.2025 - acc: 0.9333 - val_loss: 4.4994 - val_acc: 0.0000e+00\n",
            "Epoch 128/300\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.1618 - acc: 0.9833 - val_loss: 4.4037 - val_acc: 0.0000e+00\n",
            "Epoch 129/300\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.2145 - acc: 0.9333 - val_loss: 4.2815 - val_acc: 0.0000e+00\n",
            "Epoch 130/300\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.2227 - acc: 0.9500 - val_loss: 4.4846 - val_acc: 0.0000e+00\n",
            "Epoch 131/300\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.1511 - acc: 0.9500 - val_loss: 4.4827 - val_acc: 0.0000e+00\n",
            "Epoch 132/300\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.2094 - acc: 0.9333 - val_loss: 4.5185 - val_acc: 0.0000e+00\n",
            "Epoch 133/300\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.1762 - acc: 0.9667 - val_loss: 4.4923 - val_acc: 0.0000e+00\n",
            "Epoch 134/300\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.1684 - acc: 0.9833 - val_loss: 4.5041 - val_acc: 0.0000e+00\n",
            "Epoch 135/300\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.1379 - acc: 0.9833 - val_loss: 4.4808 - val_acc: 0.0000e+00\n",
            "Epoch 136/300\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.1978 - acc: 0.9500 - val_loss: 4.4898 - val_acc: 0.0000e+00\n",
            "Epoch 137/300\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.1931 - acc: 0.9667 - val_loss: 4.5742 - val_acc: 0.0000e+00\n",
            "Epoch 138/300\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.1537 - acc: 1.0000 - val_loss: 4.6734 - val_acc: 0.0000e+00\n",
            "Epoch 139/300\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.2106 - acc: 0.9667 - val_loss: 4.7330 - val_acc: 0.0000e+00\n",
            "Epoch 140/300\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.1904 - acc: 0.9500 - val_loss: 4.7860 - val_acc: 0.0000e+00\n",
            "Epoch 141/300\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.1537 - acc: 0.9667 - val_loss: 4.8553 - val_acc: 0.0000e+00\n",
            "Epoch 142/300\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.1907 - acc: 0.9500 - val_loss: 4.7759 - val_acc: 0.0000e+00\n",
            "Epoch 143/300\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.1439 - acc: 0.9833 - val_loss: 4.7487 - val_acc: 0.0000e+00\n",
            "Epoch 144/300\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.1594 - acc: 0.9500 - val_loss: 4.7750 - val_acc: 0.0000e+00\n",
            "Epoch 145/300\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.2175 - acc: 0.9333 - val_loss: 4.8209 - val_acc: 0.0000e+00\n",
            "Epoch 146/300\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.1758 - acc: 0.9500 - val_loss: 4.7403 - val_acc: 0.0000e+00\n",
            "Epoch 147/300\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.1697 - acc: 0.9833 - val_loss: 4.6065 - val_acc: 0.0000e+00\n",
            "Epoch 148/300\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.1599 - acc: 0.9833 - val_loss: 4.5989 - val_acc: 0.0000e+00\n",
            "Epoch 149/300\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.1875 - acc: 0.9500 - val_loss: 4.6912 - val_acc: 0.0000e+00\n",
            "Epoch 150/300\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.1765 - acc: 0.9667 - val_loss: 4.5970 - val_acc: 0.0000e+00\n",
            "Epoch 151/300\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.1736 - acc: 0.9667 - val_loss: 4.5791 - val_acc: 0.0000e+00\n",
            "Epoch 152/300\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.1650 - acc: 0.9833 - val_loss: 4.5007 - val_acc: 0.0000e+00\n",
            "Epoch 153/300\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.1567 - acc: 0.9667 - val_loss: 4.4842 - val_acc: 0.0000e+00\n",
            "Epoch 154/300\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.1653 - acc: 0.9667 - val_loss: 4.4036 - val_acc: 0.0000e+00\n",
            "Epoch 155/300\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.1753 - acc: 0.9667 - val_loss: 4.3474 - val_acc: 0.0000e+00\n",
            "Epoch 156/300\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.1420 - acc: 0.9833 - val_loss: 4.4893 - val_acc: 0.0000e+00\n",
            "Epoch 157/300\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.1694 - acc: 0.9667 - val_loss: 4.4901 - val_acc: 0.0000e+00\n",
            "Epoch 158/300\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.2134 - acc: 0.9333 - val_loss: 4.5002 - val_acc: 0.0000e+00\n",
            "Epoch 159/300\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.2000 - acc: 0.9500 - val_loss: 4.2454 - val_acc: 0.0000e+00\n",
            "Epoch 160/300\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.1308 - acc: 0.9833 - val_loss: 4.1434 - val_acc: 0.0000e+00\n",
            "Epoch 161/300\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.1803 - acc: 0.9667 - val_loss: 4.1471 - val_acc: 0.0000e+00\n",
            "Epoch 162/300\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.1506 - acc: 0.9833 - val_loss: 4.3257 - val_acc: 0.0000e+00\n",
            "Epoch 163/300\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.1659 - acc: 0.9667 - val_loss: 4.3371 - val_acc: 0.0000e+00\n",
            "Epoch 164/300\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.1715 - acc: 0.9667 - val_loss: 4.4976 - val_acc: 0.0000e+00\n",
            "Epoch 165/300\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.1825 - acc: 0.9667 - val_loss: 4.7930 - val_acc: 0.0000e+00\n",
            "Epoch 166/300\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.1659 - acc: 0.9833 - val_loss: 4.9608 - val_acc: 0.0000e+00\n",
            "Epoch 167/300\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.2021 - acc: 0.9667 - val_loss: 4.8218 - val_acc: 0.0000e+00\n",
            "Epoch 168/300\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.1545 - acc: 0.9333 - val_loss: 4.6360 - val_acc: 0.0000e+00\n",
            "Epoch 169/300\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.1852 - acc: 0.9333 - val_loss: 4.4199 - val_acc: 0.0000e+00\n",
            "Epoch 170/300\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.1516 - acc: 0.9667 - val_loss: 4.3737 - val_acc: 0.0000e+00\n",
            "Epoch 171/300\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.1271 - acc: 1.0000 - val_loss: 4.4988 - val_acc: 0.0000e+00\n",
            "Epoch 172/300\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.1898 - acc: 0.9333 - val_loss: 4.5688 - val_acc: 0.0000e+00\n",
            "Epoch 173/300\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.1515 - acc: 0.9667 - val_loss: 4.6509 - val_acc: 0.0000e+00\n",
            "Epoch 174/300\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.1979 - acc: 0.9667 - val_loss: 4.7319 - val_acc: 0.0000e+00\n",
            "Epoch 175/300\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.1127 - acc: 0.9833 - val_loss: 4.7407 - val_acc: 0.0000e+00\n",
            "Epoch 176/300\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.1324 - acc: 0.9833 - val_loss: 4.8052 - val_acc: 0.0000e+00\n",
            "Epoch 177/300\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.1301 - acc: 0.9833 - val_loss: 4.7306 - val_acc: 0.0000e+00\n",
            "Epoch 178/300\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.2152 - acc: 0.9167 - val_loss: 4.4052 - val_acc: 0.0000e+00\n",
            "Epoch 179/300\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.1979 - acc: 0.9500 - val_loss: 4.5399 - val_acc: 0.0000e+00\n",
            "Epoch 180/300\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.1195 - acc: 1.0000 - val_loss: 4.5414 - val_acc: 0.0000e+00\n",
            "Epoch 181/300\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.1426 - acc: 0.9833 - val_loss: 4.6867 - val_acc: 0.0000e+00\n",
            "Epoch 182/300\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.1499 - acc: 0.9500 - val_loss: 4.7680 - val_acc: 0.0000e+00\n",
            "Epoch 183/300\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.1277 - acc: 0.9833 - val_loss: 4.8559 - val_acc: 0.0000e+00\n",
            "Epoch 184/300\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.1986 - acc: 0.9333 - val_loss: 4.6395 - val_acc: 0.0000e+00\n",
            "Epoch 185/300\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.1575 - acc: 0.9833 - val_loss: 4.5000 - val_acc: 0.0000e+00\n",
            "Epoch 186/300\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.1288 - acc: 0.9833 - val_loss: 4.3548 - val_acc: 0.0000e+00\n",
            "Epoch 187/300\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.1166 - acc: 0.9833 - val_loss: 4.4926 - val_acc: 0.0000e+00\n",
            "Epoch 188/300\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.1377 - acc: 0.9667 - val_loss: 4.6183 - val_acc: 0.0000e+00\n",
            "Epoch 189/300\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.1575 - acc: 0.9667 - val_loss: 4.7501 - val_acc: 0.0000e+00\n",
            "Epoch 190/300\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.1378 - acc: 0.9667 - val_loss: 4.8547 - val_acc: 0.0000e+00\n",
            "Epoch 191/300\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.1611 - acc: 0.9500 - val_loss: 4.7645 - val_acc: 0.0000e+00\n",
            "Epoch 192/300\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.1214 - acc: 0.9833 - val_loss: 4.6811 - val_acc: 0.0000e+00\n",
            "Epoch 193/300\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.1486 - acc: 0.9833 - val_loss: 4.7750 - val_acc: 0.0000e+00\n",
            "Epoch 194/300\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.1496 - acc: 0.9667 - val_loss: 4.8261 - val_acc: 0.0000e+00\n",
            "Epoch 195/300\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.1376 - acc: 0.9667 - val_loss: 4.6950 - val_acc: 0.0000e+00\n",
            "Epoch 196/300\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.1329 - acc: 1.0000 - val_loss: 4.4286 - val_acc: 0.0000e+00\n",
            "Epoch 197/300\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.1206 - acc: 1.0000 - val_loss: 4.3044 - val_acc: 0.0000e+00\n",
            "Epoch 198/300\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.1381 - acc: 0.9833 - val_loss: 4.3159 - val_acc: 0.0000e+00\n",
            "Epoch 199/300\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.1267 - acc: 1.0000 - val_loss: 4.5201 - val_acc: 0.0000e+00\n",
            "Epoch 200/300\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.1668 - acc: 0.9667 - val_loss: 4.6720 - val_acc: 0.0000e+00\n",
            "Epoch 201/300\n",
            "\n",
            "Epoch 00201: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.1323 - acc: 0.9500 - val_loss: 4.7189 - val_acc: 0.0000e+00\n",
            "Epoch 202/300\n",
            "\n",
            "Epoch 00202: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.1348 - acc: 0.9500 - val_loss: 4.4021 - val_acc: 0.0000e+00\n",
            "Epoch 203/300\n",
            "\n",
            "Epoch 00203: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.1163 - acc: 1.0000 - val_loss: 4.3438 - val_acc: 0.0000e+00\n",
            "Epoch 204/300\n",
            "\n",
            "Epoch 00204: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.2009 - acc: 0.9500 - val_loss: 4.3153 - val_acc: 0.0000e+00\n",
            "Epoch 205/300\n",
            "\n",
            "Epoch 00205: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.1690 - acc: 0.9333 - val_loss: 4.2712 - val_acc: 0.0000e+00\n",
            "Epoch 206/300\n",
            "\n",
            "Epoch 00206: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.1392 - acc: 0.9833 - val_loss: 4.4000 - val_acc: 0.0000e+00\n",
            "Epoch 207/300\n",
            "\n",
            "Epoch 00207: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.1604 - acc: 0.9667 - val_loss: 4.6328 - val_acc: 0.0000e+00\n",
            "Epoch 208/300\n",
            "\n",
            "Epoch 00208: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.1204 - acc: 1.0000 - val_loss: 4.6525 - val_acc: 0.0000e+00\n",
            "Epoch 209/300\n",
            "\n",
            "Epoch 00209: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.1166 - acc: 1.0000 - val_loss: 4.7082 - val_acc: 0.0000e+00\n",
            "Epoch 210/300\n",
            "\n",
            "Epoch 00210: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.1316 - acc: 0.9833 - val_loss: 4.8258 - val_acc: 0.0000e+00\n",
            "Epoch 211/300\n",
            "\n",
            "Epoch 00211: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.1099 - acc: 0.9833 - val_loss: 4.7608 - val_acc: 0.0000e+00\n",
            "Epoch 212/300\n",
            "\n",
            "Epoch 00212: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.1179 - acc: 0.9833 - val_loss: 4.9906 - val_acc: 0.0000e+00\n",
            "Epoch 213/300\n",
            "\n",
            "Epoch 00213: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.1074 - acc: 0.9833 - val_loss: 5.1542 - val_acc: 0.0000e+00\n",
            "Epoch 214/300\n",
            "\n",
            "Epoch 00214: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.1284 - acc: 0.9667 - val_loss: 5.1526 - val_acc: 0.0000e+00\n",
            "Epoch 215/300\n",
            "\n",
            "Epoch 00215: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.1503 - acc: 0.9500 - val_loss: 4.9819 - val_acc: 0.0000e+00\n",
            "Epoch 216/300\n",
            "\n",
            "Epoch 00216: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.1199 - acc: 0.9833 - val_loss: 4.9538 - val_acc: 0.0000e+00\n",
            "Epoch 217/300\n",
            "\n",
            "Epoch 00217: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.1409 - acc: 0.9667 - val_loss: 4.9304 - val_acc: 0.0000e+00\n",
            "Epoch 218/300\n",
            "\n",
            "Epoch 00218: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.1090 - acc: 1.0000 - val_loss: 4.9889 - val_acc: 0.0000e+00\n",
            "Epoch 219/300\n",
            "\n",
            "Epoch 00219: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.1098 - acc: 0.9667 - val_loss: 4.9602 - val_acc: 0.0000e+00\n",
            "Epoch 220/300\n",
            "\n",
            "Epoch 00220: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.1635 - acc: 0.9500 - val_loss: 4.8395 - val_acc: 0.0000e+00\n",
            "Epoch 221/300\n",
            "\n",
            "Epoch 00221: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.0886 - acc: 1.0000 - val_loss: 4.9276 - val_acc: 0.0000e+00\n",
            "Epoch 222/300\n",
            "\n",
            "Epoch 00222: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.1385 - acc: 0.9833 - val_loss: 5.0711 - val_acc: 0.0000e+00\n",
            "Epoch 223/300\n",
            "\n",
            "Epoch 00223: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.1397 - acc: 0.9833 - val_loss: 4.9179 - val_acc: 0.0000e+00\n",
            "Epoch 224/300\n",
            "\n",
            "Epoch 00224: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.1122 - acc: 1.0000 - val_loss: 4.9367 - val_acc: 0.0000e+00\n",
            "Epoch 225/300\n",
            "\n",
            "Epoch 00225: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.1383 - acc: 0.9667 - val_loss: 4.7772 - val_acc: 0.0000e+00\n",
            "Epoch 226/300\n",
            "\n",
            "Epoch 00226: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.1158 - acc: 0.9667 - val_loss: 4.5994 - val_acc: 0.0000e+00\n",
            "Epoch 227/300\n",
            "\n",
            "Epoch 00227: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.1191 - acc: 0.9833 - val_loss: 4.5856 - val_acc: 0.0000e+00\n",
            "Epoch 228/300\n",
            "\n",
            "Epoch 00228: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.1371 - acc: 0.9833 - val_loss: 4.5347 - val_acc: 0.0000e+00\n",
            "Epoch 229/300\n",
            "\n",
            "Epoch 00229: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.1045 - acc: 1.0000 - val_loss: 4.5724 - val_acc: 0.0000e+00\n",
            "Epoch 230/300\n",
            "\n",
            "Epoch 00230: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.1250 - acc: 0.9667 - val_loss: 4.7606 - val_acc: 0.0000e+00\n",
            "Epoch 231/300\n",
            "\n",
            "Epoch 00231: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.1193 - acc: 0.9833 - val_loss: 4.6773 - val_acc: 0.0000e+00\n",
            "Epoch 232/300\n",
            "\n",
            "Epoch 00232: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.1092 - acc: 0.9833 - val_loss: 4.6446 - val_acc: 0.0000e+00\n",
            "Epoch 233/300\n",
            "\n",
            "Epoch 00233: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.1164 - acc: 0.9667 - val_loss: 4.5318 - val_acc: 0.0000e+00\n",
            "Epoch 234/300\n",
            "\n",
            "Epoch 00234: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.0931 - acc: 0.9833 - val_loss: 4.5224 - val_acc: 0.0000e+00\n",
            "Epoch 235/300\n",
            "\n",
            "Epoch 00235: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.1335 - acc: 0.9667 - val_loss: 4.6024 - val_acc: 0.0000e+00\n",
            "Epoch 236/300\n",
            "\n",
            "Epoch 00236: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.1683 - acc: 0.9333 - val_loss: 4.5925 - val_acc: 0.0000e+00\n",
            "Epoch 237/300\n",
            "\n",
            "Epoch 00237: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.1440 - acc: 0.9667 - val_loss: 4.5490 - val_acc: 0.0000e+00\n",
            "Epoch 238/300\n",
            "\n",
            "Epoch 00238: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.1090 - acc: 0.9667 - val_loss: 4.5863 - val_acc: 0.0000e+00\n",
            "Epoch 239/300\n",
            "\n",
            "Epoch 00239: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.0940 - acc: 0.9833 - val_loss: 4.5461 - val_acc: 0.0000e+00\n",
            "Epoch 240/300\n",
            "\n",
            "Epoch 00240: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.0909 - acc: 0.9833 - val_loss: 4.4498 - val_acc: 0.0000e+00\n",
            "Epoch 241/300\n",
            "\n",
            "Epoch 00241: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.1225 - acc: 0.9833 - val_loss: 4.3721 - val_acc: 0.0000e+00\n",
            "Epoch 242/300\n",
            "\n",
            "Epoch 00242: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.1181 - acc: 0.9667 - val_loss: 4.5209 - val_acc: 0.0000e+00\n",
            "Epoch 243/300\n",
            "\n",
            "Epoch 00243: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.1772 - acc: 0.9667 - val_loss: 4.7116 - val_acc: 0.0000e+00\n",
            "Epoch 244/300\n",
            "\n",
            "Epoch 00244: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.1303 - acc: 0.9833 - val_loss: 4.5557 - val_acc: 0.0000e+00\n",
            "Epoch 245/300\n",
            "\n",
            "Epoch 00245: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.1425 - acc: 0.9667 - val_loss: 4.3280 - val_acc: 0.0000e+00\n",
            "Epoch 246/300\n",
            "\n",
            "Epoch 00246: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.1053 - acc: 1.0000 - val_loss: 4.3202 - val_acc: 0.0000e+00\n",
            "Epoch 247/300\n",
            "\n",
            "Epoch 00247: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.1167 - acc: 1.0000 - val_loss: 4.4900 - val_acc: 0.0000e+00\n",
            "Epoch 248/300\n",
            "\n",
            "Epoch 00248: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.1548 - acc: 0.9667 - val_loss: 4.7455 - val_acc: 0.0000e+00\n",
            "Epoch 249/300\n",
            "\n",
            "Epoch 00249: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.1330 - acc: 0.9667 - val_loss: 4.9478 - val_acc: 0.0000e+00\n",
            "Epoch 250/300\n",
            "\n",
            "Epoch 00250: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.1061 - acc: 0.9833 - val_loss: 4.9227 - val_acc: 0.0000e+00\n",
            "Epoch 251/300\n",
            "\n",
            "Epoch 00251: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.1547 - acc: 0.9500 - val_loss: 4.7799 - val_acc: 0.0000e+00\n",
            "Epoch 252/300\n",
            "\n",
            "Epoch 00252: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.1226 - acc: 1.0000 - val_loss: 4.6821 - val_acc: 0.0000e+00\n",
            "Epoch 253/300\n",
            "\n",
            "Epoch 00253: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.1008 - acc: 0.9833 - val_loss: 4.6269 - val_acc: 0.0000e+00\n",
            "Epoch 254/300\n",
            "\n",
            "Epoch 00254: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.1258 - acc: 0.9833 - val_loss: 4.6839 - val_acc: 0.0000e+00\n",
            "Epoch 255/300\n",
            "\n",
            "Epoch 00255: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.1231 - acc: 0.9667 - val_loss: 4.7521 - val_acc: 0.0000e+00\n",
            "Epoch 256/300\n",
            "\n",
            "Epoch 00256: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.1104 - acc: 0.9833 - val_loss: 5.0945 - val_acc: 0.0000e+00\n",
            "Epoch 257/300\n",
            "\n",
            "Epoch 00257: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.1033 - acc: 0.9833 - val_loss: 5.3883 - val_acc: 0.0000e+00\n",
            "Epoch 258/300\n",
            "\n",
            "Epoch 00258: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.1221 - acc: 0.9667 - val_loss: 5.2256 - val_acc: 0.0000e+00\n",
            "Epoch 259/300\n",
            "\n",
            "Epoch 00259: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.0997 - acc: 1.0000 - val_loss: 5.0623 - val_acc: 0.0000e+00\n",
            "Epoch 260/300\n",
            "\n",
            "Epoch 00260: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.0650 - acc: 1.0000 - val_loss: 5.0324 - val_acc: 0.0000e+00\n",
            "Epoch 261/300\n",
            "\n",
            "Epoch 00261: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.1598 - acc: 0.9333 - val_loss: 4.9789 - val_acc: 0.0000e+00\n",
            "Epoch 262/300\n",
            "\n",
            "Epoch 00262: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.1241 - acc: 0.9667 - val_loss: 4.7162 - val_acc: 0.0000e+00\n",
            "Epoch 263/300\n",
            "\n",
            "Epoch 00263: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.0896 - acc: 0.9833 - val_loss: 4.5230 - val_acc: 0.0000e+00\n",
            "Epoch 264/300\n",
            "\n",
            "Epoch 00264: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.0940 - acc: 1.0000 - val_loss: 4.3706 - val_acc: 0.0000e+00\n",
            "Epoch 265/300\n",
            "\n",
            "Epoch 00265: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.1214 - acc: 0.9833 - val_loss: 4.3743 - val_acc: 0.0000e+00\n",
            "Epoch 266/300\n",
            "\n",
            "Epoch 00266: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.1394 - acc: 0.9667 - val_loss: 4.6502 - val_acc: 0.0000e+00\n",
            "Epoch 267/300\n",
            "\n",
            "Epoch 00267: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.1073 - acc: 1.0000 - val_loss: 4.7859 - val_acc: 0.0000e+00\n",
            "Epoch 268/300\n",
            "\n",
            "Epoch 00268: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.0909 - acc: 0.9833 - val_loss: 4.9692 - val_acc: 0.0000e+00\n",
            "Epoch 269/300\n",
            "\n",
            "Epoch 00269: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.1484 - acc: 0.9500 - val_loss: 4.9780 - val_acc: 0.0000e+00\n",
            "Epoch 270/300\n",
            "\n",
            "Epoch 00270: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.1267 - acc: 0.9667 - val_loss: 5.0456 - val_acc: 0.0000e+00\n",
            "Epoch 271/300\n",
            "\n",
            "Epoch 00271: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.1435 - acc: 0.9500 - val_loss: 4.9395 - val_acc: 0.0000e+00\n",
            "Epoch 272/300\n",
            "\n",
            "Epoch 00272: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.1099 - acc: 0.9833 - val_loss: 4.6829 - val_acc: 0.0000e+00\n",
            "Epoch 273/300\n",
            "\n",
            "Epoch 00273: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.1086 - acc: 1.0000 - val_loss: 4.6281 - val_acc: 0.0000e+00\n",
            "Epoch 274/300\n",
            "\n",
            "Epoch 00274: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.1080 - acc: 1.0000 - val_loss: 4.6314 - val_acc: 0.0000e+00\n",
            "Epoch 275/300\n",
            "\n",
            "Epoch 00275: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.0779 - acc: 0.9833 - val_loss: 4.8231 - val_acc: 0.0000e+00\n",
            "Epoch 276/300\n",
            "\n",
            "Epoch 00276: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.1101 - acc: 1.0000 - val_loss: 4.8729 - val_acc: 0.0000e+00\n",
            "Epoch 277/300\n",
            "\n",
            "Epoch 00277: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.1318 - acc: 0.9667 - val_loss: 4.9220 - val_acc: 0.0000e+00\n",
            "Epoch 278/300\n",
            "\n",
            "Epoch 00278: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.1249 - acc: 0.9667 - val_loss: 4.7585 - val_acc: 0.0000e+00\n",
            "Epoch 279/300\n",
            "\n",
            "Epoch 00279: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.0808 - acc: 1.0000 - val_loss: 4.8050 - val_acc: 0.0000e+00\n",
            "Epoch 280/300\n",
            "\n",
            "Epoch 00280: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.0918 - acc: 1.0000 - val_loss: 4.9331 - val_acc: 0.0000e+00\n",
            "Epoch 281/300\n",
            "\n",
            "Epoch 00281: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.0986 - acc: 0.9833 - val_loss: 4.9840 - val_acc: 0.0000e+00\n",
            "Epoch 282/300\n",
            "\n",
            "Epoch 00282: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.0925 - acc: 0.9667 - val_loss: 5.0271 - val_acc: 0.0000e+00\n",
            "Epoch 283/300\n",
            "\n",
            "Epoch 00283: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.0948 - acc: 0.9833 - val_loss: 4.7312 - val_acc: 0.0000e+00\n",
            "Epoch 284/300\n",
            "\n",
            "Epoch 00284: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.1101 - acc: 0.9667 - val_loss: 4.9031 - val_acc: 0.0000e+00\n",
            "Epoch 285/300\n",
            "\n",
            "Epoch 00285: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.1011 - acc: 0.9833 - val_loss: 5.1222 - val_acc: 0.0000e+00\n",
            "Epoch 286/300\n",
            "\n",
            "Epoch 00286: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.1032 - acc: 0.9667 - val_loss: 5.1911 - val_acc: 0.0000e+00\n",
            "Epoch 287/300\n",
            "\n",
            "Epoch 00287: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.1360 - acc: 0.9833 - val_loss: 4.8038 - val_acc: 0.0000e+00\n",
            "Epoch 288/300\n",
            "\n",
            "Epoch 00288: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.1035 - acc: 0.9833 - val_loss: 4.6896 - val_acc: 0.0000e+00\n",
            "Epoch 289/300\n",
            "\n",
            "Epoch 00289: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.0874 - acc: 1.0000 - val_loss: 4.5480 - val_acc: 0.0000e+00\n",
            "Epoch 290/300\n",
            "\n",
            "Epoch 00290: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.0654 - acc: 1.0000 - val_loss: 4.6241 - val_acc: 0.0000e+00\n",
            "Epoch 291/300\n",
            "\n",
            "Epoch 00291: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.1002 - acc: 0.9833 - val_loss: 4.6582 - val_acc: 0.0000e+00\n",
            "Epoch 292/300\n",
            "\n",
            "Epoch 00292: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.0640 - acc: 1.0000 - val_loss: 4.8365 - val_acc: 0.0000e+00\n",
            "Epoch 293/300\n",
            "\n",
            "Epoch 00293: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.1245 - acc: 0.9667 - val_loss: 5.0015 - val_acc: 0.0000e+00\n",
            "Epoch 294/300\n",
            "\n",
            "Epoch 00294: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.1310 - acc: 0.9833 - val_loss: 4.9514 - val_acc: 0.0000e+00\n",
            "Epoch 295/300\n",
            "\n",
            "Epoch 00295: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.0862 - acc: 0.9833 - val_loss: 4.6509 - val_acc: 0.0000e+00\n",
            "Epoch 296/300\n",
            "\n",
            "Epoch 00296: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.0758 - acc: 1.0000 - val_loss: 4.7033 - val_acc: 0.0000e+00\n",
            "Epoch 297/300\n",
            "\n",
            "Epoch 00297: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.1337 - acc: 0.9667 - val_loss: 4.6214 - val_acc: 0.0000e+00\n",
            "Epoch 298/300\n",
            "\n",
            "Epoch 00298: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.1559 - acc: 0.9833 - val_loss: 4.5588 - val_acc: 0.0000e+00\n",
            "Epoch 299/300\n",
            "\n",
            "Epoch 00299: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.1005 - acc: 0.9833 - val_loss: 4.4689 - val_acc: 0.0000e+00\n",
            "Epoch 300/300\n",
            "\n",
            "Epoch 00300: val_loss did not improve from 1.30210\n",
            "60/60 - 0s - loss: 0.1025 - acc: 0.9833 - val_loss: 4.5663 - val_acc: 0.0000e+00\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nOxKziVTa0wAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "# Evaluate on test data\n",
            "40/40 [==============================] - 0s 2ms/sample - loss: 1.5985 - acc: 0.5000\n",
            "test loss, test acc: [1.5984798594996392, 0.5]\n",
            "EEG_Deep/Data2A/parsed_P04T.mat\n",
            "Filtering of Training Data Finished\n",
            "EEG_Deep/Data2A/parsed_P04E.mat\n",
            "Filtering of Testing Data Finished\n",
            "(1, 2)\n",
            "Finding labels in training data\n",
            "Finding labels in testing data\n",
            "[2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1]\n",
            "(60, 12, 1536)\n",
            "(60,)\n",
            "(20, 12, 1536)\n",
            "(20,)\n",
            "[[0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]]\n",
            "X_train shape: (60, 1, 12, 1536)\n",
            "60 train samples\n",
            "20 val samples\n",
            "(40, 12, 1536)\n",
            "(40, 1)\n",
            "X_train shape: (40, 1, 12, 1536)\n",
            "40 train samples\n",
            "Train on 60 samples, validate on 20 samples\n",
            "Epoch 1/300\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 1.36936, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 1s - loss: 1.3282 - acc: 0.3500 - val_loss: 1.3694 - val_acc: 0.5500\n",
            "Epoch 2/300\n",
            "\n",
            "Epoch 00002: val_loss improved from 1.36936 to 1.35919, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 1.0604 - acc: 0.5500 - val_loss: 1.3592 - val_acc: 0.5000\n",
            "Epoch 3/300\n",
            "\n",
            "Epoch 00003: val_loss improved from 1.35919 to 1.34197, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.9265 - acc: 0.5667 - val_loss: 1.3420 - val_acc: 0.5500\n",
            "Epoch 4/300\n",
            "\n",
            "Epoch 00004: val_loss improved from 1.34197 to 1.31958, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.8731 - acc: 0.4833 - val_loss: 1.3196 - val_acc: 0.5500\n",
            "Epoch 5/300\n",
            "\n",
            "Epoch 00005: val_loss improved from 1.31958 to 1.29417, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.8317 - acc: 0.6500 - val_loss: 1.2942 - val_acc: 0.5500\n",
            "Epoch 6/300\n",
            "\n",
            "Epoch 00006: val_loss improved from 1.29417 to 1.26816, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.8079 - acc: 0.5500 - val_loss: 1.2682 - val_acc: 0.5500\n",
            "Epoch 7/300\n",
            "\n",
            "Epoch 00007: val_loss improved from 1.26816 to 1.24249, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.7900 - acc: 0.6333 - val_loss: 1.2425 - val_acc: 0.6000\n",
            "Epoch 8/300\n",
            "\n",
            "Epoch 00008: val_loss improved from 1.24249 to 1.21758, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.7796 - acc: 0.6333 - val_loss: 1.2176 - val_acc: 0.6000\n",
            "Epoch 9/300\n",
            "\n",
            "Epoch 00009: val_loss improved from 1.21758 to 1.19259, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.7676 - acc: 0.6500 - val_loss: 1.1926 - val_acc: 0.6000\n",
            "Epoch 10/300\n",
            "\n",
            "Epoch 00010: val_loss improved from 1.19259 to 1.16871, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.7595 - acc: 0.5833 - val_loss: 1.1687 - val_acc: 0.4500\n",
            "Epoch 11/300\n",
            "\n",
            "Epoch 00011: val_loss improved from 1.16871 to 1.14634, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.7446 - acc: 0.7000 - val_loss: 1.1463 - val_acc: 0.5500\n",
            "Epoch 12/300\n",
            "\n",
            "Epoch 00012: val_loss improved from 1.14634 to 1.12424, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.7480 - acc: 0.6167 - val_loss: 1.1242 - val_acc: 0.5000\n",
            "Epoch 13/300\n",
            "\n",
            "Epoch 00013: val_loss improved from 1.12424 to 1.10221, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.7112 - acc: 0.8000 - val_loss: 1.1022 - val_acc: 0.4500\n",
            "Epoch 14/300\n",
            "\n",
            "Epoch 00014: val_loss improved from 1.10221 to 1.08218, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.7028 - acc: 0.7333 - val_loss: 1.0822 - val_acc: 0.4500\n",
            "Epoch 15/300\n",
            "\n",
            "Epoch 00015: val_loss improved from 1.08218 to 1.06446, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6891 - acc: 0.8167 - val_loss: 1.0645 - val_acc: 0.4500\n",
            "Epoch 16/300\n",
            "\n",
            "Epoch 00016: val_loss improved from 1.06446 to 1.04724, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6775 - acc: 0.8167 - val_loss: 1.0472 - val_acc: 0.4500\n",
            "Epoch 17/300\n",
            "\n",
            "Epoch 00017: val_loss improved from 1.04724 to 1.03089, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6744 - acc: 0.8333 - val_loss: 1.0309 - val_acc: 0.5000\n",
            "Epoch 18/300\n",
            "\n",
            "Epoch 00018: val_loss improved from 1.03089 to 1.01786, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6663 - acc: 0.7333 - val_loss: 1.0179 - val_acc: 0.5000\n",
            "Epoch 19/300\n",
            "\n",
            "Epoch 00019: val_loss improved from 1.01786 to 1.00527, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6678 - acc: 0.7833 - val_loss: 1.0053 - val_acc: 0.5000\n",
            "Epoch 20/300\n",
            "\n",
            "Epoch 00020: val_loss improved from 1.00527 to 0.99166, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6608 - acc: 0.7833 - val_loss: 0.9917 - val_acc: 0.5500\n",
            "Epoch 21/300\n",
            "\n",
            "Epoch 00021: val_loss improved from 0.99166 to 0.97672, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6269 - acc: 0.8167 - val_loss: 0.9767 - val_acc: 0.7000\n",
            "Epoch 22/300\n",
            "\n",
            "Epoch 00022: val_loss improved from 0.97672 to 0.96520, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6138 - acc: 0.7833 - val_loss: 0.9652 - val_acc: 0.6000\n",
            "Epoch 23/300\n",
            "\n",
            "Epoch 00023: val_loss improved from 0.96520 to 0.95595, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6078 - acc: 0.8000 - val_loss: 0.9559 - val_acc: 0.5000\n",
            "Epoch 24/300\n",
            "\n",
            "Epoch 00024: val_loss improved from 0.95595 to 0.94746, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5715 - acc: 0.8667 - val_loss: 0.9475 - val_acc: 0.5500\n",
            "Epoch 25/300\n",
            "\n",
            "Epoch 00025: val_loss improved from 0.94746 to 0.93768, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5819 - acc: 0.8000 - val_loss: 0.9377 - val_acc: 0.5000\n",
            "Epoch 26/300\n",
            "\n",
            "Epoch 00026: val_loss improved from 0.93768 to 0.92922, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5551 - acc: 0.8833 - val_loss: 0.9292 - val_acc: 0.5000\n",
            "Epoch 27/300\n",
            "\n",
            "Epoch 00027: val_loss improved from 0.92922 to 0.92161, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5609 - acc: 0.7833 - val_loss: 0.9216 - val_acc: 0.5000\n",
            "Epoch 28/300\n",
            "\n",
            "Epoch 00028: val_loss improved from 0.92161 to 0.91406, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5487 - acc: 0.8833 - val_loss: 0.9141 - val_acc: 0.6000\n",
            "Epoch 29/300\n",
            "\n",
            "Epoch 00029: val_loss improved from 0.91406 to 0.90663, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5374 - acc: 0.8833 - val_loss: 0.9066 - val_acc: 0.6000\n",
            "Epoch 30/300\n",
            "\n",
            "Epoch 00030: val_loss improved from 0.90663 to 0.90293, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5249 - acc: 0.8500 - val_loss: 0.9029 - val_acc: 0.5500\n",
            "Epoch 31/300\n",
            "\n",
            "Epoch 00031: val_loss improved from 0.90293 to 0.89901, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5311 - acc: 0.8833 - val_loss: 0.8990 - val_acc: 0.5000\n",
            "Epoch 32/300\n",
            "\n",
            "Epoch 00032: val_loss improved from 0.89901 to 0.89260, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5412 - acc: 0.8000 - val_loss: 0.8926 - val_acc: 0.5500\n",
            "Epoch 33/300\n",
            "\n",
            "Epoch 00033: val_loss improved from 0.89260 to 0.88410, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5211 - acc: 0.9000 - val_loss: 0.8841 - val_acc: 0.6000\n",
            "Epoch 34/300\n",
            "\n",
            "Epoch 00034: val_loss improved from 0.88410 to 0.87850, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5329 - acc: 0.8500 - val_loss: 0.8785 - val_acc: 0.6000\n",
            "Epoch 35/300\n",
            "\n",
            "Epoch 00035: val_loss improved from 0.87850 to 0.87116, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5114 - acc: 0.8833 - val_loss: 0.8712 - val_acc: 0.5000\n",
            "Epoch 36/300\n",
            "\n",
            "Epoch 00036: val_loss improved from 0.87116 to 0.86363, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5145 - acc: 0.8667 - val_loss: 0.8636 - val_acc: 0.5500\n",
            "Epoch 37/300\n",
            "\n",
            "Epoch 00037: val_loss improved from 0.86363 to 0.85880, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5040 - acc: 0.8667 - val_loss: 0.8588 - val_acc: 0.5500\n",
            "Epoch 38/300\n",
            "\n",
            "Epoch 00038: val_loss improved from 0.85880 to 0.85610, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4938 - acc: 0.8333 - val_loss: 0.8561 - val_acc: 0.5000\n",
            "Epoch 39/300\n",
            "\n",
            "Epoch 00039: val_loss improved from 0.85610 to 0.85555, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4445 - acc: 0.9167 - val_loss: 0.8555 - val_acc: 0.4000\n",
            "Epoch 40/300\n",
            "\n",
            "Epoch 00040: val_loss improved from 0.85555 to 0.84900, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4871 - acc: 0.9000 - val_loss: 0.8490 - val_acc: 0.4000\n",
            "Epoch 41/300\n",
            "\n",
            "Epoch 00041: val_loss improved from 0.84900 to 0.84862, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4534 - acc: 0.9167 - val_loss: 0.8486 - val_acc: 0.4000\n",
            "Epoch 42/300\n",
            "\n",
            "Epoch 00042: val_loss improved from 0.84862 to 0.84458, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4842 - acc: 0.9167 - val_loss: 0.8446 - val_acc: 0.4000\n",
            "Epoch 43/300\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.84458\n",
            "60/60 - 0s - loss: 0.4632 - acc: 0.8500 - val_loss: 0.8447 - val_acc: 0.4500\n",
            "Epoch 44/300\n",
            "\n",
            "Epoch 00044: val_loss improved from 0.84458 to 0.83749, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4575 - acc: 0.9000 - val_loss: 0.8375 - val_acc: 0.4000\n",
            "Epoch 45/300\n",
            "\n",
            "Epoch 00045: val_loss improved from 0.83749 to 0.83682, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4440 - acc: 0.9667 - val_loss: 0.8368 - val_acc: 0.4000\n",
            "Epoch 46/300\n",
            "\n",
            "Epoch 00046: val_loss improved from 0.83682 to 0.83491, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4336 - acc: 0.9667 - val_loss: 0.8349 - val_acc: 0.4000\n",
            "Epoch 47/300\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.83491\n",
            "60/60 - 0s - loss: 0.4080 - acc: 0.9500 - val_loss: 0.8367 - val_acc: 0.4000\n",
            "Epoch 48/300\n",
            "\n",
            "Epoch 00048: val_loss improved from 0.83491 to 0.83357, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4471 - acc: 0.9667 - val_loss: 0.8336 - val_acc: 0.4000\n",
            "Epoch 49/300\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.83357\n",
            "60/60 - 0s - loss: 0.4104 - acc: 0.9500 - val_loss: 0.8388 - val_acc: 0.4000\n",
            "Epoch 50/300\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.83357\n",
            "60/60 - 0s - loss: 0.4358 - acc: 0.8833 - val_loss: 0.8481 - val_acc: 0.4500\n",
            "Epoch 51/300\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.83357\n",
            "60/60 - 0s - loss: 0.4290 - acc: 0.9333 - val_loss: 0.8486 - val_acc: 0.4500\n",
            "Epoch 52/300\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.83357\n",
            "60/60 - 0s - loss: 0.4125 - acc: 0.9167 - val_loss: 0.8448 - val_acc: 0.4500\n",
            "Epoch 53/300\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.83357\n",
            "60/60 - 0s - loss: 0.4049 - acc: 0.9167 - val_loss: 0.8400 - val_acc: 0.4000\n",
            "Epoch 54/300\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.83357\n",
            "60/60 - 0s - loss: 0.4088 - acc: 0.9333 - val_loss: 0.8356 - val_acc: 0.4000\n",
            "Epoch 55/300\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.83357\n",
            "60/60 - 0s - loss: 0.3976 - acc: 0.9500 - val_loss: 0.8372 - val_acc: 0.4000\n",
            "Epoch 56/300\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.83357\n",
            "60/60 - 0s - loss: 0.3938 - acc: 0.9500 - val_loss: 0.8438 - val_acc: 0.4500\n",
            "Epoch 57/300\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.83357\n",
            "60/60 - 0s - loss: 0.4147 - acc: 0.9000 - val_loss: 0.8442 - val_acc: 0.4500\n",
            "Epoch 58/300\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.83357\n",
            "60/60 - 0s - loss: 0.3877 - acc: 0.9833 - val_loss: 0.8506 - val_acc: 0.4500\n",
            "Epoch 59/300\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.83357\n",
            "60/60 - 0s - loss: 0.3745 - acc: 0.9167 - val_loss: 0.8531 - val_acc: 0.4500\n",
            "Epoch 60/300\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.83357\n",
            "60/60 - 0s - loss: 0.3494 - acc: 0.9500 - val_loss: 0.8745 - val_acc: 0.4500\n",
            "Epoch 61/300\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.83357\n",
            "60/60 - 0s - loss: 0.3681 - acc: 0.9333 - val_loss: 0.8821 - val_acc: 0.4500\n",
            "Epoch 62/300\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.83357\n",
            "60/60 - 0s - loss: 0.3430 - acc: 1.0000 - val_loss: 0.8755 - val_acc: 0.4500\n",
            "Epoch 63/300\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.83357\n",
            "60/60 - 0s - loss: 0.3693 - acc: 0.9500 - val_loss: 0.8633 - val_acc: 0.4500\n",
            "Epoch 64/300\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.83357\n",
            "60/60 - 0s - loss: 0.3841 - acc: 0.9500 - val_loss: 0.8509 - val_acc: 0.4000\n",
            "Epoch 65/300\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.83357\n",
            "60/60 - 0s - loss: 0.3503 - acc: 0.9167 - val_loss: 0.8545 - val_acc: 0.4000\n",
            "Epoch 66/300\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.83357\n",
            "60/60 - 0s - loss: 0.3547 - acc: 0.9500 - val_loss: 0.8703 - val_acc: 0.5000\n",
            "Epoch 67/300\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.83357\n",
            "60/60 - 0s - loss: 0.3739 - acc: 0.9000 - val_loss: 0.8719 - val_acc: 0.5000\n",
            "Epoch 68/300\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.83357\n",
            "60/60 - 0s - loss: 0.3525 - acc: 0.9167 - val_loss: 0.8688 - val_acc: 0.5000\n",
            "Epoch 69/300\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.83357\n",
            "60/60 - 0s - loss: 0.3855 - acc: 0.9000 - val_loss: 0.8716 - val_acc: 0.5000\n",
            "Epoch 70/300\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.83357\n",
            "60/60 - 0s - loss: 0.3472 - acc: 0.9333 - val_loss: 0.8642 - val_acc: 0.5000\n",
            "Epoch 71/300\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.83357\n",
            "60/60 - 0s - loss: 0.3845 - acc: 0.9167 - val_loss: 0.8700 - val_acc: 0.5000\n",
            "Epoch 72/300\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.83357\n",
            "60/60 - 0s - loss: 0.3248 - acc: 0.9667 - val_loss: 0.8824 - val_acc: 0.5000\n",
            "Epoch 73/300\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.83357\n",
            "60/60 - 0s - loss: 0.3245 - acc: 0.9500 - val_loss: 0.8939 - val_acc: 0.5000\n",
            "Epoch 74/300\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.83357\n",
            "60/60 - 0s - loss: 0.3091 - acc: 0.9833 - val_loss: 0.9188 - val_acc: 0.5000\n",
            "Epoch 75/300\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.83357\n",
            "60/60 - 0s - loss: 0.3325 - acc: 0.9833 - val_loss: 0.9404 - val_acc: 0.5000\n",
            "Epoch 76/300\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.83357\n",
            "60/60 - 0s - loss: 0.3221 - acc: 0.9500 - val_loss: 0.9595 - val_acc: 0.5000\n",
            "Epoch 77/300\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.83357\n",
            "60/60 - 0s - loss: 0.2739 - acc: 0.9833 - val_loss: 0.9663 - val_acc: 0.5000\n",
            "Epoch 78/300\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.83357\n",
            "60/60 - 0s - loss: 0.3047 - acc: 0.9667 - val_loss: 0.9443 - val_acc: 0.5000\n",
            "Epoch 79/300\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.83357\n",
            "60/60 - 0s - loss: 0.3384 - acc: 0.9333 - val_loss: 0.9136 - val_acc: 0.5000\n",
            "Epoch 80/300\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.83357\n",
            "60/60 - 0s - loss: 0.3051 - acc: 0.9333 - val_loss: 0.9171 - val_acc: 0.5000\n",
            "Epoch 81/300\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.83357\n",
            "60/60 - 0s - loss: 0.2623 - acc: 0.9667 - val_loss: 0.9069 - val_acc: 0.5000\n",
            "Epoch 82/300\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.83357\n",
            "60/60 - 0s - loss: 0.3150 - acc: 0.9667 - val_loss: 0.9153 - val_acc: 0.5000\n",
            "Epoch 83/300\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.83357\n",
            "60/60 - 0s - loss: 0.3063 - acc: 0.9833 - val_loss: 0.9000 - val_acc: 0.5000\n",
            "Epoch 84/300\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.83357\n",
            "60/60 - 0s - loss: 0.2632 - acc: 0.9667 - val_loss: 0.8959 - val_acc: 0.5000\n",
            "Epoch 85/300\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.83357\n",
            "60/60 - 0s - loss: 0.2770 - acc: 0.9500 - val_loss: 0.9093 - val_acc: 0.5000\n",
            "Epoch 86/300\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.83357\n",
            "60/60 - 0s - loss: 0.2265 - acc: 0.9833 - val_loss: 0.9230 - val_acc: 0.5000\n",
            "Epoch 87/300\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.83357\n",
            "60/60 - 0s - loss: 0.3108 - acc: 0.9667 - val_loss: 0.8968 - val_acc: 0.5000\n",
            "Epoch 88/300\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.83357\n",
            "60/60 - 0s - loss: 0.2475 - acc: 1.0000 - val_loss: 0.9046 - val_acc: 0.5000\n",
            "Epoch 89/300\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.83357\n",
            "60/60 - 0s - loss: 0.2617 - acc: 0.9667 - val_loss: 0.8979 - val_acc: 0.5000\n",
            "Epoch 90/300\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.83357\n",
            "60/60 - 0s - loss: 0.2670 - acc: 0.9667 - val_loss: 0.8836 - val_acc: 0.5000\n",
            "Epoch 91/300\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.83357\n",
            "60/60 - 0s - loss: 0.2526 - acc: 0.9500 - val_loss: 0.8840 - val_acc: 0.5000\n",
            "Epoch 92/300\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.83357\n",
            "60/60 - 0s - loss: 0.2695 - acc: 0.9500 - val_loss: 0.8939 - val_acc: 0.5000\n",
            "Epoch 93/300\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.83357\n",
            "60/60 - 0s - loss: 0.2671 - acc: 0.9667 - val_loss: 0.8930 - val_acc: 0.5000\n",
            "Epoch 94/300\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.83357\n",
            "60/60 - 0s - loss: 0.2662 - acc: 0.9500 - val_loss: 0.8738 - val_acc: 0.5000\n",
            "Epoch 95/300\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.83357\n",
            "60/60 - 0s - loss: 0.2756 - acc: 0.9167 - val_loss: 0.8966 - val_acc: 0.5000\n",
            "Epoch 96/300\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.83357\n",
            "60/60 - 0s - loss: 0.2189 - acc: 1.0000 - val_loss: 0.8586 - val_acc: 0.5000\n",
            "Epoch 97/300\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.83357\n",
            "60/60 - 0s - loss: 0.2248 - acc: 1.0000 - val_loss: 0.8566 - val_acc: 0.5000\n",
            "Epoch 98/300\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.83357\n",
            "60/60 - 0s - loss: 0.2657 - acc: 0.9167 - val_loss: 0.8539 - val_acc: 0.5000\n",
            "Epoch 99/300\n",
            "\n",
            "Epoch 00099: val_loss improved from 0.83357 to 0.81616, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2790 - acc: 0.9167 - val_loss: 0.8162 - val_acc: 0.5000\n",
            "Epoch 100/300\n",
            "\n",
            "Epoch 00100: val_loss improved from 0.81616 to 0.79298, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2598 - acc: 0.9833 - val_loss: 0.7930 - val_acc: 0.5000\n",
            "Epoch 101/300\n",
            "\n",
            "Epoch 00101: val_loss improved from 0.79298 to 0.77656, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2051 - acc: 0.9833 - val_loss: 0.7766 - val_acc: 0.5500\n",
            "Epoch 102/300\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.77656\n",
            "60/60 - 0s - loss: 0.1993 - acc: 0.9833 - val_loss: 0.8172 - val_acc: 0.5000\n",
            "Epoch 103/300\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.77656\n",
            "60/60 - 0s - loss: 0.1880 - acc: 0.9667 - val_loss: 0.8358 - val_acc: 0.5000\n",
            "Epoch 104/300\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.77656\n",
            "60/60 - 0s - loss: 0.1930 - acc: 1.0000 - val_loss: 0.8641 - val_acc: 0.5000\n",
            "Epoch 105/300\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 0.77656\n",
            "60/60 - 0s - loss: 0.2422 - acc: 0.9167 - val_loss: 0.8665 - val_acc: 0.5000\n",
            "Epoch 106/300\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 0.77656\n",
            "60/60 - 0s - loss: 0.2446 - acc: 0.9333 - val_loss: 0.8457 - val_acc: 0.5000\n",
            "Epoch 107/300\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.77656\n",
            "60/60 - 0s - loss: 0.1911 - acc: 0.9833 - val_loss: 0.8012 - val_acc: 0.5000\n",
            "Epoch 108/300\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 0.77656\n",
            "60/60 - 0s - loss: 0.2044 - acc: 0.9667 - val_loss: 0.7962 - val_acc: 0.5000\n",
            "Epoch 109/300\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.77656\n",
            "60/60 - 0s - loss: 0.2428 - acc: 0.9667 - val_loss: 0.8171 - val_acc: 0.5000\n",
            "Epoch 110/300\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.77656\n",
            "60/60 - 0s - loss: 0.2150 - acc: 0.9833 - val_loss: 0.8350 - val_acc: 0.5000\n",
            "Epoch 111/300\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 0.77656\n",
            "60/60 - 0s - loss: 0.1872 - acc: 0.9833 - val_loss: 0.8345 - val_acc: 0.5000\n",
            "Epoch 112/300\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 0.77656\n",
            "60/60 - 0s - loss: 0.2055 - acc: 0.9667 - val_loss: 0.8718 - val_acc: 0.5000\n",
            "Epoch 113/300\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.77656\n",
            "60/60 - 0s - loss: 0.1961 - acc: 0.9667 - val_loss: 0.8568 - val_acc: 0.5000\n",
            "Epoch 114/300\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.77656\n",
            "60/60 - 0s - loss: 0.1854 - acc: 1.0000 - val_loss: 0.8073 - val_acc: 0.6500\n",
            "Epoch 115/300\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.77656\n",
            "60/60 - 0s - loss: 0.2006 - acc: 0.9667 - val_loss: 0.7845 - val_acc: 0.6500\n",
            "Epoch 116/300\n",
            "\n",
            "Epoch 00116: val_loss improved from 0.77656 to 0.75532, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1757 - acc: 0.9833 - val_loss: 0.7553 - val_acc: 0.6500\n",
            "Epoch 117/300\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.75532\n",
            "60/60 - 0s - loss: 0.1758 - acc: 0.9833 - val_loss: 0.7611 - val_acc: 0.6500\n",
            "Epoch 118/300\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.75532\n",
            "60/60 - 0s - loss: 0.1478 - acc: 1.0000 - val_loss: 0.7558 - val_acc: 0.6500\n",
            "Epoch 119/300\n",
            "\n",
            "Epoch 00119: val_loss improved from 0.75532 to 0.73697, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2122 - acc: 0.9500 - val_loss: 0.7370 - val_acc: 0.6500\n",
            "Epoch 120/300\n",
            "\n",
            "Epoch 00120: val_loss improved from 0.73697 to 0.72932, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2227 - acc: 0.9500 - val_loss: 0.7293 - val_acc: 0.6500\n",
            "Epoch 121/300\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.72932\n",
            "60/60 - 0s - loss: 0.2043 - acc: 0.9833 - val_loss: 0.7321 - val_acc: 0.7000\n",
            "Epoch 122/300\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.72932\n",
            "60/60 - 0s - loss: 0.1460 - acc: 1.0000 - val_loss: 0.7378 - val_acc: 0.7000\n",
            "Epoch 123/300\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 0.72932\n",
            "60/60 - 0s - loss: 0.1834 - acc: 0.9667 - val_loss: 0.7423 - val_acc: 0.7000\n",
            "Epoch 124/300\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 0.72932\n",
            "60/60 - 0s - loss: 0.2176 - acc: 0.9333 - val_loss: 0.7547 - val_acc: 0.7000\n",
            "Epoch 125/300\n",
            "\n",
            "Epoch 00125: val_loss improved from 0.72932 to 0.70567, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2333 - acc: 0.9500 - val_loss: 0.7057 - val_acc: 0.7000\n",
            "Epoch 126/300\n",
            "\n",
            "Epoch 00126: val_loss improved from 0.70567 to 0.66722, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1944 - acc: 0.9500 - val_loss: 0.6672 - val_acc: 0.7000\n",
            "Epoch 127/300\n",
            "\n",
            "Epoch 00127: val_loss improved from 0.66722 to 0.66636, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1566 - acc: 0.9833 - val_loss: 0.6664 - val_acc: 0.7000\n",
            "Epoch 128/300\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 0.66636\n",
            "60/60 - 0s - loss: 0.1655 - acc: 0.9833 - val_loss: 0.6764 - val_acc: 0.7000\n",
            "Epoch 129/300\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.66636\n",
            "60/60 - 0s - loss: 0.1826 - acc: 0.9667 - val_loss: 0.6732 - val_acc: 0.7000\n",
            "Epoch 130/300\n",
            "\n",
            "Epoch 00130: val_loss improved from 0.66636 to 0.66443, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1583 - acc: 0.9833 - val_loss: 0.6644 - val_acc: 0.7000\n",
            "Epoch 131/300\n",
            "\n",
            "Epoch 00131: val_loss improved from 0.66443 to 0.63840, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1461 - acc: 1.0000 - val_loss: 0.6384 - val_acc: 0.7500\n",
            "Epoch 132/300\n",
            "\n",
            "Epoch 00132: val_loss improved from 0.63840 to 0.61864, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1767 - acc: 0.9667 - val_loss: 0.6186 - val_acc: 0.7000\n",
            "Epoch 133/300\n",
            "\n",
            "Epoch 00133: val_loss improved from 0.61864 to 0.61850, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1341 - acc: 1.0000 - val_loss: 0.6185 - val_acc: 0.7000\n",
            "Epoch 134/300\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.61850\n",
            "60/60 - 0s - loss: 0.1484 - acc: 0.9833 - val_loss: 0.6328 - val_acc: 0.7500\n",
            "Epoch 135/300\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 0.61850\n",
            "60/60 - 0s - loss: 0.1748 - acc: 0.9833 - val_loss: 0.6530 - val_acc: 0.7500\n",
            "Epoch 136/300\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.61850\n",
            "60/60 - 0s - loss: 0.1974 - acc: 0.9500 - val_loss: 0.6489 - val_acc: 0.7500\n",
            "Epoch 137/300\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 0.61850\n",
            "60/60 - 0s - loss: 0.1474 - acc: 0.9833 - val_loss: 0.6526 - val_acc: 0.7500\n",
            "Epoch 138/300\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.61850\n",
            "60/60 - 0s - loss: 0.1471 - acc: 0.9833 - val_loss: 0.6424 - val_acc: 0.7500\n",
            "Epoch 139/300\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.61850\n",
            "60/60 - 0s - loss: 0.1494 - acc: 0.9833 - val_loss: 0.6409 - val_acc: 0.7500\n",
            "Epoch 140/300\n",
            "\n",
            "Epoch 00140: val_loss improved from 0.61850 to 0.59535, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1678 - acc: 1.0000 - val_loss: 0.5954 - val_acc: 0.7000\n",
            "Epoch 141/300\n",
            "\n",
            "Epoch 00141: val_loss improved from 0.59535 to 0.58847, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2211 - acc: 0.9333 - val_loss: 0.5885 - val_acc: 0.7000\n",
            "Epoch 142/300\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.58847\n",
            "60/60 - 0s - loss: 0.1755 - acc: 0.9833 - val_loss: 0.6222 - val_acc: 0.7500\n",
            "Epoch 143/300\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 0.58847\n",
            "60/60 - 0s - loss: 0.1575 - acc: 0.9667 - val_loss: 0.6074 - val_acc: 0.7000\n",
            "Epoch 144/300\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 0.58847\n",
            "60/60 - 0s - loss: 0.1634 - acc: 0.9500 - val_loss: 0.5928 - val_acc: 0.7000\n",
            "Epoch 145/300\n",
            "\n",
            "Epoch 00145: val_loss improved from 0.58847 to 0.56508, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1675 - acc: 0.9833 - val_loss: 0.5651 - val_acc: 0.7500\n",
            "Epoch 146/300\n",
            "\n",
            "Epoch 00146: val_loss improved from 0.56508 to 0.54833, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1432 - acc: 0.9833 - val_loss: 0.5483 - val_acc: 0.7500\n",
            "Epoch 147/300\n",
            "\n",
            "Epoch 00147: val_loss improved from 0.54833 to 0.53585, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1730 - acc: 0.9667 - val_loss: 0.5358 - val_acc: 0.7500\n",
            "Epoch 148/300\n",
            "\n",
            "Epoch 00148: val_loss improved from 0.53585 to 0.53015, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2003 - acc: 0.9833 - val_loss: 0.5302 - val_acc: 0.7500\n",
            "Epoch 149/300\n",
            "\n",
            "Epoch 00149: val_loss improved from 0.53015 to 0.50706, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1917 - acc: 0.9667 - val_loss: 0.5071 - val_acc: 0.8000\n",
            "Epoch 150/300\n",
            "\n",
            "Epoch 00150: val_loss improved from 0.50706 to 0.50143, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1421 - acc: 1.0000 - val_loss: 0.5014 - val_acc: 0.8000\n",
            "Epoch 151/300\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.50143\n",
            "60/60 - 0s - loss: 0.1689 - acc: 0.9500 - val_loss: 0.5054 - val_acc: 0.8000\n",
            "Epoch 152/300\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.50143\n",
            "60/60 - 0s - loss: 0.1423 - acc: 0.9833 - val_loss: 0.5136 - val_acc: 0.8000\n",
            "Epoch 153/300\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 0.50143\n",
            "60/60 - 0s - loss: 0.1234 - acc: 1.0000 - val_loss: 0.5124 - val_acc: 0.8000\n",
            "Epoch 154/300\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.50143\n",
            "60/60 - 0s - loss: 0.1545 - acc: 0.9833 - val_loss: 0.5123 - val_acc: 0.8000\n",
            "Epoch 155/300\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.50143\n",
            "60/60 - 0s - loss: 0.1516 - acc: 0.9833 - val_loss: 0.5116 - val_acc: 0.8000\n",
            "Epoch 156/300\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.50143\n",
            "60/60 - 0s - loss: 0.1318 - acc: 1.0000 - val_loss: 0.5200 - val_acc: 0.8000\n",
            "Epoch 157/300\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.50143\n",
            "60/60 - 0s - loss: 0.1472 - acc: 0.9500 - val_loss: 0.5343 - val_acc: 0.8000\n",
            "Epoch 158/300\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.50143\n",
            "60/60 - 0s - loss: 0.1323 - acc: 1.0000 - val_loss: 0.5350 - val_acc: 0.8000\n",
            "Epoch 159/300\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.50143\n",
            "60/60 - 0s - loss: 0.1522 - acc: 0.9667 - val_loss: 0.5427 - val_acc: 0.7500\n",
            "Epoch 160/300\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.50143\n",
            "60/60 - 0s - loss: 0.1332 - acc: 0.9833 - val_loss: 0.5280 - val_acc: 0.8000\n",
            "Epoch 161/300\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.50143\n",
            "60/60 - 0s - loss: 0.1407 - acc: 0.9667 - val_loss: 0.5250 - val_acc: 0.8000\n",
            "Epoch 162/300\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.50143\n",
            "60/60 - 0s - loss: 0.1633 - acc: 0.9667 - val_loss: 0.5328 - val_acc: 0.8000\n",
            "Epoch 163/300\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.50143\n",
            "60/60 - 0s - loss: 0.1388 - acc: 1.0000 - val_loss: 0.5412 - val_acc: 0.8000\n",
            "Epoch 164/300\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.50143\n",
            "60/60 - 0s - loss: 0.1444 - acc: 0.9833 - val_loss: 0.5456 - val_acc: 0.8000\n",
            "Epoch 165/300\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.50143\n",
            "60/60 - 0s - loss: 0.1500 - acc: 0.9833 - val_loss: 0.5349 - val_acc: 0.8000\n",
            "Epoch 166/300\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 0.50143\n",
            "60/60 - 0s - loss: 0.1403 - acc: 0.9833 - val_loss: 0.5260 - val_acc: 0.8000\n",
            "Epoch 167/300\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.50143\n",
            "60/60 - 0s - loss: 0.1176 - acc: 1.0000 - val_loss: 0.5205 - val_acc: 0.8000\n",
            "Epoch 168/300\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.50143\n",
            "60/60 - 0s - loss: 0.1341 - acc: 0.9833 - val_loss: 0.5176 - val_acc: 0.8000\n",
            "Epoch 169/300\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 0.50143\n",
            "60/60 - 0s - loss: 0.1537 - acc: 1.0000 - val_loss: 0.5148 - val_acc: 0.8000\n",
            "Epoch 170/300\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.50143\n",
            "60/60 - 0s - loss: 0.1294 - acc: 0.9833 - val_loss: 0.5084 - val_acc: 0.8000\n",
            "Epoch 171/300\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.50143\n",
            "60/60 - 0s - loss: 0.1401 - acc: 0.9833 - val_loss: 0.5052 - val_acc: 0.8000\n",
            "Epoch 172/300\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.50143\n",
            "60/60 - 0s - loss: 0.1657 - acc: 0.9500 - val_loss: 0.5136 - val_acc: 0.8000\n",
            "Epoch 173/300\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.50143\n",
            "60/60 - 0s - loss: 0.1546 - acc: 0.9833 - val_loss: 0.5141 - val_acc: 0.8000\n",
            "Epoch 174/300\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.50143\n",
            "60/60 - 0s - loss: 0.1259 - acc: 1.0000 - val_loss: 0.5169 - val_acc: 0.7500\n",
            "Epoch 175/300\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.50143\n",
            "60/60 - 0s - loss: 0.1442 - acc: 1.0000 - val_loss: 0.5173 - val_acc: 0.8000\n",
            "Epoch 176/300\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 0.50143\n",
            "60/60 - 0s - loss: 0.1135 - acc: 0.9833 - val_loss: 0.5142 - val_acc: 0.8000\n",
            "Epoch 177/300\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.50143\n",
            "60/60 - 0s - loss: 0.1221 - acc: 1.0000 - val_loss: 0.5065 - val_acc: 0.8000\n",
            "Epoch 178/300\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.50143\n",
            "60/60 - 0s - loss: 0.1426 - acc: 0.9833 - val_loss: 0.5083 - val_acc: 0.8000\n",
            "Epoch 179/300\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.50143\n",
            "60/60 - 0s - loss: 0.1075 - acc: 0.9833 - val_loss: 0.5076 - val_acc: 0.8000\n",
            "Epoch 180/300\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.50143\n",
            "60/60 - 0s - loss: 0.1380 - acc: 0.9833 - val_loss: 0.5027 - val_acc: 0.8000\n",
            "Epoch 181/300\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.50143\n",
            "60/60 - 0s - loss: 0.1400 - acc: 0.9833 - val_loss: 0.5064 - val_acc: 0.8000\n",
            "Epoch 182/300\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.50143\n",
            "60/60 - 0s - loss: 0.1249 - acc: 0.9833 - val_loss: 0.5098 - val_acc: 0.8500\n",
            "Epoch 183/300\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.50143\n",
            "60/60 - 0s - loss: 0.1299 - acc: 0.9667 - val_loss: 0.5029 - val_acc: 0.8000\n",
            "Epoch 184/300\n",
            "\n",
            "Epoch 00184: val_loss improved from 0.50143 to 0.49680, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1230 - acc: 1.0000 - val_loss: 0.4968 - val_acc: 0.8000\n",
            "Epoch 185/300\n",
            "\n",
            "Epoch 00185: val_loss improved from 0.49680 to 0.49590, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1587 - acc: 0.9500 - val_loss: 0.4959 - val_acc: 0.8000\n",
            "Epoch 186/300\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.49590\n",
            "60/60 - 0s - loss: 0.1080 - acc: 1.0000 - val_loss: 0.5004 - val_acc: 0.8000\n",
            "Epoch 187/300\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 0.49590\n",
            "60/60 - 0s - loss: 0.1378 - acc: 0.9667 - val_loss: 0.5057 - val_acc: 0.8000\n",
            "Epoch 188/300\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.49590\n",
            "60/60 - 0s - loss: 0.1822 - acc: 0.9167 - val_loss: 0.5175 - val_acc: 0.7500\n",
            "Epoch 189/300\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.49590\n",
            "60/60 - 0s - loss: 0.1365 - acc: 0.9667 - val_loss: 0.5352 - val_acc: 0.8000\n",
            "Epoch 190/300\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.49590\n",
            "60/60 - 0s - loss: 0.1362 - acc: 0.9667 - val_loss: 0.5316 - val_acc: 0.8000\n",
            "Epoch 191/300\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.49590\n",
            "60/60 - 0s - loss: 0.0994 - acc: 1.0000 - val_loss: 0.5239 - val_acc: 0.7000\n",
            "Epoch 192/300\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 0.49590\n",
            "60/60 - 0s - loss: 0.0976 - acc: 1.0000 - val_loss: 0.5199 - val_acc: 0.7000\n",
            "Epoch 193/300\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.49590\n",
            "60/60 - 0s - loss: 0.1086 - acc: 1.0000 - val_loss: 0.5249 - val_acc: 0.7000\n",
            "Epoch 194/300\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.49590\n",
            "60/60 - 0s - loss: 0.1007 - acc: 1.0000 - val_loss: 0.5242 - val_acc: 0.7000\n",
            "Epoch 195/300\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.49590\n",
            "60/60 - 0s - loss: 0.1260 - acc: 0.9667 - val_loss: 0.5258 - val_acc: 0.7000\n",
            "Epoch 196/300\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.49590\n",
            "60/60 - 0s - loss: 0.1062 - acc: 0.9833 - val_loss: 0.5298 - val_acc: 0.7500\n",
            "Epoch 197/300\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.49590\n",
            "60/60 - 0s - loss: 0.1312 - acc: 0.9500 - val_loss: 0.5316 - val_acc: 0.7500\n",
            "Epoch 198/300\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.49590\n",
            "60/60 - 0s - loss: 0.1284 - acc: 0.9833 - val_loss: 0.5357 - val_acc: 0.8000\n",
            "Epoch 199/300\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.49590\n",
            "60/60 - 0s - loss: 0.1121 - acc: 1.0000 - val_loss: 0.5238 - val_acc: 0.7500\n",
            "Epoch 200/300\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.49590\n",
            "60/60 - 0s - loss: 0.1288 - acc: 1.0000 - val_loss: 0.5143 - val_acc: 0.7500\n",
            "Epoch 201/300\n",
            "\n",
            "Epoch 00201: val_loss did not improve from 0.49590\n",
            "60/60 - 0s - loss: 0.0784 - acc: 1.0000 - val_loss: 0.5039 - val_acc: 0.8000\n",
            "Epoch 202/300\n",
            "\n",
            "Epoch 00202: val_loss did not improve from 0.49590\n",
            "60/60 - 0s - loss: 0.0964 - acc: 1.0000 - val_loss: 0.5000 - val_acc: 0.8000\n",
            "Epoch 203/300\n",
            "\n",
            "Epoch 00203: val_loss did not improve from 0.49590\n",
            "60/60 - 0s - loss: 0.1168 - acc: 0.9667 - val_loss: 0.4994 - val_acc: 0.8000\n",
            "Epoch 204/300\n",
            "\n",
            "Epoch 00204: val_loss did not improve from 0.49590\n",
            "60/60 - 0s - loss: 0.1215 - acc: 0.9667 - val_loss: 0.5073 - val_acc: 0.8000\n",
            "Epoch 205/300\n",
            "\n",
            "Epoch 00205: val_loss did not improve from 0.49590\n",
            "60/60 - 0s - loss: 0.0911 - acc: 1.0000 - val_loss: 0.5214 - val_acc: 0.7500\n",
            "Epoch 206/300\n",
            "\n",
            "Epoch 00206: val_loss did not improve from 0.49590\n",
            "60/60 - 0s - loss: 0.1393 - acc: 0.9833 - val_loss: 0.5377 - val_acc: 0.7500\n",
            "Epoch 207/300\n",
            "\n",
            "Epoch 00207: val_loss did not improve from 0.49590\n",
            "60/60 - 0s - loss: 0.1018 - acc: 1.0000 - val_loss: 0.5454 - val_acc: 0.7500\n",
            "Epoch 208/300\n",
            "\n",
            "Epoch 00208: val_loss did not improve from 0.49590\n",
            "60/60 - 0s - loss: 0.1276 - acc: 1.0000 - val_loss: 0.5461 - val_acc: 0.7500\n",
            "Epoch 209/300\n",
            "\n",
            "Epoch 00209: val_loss did not improve from 0.49590\n",
            "60/60 - 0s - loss: 0.1112 - acc: 0.9833 - val_loss: 0.5461 - val_acc: 0.8000\n",
            "Epoch 210/300\n",
            "\n",
            "Epoch 00210: val_loss did not improve from 0.49590\n",
            "60/60 - 0s - loss: 0.0997 - acc: 1.0000 - val_loss: 0.5428 - val_acc: 0.7500\n",
            "Epoch 211/300\n",
            "\n",
            "Epoch 00211: val_loss did not improve from 0.49590\n",
            "60/60 - 0s - loss: 0.1231 - acc: 0.9833 - val_loss: 0.5371 - val_acc: 0.8000\n",
            "Epoch 212/300\n",
            "\n",
            "Epoch 00212: val_loss did not improve from 0.49590\n",
            "60/60 - 0s - loss: 0.1067 - acc: 1.0000 - val_loss: 0.5251 - val_acc: 0.8000\n",
            "Epoch 213/300\n",
            "\n",
            "Epoch 00213: val_loss did not improve from 0.49590\n",
            "60/60 - 0s - loss: 0.0841 - acc: 1.0000 - val_loss: 0.5108 - val_acc: 0.8000\n",
            "Epoch 214/300\n",
            "\n",
            "Epoch 00214: val_loss did not improve from 0.49590\n",
            "60/60 - 0s - loss: 0.1211 - acc: 0.9667 - val_loss: 0.5003 - val_acc: 0.8000\n",
            "Epoch 215/300\n",
            "\n",
            "Epoch 00215: val_loss improved from 0.49590 to 0.49486, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1069 - acc: 1.0000 - val_loss: 0.4949 - val_acc: 0.8000\n",
            "Epoch 216/300\n",
            "\n",
            "Epoch 00216: val_loss did not improve from 0.49486\n",
            "60/60 - 0s - loss: 0.1350 - acc: 0.9833 - val_loss: 0.4961 - val_acc: 0.8000\n",
            "Epoch 217/300\n",
            "\n",
            "Epoch 00217: val_loss did not improve from 0.49486\n",
            "60/60 - 0s - loss: 0.1179 - acc: 0.9667 - val_loss: 0.5067 - val_acc: 0.8000\n",
            "Epoch 218/300\n",
            "\n",
            "Epoch 00218: val_loss did not improve from 0.49486\n",
            "60/60 - 0s - loss: 0.1097 - acc: 1.0000 - val_loss: 0.5168 - val_acc: 0.8000\n",
            "Epoch 219/300\n",
            "\n",
            "Epoch 00219: val_loss did not improve from 0.49486\n",
            "60/60 - 0s - loss: 0.1090 - acc: 1.0000 - val_loss: 0.5322 - val_acc: 0.8000\n",
            "Epoch 220/300\n",
            "\n",
            "Epoch 00220: val_loss did not improve from 0.49486\n",
            "60/60 - 0s - loss: 0.1295 - acc: 0.9833 - val_loss: 0.5462 - val_acc: 0.8000\n",
            "Epoch 221/300\n",
            "\n",
            "Epoch 00221: val_loss did not improve from 0.49486\n",
            "60/60 - 0s - loss: 0.0796 - acc: 1.0000 - val_loss: 0.5562 - val_acc: 0.8000\n",
            "Epoch 222/300\n",
            "\n",
            "Epoch 00222: val_loss did not improve from 0.49486\n",
            "60/60 - 0s - loss: 0.1602 - acc: 0.9333 - val_loss: 0.5542 - val_acc: 0.8000\n",
            "Epoch 223/300\n",
            "\n",
            "Epoch 00223: val_loss did not improve from 0.49486\n",
            "60/60 - 0s - loss: 0.1148 - acc: 1.0000 - val_loss: 0.5579 - val_acc: 0.8000\n",
            "Epoch 224/300\n",
            "\n",
            "Epoch 00224: val_loss did not improve from 0.49486\n",
            "60/60 - 0s - loss: 0.0885 - acc: 1.0000 - val_loss: 0.5550 - val_acc: 0.7500\n",
            "Epoch 225/300\n",
            "\n",
            "Epoch 00225: val_loss did not improve from 0.49486\n",
            "60/60 - 0s - loss: 0.1009 - acc: 1.0000 - val_loss: 0.5527 - val_acc: 0.8000\n",
            "Epoch 226/300\n",
            "\n",
            "Epoch 00226: val_loss did not improve from 0.49486\n",
            "60/60 - 0s - loss: 0.1291 - acc: 0.9667 - val_loss: 0.5458 - val_acc: 0.7500\n",
            "Epoch 227/300\n",
            "\n",
            "Epoch 00227: val_loss did not improve from 0.49486\n",
            "60/60 - 0s - loss: 0.1066 - acc: 0.9833 - val_loss: 0.5454 - val_acc: 0.8000\n",
            "Epoch 228/300\n",
            "\n",
            "Epoch 00228: val_loss did not improve from 0.49486\n",
            "60/60 - 0s - loss: 0.1014 - acc: 1.0000 - val_loss: 0.5433 - val_acc: 0.8000\n",
            "Epoch 229/300\n",
            "\n",
            "Epoch 00229: val_loss did not improve from 0.49486\n",
            "60/60 - 0s - loss: 0.1068 - acc: 0.9833 - val_loss: 0.5495 - val_acc: 0.8000\n",
            "Epoch 230/300\n",
            "\n",
            "Epoch 00230: val_loss did not improve from 0.49486\n",
            "60/60 - 0s - loss: 0.1230 - acc: 0.9667 - val_loss: 0.5510 - val_acc: 0.8000\n",
            "Epoch 231/300\n",
            "\n",
            "Epoch 00231: val_loss did not improve from 0.49486\n",
            "60/60 - 0s - loss: 0.0835 - acc: 1.0000 - val_loss: 0.5392 - val_acc: 0.8000\n",
            "Epoch 232/300\n",
            "\n",
            "Epoch 00232: val_loss did not improve from 0.49486\n",
            "60/60 - 0s - loss: 0.0968 - acc: 1.0000 - val_loss: 0.5246 - val_acc: 0.8000\n",
            "Epoch 233/300\n",
            "\n",
            "Epoch 00233: val_loss did not improve from 0.49486\n",
            "60/60 - 0s - loss: 0.0963 - acc: 0.9833 - val_loss: 0.5132 - val_acc: 0.8000\n",
            "Epoch 234/300\n",
            "\n",
            "Epoch 00234: val_loss did not improve from 0.49486\n",
            "60/60 - 0s - loss: 0.1023 - acc: 0.9833 - val_loss: 0.5087 - val_acc: 0.8000\n",
            "Epoch 235/300\n",
            "\n",
            "Epoch 00235: val_loss did not improve from 0.49486\n",
            "60/60 - 0s - loss: 0.1052 - acc: 1.0000 - val_loss: 0.5060 - val_acc: 0.8000\n",
            "Epoch 236/300\n",
            "\n",
            "Epoch 00236: val_loss did not improve from 0.49486\n",
            "60/60 - 0s - loss: 0.1186 - acc: 0.9500 - val_loss: 0.5028 - val_acc: 0.8000\n",
            "Epoch 237/300\n",
            "\n",
            "Epoch 00237: val_loss did not improve from 0.49486\n",
            "60/60 - 0s - loss: 0.1135 - acc: 0.9833 - val_loss: 0.4974 - val_acc: 0.8000\n",
            "Epoch 238/300\n",
            "\n",
            "Epoch 00238: val_loss improved from 0.49486 to 0.49248, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1087 - acc: 0.9833 - val_loss: 0.4925 - val_acc: 0.8000\n",
            "Epoch 239/300\n",
            "\n",
            "Epoch 00239: val_loss did not improve from 0.49248\n",
            "60/60 - 0s - loss: 0.1203 - acc: 0.9833 - val_loss: 0.4934 - val_acc: 0.8000\n",
            "Epoch 240/300\n",
            "\n",
            "Epoch 00240: val_loss did not improve from 0.49248\n",
            "60/60 - 0s - loss: 0.0965 - acc: 1.0000 - val_loss: 0.4996 - val_acc: 0.8000\n",
            "Epoch 241/300\n",
            "\n",
            "Epoch 00241: val_loss did not improve from 0.49248\n",
            "60/60 - 0s - loss: 0.0934 - acc: 1.0000 - val_loss: 0.4973 - val_acc: 0.8000\n",
            "Epoch 242/300\n",
            "\n",
            "Epoch 00242: val_loss did not improve from 0.49248\n",
            "60/60 - 0s - loss: 0.1071 - acc: 1.0000 - val_loss: 0.4965 - val_acc: 0.8000\n",
            "Epoch 243/300\n",
            "\n",
            "Epoch 00243: val_loss did not improve from 0.49248\n",
            "60/60 - 0s - loss: 0.1246 - acc: 0.9833 - val_loss: 0.5018 - val_acc: 0.8000\n",
            "Epoch 244/300\n",
            "\n",
            "Epoch 00244: val_loss did not improve from 0.49248\n",
            "60/60 - 0s - loss: 0.0913 - acc: 1.0000 - val_loss: 0.5025 - val_acc: 0.8000\n",
            "Epoch 245/300\n",
            "\n",
            "Epoch 00245: val_loss did not improve from 0.49248\n",
            "60/60 - 0s - loss: 0.1755 - acc: 0.9167 - val_loss: 0.5045 - val_acc: 0.7500\n",
            "Epoch 246/300\n",
            "\n",
            "Epoch 00246: val_loss did not improve from 0.49248\n",
            "60/60 - 0s - loss: 0.1226 - acc: 0.9667 - val_loss: 0.4974 - val_acc: 0.8000\n",
            "Epoch 247/300\n",
            "\n",
            "Epoch 00247: val_loss improved from 0.49248 to 0.48799, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.0728 - acc: 1.0000 - val_loss: 0.4880 - val_acc: 0.8000\n",
            "Epoch 248/300\n",
            "\n",
            "Epoch 00248: val_loss improved from 0.48799 to 0.47721, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.0814 - acc: 1.0000 - val_loss: 0.4772 - val_acc: 0.8000\n",
            "Epoch 249/300\n",
            "\n",
            "Epoch 00249: val_loss improved from 0.47721 to 0.47598, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1099 - acc: 0.9833 - val_loss: 0.4760 - val_acc: 0.8500\n",
            "Epoch 250/300\n",
            "\n",
            "Epoch 00250: val_loss improved from 0.47598 to 0.47311, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.0754 - acc: 1.0000 - val_loss: 0.4731 - val_acc: 0.8000\n",
            "Epoch 251/300\n",
            "\n",
            "Epoch 00251: val_loss did not improve from 0.47311\n",
            "60/60 - 0s - loss: 0.0667 - acc: 1.0000 - val_loss: 0.4762 - val_acc: 0.8000\n",
            "Epoch 252/300\n",
            "\n",
            "Epoch 00252: val_loss did not improve from 0.47311\n",
            "60/60 - 0s - loss: 0.0680 - acc: 1.0000 - val_loss: 0.4808 - val_acc: 0.8000\n",
            "Epoch 253/300\n",
            "\n",
            "Epoch 00253: val_loss did not improve from 0.47311\n",
            "60/60 - 0s - loss: 0.0866 - acc: 1.0000 - val_loss: 0.4800 - val_acc: 0.8000\n",
            "Epoch 254/300\n",
            "\n",
            "Epoch 00254: val_loss did not improve from 0.47311\n",
            "60/60 - 0s - loss: 0.0832 - acc: 1.0000 - val_loss: 0.4831 - val_acc: 0.8000\n",
            "Epoch 255/300\n",
            "\n",
            "Epoch 00255: val_loss did not improve from 0.47311\n",
            "60/60 - 0s - loss: 0.1162 - acc: 0.9667 - val_loss: 0.4857 - val_acc: 0.8000\n",
            "Epoch 256/300\n",
            "\n",
            "Epoch 00256: val_loss did not improve from 0.47311\n",
            "60/60 - 0s - loss: 0.1060 - acc: 1.0000 - val_loss: 0.4891 - val_acc: 0.8000\n",
            "Epoch 257/300\n",
            "\n",
            "Epoch 00257: val_loss did not improve from 0.47311\n",
            "60/60 - 0s - loss: 0.0750 - acc: 1.0000 - val_loss: 0.4904 - val_acc: 0.8000\n",
            "Epoch 258/300\n",
            "\n",
            "Epoch 00258: val_loss did not improve from 0.47311\n",
            "60/60 - 0s - loss: 0.1075 - acc: 0.9833 - val_loss: 0.4815 - val_acc: 0.8000\n",
            "Epoch 259/300\n",
            "\n",
            "Epoch 00259: val_loss improved from 0.47311 to 0.46974, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.0751 - acc: 1.0000 - val_loss: 0.4697 - val_acc: 0.8000\n",
            "Epoch 260/300\n",
            "\n",
            "Epoch 00260: val_loss improved from 0.46974 to 0.45891, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.0705 - acc: 1.0000 - val_loss: 0.4589 - val_acc: 0.8000\n",
            "Epoch 261/300\n",
            "\n",
            "Epoch 00261: val_loss did not improve from 0.45891\n",
            "60/60 - 0s - loss: 0.0663 - acc: 1.0000 - val_loss: 0.4597 - val_acc: 0.8000\n",
            "Epoch 262/300\n",
            "\n",
            "Epoch 00262: val_loss did not improve from 0.45891\n",
            "60/60 - 0s - loss: 0.1122 - acc: 0.9833 - val_loss: 0.4633 - val_acc: 0.8000\n",
            "Epoch 263/300\n",
            "\n",
            "Epoch 00263: val_loss did not improve from 0.45891\n",
            "60/60 - 0s - loss: 0.0903 - acc: 0.9833 - val_loss: 0.4670 - val_acc: 0.8000\n",
            "Epoch 264/300\n",
            "\n",
            "Epoch 00264: val_loss did not improve from 0.45891\n",
            "60/60 - 0s - loss: 0.0861 - acc: 1.0000 - val_loss: 0.4760 - val_acc: 0.8000\n",
            "Epoch 265/300\n",
            "\n",
            "Epoch 00265: val_loss did not improve from 0.45891\n",
            "60/60 - 0s - loss: 0.0719 - acc: 1.0000 - val_loss: 0.4803 - val_acc: 0.8000\n",
            "Epoch 266/300\n",
            "\n",
            "Epoch 00266: val_loss did not improve from 0.45891\n",
            "60/60 - 0s - loss: 0.0794 - acc: 1.0000 - val_loss: 0.4807 - val_acc: 0.8000\n",
            "Epoch 267/300\n",
            "\n",
            "Epoch 00267: val_loss did not improve from 0.45891\n",
            "60/60 - 0s - loss: 0.0831 - acc: 0.9833 - val_loss: 0.4899 - val_acc: 0.7500\n",
            "Epoch 268/300\n",
            "\n",
            "Epoch 00268: val_loss did not improve from 0.45891\n",
            "60/60 - 0s - loss: 0.0830 - acc: 0.9833 - val_loss: 0.4972 - val_acc: 0.7500\n",
            "Epoch 269/300\n",
            "\n",
            "Epoch 00269: val_loss did not improve from 0.45891\n",
            "60/60 - 0s - loss: 0.1129 - acc: 0.9833 - val_loss: 0.4930 - val_acc: 0.7500\n",
            "Epoch 270/300\n",
            "\n",
            "Epoch 00270: val_loss did not improve from 0.45891\n",
            "60/60 - 0s - loss: 0.1125 - acc: 0.9833 - val_loss: 0.4819 - val_acc: 0.7500\n",
            "Epoch 271/300\n",
            "\n",
            "Epoch 00271: val_loss did not improve from 0.45891\n",
            "60/60 - 0s - loss: 0.0829 - acc: 0.9833 - val_loss: 0.4740 - val_acc: 0.8000\n",
            "Epoch 272/300\n",
            "\n",
            "Epoch 00272: val_loss did not improve from 0.45891\n",
            "60/60 - 0s - loss: 0.1148 - acc: 0.9667 - val_loss: 0.4721 - val_acc: 0.8000\n",
            "Epoch 273/300\n",
            "\n",
            "Epoch 00273: val_loss did not improve from 0.45891\n",
            "60/60 - 0s - loss: 0.0881 - acc: 1.0000 - val_loss: 0.4827 - val_acc: 0.8000\n",
            "Epoch 274/300\n",
            "\n",
            "Epoch 00274: val_loss did not improve from 0.45891\n",
            "60/60 - 0s - loss: 0.1034 - acc: 0.9833 - val_loss: 0.4914 - val_acc: 0.7500\n",
            "Epoch 275/300\n",
            "\n",
            "Epoch 00275: val_loss did not improve from 0.45891\n",
            "60/60 - 0s - loss: 0.0818 - acc: 1.0000 - val_loss: 0.5001 - val_acc: 0.7500\n",
            "Epoch 276/300\n",
            "\n",
            "Epoch 00276: val_loss did not improve from 0.45891\n",
            "60/60 - 0s - loss: 0.1123 - acc: 1.0000 - val_loss: 0.5116 - val_acc: 0.8000\n",
            "Epoch 277/300\n",
            "\n",
            "Epoch 00277: val_loss did not improve from 0.45891\n",
            "60/60 - 0s - loss: 0.0834 - acc: 1.0000 - val_loss: 0.5134 - val_acc: 0.7500\n",
            "Epoch 278/300\n",
            "\n",
            "Epoch 00278: val_loss did not improve from 0.45891\n",
            "60/60 - 0s - loss: 0.0862 - acc: 0.9833 - val_loss: 0.5150 - val_acc: 0.7500\n",
            "Epoch 279/300\n",
            "\n",
            "Epoch 00279: val_loss did not improve from 0.45891\n",
            "60/60 - 0s - loss: 0.0767 - acc: 1.0000 - val_loss: 0.5244 - val_acc: 0.7500\n",
            "Epoch 280/300\n",
            "\n",
            "Epoch 00280: val_loss did not improve from 0.45891\n",
            "60/60 - 0s - loss: 0.0757 - acc: 1.0000 - val_loss: 0.5290 - val_acc: 0.7500\n",
            "Epoch 281/300\n",
            "\n",
            "Epoch 00281: val_loss did not improve from 0.45891\n",
            "60/60 - 0s - loss: 0.0783 - acc: 1.0000 - val_loss: 0.5273 - val_acc: 0.7500\n",
            "Epoch 282/300\n",
            "\n",
            "Epoch 00282: val_loss did not improve from 0.45891\n",
            "60/60 - 0s - loss: 0.0814 - acc: 1.0000 - val_loss: 0.5216 - val_acc: 0.8000\n",
            "Epoch 283/300\n",
            "\n",
            "Epoch 00283: val_loss did not improve from 0.45891\n",
            "60/60 - 0s - loss: 0.0544 - acc: 1.0000 - val_loss: 0.5133 - val_acc: 0.8000\n",
            "Epoch 284/300\n",
            "\n",
            "Epoch 00284: val_loss did not improve from 0.45891\n",
            "60/60 - 0s - loss: 0.1087 - acc: 0.9667 - val_loss: 0.5024 - val_acc: 0.8000\n",
            "Epoch 285/300\n",
            "\n",
            "Epoch 00285: val_loss did not improve from 0.45891\n",
            "60/60 - 0s - loss: 0.0832 - acc: 0.9833 - val_loss: 0.4906 - val_acc: 0.8000\n",
            "Epoch 286/300\n",
            "\n",
            "Epoch 00286: val_loss did not improve from 0.45891\n",
            "60/60 - 0s - loss: 0.0554 - acc: 1.0000 - val_loss: 0.4874 - val_acc: 0.8000\n",
            "Epoch 287/300\n",
            "\n",
            "Epoch 00287: val_loss did not improve from 0.45891\n",
            "60/60 - 0s - loss: 0.0897 - acc: 0.9833 - val_loss: 0.4987 - val_acc: 0.8000\n",
            "Epoch 288/300\n",
            "\n",
            "Epoch 00288: val_loss did not improve from 0.45891\n",
            "60/60 - 0s - loss: 0.1120 - acc: 0.9833 - val_loss: 0.5197 - val_acc: 0.7000\n",
            "Epoch 289/300\n",
            "\n",
            "Epoch 00289: val_loss did not improve from 0.45891\n",
            "60/60 - 0s - loss: 0.0839 - acc: 1.0000 - val_loss: 0.5312 - val_acc: 0.7500\n",
            "Epoch 290/300\n",
            "\n",
            "Epoch 00290: val_loss did not improve from 0.45891\n",
            "60/60 - 0s - loss: 0.0867 - acc: 0.9833 - val_loss: 0.5328 - val_acc: 0.8000\n",
            "Epoch 291/300\n",
            "\n",
            "Epoch 00291: val_loss did not improve from 0.45891\n",
            "60/60 - 0s - loss: 0.0907 - acc: 1.0000 - val_loss: 0.5351 - val_acc: 0.8000\n",
            "Epoch 292/300\n",
            "\n",
            "Epoch 00292: val_loss did not improve from 0.45891\n",
            "60/60 - 0s - loss: 0.0715 - acc: 1.0000 - val_loss: 0.5337 - val_acc: 0.8000\n",
            "Epoch 293/300\n",
            "\n",
            "Epoch 00293: val_loss did not improve from 0.45891\n",
            "60/60 - 0s - loss: 0.1055 - acc: 0.9833 - val_loss: 0.5255 - val_acc: 0.8000\n",
            "Epoch 294/300\n",
            "\n",
            "Epoch 00294: val_loss did not improve from 0.45891\n",
            "60/60 - 0s - loss: 0.1191 - acc: 0.9667 - val_loss: 0.5170 - val_acc: 0.7000\n",
            "Epoch 295/300\n",
            "\n",
            "Epoch 00295: val_loss did not improve from 0.45891\n",
            "60/60 - 0s - loss: 0.0784 - acc: 1.0000 - val_loss: 0.5204 - val_acc: 0.6500\n",
            "Epoch 296/300\n",
            "\n",
            "Epoch 00296: val_loss did not improve from 0.45891\n",
            "60/60 - 0s - loss: 0.0751 - acc: 1.0000 - val_loss: 0.5167 - val_acc: 0.7000\n",
            "Epoch 297/300\n",
            "\n",
            "Epoch 00297: val_loss did not improve from 0.45891\n",
            "60/60 - 0s - loss: 0.0823 - acc: 1.0000 - val_loss: 0.5290 - val_acc: 0.7000\n",
            "Epoch 298/300\n",
            "\n",
            "Epoch 00298: val_loss did not improve from 0.45891\n",
            "60/60 - 0s - loss: 0.0785 - acc: 1.0000 - val_loss: 0.5511 - val_acc: 0.6500\n",
            "Epoch 299/300\n",
            "\n",
            "Epoch 00299: val_loss did not improve from 0.45891\n",
            "60/60 - 0s - loss: 0.0762 - acc: 1.0000 - val_loss: 0.5293 - val_acc: 0.7000\n",
            "Epoch 300/300\n",
            "\n",
            "Epoch 00300: val_loss did not improve from 0.45891\n",
            "60/60 - 0s - loss: 0.0747 - acc: 1.0000 - val_loss: 0.5264 - val_acc: 0.6500\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "# Evaluate on test data\n",
            "40/40 [==============================] - 0s 2ms/sample - loss: 0.7046 - acc: 0.6750\n",
            "test loss, test acc: [0.7045953227207065, 0.675]\n",
            "EEG_Deep/Data2A/parsed_P05T.mat\n",
            "Filtering of Training Data Finished\n",
            "EEG_Deep/Data2A/parsed_P05E.mat\n",
            "Filtering of Testing Data Finished\n",
            "(1, 2)\n",
            "Finding labels in training data\n",
            "Finding labels in testing data\n",
            "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
            "(60, 12, 1536)\n",
            "(60,)\n",
            "(20, 12, 1536)\n",
            "(20,)\n",
            "[[1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]]\n",
            "X_train shape: (60, 1, 12, 1536)\n",
            "60 train samples\n",
            "20 val samples\n",
            "(40, 12, 1536)\n",
            "(40, 1)\n",
            "X_train shape: (40, 1, 12, 1536)\n",
            "40 train samples\n",
            "Train on 60 samples, validate on 20 samples\n",
            "Epoch 1/300\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 1.37511, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 1s - loss: 1.2679 - acc: 0.5667 - val_loss: 1.3751 - val_acc: 0.0500\n",
            "Epoch 2/300\n",
            "\n",
            "Epoch 00002: val_loss improved from 1.37511 to 1.35123, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 1.0226 - acc: 0.6833 - val_loss: 1.3512 - val_acc: 0.0000e+00\n",
            "Epoch 3/300\n",
            "\n",
            "Epoch 00003: val_loss improved from 1.35123 to 1.33310, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.8758 - acc: 0.6667 - val_loss: 1.3331 - val_acc: 0.0000e+00\n",
            "Epoch 4/300\n",
            "\n",
            "Epoch 00004: val_loss improved from 1.33310 to 1.31797, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.7752 - acc: 0.6667 - val_loss: 1.3180 - val_acc: 0.0000e+00\n",
            "Epoch 5/300\n",
            "\n",
            "Epoch 00005: val_loss improved from 1.31797 to 1.30076, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.7500 - acc: 0.6667 - val_loss: 1.3008 - val_acc: 0.0000e+00\n",
            "Epoch 6/300\n",
            "\n",
            "Epoch 00006: val_loss improved from 1.30076 to 1.28765, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.7090 - acc: 0.6667 - val_loss: 1.2876 - val_acc: 0.0000e+00\n",
            "Epoch 7/300\n",
            "\n",
            "Epoch 00007: val_loss improved from 1.28765 to 1.27519, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6813 - acc: 0.6667 - val_loss: 1.2752 - val_acc: 0.0000e+00\n",
            "Epoch 8/300\n",
            "\n",
            "Epoch 00008: val_loss improved from 1.27519 to 1.26136, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6698 - acc: 0.6667 - val_loss: 1.2614 - val_acc: 0.0000e+00\n",
            "Epoch 9/300\n",
            "\n",
            "Epoch 00009: val_loss improved from 1.26136 to 1.24557, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6242 - acc: 0.7000 - val_loss: 1.2456 - val_acc: 0.0000e+00\n",
            "Epoch 10/300\n",
            "\n",
            "Epoch 00010: val_loss improved from 1.24557 to 1.23439, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6243 - acc: 0.7167 - val_loss: 1.2344 - val_acc: 0.0000e+00\n",
            "Epoch 11/300\n",
            "\n",
            "Epoch 00011: val_loss improved from 1.23439 to 1.21755, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6130 - acc: 0.7000 - val_loss: 1.2175 - val_acc: 0.0500\n",
            "Epoch 12/300\n",
            "\n",
            "Epoch 00012: val_loss improved from 1.21755 to 1.20806, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5863 - acc: 0.7667 - val_loss: 1.2081 - val_acc: 0.1000\n",
            "Epoch 13/300\n",
            "\n",
            "Epoch 00013: val_loss improved from 1.20806 to 1.19678, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5615 - acc: 0.7833 - val_loss: 1.1968 - val_acc: 0.1000\n",
            "Epoch 14/300\n",
            "\n",
            "Epoch 00014: val_loss improved from 1.19678 to 1.18644, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5779 - acc: 0.7500 - val_loss: 1.1864 - val_acc: 0.1500\n",
            "Epoch 15/300\n",
            "\n",
            "Epoch 00015: val_loss improved from 1.18644 to 1.18252, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5350 - acc: 0.7833 - val_loss: 1.1825 - val_acc: 0.1500\n",
            "Epoch 16/300\n",
            "\n",
            "Epoch 00016: val_loss improved from 1.18252 to 1.17527, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5230 - acc: 0.8333 - val_loss: 1.1753 - val_acc: 0.1500\n",
            "Epoch 17/300\n",
            "\n",
            "Epoch 00017: val_loss improved from 1.17527 to 1.17258, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5261 - acc: 0.8167 - val_loss: 1.1726 - val_acc: 0.1500\n",
            "Epoch 18/300\n",
            "\n",
            "Epoch 00018: val_loss improved from 1.17258 to 1.16293, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5209 - acc: 0.8167 - val_loss: 1.1629 - val_acc: 0.1500\n",
            "Epoch 19/300\n",
            "\n",
            "Epoch 00019: val_loss improved from 1.16293 to 1.16087, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4886 - acc: 0.8667 - val_loss: 1.1609 - val_acc: 0.1500\n",
            "Epoch 20/300\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 1.16087\n",
            "60/60 - 0s - loss: 0.5095 - acc: 0.8500 - val_loss: 1.1611 - val_acc: 0.1500\n",
            "Epoch 21/300\n",
            "\n",
            "Epoch 00021: val_loss improved from 1.16087 to 1.15055, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5141 - acc: 0.8000 - val_loss: 1.1505 - val_acc: 0.1500\n",
            "Epoch 22/300\n",
            "\n",
            "Epoch 00022: val_loss improved from 1.15055 to 1.14005, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4876 - acc: 0.8167 - val_loss: 1.1400 - val_acc: 0.1500\n",
            "Epoch 23/300\n",
            "\n",
            "Epoch 00023: val_loss improved from 1.14005 to 1.13982, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4717 - acc: 0.8333 - val_loss: 1.1398 - val_acc: 0.1500\n",
            "Epoch 24/300\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 1.13982\n",
            "60/60 - 0s - loss: 0.4944 - acc: 0.8333 - val_loss: 1.1459 - val_acc: 0.1500\n",
            "Epoch 25/300\n",
            "\n",
            "Epoch 00025: val_loss improved from 1.13982 to 1.13803, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4527 - acc: 0.8667 - val_loss: 1.1380 - val_acc: 0.2000\n",
            "Epoch 26/300\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 1.13803\n",
            "60/60 - 0s - loss: 0.5005 - acc: 0.8000 - val_loss: 1.1526 - val_acc: 0.2000\n",
            "Epoch 27/300\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 1.13803\n",
            "60/60 - 0s - loss: 0.4769 - acc: 0.7833 - val_loss: 1.1494 - val_acc: 0.2000\n",
            "Epoch 28/300\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 1.13803\n",
            "60/60 - 0s - loss: 0.4641 - acc: 0.8500 - val_loss: 1.1393 - val_acc: 0.2000\n",
            "Epoch 29/300\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 1.13803\n",
            "60/60 - 0s - loss: 0.4700 - acc: 0.8833 - val_loss: 1.1437 - val_acc: 0.2000\n",
            "Epoch 30/300\n",
            "\n",
            "Epoch 00030: val_loss improved from 1.13803 to 1.13537, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4580 - acc: 0.8000 - val_loss: 1.1354 - val_acc: 0.2000\n",
            "Epoch 31/300\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 1.13537\n",
            "60/60 - 0s - loss: 0.4326 - acc: 0.9167 - val_loss: 1.1367 - val_acc: 0.2000\n",
            "Epoch 32/300\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 1.13537\n",
            "60/60 - 0s - loss: 0.4279 - acc: 0.8500 - val_loss: 1.1524 - val_acc: 0.2000\n",
            "Epoch 33/300\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 1.13537\n",
            "60/60 - 0s - loss: 0.4312 - acc: 0.8833 - val_loss: 1.1683 - val_acc: 0.2000\n",
            "Epoch 34/300\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 1.13537\n",
            "60/60 - 0s - loss: 0.4402 - acc: 0.8500 - val_loss: 1.1558 - val_acc: 0.2500\n",
            "Epoch 35/300\n",
            "\n",
            "Epoch 00035: val_loss improved from 1.13537 to 1.13429, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4185 - acc: 0.9167 - val_loss: 1.1343 - val_acc: 0.3500\n",
            "Epoch 36/300\n",
            "\n",
            "Epoch 00036: val_loss improved from 1.13429 to 1.11333, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4199 - acc: 0.8833 - val_loss: 1.1133 - val_acc: 0.3500\n",
            "Epoch 37/300\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 1.11333\n",
            "60/60 - 0s - loss: 0.3980 - acc: 0.9167 - val_loss: 1.1212 - val_acc: 0.4000\n",
            "Epoch 38/300\n",
            "\n",
            "Epoch 00038: val_loss improved from 1.11333 to 1.11202, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4082 - acc: 0.9000 - val_loss: 1.1120 - val_acc: 0.4000\n",
            "Epoch 39/300\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 1.11202\n",
            "60/60 - 0s - loss: 0.4087 - acc: 0.9000 - val_loss: 1.1140 - val_acc: 0.4000\n",
            "Epoch 40/300\n",
            "\n",
            "Epoch 00040: val_loss improved from 1.11202 to 1.10088, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3892 - acc: 0.8833 - val_loss: 1.1009 - val_acc: 0.4500\n",
            "Epoch 41/300\n",
            "\n",
            "Epoch 00041: val_loss improved from 1.10088 to 1.07578, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3980 - acc: 0.8833 - val_loss: 1.0758 - val_acc: 0.4500\n",
            "Epoch 42/300\n",
            "\n",
            "Epoch 00042: val_loss improved from 1.07578 to 1.07263, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3763 - acc: 0.9333 - val_loss: 1.0726 - val_acc: 0.4500\n",
            "Epoch 43/300\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 1.07263\n",
            "60/60 - 0s - loss: 0.3771 - acc: 0.9000 - val_loss: 1.0821 - val_acc: 0.4500\n",
            "Epoch 44/300\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 1.07263\n",
            "60/60 - 0s - loss: 0.3858 - acc: 0.9000 - val_loss: 1.0807 - val_acc: 0.5000\n",
            "Epoch 45/300\n",
            "\n",
            "Epoch 00045: val_loss improved from 1.07263 to 1.06182, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3351 - acc: 0.8833 - val_loss: 1.0618 - val_acc: 0.5000\n",
            "Epoch 46/300\n",
            "\n",
            "Epoch 00046: val_loss improved from 1.06182 to 1.03764, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3744 - acc: 0.9167 - val_loss: 1.0376 - val_acc: 0.5000\n",
            "Epoch 47/300\n",
            "\n",
            "Epoch 00047: val_loss improved from 1.03764 to 1.01566, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3701 - acc: 0.8833 - val_loss: 1.0157 - val_acc: 0.5500\n",
            "Epoch 48/300\n",
            "\n",
            "Epoch 00048: val_loss improved from 1.01566 to 0.97079, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3442 - acc: 0.9167 - val_loss: 0.9708 - val_acc: 0.5500\n",
            "Epoch 49/300\n",
            "\n",
            "Epoch 00049: val_loss improved from 0.97079 to 0.93353, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3285 - acc: 0.9500 - val_loss: 0.9335 - val_acc: 0.6000\n",
            "Epoch 50/300\n",
            "\n",
            "Epoch 00050: val_loss improved from 0.93353 to 0.92080, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3210 - acc: 0.9667 - val_loss: 0.9208 - val_acc: 0.6000\n",
            "Epoch 51/300\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.92080\n",
            "60/60 - 0s - loss: 0.3303 - acc: 0.9500 - val_loss: 0.9229 - val_acc: 0.6000\n",
            "Epoch 52/300\n",
            "\n",
            "Epoch 00052: val_loss improved from 0.92080 to 0.91223, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2971 - acc: 0.9667 - val_loss: 0.9122 - val_acc: 0.6000\n",
            "Epoch 53/300\n",
            "\n",
            "Epoch 00053: val_loss improved from 0.91223 to 0.88914, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3206 - acc: 0.9500 - val_loss: 0.8891 - val_acc: 0.6000\n",
            "Epoch 54/300\n",
            "\n",
            "Epoch 00054: val_loss improved from 0.88914 to 0.86938, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3144 - acc: 0.9333 - val_loss: 0.8694 - val_acc: 0.6000\n",
            "Epoch 55/300\n",
            "\n",
            "Epoch 00055: val_loss improved from 0.86938 to 0.85964, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3033 - acc: 0.9667 - val_loss: 0.8596 - val_acc: 0.6500\n",
            "Epoch 56/300\n",
            "\n",
            "Epoch 00056: val_loss improved from 0.85964 to 0.84509, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3257 - acc: 0.9167 - val_loss: 0.8451 - val_acc: 0.7000\n",
            "Epoch 57/300\n",
            "\n",
            "Epoch 00057: val_loss improved from 0.84509 to 0.79326, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2779 - acc: 0.9833 - val_loss: 0.7933 - val_acc: 0.7000\n",
            "Epoch 58/300\n",
            "\n",
            "Epoch 00058: val_loss improved from 0.79326 to 0.75477, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2962 - acc: 0.9833 - val_loss: 0.7548 - val_acc: 0.7000\n",
            "Epoch 59/300\n",
            "\n",
            "Epoch 00059: val_loss improved from 0.75477 to 0.74147, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2959 - acc: 0.9667 - val_loss: 0.7415 - val_acc: 0.7000\n",
            "Epoch 60/300\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.74147\n",
            "60/60 - 0s - loss: 0.2732 - acc: 0.9833 - val_loss: 0.7584 - val_acc: 0.7000\n",
            "Epoch 61/300\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.74147\n",
            "60/60 - 0s - loss: 0.2733 - acc: 0.9833 - val_loss: 0.7799 - val_acc: 0.7000\n",
            "Epoch 62/300\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.74147\n",
            "60/60 - 0s - loss: 0.2538 - acc: 0.9667 - val_loss: 0.7886 - val_acc: 0.7000\n",
            "Epoch 63/300\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.74147\n",
            "60/60 - 0s - loss: 0.2282 - acc: 1.0000 - val_loss: 0.7628 - val_acc: 0.7500\n",
            "Epoch 64/300\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.74147\n",
            "60/60 - 0s - loss: 0.2240 - acc: 1.0000 - val_loss: 0.7526 - val_acc: 0.7500\n",
            "Epoch 65/300\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.74147\n",
            "60/60 - 0s - loss: 0.2452 - acc: 0.9500 - val_loss: 0.7438 - val_acc: 0.7500\n",
            "Epoch 66/300\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.74147\n",
            "60/60 - 0s - loss: 0.2348 - acc: 0.9667 - val_loss: 0.7468 - val_acc: 0.7500\n",
            "Epoch 67/300\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.74147\n",
            "60/60 - 0s - loss: 0.2474 - acc: 0.9667 - val_loss: 0.7522 - val_acc: 0.7500\n",
            "Epoch 68/300\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.74147\n",
            "60/60 - 0s - loss: 0.2496 - acc: 0.9333 - val_loss: 0.7511 - val_acc: 0.7500\n",
            "Epoch 69/300\n",
            "\n",
            "Epoch 00069: val_loss improved from 0.74147 to 0.72419, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2154 - acc: 0.9833 - val_loss: 0.7242 - val_acc: 0.7500\n",
            "Epoch 70/300\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.72419\n",
            "60/60 - 0s - loss: 0.2322 - acc: 0.9500 - val_loss: 0.7281 - val_acc: 0.7500\n",
            "Epoch 71/300\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.72419\n",
            "60/60 - 0s - loss: 0.2244 - acc: 0.9667 - val_loss: 0.7516 - val_acc: 0.7500\n",
            "Epoch 72/300\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.72419\n",
            "60/60 - 0s - loss: 0.2241 - acc: 0.9667 - val_loss: 0.7471 - val_acc: 0.7500\n",
            "Epoch 73/300\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.72419\n",
            "60/60 - 0s - loss: 0.2201 - acc: 0.9667 - val_loss: 0.7436 - val_acc: 0.7500\n",
            "Epoch 74/300\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.72419\n",
            "60/60 - 0s - loss: 0.2377 - acc: 0.9833 - val_loss: 0.7415 - val_acc: 0.7500\n",
            "Epoch 75/300\n",
            "\n",
            "Epoch 00075: val_loss improved from 0.72419 to 0.70472, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2434 - acc: 0.9667 - val_loss: 0.7047 - val_acc: 0.7500\n",
            "Epoch 76/300\n",
            "\n",
            "Epoch 00076: val_loss improved from 0.70472 to 0.69251, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2343 - acc: 0.9833 - val_loss: 0.6925 - val_acc: 0.7500\n",
            "Epoch 77/300\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.69251\n",
            "60/60 - 0s - loss: 0.2176 - acc: 0.9833 - val_loss: 0.6942 - val_acc: 0.7500\n",
            "Epoch 78/300\n",
            "\n",
            "Epoch 00078: val_loss improved from 0.69251 to 0.68359, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2022 - acc: 0.9667 - val_loss: 0.6836 - val_acc: 0.8000\n",
            "Epoch 79/300\n",
            "\n",
            "Epoch 00079: val_loss improved from 0.68359 to 0.67296, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2228 - acc: 0.9833 - val_loss: 0.6730 - val_acc: 0.8000\n",
            "Epoch 80/300\n",
            "\n",
            "Epoch 00080: val_loss improved from 0.67296 to 0.62082, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2190 - acc: 0.9667 - val_loss: 0.6208 - val_acc: 0.8000\n",
            "Epoch 81/300\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.62082\n",
            "60/60 - 0s - loss: 0.1993 - acc: 1.0000 - val_loss: 0.6314 - val_acc: 0.8000\n",
            "Epoch 82/300\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.62082\n",
            "60/60 - 0s - loss: 0.2404 - acc: 0.9333 - val_loss: 0.6326 - val_acc: 0.8000\n",
            "Epoch 83/300\n",
            "\n",
            "Epoch 00083: val_loss improved from 0.62082 to 0.62017, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2251 - acc: 1.0000 - val_loss: 0.6202 - val_acc: 0.8000\n",
            "Epoch 84/300\n",
            "\n",
            "Epoch 00084: val_loss improved from 0.62017 to 0.61980, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2020 - acc: 1.0000 - val_loss: 0.6198 - val_acc: 0.8000\n",
            "Epoch 85/300\n",
            "\n",
            "Epoch 00085: val_loss improved from 0.61980 to 0.59904, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2059 - acc: 0.9833 - val_loss: 0.5990 - val_acc: 0.8000\n",
            "Epoch 86/300\n",
            "\n",
            "Epoch 00086: val_loss improved from 0.59904 to 0.57943, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2229 - acc: 0.9667 - val_loss: 0.5794 - val_acc: 0.8000\n",
            "Epoch 87/300\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.2040 - acc: 0.9667 - val_loss: 0.6271 - val_acc: 0.8000\n",
            "Epoch 88/300\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.1795 - acc: 1.0000 - val_loss: 0.6544 - val_acc: 0.8000\n",
            "Epoch 89/300\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.2209 - acc: 0.9667 - val_loss: 0.6670 - val_acc: 0.7500\n",
            "Epoch 90/300\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.1825 - acc: 0.9833 - val_loss: 0.7566 - val_acc: 0.7500\n",
            "Epoch 91/300\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.1896 - acc: 0.9833 - val_loss: 0.8226 - val_acc: 0.7000\n",
            "Epoch 92/300\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.1905 - acc: 0.9833 - val_loss: 0.8637 - val_acc: 0.7000\n",
            "Epoch 93/300\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.1934 - acc: 1.0000 - val_loss: 0.8745 - val_acc: 0.7500\n",
            "Epoch 94/300\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.2006 - acc: 0.9833 - val_loss: 0.8782 - val_acc: 0.7500\n",
            "Epoch 95/300\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.2300 - acc: 0.9500 - val_loss: 0.8776 - val_acc: 0.7500\n",
            "Epoch 96/300\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.1629 - acc: 1.0000 - val_loss: 0.8959 - val_acc: 0.7500\n",
            "Epoch 97/300\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.1965 - acc: 0.9833 - val_loss: 0.9150 - val_acc: 0.7500\n",
            "Epoch 98/300\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.1675 - acc: 0.9667 - val_loss: 0.8949 - val_acc: 0.7500\n",
            "Epoch 99/300\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.1752 - acc: 0.9667 - val_loss: 0.8956 - val_acc: 0.7500\n",
            "Epoch 100/300\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.2014 - acc: 0.9667 - val_loss: 0.9166 - val_acc: 0.7500\n",
            "Epoch 101/300\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.1616 - acc: 1.0000 - val_loss: 0.9729 - val_acc: 0.7000\n",
            "Epoch 102/300\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.1958 - acc: 0.9833 - val_loss: 0.9930 - val_acc: 0.7000\n",
            "Epoch 103/300\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.1888 - acc: 0.9833 - val_loss: 1.0139 - val_acc: 0.7000\n",
            "Epoch 104/300\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.1820 - acc: 1.0000 - val_loss: 0.9546 - val_acc: 0.7000\n",
            "Epoch 105/300\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.2032 - acc: 0.9500 - val_loss: 0.9248 - val_acc: 0.7500\n",
            "Epoch 106/300\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.1808 - acc: 0.9833 - val_loss: 0.9307 - val_acc: 0.7500\n",
            "Epoch 107/300\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.1779 - acc: 0.9833 - val_loss: 0.9282 - val_acc: 0.7500\n",
            "Epoch 108/300\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.1595 - acc: 0.9833 - val_loss: 1.0153 - val_acc: 0.7000\n",
            "Epoch 109/300\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.1581 - acc: 0.9833 - val_loss: 1.0722 - val_acc: 0.6500\n",
            "Epoch 110/300\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.1871 - acc: 0.9667 - val_loss: 1.0529 - val_acc: 0.7000\n",
            "Epoch 111/300\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.1733 - acc: 1.0000 - val_loss: 1.0385 - val_acc: 0.7000\n",
            "Epoch 112/300\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.1645 - acc: 1.0000 - val_loss: 1.0489 - val_acc: 0.7000\n",
            "Epoch 113/300\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.1714 - acc: 0.9833 - val_loss: 1.0985 - val_acc: 0.7000\n",
            "Epoch 114/300\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.1607 - acc: 0.9833 - val_loss: 1.1447 - val_acc: 0.7000\n",
            "Epoch 115/300\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.1604 - acc: 0.9667 - val_loss: 1.2180 - val_acc: 0.6500\n",
            "Epoch 116/300\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.1961 - acc: 0.9500 - val_loss: 1.3264 - val_acc: 0.5500\n",
            "Epoch 117/300\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.1642 - acc: 0.9833 - val_loss: 1.3895 - val_acc: 0.5000\n",
            "Epoch 118/300\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.1975 - acc: 0.9667 - val_loss: 1.4864 - val_acc: 0.5000\n",
            "Epoch 119/300\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.1569 - acc: 0.9833 - val_loss: 1.4479 - val_acc: 0.5000\n",
            "Epoch 120/300\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.1704 - acc: 0.9833 - val_loss: 1.4706 - val_acc: 0.5000\n",
            "Epoch 121/300\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.1449 - acc: 1.0000 - val_loss: 1.5092 - val_acc: 0.5000\n",
            "Epoch 122/300\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.1717 - acc: 0.9667 - val_loss: 1.5399 - val_acc: 0.4500\n",
            "Epoch 123/300\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.1947 - acc: 0.9667 - val_loss: 1.5289 - val_acc: 0.4500\n",
            "Epoch 124/300\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.1448 - acc: 1.0000 - val_loss: 1.4403 - val_acc: 0.5000\n",
            "Epoch 125/300\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.1254 - acc: 1.0000 - val_loss: 1.3907 - val_acc: 0.6000\n",
            "Epoch 126/300\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.1440 - acc: 0.9833 - val_loss: 1.3971 - val_acc: 0.6000\n",
            "Epoch 127/300\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.1475 - acc: 0.9833 - val_loss: 1.4754 - val_acc: 0.5000\n",
            "Epoch 128/300\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.1519 - acc: 1.0000 - val_loss: 1.4658 - val_acc: 0.5000\n",
            "Epoch 129/300\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.1282 - acc: 0.9833 - val_loss: 1.5148 - val_acc: 0.5000\n",
            "Epoch 130/300\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.1613 - acc: 0.9833 - val_loss: 1.5535 - val_acc: 0.5000\n",
            "Epoch 131/300\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.1550 - acc: 1.0000 - val_loss: 1.5777 - val_acc: 0.5000\n",
            "Epoch 132/300\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.1416 - acc: 0.9833 - val_loss: 1.5782 - val_acc: 0.5000\n",
            "Epoch 133/300\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.1406 - acc: 1.0000 - val_loss: 1.6523 - val_acc: 0.5000\n",
            "Epoch 134/300\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.1609 - acc: 0.9833 - val_loss: 1.5908 - val_acc: 0.5000\n",
            "Epoch 135/300\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.1238 - acc: 1.0000 - val_loss: 1.5154 - val_acc: 0.5000\n",
            "Epoch 136/300\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.1445 - acc: 0.9833 - val_loss: 1.4387 - val_acc: 0.5500\n",
            "Epoch 137/300\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.1740 - acc: 0.9833 - val_loss: 1.4156 - val_acc: 0.5500\n",
            "Epoch 138/300\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.1312 - acc: 1.0000 - val_loss: 1.5283 - val_acc: 0.5000\n",
            "Epoch 139/300\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.1228 - acc: 0.9833 - val_loss: 1.5710 - val_acc: 0.5000\n",
            "Epoch 140/300\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.1229 - acc: 1.0000 - val_loss: 1.5741 - val_acc: 0.4500\n",
            "Epoch 141/300\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.1270 - acc: 1.0000 - val_loss: 1.5217 - val_acc: 0.5000\n",
            "Epoch 142/300\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.1542 - acc: 0.9667 - val_loss: 1.4133 - val_acc: 0.6000\n",
            "Epoch 143/300\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.1390 - acc: 0.9833 - val_loss: 1.3210 - val_acc: 0.6000\n",
            "Epoch 144/300\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.1453 - acc: 0.9667 - val_loss: 1.2822 - val_acc: 0.6000\n",
            "Epoch 145/300\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.1474 - acc: 0.9833 - val_loss: 1.3299 - val_acc: 0.6000\n",
            "Epoch 146/300\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.1354 - acc: 1.0000 - val_loss: 1.3624 - val_acc: 0.5500\n",
            "Epoch 147/300\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.0872 - acc: 1.0000 - val_loss: 1.3796 - val_acc: 0.6000\n",
            "Epoch 148/300\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.1395 - acc: 0.9833 - val_loss: 1.3590 - val_acc: 0.6500\n",
            "Epoch 149/300\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.1307 - acc: 1.0000 - val_loss: 1.3304 - val_acc: 0.6500\n",
            "Epoch 150/300\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.1488 - acc: 0.9667 - val_loss: 1.3450 - val_acc: 0.6500\n",
            "Epoch 151/300\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.1504 - acc: 0.9833 - val_loss: 1.3506 - val_acc: 0.6500\n",
            "Epoch 152/300\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.1924 - acc: 0.9167 - val_loss: 1.4945 - val_acc: 0.6500\n",
            "Epoch 153/300\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.1178 - acc: 1.0000 - val_loss: 1.4748 - val_acc: 0.6500\n",
            "Epoch 154/300\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.1441 - acc: 1.0000 - val_loss: 1.5370 - val_acc: 0.5500\n",
            "Epoch 155/300\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.1387 - acc: 1.0000 - val_loss: 1.4738 - val_acc: 0.6000\n",
            "Epoch 156/300\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.1539 - acc: 0.9833 - val_loss: 1.3839 - val_acc: 0.6500\n",
            "Epoch 157/300\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.1436 - acc: 0.9667 - val_loss: 1.4481 - val_acc: 0.5500\n",
            "Epoch 158/300\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.1183 - acc: 1.0000 - val_loss: 1.4183 - val_acc: 0.6500\n",
            "Epoch 159/300\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.1456 - acc: 1.0000 - val_loss: 1.4495 - val_acc: 0.6500\n",
            "Epoch 160/300\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.1252 - acc: 0.9833 - val_loss: 1.5186 - val_acc: 0.6000\n",
            "Epoch 161/300\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.1181 - acc: 1.0000 - val_loss: 1.4908 - val_acc: 0.6500\n",
            "Epoch 162/300\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.1270 - acc: 1.0000 - val_loss: 1.5180 - val_acc: 0.6500\n",
            "Epoch 163/300\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.1511 - acc: 0.9833 - val_loss: 1.5701 - val_acc: 0.5500\n",
            "Epoch 164/300\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.1354 - acc: 0.9833 - val_loss: 1.6305 - val_acc: 0.5500\n",
            "Epoch 165/300\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.1465 - acc: 0.9667 - val_loss: 1.6145 - val_acc: 0.5500\n",
            "Epoch 166/300\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.1433 - acc: 1.0000 - val_loss: 1.6318 - val_acc: 0.5000\n",
            "Epoch 167/300\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.0968 - acc: 1.0000 - val_loss: 1.7453 - val_acc: 0.5000\n",
            "Epoch 168/300\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.1301 - acc: 0.9833 - val_loss: 1.7760 - val_acc: 0.4500\n",
            "Epoch 169/300\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.0918 - acc: 1.0000 - val_loss: 1.6737 - val_acc: 0.5000\n",
            "Epoch 170/300\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.1240 - acc: 0.9833 - val_loss: 1.6040 - val_acc: 0.5000\n",
            "Epoch 171/300\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.0985 - acc: 1.0000 - val_loss: 1.6412 - val_acc: 0.5000\n",
            "Epoch 172/300\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.1069 - acc: 0.9833 - val_loss: 1.6353 - val_acc: 0.5000\n",
            "Epoch 173/300\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.1352 - acc: 0.9667 - val_loss: 1.7402 - val_acc: 0.5000\n",
            "Epoch 174/300\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.1329 - acc: 0.9833 - val_loss: 1.6473 - val_acc: 0.5000\n",
            "Epoch 175/300\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.1265 - acc: 1.0000 - val_loss: 1.5760 - val_acc: 0.5000\n",
            "Epoch 176/300\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.1428 - acc: 0.9833 - val_loss: 1.4976 - val_acc: 0.6000\n",
            "Epoch 177/300\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.1083 - acc: 1.0000 - val_loss: 1.6093 - val_acc: 0.5000\n",
            "Epoch 178/300\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.1383 - acc: 0.9500 - val_loss: 1.6743 - val_acc: 0.5000\n",
            "Epoch 179/300\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.1322 - acc: 0.9833 - val_loss: 1.6662 - val_acc: 0.5000\n",
            "Epoch 180/300\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.1269 - acc: 0.9833 - val_loss: 1.6460 - val_acc: 0.5000\n",
            "Epoch 181/300\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.1769 - acc: 0.9500 - val_loss: 1.6546 - val_acc: 0.5000\n",
            "Epoch 182/300\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.1573 - acc: 1.0000 - val_loss: 1.4145 - val_acc: 0.5500\n",
            "Epoch 183/300\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.1304 - acc: 1.0000 - val_loss: 1.3809 - val_acc: 0.5500\n",
            "Epoch 184/300\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.1526 - acc: 0.9833 - val_loss: 1.3899 - val_acc: 0.5500\n",
            "Epoch 185/300\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.1251 - acc: 0.9833 - val_loss: 1.3600 - val_acc: 0.5500\n",
            "Epoch 186/300\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.1281 - acc: 0.9833 - val_loss: 1.4739 - val_acc: 0.5000\n",
            "Epoch 187/300\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.1175 - acc: 0.9833 - val_loss: 1.5642 - val_acc: 0.5000\n",
            "Epoch 188/300\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.1239 - acc: 1.0000 - val_loss: 1.5980 - val_acc: 0.5000\n",
            "Epoch 189/300\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.1331 - acc: 1.0000 - val_loss: 1.7023 - val_acc: 0.5000\n",
            "Epoch 190/300\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.1229 - acc: 1.0000 - val_loss: 1.7280 - val_acc: 0.5000\n",
            "Epoch 191/300\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.1472 - acc: 1.0000 - val_loss: 1.7232 - val_acc: 0.4500\n",
            "Epoch 192/300\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.1443 - acc: 1.0000 - val_loss: 1.5675 - val_acc: 0.5500\n",
            "Epoch 193/300\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.1151 - acc: 1.0000 - val_loss: 1.5336 - val_acc: 0.5500\n",
            "Epoch 194/300\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.0848 - acc: 1.0000 - val_loss: 1.6083 - val_acc: 0.4500\n",
            "Epoch 195/300\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.1060 - acc: 0.9833 - val_loss: 1.6590 - val_acc: 0.4500\n",
            "Epoch 196/300\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.0837 - acc: 1.0000 - val_loss: 1.6753 - val_acc: 0.4500\n",
            "Epoch 197/300\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.0974 - acc: 1.0000 - val_loss: 1.7219 - val_acc: 0.4500\n",
            "Epoch 198/300\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.1279 - acc: 0.9833 - val_loss: 1.8249 - val_acc: 0.4000\n",
            "Epoch 199/300\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.0881 - acc: 1.0000 - val_loss: 1.8602 - val_acc: 0.4000\n",
            "Epoch 200/300\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.1230 - acc: 1.0000 - val_loss: 1.8415 - val_acc: 0.5000\n",
            "Epoch 201/300\n",
            "\n",
            "Epoch 00201: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.1056 - acc: 1.0000 - val_loss: 1.7401 - val_acc: 0.4500\n",
            "Epoch 202/300\n",
            "\n",
            "Epoch 00202: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.1213 - acc: 1.0000 - val_loss: 1.6669 - val_acc: 0.5000\n",
            "Epoch 203/300\n",
            "\n",
            "Epoch 00203: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.1985 - acc: 0.9167 - val_loss: 1.5109 - val_acc: 0.6000\n",
            "Epoch 204/300\n",
            "\n",
            "Epoch 00204: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.0927 - acc: 1.0000 - val_loss: 1.3082 - val_acc: 0.6500\n",
            "Epoch 205/300\n",
            "\n",
            "Epoch 00205: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.1201 - acc: 0.9833 - val_loss: 1.2827 - val_acc: 0.6500\n",
            "Epoch 206/300\n",
            "\n",
            "Epoch 00206: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.1244 - acc: 0.9833 - val_loss: 1.4713 - val_acc: 0.5000\n",
            "Epoch 207/300\n",
            "\n",
            "Epoch 00207: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.1018 - acc: 1.0000 - val_loss: 1.5844 - val_acc: 0.5000\n",
            "Epoch 208/300\n",
            "\n",
            "Epoch 00208: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.1131 - acc: 1.0000 - val_loss: 1.6491 - val_acc: 0.5000\n",
            "Epoch 209/300\n",
            "\n",
            "Epoch 00209: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.1224 - acc: 1.0000 - val_loss: 1.6120 - val_acc: 0.5000\n",
            "Epoch 210/300\n",
            "\n",
            "Epoch 00210: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.1126 - acc: 0.9833 - val_loss: 1.6876 - val_acc: 0.5000\n",
            "Epoch 211/300\n",
            "\n",
            "Epoch 00211: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.0943 - acc: 1.0000 - val_loss: 1.8722 - val_acc: 0.4500\n",
            "Epoch 212/300\n",
            "\n",
            "Epoch 00212: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.0930 - acc: 1.0000 - val_loss: 1.8748 - val_acc: 0.4500\n",
            "Epoch 213/300\n",
            "\n",
            "Epoch 00213: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.1039 - acc: 1.0000 - val_loss: 1.9314 - val_acc: 0.4500\n",
            "Epoch 214/300\n",
            "\n",
            "Epoch 00214: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.1058 - acc: 0.9833 - val_loss: 1.9376 - val_acc: 0.5000\n",
            "Epoch 215/300\n",
            "\n",
            "Epoch 00215: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.0939 - acc: 1.0000 - val_loss: 1.9812 - val_acc: 0.5000\n",
            "Epoch 216/300\n",
            "\n",
            "Epoch 00216: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.1020 - acc: 1.0000 - val_loss: 2.0425 - val_acc: 0.4500\n",
            "Epoch 217/300\n",
            "\n",
            "Epoch 00217: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.0985 - acc: 1.0000 - val_loss: 2.1127 - val_acc: 0.4000\n",
            "Epoch 218/300\n",
            "\n",
            "Epoch 00218: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.1413 - acc: 0.9667 - val_loss: 2.0780 - val_acc: 0.4000\n",
            "Epoch 219/300\n",
            "\n",
            "Epoch 00219: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.0937 - acc: 1.0000 - val_loss: 1.9817 - val_acc: 0.4500\n",
            "Epoch 220/300\n",
            "\n",
            "Epoch 00220: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.0945 - acc: 0.9833 - val_loss: 1.9471 - val_acc: 0.4500\n",
            "Epoch 221/300\n",
            "\n",
            "Epoch 00221: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.0844 - acc: 1.0000 - val_loss: 1.8049 - val_acc: 0.5500\n",
            "Epoch 222/300\n",
            "\n",
            "Epoch 00222: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.0734 - acc: 1.0000 - val_loss: 1.6765 - val_acc: 0.5500\n",
            "Epoch 223/300\n",
            "\n",
            "Epoch 00223: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.1164 - acc: 0.9833 - val_loss: 1.6663 - val_acc: 0.5500\n",
            "Epoch 224/300\n",
            "\n",
            "Epoch 00224: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.0780 - acc: 1.0000 - val_loss: 1.7741 - val_acc: 0.5500\n",
            "Epoch 225/300\n",
            "\n",
            "Epoch 00225: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.1214 - acc: 1.0000 - val_loss: 1.9271 - val_acc: 0.4000\n",
            "Epoch 226/300\n",
            "\n",
            "Epoch 00226: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.1142 - acc: 0.9833 - val_loss: 2.0850 - val_acc: 0.4000\n",
            "Epoch 227/300\n",
            "\n",
            "Epoch 00227: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.0891 - acc: 1.0000 - val_loss: 2.2426 - val_acc: 0.4000\n",
            "Epoch 228/300\n",
            "\n",
            "Epoch 00228: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.0818 - acc: 1.0000 - val_loss: 2.2372 - val_acc: 0.4000\n",
            "Epoch 229/300\n",
            "\n",
            "Epoch 00229: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.0869 - acc: 1.0000 - val_loss: 2.0880 - val_acc: 0.4000\n",
            "Epoch 230/300\n",
            "\n",
            "Epoch 00230: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.0902 - acc: 1.0000 - val_loss: 2.0875 - val_acc: 0.4000\n",
            "Epoch 231/300\n",
            "\n",
            "Epoch 00231: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.0897 - acc: 0.9833 - val_loss: 1.9278 - val_acc: 0.5000\n",
            "Epoch 232/300\n",
            "\n",
            "Epoch 00232: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.0979 - acc: 1.0000 - val_loss: 1.9944 - val_acc: 0.4500\n",
            "Epoch 233/300\n",
            "\n",
            "Epoch 00233: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.1005 - acc: 1.0000 - val_loss: 1.9220 - val_acc: 0.5000\n",
            "Epoch 234/300\n",
            "\n",
            "Epoch 00234: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.0899 - acc: 0.9833 - val_loss: 1.9347 - val_acc: 0.5000\n",
            "Epoch 235/300\n",
            "\n",
            "Epoch 00235: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.0818 - acc: 1.0000 - val_loss: 2.1127 - val_acc: 0.4000\n",
            "Epoch 236/300\n",
            "\n",
            "Epoch 00236: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.0843 - acc: 1.0000 - val_loss: 2.0893 - val_acc: 0.4500\n",
            "Epoch 237/300\n",
            "\n",
            "Epoch 00237: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.0791 - acc: 1.0000 - val_loss: 2.0097 - val_acc: 0.5000\n",
            "Epoch 238/300\n",
            "\n",
            "Epoch 00238: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.0938 - acc: 0.9833 - val_loss: 2.0573 - val_acc: 0.5000\n",
            "Epoch 239/300\n",
            "\n",
            "Epoch 00239: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.0756 - acc: 1.0000 - val_loss: 2.0648 - val_acc: 0.5000\n",
            "Epoch 240/300\n",
            "\n",
            "Epoch 00240: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.0854 - acc: 1.0000 - val_loss: 2.0775 - val_acc: 0.4500\n",
            "Epoch 241/300\n",
            "\n",
            "Epoch 00241: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.0863 - acc: 0.9833 - val_loss: 2.0134 - val_acc: 0.4500\n",
            "Epoch 242/300\n",
            "\n",
            "Epoch 00242: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.0605 - acc: 1.0000 - val_loss: 2.1069 - val_acc: 0.4500\n",
            "Epoch 243/300\n",
            "\n",
            "Epoch 00243: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.0837 - acc: 1.0000 - val_loss: 2.1627 - val_acc: 0.4000\n",
            "Epoch 244/300\n",
            "\n",
            "Epoch 00244: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.0901 - acc: 1.0000 - val_loss: 2.1717 - val_acc: 0.4000\n",
            "Epoch 245/300\n",
            "\n",
            "Epoch 00245: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.0683 - acc: 1.0000 - val_loss: 1.9684 - val_acc: 0.5000\n",
            "Epoch 246/300\n",
            "\n",
            "Epoch 00246: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.0670 - acc: 1.0000 - val_loss: 1.9410 - val_acc: 0.5000\n",
            "Epoch 247/300\n",
            "\n",
            "Epoch 00247: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.0522 - acc: 1.0000 - val_loss: 2.0503 - val_acc: 0.5000\n",
            "Epoch 248/300\n",
            "\n",
            "Epoch 00248: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.0830 - acc: 1.0000 - val_loss: 2.1929 - val_acc: 0.4000\n",
            "Epoch 249/300\n",
            "\n",
            "Epoch 00249: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.1106 - acc: 0.9833 - val_loss: 2.1940 - val_acc: 0.4000\n",
            "Epoch 250/300\n",
            "\n",
            "Epoch 00250: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.0852 - acc: 1.0000 - val_loss: 2.1435 - val_acc: 0.4000\n",
            "Epoch 251/300\n",
            "\n",
            "Epoch 00251: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.0806 - acc: 1.0000 - val_loss: 2.1137 - val_acc: 0.4500\n",
            "Epoch 252/300\n",
            "\n",
            "Epoch 00252: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.0851 - acc: 1.0000 - val_loss: 2.2125 - val_acc: 0.4500\n",
            "Epoch 253/300\n",
            "\n",
            "Epoch 00253: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.0749 - acc: 1.0000 - val_loss: 2.1815 - val_acc: 0.4500\n",
            "Epoch 254/300\n",
            "\n",
            "Epoch 00254: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.1004 - acc: 0.9833 - val_loss: 1.9593 - val_acc: 0.5000\n",
            "Epoch 255/300\n",
            "\n",
            "Epoch 00255: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.0778 - acc: 1.0000 - val_loss: 1.9253 - val_acc: 0.5000\n",
            "Epoch 256/300\n",
            "\n",
            "Epoch 00256: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.0975 - acc: 0.9833 - val_loss: 2.1073 - val_acc: 0.4500\n",
            "Epoch 257/300\n",
            "\n",
            "Epoch 00257: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.0594 - acc: 1.0000 - val_loss: 2.4321 - val_acc: 0.4000\n",
            "Epoch 258/300\n",
            "\n",
            "Epoch 00258: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.0690 - acc: 1.0000 - val_loss: 2.6486 - val_acc: 0.4000\n",
            "Epoch 259/300\n",
            "\n",
            "Epoch 00259: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.1122 - acc: 0.9667 - val_loss: 2.6243 - val_acc: 0.4000\n",
            "Epoch 260/300\n",
            "\n",
            "Epoch 00260: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.0863 - acc: 1.0000 - val_loss: 2.5206 - val_acc: 0.4500\n",
            "Epoch 261/300\n",
            "\n",
            "Epoch 00261: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.0949 - acc: 0.9833 - val_loss: 2.4964 - val_acc: 0.4500\n",
            "Epoch 262/300\n",
            "\n",
            "Epoch 00262: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.1059 - acc: 0.9833 - val_loss: 2.4331 - val_acc: 0.4500\n",
            "Epoch 263/300\n",
            "\n",
            "Epoch 00263: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.0974 - acc: 0.9833 - val_loss: 2.5915 - val_acc: 0.4000\n",
            "Epoch 264/300\n",
            "\n",
            "Epoch 00264: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.0678 - acc: 1.0000 - val_loss: 2.7661 - val_acc: 0.4000\n",
            "Epoch 265/300\n",
            "\n",
            "Epoch 00265: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.0912 - acc: 1.0000 - val_loss: 2.4739 - val_acc: 0.4000\n",
            "Epoch 266/300\n",
            "\n",
            "Epoch 00266: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.1197 - acc: 0.9667 - val_loss: 1.9509 - val_acc: 0.6000\n",
            "Epoch 267/300\n",
            "\n",
            "Epoch 00267: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.0568 - acc: 1.0000 - val_loss: 1.7023 - val_acc: 0.6000\n",
            "Epoch 268/300\n",
            "\n",
            "Epoch 00268: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.0758 - acc: 1.0000 - val_loss: 1.8657 - val_acc: 0.5000\n",
            "Epoch 269/300\n",
            "\n",
            "Epoch 00269: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.0839 - acc: 0.9833 - val_loss: 2.0800 - val_acc: 0.5000\n",
            "Epoch 270/300\n",
            "\n",
            "Epoch 00270: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.1404 - acc: 0.9833 - val_loss: 2.3191 - val_acc: 0.4500\n",
            "Epoch 271/300\n",
            "\n",
            "Epoch 00271: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.1381 - acc: 0.9333 - val_loss: 2.4918 - val_acc: 0.4000\n",
            "Epoch 272/300\n",
            "\n",
            "Epoch 00272: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.0798 - acc: 1.0000 - val_loss: 2.6668 - val_acc: 0.4000\n",
            "Epoch 273/300\n",
            "\n",
            "Epoch 00273: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.0939 - acc: 0.9833 - val_loss: 2.8211 - val_acc: 0.4000\n",
            "Epoch 274/300\n",
            "\n",
            "Epoch 00274: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.1028 - acc: 0.9833 - val_loss: 2.5528 - val_acc: 0.4000\n",
            "Epoch 275/300\n",
            "\n",
            "Epoch 00275: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.0561 - acc: 1.0000 - val_loss: 2.2873 - val_acc: 0.4500\n",
            "Epoch 276/300\n",
            "\n",
            "Epoch 00276: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.0716 - acc: 1.0000 - val_loss: 2.1812 - val_acc: 0.4500\n",
            "Epoch 277/300\n",
            "\n",
            "Epoch 00277: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.0769 - acc: 1.0000 - val_loss: 2.3336 - val_acc: 0.4500\n",
            "Epoch 278/300\n",
            "\n",
            "Epoch 00278: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.0802 - acc: 1.0000 - val_loss: 2.5146 - val_acc: 0.4000\n",
            "Epoch 279/300\n",
            "\n",
            "Epoch 00279: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.0647 - acc: 1.0000 - val_loss: 2.7708 - val_acc: 0.4000\n",
            "Epoch 280/300\n",
            "\n",
            "Epoch 00280: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.0776 - acc: 1.0000 - val_loss: 2.8600 - val_acc: 0.4000\n",
            "Epoch 281/300\n",
            "\n",
            "Epoch 00281: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.0599 - acc: 1.0000 - val_loss: 2.6497 - val_acc: 0.4000\n",
            "Epoch 282/300\n",
            "\n",
            "Epoch 00282: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.0644 - acc: 1.0000 - val_loss: 2.3657 - val_acc: 0.4500\n",
            "Epoch 283/300\n",
            "\n",
            "Epoch 00283: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.0712 - acc: 1.0000 - val_loss: 2.1871 - val_acc: 0.4500\n",
            "Epoch 284/300\n",
            "\n",
            "Epoch 00284: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.0647 - acc: 1.0000 - val_loss: 2.1958 - val_acc: 0.4500\n",
            "Epoch 285/300\n",
            "\n",
            "Epoch 00285: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.0655 - acc: 1.0000 - val_loss: 2.4201 - val_acc: 0.4500\n",
            "Epoch 286/300\n",
            "\n",
            "Epoch 00286: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.0688 - acc: 0.9833 - val_loss: 2.7646 - val_acc: 0.4000\n",
            "Epoch 287/300\n",
            "\n",
            "Epoch 00287: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.1050 - acc: 0.9833 - val_loss: 2.6539 - val_acc: 0.4500\n",
            "Epoch 288/300\n",
            "\n",
            "Epoch 00288: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.0812 - acc: 1.0000 - val_loss: 2.4439 - val_acc: 0.4500\n",
            "Epoch 289/300\n",
            "\n",
            "Epoch 00289: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.0646 - acc: 1.0000 - val_loss: 2.1805 - val_acc: 0.4500\n",
            "Epoch 290/300\n",
            "\n",
            "Epoch 00290: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.0550 - acc: 1.0000 - val_loss: 2.0791 - val_acc: 0.5000\n",
            "Epoch 291/300\n",
            "\n",
            "Epoch 00291: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.0979 - acc: 1.0000 - val_loss: 2.0549 - val_acc: 0.5000\n",
            "Epoch 292/300\n",
            "\n",
            "Epoch 00292: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.0580 - acc: 1.0000 - val_loss: 2.2758 - val_acc: 0.4500\n",
            "Epoch 293/300\n",
            "\n",
            "Epoch 00293: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.0756 - acc: 0.9833 - val_loss: 2.5593 - val_acc: 0.4000\n",
            "Epoch 294/300\n",
            "\n",
            "Epoch 00294: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.0958 - acc: 0.9833 - val_loss: 2.6106 - val_acc: 0.4000\n",
            "Epoch 295/300\n",
            "\n",
            "Epoch 00295: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.0729 - acc: 1.0000 - val_loss: 2.6905 - val_acc: 0.4000\n",
            "Epoch 296/300\n",
            "\n",
            "Epoch 00296: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.0744 - acc: 1.0000 - val_loss: 2.5375 - val_acc: 0.4000\n",
            "Epoch 297/300\n",
            "\n",
            "Epoch 00297: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.0973 - acc: 0.9833 - val_loss: 2.1307 - val_acc: 0.4500\n",
            "Epoch 298/300\n",
            "\n",
            "Epoch 00298: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.0823 - acc: 0.9833 - val_loss: 2.1411 - val_acc: 0.4500\n",
            "Epoch 299/300\n",
            "\n",
            "Epoch 00299: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.0792 - acc: 0.9833 - val_loss: 2.1499 - val_acc: 0.4500\n",
            "Epoch 300/300\n",
            "\n",
            "Epoch 00300: val_loss did not improve from 0.57943\n",
            "60/60 - 0s - loss: 0.0683 - acc: 1.0000 - val_loss: 2.1039 - val_acc: 0.5000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "# Evaluate on test data\n",
            "40/40 [==============================] - 0s 2ms/sample - loss: 1.3059 - acc: 0.5500\n",
            "test loss, test acc: [1.3058566440071444, 0.55]\n",
            "EEG_Deep/Data2A/parsed_P06T.mat\n",
            "Filtering of Training Data Finished\n",
            "EEG_Deep/Data2A/parsed_P06E.mat\n",
            "Filtering of Testing Data Finished\n",
            "(1, 2)\n",
            "Finding labels in training data\n",
            "Finding labels in testing data\n",
            "[2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1]\n",
            "(60, 12, 1536)\n",
            "(60,)\n",
            "(20, 12, 1536)\n",
            "(20,)\n",
            "[[0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]]\n",
            "X_train shape: (60, 1, 12, 1536)\n",
            "60 train samples\n",
            "20 val samples\n",
            "(40, 12, 1536)\n",
            "(40, 1)\n",
            "X_train shape: (40, 1, 12, 1536)\n",
            "40 train samples\n",
            "Train on 60 samples, validate on 20 samples\n",
            "Epoch 1/300\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 1.33738, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 1s - loss: 1.2548 - acc: 0.5000 - val_loss: 1.3374 - val_acc: 0.5000\n",
            "Epoch 2/300\n",
            "\n",
            "Epoch 00002: val_loss improved from 1.33738 to 1.30363, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 1.0771 - acc: 0.5667 - val_loss: 1.3036 - val_acc: 0.5500\n",
            "Epoch 3/300\n",
            "\n",
            "Epoch 00003: val_loss improved from 1.30363 to 1.27840, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.9556 - acc: 0.4833 - val_loss: 1.2784 - val_acc: 0.5500\n",
            "Epoch 4/300\n",
            "\n",
            "Epoch 00004: val_loss improved from 1.27840 to 1.25784, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.8804 - acc: 0.5833 - val_loss: 1.2578 - val_acc: 0.6500\n",
            "Epoch 5/300\n",
            "\n",
            "Epoch 00005: val_loss improved from 1.25784 to 1.24035, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.8211 - acc: 0.6500 - val_loss: 1.2404 - val_acc: 0.7000\n",
            "Epoch 6/300\n",
            "\n",
            "Epoch 00006: val_loss improved from 1.24035 to 1.22491, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.8022 - acc: 0.7667 - val_loss: 1.2249 - val_acc: 0.5000\n",
            "Epoch 7/300\n",
            "\n",
            "Epoch 00007: val_loss improved from 1.22491 to 1.20867, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.7740 - acc: 0.7833 - val_loss: 1.2087 - val_acc: 0.4500\n",
            "Epoch 8/300\n",
            "\n",
            "Epoch 00008: val_loss improved from 1.20867 to 1.19235, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.7582 - acc: 0.8500 - val_loss: 1.1924 - val_acc: 0.4500\n",
            "Epoch 9/300\n",
            "\n",
            "Epoch 00009: val_loss improved from 1.19235 to 1.17519, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.7391 - acc: 0.8167 - val_loss: 1.1752 - val_acc: 0.5000\n",
            "Epoch 10/300\n",
            "\n",
            "Epoch 00010: val_loss improved from 1.17519 to 1.15875, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.7200 - acc: 0.8333 - val_loss: 1.1588 - val_acc: 0.5000\n",
            "Epoch 11/300\n",
            "\n",
            "Epoch 00011: val_loss improved from 1.15875 to 1.14264, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.7005 - acc: 0.8500 - val_loss: 1.1426 - val_acc: 0.4500\n",
            "Epoch 12/300\n",
            "\n",
            "Epoch 00012: val_loss improved from 1.14264 to 1.12847, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6582 - acc: 0.9333 - val_loss: 1.1285 - val_acc: 0.4500\n",
            "Epoch 13/300\n",
            "\n",
            "Epoch 00013: val_loss improved from 1.12847 to 1.11525, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6630 - acc: 0.9000 - val_loss: 1.1153 - val_acc: 0.4500\n",
            "Epoch 14/300\n",
            "\n",
            "Epoch 00014: val_loss improved from 1.11525 to 1.10253, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6588 - acc: 0.8667 - val_loss: 1.1025 - val_acc: 0.4500\n",
            "Epoch 15/300\n",
            "\n",
            "Epoch 00015: val_loss improved from 1.10253 to 1.09214, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6324 - acc: 0.9167 - val_loss: 1.0921 - val_acc: 0.3500\n",
            "Epoch 16/300\n",
            "\n",
            "Epoch 00016: val_loss improved from 1.09214 to 1.08196, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6174 - acc: 0.8500 - val_loss: 1.0820 - val_acc: 0.3500\n",
            "Epoch 17/300\n",
            "\n",
            "Epoch 00017: val_loss improved from 1.08196 to 1.07342, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6069 - acc: 0.8667 - val_loss: 1.0734 - val_acc: 0.3500\n",
            "Epoch 18/300\n",
            "\n",
            "Epoch 00018: val_loss improved from 1.07342 to 1.06614, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5583 - acc: 0.9000 - val_loss: 1.0661 - val_acc: 0.3000\n",
            "Epoch 19/300\n",
            "\n",
            "Epoch 00019: val_loss improved from 1.06614 to 1.05904, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5800 - acc: 0.8333 - val_loss: 1.0590 - val_acc: 0.3000\n",
            "Epoch 20/300\n",
            "\n",
            "Epoch 00020: val_loss improved from 1.05904 to 1.05260, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5542 - acc: 0.9167 - val_loss: 1.0526 - val_acc: 0.3000\n",
            "Epoch 21/300\n",
            "\n",
            "Epoch 00021: val_loss improved from 1.05260 to 1.04578, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5268 - acc: 0.8833 - val_loss: 1.0458 - val_acc: 0.3000\n",
            "Epoch 22/300\n",
            "\n",
            "Epoch 00022: val_loss improved from 1.04578 to 1.03729, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5201 - acc: 0.9167 - val_loss: 1.0373 - val_acc: 0.3000\n",
            "Epoch 23/300\n",
            "\n",
            "Epoch 00023: val_loss improved from 1.03729 to 1.03267, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5317 - acc: 0.9000 - val_loss: 1.0327 - val_acc: 0.3000\n",
            "Epoch 24/300\n",
            "\n",
            "Epoch 00024: val_loss improved from 1.03267 to 1.02805, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5095 - acc: 0.8667 - val_loss: 1.0280 - val_acc: 0.3000\n",
            "Epoch 25/300\n",
            "\n",
            "Epoch 00025: val_loss improved from 1.02805 to 1.02313, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4982 - acc: 0.8833 - val_loss: 1.0231 - val_acc: 0.3000\n",
            "Epoch 26/300\n",
            "\n",
            "Epoch 00026: val_loss improved from 1.02313 to 1.01918, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4911 - acc: 0.9167 - val_loss: 1.0192 - val_acc: 0.3000\n",
            "Epoch 27/300\n",
            "\n",
            "Epoch 00027: val_loss improved from 1.01918 to 1.01763, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4772 - acc: 0.9500 - val_loss: 1.0176 - val_acc: 0.3000\n",
            "Epoch 28/300\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 1.01763\n",
            "60/60 - 0s - loss: 0.4739 - acc: 0.9000 - val_loss: 1.0221 - val_acc: 0.3000\n",
            "Epoch 29/300\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 1.01763\n",
            "60/60 - 0s - loss: 0.4605 - acc: 0.9167 - val_loss: 1.0204 - val_acc: 0.3000\n",
            "Epoch 30/300\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 1.01763\n",
            "60/60 - 0s - loss: 0.4550 - acc: 0.9333 - val_loss: 1.0183 - val_acc: 0.3000\n",
            "Epoch 31/300\n",
            "\n",
            "Epoch 00031: val_loss improved from 1.01763 to 1.01402, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4519 - acc: 0.9500 - val_loss: 1.0140 - val_acc: 0.3000\n",
            "Epoch 32/300\n",
            "\n",
            "Epoch 00032: val_loss improved from 1.01402 to 1.00741, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4301 - acc: 0.9667 - val_loss: 1.0074 - val_acc: 0.3000\n",
            "Epoch 33/300\n",
            "\n",
            "Epoch 00033: val_loss improved from 1.00741 to 1.00077, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4279 - acc: 0.9667 - val_loss: 1.0008 - val_acc: 0.3000\n",
            "Epoch 34/300\n",
            "\n",
            "Epoch 00034: val_loss improved from 1.00077 to 0.99330, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4245 - acc: 0.9833 - val_loss: 0.9933 - val_acc: 0.3500\n",
            "Epoch 35/300\n",
            "\n",
            "Epoch 00035: val_loss improved from 0.99330 to 0.98282, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4150 - acc: 0.9167 - val_loss: 0.9828 - val_acc: 0.3500\n",
            "Epoch 36/300\n",
            "\n",
            "Epoch 00036: val_loss improved from 0.98282 to 0.97370, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4089 - acc: 0.9167 - val_loss: 0.9737 - val_acc: 0.3500\n",
            "Epoch 37/300\n",
            "\n",
            "Epoch 00037: val_loss improved from 0.97370 to 0.97136, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4594 - acc: 0.9167 - val_loss: 0.9714 - val_acc: 0.3500\n",
            "Epoch 38/300\n",
            "\n",
            "Epoch 00038: val_loss improved from 0.97136 to 0.97077, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4027 - acc: 0.9500 - val_loss: 0.9708 - val_acc: 0.4000\n",
            "Epoch 39/300\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.97077\n",
            "60/60 - 0s - loss: 0.4240 - acc: 0.9333 - val_loss: 0.9732 - val_acc: 0.4000\n",
            "Epoch 40/300\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.97077\n",
            "60/60 - 0s - loss: 0.3559 - acc: 0.9833 - val_loss: 0.9743 - val_acc: 0.4000\n",
            "Epoch 41/300\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.97077\n",
            "60/60 - 0s - loss: 0.3804 - acc: 0.9500 - val_loss: 0.9768 - val_acc: 0.4000\n",
            "Epoch 42/300\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.97077\n",
            "60/60 - 0s - loss: 0.3796 - acc: 0.9667 - val_loss: 0.9785 - val_acc: 0.4000\n",
            "Epoch 43/300\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.97077\n",
            "60/60 - 0s - loss: 0.3508 - acc: 0.9500 - val_loss: 0.9779 - val_acc: 0.4000\n",
            "Epoch 44/300\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.97077\n",
            "60/60 - 0s - loss: 0.3715 - acc: 1.0000 - val_loss: 0.9771 - val_acc: 0.4000\n",
            "Epoch 45/300\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.97077\n",
            "60/60 - 0s - loss: 0.3565 - acc: 0.9667 - val_loss: 0.9782 - val_acc: 0.3500\n",
            "Epoch 46/300\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.97077\n",
            "60/60 - 0s - loss: 0.3594 - acc: 0.9500 - val_loss: 0.9811 - val_acc: 0.3500\n",
            "Epoch 47/300\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.97077\n",
            "60/60 - 0s - loss: 0.3516 - acc: 0.9500 - val_loss: 0.9766 - val_acc: 0.3500\n",
            "Epoch 48/300\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.97077\n",
            "60/60 - 0s - loss: 0.3469 - acc: 0.9500 - val_loss: 0.9738 - val_acc: 0.3500\n",
            "Epoch 49/300\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.97077\n",
            "60/60 - 0s - loss: 0.3515 - acc: 0.9500 - val_loss: 0.9722 - val_acc: 0.3500\n",
            "Epoch 50/300\n",
            "\n",
            "Epoch 00050: val_loss improved from 0.97077 to 0.96704, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3238 - acc: 0.9833 - val_loss: 0.9670 - val_acc: 0.3500\n",
            "Epoch 51/300\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.96704\n",
            "60/60 - 0s - loss: 0.3389 - acc: 0.9667 - val_loss: 0.9691 - val_acc: 0.3000\n",
            "Epoch 52/300\n",
            "\n",
            "Epoch 00052: val_loss improved from 0.96704 to 0.96155, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3297 - acc: 0.9833 - val_loss: 0.9616 - val_acc: 0.3000\n",
            "Epoch 53/300\n",
            "\n",
            "Epoch 00053: val_loss improved from 0.96155 to 0.95072, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3507 - acc: 0.9500 - val_loss: 0.9507 - val_acc: 0.3500\n",
            "Epoch 54/300\n",
            "\n",
            "Epoch 00054: val_loss improved from 0.95072 to 0.93925, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3295 - acc: 0.9500 - val_loss: 0.9393 - val_acc: 0.3500\n",
            "Epoch 55/300\n",
            "\n",
            "Epoch 00055: val_loss improved from 0.93925 to 0.92399, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3227 - acc: 0.9500 - val_loss: 0.9240 - val_acc: 0.3500\n",
            "Epoch 56/300\n",
            "\n",
            "Epoch 00056: val_loss improved from 0.92399 to 0.91869, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2787 - acc: 0.9833 - val_loss: 0.9187 - val_acc: 0.3000\n",
            "Epoch 57/300\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.2984 - acc: 0.9667 - val_loss: 0.9215 - val_acc: 0.2500\n",
            "Epoch 58/300\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.3435 - acc: 0.9500 - val_loss: 0.9311 - val_acc: 0.2500\n",
            "Epoch 59/300\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.2954 - acc: 0.9833 - val_loss: 0.9456 - val_acc: 0.2500\n",
            "Epoch 60/300\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.2838 - acc: 0.9667 - val_loss: 0.9625 - val_acc: 0.2500\n",
            "Epoch 61/300\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.3052 - acc: 0.9500 - val_loss: 0.9753 - val_acc: 0.2500\n",
            "Epoch 62/300\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.2768 - acc: 0.9833 - val_loss: 0.9800 - val_acc: 0.2000\n",
            "Epoch 63/300\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.2620 - acc: 0.9667 - val_loss: 0.9874 - val_acc: 0.2500\n",
            "Epoch 64/300\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.2729 - acc: 0.9667 - val_loss: 0.9928 - val_acc: 0.2500\n",
            "Epoch 65/300\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.2679 - acc: 0.9500 - val_loss: 0.9947 - val_acc: 0.2500\n",
            "Epoch 66/300\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.2788 - acc: 0.9833 - val_loss: 1.0124 - val_acc: 0.3000\n",
            "Epoch 67/300\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.2451 - acc: 0.9833 - val_loss: 1.0253 - val_acc: 0.3500\n",
            "Epoch 68/300\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.2369 - acc: 0.9833 - val_loss: 1.0375 - val_acc: 0.3500\n",
            "Epoch 69/300\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.2683 - acc: 0.9667 - val_loss: 1.0243 - val_acc: 0.3500\n",
            "Epoch 70/300\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.2511 - acc: 0.9667 - val_loss: 1.0229 - val_acc: 0.3500\n",
            "Epoch 71/300\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.2612 - acc: 0.9833 - val_loss: 1.0232 - val_acc: 0.3500\n",
            "Epoch 72/300\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.2826 - acc: 0.9500 - val_loss: 1.0270 - val_acc: 0.3500\n",
            "Epoch 73/300\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.2235 - acc: 0.9833 - val_loss: 1.0290 - val_acc: 0.3500\n",
            "Epoch 74/300\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.2110 - acc: 1.0000 - val_loss: 1.0517 - val_acc: 0.4500\n",
            "Epoch 75/300\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.2135 - acc: 0.9667 - val_loss: 1.0503 - val_acc: 0.4500\n",
            "Epoch 76/300\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.2304 - acc: 0.9333 - val_loss: 1.0427 - val_acc: 0.4000\n",
            "Epoch 77/300\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.2410 - acc: 0.9833 - val_loss: 1.0266 - val_acc: 0.3500\n",
            "Epoch 78/300\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.2259 - acc: 0.9667 - val_loss: 1.0339 - val_acc: 0.3500\n",
            "Epoch 79/300\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.1988 - acc: 0.9833 - val_loss: 1.0430 - val_acc: 0.3500\n",
            "Epoch 80/300\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.2056 - acc: 0.9833 - val_loss: 1.0609 - val_acc: 0.4000\n",
            "Epoch 81/300\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.1785 - acc: 0.9500 - val_loss: 1.0814 - val_acc: 0.4000\n",
            "Epoch 82/300\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.2065 - acc: 0.9833 - val_loss: 1.0926 - val_acc: 0.4000\n",
            "Epoch 83/300\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.2438 - acc: 0.9667 - val_loss: 1.1216 - val_acc: 0.4500\n",
            "Epoch 84/300\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.2172 - acc: 0.9833 - val_loss: 1.1409 - val_acc: 0.4500\n",
            "Epoch 85/300\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.1719 - acc: 0.9833 - val_loss: 1.1304 - val_acc: 0.4500\n",
            "Epoch 86/300\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.2338 - acc: 0.9333 - val_loss: 1.1214 - val_acc: 0.4500\n",
            "Epoch 87/300\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.2150 - acc: 0.9333 - val_loss: 1.0896 - val_acc: 0.4000\n",
            "Epoch 88/300\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.1949 - acc: 0.9833 - val_loss: 1.0649 - val_acc: 0.4000\n",
            "Epoch 89/300\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.1827 - acc: 0.9667 - val_loss: 1.0686 - val_acc: 0.4000\n",
            "Epoch 90/300\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.1716 - acc: 0.9667 - val_loss: 1.0644 - val_acc: 0.4000\n",
            "Epoch 91/300\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.1890 - acc: 0.9667 - val_loss: 1.0548 - val_acc: 0.4000\n",
            "Epoch 92/300\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.1682 - acc: 1.0000 - val_loss: 1.0280 - val_acc: 0.4000\n",
            "Epoch 93/300\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.1726 - acc: 0.9833 - val_loss: 1.0186 - val_acc: 0.4000\n",
            "Epoch 94/300\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.2010 - acc: 0.9667 - val_loss: 0.9922 - val_acc: 0.4000\n",
            "Epoch 95/300\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.1470 - acc: 0.9833 - val_loss: 0.9881 - val_acc: 0.4000\n",
            "Epoch 96/300\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.1440 - acc: 1.0000 - val_loss: 1.0256 - val_acc: 0.4000\n",
            "Epoch 97/300\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.1445 - acc: 0.9667 - val_loss: 1.0412 - val_acc: 0.4000\n",
            "Epoch 98/300\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.1377 - acc: 0.9667 - val_loss: 1.0530 - val_acc: 0.4000\n",
            "Epoch 99/300\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.1488 - acc: 1.0000 - val_loss: 1.0702 - val_acc: 0.4000\n",
            "Epoch 100/300\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.1552 - acc: 0.9667 - val_loss: 1.0870 - val_acc: 0.4000\n",
            "Epoch 101/300\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.1499 - acc: 1.0000 - val_loss: 1.0834 - val_acc: 0.4000\n",
            "Epoch 102/300\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.1639 - acc: 0.9667 - val_loss: 1.0657 - val_acc: 0.4000\n",
            "Epoch 103/300\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.1518 - acc: 1.0000 - val_loss: 1.0707 - val_acc: 0.4000\n",
            "Epoch 104/300\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.1454 - acc: 0.9833 - val_loss: 1.0658 - val_acc: 0.3000\n",
            "Epoch 105/300\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.1620 - acc: 0.9833 - val_loss: 1.0781 - val_acc: 0.3000\n",
            "Epoch 106/300\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.1353 - acc: 1.0000 - val_loss: 1.1107 - val_acc: 0.3500\n",
            "Epoch 107/300\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.1310 - acc: 0.9833 - val_loss: 1.1392 - val_acc: 0.4000\n",
            "Epoch 108/300\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.1078 - acc: 1.0000 - val_loss: 1.1678 - val_acc: 0.4000\n",
            "Epoch 109/300\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.1110 - acc: 0.9833 - val_loss: 1.2264 - val_acc: 0.4000\n",
            "Epoch 110/300\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.1143 - acc: 0.9833 - val_loss: 1.2630 - val_acc: 0.4000\n",
            "Epoch 111/300\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.1767 - acc: 0.9667 - val_loss: 1.2424 - val_acc: 0.4000\n",
            "Epoch 112/300\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.1323 - acc: 0.9833 - val_loss: 1.1823 - val_acc: 0.3500\n",
            "Epoch 113/300\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.1019 - acc: 1.0000 - val_loss: 1.1876 - val_acc: 0.4000\n",
            "Epoch 114/300\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.1417 - acc: 0.9667 - val_loss: 1.1925 - val_acc: 0.4500\n",
            "Epoch 115/300\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0838 - acc: 1.0000 - val_loss: 1.2563 - val_acc: 0.4000\n",
            "Epoch 116/300\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.1216 - acc: 0.9833 - val_loss: 1.2730 - val_acc: 0.4000\n",
            "Epoch 117/300\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.1033 - acc: 1.0000 - val_loss: 1.2919 - val_acc: 0.3500\n",
            "Epoch 118/300\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.1184 - acc: 0.9833 - val_loss: 1.3231 - val_acc: 0.3000\n",
            "Epoch 119/300\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0881 - acc: 1.0000 - val_loss: 1.3661 - val_acc: 0.3000\n",
            "Epoch 120/300\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0959 - acc: 0.9833 - val_loss: 1.3894 - val_acc: 0.3000\n",
            "Epoch 121/300\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.1135 - acc: 0.9833 - val_loss: 1.3992 - val_acc: 0.3500\n",
            "Epoch 122/300\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0840 - acc: 1.0000 - val_loss: 1.3861 - val_acc: 0.4000\n",
            "Epoch 123/300\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.1076 - acc: 0.9667 - val_loss: 1.3805 - val_acc: 0.4000\n",
            "Epoch 124/300\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0913 - acc: 0.9833 - val_loss: 1.4072 - val_acc: 0.4000\n",
            "Epoch 125/300\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0848 - acc: 1.0000 - val_loss: 1.4464 - val_acc: 0.4500\n",
            "Epoch 126/300\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0804 - acc: 1.0000 - val_loss: 1.4748 - val_acc: 0.4000\n",
            "Epoch 127/300\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0803 - acc: 1.0000 - val_loss: 1.5161 - val_acc: 0.3500\n",
            "Epoch 128/300\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.1044 - acc: 0.9833 - val_loss: 1.5501 - val_acc: 0.3500\n",
            "Epoch 129/300\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.1072 - acc: 0.9833 - val_loss: 1.5969 - val_acc: 0.3500\n",
            "Epoch 130/300\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.1035 - acc: 0.9667 - val_loss: 1.5851 - val_acc: 0.3500\n",
            "Epoch 131/300\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0971 - acc: 0.9833 - val_loss: 1.5429 - val_acc: 0.4000\n",
            "Epoch 132/300\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0741 - acc: 1.0000 - val_loss: 1.5511 - val_acc: 0.4000\n",
            "Epoch 133/300\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0890 - acc: 1.0000 - val_loss: 1.5941 - val_acc: 0.4000\n",
            "Epoch 134/300\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0858 - acc: 1.0000 - val_loss: 1.6177 - val_acc: 0.4000\n",
            "Epoch 135/300\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0611 - acc: 1.0000 - val_loss: 1.6538 - val_acc: 0.4000\n",
            "Epoch 136/300\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0661 - acc: 1.0000 - val_loss: 1.6788 - val_acc: 0.4000\n",
            "Epoch 137/300\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0728 - acc: 1.0000 - val_loss: 1.6260 - val_acc: 0.4500\n",
            "Epoch 138/300\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0560 - acc: 1.0000 - val_loss: 1.5971 - val_acc: 0.4500\n",
            "Epoch 139/300\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0795 - acc: 1.0000 - val_loss: 1.6098 - val_acc: 0.4500\n",
            "Epoch 140/300\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.1187 - acc: 0.9833 - val_loss: 1.5865 - val_acc: 0.5000\n",
            "Epoch 141/300\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0817 - acc: 0.9833 - val_loss: 1.6367 - val_acc: 0.4500\n",
            "Epoch 142/300\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0811 - acc: 0.9833 - val_loss: 1.7059 - val_acc: 0.4500\n",
            "Epoch 143/300\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0605 - acc: 1.0000 - val_loss: 1.7392 - val_acc: 0.4500\n",
            "Epoch 144/300\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0501 - acc: 1.0000 - val_loss: 1.7348 - val_acc: 0.4500\n",
            "Epoch 145/300\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0783 - acc: 0.9833 - val_loss: 1.7590 - val_acc: 0.4000\n",
            "Epoch 146/300\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0853 - acc: 0.9833 - val_loss: 1.7242 - val_acc: 0.4500\n",
            "Epoch 147/300\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0794 - acc: 1.0000 - val_loss: 1.6558 - val_acc: 0.4500\n",
            "Epoch 148/300\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0745 - acc: 1.0000 - val_loss: 1.6520 - val_acc: 0.4500\n",
            "Epoch 149/300\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0571 - acc: 1.0000 - val_loss: 1.6598 - val_acc: 0.5000\n",
            "Epoch 150/300\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0603 - acc: 1.0000 - val_loss: 1.7143 - val_acc: 0.5000\n",
            "Epoch 151/300\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0770 - acc: 0.9833 - val_loss: 1.7937 - val_acc: 0.4500\n",
            "Epoch 152/300\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0642 - acc: 0.9833 - val_loss: 1.9337 - val_acc: 0.3500\n",
            "Epoch 153/300\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0670 - acc: 0.9833 - val_loss: 1.9898 - val_acc: 0.3500\n",
            "Epoch 154/300\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0816 - acc: 0.9833 - val_loss: 1.9679 - val_acc: 0.4000\n",
            "Epoch 155/300\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0642 - acc: 1.0000 - val_loss: 1.8878 - val_acc: 0.4500\n",
            "Epoch 156/300\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0435 - acc: 1.0000 - val_loss: 1.8772 - val_acc: 0.4500\n",
            "Epoch 157/300\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0917 - acc: 0.9833 - val_loss: 1.9180 - val_acc: 0.4500\n",
            "Epoch 158/300\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.1447 - acc: 0.9667 - val_loss: 1.8629 - val_acc: 0.4500\n",
            "Epoch 159/300\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0571 - acc: 1.0000 - val_loss: 1.7680 - val_acc: 0.4500\n",
            "Epoch 160/300\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.1004 - acc: 0.9667 - val_loss: 1.7437 - val_acc: 0.4500\n",
            "Epoch 161/300\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0524 - acc: 1.0000 - val_loss: 1.7639 - val_acc: 0.4500\n",
            "Epoch 162/300\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0510 - acc: 1.0000 - val_loss: 1.7716 - val_acc: 0.4500\n",
            "Epoch 163/300\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0643 - acc: 0.9833 - val_loss: 1.8380 - val_acc: 0.4500\n",
            "Epoch 164/300\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0662 - acc: 0.9833 - val_loss: 1.9773 - val_acc: 0.4000\n",
            "Epoch 165/300\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.1181 - acc: 0.9833 - val_loss: 1.9920 - val_acc: 0.4500\n",
            "Epoch 166/300\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0684 - acc: 0.9833 - val_loss: 1.9590 - val_acc: 0.4500\n",
            "Epoch 167/300\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0599 - acc: 1.0000 - val_loss: 1.9512 - val_acc: 0.4500\n",
            "Epoch 168/300\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0498 - acc: 1.0000 - val_loss: 1.9731 - val_acc: 0.4500\n",
            "Epoch 169/300\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0791 - acc: 1.0000 - val_loss: 1.9544 - val_acc: 0.4500\n",
            "Epoch 170/300\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0863 - acc: 0.9833 - val_loss: 1.9360 - val_acc: 0.4500\n",
            "Epoch 171/300\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0600 - acc: 0.9833 - val_loss: 1.9053 - val_acc: 0.4500\n",
            "Epoch 172/300\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0867 - acc: 0.9833 - val_loss: 1.8952 - val_acc: 0.4500\n",
            "Epoch 173/300\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.1361 - acc: 0.9833 - val_loss: 1.8166 - val_acc: 0.4500\n",
            "Epoch 174/300\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0644 - acc: 1.0000 - val_loss: 1.7659 - val_acc: 0.4500\n",
            "Epoch 175/300\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.1047 - acc: 1.0000 - val_loss: 1.7191 - val_acc: 0.4500\n",
            "Epoch 176/300\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0832 - acc: 0.9667 - val_loss: 1.7094 - val_acc: 0.4500\n",
            "Epoch 177/300\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0542 - acc: 1.0000 - val_loss: 1.7419 - val_acc: 0.4500\n",
            "Epoch 178/300\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0667 - acc: 1.0000 - val_loss: 1.8100 - val_acc: 0.4500\n",
            "Epoch 179/300\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0693 - acc: 1.0000 - val_loss: 1.8272 - val_acc: 0.4500\n",
            "Epoch 180/300\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0502 - acc: 1.0000 - val_loss: 1.8273 - val_acc: 0.4500\n",
            "Epoch 181/300\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0537 - acc: 1.0000 - val_loss: 1.8547 - val_acc: 0.5000\n",
            "Epoch 182/300\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0440 - acc: 1.0000 - val_loss: 1.9124 - val_acc: 0.5000\n",
            "Epoch 183/300\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0641 - acc: 0.9833 - val_loss: 1.9654 - val_acc: 0.5500\n",
            "Epoch 184/300\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0476 - acc: 1.0000 - val_loss: 1.9759 - val_acc: 0.5500\n",
            "Epoch 185/300\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0592 - acc: 0.9833 - val_loss: 1.9706 - val_acc: 0.5500\n",
            "Epoch 186/300\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0887 - acc: 0.9833 - val_loss: 2.0263 - val_acc: 0.4500\n",
            "Epoch 187/300\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0480 - acc: 1.0000 - val_loss: 1.9962 - val_acc: 0.5000\n",
            "Epoch 188/300\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0714 - acc: 0.9833 - val_loss: 1.9453 - val_acc: 0.5500\n",
            "Epoch 189/300\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0535 - acc: 1.0000 - val_loss: 1.9001 - val_acc: 0.5500\n",
            "Epoch 190/300\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0417 - acc: 1.0000 - val_loss: 1.8540 - val_acc: 0.5500\n",
            "Epoch 191/300\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0320 - acc: 1.0000 - val_loss: 1.8328 - val_acc: 0.5500\n",
            "Epoch 192/300\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0435 - acc: 1.0000 - val_loss: 1.8567 - val_acc: 0.5500\n",
            "Epoch 193/300\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0459 - acc: 1.0000 - val_loss: 1.9300 - val_acc: 0.5500\n",
            "Epoch 194/300\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0503 - acc: 1.0000 - val_loss: 2.0128 - val_acc: 0.4500\n",
            "Epoch 195/300\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0606 - acc: 1.0000 - val_loss: 2.0759 - val_acc: 0.4500\n",
            "Epoch 196/300\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0760 - acc: 0.9833 - val_loss: 2.0179 - val_acc: 0.5500\n",
            "Epoch 197/300\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0624 - acc: 1.0000 - val_loss: 2.0258 - val_acc: 0.5500\n",
            "Epoch 198/300\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0576 - acc: 1.0000 - val_loss: 2.0473 - val_acc: 0.5500\n",
            "Epoch 199/300\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0741 - acc: 0.9833 - val_loss: 2.0336 - val_acc: 0.5500\n",
            "Epoch 200/300\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0805 - acc: 0.9833 - val_loss: 2.0759 - val_acc: 0.4500\n",
            "Epoch 201/300\n",
            "\n",
            "Epoch 00201: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0461 - acc: 0.9833 - val_loss: 2.0848 - val_acc: 0.4500\n",
            "Epoch 202/300\n",
            "\n",
            "Epoch 00202: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0416 - acc: 1.0000 - val_loss: 2.0416 - val_acc: 0.5500\n",
            "Epoch 203/300\n",
            "\n",
            "Epoch 00203: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0522 - acc: 0.9833 - val_loss: 1.9977 - val_acc: 0.5500\n",
            "Epoch 204/300\n",
            "\n",
            "Epoch 00204: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0254 - acc: 1.0000 - val_loss: 1.9253 - val_acc: 0.5500\n",
            "Epoch 205/300\n",
            "\n",
            "Epoch 00205: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0662 - acc: 1.0000 - val_loss: 1.9549 - val_acc: 0.5000\n",
            "Epoch 206/300\n",
            "\n",
            "Epoch 00206: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0648 - acc: 0.9833 - val_loss: 1.9744 - val_acc: 0.4500\n",
            "Epoch 207/300\n",
            "\n",
            "Epoch 00207: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0478 - acc: 0.9833 - val_loss: 1.9265 - val_acc: 0.5000\n",
            "Epoch 208/300\n",
            "\n",
            "Epoch 00208: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0303 - acc: 1.0000 - val_loss: 1.9144 - val_acc: 0.5000\n",
            "Epoch 209/300\n",
            "\n",
            "Epoch 00209: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0489 - acc: 1.0000 - val_loss: 1.9393 - val_acc: 0.5000\n",
            "Epoch 210/300\n",
            "\n",
            "Epoch 00210: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0826 - acc: 0.9833 - val_loss: 1.9968 - val_acc: 0.5000\n",
            "Epoch 211/300\n",
            "\n",
            "Epoch 00211: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0255 - acc: 1.0000 - val_loss: 2.0854 - val_acc: 0.4500\n",
            "Epoch 212/300\n",
            "\n",
            "Epoch 00212: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0439 - acc: 1.0000 - val_loss: 2.1641 - val_acc: 0.4500\n",
            "Epoch 213/300\n",
            "\n",
            "Epoch 00213: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.1382 - acc: 0.9833 - val_loss: 2.1739 - val_acc: 0.4500\n",
            "Epoch 214/300\n",
            "\n",
            "Epoch 00214: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0589 - acc: 0.9667 - val_loss: 2.1548 - val_acc: 0.4500\n",
            "Epoch 215/300\n",
            "\n",
            "Epoch 00215: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0522 - acc: 0.9833 - val_loss: 2.1717 - val_acc: 0.4500\n",
            "Epoch 216/300\n",
            "\n",
            "Epoch 00216: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0548 - acc: 1.0000 - val_loss: 2.2078 - val_acc: 0.4500\n",
            "Epoch 217/300\n",
            "\n",
            "Epoch 00217: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0428 - acc: 1.0000 - val_loss: 2.2216 - val_acc: 0.4500\n",
            "Epoch 218/300\n",
            "\n",
            "Epoch 00218: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0261 - acc: 1.0000 - val_loss: 2.2151 - val_acc: 0.4500\n",
            "Epoch 219/300\n",
            "\n",
            "Epoch 00219: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0954 - acc: 0.9667 - val_loss: 2.1180 - val_acc: 0.5000\n",
            "Epoch 220/300\n",
            "\n",
            "Epoch 00220: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0619 - acc: 0.9833 - val_loss: 2.0885 - val_acc: 0.5000\n",
            "Epoch 221/300\n",
            "\n",
            "Epoch 00221: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0291 - acc: 1.0000 - val_loss: 2.0838 - val_acc: 0.5000\n",
            "Epoch 222/300\n",
            "\n",
            "Epoch 00222: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0720 - acc: 0.9833 - val_loss: 2.0481 - val_acc: 0.4500\n",
            "Epoch 223/300\n",
            "\n",
            "Epoch 00223: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0496 - acc: 1.0000 - val_loss: 2.0357 - val_acc: 0.5000\n",
            "Epoch 224/300\n",
            "\n",
            "Epoch 00224: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0480 - acc: 1.0000 - val_loss: 2.0455 - val_acc: 0.5000\n",
            "Epoch 225/300\n",
            "\n",
            "Epoch 00225: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0595 - acc: 1.0000 - val_loss: 2.0843 - val_acc: 0.5000\n",
            "Epoch 226/300\n",
            "\n",
            "Epoch 00226: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0440 - acc: 1.0000 - val_loss: 2.1362 - val_acc: 0.5000\n",
            "Epoch 227/300\n",
            "\n",
            "Epoch 00227: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0449 - acc: 1.0000 - val_loss: 2.1419 - val_acc: 0.5500\n",
            "Epoch 228/300\n",
            "\n",
            "Epoch 00228: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0551 - acc: 0.9833 - val_loss: 2.1147 - val_acc: 0.6000\n",
            "Epoch 229/300\n",
            "\n",
            "Epoch 00229: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0333 - acc: 1.0000 - val_loss: 2.0799 - val_acc: 0.6000\n",
            "Epoch 230/300\n",
            "\n",
            "Epoch 00230: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0507 - acc: 1.0000 - val_loss: 2.0518 - val_acc: 0.6000\n",
            "Epoch 231/300\n",
            "\n",
            "Epoch 00231: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0351 - acc: 1.0000 - val_loss: 2.0524 - val_acc: 0.6500\n",
            "Epoch 232/300\n",
            "\n",
            "Epoch 00232: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0414 - acc: 1.0000 - val_loss: 2.1096 - val_acc: 0.6500\n",
            "Epoch 233/300\n",
            "\n",
            "Epoch 00233: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0291 - acc: 1.0000 - val_loss: 2.1629 - val_acc: 0.6000\n",
            "Epoch 234/300\n",
            "\n",
            "Epoch 00234: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0488 - acc: 1.0000 - val_loss: 2.2275 - val_acc: 0.6000\n",
            "Epoch 235/300\n",
            "\n",
            "Epoch 00235: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0322 - acc: 1.0000 - val_loss: 2.2965 - val_acc: 0.5500\n",
            "Epoch 236/300\n",
            "\n",
            "Epoch 00236: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0357 - acc: 1.0000 - val_loss: 2.3181 - val_acc: 0.5500\n",
            "Epoch 237/300\n",
            "\n",
            "Epoch 00237: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0260 - acc: 1.0000 - val_loss: 2.3369 - val_acc: 0.5500\n",
            "Epoch 238/300\n",
            "\n",
            "Epoch 00238: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0236 - acc: 1.0000 - val_loss: 2.3699 - val_acc: 0.5000\n",
            "Epoch 239/300\n",
            "\n",
            "Epoch 00239: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0633 - acc: 0.9833 - val_loss: 2.3854 - val_acc: 0.4500\n",
            "Epoch 240/300\n",
            "\n",
            "Epoch 00240: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0233 - acc: 1.0000 - val_loss: 2.3755 - val_acc: 0.5000\n",
            "Epoch 241/300\n",
            "\n",
            "Epoch 00241: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0266 - acc: 1.0000 - val_loss: 2.3533 - val_acc: 0.5000\n",
            "Epoch 242/300\n",
            "\n",
            "Epoch 00242: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0392 - acc: 1.0000 - val_loss: 2.3928 - val_acc: 0.5000\n",
            "Epoch 243/300\n",
            "\n",
            "Epoch 00243: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0329 - acc: 0.9833 - val_loss: 2.3684 - val_acc: 0.5000\n",
            "Epoch 244/300\n",
            "\n",
            "Epoch 00244: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0565 - acc: 0.9833 - val_loss: 2.2956 - val_acc: 0.5500\n",
            "Epoch 245/300\n",
            "\n",
            "Epoch 00245: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0437 - acc: 0.9833 - val_loss: 2.2131 - val_acc: 0.5500\n",
            "Epoch 246/300\n",
            "\n",
            "Epoch 00246: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0418 - acc: 1.0000 - val_loss: 2.1828 - val_acc: 0.6000\n",
            "Epoch 247/300\n",
            "\n",
            "Epoch 00247: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0527 - acc: 0.9833 - val_loss: 2.2053 - val_acc: 0.6000\n",
            "Epoch 248/300\n",
            "\n",
            "Epoch 00248: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0409 - acc: 1.0000 - val_loss: 2.2451 - val_acc: 0.6000\n",
            "Epoch 249/300\n",
            "\n",
            "Epoch 00249: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0239 - acc: 1.0000 - val_loss: 2.2697 - val_acc: 0.6000\n",
            "Epoch 250/300\n",
            "\n",
            "Epoch 00250: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0331 - acc: 1.0000 - val_loss: 2.3054 - val_acc: 0.6000\n",
            "Epoch 251/300\n",
            "\n",
            "Epoch 00251: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0753 - acc: 0.9833 - val_loss: 2.2752 - val_acc: 0.5500\n",
            "Epoch 252/300\n",
            "\n",
            "Epoch 00252: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0559 - acc: 1.0000 - val_loss: 2.2538 - val_acc: 0.4500\n",
            "Epoch 253/300\n",
            "\n",
            "Epoch 00253: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0308 - acc: 1.0000 - val_loss: 2.3235 - val_acc: 0.4500\n",
            "Epoch 254/300\n",
            "\n",
            "Epoch 00254: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0991 - acc: 0.9667 - val_loss: 2.3577 - val_acc: 0.4500\n",
            "Epoch 255/300\n",
            "\n",
            "Epoch 00255: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0398 - acc: 1.0000 - val_loss: 2.2864 - val_acc: 0.4500\n",
            "Epoch 256/300\n",
            "\n",
            "Epoch 00256: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0289 - acc: 1.0000 - val_loss: 2.2379 - val_acc: 0.4000\n",
            "Epoch 257/300\n",
            "\n",
            "Epoch 00257: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0433 - acc: 1.0000 - val_loss: 2.2908 - val_acc: 0.4000\n",
            "Epoch 258/300\n",
            "\n",
            "Epoch 00258: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0754 - acc: 0.9833 - val_loss: 2.2873 - val_acc: 0.4000\n",
            "Epoch 259/300\n",
            "\n",
            "Epoch 00259: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0910 - acc: 0.9833 - val_loss: 2.1109 - val_acc: 0.5000\n",
            "Epoch 260/300\n",
            "\n",
            "Epoch 00260: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0276 - acc: 1.0000 - val_loss: 1.9464 - val_acc: 0.5000\n",
            "Epoch 261/300\n",
            "\n",
            "Epoch 00261: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0763 - acc: 0.9667 - val_loss: 1.9286 - val_acc: 0.5000\n",
            "Epoch 262/300\n",
            "\n",
            "Epoch 00262: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0484 - acc: 1.0000 - val_loss: 1.8937 - val_acc: 0.5500\n",
            "Epoch 263/300\n",
            "\n",
            "Epoch 00263: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0351 - acc: 1.0000 - val_loss: 1.9612 - val_acc: 0.5000\n",
            "Epoch 264/300\n",
            "\n",
            "Epoch 00264: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0284 - acc: 1.0000 - val_loss: 2.0321 - val_acc: 0.4500\n",
            "Epoch 265/300\n",
            "\n",
            "Epoch 00265: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0369 - acc: 1.0000 - val_loss: 2.1196 - val_acc: 0.4500\n",
            "Epoch 266/300\n",
            "\n",
            "Epoch 00266: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0520 - acc: 1.0000 - val_loss: 2.1980 - val_acc: 0.4500\n",
            "Epoch 267/300\n",
            "\n",
            "Epoch 00267: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0575 - acc: 0.9833 - val_loss: 2.1794 - val_acc: 0.4500\n",
            "Epoch 268/300\n",
            "\n",
            "Epoch 00268: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0268 - acc: 1.0000 - val_loss: 2.1589 - val_acc: 0.4500\n",
            "Epoch 269/300\n",
            "\n",
            "Epoch 00269: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0499 - acc: 0.9833 - val_loss: 2.1142 - val_acc: 0.4500\n",
            "Epoch 270/300\n",
            "\n",
            "Epoch 00270: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0282 - acc: 1.0000 - val_loss: 2.1211 - val_acc: 0.5000\n",
            "Epoch 271/300\n",
            "\n",
            "Epoch 00271: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0348 - acc: 1.0000 - val_loss: 2.1750 - val_acc: 0.5000\n",
            "Epoch 272/300\n",
            "\n",
            "Epoch 00272: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0240 - acc: 1.0000 - val_loss: 2.2502 - val_acc: 0.5000\n",
            "Epoch 273/300\n",
            "\n",
            "Epoch 00273: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0395 - acc: 1.0000 - val_loss: 2.2983 - val_acc: 0.4500\n",
            "Epoch 274/300\n",
            "\n",
            "Epoch 00274: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0348 - acc: 1.0000 - val_loss: 2.2993 - val_acc: 0.4500\n",
            "Epoch 275/300\n",
            "\n",
            "Epoch 00275: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0302 - acc: 1.0000 - val_loss: 2.2811 - val_acc: 0.4500\n",
            "Epoch 276/300\n",
            "\n",
            "Epoch 00276: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0297 - acc: 1.0000 - val_loss: 2.2489 - val_acc: 0.4500\n",
            "Epoch 277/300\n",
            "\n",
            "Epoch 00277: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0204 - acc: 1.0000 - val_loss: 2.2377 - val_acc: 0.4500\n",
            "Epoch 278/300\n",
            "\n",
            "Epoch 00278: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0143 - acc: 1.0000 - val_loss: 2.2380 - val_acc: 0.4500\n",
            "Epoch 279/300\n",
            "\n",
            "Epoch 00279: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0444 - acc: 0.9833 - val_loss: 2.3057 - val_acc: 0.4500\n",
            "Epoch 280/300\n",
            "\n",
            "Epoch 00280: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0378 - acc: 1.0000 - val_loss: 2.3241 - val_acc: 0.4500\n",
            "Epoch 281/300\n",
            "\n",
            "Epoch 00281: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0253 - acc: 1.0000 - val_loss: 2.3152 - val_acc: 0.5500\n",
            "Epoch 282/300\n",
            "\n",
            "Epoch 00282: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0223 - acc: 1.0000 - val_loss: 2.3096 - val_acc: 0.5500\n",
            "Epoch 283/300\n",
            "\n",
            "Epoch 00283: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0356 - acc: 1.0000 - val_loss: 2.3169 - val_acc: 0.5500\n",
            "Epoch 284/300\n",
            "\n",
            "Epoch 00284: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0286 - acc: 1.0000 - val_loss: 2.3551 - val_acc: 0.5500\n",
            "Epoch 285/300\n",
            "\n",
            "Epoch 00285: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0394 - acc: 1.0000 - val_loss: 2.4082 - val_acc: 0.5500\n",
            "Epoch 286/300\n",
            "\n",
            "Epoch 00286: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0455 - acc: 0.9833 - val_loss: 2.3856 - val_acc: 0.5500\n",
            "Epoch 287/300\n",
            "\n",
            "Epoch 00287: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0696 - acc: 0.9833 - val_loss: 2.3777 - val_acc: 0.6000\n",
            "Epoch 288/300\n",
            "\n",
            "Epoch 00288: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0167 - acc: 1.0000 - val_loss: 2.3387 - val_acc: 0.6000\n",
            "Epoch 289/300\n",
            "\n",
            "Epoch 00289: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0431 - acc: 0.9833 - val_loss: 2.3344 - val_acc: 0.6500\n",
            "Epoch 290/300\n",
            "\n",
            "Epoch 00290: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.1863 - acc: 0.9333 - val_loss: 2.2635 - val_acc: 0.6500\n",
            "Epoch 291/300\n",
            "\n",
            "Epoch 00291: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0599 - acc: 0.9833 - val_loss: 2.0771 - val_acc: 0.6000\n",
            "Epoch 292/300\n",
            "\n",
            "Epoch 00292: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0460 - acc: 1.0000 - val_loss: 1.9986 - val_acc: 0.6000\n",
            "Epoch 293/300\n",
            "\n",
            "Epoch 00293: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0387 - acc: 1.0000 - val_loss: 1.9708 - val_acc: 0.6000\n",
            "Epoch 294/300\n",
            "\n",
            "Epoch 00294: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0949 - acc: 0.9833 - val_loss: 1.9484 - val_acc: 0.6000\n",
            "Epoch 295/300\n",
            "\n",
            "Epoch 00295: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0492 - acc: 1.0000 - val_loss: 1.9798 - val_acc: 0.6000\n",
            "Epoch 296/300\n",
            "\n",
            "Epoch 00296: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0675 - acc: 0.9667 - val_loss: 2.0269 - val_acc: 0.6500\n",
            "Epoch 297/300\n",
            "\n",
            "Epoch 00297: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0256 - acc: 1.0000 - val_loss: 2.1032 - val_acc: 0.6500\n",
            "Epoch 298/300\n",
            "\n",
            "Epoch 00298: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0379 - acc: 1.0000 - val_loss: 2.2033 - val_acc: 0.6500\n",
            "Epoch 299/300\n",
            "\n",
            "Epoch 00299: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0548 - acc: 0.9833 - val_loss: 2.2572 - val_acc: 0.6000\n",
            "Epoch 300/300\n",
            "\n",
            "Epoch 00300: val_loss did not improve from 0.91869\n",
            "60/60 - 0s - loss: 0.0413 - acc: 1.0000 - val_loss: 2.2445 - val_acc: 0.6000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nq2RMUlUAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "# Evaluate on test data\n",
            "40/40 [==============================] - 0s 2ms/sample - loss: 1.7340 - acc: 0.7500\n",
            "test loss, test acc: [1.7339836712599208, 0.75]\n",
            "EEG_Deep/Data2A/parsed_P07T.mat\n",
            "Filtering of Training Data Finished\n",
            "EEG_Deep/Data2A/parsed_P07E.mat\n",
            "Filtering of Testing Data Finished\n",
            "(1, 2)\n",
            "Finding labels in training data\n",
            "Finding labels in testing data\n",
            "[2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1]\n",
            "(60, 12, 1536)\n",
            "(60,)\n",
            "(20, 12, 1536)\n",
            "(20,)\n",
            "[[0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]]\n",
            "X_train shape: (60, 1, 12, 1536)\n",
            "60 train samples\n",
            "20 val samples\n",
            "(40, 12, 1536)\n",
            "(40, 1)\n",
            "X_train shape: (40, 1, 12, 1536)\n",
            "40 train samples\n",
            "Train on 60 samples, validate on 20 samples\n",
            "Epoch 1/300\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 1.35290, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 1s - loss: 1.3405 - acc: 0.3833 - val_loss: 1.3529 - val_acc: 0.6000\n",
            "Epoch 2/300\n",
            "\n",
            "Epoch 00002: val_loss improved from 1.35290 to 1.32883, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 1.1062 - acc: 0.5000 - val_loss: 1.3288 - val_acc: 0.4500\n",
            "Epoch 3/300\n",
            "\n",
            "Epoch 00003: val_loss improved from 1.32883 to 1.30207, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.9447 - acc: 0.6333 - val_loss: 1.3021 - val_acc: 0.4500\n",
            "Epoch 4/300\n",
            "\n",
            "Epoch 00004: val_loss improved from 1.30207 to 1.27491, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.8554 - acc: 0.7667 - val_loss: 1.2749 - val_acc: 0.4500\n",
            "Epoch 5/300\n",
            "\n",
            "Epoch 00005: val_loss improved from 1.27491 to 1.25005, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.7998 - acc: 0.7833 - val_loss: 1.2501 - val_acc: 0.4500\n",
            "Epoch 6/300\n",
            "\n",
            "Epoch 00006: val_loss improved from 1.25005 to 1.22559, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.7683 - acc: 0.7500 - val_loss: 1.2256 - val_acc: 0.4500\n",
            "Epoch 7/300\n",
            "\n",
            "Epoch 00007: val_loss improved from 1.22559 to 1.20105, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.7263 - acc: 0.7833 - val_loss: 1.2010 - val_acc: 0.4500\n",
            "Epoch 8/300\n",
            "\n",
            "Epoch 00008: val_loss improved from 1.20105 to 1.17566, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6970 - acc: 0.8167 - val_loss: 1.1757 - val_acc: 0.4500\n",
            "Epoch 9/300\n",
            "\n",
            "Epoch 00009: val_loss improved from 1.17566 to 1.15211, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6794 - acc: 0.8000 - val_loss: 1.1521 - val_acc: 0.4500\n",
            "Epoch 10/300\n",
            "\n",
            "Epoch 00010: val_loss improved from 1.15211 to 1.12915, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6527 - acc: 0.7833 - val_loss: 1.1292 - val_acc: 0.5000\n",
            "Epoch 11/300\n",
            "\n",
            "Epoch 00011: val_loss improved from 1.12915 to 1.10703, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6362 - acc: 0.8000 - val_loss: 1.1070 - val_acc: 0.5000\n",
            "Epoch 12/300\n",
            "\n",
            "Epoch 00012: val_loss improved from 1.10703 to 1.08653, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6078 - acc: 0.8500 - val_loss: 1.0865 - val_acc: 0.5000\n",
            "Epoch 13/300\n",
            "\n",
            "Epoch 00013: val_loss improved from 1.08653 to 1.06582, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6202 - acc: 0.8000 - val_loss: 1.0658 - val_acc: 0.5000\n",
            "Epoch 14/300\n",
            "\n",
            "Epoch 00014: val_loss improved from 1.06582 to 1.04944, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5752 - acc: 0.8500 - val_loss: 1.0494 - val_acc: 0.5000\n",
            "Epoch 15/300\n",
            "\n",
            "Epoch 00015: val_loss improved from 1.04944 to 1.03553, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5756 - acc: 0.8500 - val_loss: 1.0355 - val_acc: 0.5000\n",
            "Epoch 16/300\n",
            "\n",
            "Epoch 00016: val_loss improved from 1.03553 to 1.01930, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5502 - acc: 0.8667 - val_loss: 1.0193 - val_acc: 0.5000\n",
            "Epoch 17/300\n",
            "\n",
            "Epoch 00017: val_loss improved from 1.01930 to 1.00614, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5529 - acc: 0.8500 - val_loss: 1.0061 - val_acc: 0.5000\n",
            "Epoch 18/300\n",
            "\n",
            "Epoch 00018: val_loss improved from 1.00614 to 0.99456, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5252 - acc: 0.9000 - val_loss: 0.9946 - val_acc: 0.6000\n",
            "Epoch 19/300\n",
            "\n",
            "Epoch 00019: val_loss improved from 0.99456 to 0.98278, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5151 - acc: 0.9167 - val_loss: 0.9828 - val_acc: 0.6000\n",
            "Epoch 20/300\n",
            "\n",
            "Epoch 00020: val_loss improved from 0.98278 to 0.97348, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5144 - acc: 0.8667 - val_loss: 0.9735 - val_acc: 0.5500\n",
            "Epoch 21/300\n",
            "\n",
            "Epoch 00021: val_loss improved from 0.97348 to 0.96233, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4912 - acc: 0.9333 - val_loss: 0.9623 - val_acc: 0.5500\n",
            "Epoch 22/300\n",
            "\n",
            "Epoch 00022: val_loss improved from 0.96233 to 0.95233, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4694 - acc: 0.9167 - val_loss: 0.9523 - val_acc: 0.5500\n",
            "Epoch 23/300\n",
            "\n",
            "Epoch 00023: val_loss improved from 0.95233 to 0.94448, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4618 - acc: 0.9333 - val_loss: 0.9445 - val_acc: 0.5500\n",
            "Epoch 24/300\n",
            "\n",
            "Epoch 00024: val_loss improved from 0.94448 to 0.93612, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4754 - acc: 0.9333 - val_loss: 0.9361 - val_acc: 0.6000\n",
            "Epoch 25/300\n",
            "\n",
            "Epoch 00025: val_loss improved from 0.93612 to 0.93008, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4731 - acc: 0.8833 - val_loss: 0.9301 - val_acc: 0.5500\n",
            "Epoch 26/300\n",
            "\n",
            "Epoch 00026: val_loss improved from 0.93008 to 0.92426, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4444 - acc: 0.9167 - val_loss: 0.9243 - val_acc: 0.6000\n",
            "Epoch 27/300\n",
            "\n",
            "Epoch 00027: val_loss improved from 0.92426 to 0.91922, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4556 - acc: 0.9667 - val_loss: 0.9192 - val_acc: 0.6000\n",
            "Epoch 28/300\n",
            "\n",
            "Epoch 00028: val_loss improved from 0.91922 to 0.91394, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4502 - acc: 0.9167 - val_loss: 0.9139 - val_acc: 0.6000\n",
            "Epoch 29/300\n",
            "\n",
            "Epoch 00029: val_loss improved from 0.91394 to 0.91350, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4335 - acc: 0.9333 - val_loss: 0.9135 - val_acc: 0.6000\n",
            "Epoch 30/300\n",
            "\n",
            "Epoch 00030: val_loss improved from 0.91350 to 0.91034, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4197 - acc: 0.9833 - val_loss: 0.9103 - val_acc: 0.5500\n",
            "Epoch 31/300\n",
            "\n",
            "Epoch 00031: val_loss improved from 0.91034 to 0.90614, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4159 - acc: 0.9000 - val_loss: 0.9061 - val_acc: 0.5500\n",
            "Epoch 32/300\n",
            "\n",
            "Epoch 00032: val_loss improved from 0.90614 to 0.89781, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4434 - acc: 0.9000 - val_loss: 0.8978 - val_acc: 0.4500\n",
            "Epoch 33/300\n",
            "\n",
            "Epoch 00033: val_loss improved from 0.89781 to 0.89133, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3959 - acc: 0.9667 - val_loss: 0.8913 - val_acc: 0.4500\n",
            "Epoch 34/300\n",
            "\n",
            "Epoch 00034: val_loss improved from 0.89133 to 0.88613, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3620 - acc: 0.9500 - val_loss: 0.8861 - val_acc: 0.4500\n",
            "Epoch 35/300\n",
            "\n",
            "Epoch 00035: val_loss improved from 0.88613 to 0.87959, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3782 - acc: 0.9833 - val_loss: 0.8796 - val_acc: 0.4500\n",
            "Epoch 36/300\n",
            "\n",
            "Epoch 00036: val_loss improved from 0.87959 to 0.87573, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4091 - acc: 0.9000 - val_loss: 0.8757 - val_acc: 0.4500\n",
            "Epoch 37/300\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.87573\n",
            "60/60 - 0s - loss: 0.3521 - acc: 0.9667 - val_loss: 0.8769 - val_acc: 0.4500\n",
            "Epoch 38/300\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.87573\n",
            "60/60 - 0s - loss: 0.3921 - acc: 0.9000 - val_loss: 0.8761 - val_acc: 0.4500\n",
            "Epoch 39/300\n",
            "\n",
            "Epoch 00039: val_loss improved from 0.87573 to 0.87390, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3786 - acc: 0.9500 - val_loss: 0.8739 - val_acc: 0.4500\n",
            "Epoch 40/300\n",
            "\n",
            "Epoch 00040: val_loss improved from 0.87390 to 0.87182, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3848 - acc: 0.9333 - val_loss: 0.8718 - val_acc: 0.4500\n",
            "Epoch 41/300\n",
            "\n",
            "Epoch 00041: val_loss improved from 0.87182 to 0.86328, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4110 - acc: 0.9500 - val_loss: 0.8633 - val_acc: 0.4500\n",
            "Epoch 42/300\n",
            "\n",
            "Epoch 00042: val_loss improved from 0.86328 to 0.85262, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3972 - acc: 0.9333 - val_loss: 0.8526 - val_acc: 0.5000\n",
            "Epoch 43/300\n",
            "\n",
            "Epoch 00043: val_loss improved from 0.85262 to 0.84255, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3557 - acc: 0.9500 - val_loss: 0.8425 - val_acc: 0.5000\n",
            "Epoch 44/300\n",
            "\n",
            "Epoch 00044: val_loss improved from 0.84255 to 0.83706, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3804 - acc: 0.9333 - val_loss: 0.8371 - val_acc: 0.5000\n",
            "Epoch 45/300\n",
            "\n",
            "Epoch 00045: val_loss improved from 0.83706 to 0.83257, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3320 - acc: 0.9667 - val_loss: 0.8326 - val_acc: 0.5000\n",
            "Epoch 46/300\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.3261 - acc: 0.9333 - val_loss: 0.8342 - val_acc: 0.5000\n",
            "Epoch 47/300\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.3422 - acc: 0.9000 - val_loss: 0.8431 - val_acc: 0.5500\n",
            "Epoch 48/300\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.3593 - acc: 0.9333 - val_loss: 0.8509 - val_acc: 0.5500\n",
            "Epoch 49/300\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.3499 - acc: 0.9667 - val_loss: 0.8572 - val_acc: 0.5500\n",
            "Epoch 50/300\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.3264 - acc: 0.9833 - val_loss: 0.8646 - val_acc: 0.5500\n",
            "Epoch 51/300\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.3325 - acc: 0.9833 - val_loss: 0.8649 - val_acc: 0.5000\n",
            "Epoch 52/300\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.3537 - acc: 0.9333 - val_loss: 0.8704 - val_acc: 0.4500\n",
            "Epoch 53/300\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.3243 - acc: 0.9833 - val_loss: 0.8644 - val_acc: 0.4500\n",
            "Epoch 54/300\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.3638 - acc: 0.9167 - val_loss: 0.8600 - val_acc: 0.4500\n",
            "Epoch 55/300\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.3373 - acc: 0.9500 - val_loss: 0.8614 - val_acc: 0.4500\n",
            "Epoch 56/300\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.3093 - acc: 0.9667 - val_loss: 0.8643 - val_acc: 0.4000\n",
            "Epoch 57/300\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.3314 - acc: 0.9667 - val_loss: 0.8727 - val_acc: 0.4500\n",
            "Epoch 58/300\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.3173 - acc: 0.9667 - val_loss: 0.8816 - val_acc: 0.4500\n",
            "Epoch 59/300\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.3347 - acc: 0.9333 - val_loss: 0.8874 - val_acc: 0.4000\n",
            "Epoch 60/300\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.3510 - acc: 0.9500 - val_loss: 0.8755 - val_acc: 0.4500\n",
            "Epoch 61/300\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.2767 - acc: 1.0000 - val_loss: 0.8757 - val_acc: 0.4500\n",
            "Epoch 62/300\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.2847 - acc: 0.9833 - val_loss: 0.8924 - val_acc: 0.4000\n",
            "Epoch 63/300\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.2736 - acc: 1.0000 - val_loss: 0.9087 - val_acc: 0.4000\n",
            "Epoch 64/300\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.2878 - acc: 0.9333 - val_loss: 0.9173 - val_acc: 0.4000\n",
            "Epoch 65/300\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.2711 - acc: 0.9833 - val_loss: 0.9272 - val_acc: 0.4000\n",
            "Epoch 66/300\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.3094 - acc: 0.9500 - val_loss: 0.9265 - val_acc: 0.4000\n",
            "Epoch 67/300\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.2326 - acc: 1.0000 - val_loss: 0.9114 - val_acc: 0.4000\n",
            "Epoch 68/300\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.2806 - acc: 0.9333 - val_loss: 0.9070 - val_acc: 0.4000\n",
            "Epoch 69/300\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.2771 - acc: 0.9167 - val_loss: 0.8989 - val_acc: 0.4000\n",
            "Epoch 70/300\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.2671 - acc: 1.0000 - val_loss: 0.9045 - val_acc: 0.4000\n",
            "Epoch 71/300\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.2634 - acc: 0.9667 - val_loss: 0.9217 - val_acc: 0.4000\n",
            "Epoch 72/300\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.2695 - acc: 0.9667 - val_loss: 0.9394 - val_acc: 0.4000\n",
            "Epoch 73/300\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.2392 - acc: 0.9667 - val_loss: 0.9632 - val_acc: 0.4500\n",
            "Epoch 74/300\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.2667 - acc: 0.9833 - val_loss: 0.9687 - val_acc: 0.4500\n",
            "Epoch 75/300\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.2798 - acc: 0.9333 - val_loss: 0.9732 - val_acc: 0.4500\n",
            "Epoch 76/300\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.2545 - acc: 0.9500 - val_loss: 0.9829 - val_acc: 0.4500\n",
            "Epoch 77/300\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.2443 - acc: 0.9833 - val_loss: 0.9881 - val_acc: 0.4500\n",
            "Epoch 78/300\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.2374 - acc: 0.9833 - val_loss: 0.9885 - val_acc: 0.4500\n",
            "Epoch 79/300\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.2538 - acc: 0.9833 - val_loss: 0.9826 - val_acc: 0.4500\n",
            "Epoch 80/300\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.2478 - acc: 0.9833 - val_loss: 0.9926 - val_acc: 0.4500\n",
            "Epoch 81/300\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.2982 - acc: 0.9333 - val_loss: 1.0022 - val_acc: 0.4500\n",
            "Epoch 82/300\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.2504 - acc: 0.9667 - val_loss: 0.9964 - val_acc: 0.4500\n",
            "Epoch 83/300\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.2443 - acc: 0.9833 - val_loss: 0.9768 - val_acc: 0.4500\n",
            "Epoch 84/300\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.2459 - acc: 0.9667 - val_loss: 0.9682 - val_acc: 0.4000\n",
            "Epoch 85/300\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.2166 - acc: 1.0000 - val_loss: 0.9688 - val_acc: 0.4000\n",
            "Epoch 86/300\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.2484 - acc: 0.9667 - val_loss: 0.9790 - val_acc: 0.4000\n",
            "Epoch 87/300\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.2510 - acc: 1.0000 - val_loss: 0.9744 - val_acc: 0.4000\n",
            "Epoch 88/300\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.2501 - acc: 0.9833 - val_loss: 0.9800 - val_acc: 0.4000\n",
            "Epoch 89/300\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.2162 - acc: 0.9500 - val_loss: 0.9934 - val_acc: 0.4000\n",
            "Epoch 90/300\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.2523 - acc: 0.9500 - val_loss: 1.0031 - val_acc: 0.4000\n",
            "Epoch 91/300\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.2144 - acc: 0.9833 - val_loss: 1.0399 - val_acc: 0.4000\n",
            "Epoch 92/300\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.2466 - acc: 0.9833 - val_loss: 1.0574 - val_acc: 0.4000\n",
            "Epoch 93/300\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.2167 - acc: 0.9667 - val_loss: 1.0673 - val_acc: 0.4500\n",
            "Epoch 94/300\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.2263 - acc: 0.9667 - val_loss: 1.0618 - val_acc: 0.4500\n",
            "Epoch 95/300\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.2174 - acc: 0.9833 - val_loss: 1.0468 - val_acc: 0.4000\n",
            "Epoch 96/300\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.2300 - acc: 0.9667 - val_loss: 1.0357 - val_acc: 0.4000\n",
            "Epoch 97/300\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.2126 - acc: 0.9833 - val_loss: 1.0473 - val_acc: 0.4000\n",
            "Epoch 98/300\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.2261 - acc: 0.9833 - val_loss: 1.0545 - val_acc: 0.4000\n",
            "Epoch 99/300\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.2192 - acc: 0.9667 - val_loss: 1.0629 - val_acc: 0.4000\n",
            "Epoch 100/300\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.2196 - acc: 0.9667 - val_loss: 1.0507 - val_acc: 0.4000\n",
            "Epoch 101/300\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.2194 - acc: 0.9667 - val_loss: 1.0539 - val_acc: 0.4000\n",
            "Epoch 102/300\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.2391 - acc: 1.0000 - val_loss: 1.0639 - val_acc: 0.4500\n",
            "Epoch 103/300\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.2174 - acc: 0.9500 - val_loss: 1.0652 - val_acc: 0.4500\n",
            "Epoch 104/300\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.2402 - acc: 0.9667 - val_loss: 1.1083 - val_acc: 0.4500\n",
            "Epoch 105/300\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.2247 - acc: 0.9833 - val_loss: 1.1507 - val_acc: 0.4500\n",
            "Epoch 106/300\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.2172 - acc: 0.9500 - val_loss: 1.1735 - val_acc: 0.4500\n",
            "Epoch 107/300\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.1961 - acc: 0.9667 - val_loss: 1.1653 - val_acc: 0.4500\n",
            "Epoch 108/300\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.1936 - acc: 1.0000 - val_loss: 1.1214 - val_acc: 0.4500\n",
            "Epoch 109/300\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.2481 - acc: 0.9667 - val_loss: 1.1092 - val_acc: 0.4500\n",
            "Epoch 110/300\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.1839 - acc: 1.0000 - val_loss: 1.0948 - val_acc: 0.4500\n",
            "Epoch 111/300\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.2187 - acc: 0.9833 - val_loss: 1.0639 - val_acc: 0.4500\n",
            "Epoch 112/300\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.2017 - acc: 1.0000 - val_loss: 1.0777 - val_acc: 0.4500\n",
            "Epoch 113/300\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.1989 - acc: 1.0000 - val_loss: 1.0857 - val_acc: 0.4500\n",
            "Epoch 114/300\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.1789 - acc: 0.9833 - val_loss: 1.1174 - val_acc: 0.4500\n",
            "Epoch 115/300\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.2060 - acc: 0.9667 - val_loss: 1.1258 - val_acc: 0.4500\n",
            "Epoch 116/300\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.1935 - acc: 0.9667 - val_loss: 1.0795 - val_acc: 0.4000\n",
            "Epoch 117/300\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.1910 - acc: 0.9833 - val_loss: 1.0946 - val_acc: 0.4000\n",
            "Epoch 118/300\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.2169 - acc: 0.9833 - val_loss: 1.1058 - val_acc: 0.4000\n",
            "Epoch 119/300\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.2072 - acc: 0.9500 - val_loss: 1.0812 - val_acc: 0.4000\n",
            "Epoch 120/300\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.1921 - acc: 1.0000 - val_loss: 1.1197 - val_acc: 0.4000\n",
            "Epoch 121/300\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.1742 - acc: 1.0000 - val_loss: 1.1715 - val_acc: 0.4500\n",
            "Epoch 122/300\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.1749 - acc: 1.0000 - val_loss: 1.2154 - val_acc: 0.4500\n",
            "Epoch 123/300\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.1943 - acc: 0.9667 - val_loss: 1.2142 - val_acc: 0.4500\n",
            "Epoch 124/300\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.1873 - acc: 0.9833 - val_loss: 1.1952 - val_acc: 0.4500\n",
            "Epoch 125/300\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.1858 - acc: 0.9833 - val_loss: 1.1486 - val_acc: 0.4000\n",
            "Epoch 126/300\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.1935 - acc: 0.9667 - val_loss: 1.1516 - val_acc: 0.4500\n",
            "Epoch 127/300\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.1964 - acc: 0.9833 - val_loss: 1.1437 - val_acc: 0.4500\n",
            "Epoch 128/300\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.1715 - acc: 1.0000 - val_loss: 1.1575 - val_acc: 0.4500\n",
            "Epoch 129/300\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.2145 - acc: 0.9500 - val_loss: 1.1637 - val_acc: 0.4500\n",
            "Epoch 130/300\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.1690 - acc: 1.0000 - val_loss: 1.1478 - val_acc: 0.4500\n",
            "Epoch 131/300\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.1824 - acc: 1.0000 - val_loss: 1.1444 - val_acc: 0.4500\n",
            "Epoch 132/300\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.1994 - acc: 0.9833 - val_loss: 1.1271 - val_acc: 0.4500\n",
            "Epoch 133/300\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.1971 - acc: 0.9833 - val_loss: 1.1580 - val_acc: 0.4500\n",
            "Epoch 134/300\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.1480 - acc: 1.0000 - val_loss: 1.1896 - val_acc: 0.4500\n",
            "Epoch 135/300\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.1872 - acc: 0.9667 - val_loss: 1.1851 - val_acc: 0.4500\n",
            "Epoch 136/300\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.1832 - acc: 0.9833 - val_loss: 1.1682 - val_acc: 0.4500\n",
            "Epoch 137/300\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.1901 - acc: 1.0000 - val_loss: 1.1574 - val_acc: 0.4500\n",
            "Epoch 138/300\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.1946 - acc: 0.9833 - val_loss: 1.1598 - val_acc: 0.4500\n",
            "Epoch 139/300\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.1608 - acc: 1.0000 - val_loss: 1.1874 - val_acc: 0.4500\n",
            "Epoch 140/300\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.1777 - acc: 0.9833 - val_loss: 1.1993 - val_acc: 0.4500\n",
            "Epoch 141/300\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.1488 - acc: 0.9833 - val_loss: 1.1940 - val_acc: 0.4500\n",
            "Epoch 142/300\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.1653 - acc: 1.0000 - val_loss: 1.2117 - val_acc: 0.4500\n",
            "Epoch 143/300\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.1779 - acc: 0.9667 - val_loss: 1.2238 - val_acc: 0.4500\n",
            "Epoch 144/300\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.1823 - acc: 0.9833 - val_loss: 1.1774 - val_acc: 0.4500\n",
            "Epoch 145/300\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.1750 - acc: 0.9833 - val_loss: 1.2309 - val_acc: 0.4500\n",
            "Epoch 146/300\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.1593 - acc: 1.0000 - val_loss: 1.2288 - val_acc: 0.4500\n",
            "Epoch 147/300\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.1781 - acc: 0.9667 - val_loss: 1.2470 - val_acc: 0.4500\n",
            "Epoch 148/300\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.1343 - acc: 1.0000 - val_loss: 1.2214 - val_acc: 0.4500\n",
            "Epoch 149/300\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.1440 - acc: 0.9833 - val_loss: 1.2159 - val_acc: 0.4500\n",
            "Epoch 150/300\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.1663 - acc: 0.9667 - val_loss: 1.2425 - val_acc: 0.4500\n",
            "Epoch 151/300\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.1623 - acc: 0.9833 - val_loss: 1.2193 - val_acc: 0.4500\n",
            "Epoch 152/300\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.1707 - acc: 1.0000 - val_loss: 1.2346 - val_acc: 0.4500\n",
            "Epoch 153/300\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.1591 - acc: 0.9833 - val_loss: 1.2043 - val_acc: 0.4500\n",
            "Epoch 154/300\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.1491 - acc: 0.9833 - val_loss: 1.1639 - val_acc: 0.4500\n",
            "Epoch 155/300\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.1708 - acc: 0.9667 - val_loss: 1.1743 - val_acc: 0.4500\n",
            "Epoch 156/300\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.1382 - acc: 0.9833 - val_loss: 1.2331 - val_acc: 0.4500\n",
            "Epoch 157/300\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.1367 - acc: 1.0000 - val_loss: 1.2874 - val_acc: 0.4500\n",
            "Epoch 158/300\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.1486 - acc: 0.9833 - val_loss: 1.2649 - val_acc: 0.4500\n",
            "Epoch 159/300\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.1553 - acc: 1.0000 - val_loss: 1.2351 - val_acc: 0.4500\n",
            "Epoch 160/300\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.1819 - acc: 0.9833 - val_loss: 1.2757 - val_acc: 0.5000\n",
            "Epoch 161/300\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.1457 - acc: 0.9833 - val_loss: 1.3174 - val_acc: 0.5000\n",
            "Epoch 162/300\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.1908 - acc: 1.0000 - val_loss: 1.2407 - val_acc: 0.4500\n",
            "Epoch 163/300\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.1638 - acc: 1.0000 - val_loss: 1.1803 - val_acc: 0.4500\n",
            "Epoch 164/300\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.1688 - acc: 0.9833 - val_loss: 1.1935 - val_acc: 0.4500\n",
            "Epoch 165/300\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.1517 - acc: 0.9833 - val_loss: 1.1794 - val_acc: 0.4500\n",
            "Epoch 166/300\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.1600 - acc: 0.9500 - val_loss: 1.2126 - val_acc: 0.4500\n",
            "Epoch 167/300\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.1418 - acc: 1.0000 - val_loss: 1.2302 - val_acc: 0.4500\n",
            "Epoch 168/300\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.1971 - acc: 0.9667 - val_loss: 1.2032 - val_acc: 0.4500\n",
            "Epoch 169/300\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.1466 - acc: 1.0000 - val_loss: 1.1884 - val_acc: 0.4500\n",
            "Epoch 170/300\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.1613 - acc: 0.9667 - val_loss: 1.1859 - val_acc: 0.4500\n",
            "Epoch 171/300\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.1343 - acc: 1.0000 - val_loss: 1.1913 - val_acc: 0.4500\n",
            "Epoch 172/300\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.1548 - acc: 1.0000 - val_loss: 1.2434 - val_acc: 0.4500\n",
            "Epoch 173/300\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.1495 - acc: 1.0000 - val_loss: 1.2208 - val_acc: 0.4500\n",
            "Epoch 174/300\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.1504 - acc: 1.0000 - val_loss: 1.2258 - val_acc: 0.4500\n",
            "Epoch 175/300\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.1261 - acc: 1.0000 - val_loss: 1.2642 - val_acc: 0.4500\n",
            "Epoch 176/300\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.1294 - acc: 1.0000 - val_loss: 1.2555 - val_acc: 0.4500\n",
            "Epoch 177/300\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.1351 - acc: 1.0000 - val_loss: 1.2164 - val_acc: 0.4500\n",
            "Epoch 178/300\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.1513 - acc: 1.0000 - val_loss: 1.1491 - val_acc: 0.4500\n",
            "Epoch 179/300\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.1494 - acc: 0.9833 - val_loss: 1.1782 - val_acc: 0.4500\n",
            "Epoch 180/300\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.1220 - acc: 1.0000 - val_loss: 1.2137 - val_acc: 0.4500\n",
            "Epoch 181/300\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.1191 - acc: 1.0000 - val_loss: 1.2732 - val_acc: 0.4500\n",
            "Epoch 182/300\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.1460 - acc: 1.0000 - val_loss: 1.2761 - val_acc: 0.4500\n",
            "Epoch 183/300\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.1515 - acc: 1.0000 - val_loss: 1.3236 - val_acc: 0.4500\n",
            "Epoch 184/300\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.1158 - acc: 1.0000 - val_loss: 1.3403 - val_acc: 0.4500\n",
            "Epoch 185/300\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.1315 - acc: 0.9833 - val_loss: 1.3357 - val_acc: 0.4500\n",
            "Epoch 186/300\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.1344 - acc: 1.0000 - val_loss: 1.2820 - val_acc: 0.5000\n",
            "Epoch 187/300\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.1465 - acc: 0.9833 - val_loss: 1.2840 - val_acc: 0.5000\n",
            "Epoch 188/300\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.1194 - acc: 1.0000 - val_loss: 1.3252 - val_acc: 0.5000\n",
            "Epoch 189/300\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.1380 - acc: 1.0000 - val_loss: 1.3681 - val_acc: 0.5000\n",
            "Epoch 190/300\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.1227 - acc: 1.0000 - val_loss: 1.3429 - val_acc: 0.5000\n",
            "Epoch 191/300\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.1160 - acc: 1.0000 - val_loss: 1.3058 - val_acc: 0.5000\n",
            "Epoch 192/300\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.1216 - acc: 1.0000 - val_loss: 1.3178 - val_acc: 0.5000\n",
            "Epoch 193/300\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.1271 - acc: 0.9667 - val_loss: 1.3178 - val_acc: 0.4500\n",
            "Epoch 194/300\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.1261 - acc: 1.0000 - val_loss: 1.2609 - val_acc: 0.4500\n",
            "Epoch 195/300\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.1467 - acc: 0.9833 - val_loss: 1.1995 - val_acc: 0.4500\n",
            "Epoch 196/300\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.1381 - acc: 1.0000 - val_loss: 1.1876 - val_acc: 0.4500\n",
            "Epoch 197/300\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.1232 - acc: 1.0000 - val_loss: 1.2432 - val_acc: 0.4500\n",
            "Epoch 198/300\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.1304 - acc: 1.0000 - val_loss: 1.2532 - val_acc: 0.4500\n",
            "Epoch 199/300\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.1307 - acc: 0.9833 - val_loss: 1.1868 - val_acc: 0.4500\n",
            "Epoch 200/300\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.1436 - acc: 0.9667 - val_loss: 1.1901 - val_acc: 0.4500\n",
            "Epoch 201/300\n",
            "\n",
            "Epoch 00201: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.1239 - acc: 0.9833 - val_loss: 1.2621 - val_acc: 0.4500\n",
            "Epoch 202/300\n",
            "\n",
            "Epoch 00202: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.1140 - acc: 1.0000 - val_loss: 1.2907 - val_acc: 0.4500\n",
            "Epoch 203/300\n",
            "\n",
            "Epoch 00203: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.1438 - acc: 1.0000 - val_loss: 1.2829 - val_acc: 0.4500\n",
            "Epoch 204/300\n",
            "\n",
            "Epoch 00204: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.1175 - acc: 0.9833 - val_loss: 1.2786 - val_acc: 0.4500\n",
            "Epoch 205/300\n",
            "\n",
            "Epoch 00205: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.1266 - acc: 1.0000 - val_loss: 1.2513 - val_acc: 0.4500\n",
            "Epoch 206/300\n",
            "\n",
            "Epoch 00206: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.1143 - acc: 1.0000 - val_loss: 1.2933 - val_acc: 0.4500\n",
            "Epoch 207/300\n",
            "\n",
            "Epoch 00207: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.1173 - acc: 1.0000 - val_loss: 1.2801 - val_acc: 0.4500\n",
            "Epoch 208/300\n",
            "\n",
            "Epoch 00208: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.1112 - acc: 1.0000 - val_loss: 1.2870 - val_acc: 0.4500\n",
            "Epoch 209/300\n",
            "\n",
            "Epoch 00209: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.1571 - acc: 0.9667 - val_loss: 1.2801 - val_acc: 0.4500\n",
            "Epoch 210/300\n",
            "\n",
            "Epoch 00210: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.1166 - acc: 1.0000 - val_loss: 1.2589 - val_acc: 0.4500\n",
            "Epoch 211/300\n",
            "\n",
            "Epoch 00211: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.1326 - acc: 1.0000 - val_loss: 1.2610 - val_acc: 0.4500\n",
            "Epoch 212/300\n",
            "\n",
            "Epoch 00212: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.1148 - acc: 0.9833 - val_loss: 1.2366 - val_acc: 0.4500\n",
            "Epoch 213/300\n",
            "\n",
            "Epoch 00213: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.1462 - acc: 0.9833 - val_loss: 1.1686 - val_acc: 0.4000\n",
            "Epoch 214/300\n",
            "\n",
            "Epoch 00214: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.1179 - acc: 1.0000 - val_loss: 1.1073 - val_acc: 0.3500\n",
            "Epoch 215/300\n",
            "\n",
            "Epoch 00215: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.1075 - acc: 1.0000 - val_loss: 1.1418 - val_acc: 0.3500\n",
            "Epoch 216/300\n",
            "\n",
            "Epoch 00216: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.1141 - acc: 1.0000 - val_loss: 1.1546 - val_acc: 0.3500\n",
            "Epoch 217/300\n",
            "\n",
            "Epoch 00217: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.1160 - acc: 1.0000 - val_loss: 1.1702 - val_acc: 0.3500\n",
            "Epoch 218/300\n",
            "\n",
            "Epoch 00218: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.1267 - acc: 0.9833 - val_loss: 1.1751 - val_acc: 0.3500\n",
            "Epoch 219/300\n",
            "\n",
            "Epoch 00219: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.1244 - acc: 0.9667 - val_loss: 1.1747 - val_acc: 0.3500\n",
            "Epoch 220/300\n",
            "\n",
            "Epoch 00220: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.1454 - acc: 0.9833 - val_loss: 1.1887 - val_acc: 0.4000\n",
            "Epoch 221/300\n",
            "\n",
            "Epoch 00221: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.0983 - acc: 1.0000 - val_loss: 1.1975 - val_acc: 0.4000\n",
            "Epoch 222/300\n",
            "\n",
            "Epoch 00222: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.1150 - acc: 0.9833 - val_loss: 1.2214 - val_acc: 0.4000\n",
            "Epoch 223/300\n",
            "\n",
            "Epoch 00223: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.0829 - acc: 1.0000 - val_loss: 1.2192 - val_acc: 0.4000\n",
            "Epoch 224/300\n",
            "\n",
            "Epoch 00224: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.0874 - acc: 1.0000 - val_loss: 1.2619 - val_acc: 0.4000\n",
            "Epoch 225/300\n",
            "\n",
            "Epoch 00225: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.1217 - acc: 0.9833 - val_loss: 1.3162 - val_acc: 0.4000\n",
            "Epoch 226/300\n",
            "\n",
            "Epoch 00226: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.1093 - acc: 1.0000 - val_loss: 1.2578 - val_acc: 0.4000\n",
            "Epoch 227/300\n",
            "\n",
            "Epoch 00227: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.1003 - acc: 1.0000 - val_loss: 1.2306 - val_acc: 0.4000\n",
            "Epoch 228/300\n",
            "\n",
            "Epoch 00228: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.1088 - acc: 1.0000 - val_loss: 1.2521 - val_acc: 0.4000\n",
            "Epoch 229/300\n",
            "\n",
            "Epoch 00229: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.1140 - acc: 1.0000 - val_loss: 1.2722 - val_acc: 0.4500\n",
            "Epoch 230/300\n",
            "\n",
            "Epoch 00230: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.0972 - acc: 1.0000 - val_loss: 1.2562 - val_acc: 0.4500\n",
            "Epoch 231/300\n",
            "\n",
            "Epoch 00231: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.1041 - acc: 0.9833 - val_loss: 1.2130 - val_acc: 0.4000\n",
            "Epoch 232/300\n",
            "\n",
            "Epoch 00232: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.0946 - acc: 1.0000 - val_loss: 1.2085 - val_acc: 0.4000\n",
            "Epoch 233/300\n",
            "\n",
            "Epoch 00233: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.1089 - acc: 1.0000 - val_loss: 1.1804 - val_acc: 0.4000\n",
            "Epoch 234/300\n",
            "\n",
            "Epoch 00234: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.0926 - acc: 1.0000 - val_loss: 1.1704 - val_acc: 0.4000\n",
            "Epoch 235/300\n",
            "\n",
            "Epoch 00235: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.0980 - acc: 1.0000 - val_loss: 1.1923 - val_acc: 0.4000\n",
            "Epoch 236/300\n",
            "\n",
            "Epoch 00236: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.1102 - acc: 1.0000 - val_loss: 1.2176 - val_acc: 0.4000\n",
            "Epoch 237/300\n",
            "\n",
            "Epoch 00237: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.0999 - acc: 1.0000 - val_loss: 1.2000 - val_acc: 0.4000\n",
            "Epoch 238/300\n",
            "\n",
            "Epoch 00238: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.1113 - acc: 1.0000 - val_loss: 1.2040 - val_acc: 0.4500\n",
            "Epoch 239/300\n",
            "\n",
            "Epoch 00239: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.1087 - acc: 1.0000 - val_loss: 1.2162 - val_acc: 0.4500\n",
            "Epoch 240/300\n",
            "\n",
            "Epoch 00240: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.1090 - acc: 1.0000 - val_loss: 1.2038 - val_acc: 0.4500\n",
            "Epoch 241/300\n",
            "\n",
            "Epoch 00241: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.1002 - acc: 1.0000 - val_loss: 1.2251 - val_acc: 0.4500\n",
            "Epoch 242/300\n",
            "\n",
            "Epoch 00242: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.0992 - acc: 1.0000 - val_loss: 1.2577 - val_acc: 0.4500\n",
            "Epoch 243/300\n",
            "\n",
            "Epoch 00243: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.1107 - acc: 0.9833 - val_loss: 1.2353 - val_acc: 0.4000\n",
            "Epoch 244/300\n",
            "\n",
            "Epoch 00244: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.1266 - acc: 1.0000 - val_loss: 1.2394 - val_acc: 0.4000\n",
            "Epoch 245/300\n",
            "\n",
            "Epoch 00245: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.0978 - acc: 1.0000 - val_loss: 1.2089 - val_acc: 0.4000\n",
            "Epoch 246/300\n",
            "\n",
            "Epoch 00246: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.0782 - acc: 1.0000 - val_loss: 1.1920 - val_acc: 0.4000\n",
            "Epoch 247/300\n",
            "\n",
            "Epoch 00247: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.1234 - acc: 1.0000 - val_loss: 1.2405 - val_acc: 0.4000\n",
            "Epoch 248/300\n",
            "\n",
            "Epoch 00248: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.1004 - acc: 1.0000 - val_loss: 1.1984 - val_acc: 0.4000\n",
            "Epoch 249/300\n",
            "\n",
            "Epoch 00249: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.0987 - acc: 1.0000 - val_loss: 1.1895 - val_acc: 0.4000\n",
            "Epoch 250/300\n",
            "\n",
            "Epoch 00250: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.1046 - acc: 0.9833 - val_loss: 1.2293 - val_acc: 0.4000\n",
            "Epoch 251/300\n",
            "\n",
            "Epoch 00251: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.0931 - acc: 0.9833 - val_loss: 1.2548 - val_acc: 0.4000\n",
            "Epoch 252/300\n",
            "\n",
            "Epoch 00252: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.0976 - acc: 1.0000 - val_loss: 1.2518 - val_acc: 0.4000\n",
            "Epoch 253/300\n",
            "\n",
            "Epoch 00253: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.0951 - acc: 0.9833 - val_loss: 1.2780 - val_acc: 0.4000\n",
            "Epoch 254/300\n",
            "\n",
            "Epoch 00254: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.1068 - acc: 1.0000 - val_loss: 1.2404 - val_acc: 0.4000\n",
            "Epoch 255/300\n",
            "\n",
            "Epoch 00255: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.1040 - acc: 1.0000 - val_loss: 1.1907 - val_acc: 0.4000\n",
            "Epoch 256/300\n",
            "\n",
            "Epoch 00256: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.1003 - acc: 1.0000 - val_loss: 1.1559 - val_acc: 0.4500\n",
            "Epoch 257/300\n",
            "\n",
            "Epoch 00257: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.0850 - acc: 1.0000 - val_loss: 1.1757 - val_acc: 0.4000\n",
            "Epoch 258/300\n",
            "\n",
            "Epoch 00258: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.1206 - acc: 0.9833 - val_loss: 1.2122 - val_acc: 0.4000\n",
            "Epoch 259/300\n",
            "\n",
            "Epoch 00259: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.0976 - acc: 1.0000 - val_loss: 1.2915 - val_acc: 0.4500\n",
            "Epoch 260/300\n",
            "\n",
            "Epoch 00260: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.0946 - acc: 1.0000 - val_loss: 1.2725 - val_acc: 0.4000\n",
            "Epoch 261/300\n",
            "\n",
            "Epoch 00261: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.1038 - acc: 0.9833 - val_loss: 1.1916 - val_acc: 0.4000\n",
            "Epoch 262/300\n",
            "\n",
            "Epoch 00262: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.0949 - acc: 1.0000 - val_loss: 1.1665 - val_acc: 0.4000\n",
            "Epoch 263/300\n",
            "\n",
            "Epoch 00263: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.0815 - acc: 1.0000 - val_loss: 1.1708 - val_acc: 0.4000\n",
            "Epoch 264/300\n",
            "\n",
            "Epoch 00264: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.1020 - acc: 1.0000 - val_loss: 1.2783 - val_acc: 0.4000\n",
            "Epoch 265/300\n",
            "\n",
            "Epoch 00265: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.1008 - acc: 1.0000 - val_loss: 1.2715 - val_acc: 0.4000\n",
            "Epoch 266/300\n",
            "\n",
            "Epoch 00266: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.1056 - acc: 1.0000 - val_loss: 1.1842 - val_acc: 0.4000\n",
            "Epoch 267/300\n",
            "\n",
            "Epoch 00267: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.0933 - acc: 0.9833 - val_loss: 1.1615 - val_acc: 0.4000\n",
            "Epoch 268/300\n",
            "\n",
            "Epoch 00268: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.1135 - acc: 0.9833 - val_loss: 1.1358 - val_acc: 0.4000\n",
            "Epoch 269/300\n",
            "\n",
            "Epoch 00269: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.1023 - acc: 1.0000 - val_loss: 1.0929 - val_acc: 0.4000\n",
            "Epoch 270/300\n",
            "\n",
            "Epoch 00270: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.1134 - acc: 1.0000 - val_loss: 1.1537 - val_acc: 0.4000\n",
            "Epoch 271/300\n",
            "\n",
            "Epoch 00271: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.0753 - acc: 1.0000 - val_loss: 1.1775 - val_acc: 0.4000\n",
            "Epoch 272/300\n",
            "\n",
            "Epoch 00272: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.0854 - acc: 1.0000 - val_loss: 1.1928 - val_acc: 0.4000\n",
            "Epoch 273/300\n",
            "\n",
            "Epoch 00273: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.0698 - acc: 1.0000 - val_loss: 1.2213 - val_acc: 0.4000\n",
            "Epoch 274/300\n",
            "\n",
            "Epoch 00274: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.1071 - acc: 1.0000 - val_loss: 1.2172 - val_acc: 0.4000\n",
            "Epoch 275/300\n",
            "\n",
            "Epoch 00275: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.0822 - acc: 1.0000 - val_loss: 1.1351 - val_acc: 0.3500\n",
            "Epoch 276/300\n",
            "\n",
            "Epoch 00276: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.1057 - acc: 1.0000 - val_loss: 1.1591 - val_acc: 0.3500\n",
            "Epoch 277/300\n",
            "\n",
            "Epoch 00277: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.1270 - acc: 1.0000 - val_loss: 1.2178 - val_acc: 0.4000\n",
            "Epoch 278/300\n",
            "\n",
            "Epoch 00278: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.0944 - acc: 1.0000 - val_loss: 1.3173 - val_acc: 0.4000\n",
            "Epoch 279/300\n",
            "\n",
            "Epoch 00279: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.0752 - acc: 1.0000 - val_loss: 1.2998 - val_acc: 0.4000\n",
            "Epoch 280/300\n",
            "\n",
            "Epoch 00280: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.0767 - acc: 1.0000 - val_loss: 1.2746 - val_acc: 0.4000\n",
            "Epoch 281/300\n",
            "\n",
            "Epoch 00281: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.0735 - acc: 1.0000 - val_loss: 1.2362 - val_acc: 0.4000\n",
            "Epoch 282/300\n",
            "\n",
            "Epoch 00282: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.0753 - acc: 1.0000 - val_loss: 1.2177 - val_acc: 0.3500\n",
            "Epoch 283/300\n",
            "\n",
            "Epoch 00283: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.0845 - acc: 1.0000 - val_loss: 1.2309 - val_acc: 0.3500\n",
            "Epoch 284/300\n",
            "\n",
            "Epoch 00284: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.0872 - acc: 1.0000 - val_loss: 1.2211 - val_acc: 0.3500\n",
            "Epoch 285/300\n",
            "\n",
            "Epoch 00285: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.0964 - acc: 1.0000 - val_loss: 1.2008 - val_acc: 0.3500\n",
            "Epoch 286/300\n",
            "\n",
            "Epoch 00286: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.0750 - acc: 1.0000 - val_loss: 1.2076 - val_acc: 0.3500\n",
            "Epoch 287/300\n",
            "\n",
            "Epoch 00287: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.0827 - acc: 1.0000 - val_loss: 1.2340 - val_acc: 0.4000\n",
            "Epoch 288/300\n",
            "\n",
            "Epoch 00288: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.0817 - acc: 0.9833 - val_loss: 1.2956 - val_acc: 0.4000\n",
            "Epoch 289/300\n",
            "\n",
            "Epoch 00289: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.0916 - acc: 1.0000 - val_loss: 1.3250 - val_acc: 0.4500\n",
            "Epoch 290/300\n",
            "\n",
            "Epoch 00290: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.0608 - acc: 1.0000 - val_loss: 1.3131 - val_acc: 0.4500\n",
            "Epoch 291/300\n",
            "\n",
            "Epoch 00291: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.0790 - acc: 1.0000 - val_loss: 1.3165 - val_acc: 0.4500\n",
            "Epoch 292/300\n",
            "\n",
            "Epoch 00292: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.0830 - acc: 0.9833 - val_loss: 1.3343 - val_acc: 0.5000\n",
            "Epoch 293/300\n",
            "\n",
            "Epoch 00293: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.0585 - acc: 1.0000 - val_loss: 1.3334 - val_acc: 0.5000\n",
            "Epoch 294/300\n",
            "\n",
            "Epoch 00294: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.0863 - acc: 1.0000 - val_loss: 1.3172 - val_acc: 0.5000\n",
            "Epoch 295/300\n",
            "\n",
            "Epoch 00295: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.0797 - acc: 1.0000 - val_loss: 1.3053 - val_acc: 0.5000\n",
            "Epoch 296/300\n",
            "\n",
            "Epoch 00296: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.0682 - acc: 1.0000 - val_loss: 1.2657 - val_acc: 0.4500\n",
            "Epoch 297/300\n",
            "\n",
            "Epoch 00297: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.0671 - acc: 1.0000 - val_loss: 1.2796 - val_acc: 0.4500\n",
            "Epoch 298/300\n",
            "\n",
            "Epoch 00298: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.0668 - acc: 1.0000 - val_loss: 1.2584 - val_acc: 0.4000\n",
            "Epoch 299/300\n",
            "\n",
            "Epoch 00299: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.0853 - acc: 1.0000 - val_loss: 1.2970 - val_acc: 0.4000\n",
            "Epoch 300/300\n",
            "\n",
            "Epoch 00300: val_loss did not improve from 0.83257\n",
            "60/60 - 0s - loss: 0.0605 - acc: 1.0000 - val_loss: 1.3182 - val_acc: 0.4000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "# Evaluate on test data\n",
            "40/40 [==============================] - 0s 2ms/sample - loss: 0.8570 - acc: 0.5750\n",
            "test loss, test acc: [0.8569754926254973, 0.575]\n",
            "EEG_Deep/Data2A/parsed_P08T.mat\n",
            "Filtering of Training Data Finished\n",
            "EEG_Deep/Data2A/parsed_P08E.mat\n",
            "Filtering of Testing Data Finished\n",
            "(1, 2)\n",
            "Finding labels in training data\n",
            "Finding labels in testing data\n",
            "[2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1]\n",
            "(60, 12, 1536)\n",
            "(60,)\n",
            "(20, 12, 1536)\n",
            "(20,)\n",
            "[[0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]]\n",
            "X_train shape: (60, 1, 12, 1536)\n",
            "60 train samples\n",
            "20 val samples\n",
            "(40, 12, 1536)\n",
            "(40, 1)\n",
            "X_train shape: (40, 1, 12, 1536)\n",
            "40 train samples\n",
            "Train on 60 samples, validate on 20 samples\n",
            "Epoch 1/300\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 1.36267, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 1s - loss: 1.4118 - acc: 0.2500 - val_loss: 1.3627 - val_acc: 0.4500\n",
            "Epoch 2/300\n",
            "\n",
            "Epoch 00002: val_loss improved from 1.36267 to 1.34868, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 1.1039 - acc: 0.5500 - val_loss: 1.3487 - val_acc: 0.5500\n",
            "Epoch 3/300\n",
            "\n",
            "Epoch 00003: val_loss improved from 1.34868 to 1.32825, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.9607 - acc: 0.6500 - val_loss: 1.3283 - val_acc: 0.5500\n",
            "Epoch 4/300\n",
            "\n",
            "Epoch 00004: val_loss improved from 1.32825 to 1.30566, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.8763 - acc: 0.7333 - val_loss: 1.3057 - val_acc: 0.7000\n",
            "Epoch 5/300\n",
            "\n",
            "Epoch 00005: val_loss improved from 1.30566 to 1.28260, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.8141 - acc: 0.8000 - val_loss: 1.2826 - val_acc: 0.6500\n",
            "Epoch 6/300\n",
            "\n",
            "Epoch 00006: val_loss improved from 1.28260 to 1.25912, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.7786 - acc: 0.8167 - val_loss: 1.2591 - val_acc: 0.6000\n",
            "Epoch 7/300\n",
            "\n",
            "Epoch 00007: val_loss improved from 1.25912 to 1.23573, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.7460 - acc: 0.7500 - val_loss: 1.2357 - val_acc: 0.6000\n",
            "Epoch 8/300\n",
            "\n",
            "Epoch 00008: val_loss improved from 1.23573 to 1.21146, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.7195 - acc: 0.8500 - val_loss: 1.2115 - val_acc: 0.6500\n",
            "Epoch 9/300\n",
            "\n",
            "Epoch 00009: val_loss improved from 1.21146 to 1.18843, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6774 - acc: 0.8667 - val_loss: 1.1884 - val_acc: 0.6500\n",
            "Epoch 10/300\n",
            "\n",
            "Epoch 00010: val_loss improved from 1.18843 to 1.16638, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6593 - acc: 0.8667 - val_loss: 1.1664 - val_acc: 0.6500\n",
            "Epoch 11/300\n",
            "\n",
            "Epoch 00011: val_loss improved from 1.16638 to 1.14624, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6385 - acc: 0.8500 - val_loss: 1.1462 - val_acc: 0.6500\n",
            "Epoch 12/300\n",
            "\n",
            "Epoch 00012: val_loss improved from 1.14624 to 1.12723, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6315 - acc: 0.8333 - val_loss: 1.1272 - val_acc: 0.7000\n",
            "Epoch 13/300\n",
            "\n",
            "Epoch 00013: val_loss improved from 1.12723 to 1.10815, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5943 - acc: 0.9333 - val_loss: 1.1082 - val_acc: 0.7000\n",
            "Epoch 14/300\n",
            "\n",
            "Epoch 00014: val_loss improved from 1.10815 to 1.09099, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5702 - acc: 0.9167 - val_loss: 1.0910 - val_acc: 0.6500\n",
            "Epoch 15/300\n",
            "\n",
            "Epoch 00015: val_loss improved from 1.09099 to 1.07530, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5466 - acc: 0.8833 - val_loss: 1.0753 - val_acc: 0.6500\n",
            "Epoch 16/300\n",
            "\n",
            "Epoch 00016: val_loss improved from 1.07530 to 1.05976, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5233 - acc: 0.8833 - val_loss: 1.0598 - val_acc: 0.6500\n",
            "Epoch 17/300\n",
            "\n",
            "Epoch 00017: val_loss improved from 1.05976 to 1.04416, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5310 - acc: 0.8833 - val_loss: 1.0442 - val_acc: 0.7000\n",
            "Epoch 18/300\n",
            "\n",
            "Epoch 00018: val_loss improved from 1.04416 to 1.03159, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5125 - acc: 0.8667 - val_loss: 1.0316 - val_acc: 0.6500\n",
            "Epoch 19/300\n",
            "\n",
            "Epoch 00019: val_loss improved from 1.03159 to 1.01972, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4797 - acc: 0.8833 - val_loss: 1.0197 - val_acc: 0.6500\n",
            "Epoch 20/300\n",
            "\n",
            "Epoch 00020: val_loss improved from 1.01972 to 1.00816, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4854 - acc: 0.9333 - val_loss: 1.0082 - val_acc: 0.6500\n",
            "Epoch 21/300\n",
            "\n",
            "Epoch 00021: val_loss improved from 1.00816 to 0.99579, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4470 - acc: 0.9667 - val_loss: 0.9958 - val_acc: 0.6500\n",
            "Epoch 22/300\n",
            "\n",
            "Epoch 00022: val_loss improved from 0.99579 to 0.98181, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4422 - acc: 0.9667 - val_loss: 0.9818 - val_acc: 0.6500\n",
            "Epoch 23/300\n",
            "\n",
            "Epoch 00023: val_loss improved from 0.98181 to 0.96894, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4470 - acc: 0.9333 - val_loss: 0.9689 - val_acc: 0.6500\n",
            "Epoch 24/300\n",
            "\n",
            "Epoch 00024: val_loss improved from 0.96894 to 0.95466, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4422 - acc: 0.9167 - val_loss: 0.9547 - val_acc: 0.7000\n",
            "Epoch 25/300\n",
            "\n",
            "Epoch 00025: val_loss improved from 0.95466 to 0.94292, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4413 - acc: 0.9500 - val_loss: 0.9429 - val_acc: 0.7000\n",
            "Epoch 26/300\n",
            "\n",
            "Epoch 00026: val_loss improved from 0.94292 to 0.93387, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4162 - acc: 0.9000 - val_loss: 0.9339 - val_acc: 0.7000\n",
            "Epoch 27/300\n",
            "\n",
            "Epoch 00027: val_loss improved from 0.93387 to 0.92325, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4114 - acc: 0.9333 - val_loss: 0.9232 - val_acc: 0.6500\n",
            "Epoch 28/300\n",
            "\n",
            "Epoch 00028: val_loss improved from 0.92325 to 0.91598, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4051 - acc: 0.9500 - val_loss: 0.9160 - val_acc: 0.6000\n",
            "Epoch 29/300\n",
            "\n",
            "Epoch 00029: val_loss improved from 0.91598 to 0.90885, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4020 - acc: 0.9667 - val_loss: 0.9089 - val_acc: 0.6500\n",
            "Epoch 30/300\n",
            "\n",
            "Epoch 00030: val_loss improved from 0.90885 to 0.90389, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3916 - acc: 0.9333 - val_loss: 0.9039 - val_acc: 0.6500\n",
            "Epoch 31/300\n",
            "\n",
            "Epoch 00031: val_loss improved from 0.90389 to 0.89430, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3838 - acc: 0.9333 - val_loss: 0.8943 - val_acc: 0.6500\n",
            "Epoch 32/300\n",
            "\n",
            "Epoch 00032: val_loss improved from 0.89430 to 0.88424, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3759 - acc: 0.9167 - val_loss: 0.8842 - val_acc: 0.6500\n",
            "Epoch 33/300\n",
            "\n",
            "Epoch 00033: val_loss improved from 0.88424 to 0.87327, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3805 - acc: 0.9500 - val_loss: 0.8733 - val_acc: 0.6500\n",
            "Epoch 34/300\n",
            "\n",
            "Epoch 00034: val_loss improved from 0.87327 to 0.86409, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3447 - acc: 0.9500 - val_loss: 0.8641 - val_acc: 0.6000\n",
            "Epoch 35/300\n",
            "\n",
            "Epoch 00035: val_loss improved from 0.86409 to 0.86015, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4005 - acc: 0.9500 - val_loss: 0.8601 - val_acc: 0.6000\n",
            "Epoch 36/300\n",
            "\n",
            "Epoch 00036: val_loss improved from 0.86015 to 0.85703, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3460 - acc: 0.9833 - val_loss: 0.8570 - val_acc: 0.6000\n",
            "Epoch 37/300\n",
            "\n",
            "Epoch 00037: val_loss improved from 0.85703 to 0.85031, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3584 - acc: 0.9333 - val_loss: 0.8503 - val_acc: 0.6000\n",
            "Epoch 38/300\n",
            "\n",
            "Epoch 00038: val_loss improved from 0.85031 to 0.84285, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3387 - acc: 0.9500 - val_loss: 0.8428 - val_acc: 0.6000\n",
            "Epoch 39/300\n",
            "\n",
            "Epoch 00039: val_loss improved from 0.84285 to 0.83962, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3485 - acc: 0.9500 - val_loss: 0.8396 - val_acc: 0.6000\n",
            "Epoch 40/300\n",
            "\n",
            "Epoch 00040: val_loss improved from 0.83962 to 0.83738, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3132 - acc: 0.9500 - val_loss: 0.8374 - val_acc: 0.5500\n",
            "Epoch 41/300\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.83738\n",
            "60/60 - 0s - loss: 0.3479 - acc: 0.9333 - val_loss: 0.8388 - val_acc: 0.5000\n",
            "Epoch 42/300\n",
            "\n",
            "Epoch 00042: val_loss improved from 0.83738 to 0.83496, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3164 - acc: 0.9833 - val_loss: 0.8350 - val_acc: 0.5000\n",
            "Epoch 43/300\n",
            "\n",
            "Epoch 00043: val_loss improved from 0.83496 to 0.83229, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3166 - acc: 0.9500 - val_loss: 0.8323 - val_acc: 0.5000\n",
            "Epoch 44/300\n",
            "\n",
            "Epoch 00044: val_loss improved from 0.83229 to 0.82870, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3299 - acc: 0.9500 - val_loss: 0.8287 - val_acc: 0.5000\n",
            "Epoch 45/300\n",
            "\n",
            "Epoch 00045: val_loss improved from 0.82870 to 0.82520, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3123 - acc: 0.9667 - val_loss: 0.8252 - val_acc: 0.5000\n",
            "Epoch 46/300\n",
            "\n",
            "Epoch 00046: val_loss improved from 0.82520 to 0.81885, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3430 - acc: 0.9500 - val_loss: 0.8189 - val_acc: 0.5500\n",
            "Epoch 47/300\n",
            "\n",
            "Epoch 00047: val_loss improved from 0.81885 to 0.81529, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2991 - acc: 0.9833 - val_loss: 0.8153 - val_acc: 0.5500\n",
            "Epoch 48/300\n",
            "\n",
            "Epoch 00048: val_loss improved from 0.81529 to 0.81429, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3046 - acc: 0.9667 - val_loss: 0.8143 - val_acc: 0.5500\n",
            "Epoch 49/300\n",
            "\n",
            "Epoch 00049: val_loss improved from 0.81429 to 0.81219, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3025 - acc: 0.9833 - val_loss: 0.8122 - val_acc: 0.5500\n",
            "Epoch 50/300\n",
            "\n",
            "Epoch 00050: val_loss improved from 0.81219 to 0.81219, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3035 - acc: 1.0000 - val_loss: 0.8122 - val_acc: 0.5500\n",
            "Epoch 51/300\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.81219\n",
            "60/60 - 0s - loss: 0.3335 - acc: 0.9667 - val_loss: 0.8131 - val_acc: 0.5000\n",
            "Epoch 52/300\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.81219\n",
            "60/60 - 0s - loss: 0.3185 - acc: 0.9833 - val_loss: 0.8129 - val_acc: 0.5000\n",
            "Epoch 53/300\n",
            "\n",
            "Epoch 00053: val_loss improved from 0.81219 to 0.81187, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3208 - acc: 0.9500 - val_loss: 0.8119 - val_acc: 0.5500\n",
            "Epoch 54/300\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.81187\n",
            "60/60 - 0s - loss: 0.3176 - acc: 1.0000 - val_loss: 0.8153 - val_acc: 0.5500\n",
            "Epoch 55/300\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.81187\n",
            "60/60 - 0s - loss: 0.2988 - acc: 0.9667 - val_loss: 0.8167 - val_acc: 0.6000\n",
            "Epoch 56/300\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.81187\n",
            "60/60 - 0s - loss: 0.3060 - acc: 0.9667 - val_loss: 0.8212 - val_acc: 0.4500\n",
            "Epoch 57/300\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.81187\n",
            "60/60 - 0s - loss: 0.2825 - acc: 0.9833 - val_loss: 0.8162 - val_acc: 0.5000\n",
            "Epoch 58/300\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.81187\n",
            "60/60 - 0s - loss: 0.2837 - acc: 0.9667 - val_loss: 0.8125 - val_acc: 0.4500\n",
            "Epoch 59/300\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.81187\n",
            "60/60 - 0s - loss: 0.2913 - acc: 0.9833 - val_loss: 0.8182 - val_acc: 0.4000\n",
            "Epoch 60/300\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.81187\n",
            "60/60 - 0s - loss: 0.2653 - acc: 0.9833 - val_loss: 0.8165 - val_acc: 0.4000\n",
            "Epoch 61/300\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.81187\n",
            "60/60 - 0s - loss: 0.2763 - acc: 0.9833 - val_loss: 0.8142 - val_acc: 0.4500\n",
            "Epoch 62/300\n",
            "\n",
            "Epoch 00062: val_loss improved from 0.81187 to 0.81056, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2873 - acc: 0.9833 - val_loss: 0.8106 - val_acc: 0.4500\n",
            "Epoch 63/300\n",
            "\n",
            "Epoch 00063: val_loss improved from 0.81056 to 0.81029, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2712 - acc: 1.0000 - val_loss: 0.8103 - val_acc: 0.4000\n",
            "Epoch 64/300\n",
            "\n",
            "Epoch 00064: val_loss improved from 0.81029 to 0.80508, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2540 - acc: 1.0000 - val_loss: 0.8051 - val_acc: 0.4000\n",
            "Epoch 65/300\n",
            "\n",
            "Epoch 00065: val_loss improved from 0.80508 to 0.79691, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2722 - acc: 0.9667 - val_loss: 0.7969 - val_acc: 0.4000\n",
            "Epoch 66/300\n",
            "\n",
            "Epoch 00066: val_loss improved from 0.79691 to 0.79197, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2489 - acc: 0.9667 - val_loss: 0.7920 - val_acc: 0.4000\n",
            "Epoch 67/300\n",
            "\n",
            "Epoch 00067: val_loss improved from 0.79197 to 0.78486, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2879 - acc: 0.9667 - val_loss: 0.7849 - val_acc: 0.4000\n",
            "Epoch 68/300\n",
            "\n",
            "Epoch 00068: val_loss improved from 0.78486 to 0.77410, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2625 - acc: 0.9833 - val_loss: 0.7741 - val_acc: 0.5500\n",
            "Epoch 69/300\n",
            "\n",
            "Epoch 00069: val_loss improved from 0.77410 to 0.77362, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2533 - acc: 1.0000 - val_loss: 0.7736 - val_acc: 0.5000\n",
            "Epoch 70/300\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.77362\n",
            "60/60 - 0s - loss: 0.2616 - acc: 1.0000 - val_loss: 0.7767 - val_acc: 0.5000\n",
            "Epoch 71/300\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.77362\n",
            "60/60 - 0s - loss: 0.2425 - acc: 1.0000 - val_loss: 0.7800 - val_acc: 0.5500\n",
            "Epoch 72/300\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.77362\n",
            "60/60 - 0s - loss: 0.2413 - acc: 0.9833 - val_loss: 0.7809 - val_acc: 0.4000\n",
            "Epoch 73/300\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.77362\n",
            "60/60 - 0s - loss: 0.2407 - acc: 0.9833 - val_loss: 0.7786 - val_acc: 0.4000\n",
            "Epoch 74/300\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.77362\n",
            "60/60 - 0s - loss: 0.2310 - acc: 0.9833 - val_loss: 0.7767 - val_acc: 0.4500\n",
            "Epoch 75/300\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.77362\n",
            "60/60 - 0s - loss: 0.2545 - acc: 1.0000 - val_loss: 0.7783 - val_acc: 0.4500\n",
            "Epoch 76/300\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.77362\n",
            "60/60 - 0s - loss: 0.2343 - acc: 0.9833 - val_loss: 0.7799 - val_acc: 0.4500\n",
            "Epoch 77/300\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.77362\n",
            "60/60 - 0s - loss: 0.2404 - acc: 1.0000 - val_loss: 0.7865 - val_acc: 0.5000\n",
            "Epoch 78/300\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.77362\n",
            "60/60 - 0s - loss: 0.2283 - acc: 1.0000 - val_loss: 0.7893 - val_acc: 0.5000\n",
            "Epoch 79/300\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.77362\n",
            "60/60 - 0s - loss: 0.2494 - acc: 0.9833 - val_loss: 0.7932 - val_acc: 0.5000\n",
            "Epoch 80/300\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.77362\n",
            "60/60 - 0s - loss: 0.2416 - acc: 1.0000 - val_loss: 0.7962 - val_acc: 0.5000\n",
            "Epoch 81/300\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.77362\n",
            "60/60 - 0s - loss: 0.2248 - acc: 1.0000 - val_loss: 0.7989 - val_acc: 0.5000\n",
            "Epoch 82/300\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.77362\n",
            "60/60 - 0s - loss: 0.2388 - acc: 1.0000 - val_loss: 0.7961 - val_acc: 0.5000\n",
            "Epoch 83/300\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.77362\n",
            "60/60 - 0s - loss: 0.2157 - acc: 1.0000 - val_loss: 0.7844 - val_acc: 0.5000\n",
            "Epoch 84/300\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.77362\n",
            "60/60 - 0s - loss: 0.2198 - acc: 0.9833 - val_loss: 0.7778 - val_acc: 0.6000\n",
            "Epoch 85/300\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.77362\n",
            "60/60 - 0s - loss: 0.2212 - acc: 0.9833 - val_loss: 0.7757 - val_acc: 0.6000\n",
            "Epoch 86/300\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.77362\n",
            "60/60 - 0s - loss: 0.2141 - acc: 0.9833 - val_loss: 0.7768 - val_acc: 0.5500\n",
            "Epoch 87/300\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.77362\n",
            "60/60 - 0s - loss: 0.1929 - acc: 1.0000 - val_loss: 0.7820 - val_acc: 0.5500\n",
            "Epoch 88/300\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.77362\n",
            "60/60 - 0s - loss: 0.1920 - acc: 1.0000 - val_loss: 0.7831 - val_acc: 0.5500\n",
            "Epoch 89/300\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.77362\n",
            "60/60 - 0s - loss: 0.2093 - acc: 0.9833 - val_loss: 0.7809 - val_acc: 0.5500\n",
            "Epoch 90/300\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.77362\n",
            "60/60 - 0s - loss: 0.2251 - acc: 1.0000 - val_loss: 0.7828 - val_acc: 0.5500\n",
            "Epoch 91/300\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.77362\n",
            "60/60 - 0s - loss: 0.2272 - acc: 0.9833 - val_loss: 0.7793 - val_acc: 0.5500\n",
            "Epoch 92/300\n",
            "\n",
            "Epoch 00092: val_loss improved from 0.77362 to 0.77358, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2044 - acc: 1.0000 - val_loss: 0.7736 - val_acc: 0.5500\n",
            "Epoch 93/300\n",
            "\n",
            "Epoch 00093: val_loss improved from 0.77358 to 0.76963, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1989 - acc: 0.9833 - val_loss: 0.7696 - val_acc: 0.5000\n",
            "Epoch 94/300\n",
            "\n",
            "Epoch 00094: val_loss improved from 0.76963 to 0.76632, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1883 - acc: 1.0000 - val_loss: 0.7663 - val_acc: 0.5000\n",
            "Epoch 95/300\n",
            "\n",
            "Epoch 00095: val_loss improved from 0.76632 to 0.76283, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1926 - acc: 1.0000 - val_loss: 0.7628 - val_acc: 0.5500\n",
            "Epoch 96/300\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.76283\n",
            "60/60 - 0s - loss: 0.1902 - acc: 1.0000 - val_loss: 0.7635 - val_acc: 0.5500\n",
            "Epoch 97/300\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.76283\n",
            "60/60 - 0s - loss: 0.1937 - acc: 0.9833 - val_loss: 0.7644 - val_acc: 0.6000\n",
            "Epoch 98/300\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.76283\n",
            "60/60 - 0s - loss: 0.1929 - acc: 1.0000 - val_loss: 0.7692 - val_acc: 0.6500\n",
            "Epoch 99/300\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.76283\n",
            "60/60 - 0s - loss: 0.1994 - acc: 1.0000 - val_loss: 0.7760 - val_acc: 0.6500\n",
            "Epoch 100/300\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.76283\n",
            "60/60 - 0s - loss: 0.1907 - acc: 1.0000 - val_loss: 0.7865 - val_acc: 0.6000\n",
            "Epoch 101/300\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.76283\n",
            "60/60 - 0s - loss: 0.2030 - acc: 0.9833 - val_loss: 0.7911 - val_acc: 0.6000\n",
            "Epoch 102/300\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.76283\n",
            "60/60 - 0s - loss: 0.2016 - acc: 1.0000 - val_loss: 0.7955 - val_acc: 0.6000\n",
            "Epoch 103/300\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.76283\n",
            "60/60 - 0s - loss: 0.1741 - acc: 1.0000 - val_loss: 0.7997 - val_acc: 0.6000\n",
            "Epoch 104/300\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.76283\n",
            "60/60 - 0s - loss: 0.2051 - acc: 1.0000 - val_loss: 0.7977 - val_acc: 0.6500\n",
            "Epoch 105/300\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 0.76283\n",
            "60/60 - 0s - loss: 0.1998 - acc: 0.9833 - val_loss: 0.7937 - val_acc: 0.6500\n",
            "Epoch 106/300\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 0.76283\n",
            "60/60 - 0s - loss: 0.1731 - acc: 1.0000 - val_loss: 0.7905 - val_acc: 0.6500\n",
            "Epoch 107/300\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.76283\n",
            "60/60 - 0s - loss: 0.1699 - acc: 1.0000 - val_loss: 0.7812 - val_acc: 0.6500\n",
            "Epoch 108/300\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 0.76283\n",
            "60/60 - 0s - loss: 0.1941 - acc: 1.0000 - val_loss: 0.7747 - val_acc: 0.6500\n",
            "Epoch 109/300\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.76283\n",
            "60/60 - 0s - loss: 0.1960 - acc: 1.0000 - val_loss: 0.7763 - val_acc: 0.6500\n",
            "Epoch 110/300\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.76283\n",
            "60/60 - 0s - loss: 0.1954 - acc: 1.0000 - val_loss: 0.7719 - val_acc: 0.6000\n",
            "Epoch 111/300\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 0.76283\n",
            "60/60 - 0s - loss: 0.1592 - acc: 1.0000 - val_loss: 0.7670 - val_acc: 0.6000\n",
            "Epoch 112/300\n",
            "\n",
            "Epoch 00112: val_loss improved from 0.76283 to 0.76076, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1523 - acc: 1.0000 - val_loss: 0.7608 - val_acc: 0.6000\n",
            "Epoch 113/300\n",
            "\n",
            "Epoch 00113: val_loss improved from 0.76076 to 0.75615, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1672 - acc: 1.0000 - val_loss: 0.7561 - val_acc: 0.6000\n",
            "Epoch 114/300\n",
            "\n",
            "Epoch 00114: val_loss improved from 0.75615 to 0.75204, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1618 - acc: 1.0000 - val_loss: 0.7520 - val_acc: 0.6500\n",
            "Epoch 115/300\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.75204\n",
            "60/60 - 0s - loss: 0.1666 - acc: 1.0000 - val_loss: 0.7521 - val_acc: 0.6500\n",
            "Epoch 116/300\n",
            "\n",
            "Epoch 00116: val_loss improved from 0.75204 to 0.74986, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1540 - acc: 1.0000 - val_loss: 0.7499 - val_acc: 0.6500\n",
            "Epoch 117/300\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.1701 - acc: 0.9833 - val_loss: 0.7511 - val_acc: 0.6500\n",
            "Epoch 118/300\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.1623 - acc: 1.0000 - val_loss: 0.7536 - val_acc: 0.6500\n",
            "Epoch 119/300\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.1749 - acc: 1.0000 - val_loss: 0.7613 - val_acc: 0.6500\n",
            "Epoch 120/300\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.1495 - acc: 1.0000 - val_loss: 0.7685 - val_acc: 0.6500\n",
            "Epoch 121/300\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.1584 - acc: 0.9833 - val_loss: 0.7732 - val_acc: 0.6500\n",
            "Epoch 122/300\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.1480 - acc: 1.0000 - val_loss: 0.7649 - val_acc: 0.6500\n",
            "Epoch 123/300\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.1684 - acc: 0.9833 - val_loss: 0.7641 - val_acc: 0.6500\n",
            "Epoch 124/300\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.1541 - acc: 1.0000 - val_loss: 0.7678 - val_acc: 0.6500\n",
            "Epoch 125/300\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.1444 - acc: 1.0000 - val_loss: 0.7677 - val_acc: 0.7000\n",
            "Epoch 126/300\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.1405 - acc: 1.0000 - val_loss: 0.7686 - val_acc: 0.7000\n",
            "Epoch 127/300\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.1582 - acc: 1.0000 - val_loss: 0.7786 - val_acc: 0.6500\n",
            "Epoch 128/300\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.1411 - acc: 1.0000 - val_loss: 0.7843 - val_acc: 0.6500\n",
            "Epoch 129/300\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.1565 - acc: 1.0000 - val_loss: 0.7941 - val_acc: 0.6500\n",
            "Epoch 130/300\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.1500 - acc: 1.0000 - val_loss: 0.7971 - val_acc: 0.6500\n",
            "Epoch 131/300\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.1513 - acc: 1.0000 - val_loss: 0.8006 - val_acc: 0.6500\n",
            "Epoch 132/300\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.1559 - acc: 1.0000 - val_loss: 0.8041 - val_acc: 0.6500\n",
            "Epoch 133/300\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.1383 - acc: 1.0000 - val_loss: 0.8018 - val_acc: 0.6500\n",
            "Epoch 134/300\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.1466 - acc: 1.0000 - val_loss: 0.8002 - val_acc: 0.6500\n",
            "Epoch 135/300\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.1530 - acc: 1.0000 - val_loss: 0.8036 - val_acc: 0.6000\n",
            "Epoch 136/300\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.1520 - acc: 1.0000 - val_loss: 0.8015 - val_acc: 0.6000\n",
            "Epoch 137/300\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.1724 - acc: 1.0000 - val_loss: 0.7979 - val_acc: 0.6500\n",
            "Epoch 138/300\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.1447 - acc: 1.0000 - val_loss: 0.7935 - val_acc: 0.6500\n",
            "Epoch 139/300\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.1592 - acc: 1.0000 - val_loss: 0.7941 - val_acc: 0.6500\n",
            "Epoch 140/300\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.1429 - acc: 1.0000 - val_loss: 0.8043 - val_acc: 0.6500\n",
            "Epoch 141/300\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.1496 - acc: 0.9833 - val_loss: 0.8139 - val_acc: 0.6500\n",
            "Epoch 142/300\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.1349 - acc: 1.0000 - val_loss: 0.8174 - val_acc: 0.6500\n",
            "Epoch 143/300\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.1641 - acc: 0.9833 - val_loss: 0.8175 - val_acc: 0.6500\n",
            "Epoch 144/300\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.1496 - acc: 1.0000 - val_loss: 0.8178 - val_acc: 0.6500\n",
            "Epoch 145/300\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.1487 - acc: 1.0000 - val_loss: 0.8128 - val_acc: 0.6500\n",
            "Epoch 146/300\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.1486 - acc: 1.0000 - val_loss: 0.8146 - val_acc: 0.6000\n",
            "Epoch 147/300\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.1420 - acc: 1.0000 - val_loss: 0.8238 - val_acc: 0.6000\n",
            "Epoch 148/300\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.1321 - acc: 1.0000 - val_loss: 0.8302 - val_acc: 0.6500\n",
            "Epoch 149/300\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.1350 - acc: 1.0000 - val_loss: 0.8293 - val_acc: 0.6500\n",
            "Epoch 150/300\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.1352 - acc: 1.0000 - val_loss: 0.8355 - val_acc: 0.6500\n",
            "Epoch 151/300\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.1350 - acc: 1.0000 - val_loss: 0.8364 - val_acc: 0.6500\n",
            "Epoch 152/300\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.1621 - acc: 1.0000 - val_loss: 0.8432 - val_acc: 0.6500\n",
            "Epoch 153/300\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.1299 - acc: 1.0000 - val_loss: 0.8496 - val_acc: 0.6500\n",
            "Epoch 154/300\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.1376 - acc: 1.0000 - val_loss: 0.8454 - val_acc: 0.6500\n",
            "Epoch 155/300\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.1272 - acc: 1.0000 - val_loss: 0.8304 - val_acc: 0.6500\n",
            "Epoch 156/300\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.1227 - acc: 1.0000 - val_loss: 0.8228 - val_acc: 0.6500\n",
            "Epoch 157/300\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.1321 - acc: 1.0000 - val_loss: 0.8228 - val_acc: 0.6500\n",
            "Epoch 158/300\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.1348 - acc: 1.0000 - val_loss: 0.8289 - val_acc: 0.6500\n",
            "Epoch 159/300\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.1279 - acc: 1.0000 - val_loss: 0.8340 - val_acc: 0.6500\n",
            "Epoch 160/300\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.1206 - acc: 1.0000 - val_loss: 0.8332 - val_acc: 0.6500\n",
            "Epoch 161/300\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.1251 - acc: 1.0000 - val_loss: 0.8342 - val_acc: 0.6500\n",
            "Epoch 162/300\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.1319 - acc: 1.0000 - val_loss: 0.8435 - val_acc: 0.6000\n",
            "Epoch 163/300\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.1296 - acc: 1.0000 - val_loss: 0.8510 - val_acc: 0.6000\n",
            "Epoch 164/300\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.1425 - acc: 1.0000 - val_loss: 0.8483 - val_acc: 0.6000\n",
            "Epoch 165/300\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.1270 - acc: 1.0000 - val_loss: 0.8342 - val_acc: 0.6500\n",
            "Epoch 166/300\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.1227 - acc: 1.0000 - val_loss: 0.8328 - val_acc: 0.6500\n",
            "Epoch 167/300\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.1337 - acc: 1.0000 - val_loss: 0.8346 - val_acc: 0.6500\n",
            "Epoch 168/300\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.1298 - acc: 0.9833 - val_loss: 0.8392 - val_acc: 0.6500\n",
            "Epoch 169/300\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.1079 - acc: 1.0000 - val_loss: 0.8381 - val_acc: 0.6500\n",
            "Epoch 170/300\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.1286 - acc: 1.0000 - val_loss: 0.8449 - val_acc: 0.6500\n",
            "Epoch 171/300\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.1407 - acc: 0.9833 - val_loss: 0.8518 - val_acc: 0.6500\n",
            "Epoch 172/300\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.1446 - acc: 1.0000 - val_loss: 0.8622 - val_acc: 0.6500\n",
            "Epoch 173/300\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.1436 - acc: 1.0000 - val_loss: 0.8673 - val_acc: 0.6500\n",
            "Epoch 174/300\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.1306 - acc: 1.0000 - val_loss: 0.8729 - val_acc: 0.6500\n",
            "Epoch 175/300\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.1164 - acc: 1.0000 - val_loss: 0.8713 - val_acc: 0.6500\n",
            "Epoch 176/300\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.1478 - acc: 1.0000 - val_loss: 0.8679 - val_acc: 0.6500\n",
            "Epoch 177/300\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.1299 - acc: 1.0000 - val_loss: 0.8754 - val_acc: 0.6500\n",
            "Epoch 178/300\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.1155 - acc: 1.0000 - val_loss: 0.8799 - val_acc: 0.6000\n",
            "Epoch 179/300\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.1182 - acc: 1.0000 - val_loss: 0.8761 - val_acc: 0.6000\n",
            "Epoch 180/300\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.1117 - acc: 1.0000 - val_loss: 0.8736 - val_acc: 0.6000\n",
            "Epoch 181/300\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.1278 - acc: 1.0000 - val_loss: 0.8675 - val_acc: 0.6500\n",
            "Epoch 182/300\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.1184 - acc: 1.0000 - val_loss: 0.8677 - val_acc: 0.6500\n",
            "Epoch 183/300\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.1150 - acc: 1.0000 - val_loss: 0.8665 - val_acc: 0.6500\n",
            "Epoch 184/300\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.1210 - acc: 0.9833 - val_loss: 0.8680 - val_acc: 0.6500\n",
            "Epoch 185/300\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.1092 - acc: 1.0000 - val_loss: 0.8736 - val_acc: 0.6500\n",
            "Epoch 186/300\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.1228 - acc: 1.0000 - val_loss: 0.8743 - val_acc: 0.6000\n",
            "Epoch 187/300\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.1138 - acc: 1.0000 - val_loss: 0.8701 - val_acc: 0.6000\n",
            "Epoch 188/300\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.1344 - acc: 1.0000 - val_loss: 0.8713 - val_acc: 0.6500\n",
            "Epoch 189/300\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.1022 - acc: 1.0000 - val_loss: 0.8746 - val_acc: 0.6500\n",
            "Epoch 190/300\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.1105 - acc: 1.0000 - val_loss: 0.8850 - val_acc: 0.6500\n",
            "Epoch 191/300\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.1074 - acc: 1.0000 - val_loss: 0.8915 - val_acc: 0.6500\n",
            "Epoch 192/300\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.1177 - acc: 1.0000 - val_loss: 0.9000 - val_acc: 0.6500\n",
            "Epoch 193/300\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.1187 - acc: 1.0000 - val_loss: 0.9093 - val_acc: 0.6500\n",
            "Epoch 194/300\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.1135 - acc: 1.0000 - val_loss: 0.9031 - val_acc: 0.6000\n",
            "Epoch 195/300\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.1145 - acc: 1.0000 - val_loss: 0.8942 - val_acc: 0.6500\n",
            "Epoch 196/300\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.1007 - acc: 1.0000 - val_loss: 0.8861 - val_acc: 0.6500\n",
            "Epoch 197/300\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.1212 - acc: 1.0000 - val_loss: 0.8794 - val_acc: 0.6500\n",
            "Epoch 198/300\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.1036 - acc: 1.0000 - val_loss: 0.8829 - val_acc: 0.6500\n",
            "Epoch 199/300\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.1098 - acc: 1.0000 - val_loss: 0.8928 - val_acc: 0.6000\n",
            "Epoch 200/300\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.1072 - acc: 1.0000 - val_loss: 0.9015 - val_acc: 0.6000\n",
            "Epoch 201/300\n",
            "\n",
            "Epoch 00201: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.1035 - acc: 1.0000 - val_loss: 0.8763 - val_acc: 0.6000\n",
            "Epoch 202/300\n",
            "\n",
            "Epoch 00202: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.1145 - acc: 1.0000 - val_loss: 0.8630 - val_acc: 0.6000\n",
            "Epoch 203/300\n",
            "\n",
            "Epoch 00203: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.1263 - acc: 1.0000 - val_loss: 0.8621 - val_acc: 0.6000\n",
            "Epoch 204/300\n",
            "\n",
            "Epoch 00204: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.1027 - acc: 1.0000 - val_loss: 0.8623 - val_acc: 0.6000\n",
            "Epoch 205/300\n",
            "\n",
            "Epoch 00205: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.1123 - acc: 1.0000 - val_loss: 0.8618 - val_acc: 0.6500\n",
            "Epoch 206/300\n",
            "\n",
            "Epoch 00206: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.0883 - acc: 1.0000 - val_loss: 0.8660 - val_acc: 0.6500\n",
            "Epoch 207/300\n",
            "\n",
            "Epoch 00207: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.1092 - acc: 1.0000 - val_loss: 0.8704 - val_acc: 0.6500\n",
            "Epoch 208/300\n",
            "\n",
            "Epoch 00208: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.0969 - acc: 1.0000 - val_loss: 0.8717 - val_acc: 0.6500\n",
            "Epoch 209/300\n",
            "\n",
            "Epoch 00209: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.1089 - acc: 1.0000 - val_loss: 0.8795 - val_acc: 0.6000\n",
            "Epoch 210/300\n",
            "\n",
            "Epoch 00210: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.1155 - acc: 1.0000 - val_loss: 0.8903 - val_acc: 0.6000\n",
            "Epoch 211/300\n",
            "\n",
            "Epoch 00211: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.1061 - acc: 1.0000 - val_loss: 0.8944 - val_acc: 0.6500\n",
            "Epoch 212/300\n",
            "\n",
            "Epoch 00212: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.0981 - acc: 1.0000 - val_loss: 0.8994 - val_acc: 0.6500\n",
            "Epoch 213/300\n",
            "\n",
            "Epoch 00213: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.1020 - acc: 1.0000 - val_loss: 0.9063 - val_acc: 0.6500\n",
            "Epoch 214/300\n",
            "\n",
            "Epoch 00214: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.1091 - acc: 1.0000 - val_loss: 0.9095 - val_acc: 0.6500\n",
            "Epoch 215/300\n",
            "\n",
            "Epoch 00215: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.1072 - acc: 1.0000 - val_loss: 0.9242 - val_acc: 0.6500\n",
            "Epoch 216/300\n",
            "\n",
            "Epoch 00216: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.1014 - acc: 1.0000 - val_loss: 0.9286 - val_acc: 0.6500\n",
            "Epoch 217/300\n",
            "\n",
            "Epoch 00217: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.0873 - acc: 1.0000 - val_loss: 0.9262 - val_acc: 0.6500\n",
            "Epoch 218/300\n",
            "\n",
            "Epoch 00218: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.1068 - acc: 1.0000 - val_loss: 0.9291 - val_acc: 0.6500\n",
            "Epoch 219/300\n",
            "\n",
            "Epoch 00219: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.1054 - acc: 1.0000 - val_loss: 0.9303 - val_acc: 0.6000\n",
            "Epoch 220/300\n",
            "\n",
            "Epoch 00220: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.1080 - acc: 1.0000 - val_loss: 0.9122 - val_acc: 0.6000\n",
            "Epoch 221/300\n",
            "\n",
            "Epoch 00221: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.1029 - acc: 1.0000 - val_loss: 0.8921 - val_acc: 0.6000\n",
            "Epoch 222/300\n",
            "\n",
            "Epoch 00222: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.1116 - acc: 1.0000 - val_loss: 0.8740 - val_acc: 0.6000\n",
            "Epoch 223/300\n",
            "\n",
            "Epoch 00223: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.1029 - acc: 1.0000 - val_loss: 0.8715 - val_acc: 0.6000\n",
            "Epoch 224/300\n",
            "\n",
            "Epoch 00224: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.1055 - acc: 1.0000 - val_loss: 0.8716 - val_acc: 0.6000\n",
            "Epoch 225/300\n",
            "\n",
            "Epoch 00225: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.1038 - acc: 1.0000 - val_loss: 0.8596 - val_acc: 0.6000\n",
            "Epoch 226/300\n",
            "\n",
            "Epoch 00226: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.0927 - acc: 1.0000 - val_loss: 0.8609 - val_acc: 0.6500\n",
            "Epoch 227/300\n",
            "\n",
            "Epoch 00227: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.0941 - acc: 1.0000 - val_loss: 0.8700 - val_acc: 0.6500\n",
            "Epoch 228/300\n",
            "\n",
            "Epoch 00228: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.1127 - acc: 1.0000 - val_loss: 0.8899 - val_acc: 0.6000\n",
            "Epoch 229/300\n",
            "\n",
            "Epoch 00229: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.1141 - acc: 1.0000 - val_loss: 0.8950 - val_acc: 0.6000\n",
            "Epoch 230/300\n",
            "\n",
            "Epoch 00230: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.1072 - acc: 1.0000 - val_loss: 0.8915 - val_acc: 0.6000\n",
            "Epoch 231/300\n",
            "\n",
            "Epoch 00231: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.1148 - acc: 1.0000 - val_loss: 0.8820 - val_acc: 0.6000\n",
            "Epoch 232/300\n",
            "\n",
            "Epoch 00232: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.1009 - acc: 1.0000 - val_loss: 0.8883 - val_acc: 0.6000\n",
            "Epoch 233/300\n",
            "\n",
            "Epoch 00233: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.1099 - acc: 1.0000 - val_loss: 0.9099 - val_acc: 0.5500\n",
            "Epoch 234/300\n",
            "\n",
            "Epoch 00234: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.0898 - acc: 1.0000 - val_loss: 0.9149 - val_acc: 0.5500\n",
            "Epoch 235/300\n",
            "\n",
            "Epoch 00235: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.1063 - acc: 1.0000 - val_loss: 0.8957 - val_acc: 0.6000\n",
            "Epoch 236/300\n",
            "\n",
            "Epoch 00236: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.0892 - acc: 1.0000 - val_loss: 0.8759 - val_acc: 0.6500\n",
            "Epoch 237/300\n",
            "\n",
            "Epoch 00237: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.1032 - acc: 0.9833 - val_loss: 0.8689 - val_acc: 0.6500\n",
            "Epoch 238/300\n",
            "\n",
            "Epoch 00238: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.1149 - acc: 0.9833 - val_loss: 0.8767 - val_acc: 0.6000\n",
            "Epoch 239/300\n",
            "\n",
            "Epoch 00239: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.0946 - acc: 1.0000 - val_loss: 0.8815 - val_acc: 0.6500\n",
            "Epoch 240/300\n",
            "\n",
            "Epoch 00240: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.0960 - acc: 1.0000 - val_loss: 0.8898 - val_acc: 0.6000\n",
            "Epoch 241/300\n",
            "\n",
            "Epoch 00241: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.1127 - acc: 0.9833 - val_loss: 0.8998 - val_acc: 0.6000\n",
            "Epoch 242/300\n",
            "\n",
            "Epoch 00242: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.0980 - acc: 1.0000 - val_loss: 0.9151 - val_acc: 0.6000\n",
            "Epoch 243/300\n",
            "\n",
            "Epoch 00243: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.0826 - acc: 1.0000 - val_loss: 0.9162 - val_acc: 0.6000\n",
            "Epoch 244/300\n",
            "\n",
            "Epoch 00244: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.1021 - acc: 1.0000 - val_loss: 0.8886 - val_acc: 0.6500\n",
            "Epoch 245/300\n",
            "\n",
            "Epoch 00245: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.1217 - acc: 1.0000 - val_loss: 0.8815 - val_acc: 0.6500\n",
            "Epoch 246/300\n",
            "\n",
            "Epoch 00246: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.0963 - acc: 1.0000 - val_loss: 0.8821 - val_acc: 0.7000\n",
            "Epoch 247/300\n",
            "\n",
            "Epoch 00247: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.0848 - acc: 1.0000 - val_loss: 0.8866 - val_acc: 0.6500\n",
            "Epoch 248/300\n",
            "\n",
            "Epoch 00248: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.0943 - acc: 1.0000 - val_loss: 0.8876 - val_acc: 0.6500\n",
            "Epoch 249/300\n",
            "\n",
            "Epoch 00249: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.1232 - acc: 0.9833 - val_loss: 0.8914 - val_acc: 0.6500\n",
            "Epoch 250/300\n",
            "\n",
            "Epoch 00250: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.1035 - acc: 1.0000 - val_loss: 0.9218 - val_acc: 0.6000\n",
            "Epoch 251/300\n",
            "\n",
            "Epoch 00251: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.0941 - acc: 1.0000 - val_loss: 0.9089 - val_acc: 0.6000\n",
            "Epoch 252/300\n",
            "\n",
            "Epoch 00252: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.1022 - acc: 1.0000 - val_loss: 0.8731 - val_acc: 0.6000\n",
            "Epoch 253/300\n",
            "\n",
            "Epoch 00253: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.0935 - acc: 1.0000 - val_loss: 0.8637 - val_acc: 0.6000\n",
            "Epoch 254/300\n",
            "\n",
            "Epoch 00254: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.0943 - acc: 1.0000 - val_loss: 0.8687 - val_acc: 0.6500\n",
            "Epoch 255/300\n",
            "\n",
            "Epoch 00255: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.1001 - acc: 1.0000 - val_loss: 0.8758 - val_acc: 0.6000\n",
            "Epoch 256/300\n",
            "\n",
            "Epoch 00256: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.1013 - acc: 1.0000 - val_loss: 0.8973 - val_acc: 0.6000\n",
            "Epoch 257/300\n",
            "\n",
            "Epoch 00257: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.0977 - acc: 1.0000 - val_loss: 0.9004 - val_acc: 0.6000\n",
            "Epoch 258/300\n",
            "\n",
            "Epoch 00258: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.1226 - acc: 0.9833 - val_loss: 0.8927 - val_acc: 0.6000\n",
            "Epoch 259/300\n",
            "\n",
            "Epoch 00259: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.0909 - acc: 1.0000 - val_loss: 0.8888 - val_acc: 0.6500\n",
            "Epoch 260/300\n",
            "\n",
            "Epoch 00260: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.1122 - acc: 0.9667 - val_loss: 0.9019 - val_acc: 0.6000\n",
            "Epoch 261/300\n",
            "\n",
            "Epoch 00261: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.0746 - acc: 1.0000 - val_loss: 0.9187 - val_acc: 0.6000\n",
            "Epoch 262/300\n",
            "\n",
            "Epoch 00262: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.0991 - acc: 1.0000 - val_loss: 0.9211 - val_acc: 0.6500\n",
            "Epoch 263/300\n",
            "\n",
            "Epoch 00263: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.0935 - acc: 1.0000 - val_loss: 0.9141 - val_acc: 0.6000\n",
            "Epoch 264/300\n",
            "\n",
            "Epoch 00264: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.1050 - acc: 1.0000 - val_loss: 0.9012 - val_acc: 0.6500\n",
            "Epoch 265/300\n",
            "\n",
            "Epoch 00265: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.0921 - acc: 1.0000 - val_loss: 0.8907 - val_acc: 0.6500\n",
            "Epoch 266/300\n",
            "\n",
            "Epoch 00266: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.1010 - acc: 1.0000 - val_loss: 0.8869 - val_acc: 0.6500\n",
            "Epoch 267/300\n",
            "\n",
            "Epoch 00267: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.0920 - acc: 1.0000 - val_loss: 0.8927 - val_acc: 0.6000\n",
            "Epoch 268/300\n",
            "\n",
            "Epoch 00268: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.0847 - acc: 1.0000 - val_loss: 0.8934 - val_acc: 0.6000\n",
            "Epoch 269/300\n",
            "\n",
            "Epoch 00269: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.1078 - acc: 1.0000 - val_loss: 0.8920 - val_acc: 0.6500\n",
            "Epoch 270/300\n",
            "\n",
            "Epoch 00270: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.0860 - acc: 1.0000 - val_loss: 0.8960 - val_acc: 0.6500\n",
            "Epoch 271/300\n",
            "\n",
            "Epoch 00271: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.0946 - acc: 1.0000 - val_loss: 0.8918 - val_acc: 0.6500\n",
            "Epoch 272/300\n",
            "\n",
            "Epoch 00272: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.0848 - acc: 1.0000 - val_loss: 0.8886 - val_acc: 0.6500\n",
            "Epoch 273/300\n",
            "\n",
            "Epoch 00273: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.0949 - acc: 1.0000 - val_loss: 0.8985 - val_acc: 0.6000\n",
            "Epoch 274/300\n",
            "\n",
            "Epoch 00274: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.0925 - acc: 1.0000 - val_loss: 0.9036 - val_acc: 0.6000\n",
            "Epoch 275/300\n",
            "\n",
            "Epoch 00275: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.1050 - acc: 1.0000 - val_loss: 0.9017 - val_acc: 0.6000\n",
            "Epoch 276/300\n",
            "\n",
            "Epoch 00276: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.0882 - acc: 1.0000 - val_loss: 0.8915 - val_acc: 0.6000\n",
            "Epoch 277/300\n",
            "\n",
            "Epoch 00277: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.0857 - acc: 1.0000 - val_loss: 0.8822 - val_acc: 0.6000\n",
            "Epoch 278/300\n",
            "\n",
            "Epoch 00278: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.0832 - acc: 1.0000 - val_loss: 0.8848 - val_acc: 0.6000\n",
            "Epoch 279/300\n",
            "\n",
            "Epoch 00279: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.0930 - acc: 1.0000 - val_loss: 0.8946 - val_acc: 0.6000\n",
            "Epoch 280/300\n",
            "\n",
            "Epoch 00280: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.1003 - acc: 1.0000 - val_loss: 0.8903 - val_acc: 0.6000\n",
            "Epoch 281/300\n",
            "\n",
            "Epoch 00281: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.0829 - acc: 1.0000 - val_loss: 0.8949 - val_acc: 0.6000\n",
            "Epoch 282/300\n",
            "\n",
            "Epoch 00282: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.0771 - acc: 1.0000 - val_loss: 0.8927 - val_acc: 0.6000\n",
            "Epoch 283/300\n",
            "\n",
            "Epoch 00283: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.0857 - acc: 1.0000 - val_loss: 0.8926 - val_acc: 0.6000\n",
            "Epoch 284/300\n",
            "\n",
            "Epoch 00284: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.0755 - acc: 1.0000 - val_loss: 0.8882 - val_acc: 0.6000\n",
            "Epoch 285/300\n",
            "\n",
            "Epoch 00285: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.0838 - acc: 1.0000 - val_loss: 0.8825 - val_acc: 0.6000\n",
            "Epoch 286/300\n",
            "\n",
            "Epoch 00286: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.0681 - acc: 1.0000 - val_loss: 0.8734 - val_acc: 0.6000\n",
            "Epoch 287/300\n",
            "\n",
            "Epoch 00287: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.0726 - acc: 1.0000 - val_loss: 0.8646 - val_acc: 0.6500\n",
            "Epoch 288/300\n",
            "\n",
            "Epoch 00288: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.0889 - acc: 1.0000 - val_loss: 0.8741 - val_acc: 0.6500\n",
            "Epoch 289/300\n",
            "\n",
            "Epoch 00289: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.0856 - acc: 1.0000 - val_loss: 0.8866 - val_acc: 0.6000\n",
            "Epoch 290/300\n",
            "\n",
            "Epoch 00290: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.0957 - acc: 1.0000 - val_loss: 0.8978 - val_acc: 0.6000\n",
            "Epoch 291/300\n",
            "\n",
            "Epoch 00291: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.0839 - acc: 1.0000 - val_loss: 0.9060 - val_acc: 0.6000\n",
            "Epoch 292/300\n",
            "\n",
            "Epoch 00292: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.0809 - acc: 1.0000 - val_loss: 0.9022 - val_acc: 0.6000\n",
            "Epoch 293/300\n",
            "\n",
            "Epoch 00293: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.0660 - acc: 1.0000 - val_loss: 0.9015 - val_acc: 0.6000\n",
            "Epoch 294/300\n",
            "\n",
            "Epoch 00294: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.0925 - acc: 1.0000 - val_loss: 0.8873 - val_acc: 0.6000\n",
            "Epoch 295/300\n",
            "\n",
            "Epoch 00295: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.0662 - acc: 1.0000 - val_loss: 0.8850 - val_acc: 0.6000\n",
            "Epoch 296/300\n",
            "\n",
            "Epoch 00296: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.0829 - acc: 1.0000 - val_loss: 0.8820 - val_acc: 0.6500\n",
            "Epoch 297/300\n",
            "\n",
            "Epoch 00297: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.0861 - acc: 1.0000 - val_loss: 0.8707 - val_acc: 0.6500\n",
            "Epoch 298/300\n",
            "\n",
            "Epoch 00298: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.0938 - acc: 1.0000 - val_loss: 0.8852 - val_acc: 0.6500\n",
            "Epoch 299/300\n",
            "\n",
            "Epoch 00299: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.0831 - acc: 1.0000 - val_loss: 0.8841 - val_acc: 0.6500\n",
            "Epoch 300/300\n",
            "\n",
            "Epoch 00300: val_loss did not improve from 0.74986\n",
            "60/60 - 0s - loss: 0.0900 - acc: 1.0000 - val_loss: 0.8607 - val_acc: 0.6500\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\noihKwFFFoCiKEnBUESiKogSc/w9Nab4/mhD/CwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "# Evaluate on test data\n",
            "40/40 [==============================] - 0s 2ms/sample - loss: 1.2975 - acc: 0.3750\n",
            "test loss, test acc: [1.2974810251034796, 0.375]\n",
            "EEG_Deep/Data2A/parsed_P09T.mat\n",
            "Filtering of Training Data Finished\n",
            "EEG_Deep/Data2A/parsed_P09E.mat\n",
            "Filtering of Testing Data Finished\n",
            "(1, 2)\n",
            "Finding labels in training data\n",
            "Finding labels in testing data\n",
            "[1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2]\n",
            "(60, 12, 1536)\n",
            "(60,)\n",
            "(20, 12, 1536)\n",
            "(20,)\n",
            "[[1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]]\n",
            "X_train shape: (60, 1, 12, 1536)\n",
            "60 train samples\n",
            "20 val samples\n",
            "(40, 12, 1536)\n",
            "(40, 1)\n",
            "X_train shape: (40, 1, 12, 1536)\n",
            "40 train samples\n",
            "Train on 60 samples, validate on 20 samples\n",
            "Epoch 1/300\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 1.34044, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 1s - loss: 1.3147 - acc: 0.5000 - val_loss: 1.3404 - val_acc: 0.5000\n",
            "Epoch 2/300\n",
            "\n",
            "Epoch 00002: val_loss improved from 1.34044 to 1.29670, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 1.1295 - acc: 0.4667 - val_loss: 1.2967 - val_acc: 0.6500\n",
            "Epoch 3/300\n",
            "\n",
            "Epoch 00003: val_loss improved from 1.29670 to 1.26270, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.9475 - acc: 0.6833 - val_loss: 1.2627 - val_acc: 0.5500\n",
            "Epoch 4/300\n",
            "\n",
            "Epoch 00004: val_loss improved from 1.26270 to 1.23272, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.8602 - acc: 0.8000 - val_loss: 1.2327 - val_acc: 0.5500\n",
            "Epoch 5/300\n",
            "\n",
            "Epoch 00005: val_loss improved from 1.23272 to 1.20640, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.8072 - acc: 0.7333 - val_loss: 1.2064 - val_acc: 0.6000\n",
            "Epoch 6/300\n",
            "\n",
            "Epoch 00006: val_loss improved from 1.20640 to 1.18384, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.7888 - acc: 0.7333 - val_loss: 1.1838 - val_acc: 0.6500\n",
            "Epoch 7/300\n",
            "\n",
            "Epoch 00007: val_loss improved from 1.18384 to 1.16386, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.7481 - acc: 0.6667 - val_loss: 1.1639 - val_acc: 0.6000\n",
            "Epoch 8/300\n",
            "\n",
            "Epoch 00008: val_loss improved from 1.16386 to 1.14707, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.7303 - acc: 0.7333 - val_loss: 1.1471 - val_acc: 0.6000\n",
            "Epoch 9/300\n",
            "\n",
            "Epoch 00009: val_loss improved from 1.14707 to 1.12870, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.7107 - acc: 0.7333 - val_loss: 1.1287 - val_acc: 0.6000\n",
            "Epoch 10/300\n",
            "\n",
            "Epoch 00010: val_loss improved from 1.12870 to 1.11078, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6770 - acc: 0.9000 - val_loss: 1.1108 - val_acc: 0.6000\n",
            "Epoch 11/300\n",
            "\n",
            "Epoch 00011: val_loss improved from 1.11078 to 1.09356, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6573 - acc: 0.8500 - val_loss: 1.0936 - val_acc: 0.6000\n",
            "Epoch 12/300\n",
            "\n",
            "Epoch 00012: val_loss improved from 1.09356 to 1.07734, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6375 - acc: 0.8667 - val_loss: 1.0773 - val_acc: 0.6000\n",
            "Epoch 13/300\n",
            "\n",
            "Epoch 00013: val_loss improved from 1.07734 to 1.06135, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6572 - acc: 0.8000 - val_loss: 1.0614 - val_acc: 0.6000\n",
            "Epoch 14/300\n",
            "\n",
            "Epoch 00014: val_loss improved from 1.06135 to 1.04513, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6270 - acc: 0.7833 - val_loss: 1.0451 - val_acc: 0.6000\n",
            "Epoch 15/300\n",
            "\n",
            "Epoch 00015: val_loss improved from 1.04513 to 1.03043, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6004 - acc: 0.8500 - val_loss: 1.0304 - val_acc: 0.6000\n",
            "Epoch 16/300\n",
            "\n",
            "Epoch 00016: val_loss improved from 1.03043 to 1.01684, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5996 - acc: 0.8167 - val_loss: 1.0168 - val_acc: 0.6000\n",
            "Epoch 17/300\n",
            "\n",
            "Epoch 00017: val_loss improved from 1.01684 to 1.00522, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6047 - acc: 0.8167 - val_loss: 1.0052 - val_acc: 0.6000\n",
            "Epoch 18/300\n",
            "\n",
            "Epoch 00018: val_loss improved from 1.00522 to 0.99423, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5724 - acc: 0.8167 - val_loss: 0.9942 - val_acc: 0.6000\n",
            "Epoch 19/300\n",
            "\n",
            "Epoch 00019: val_loss improved from 0.99423 to 0.98507, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5967 - acc: 0.8167 - val_loss: 0.9851 - val_acc: 0.6000\n",
            "Epoch 20/300\n",
            "\n",
            "Epoch 00020: val_loss improved from 0.98507 to 0.97551, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5455 - acc: 0.9500 - val_loss: 0.9755 - val_acc: 0.6000\n",
            "Epoch 21/300\n",
            "\n",
            "Epoch 00021: val_loss improved from 0.97551 to 0.96742, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5368 - acc: 0.8833 - val_loss: 0.9674 - val_acc: 0.6000\n",
            "Epoch 22/300\n",
            "\n",
            "Epoch 00022: val_loss improved from 0.96742 to 0.95859, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5515 - acc: 0.8667 - val_loss: 0.9586 - val_acc: 0.6000\n",
            "Epoch 23/300\n",
            "\n",
            "Epoch 00023: val_loss improved from 0.95859 to 0.94910, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5451 - acc: 0.8833 - val_loss: 0.9491 - val_acc: 0.6000\n",
            "Epoch 24/300\n",
            "\n",
            "Epoch 00024: val_loss improved from 0.94910 to 0.93934, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5204 - acc: 0.8667 - val_loss: 0.9393 - val_acc: 0.6000\n",
            "Epoch 25/300\n",
            "\n",
            "Epoch 00025: val_loss improved from 0.93934 to 0.93239, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5048 - acc: 0.9000 - val_loss: 0.9324 - val_acc: 0.6000\n",
            "Epoch 26/300\n",
            "\n",
            "Epoch 00026: val_loss improved from 0.93239 to 0.92983, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4925 - acc: 0.9000 - val_loss: 0.9298 - val_acc: 0.6500\n",
            "Epoch 27/300\n",
            "\n",
            "Epoch 00027: val_loss improved from 0.92983 to 0.92573, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4703 - acc: 0.9167 - val_loss: 0.9257 - val_acc: 0.6500\n",
            "Epoch 28/300\n",
            "\n",
            "Epoch 00028: val_loss improved from 0.92573 to 0.92188, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4781 - acc: 0.9500 - val_loss: 0.9219 - val_acc: 0.6500\n",
            "Epoch 29/300\n",
            "\n",
            "Epoch 00029: val_loss improved from 0.92188 to 0.92070, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4713 - acc: 0.9500 - val_loss: 0.9207 - val_acc: 0.6500\n",
            "Epoch 30/300\n",
            "\n",
            "Epoch 00030: val_loss improved from 0.92070 to 0.91383, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4566 - acc: 0.9667 - val_loss: 0.9138 - val_acc: 0.6500\n",
            "Epoch 31/300\n",
            "\n",
            "Epoch 00031: val_loss improved from 0.91383 to 0.90451, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4409 - acc: 0.9333 - val_loss: 0.9045 - val_acc: 0.7000\n",
            "Epoch 32/300\n",
            "\n",
            "Epoch 00032: val_loss improved from 0.90451 to 0.89676, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4181 - acc: 0.9667 - val_loss: 0.8968 - val_acc: 0.7000\n",
            "Epoch 33/300\n",
            "\n",
            "Epoch 00033: val_loss improved from 0.89676 to 0.89224, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4077 - acc: 0.9333 - val_loss: 0.8922 - val_acc: 0.7000\n",
            "Epoch 34/300\n",
            "\n",
            "Epoch 00034: val_loss improved from 0.89224 to 0.89041, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4198 - acc: 0.9833 - val_loss: 0.8904 - val_acc: 0.7000\n",
            "Epoch 35/300\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.4030 - acc: 0.9500 - val_loss: 0.8928 - val_acc: 0.6000\n",
            "Epoch 36/300\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.3725 - acc: 0.9833 - val_loss: 0.8918 - val_acc: 0.6000\n",
            "Epoch 37/300\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.4054 - acc: 0.9333 - val_loss: 0.8955 - val_acc: 0.6000\n",
            "Epoch 38/300\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.3740 - acc: 0.9333 - val_loss: 0.8948 - val_acc: 0.5500\n",
            "Epoch 39/300\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.4011 - acc: 0.9000 - val_loss: 0.8938 - val_acc: 0.5500\n",
            "Epoch 40/300\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.3681 - acc: 0.9500 - val_loss: 0.8973 - val_acc: 0.5000\n",
            "Epoch 41/300\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.3361 - acc: 0.9667 - val_loss: 0.9096 - val_acc: 0.5000\n",
            "Epoch 42/300\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.3999 - acc: 0.9167 - val_loss: 0.9088 - val_acc: 0.5000\n",
            "Epoch 43/300\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.3463 - acc: 0.9333 - val_loss: 0.8967 - val_acc: 0.5000\n",
            "Epoch 44/300\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.3556 - acc: 0.9333 - val_loss: 0.8948 - val_acc: 0.5000\n",
            "Epoch 45/300\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.3479 - acc: 0.9667 - val_loss: 0.9144 - val_acc: 0.5000\n",
            "Epoch 46/300\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.3488 - acc: 0.9500 - val_loss: 0.9203 - val_acc: 0.5000\n",
            "Epoch 47/300\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.3072 - acc: 0.9333 - val_loss: 0.9272 - val_acc: 0.5000\n",
            "Epoch 48/300\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.3443 - acc: 0.9333 - val_loss: 0.9414 - val_acc: 0.5000\n",
            "Epoch 49/300\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.2965 - acc: 0.9833 - val_loss: 0.9469 - val_acc: 0.5000\n",
            "Epoch 50/300\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.3450 - acc: 0.9167 - val_loss: 0.9600 - val_acc: 0.5000\n",
            "Epoch 51/300\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.3213 - acc: 0.9167 - val_loss: 0.9617 - val_acc: 0.5000\n",
            "Epoch 52/300\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.3131 - acc: 0.9500 - val_loss: 0.9551 - val_acc: 0.5000\n",
            "Epoch 53/300\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.3707 - acc: 0.9333 - val_loss: 0.9475 - val_acc: 0.5000\n",
            "Epoch 54/300\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.2918 - acc: 0.9833 - val_loss: 0.9765 - val_acc: 0.5000\n",
            "Epoch 55/300\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.3245 - acc: 0.9333 - val_loss: 0.9747 - val_acc: 0.5000\n",
            "Epoch 56/300\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.3168 - acc: 0.9167 - val_loss: 0.9570 - val_acc: 0.5000\n",
            "Epoch 57/300\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.2958 - acc: 0.9833 - val_loss: 0.9577 - val_acc: 0.5000\n",
            "Epoch 58/300\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.3236 - acc: 0.9333 - val_loss: 0.9864 - val_acc: 0.5000\n",
            "Epoch 59/300\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.2989 - acc: 0.9667 - val_loss: 1.0183 - val_acc: 0.5000\n",
            "Epoch 60/300\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.3359 - acc: 0.9000 - val_loss: 1.0220 - val_acc: 0.5000\n",
            "Epoch 61/300\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.2929 - acc: 0.9667 - val_loss: 1.0003 - val_acc: 0.5000\n",
            "Epoch 62/300\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.2482 - acc: 1.0000 - val_loss: 1.0086 - val_acc: 0.5000\n",
            "Epoch 63/300\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.2930 - acc: 0.9833 - val_loss: 1.0087 - val_acc: 0.5000\n",
            "Epoch 64/300\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.3075 - acc: 0.9333 - val_loss: 1.0113 - val_acc: 0.5000\n",
            "Epoch 65/300\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.2958 - acc: 0.9833 - val_loss: 1.0049 - val_acc: 0.5000\n",
            "Epoch 66/300\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.3147 - acc: 1.0000 - val_loss: 1.0159 - val_acc: 0.5000\n",
            "Epoch 67/300\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.2859 - acc: 0.9333 - val_loss: 1.0130 - val_acc: 0.5000\n",
            "Epoch 68/300\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.2686 - acc: 0.9667 - val_loss: 0.9979 - val_acc: 0.5000\n",
            "Epoch 69/300\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.2868 - acc: 0.9500 - val_loss: 1.0034 - val_acc: 0.5000\n",
            "Epoch 70/300\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.2933 - acc: 0.9333 - val_loss: 1.0435 - val_acc: 0.5000\n",
            "Epoch 71/300\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.2708 - acc: 0.9667 - val_loss: 1.0528 - val_acc: 0.5000\n",
            "Epoch 72/300\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.2576 - acc: 0.9833 - val_loss: 1.0617 - val_acc: 0.5000\n",
            "Epoch 73/300\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.2804 - acc: 0.9333 - val_loss: 1.0522 - val_acc: 0.5000\n",
            "Epoch 74/300\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.2945 - acc: 0.9333 - val_loss: 1.0340 - val_acc: 0.5000\n",
            "Epoch 75/300\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.2514 - acc: 0.9500 - val_loss: 1.0345 - val_acc: 0.5000\n",
            "Epoch 76/300\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.2666 - acc: 0.9667 - val_loss: 1.0509 - val_acc: 0.5000\n",
            "Epoch 77/300\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.2984 - acc: 0.9333 - val_loss: 1.0746 - val_acc: 0.5000\n",
            "Epoch 78/300\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.2705 - acc: 0.9500 - val_loss: 1.0738 - val_acc: 0.5000\n",
            "Epoch 79/300\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.2292 - acc: 0.9833 - val_loss: 1.0949 - val_acc: 0.5000\n",
            "Epoch 80/300\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.2879 - acc: 0.9167 - val_loss: 1.0581 - val_acc: 0.5000\n",
            "Epoch 81/300\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.2475 - acc: 0.9500 - val_loss: 1.0535 - val_acc: 0.5000\n",
            "Epoch 82/300\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.2513 - acc: 0.9833 - val_loss: 1.0770 - val_acc: 0.5000\n",
            "Epoch 83/300\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.2961 - acc: 0.9500 - val_loss: 1.1225 - val_acc: 0.5000\n",
            "Epoch 84/300\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.2463 - acc: 0.9667 - val_loss: 1.1140 - val_acc: 0.5000\n",
            "Epoch 85/300\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.2490 - acc: 1.0000 - val_loss: 1.1174 - val_acc: 0.5000\n",
            "Epoch 86/300\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.2499 - acc: 0.9667 - val_loss: 1.1325 - val_acc: 0.5000\n",
            "Epoch 87/300\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.2326 - acc: 0.9833 - val_loss: 1.1618 - val_acc: 0.5000\n",
            "Epoch 88/300\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.2609 - acc: 0.9500 - val_loss: 1.1455 - val_acc: 0.5000\n",
            "Epoch 89/300\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.2528 - acc: 0.9667 - val_loss: 1.1327 - val_acc: 0.5000\n",
            "Epoch 90/300\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.2316 - acc: 0.9833 - val_loss: 1.0901 - val_acc: 0.5000\n",
            "Epoch 91/300\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.2346 - acc: 0.9667 - val_loss: 1.0738 - val_acc: 0.5000\n",
            "Epoch 92/300\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.2744 - acc: 0.9667 - val_loss: 1.0859 - val_acc: 0.5000\n",
            "Epoch 93/300\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.2227 - acc: 1.0000 - val_loss: 1.0816 - val_acc: 0.5000\n",
            "Epoch 94/300\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.2204 - acc: 1.0000 - val_loss: 1.0795 - val_acc: 0.5000\n",
            "Epoch 95/300\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.2292 - acc: 0.9833 - val_loss: 1.0934 - val_acc: 0.5000\n",
            "Epoch 96/300\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.2566 - acc: 0.9667 - val_loss: 1.1316 - val_acc: 0.5000\n",
            "Epoch 97/300\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.2534 - acc: 0.9333 - val_loss: 1.1489 - val_acc: 0.5000\n",
            "Epoch 98/300\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.2352 - acc: 0.9833 - val_loss: 1.1112 - val_acc: 0.5000\n",
            "Epoch 99/300\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.2141 - acc: 0.9667 - val_loss: 1.1301 - val_acc: 0.5000\n",
            "Epoch 100/300\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.2389 - acc: 0.9667 - val_loss: 1.1610 - val_acc: 0.5000\n",
            "Epoch 101/300\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.2364 - acc: 0.9500 - val_loss: 1.1703 - val_acc: 0.5000\n",
            "Epoch 102/300\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.2599 - acc: 0.9833 - val_loss: 1.1642 - val_acc: 0.5000\n",
            "Epoch 103/300\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.2337 - acc: 0.9667 - val_loss: 1.1369 - val_acc: 0.5000\n",
            "Epoch 104/300\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.2332 - acc: 1.0000 - val_loss: 1.0886 - val_acc: 0.5000\n",
            "Epoch 105/300\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.2183 - acc: 1.0000 - val_loss: 1.0736 - val_acc: 0.5000\n",
            "Epoch 106/300\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.2130 - acc: 0.9833 - val_loss: 1.1052 - val_acc: 0.5000\n",
            "Epoch 107/300\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.1941 - acc: 1.0000 - val_loss: 1.1625 - val_acc: 0.5000\n",
            "Epoch 108/300\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.2202 - acc: 1.0000 - val_loss: 1.1954 - val_acc: 0.5000\n",
            "Epoch 109/300\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.2014 - acc: 1.0000 - val_loss: 1.1656 - val_acc: 0.5000\n",
            "Epoch 110/300\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.2225 - acc: 0.9833 - val_loss: 1.1718 - val_acc: 0.5000\n",
            "Epoch 111/300\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.2133 - acc: 0.9500 - val_loss: 1.1872 - val_acc: 0.5000\n",
            "Epoch 112/300\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.1996 - acc: 0.9667 - val_loss: 1.2253 - val_acc: 0.5000\n",
            "Epoch 113/300\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.2297 - acc: 0.9500 - val_loss: 1.2353 - val_acc: 0.5000\n",
            "Epoch 114/300\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.2252 - acc: 0.9500 - val_loss: 1.2021 - val_acc: 0.5000\n",
            "Epoch 115/300\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.1803 - acc: 1.0000 - val_loss: 1.1893 - val_acc: 0.5000\n",
            "Epoch 116/300\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.2274 - acc: 0.9500 - val_loss: 1.1803 - val_acc: 0.5000\n",
            "Epoch 117/300\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.2069 - acc: 0.9500 - val_loss: 1.1868 - val_acc: 0.5000\n",
            "Epoch 118/300\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.2113 - acc: 1.0000 - val_loss: 1.1776 - val_acc: 0.5000\n",
            "Epoch 119/300\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.2158 - acc: 0.9833 - val_loss: 1.1413 - val_acc: 0.5000\n",
            "Epoch 120/300\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.2055 - acc: 0.9500 - val_loss: 1.1530 - val_acc: 0.5000\n",
            "Epoch 121/300\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.2125 - acc: 0.9667 - val_loss: 1.2139 - val_acc: 0.5000\n",
            "Epoch 122/300\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.2412 - acc: 0.9333 - val_loss: 1.2482 - val_acc: 0.5000\n",
            "Epoch 123/300\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.2191 - acc: 0.9500 - val_loss: 1.2352 - val_acc: 0.5000\n",
            "Epoch 124/300\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.2052 - acc: 0.9833 - val_loss: 1.1865 - val_acc: 0.5000\n",
            "Epoch 125/300\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.2208 - acc: 1.0000 - val_loss: 1.1589 - val_acc: 0.5000\n",
            "Epoch 126/300\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.2096 - acc: 0.9667 - val_loss: 1.1276 - val_acc: 0.5000\n",
            "Epoch 127/300\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.2411 - acc: 0.9667 - val_loss: 1.1154 - val_acc: 0.5000\n",
            "Epoch 128/300\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.1736 - acc: 1.0000 - val_loss: 1.1728 - val_acc: 0.5000\n",
            "Epoch 129/300\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.1848 - acc: 1.0000 - val_loss: 1.2591 - val_acc: 0.5000\n",
            "Epoch 130/300\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.1911 - acc: 1.0000 - val_loss: 1.2874 - val_acc: 0.5000\n",
            "Epoch 131/300\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.1815 - acc: 0.9667 - val_loss: 1.2901 - val_acc: 0.5000\n",
            "Epoch 132/300\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.1909 - acc: 1.0000 - val_loss: 1.2569 - val_acc: 0.5000\n",
            "Epoch 133/300\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.1907 - acc: 0.9833 - val_loss: 1.2056 - val_acc: 0.5000\n",
            "Epoch 134/300\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.2084 - acc: 0.9833 - val_loss: 1.1903 - val_acc: 0.5000\n",
            "Epoch 135/300\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.2286 - acc: 0.9667 - val_loss: 1.2467 - val_acc: 0.5000\n",
            "Epoch 136/300\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.1783 - acc: 0.9833 - val_loss: 1.2766 - val_acc: 0.5000\n",
            "Epoch 137/300\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.1979 - acc: 0.9667 - val_loss: 1.2509 - val_acc: 0.5000\n",
            "Epoch 138/300\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.2086 - acc: 0.9833 - val_loss: 1.2658 - val_acc: 0.5000\n",
            "Epoch 139/300\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.2236 - acc: 0.9833 - val_loss: 1.2597 - val_acc: 0.5000\n",
            "Epoch 140/300\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.1821 - acc: 1.0000 - val_loss: 1.3042 - val_acc: 0.5000\n",
            "Epoch 141/300\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.1710 - acc: 1.0000 - val_loss: 1.3186 - val_acc: 0.5000\n",
            "Epoch 142/300\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.1613 - acc: 1.0000 - val_loss: 1.3680 - val_acc: 0.5000\n",
            "Epoch 143/300\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.2300 - acc: 0.9833 - val_loss: 1.3875 - val_acc: 0.5000\n",
            "Epoch 144/300\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.2286 - acc: 0.9667 - val_loss: 1.3713 - val_acc: 0.5000\n",
            "Epoch 145/300\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.1941 - acc: 1.0000 - val_loss: 1.2925 - val_acc: 0.5000\n",
            "Epoch 146/300\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.1683 - acc: 0.9833 - val_loss: 1.2421 - val_acc: 0.5000\n",
            "Epoch 147/300\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.2324 - acc: 0.9333 - val_loss: 1.2239 - val_acc: 0.5000\n",
            "Epoch 148/300\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.2097 - acc: 0.9667 - val_loss: 1.1448 - val_acc: 0.5000\n",
            "Epoch 149/300\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.1889 - acc: 0.9833 - val_loss: 1.1726 - val_acc: 0.5000\n",
            "Epoch 150/300\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.1681 - acc: 0.9833 - val_loss: 1.2477 - val_acc: 0.5000\n",
            "Epoch 151/300\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.1673 - acc: 0.9833 - val_loss: 1.2898 - val_acc: 0.5000\n",
            "Epoch 152/300\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.2070 - acc: 0.9833 - val_loss: 1.2902 - val_acc: 0.5000\n",
            "Epoch 153/300\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.1961 - acc: 0.9833 - val_loss: 1.3103 - val_acc: 0.5000\n",
            "Epoch 154/300\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.1820 - acc: 0.9667 - val_loss: 1.2835 - val_acc: 0.5000\n",
            "Epoch 155/300\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.1651 - acc: 1.0000 - val_loss: 1.2219 - val_acc: 0.5000\n",
            "Epoch 156/300\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.1806 - acc: 0.9833 - val_loss: 1.2405 - val_acc: 0.5000\n",
            "Epoch 157/300\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.1550 - acc: 1.0000 - val_loss: 1.2608 - val_acc: 0.5000\n",
            "Epoch 158/300\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.2001 - acc: 0.9667 - val_loss: 1.3416 - val_acc: 0.5000\n",
            "Epoch 159/300\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.1690 - acc: 0.9833 - val_loss: 1.3466 - val_acc: 0.5000\n",
            "Epoch 160/300\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.1501 - acc: 1.0000 - val_loss: 1.3297 - val_acc: 0.5000\n",
            "Epoch 161/300\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.1863 - acc: 0.9667 - val_loss: 1.3469 - val_acc: 0.5000\n",
            "Epoch 162/300\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.1630 - acc: 1.0000 - val_loss: 1.4637 - val_acc: 0.4500\n",
            "Epoch 163/300\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.1622 - acc: 0.9833 - val_loss: 1.4974 - val_acc: 0.4500\n",
            "Epoch 164/300\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.2146 - acc: 0.9000 - val_loss: 1.3940 - val_acc: 0.4500\n",
            "Epoch 165/300\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.1773 - acc: 0.9833 - val_loss: 1.3063 - val_acc: 0.5000\n",
            "Epoch 166/300\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.1410 - acc: 1.0000 - val_loss: 1.3687 - val_acc: 0.5000\n",
            "Epoch 167/300\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.2121 - acc: 0.9667 - val_loss: 1.4139 - val_acc: 0.4500\n",
            "Epoch 168/300\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.1819 - acc: 1.0000 - val_loss: 1.4293 - val_acc: 0.4500\n",
            "Epoch 169/300\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.1647 - acc: 0.9833 - val_loss: 1.4048 - val_acc: 0.4500\n",
            "Epoch 170/300\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.1632 - acc: 0.9667 - val_loss: 1.3296 - val_acc: 0.4500\n",
            "Epoch 171/300\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.1819 - acc: 0.9833 - val_loss: 1.2903 - val_acc: 0.4500\n",
            "Epoch 172/300\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.1762 - acc: 1.0000 - val_loss: 1.2475 - val_acc: 0.5000\n",
            "Epoch 173/300\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.1802 - acc: 1.0000 - val_loss: 1.1993 - val_acc: 0.5000\n",
            "Epoch 174/300\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.1836 - acc: 0.9833 - val_loss: 1.2112 - val_acc: 0.5000\n",
            "Epoch 175/300\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.1577 - acc: 0.9667 - val_loss: 1.1924 - val_acc: 0.5000\n",
            "Epoch 176/300\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.1488 - acc: 1.0000 - val_loss: 1.2218 - val_acc: 0.5000\n",
            "Epoch 177/300\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.1506 - acc: 1.0000 - val_loss: 1.2153 - val_acc: 0.5000\n",
            "Epoch 178/300\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.1537 - acc: 1.0000 - val_loss: 1.1979 - val_acc: 0.5000\n",
            "Epoch 179/300\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.1634 - acc: 0.9833 - val_loss: 1.1885 - val_acc: 0.5000\n",
            "Epoch 180/300\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.1505 - acc: 1.0000 - val_loss: 1.1900 - val_acc: 0.5000\n",
            "Epoch 181/300\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.1509 - acc: 0.9833 - val_loss: 1.2399 - val_acc: 0.5000\n",
            "Epoch 182/300\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.1408 - acc: 0.9833 - val_loss: 1.2858 - val_acc: 0.5000\n",
            "Epoch 183/300\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.1562 - acc: 1.0000 - val_loss: 1.2954 - val_acc: 0.5000\n",
            "Epoch 184/300\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.1319 - acc: 1.0000 - val_loss: 1.3024 - val_acc: 0.5000\n",
            "Epoch 185/300\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.1743 - acc: 0.9833 - val_loss: 1.3068 - val_acc: 0.5000\n",
            "Epoch 186/300\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.1373 - acc: 1.0000 - val_loss: 1.2579 - val_acc: 0.5000\n",
            "Epoch 187/300\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.1876 - acc: 0.9500 - val_loss: 1.1937 - val_acc: 0.5000\n",
            "Epoch 188/300\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.1692 - acc: 0.9667 - val_loss: 1.1416 - val_acc: 0.5500\n",
            "Epoch 189/300\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.1445 - acc: 0.9833 - val_loss: 1.1492 - val_acc: 0.5500\n",
            "Epoch 190/300\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.1471 - acc: 0.9833 - val_loss: 1.1149 - val_acc: 0.5500\n",
            "Epoch 191/300\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.1433 - acc: 1.0000 - val_loss: 1.0921 - val_acc: 0.5500\n",
            "Epoch 192/300\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.1625 - acc: 1.0000 - val_loss: 1.1051 - val_acc: 0.5500\n",
            "Epoch 193/300\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.1674 - acc: 0.9833 - val_loss: 1.1299 - val_acc: 0.5500\n",
            "Epoch 194/300\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.1688 - acc: 0.9667 - val_loss: 1.2255 - val_acc: 0.5000\n",
            "Epoch 195/300\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.1440 - acc: 0.9833 - val_loss: 1.3099 - val_acc: 0.5000\n",
            "Epoch 196/300\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.1562 - acc: 1.0000 - val_loss: 1.3633 - val_acc: 0.5000\n",
            "Epoch 197/300\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.1323 - acc: 0.9833 - val_loss: 1.3261 - val_acc: 0.5000\n",
            "Epoch 198/300\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.1720 - acc: 0.9667 - val_loss: 1.2580 - val_acc: 0.5000\n",
            "Epoch 199/300\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.1682 - acc: 0.9833 - val_loss: 1.2026 - val_acc: 0.5000\n",
            "Epoch 200/300\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.1215 - acc: 1.0000 - val_loss: 1.1976 - val_acc: 0.5000\n",
            "Epoch 201/300\n",
            "\n",
            "Epoch 00201: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.1251 - acc: 1.0000 - val_loss: 1.1754 - val_acc: 0.5000\n",
            "Epoch 202/300\n",
            "\n",
            "Epoch 00202: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.1553 - acc: 0.9833 - val_loss: 1.2056 - val_acc: 0.5000\n",
            "Epoch 203/300\n",
            "\n",
            "Epoch 00203: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.1283 - acc: 1.0000 - val_loss: 1.1895 - val_acc: 0.5000\n",
            "Epoch 204/300\n",
            "\n",
            "Epoch 00204: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.1431 - acc: 0.9667 - val_loss: 1.0959 - val_acc: 0.5500\n",
            "Epoch 205/300\n",
            "\n",
            "Epoch 00205: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.1431 - acc: 0.9833 - val_loss: 1.0754 - val_acc: 0.5000\n",
            "Epoch 206/300\n",
            "\n",
            "Epoch 00206: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.1897 - acc: 0.9667 - val_loss: 1.0419 - val_acc: 0.5000\n",
            "Epoch 207/300\n",
            "\n",
            "Epoch 00207: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.1204 - acc: 1.0000 - val_loss: 1.0863 - val_acc: 0.5500\n",
            "Epoch 208/300\n",
            "\n",
            "Epoch 00208: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.1502 - acc: 0.9833 - val_loss: 1.0607 - val_acc: 0.5500\n",
            "Epoch 209/300\n",
            "\n",
            "Epoch 00209: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.1473 - acc: 1.0000 - val_loss: 1.0935 - val_acc: 0.5500\n",
            "Epoch 210/300\n",
            "\n",
            "Epoch 00210: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.1403 - acc: 1.0000 - val_loss: 1.0991 - val_acc: 0.5500\n",
            "Epoch 211/300\n",
            "\n",
            "Epoch 00211: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.1326 - acc: 1.0000 - val_loss: 1.1178 - val_acc: 0.5000\n",
            "Epoch 212/300\n",
            "\n",
            "Epoch 00212: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.1454 - acc: 1.0000 - val_loss: 1.1433 - val_acc: 0.5000\n",
            "Epoch 213/300\n",
            "\n",
            "Epoch 00213: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.1627 - acc: 0.9833 - val_loss: 1.2044 - val_acc: 0.5000\n",
            "Epoch 214/300\n",
            "\n",
            "Epoch 00214: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.1443 - acc: 0.9667 - val_loss: 1.1868 - val_acc: 0.5500\n",
            "Epoch 215/300\n",
            "\n",
            "Epoch 00215: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.1365 - acc: 1.0000 - val_loss: 1.2342 - val_acc: 0.5500\n",
            "Epoch 216/300\n",
            "\n",
            "Epoch 00216: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.1439 - acc: 1.0000 - val_loss: 1.3329 - val_acc: 0.5500\n",
            "Epoch 217/300\n",
            "\n",
            "Epoch 00217: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.1314 - acc: 1.0000 - val_loss: 1.3838 - val_acc: 0.5500\n",
            "Epoch 218/300\n",
            "\n",
            "Epoch 00218: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.2259 - acc: 0.9333 - val_loss: 1.2766 - val_acc: 0.5000\n",
            "Epoch 219/300\n",
            "\n",
            "Epoch 00219: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.1358 - acc: 1.0000 - val_loss: 1.1359 - val_acc: 0.5500\n",
            "Epoch 220/300\n",
            "\n",
            "Epoch 00220: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.1471 - acc: 1.0000 - val_loss: 1.0536 - val_acc: 0.6000\n",
            "Epoch 221/300\n",
            "\n",
            "Epoch 00221: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.1155 - acc: 1.0000 - val_loss: 1.0847 - val_acc: 0.5500\n",
            "Epoch 222/300\n",
            "\n",
            "Epoch 00222: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.1641 - acc: 1.0000 - val_loss: 1.1007 - val_acc: 0.5500\n",
            "Epoch 223/300\n",
            "\n",
            "Epoch 00223: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.1193 - acc: 1.0000 - val_loss: 1.1108 - val_acc: 0.5500\n",
            "Epoch 224/300\n",
            "\n",
            "Epoch 00224: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.1406 - acc: 0.9833 - val_loss: 1.0948 - val_acc: 0.5500\n",
            "Epoch 225/300\n",
            "\n",
            "Epoch 00225: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.1422 - acc: 1.0000 - val_loss: 1.1561 - val_acc: 0.5000\n",
            "Epoch 226/300\n",
            "\n",
            "Epoch 00226: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.1298 - acc: 0.9833 - val_loss: 1.1832 - val_acc: 0.5000\n",
            "Epoch 227/300\n",
            "\n",
            "Epoch 00227: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.1214 - acc: 1.0000 - val_loss: 1.2108 - val_acc: 0.5000\n",
            "Epoch 228/300\n",
            "\n",
            "Epoch 00228: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.1842 - acc: 0.9500 - val_loss: 1.1804 - val_acc: 0.5000\n",
            "Epoch 229/300\n",
            "\n",
            "Epoch 00229: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.1406 - acc: 1.0000 - val_loss: 1.1022 - val_acc: 0.5500\n",
            "Epoch 230/300\n",
            "\n",
            "Epoch 00230: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.1373 - acc: 0.9833 - val_loss: 1.0826 - val_acc: 0.5500\n",
            "Epoch 231/300\n",
            "\n",
            "Epoch 00231: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.1579 - acc: 0.9667 - val_loss: 1.0720 - val_acc: 0.5500\n",
            "Epoch 232/300\n",
            "\n",
            "Epoch 00232: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.1243 - acc: 1.0000 - val_loss: 1.1010 - val_acc: 0.5500\n",
            "Epoch 233/300\n",
            "\n",
            "Epoch 00233: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.1307 - acc: 1.0000 - val_loss: 1.1638 - val_acc: 0.5500\n",
            "Epoch 234/300\n",
            "\n",
            "Epoch 00234: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.1509 - acc: 0.9833 - val_loss: 1.1863 - val_acc: 0.5500\n",
            "Epoch 235/300\n",
            "\n",
            "Epoch 00235: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.1055 - acc: 1.0000 - val_loss: 1.1989 - val_acc: 0.5500\n",
            "Epoch 236/300\n",
            "\n",
            "Epoch 00236: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.1139 - acc: 1.0000 - val_loss: 1.2319 - val_acc: 0.5500\n",
            "Epoch 237/300\n",
            "\n",
            "Epoch 00237: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.1521 - acc: 0.9833 - val_loss: 1.1512 - val_acc: 0.5500\n",
            "Epoch 238/300\n",
            "\n",
            "Epoch 00238: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.1361 - acc: 0.9833 - val_loss: 1.0733 - val_acc: 0.5500\n",
            "Epoch 239/300\n",
            "\n",
            "Epoch 00239: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.1304 - acc: 1.0000 - val_loss: 1.0385 - val_acc: 0.6000\n",
            "Epoch 240/300\n",
            "\n",
            "Epoch 00240: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.1226 - acc: 1.0000 - val_loss: 1.0384 - val_acc: 0.6000\n",
            "Epoch 241/300\n",
            "\n",
            "Epoch 00241: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.1307 - acc: 0.9833 - val_loss: 1.0074 - val_acc: 0.6000\n",
            "Epoch 242/300\n",
            "\n",
            "Epoch 00242: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.1044 - acc: 1.0000 - val_loss: 1.0369 - val_acc: 0.6000\n",
            "Epoch 243/300\n",
            "\n",
            "Epoch 00243: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.1283 - acc: 0.9833 - val_loss: 1.0852 - val_acc: 0.6000\n",
            "Epoch 244/300\n",
            "\n",
            "Epoch 00244: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.1143 - acc: 0.9833 - val_loss: 1.0941 - val_acc: 0.5500\n",
            "Epoch 245/300\n",
            "\n",
            "Epoch 00245: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.0904 - acc: 1.0000 - val_loss: 1.1446 - val_acc: 0.5500\n",
            "Epoch 246/300\n",
            "\n",
            "Epoch 00246: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.1410 - acc: 0.9833 - val_loss: 1.1247 - val_acc: 0.5500\n",
            "Epoch 247/300\n",
            "\n",
            "Epoch 00247: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.1221 - acc: 1.0000 - val_loss: 1.1352 - val_acc: 0.5500\n",
            "Epoch 248/300\n",
            "\n",
            "Epoch 00248: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.1388 - acc: 0.9833 - val_loss: 1.1068 - val_acc: 0.5500\n",
            "Epoch 249/300\n",
            "\n",
            "Epoch 00249: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.1188 - acc: 1.0000 - val_loss: 1.1205 - val_acc: 0.5500\n",
            "Epoch 250/300\n",
            "\n",
            "Epoch 00250: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.1418 - acc: 1.0000 - val_loss: 1.1297 - val_acc: 0.5500\n",
            "Epoch 251/300\n",
            "\n",
            "Epoch 00251: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.1366 - acc: 0.9667 - val_loss: 1.1526 - val_acc: 0.5500\n",
            "Epoch 252/300\n",
            "\n",
            "Epoch 00252: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.1558 - acc: 0.9667 - val_loss: 1.1169 - val_acc: 0.5500\n",
            "Epoch 253/300\n",
            "\n",
            "Epoch 00253: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.1390 - acc: 0.9833 - val_loss: 1.0628 - val_acc: 0.5500\n",
            "Epoch 254/300\n",
            "\n",
            "Epoch 00254: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.1357 - acc: 0.9833 - val_loss: 1.0587 - val_acc: 0.5500\n",
            "Epoch 255/300\n",
            "\n",
            "Epoch 00255: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.1197 - acc: 1.0000 - val_loss: 1.0767 - val_acc: 0.5500\n",
            "Epoch 256/300\n",
            "\n",
            "Epoch 00256: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.1052 - acc: 0.9833 - val_loss: 1.0574 - val_acc: 0.5500\n",
            "Epoch 257/300\n",
            "\n",
            "Epoch 00257: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.1331 - acc: 1.0000 - val_loss: 1.0426 - val_acc: 0.5500\n",
            "Epoch 258/300\n",
            "\n",
            "Epoch 00258: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.1146 - acc: 1.0000 - val_loss: 1.0947 - val_acc: 0.5500\n",
            "Epoch 259/300\n",
            "\n",
            "Epoch 00259: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.1232 - acc: 1.0000 - val_loss: 1.0892 - val_acc: 0.5500\n",
            "Epoch 260/300\n",
            "\n",
            "Epoch 00260: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.1095 - acc: 0.9667 - val_loss: 1.0773 - val_acc: 0.5500\n",
            "Epoch 261/300\n",
            "\n",
            "Epoch 00261: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.1094 - acc: 1.0000 - val_loss: 1.0412 - val_acc: 0.5500\n",
            "Epoch 262/300\n",
            "\n",
            "Epoch 00262: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.1103 - acc: 1.0000 - val_loss: 0.9887 - val_acc: 0.5500\n",
            "Epoch 263/300\n",
            "\n",
            "Epoch 00263: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.1051 - acc: 1.0000 - val_loss: 0.9837 - val_acc: 0.5500\n",
            "Epoch 264/300\n",
            "\n",
            "Epoch 00264: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.0791 - acc: 1.0000 - val_loss: 0.9457 - val_acc: 0.6000\n",
            "Epoch 265/300\n",
            "\n",
            "Epoch 00265: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.1187 - acc: 1.0000 - val_loss: 0.9877 - val_acc: 0.5500\n",
            "Epoch 266/300\n",
            "\n",
            "Epoch 00266: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.1229 - acc: 1.0000 - val_loss: 1.0593 - val_acc: 0.5500\n",
            "Epoch 267/300\n",
            "\n",
            "Epoch 00267: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.1000 - acc: 1.0000 - val_loss: 1.0974 - val_acc: 0.5500\n",
            "Epoch 268/300\n",
            "\n",
            "Epoch 00268: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.0993 - acc: 0.9833 - val_loss: 1.0490 - val_acc: 0.5500\n",
            "Epoch 269/300\n",
            "\n",
            "Epoch 00269: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.1025 - acc: 0.9833 - val_loss: 1.0543 - val_acc: 0.5500\n",
            "Epoch 270/300\n",
            "\n",
            "Epoch 00270: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.0952 - acc: 1.0000 - val_loss: 1.1368 - val_acc: 0.5500\n",
            "Epoch 271/300\n",
            "\n",
            "Epoch 00271: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.1032 - acc: 0.9667 - val_loss: 1.1368 - val_acc: 0.5500\n",
            "Epoch 272/300\n",
            "\n",
            "Epoch 00272: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.1061 - acc: 0.9833 - val_loss: 1.1708 - val_acc: 0.5500\n",
            "Epoch 273/300\n",
            "\n",
            "Epoch 00273: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.1287 - acc: 1.0000 - val_loss: 1.1434 - val_acc: 0.5500\n",
            "Epoch 274/300\n",
            "\n",
            "Epoch 00274: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.1025 - acc: 1.0000 - val_loss: 1.2546 - val_acc: 0.5500\n",
            "Epoch 275/300\n",
            "\n",
            "Epoch 00275: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.1144 - acc: 0.9833 - val_loss: 1.3342 - val_acc: 0.5500\n",
            "Epoch 276/300\n",
            "\n",
            "Epoch 00276: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.1117 - acc: 1.0000 - val_loss: 1.2228 - val_acc: 0.5500\n",
            "Epoch 277/300\n",
            "\n",
            "Epoch 00277: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.1118 - acc: 0.9833 - val_loss: 1.1696 - val_acc: 0.5500\n",
            "Epoch 278/300\n",
            "\n",
            "Epoch 00278: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.1028 - acc: 1.0000 - val_loss: 1.1184 - val_acc: 0.5500\n",
            "Epoch 279/300\n",
            "\n",
            "Epoch 00279: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.1199 - acc: 0.9833 - val_loss: 1.0367 - val_acc: 0.6000\n",
            "Epoch 280/300\n",
            "\n",
            "Epoch 00280: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.1154 - acc: 1.0000 - val_loss: 1.0031 - val_acc: 0.5500\n",
            "Epoch 281/300\n",
            "\n",
            "Epoch 00281: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.1351 - acc: 0.9667 - val_loss: 1.0193 - val_acc: 0.5500\n",
            "Epoch 282/300\n",
            "\n",
            "Epoch 00282: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.1112 - acc: 1.0000 - val_loss: 1.0641 - val_acc: 0.5500\n",
            "Epoch 283/300\n",
            "\n",
            "Epoch 00283: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.1036 - acc: 1.0000 - val_loss: 1.1293 - val_acc: 0.5500\n",
            "Epoch 284/300\n",
            "\n",
            "Epoch 00284: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.1022 - acc: 1.0000 - val_loss: 1.1645 - val_acc: 0.5500\n",
            "Epoch 285/300\n",
            "\n",
            "Epoch 00285: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.0841 - acc: 1.0000 - val_loss: 1.2019 - val_acc: 0.5500\n",
            "Epoch 286/300\n",
            "\n",
            "Epoch 00286: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.0900 - acc: 1.0000 - val_loss: 1.1715 - val_acc: 0.5500\n",
            "Epoch 287/300\n",
            "\n",
            "Epoch 00287: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.1177 - acc: 1.0000 - val_loss: 1.1381 - val_acc: 0.5500\n",
            "Epoch 288/300\n",
            "\n",
            "Epoch 00288: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.1150 - acc: 0.9833 - val_loss: 1.0224 - val_acc: 0.6500\n",
            "Epoch 289/300\n",
            "\n",
            "Epoch 00289: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.0971 - acc: 1.0000 - val_loss: 0.9146 - val_acc: 0.6500\n",
            "Epoch 290/300\n",
            "\n",
            "Epoch 00290: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.1368 - acc: 0.9833 - val_loss: 0.9001 - val_acc: 0.6500\n",
            "Epoch 291/300\n",
            "\n",
            "Epoch 00291: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.0868 - acc: 1.0000 - val_loss: 0.9474 - val_acc: 0.6500\n",
            "Epoch 292/300\n",
            "\n",
            "Epoch 00292: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.0768 - acc: 1.0000 - val_loss: 1.0271 - val_acc: 0.6500\n",
            "Epoch 293/300\n",
            "\n",
            "Epoch 00293: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.1170 - acc: 1.0000 - val_loss: 1.0654 - val_acc: 0.5500\n",
            "Epoch 294/300\n",
            "\n",
            "Epoch 00294: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.0865 - acc: 1.0000 - val_loss: 1.1019 - val_acc: 0.5500\n",
            "Epoch 295/300\n",
            "\n",
            "Epoch 00295: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.1133 - acc: 0.9833 - val_loss: 1.1313 - val_acc: 0.5500\n",
            "Epoch 296/300\n",
            "\n",
            "Epoch 00296: val_loss did not improve from 0.89041\n",
            "60/60 - 0s - loss: 0.1194 - acc: 0.9833 - val_loss: 1.0006 - val_acc: 0.6000\n",
            "Epoch 297/300\n",
            "\n",
            "Epoch 00297: val_loss improved from 0.89041 to 0.88483, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1033 - acc: 0.9833 - val_loss: 0.8848 - val_acc: 0.6500\n",
            "Epoch 298/300\n",
            "\n",
            "Epoch 00298: val_loss improved from 0.88483 to 0.83002, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1096 - acc: 1.0000 - val_loss: 0.8300 - val_acc: 0.6500\n",
            "Epoch 299/300\n",
            "\n",
            "Epoch 00299: val_loss did not improve from 0.83002\n",
            "60/60 - 0s - loss: 0.0973 - acc: 0.9833 - val_loss: 0.8932 - val_acc: 0.6500\n",
            "Epoch 300/300\n",
            "\n",
            "Epoch 00300: val_loss did not improve from 0.83002\n",
            "60/60 - 0s - loss: 0.0902 - acc: 1.0000 - val_loss: 1.0874 - val_acc: 0.5500\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "# Evaluate on test data\n",
            "40/40 [==============================] - 0s 2ms/sample - loss: 0.7048 - acc: 0.6750\n",
            "test loss, test acc: [0.7047546039451845, 0.675]\n",
            "[[0.         0.         0.         0.         0.         0.82618707]\n",
            " [0.         0.         0.         0.         0.         0.90161943]\n",
            " [0.         0.         0.         0.         0.         1.59847986]\n",
            " [0.         0.         0.         0.         0.         0.70459532]\n",
            " [0.         0.         0.         0.         0.         1.30585664]\n",
            " [0.         0.         0.         0.         0.         1.73398367]\n",
            " [0.         0.         0.         0.         0.         0.85697549]\n",
            " [0.         0.         0.         0.         0.         1.29748103]\n",
            " [0.         0.         0.         0.         0.         0.7047546 ]]\n",
            "[[0.         0.         0.         0.         0.         0.5       ]\n",
            " [0.         0.         0.         0.         0.         0.55000001]\n",
            " [0.         0.         0.         0.         0.         0.5       ]\n",
            " [0.         0.         0.         0.         0.         0.67500001]\n",
            " [0.         0.         0.         0.         0.         0.55000001]\n",
            " [0.         0.         0.         0.         0.         0.75      ]\n",
            " [0.         0.         0.         0.         0.         0.57499999]\n",
            " [0.         0.         0.         0.         0.         0.375     ]\n",
            " [0.         0.         0.         0.         0.         0.67500001]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kd6FTGs7vBI3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "df_accl_all = pd.DataFrame({'12': acc_all[:, 0], '13': acc_all[:, 1],'14': acc_all[:, 2], '23': acc_all[:, 3],'24': acc_all[:, 4], '34': acc_all[:, 5]})\n",
        "df_accl_all.to_csv (r'EEG_Deep/df_accl_all.csv', index = None, header=True) #Don't forget to add '.csv' at the end of the path\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}