{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "main_Single_Trial_Classification_PatientData",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sagihaider/EEG_Deep/blob/master/main_Single_Trial_Classification_PatientData.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X110PE9MjrlE",
        "colab_type": "code",
        "outputId": "53a19c25-523f-4c98-aa46-72769883acb2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "!git clone https://github.com/sagihaider/EEG_Deep.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'EEG_Deep'...\n",
            "remote: Enumerating objects: 3, done.\u001b[K\n",
            "remote: Counting objects: 100% (3/3), done.\u001b[K\n",
            "remote: Compressing objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 156 (delta 0), reused 0 (delta 0), pack-reused 153\u001b[K\n",
            "Receiving objects: 100% (156/156), 856.35 MiB | 15.86 MiB/s, done.\n",
            "Resolving deltas: 100% (53/53), done.\n",
            "Checking out files: 100% (57/57), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ioLzMoiS60Qf",
        "colab_type": "code",
        "outputId": "7346b418-04f3-40a7-c6c4-5fa155368bbb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 63
        }
      },
      "source": [
        "import scipy.io as spio\n",
        "import numpy as np\n",
        "from importlib.machinery import SourceFileLoader\n",
        "\n",
        "# EEGNet-specific imports\n",
        "from EEG_Deep.EEGModels import EEGNet, ShallowConvNet, DeepConvNet\n",
        "from tensorflow.keras import utils as np_utils\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras import backend as K\n",
        "K.set_image_data_format('channels_first')\n",
        "\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# tools for plotting confusion matrices\n",
        "from matplotlib import pyplot as plt\n",
        "from scipy.signal import butter, lfilter"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mo80Jg_Pn5lP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Band-pass Filter\n",
        "def butter_bandpass_filter(data, lowcut, highcut, fs, order=4):\n",
        "    nyq = 0.5 * fs\n",
        "    low = lowcut / nyq\n",
        "    high = highcut / nyq\n",
        "    b, a = butter(order, [low, high], btype='band')\n",
        "    y = lfilter(b, a, data)\n",
        "    return y\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZjtnE_6RSpum",
        "colab_type": "code",
        "outputId": "2768d9a4-2b5b-419e-a947-07c46affcf1e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from numpy import zeros\n",
        "K.clear_session()\n",
        "cols = 6\n",
        "rows = 10\n",
        "acc_all = zeros([rows, cols])\n",
        "loss_all = zeros([rows, cols])\n",
        "X_tr_c12 = np.empty([80, 12, 4096])\n",
        "X_ts_c12 = np.empty([80, 12, 4096])\n",
        "result=[]\n",
        "\n",
        "from itertools import combinations \n",
        "comb = combinations([1, 2, 3, 4], 2) \n",
        "  # Print the obtained combinations \n",
        "bincomb=[]\n",
        "for i in list(comb): \n",
        "    bincomb.append(i)\n",
        "\n",
        "for x in range(1,2):\n",
        "  fName = 'EEG_Deep/Data2A/parsed_P0' + str(x) + 'T.mat'  # Load Data\n",
        "  print(fName)\n",
        "  mat = spio.loadmat(fName)\n",
        "  r_X_tr = mat['RawEEGData']\n",
        "  r_y_tr = mat['Labels']\n",
        "\n",
        "  ### Filter Data ###\n",
        "  for t in range(r_X_tr.shape[0]):\n",
        "    tril = r_X_tr[t,:,:]\n",
        "    # tril = tril.transpose()\n",
        "    tril_filtered = butter_bandpass_filter(tril, \n",
        "                                              lowcut=4, \n",
        "                                              highcut=40, \n",
        "                                              fs=512,\n",
        "                                              order=4)\n",
        "    # tril_filtered = tril_filtered.transpose()\n",
        "    X_tr_c12[t,:,:] = tril_filtered\n",
        "\n",
        "  print(\"Filtering of Training Data Finished\")\n",
        "  ## Test Data Load \n",
        "\n",
        "  fName = 'EEG_Deep/Data2A/parsed_P0' + str(x) + 'E.mat'  # Load Data\n",
        "  print(fName)\n",
        "  mat = spio.loadmat(fName)\n",
        "  r_X_ts = mat['RawEEGData']\n",
        "  r_y_ts = mat['Labels']\n",
        "  for t in range(r_X_ts.shape[0]):\n",
        "    tril = r_X_ts[t,:,:]\n",
        "    # tril = tril.transpose()\n",
        "    tril_filtered = butter_bandpass_filter(tril, \n",
        "                                              lowcut=4, \n",
        "                                              highcut=40, \n",
        "                                              fs=512,\n",
        "                                              order=4)\n",
        "    # tril_filtered = tril_filtered.transpose()\n",
        "    X_ts_c12[t,:,:] = tril_filtered\n",
        "  \n",
        "  print(\"Filtering of Testing Data Finished\")    \n",
        "\n",
        "  for k, com in enumerate(bincomb):\n",
        "      print(com)\n",
        "      # # Find labels to specific class\n",
        "      print(\"Finding labels in training data\")\n",
        "      class1indx = list(np.where(r_y_tr == com[0]))\n",
        "      class2indx = list(np.where(r_y_tr == com[1]))\n",
        "      c1=list(class1indx[0])\n",
        "      c2=list(class2indx[0])\n",
        "      y_tr_c12 = c1 + c2\n",
        "      y_tr_c12.sort()\n",
        "      # print(y_tr_c12)\n",
        "      x_tr_12 = X_tr_c12[y_tr_c12,:,:]\n",
        "      y_tr_12 = r_y_tr[y_tr_c12]\n",
        "      # print(np.shape(x_tr_12))\n",
        "      # print(np.shape(y_tr_12))\n",
        "      # # Find labels to specific class\n",
        "      print(\"Finding labels in testing data\")\n",
        "      class1indx = list(np.where(r_y_ts == com[0]))\n",
        "      class2indx = list(np.where(r_y_ts == com[1]))\n",
        "      c1=list(class1indx[0])\n",
        "      c2=list(class2indx[0])\n",
        "      y_ts_c12 = c1 + c2\n",
        "      y_ts_c12.sort()\n",
        "      # print(y_ts_c12)\n",
        "      x_ts_12 = X_ts_c12[y_ts_c12,:,:]\n",
        "      y_ts_12 = r_y_ts[y_ts_c12]\n",
        "      # print(np.shape(x_ts_12))\n",
        "      # print(np.shape(y_ts_12))\n",
        "      del class1indx, class2indx, c1, c2\n",
        "      # split data of each subject in training and validation\n",
        "      X_train = x_tr_12[0:60,:,1792:3328]\n",
        "      Y_train = y_tr_12[0:60].ravel()\n",
        "      X_val   = x_tr_12[60:,:,1792:3328]\n",
        "      Y_val   = y_tr_12[60:].ravel()\n",
        "      print(Y_val)\n",
        "      print(np.shape(X_train))\n",
        "      print(np.shape(Y_train))\n",
        "      print(np.shape(X_val))\n",
        "      print(np.shape(Y_val))\n",
        "  \n",
        "      # convert labels to one-hot encodings.\n",
        "      Y_train      = np_utils.to_categorical(Y_train-1, num_classes=4)\n",
        "      Y_val       = np_utils.to_categorical(Y_val-1, num_classes=4)\n",
        "      print(Y_val)\n",
        "\n",
        "      kernels, chans, samples = 1, 12, 1536\n",
        "      # convert data to NCHW (trials, kernels, channels, samples) format. Data \n",
        "      # contains 22 channels and 500 time-points. Set the number of kernels to 1.\n",
        "      X_train = X_train.reshape(X_train.shape[0], kernels, chans, samples)\n",
        "      X_val   = X_val.reshape(X_val.shape[0], kernels, chans, samples)\n",
        "   \n",
        "      print('X_train shape:', X_train.shape)\n",
        "      print(X_train.shape[0], 'train samples')\n",
        "      print(X_val.shape[0], 'val samples')\n",
        "\n",
        "      X_test      = x_ts_12[:,:,1792:3328]\n",
        "      Y_test      = y_ts_12[:]\n",
        "      print(np.shape(X_test))\n",
        "      print(np.shape(Y_test))\n",
        "\n",
        "      #convert labels to one-hot encodings.\n",
        "      Y_test      = np_utils.to_categorical(Y_test-1, num_classes=4)\n",
        "\n",
        "      # convert data to NCHW (trials, kernels, channels, samples) format. Data \n",
        "      # contains 22 channels and 500 time-points. Set the number of kernels to 1.\n",
        "      X_test      = X_test.reshape(X_test.shape[0], kernels, chans, samples)\n",
        "\n",
        "      print('X_train shape:', X_test.shape)\n",
        "      print(X_test.shape[0], 'train samples')\n",
        "\n",
        "      # configure the EEGNet-8,2,16 model with kernel length of 32 samples (other \n",
        "      # model configurations may do better, but this is a good starting point)\n",
        "      model = EEGNet(nb_classes = 4, Chans = 12, Samples = 1536,\n",
        "                     dropoutRate = 0.5, kernLength = 25, F1 = 8, \n",
        "                     D = 2, F2 = 16, norm_rate = 0.25, dropoutType = 'Dropout')\n",
        "      \n",
        "      # compile the model and set the optimizers\n",
        "      model.compile(loss='categorical_crossentropy', optimizer='adam', \n",
        "                    metrics = ['accuracy'])\n",
        "\n",
        "      # count number of parameters in the model\n",
        "      numParams    = model.count_params() \n",
        "\n",
        "      # set a valid path for your system to record model checkpoints\n",
        "      checkpointer = ModelCheckpoint(filepath='/tmp/checkpoint.h5', verbose=1,\n",
        "                                     save_best_only=True)\n",
        "  \n",
        "      # the syntax is {class_1:weight_1, class_2:weight_2,...}. Here just setting\n",
        "      # the weights all to be 1\n",
        "      class_weights = {0:1, 1:1, 2:1, 3:1}\n",
        "\n",
        "      history = model.fit(X_train, Y_train, batch_size = 16, epochs = 100, \n",
        "                      verbose = 2, validation_data=(X_val, Y_val),\n",
        "                      callbacks=[checkpointer], class_weight = class_weights)\n",
        "  \n",
        "      # Plot training & validation accuracy values\n",
        "      plt.plot(history.history['acc'])\n",
        "      plt.plot(history.history['val_acc'])\n",
        "      plt.title('Model accuracy')\n",
        "      plt.ylabel('Accuracy')\n",
        "      plt.xlabel('Epoch')\n",
        "      plt.legend(['Train', 'Test'], loc='upper left')\n",
        "      plt.show()\n",
        "      figName = 'Accuracy_A0' + str(x) + '.png'  \n",
        "      plt.savefig(figName)\n",
        "\n",
        "      print('\\n# Evaluate on test data')\n",
        "      results = model.evaluate(X_test, Y_test, batch_size=1)\n",
        "      print('test loss, test acc:', results)\n",
        "\n",
        "      loss_all[x - 1, k-1] = results[0]\n",
        "      acc_all[x - 1, k-1] = results[1]\n",
        "\n",
        "      from keras import backend as K \n",
        "      # Do some code, e.g. train and save model\n",
        "      K.clear_session()\n",
        "\n",
        "\n",
        "print(loss_all)\n",
        "print(acc_all)\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "EEG_Deep/Data2A/parsed_P01T.mat\n",
            "Filtering of Training Data Finished\n",
            "EEG_Deep/Data2A/parsed_P01E.mat\n",
            "Filtering of Testing Data Finished\n",
            "(1, 2)\n",
            "Finding labels in training data\n",
            "Finding labels in testing data\n",
            "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
            "(60, 12, 1536)\n",
            "(60,)\n",
            "(20, 12, 1536)\n",
            "(20,)\n",
            "[[1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]]\n",
            "X_train shape: (60, 1, 12, 1536)\n",
            "60 train samples\n",
            "20 val samples\n",
            "(40, 12, 1536)\n",
            "(40, 1)\n",
            "X_train shape: (40, 1, 12, 1536)\n",
            "40 train samples\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "Train on 60 samples, validate on 20 samples\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 1.36211, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 4s - loss: 1.3721 - acc: 0.4000 - val_loss: 1.3621 - val_acc: 0.0000e+00\n",
            "Epoch 2/100\n",
            "\n",
            "Epoch 00002: val_loss improved from 1.36211 to 1.35176, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 1.0280 - acc: 0.6667 - val_loss: 1.3518 - val_acc: 0.0000e+00\n",
            "Epoch 3/100\n",
            "\n",
            "Epoch 00003: val_loss improved from 1.35176 to 1.35158, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.8848 - acc: 0.6667 - val_loss: 1.3516 - val_acc: 0.0000e+00\n",
            "Epoch 4/100\n",
            "\n",
            "Epoch 00004: val_loss improved from 1.35158 to 1.35094, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.8144 - acc: 0.6667 - val_loss: 1.3509 - val_acc: 0.0000e+00\n",
            "Epoch 5/100\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 1.35094\n",
            "60/60 - 0s - loss: 0.7524 - acc: 0.6667 - val_loss: 1.3534 - val_acc: 0.0000e+00\n",
            "Epoch 6/100\n",
            "\n",
            "Epoch 00006: val_loss improved from 1.35094 to 1.34311, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.7425 - acc: 0.6667 - val_loss: 1.3431 - val_acc: 0.0000e+00\n",
            "Epoch 7/100\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 1.34311\n",
            "60/60 - 0s - loss: 0.7030 - acc: 0.6667 - val_loss: 1.3435 - val_acc: 0.0000e+00\n",
            "Epoch 8/100\n",
            "\n",
            "Epoch 00008: val_loss improved from 1.34311 to 1.33380, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.7091 - acc: 0.6667 - val_loss: 1.3338 - val_acc: 0.0000e+00\n",
            "Epoch 9/100\n",
            "\n",
            "Epoch 00009: val_loss improved from 1.33380 to 1.32402, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6680 - acc: 0.6667 - val_loss: 1.3240 - val_acc: 0.0000e+00\n",
            "Epoch 10/100\n",
            "\n",
            "Epoch 00010: val_loss improved from 1.32402 to 1.31123, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6649 - acc: 0.6667 - val_loss: 1.3112 - val_acc: 0.0000e+00\n",
            "Epoch 11/100\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 1.31123\n",
            "60/60 - 0s - loss: 0.6579 - acc: 0.6667 - val_loss: 1.3122 - val_acc: 0.0000e+00\n",
            "Epoch 12/100\n",
            "\n",
            "Epoch 00012: val_loss improved from 1.31123 to 1.31020, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6346 - acc: 0.6667 - val_loss: 1.3102 - val_acc: 0.0000e+00\n",
            "Epoch 13/100\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 1.31020\n",
            "60/60 - 0s - loss: 0.6214 - acc: 0.6667 - val_loss: 1.3198 - val_acc: 0.0000e+00\n",
            "Epoch 14/100\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 1.31020\n",
            "60/60 - 0s - loss: 0.6007 - acc: 0.6667 - val_loss: 1.3416 - val_acc: 0.0000e+00\n",
            "Epoch 15/100\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 1.31020\n",
            "60/60 - 0s - loss: 0.6055 - acc: 0.7167 - val_loss: 1.3511 - val_acc: 0.0000e+00\n",
            "Epoch 16/100\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 1.31020\n",
            "60/60 - 0s - loss: 0.6091 - acc: 0.7167 - val_loss: 1.3538 - val_acc: 0.0000e+00\n",
            "Epoch 17/100\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 1.31020\n",
            "60/60 - 0s - loss: 0.5952 - acc: 0.7000 - val_loss: 1.3807 - val_acc: 0.0000e+00\n",
            "Epoch 18/100\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 1.31020\n",
            "60/60 - 0s - loss: 0.5782 - acc: 0.7500 - val_loss: 1.3787 - val_acc: 0.0000e+00\n",
            "Epoch 19/100\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 1.31020\n",
            "60/60 - 0s - loss: 0.5247 - acc: 0.8000 - val_loss: 1.4025 - val_acc: 0.0000e+00\n",
            "Epoch 20/100\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 1.31020\n",
            "60/60 - 0s - loss: 0.5284 - acc: 0.7833 - val_loss: 1.4293 - val_acc: 0.0000e+00\n",
            "Epoch 21/100\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 1.31020\n",
            "60/60 - 0s - loss: 0.5162 - acc: 0.8333 - val_loss: 1.4575 - val_acc: 0.0000e+00\n",
            "Epoch 22/100\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 1.31020\n",
            "60/60 - 0s - loss: 0.5012 - acc: 0.8500 - val_loss: 1.4921 - val_acc: 0.0000e+00\n",
            "Epoch 23/100\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 1.31020\n",
            "60/60 - 0s - loss: 0.5054 - acc: 0.8500 - val_loss: 1.5146 - val_acc: 0.0000e+00\n",
            "Epoch 24/100\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 1.31020\n",
            "60/60 - 0s - loss: 0.4611 - acc: 0.8833 - val_loss: 1.5415 - val_acc: 0.0000e+00\n",
            "Epoch 25/100\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 1.31020\n",
            "60/60 - 0s - loss: 0.5073 - acc: 0.8667 - val_loss: 1.5929 - val_acc: 0.0000e+00\n",
            "Epoch 26/100\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 1.31020\n",
            "60/60 - 0s - loss: 0.4778 - acc: 0.8667 - val_loss: 1.6249 - val_acc: 0.0000e+00\n",
            "Epoch 27/100\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 1.31020\n",
            "60/60 - 0s - loss: 0.4537 - acc: 0.8500 - val_loss: 1.6762 - val_acc: 0.0000e+00\n",
            "Epoch 28/100\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 1.31020\n",
            "60/60 - 0s - loss: 0.4357 - acc: 0.8833 - val_loss: 1.7405 - val_acc: 0.0000e+00\n",
            "Epoch 29/100\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 1.31020\n",
            "60/60 - 0s - loss: 0.4239 - acc: 0.9000 - val_loss: 1.7889 - val_acc: 0.0000e+00\n",
            "Epoch 30/100\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 1.31020\n",
            "60/60 - 0s - loss: 0.4251 - acc: 0.9000 - val_loss: 1.8335 - val_acc: 0.0000e+00\n",
            "Epoch 31/100\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 1.31020\n",
            "60/60 - 0s - loss: 0.4054 - acc: 0.9167 - val_loss: 1.8788 - val_acc: 0.0000e+00\n",
            "Epoch 32/100\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 1.31020\n",
            "60/60 - 0s - loss: 0.3870 - acc: 0.9167 - val_loss: 1.9333 - val_acc: 0.0000e+00\n",
            "Epoch 33/100\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 1.31020\n",
            "60/60 - 0s - loss: 0.3878 - acc: 0.9500 - val_loss: 2.0147 - val_acc: 0.0000e+00\n",
            "Epoch 34/100\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 1.31020\n",
            "60/60 - 0s - loss: 0.3827 - acc: 0.9500 - val_loss: 2.0803 - val_acc: 0.0000e+00\n",
            "Epoch 35/100\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 1.31020\n",
            "60/60 - 0s - loss: 0.4013 - acc: 0.9500 - val_loss: 2.1945 - val_acc: 0.0000e+00\n",
            "Epoch 36/100\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 1.31020\n",
            "60/60 - 0s - loss: 0.3534 - acc: 0.9667 - val_loss: 2.2808 - val_acc: 0.0000e+00\n",
            "Epoch 37/100\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 1.31020\n",
            "60/60 - 0s - loss: 0.3274 - acc: 0.9667 - val_loss: 2.3704 - val_acc: 0.0000e+00\n",
            "Epoch 38/100\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 1.31020\n",
            "60/60 - 0s - loss: 0.3168 - acc: 0.9833 - val_loss: 2.4641 - val_acc: 0.0000e+00\n",
            "Epoch 39/100\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 1.31020\n",
            "60/60 - 0s - loss: 0.3715 - acc: 0.9333 - val_loss: 2.5470 - val_acc: 0.0000e+00\n",
            "Epoch 40/100\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 1.31020\n",
            "60/60 - 0s - loss: 0.3279 - acc: 0.9333 - val_loss: 2.6157 - val_acc: 0.0000e+00\n",
            "Epoch 41/100\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 1.31020\n",
            "60/60 - 0s - loss: 0.3147 - acc: 0.9500 - val_loss: 2.6590 - val_acc: 0.0000e+00\n",
            "Epoch 42/100\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 1.31020\n",
            "60/60 - 0s - loss: 0.3219 - acc: 0.9500 - val_loss: 2.7419 - val_acc: 0.0000e+00\n",
            "Epoch 43/100\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 1.31020\n",
            "60/60 - 0s - loss: 0.3114 - acc: 0.9667 - val_loss: 2.8932 - val_acc: 0.0000e+00\n",
            "Epoch 44/100\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 1.31020\n",
            "60/60 - 0s - loss: 0.3012 - acc: 0.9667 - val_loss: 2.9976 - val_acc: 0.0000e+00\n",
            "Epoch 45/100\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 1.31020\n",
            "60/60 - 0s - loss: 0.3070 - acc: 0.9667 - val_loss: 3.0995 - val_acc: 0.0000e+00\n",
            "Epoch 46/100\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 1.31020\n",
            "60/60 - 0s - loss: 0.2924 - acc: 0.9833 - val_loss: 3.2096 - val_acc: 0.0000e+00\n",
            "Epoch 47/100\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 1.31020\n",
            "60/60 - 0s - loss: 0.3012 - acc: 0.9500 - val_loss: 3.3517 - val_acc: 0.0000e+00\n",
            "Epoch 48/100\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 1.31020\n",
            "60/60 - 0s - loss: 0.2819 - acc: 0.9667 - val_loss: 3.4351 - val_acc: 0.0000e+00\n",
            "Epoch 49/100\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 1.31020\n",
            "60/60 - 0s - loss: 0.3099 - acc: 0.9500 - val_loss: 3.4893 - val_acc: 0.0000e+00\n",
            "Epoch 50/100\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 1.31020\n",
            "60/60 - 0s - loss: 0.2719 - acc: 1.0000 - val_loss: 3.5156 - val_acc: 0.0000e+00\n",
            "Epoch 51/100\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 1.31020\n",
            "60/60 - 0s - loss: 0.2805 - acc: 0.9667 - val_loss: 3.5949 - val_acc: 0.0000e+00\n",
            "Epoch 52/100\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 1.31020\n",
            "60/60 - 0s - loss: 0.2629 - acc: 0.9667 - val_loss: 3.6781 - val_acc: 0.0000e+00\n",
            "Epoch 53/100\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 1.31020\n",
            "60/60 - 0s - loss: 0.2734 - acc: 0.9667 - val_loss: 3.8025 - val_acc: 0.0000e+00\n",
            "Epoch 54/100\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 1.31020\n",
            "60/60 - 0s - loss: 0.2666 - acc: 0.9833 - val_loss: 3.8851 - val_acc: 0.0000e+00\n",
            "Epoch 55/100\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 1.31020\n",
            "60/60 - 0s - loss: 0.2784 - acc: 0.9333 - val_loss: 3.9339 - val_acc: 0.0000e+00\n",
            "Epoch 56/100\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 1.31020\n",
            "60/60 - 0s - loss: 0.2593 - acc: 0.9667 - val_loss: 3.9241 - val_acc: 0.0000e+00\n",
            "Epoch 57/100\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 1.31020\n",
            "60/60 - 0s - loss: 0.2474 - acc: 0.9667 - val_loss: 3.9706 - val_acc: 0.0000e+00\n",
            "Epoch 58/100\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 1.31020\n",
            "60/60 - 0s - loss: 0.2696 - acc: 0.9667 - val_loss: 3.9233 - val_acc: 0.0500\n",
            "Epoch 59/100\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 1.31020\n",
            "60/60 - 0s - loss: 0.2617 - acc: 0.9667 - val_loss: 3.8549 - val_acc: 0.0500\n",
            "Epoch 60/100\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 1.31020\n",
            "60/60 - 0s - loss: 0.2734 - acc: 0.9667 - val_loss: 3.8720 - val_acc: 0.0500\n",
            "Epoch 61/100\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 1.31020\n",
            "60/60 - 0s - loss: 0.2467 - acc: 0.9833 - val_loss: 3.8956 - val_acc: 0.0500\n",
            "Epoch 62/100\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 1.31020\n",
            "60/60 - 0s - loss: 0.2264 - acc: 1.0000 - val_loss: 3.8798 - val_acc: 0.0500\n",
            "Epoch 63/100\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 1.31020\n",
            "60/60 - 0s - loss: 0.2444 - acc: 0.9833 - val_loss: 3.9138 - val_acc: 0.0500\n",
            "Epoch 64/100\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 1.31020\n",
            "60/60 - 0s - loss: 0.2137 - acc: 0.9667 - val_loss: 3.8962 - val_acc: 0.1500\n",
            "Epoch 65/100\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 1.31020\n",
            "60/60 - 0s - loss: 0.2181 - acc: 0.9833 - val_loss: 3.9166 - val_acc: 0.1500\n",
            "Epoch 66/100\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 1.31020\n",
            "60/60 - 0s - loss: 0.2408 - acc: 0.9500 - val_loss: 3.9162 - val_acc: 0.1500\n",
            "Epoch 67/100\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 1.31020\n",
            "60/60 - 0s - loss: 0.2019 - acc: 0.9833 - val_loss: 3.9377 - val_acc: 0.1500\n",
            "Epoch 68/100\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 1.31020\n",
            "60/60 - 0s - loss: 0.2150 - acc: 0.9667 - val_loss: 3.9143 - val_acc: 0.1500\n",
            "Epoch 69/100\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 1.31020\n",
            "60/60 - 0s - loss: 0.2406 - acc: 0.9667 - val_loss: 3.9807 - val_acc: 0.1500\n",
            "Epoch 70/100\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 1.31020\n",
            "60/60 - 0s - loss: 0.2312 - acc: 0.9667 - val_loss: 3.9542 - val_acc: 0.1500\n",
            "Epoch 71/100\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 1.31020\n",
            "60/60 - 0s - loss: 0.1786 - acc: 0.9833 - val_loss: 3.9861 - val_acc: 0.1500\n",
            "Epoch 72/100\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 1.31020\n",
            "60/60 - 0s - loss: 0.2108 - acc: 1.0000 - val_loss: 3.9698 - val_acc: 0.1500\n",
            "Epoch 73/100\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 1.31020\n",
            "60/60 - 0s - loss: 0.1844 - acc: 1.0000 - val_loss: 3.9702 - val_acc: 0.1500\n",
            "Epoch 74/100\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 1.31020\n",
            "60/60 - 0s - loss: 0.2178 - acc: 0.9667 - val_loss: 3.9183 - val_acc: 0.2500\n",
            "Epoch 75/100\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 1.31020\n",
            "60/60 - 0s - loss: 0.2094 - acc: 0.9833 - val_loss: 3.8564 - val_acc: 0.2500\n",
            "Epoch 76/100\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 1.31020\n",
            "60/60 - 0s - loss: 0.1847 - acc: 0.9833 - val_loss: 3.8295 - val_acc: 0.3000\n",
            "Epoch 77/100\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 1.31020\n",
            "60/60 - 0s - loss: 0.2238 - acc: 0.9833 - val_loss: 3.8070 - val_acc: 0.3000\n",
            "Epoch 78/100\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 1.31020\n",
            "60/60 - 0s - loss: 0.1759 - acc: 0.9833 - val_loss: 3.7869 - val_acc: 0.3000\n",
            "Epoch 79/100\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 1.31020\n",
            "60/60 - 0s - loss: 0.1843 - acc: 1.0000 - val_loss: 3.7876 - val_acc: 0.3000\n",
            "Epoch 80/100\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 1.31020\n",
            "60/60 - 0s - loss: 0.1901 - acc: 0.9833 - val_loss: 3.7472 - val_acc: 0.3000\n",
            "Epoch 81/100\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 1.31020\n",
            "60/60 - 0s - loss: 0.1990 - acc: 0.9833 - val_loss: 3.7466 - val_acc: 0.3000\n",
            "Epoch 82/100\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 1.31020\n",
            "60/60 - 0s - loss: 0.1808 - acc: 1.0000 - val_loss: 3.7210 - val_acc: 0.3000\n",
            "Epoch 83/100\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 1.31020\n",
            "60/60 - 0s - loss: 0.1610 - acc: 1.0000 - val_loss: 3.7492 - val_acc: 0.3000\n",
            "Epoch 84/100\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 1.31020\n",
            "60/60 - 0s - loss: 0.2066 - acc: 0.9833 - val_loss: 3.7951 - val_acc: 0.3500\n",
            "Epoch 85/100\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 1.31020\n",
            "60/60 - 0s - loss: 0.1823 - acc: 1.0000 - val_loss: 3.8149 - val_acc: 0.4000\n",
            "Epoch 86/100\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 1.31020\n",
            "60/60 - 0s - loss: 0.2010 - acc: 1.0000 - val_loss: 3.8481 - val_acc: 0.4000\n",
            "Epoch 87/100\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 1.31020\n",
            "60/60 - 0s - loss: 0.1844 - acc: 0.9667 - val_loss: 3.8623 - val_acc: 0.4000\n",
            "Epoch 88/100\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 1.31020\n",
            "60/60 - 0s - loss: 0.1690 - acc: 0.9833 - val_loss: 3.8338 - val_acc: 0.4000\n",
            "Epoch 89/100\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 1.31020\n",
            "60/60 - 0s - loss: 0.1963 - acc: 0.9833 - val_loss: 3.7452 - val_acc: 0.4000\n",
            "Epoch 90/100\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 1.31020\n",
            "60/60 - 0s - loss: 0.1935 - acc: 0.9667 - val_loss: 3.6512 - val_acc: 0.4000\n",
            "Epoch 91/100\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 1.31020\n",
            "60/60 - 0s - loss: 0.1824 - acc: 0.9833 - val_loss: 3.5584 - val_acc: 0.4000\n",
            "Epoch 92/100\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 1.31020\n",
            "60/60 - 0s - loss: 0.1969 - acc: 0.9833 - val_loss: 3.4784 - val_acc: 0.4000\n",
            "Epoch 93/100\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 1.31020\n",
            "60/60 - 0s - loss: 0.1928 - acc: 0.9667 - val_loss: 3.3926 - val_acc: 0.4000\n",
            "Epoch 94/100\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 1.31020\n",
            "60/60 - 0s - loss: 0.1550 - acc: 0.9833 - val_loss: 3.3603 - val_acc: 0.4000\n",
            "Epoch 95/100\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 1.31020\n",
            "60/60 - 0s - loss: 0.2110 - acc: 0.9667 - val_loss: 3.3136 - val_acc: 0.4500\n",
            "Epoch 96/100\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 1.31020\n",
            "60/60 - 0s - loss: 0.1471 - acc: 1.0000 - val_loss: 3.3702 - val_acc: 0.4500\n",
            "Epoch 97/100\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 1.31020\n",
            "60/60 - 0s - loss: 0.1835 - acc: 0.9833 - val_loss: 3.4256 - val_acc: 0.4000\n",
            "Epoch 98/100\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 1.31020\n",
            "60/60 - 0s - loss: 0.1809 - acc: 1.0000 - val_loss: 3.3490 - val_acc: 0.4000\n",
            "Epoch 99/100\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 1.31020\n",
            "60/60 - 0s - loss: 0.1481 - acc: 1.0000 - val_loss: 3.3254 - val_acc: 0.4500\n",
            "Epoch 100/100\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 1.31020\n",
            "60/60 - 0s - loss: 0.1773 - acc: 0.9833 - val_loss: 3.2938 - val_acc: 0.5000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3deXhU5dn48e+dPYGEAIGwE3YIIAJh\nV3GhCGKlrSuKCoKpvrXaWtvSvr5qre2r/XVz4ZWioIgoitYWCyq4oIjsiywhQECWsIQkQBIIWSZ5\nfn+ckzAJWQbImZnM3J/rmitztjn3cfDc8yznecQYg1JKqeAV4usAlFJK+ZYmAqWUCnKaCJRSKshp\nIlBKqSCniUAppYKcJgKllApymghUUBCRJBExIhLmwb5TRORrb8SllD/QRKD8jojsF5ESEUmotn6z\nfTNP8k1kSgUmTQTKX30HTKpYEJH+QIzvwvEPnpRolLpQmgiUv5oP3OO2fC/whvsOItJMRN4QkWwR\nOSAij4tIiL0tVET+LCI5IrIPmFDDsXNE5KiIHBaRZ0Qk1JPARGSRiBwTkTwR+UpE+rptixaRv9jx\n5InI1yISbW+7QkS+EZFTInJIRKbY61eIyHS3z6hSNWWXgn4iInuAPfa65+3PyBeRjSJypdv+oSLy\nWxHZKyIF9vaOIjJTRP5S7VoWi8jPPbluFbg0ESh/tQaIE5E+9g36DuDNavu8CDQDugKjsRLHVHvb\n/cCNwEAgBbil2rGvAy6gu73PWGA6nvkI6AG0BjYBC9y2/RkYDIwEWgC/AspFpLN93ItAK+ByYIuH\n5wP4ATAMSLaX19uf0QJ4C1gkIlH2tkexSlM3AHHAfUAhMA+Y5JYsE4Ax9vEqmBlj9KUvv3oB+7Fu\nUI8D/wuMA5YDYYABkoBQoARIdjvux8AK+/3nwANu28bax4YBiUAxEO22fRLwhf1+CvC1h7HG25/b\nDOuH1VlgQA37/Qb4oJbPWAFMd1uucn7786+tJ46TFecFdgETa9lvJ/A9+/1DwFJff9/68v1L6xuV\nP5sPfAV0oVq1EJAAhAMH3NYdANrb79sBh6ptq9DZPvaoiFSsC6m2f43s0skfgFuxftmXu8UTCUQB\ne2s4tGMt6z1VJTYReQyYhnWdBuuXf0Xjel3nmgdMxkqsk4HnLyEmFSC0akj5LWPMAaxG4xuAf1bb\nnAOUYt3UK3QCDtvvj2LdEN23VTiEVSJIMMbE2684Y0xf6ncnMBGrxNIMq3QCIHZMRUC3Go47VMt6\ngDNUbQhvU8M+lcME2+0BvwJuA5obY+KBPDuG+s71JjBRRAYAfYB/1bKfCiKaCJS/m4ZVLXLGfaUx\npgx4F/iDiMTadfCPcq4d4V3gYRHpICLNgRluxx4FlgF/EZE4EQkRkW4iMtqDeGKxkkgu1s37j26f\nWw7MBf4qIu3sRtsRIhKJ1Y4wRkRuE5EwEWkpIpfbh24BfiQiMSLS3b7m+mJwAdlAmIg8gVUiqPAq\n8HsR6SGWy0SkpR1jJlb7wnzgfWPMWQ+uWQU4TQTKrxlj9hpjNtSy+adYv6b3AV9jNXrOtbe9AnwC\nfIvVoFu9RHEPEAGkYdWvvwe09SCkN7CqmQ7bx66ptv0xYBvWzfYE8BwQYow5iFWy+YW9fgswwD7m\nb1jtHVlYVTcLqNsnwMfAbjuWIqpWHf0VKxEuA/KBOUC02/Z5QH+sZKAUYoxOTKNUMBGRq7BKTp2N\n3gAUWiJQKqiISDjwCPCqJgFVQROBUkFCRPoAp7CqwP7u43CUH9GqIaWUCnJaIlBKqSDX6B4oS0hI\nMElJSb4OQymlGpWNGzfmGGNa1bSt0SWCpKQkNmyorTehUkqpmojIgdq2adWQUkoFOU0ESikV5DQR\nKKVUkGt0bQQ1KS0tJTMzk6KiIl+H4jVRUVF06NCB8PBwX4eilGrkAiIRZGZmEhsbS1JSEm7DCgcs\nYwy5ublkZmbSpUsXX4ejlGrkHKsaEpG5InJcRLbXsl1E5AURyRCRrSIy6GLPVVRURMuWLYMiCQCI\nCC1btgyqEpBSyjlOthG8jjWzVG3GY0331wNIBV6+lJMFSxKoEGzXq5RyjmNVQ8aYr0QkqY5dJgJv\n2ANfrRGReBFpa48Vr1Sj8c3eHJpGhnFZh3hfh1Jp6bajdG/dlJ6JsV49b1FpGQvWHiSvsASA0JAQ\n7hreiYSmkZf0uVszT5FdUMx1fRJr3edo3lkWbcjEVWZNGte8SQR3DetMRNi537tnS8pYsPYA+WdL\nAQgLDeHu4Z1p3iTikuK7UHmFpSxLO8YPB7YnLPRcfGeKXSxYe4DTRS4AIsNDuXNoJ8fj82UbQXuq\njqGeaa87LxGISCpWqYFOnTpV3+xzubm5XHfddQAcO3aM0NBQWrWyHuBbt24dERH1f4lTp05lxowZ\n9OrVy9FYVcNylZXz0FubiYkIZcVjV1f5n9pXXl25j2eW7KRZdDjv/ngEvdp4Jxm4ysp5+O3NLEvL\noqLAagxkFRTxxx/2v+jP3XjgJJNfXcvZ0jL+fOsAbhnc4bx9jucXccfsNRzILaxy7k0HT/H32y8n\nNEQocZXz4IKNrNiVXWWfk4UlPPl9Tyanazh/+iSdBWsPsva7E/zp5ssICRGKSstInb+BVRm5VeL7\nePsx3rp/GLFRznUMaRSNxcaY2cBsgJSUFL8bJa9ly5Zs2bIFgKeeeoqmTZvy2GOPVdmnYpLokJCa\nbxSvvfaa43GqhrfxwElOnCnhxBlYsu0oEy9vX/9BDnp3wyGeWbKTa3q1YseRfO6es5b3HhhJp5Yx\n9R98CcrLDTP+uY1laVk89f1kpoyyOjH89oNtvLcxk5+N6UHr2KgL/tydR/OZ+to6EuMiadMsil+/\nv5XYqDCu73tuNs+8wlLumbuO7IJi/vlfIxnUqTkAL6/Yy3MfpxMXFcbTE/vx6LtbWLErm//9UX8m\nDbV+UD767hYWrjvEw9f28Fqp4HhBEYs2ZtKheTTvbcwkLiqc397Qm4ff3syqjFz+cusAbraT3efp\nWaS+sZHp8zYw776hRIWHOhKTL3++HKbqnLIdODffbEDIyMggOTmZu+66i759+3L06FFSU1NJSUmh\nb9++PP3005X7XnHFFWzZsgWXy0V8fDwzZsxgwIABjBgxguPHj/vwKvxLsauMgqJSR8+RXVCMp6Py\nLk/LIiI0hC4JTXh5xV6PjisrN5w8U3KpYZ7n4+1HmfH+Vq7skcCsuwfz5vRhlJSVM3nOWo7nn9+x\noKi0jPwa/lu6ysovKD5jDM8s2cl7GzP5+ZielUkAIPXKrrjKynlt1f4qxxzPL2LLoVN1vlZl5HDP\n3HXERIQxf9owXr13CP3aN+Onb21m8bdH2HLoFJsPnmTq6+vYl32G2XenVCYBgAev7sYDo7uxYO1B\nbnh+Jf/ZepQZ43tXJgGAB0Z342xpGfNWV40vyy2+bw+dosRVft51FxSVVok3r/D8/5alZeXknC6u\nsu71VfspLStn/rRhTB2VxNxV3zHhha9ZlpbFk99PrkwCANf2TuQvtw1g3f4T/GTBJkrLzo+jIfiy\nRLAYeEhEFgLDgLyGaB/43Yc7SDuSf8nBuUtuF3fRRcf09HTeeOMNUlJSAHj22Wdp0aIFLpeLa665\nhltuuYXk5OQqx+Tl5TF69GieffZZHn30UebOncuMGTNq+vigcrygiNtmrSY+JoJ//WSUI+f4YHMm\nj777LU/cmMzUUXV3zTXGsCwti1HdWzLhsnY8tuhbVuzO5pperWs9pthVxrTXN7DtcB7fzLiWJpEN\n87/g13tyePjtLQzoGM+syYOJDAulZ2Isr08dyp2vrGHynLW8++MRxMdYv3qzC4q57R+rCQsRPv7Z\nVYSGnOt88NSHO3hvYybzpw1jSFKLes/9wmcZzF31HfeN6sLD13Wvsi0poQnj+7flzdUHePDqbsRF\nhbN+/wnunrOWotL6b2rNY8J568cj6NjCKtHMmzqE2/6xmoff3ly5T4jA/901iCt6JJx3/K/H9SLv\nbClvrztYmRjc9UyMZUyf1sz7Zj+pV3UlJiKMVRk5TH1tPSVuN93LO8azYPqwyu8r82Qht85azdG8\ncwk2oWkEix4YSZeEJoCVaO+du44th04xf9owhnZpQUFRKfPXHGB8vzZ0SWjC/0xIJv+si/c3WaWm\nmv7NTby8PQVFLh7/13ZeWbmP/7q6+3n7XCrHEoGIvA1cDSSISCbwJBAOYIyZBSzFmsM1AygEpjoV\niy9169atMgkAvP3228yZMweXy8WRI0dIS0s7LxFER0czfvx4AAYPHszKlSu9GrM/yiss5Z4569if\nWwi5hRzMLWzw6o7laVk8tmgrAsz6ci93DutEZFjtRfHdWac5eKKQB0Z346YB7fjLsl3MWrG31kRQ\nUYf+dUYOACv3ZDOunyfTJNdt08GTpM7fQNdWTXh9ytAqyeXyjvG8ek8KU15bz5TX1rNg+jBc5YZ7\n5q7jQO4Zyg18suMYN/S34jiWV8Q76w9RVm647/X1LEwdTt92zWo992urvuNvn+7mlsEdeHxCnxp7\nsz04uhtLth7lrbUHuaJ7Ave9tp528dH8dnyfKgmoJsnt4kiMO1elFB8TwfsPjmTDgZNgF77aN4+u\ntVFcRPjDD/pxz4jO9K6lreTBq7tx88ureWf9IS7vGM/9b2ygS0ITfjWuFyEiHDpZyO8+TCN1/gbm\n3DuEgiIXd89Zx5liFy9MGkhsZBhnS8t4/F/bmfzqWt57cAQJTSP5yYJNrNt/gjZxUUx7fT1vpw5n\nVUYOBUWuyoQUEiL86ZbLmH5ll1rjA5g8vDMtmkRwbe/af2RcCid7DU2qZ7sBftLQ5/V2o099mjRp\nUvl+z549PP/886xbt474+HgmT55c47MA7o3LoaGhuFwur8TqrwpLXJXF/z/+sD+//WAby9KOMf3K\nrg12jm/25vCTtzbRr30zHriqKw8u2MS/Nx/htiEdaz1m2Y5jiMCY5NZEhIUw7YouPLNkJ5sOnqxS\nRQHn6tA/2ZHF4xP68OLnGSxLy7rkRJB+LJ8pc9fRKjaSN6YNpVnM+Q2KI7sn8OKdA/mvBZtInb+B\n4tJyMo4XMGfKEJ7+MI2XV+xlfL82iAhzV31HuYG37h/Oo+9s4Z4561j0wAi6tmp63ue+tzGT332Y\nxvV9E3n2R/0JqeWm3q99M67onsCrK/fxylf7iIsO581pw2gXH31R1xwbFV5nqau6kBChT9u4WrcP\n7tyCIUnNmfXlXopKy0loGsn8aUNp7ZaAmkSE8YtF3/LQW5s5cuosx/KKeHP6UAZ3Pldi6tg8hkmv\nrGHyq2vp3TaOz9KP88wP+nFt79bcOms198xdR4gIo7q3rNLDLLSe+CpUJGsnNIrG4kCRn59PbGws\ncXFxHD16lE8++YRx4+p61CI4nSl28dBbm/g2Mw+AElc5hSUu/u+uQYzr15Z53+xneVrWBSeCT3Yc\n43eLd1BUQ31v/tlS+xf1EOJjwkluG8esr/Zyy+AOhIQIOaeL+a83N3F171aVRfNlaVkM7Bhf2Qg6\naWgnXvw8g7teWUt0RNWSRFm5Ie9sKT8b04PpV3Yl7Ug+n+08jqusvLKn0eq9uTyycDOucs/7Q5wu\ndtE8xrqx1tUYe33fNvzp5sv4xaJvCRF46c5BXNOrNcfyivjNP7fxzd5c+rVrxoI1B5jQvy3Du7Zk\n/vRh3DZrNeOfX1ljFdbJwhJGdW/J83cMrLe31INXd+OuV9eS0DSC+dOGXnQScMqDV3fjvtc30Do2\nkgXTh1VJAgA3D+5AflEpv/swjfBQ4dV7h1RJAgD9OzRjzr0p3DN3HXuzz/DL63sxeXhnAN6cPoxb\nZ31Dzuli/j76cq9dl6c0EXjRoEGDSE5Opnfv3nTu3JlRo5yp527Mil1l/Hj+Rr7Zm8OtgztW9gG/\npncrru1t9SEf2zeRmV9kcOJMCS087Onx9Z4cfvrWZrq1bsp1nZuftz06IpTpV3Sp7DnywNXdKrtC\njuzeknvnrmPHkXzW7T9BZFgo4/u1YdvhPH49rnflZzSJDOOvtw1gxa7sGmPo1SaWu4Z1qryGf24+\nzLr9JxjZzarb/tvy3RhgwgX88gsLFe4e3rmyDr0uNw/uQExEKBFhIZX98X84sD1/Xb6bl1fsZUS3\nlpwpKaustujWqilvpw7nrbUHKashOcXHhPPA6G4e9WQZ2a0lv/9BP0Z0bVFj6cLXrunVmqcn9uWK\n7gm1/recOqoL8THhtI6NYlT389sjAIZ1bcn8acP4Luc0t6WcK012SWjCwtThrN53glHdWzpyDZei\n0c1ZnJKSYqpPTLNz50769Onjo4h8J9Cuu6JP/sc7jtXaXxxgW2Ye33/p6zr3cbfpoNUPvVOLGN5J\nHVFj9UlNsVz7ly+JjwknKiyUTQdP8o+7B7NoQyYf7zjGlT0SWLknh89+MZpuF3FjKyxxMfDp5dw5\nrBNPfr8vGw+c4OaXV/PEjcncd4V3x4+a9eVenv0onaaRYQzu3Jx59w316vmVd4jIRmNMSk3btESg\nvKqs3PDnZbvILig+b1vmyULW7DvBEzcm13mD79c+jrbNoli241iN+20/nMf81Qcos3/kLE/LqrMO\nvSZhoSHcf1VX/udf2xGBF+4YyHV9ErmiRwLT521g5Z4curZqclFJACAmIowruiewbEcWT9yYzMsr\n9hEfE84dQ2tvk3DKncM6MfPzDAqKXef1qlHBQROB8qqPtx/j5RV7SYyLJKyGh+tmjO9d7y9iEeF7\nyYm8u+EQZ0vKqtTH7zyaz52vrKHcQLNo66bfJaEJL04aeMEPNN06uAOf78xiwmXt+P6AdgBEhoXy\nj7sH88v3tjK6Z43Tv3psbN9EPks/zuJvj/Dpziweua4HMRHe/18yLiqcX4ztybeZeQzvWn93URV4\nNBEorzHG8PKXGXRJaMKnj46ut+tgXcYmt+GN1Qf4OiOH7yVb9d37c85w9xzrAaRFD4zwqN68LlHh\nobw29fxqkpiIMGbeedGD5Va6rk8iItuY8f42osNDuXdk0iV/5sWaUs8zEyqwaSJQXrMqI5fth/P5\n3x/1v6QkADCsawtio8JYsPYApWXllJUbnvs4nbLychamXnoS8IaEppEM7tScDQdOMmVkkscN30o1\nNE0Eymte/jKD1rGR/GjQpY/HEx4awvV92/DexszKXjpNI8N46/5hdG/t3RE3L8X3B7Rj+5E8pl+p\nv8iV72giUF6xLTOPVRm5zBjfu86ndS/EH37Yj/vdniVIjIusHEKhsbh7eGcmXt6u0cWtAosmggbQ\nEMNQA8ydO5cbbriBNm3a1L9zI1BRZQNWaSA2KqyyH31DiAwL9doQy04JCRFNAsrnNBE0AE+GofbE\n3LlzGTRoUEAkgo+3H+Nn72yuMrDYg1d3c3RMdaXUxdFE4LB58+Yxc+ZMSkpKGDlyJC+99BLl5eVM\nnTqVLVu2YIwhNTWVxMREtmzZwu233050dPQFlST8jTUS5mb6tI2tHEsnPFS4vY5xe5RSvhN4ieCj\nGXBsW8N+Zpv+MP7ZCz5s+/btfPDBB3zzzTeEhYWRmprKwoUL6datGzk5OWzbZsV56tQp4uPjefHF\nF3nppZe4/HL/G4vEU+4jYb5x3zCPH+BSSvlO4CUCP/Lpp5+yfv36ymGoz549S8eOHbn++uvZtWsX\nDz/8MBMmTGDs2LE+jvTibTl0ir9/urty4o5th/Mu+ClepZRvBV4iuIhf7k4xxnDffffx+9///rxt\nW7du5aOPPmLmzJm8//77zJ492wcRXpqdR/O5Z85aIsJC6ZJg9dsfmtSCp27qe1HTEiqlfCPwEoEf\nGTNmDLfccguPPPIICQkJ5ObmcubMGaKjo4mKiuLWW2+lR48eTJ8+HYDY2FgKCgp8HLVnGvopXqWU\n72gicFD//v158sknGTNmDOXl5YSHhzNr1ixCQ0OZNm0axhhEhOeeew6AqVOnMn36dK83Fqcfy6es\n3NQ5E5W7rPwiJs9ZS7kxvDl9mCYBpRo5HYa6EWuI6y52lXHVn77gVGEp8+4byvCu9Y+V/tiib/nP\n1iMs+vFI+nfwLHkopXyrrmGo655WSAW8f28+QlZ+MbFR4Uyft4Ft9qxgtTly6iz/3nKYO4Z00iSg\nVIDQRBDEyssNs77aS992cXz401HEx4Rz72vrWPfdCfZln2Zf9mlOF1edL3nO19actjo2jlKBI2Da\nCCrq24NFQ1TpLUvLYl/2GV6cNJC2zaJ5c9owbpm1mtv+sbpynxZNIngndTg9EmM5VVjC2+sOctOA\ndnRoru0CSgWKgEgEUVFR5Obm0rJly6BIBsYYcnNziYq6+C6a1twAe+nUIobx/awhLZISmvDhT0ex\n7rsTAJSWWUM73z1nHYseGMEHmw9TWFLGj0df2KTxSin/FhCJoEOHDmRmZpKdXfOk4YEoKiqKDh3q\nn6+3Nmv2neDbQ6d45gf9CAs9V0PYtlk0Ey8/N0x0v/Zx3P6PNUyes5aCIhfX9m5N7zZxlxS7Usq/\nBEQiCA8Pp0sXrbOuy9mSMn6xaAtpR/IBOHGmhISmEfVO/t67TRyvTR3C5FfXUlhSpnPaKhWAAiIR\nqLqVuMp54M2NrNyTzfh+bQkLtarPbrysHVHh9c8NMKhTc+ZPG8qmA6cYktTc6XCVUl6miSDAlZUb\nfv7OFr7cnc1zN/fn9iEXNx/A4M4tGNxZJzZXKhBp99EA9+Ti7SzZdpT/vqHPRScBpVRg00QQwI7m\nneXNNQe5d0Rn7r9Ke/oopWqmiSCAfZqWBcDdI5J8G4hSyq9pIghgy9Ky6NqqCd1bN/V1KEopP6aJ\nIEDlnS1l9d5cvpec6OtQlFJ+ztFEICLjRGSXiGSIyIwatncSkS9EZLOIbBWRG5yMJ5is2HUcV7lh\nbHIbX4eilPJzjiUCEQkFZgLjgWRgkogkV9vtceBdY8xA4A7g/5yKJ9gsT8sioWkkAzvG+zoUpZSf\nc7JEMBTIMMbsM8aUAAuBidX2MUDFeAXNgCMOxhOwTp4pYfzzK1m47iBgzTGwYlc230tuTUhI4I+9\npJS6NE4+UNYeOOS2nAkMq7bPU8AyEfkp0AQYU9MHiUgqkArQqZP2ha9ueVoWO4/m85sPthETGUZc\nVBini11aLaSU8oivG4snAa8bYzoANwDzReS8mIwxs40xKcaYlFatWnk9SH+3LO0Y7ZpFMSSpBY++\ns4W/fbqHmIhQRnSrf7YxpZRyMhEcBjq6LXew17mbBrwLYIxZDUQBCQ7GFHAKS1ys3JPD2L5tePXe\nFHq3jeXbQ6e4ulcrj8YRUkopJxPBeqCHiHQRkQisxuDF1fY5CFwHICJ9sBJB8Iwl3QC+2p1Dsauc\nscmJxEWFM2/qUMb3a8O0K/RJYqWUZxxrIzDGuETkIeATIBSYa4zZISJPAxuMMYuBXwCviMjPsRqO\np5iGmHoriCxPyyIuKowhXawB4Vo2jeTlyYN9HJVSqjFxdPRRY8xSYGm1dU+4vU8DRjkZQyBzlZXz\nWXoW1/VJJDzU1809SqnGSu8ejdj6/Sc5VVjKWH16WCl1CTQRNGLL07KICAvhqp7ak0opdfE0ETRS\nxhiWpR3jiu4JNInU+YWUUhdPE0EjtfNoAZknz+qgckqpS6aJoJFanpaFCIzpo4lAKXVpNBE0UsvS\njjGoU3NaxUb6OhSlVCOniaAROnzqLDuO5GtvIaVUg9BE0Agt33EMQNsHlFINQhNBI7R8ZxbdWzel\nayudglIpdem036EfyjldzNtrD+Iqt0bbaB4Tzp3DOhMRFkJeYSlr9p0g9SodS0gp1TA0EfihV1d+\nx6wv91ZZt/7ASV64YyBf7DpOWbnR9gGlVIPRROCHKh4Ue3O6NY/P7K/28sel6cRFhXGqsJTWsZEM\n6KBTUCqlGoYmAj+Tcfw0+7LPMGVkUuW61Ku6caqwlP9bYZUS7hzWSaegVEo1GG0s9jPL07KA8x8U\n++X1vZg83Jqm84Z+bb0el1IqcGmJwM8sSztG//bNaBcfXWW9iPD0Tf2YMjKJ7q1jfRSdUioQaYnA\njxzPL2LLoVO1NgSHhIgmAaVUg9NE4Ec+3XkcY+B7fbVHkFLKezQR+JFlacfo1CKGXon6q18p5T2a\nCPzE6WIX32TkMjY5ERHtEaSU8p6gbCw+mneWFz/PoNRV7utQKuWcLqakrFzHD1JKeV1QJoLPdh7n\nrbUHSYyLJNSPfn0P79qCwZ2b+zoMpVSQCcpEkF9UCsCKx64hOiLUx9EopZRvBWUbQUGRi7AQISo8\nKC9fKaWqCMo7YUFRKbFRYdooq5RSBG0icBEbFe7rMJRSyi8EcSIIyuYRpZQ6T5AmglJNBEopZQvS\nRKBVQ0opVSGIE4GWCJRSCoI2EZQSpyUCpZQCHE4EIjJORHaJSIaIzKhln9tEJE1EdojIW07GA2CM\n4XSxlgiUUqqCY3dDEQkFZgLfAzKB9SKy2BiT5rZPD+A3wChjzEkRae1UPBXOlJRRbqBppCYCpZQC\nZ0sEQ4EMY8w+Y0wJsBCYWG2f+4GZxpiTAMaY4w7GA1jVQoA2FiullK3eRCAiPxWRixkJrT1wyG05\n017nrifQU0RWicgaERlXSwypIrJBRDZkZ2dfRCjnFBS5ALRqSCmlbJ6UCBKxqnXetev8G3JchjCg\nB3A1MAl4RUTiq+9kjJltjEkxxqS0atXqkk54rkSgiUAppcCDRGCMeRzrZj0HmALsEZE/iki3eg49\nDHR0W+5gr3OXCSw2xpQaY74Ddtvnckx+ZYlAq4aUUgo8bCMwxhjgmP1yAc2B90TkT3Ucth7oISJd\nRCQCuANYXG2ff2GVBhCRBKyqon0XcgEXqqJqKE5LBEopBXjQa0hEHgHuAXKAV4FfGmNKRSQE2AP8\nqqbjjDEuEXkI+AQIBeYaY3aIyNPABmPMYnvbWBFJA8rsz85tiAurjTYWK6VUVZ78LG4B/MgYc8B9\npTGmXERurOtAY8xSYGm1dU+4vTfAo/bLK7SxWCmlqvKkaugj4ETFgojEicgwAGPMTqcCc0pBUSmh\nIUKMzkymlFKAZ4ngZeC02/Jpe12jVFDkommkTkqjlFIVPEkEYlfhAFaVEI14rmMdcE4pparyJBHs\nE5GHRSTcfj2Cwz17nKRDUFfKn6sAABL0SURBVCulVFWeJIIHgJFYzwBkAsOAVCeDclJBUSmxOs6Q\nUkpVqveOaI//c4cXYvGKgiIXbZtF+ToMpZTyG548RxAFTAP6ApV3UGPMfQ7G5ZiC4lJ6RjX1dRhK\nKeU3PKkamg+0Aa4HvsQaKqLAyaCcpG0ESilVlSeJoLsx5n+AM8aYecAErHaCRscYo72GlFKqGk8S\nQan995SI9AOaAY5PIOOEs6VllJUbLREopZQbT34az7bnI3gca9C4psD/OBqVQ3R4CaWUOl+dd0R7\nYLl8ewaxr4CuXonKIToXgVJKna/OqiH7KeIaRxdtjPIrh6DWqiGllKrgSRvBpyLymIh0FJEWFS/H\nI3OAVg0ppRqlslJ48xbI+NSRj/fkjni7/fcnbusMjbCaSOciUEo1SgdWQcZyGDzFkY/35MniLo6c\n2Qe0RKCUapTSl0BYNHS71pGP9+TJ4ntqWm+MeaPhw3FWRYmgqSYCpVRjYQykL7WSQESMI6fw5I44\nxO19FHAdsAlodIngdJELEWgaoYlAKdVIHP0W8jPhmt84dgpPqoZ+6r4sIvHAQsciclB+kYumEWGE\nhOikNEqpRmLXUpAQ6DnOsVN40muoujNAo2w30OEllFKNTvoS6DQCmiQ4dgpP2gg+xOolBFbiSAbe\ndSwiBxUUlWqPIaVU43FyP2Rth7F/cPQ0nvw8/rPbexdwwBiT6VA8jtISgVKqUUlfav3tfYOjp/Hk\nrngQOGqMKQIQkWgRSTLG7Hc0MgcUFJfSqmmkr8NQSinPpC+B1n2hhbOPbXmSCBZhTVVZocxeN6Tm\n3f1XQZGLrgk6KY1Sygf2r4JPnwJTZi03aQW3vg7h0TXvX3gCDn4DVz7meGieNBaHGWNKKhbs9xHO\nheQcrRpSSvnMun9AdjpEN4fQSNj9Mez9vPb9d38MptzxaiHwLBFki8hNFQsiMhHIcS4kZ1iT0mhj\nsVLKB0qLYM+n0P8WmPw+3LsYoppZVT+1SV8Cce2h7eWOh+fJz+MHgAUi8pK9nAnU+LSxPyt2lVNa\nZrREoJTyvu++gtIz0GuCtRwaDj2uh10fQZkLQqvdl0oKIeMzGDgZxPnnnuotERhj9hpjhmN1G002\nxow0xmQ4HlkDy7eHl4jTRKCU8rb0/0BELHS58ty63hPg7Ak4tPb8/fetANdZax8vqDcRiMgfRSTe\nGHPaGHNaRJqLyDPeCK4hVQw4p+MMKaW8qrzc+uXf43sQ5tZrsft1VltBTdVD6UsgshkkXeGVED1p\nIxhvjDlVsWDPVuZ860UDqxx5NFLbCJRSXnR4A5w5fv6v+8hY6Doadi2xBparUF4Guz+CnmOtKiQv\n8CQRhIpIZRoTkWig0XXGP61DUCulfCH9PxASbpUIqus9wXp6+HjauXWH1kJhrteqhcCzRLAA+ExE\nponIdGA5MM+TDxeRcSKyS0QyRGRGHfvdLCJGRFI8C/vC6aQ0SimfSF9qtQ1ENTt/W8/xgJx7ghis\naqHQCOg+xmshetJY/BzwDNAH6AV8AnSu7zgRCQVmAuOxGponiUhyDfvFAo8ANbSYNBydlEYp5XXZ\nuyF3D/SqpTY9NhE6DLFKDWDPPbAEuoy2qo68xNO7YhbWwHO3At8B73twzFAgwxizD0BEFgITgbRq\n+/0eeA74pYexXJRzvYa0RKBU0MrLhHk3Qclp75yvtMj6W1siAKsK6NMn4c89rURw5jiMesQ78dlq\nTQQi0hOYZL9ygHcAMcZc4+FntwcOuS1nAsOqnWMQ0NEYs0REak0EIpIKpAJ06tTJw9NX1SMxlttT\nOmqvIaWC2Y4P4MReq39+iJfuBa16Q7P2tW8feDcUHAWXnTTCY6Dfzd6JzVbXf4l0YCVwY8VzAyLy\n84Y6sYiEAH8FptS3rzFmNjAbICUlxdSze41G92zF6J6tLuZQpVSgSF8Cif1h4kxfR3JOk5Yw/jmf\nhlBXG8GPgKPAFyLyiohcB1zII26HgY5uyx3sdRVigX7AChHZDwwHFjvZYKyUCmKns+HgGq/2xmks\nak0Exph/GWPuAHoDXwA/A1qLyMsiMtaDz14P9BCRLiISAdwBLHb7/DxjTIIxJskYkwSsAW4yxmy4\nhOtRSqma7f4YMF4ZxK2x8aTX0BljzFvGmO9j/arfDPzag+NcwENYvYx2Au8aY3aIyNPug9gppZRX\npC+BZh2hzWW+jsTvXFBrif1UcWV9vQf7LwWWVlv3RC37Xn0hsSillMdKzsC+L2DwFK8M4tbYXMzk\n9Uop1bjs/dzqlVNXN84gpolAKRX40pdAVDx0Hln/vkFIE4FSKrCVuayG4p7jvDaIW2OjiUApFdgO\nroazJ7W3UB00ESilAlv6Emvc/27X+ToSv6WJQCkVuIyxxvvvejVENvV1NH5LE4FSKnBlbYdTB/Vp\n4npoIlBKBa70JYBAr/G+jsSvaSJQSgWu9CXQcSg0be3rSPyaJgKlVGA6dRCObdVqIQ9oIlBKBaZd\nH1l/e9/o2zgaAU0ESqnAlP4fSOgFLbv5OhK/p9N1KaV8q/i0NT1jQyophP2rvD7lY2OliUAp5TvG\nwOzRkJvhzOdrtZBHNBEopXzn2FYrCQxNhfaDG/azo1tAhwb+zACliUAp5TvpS0BCYPSvoUmCr6MJ\nWtpYrJTynfSl0HG4JgEf00SglPKNk/sha5uOCuoHNBEopXyjop+/zhrmc5oIlFK+kb4EWidrP38/\noIlAKeV9hSfgwCotDfgJTQRKKe/b/QmYch0HyE9o91GllHfk7IGCY9b7rQshth20G+jbmBSgiUAp\n5Q0lZ+DlUVBWfG7dsAdAxHcxqUqaCJRSzsvZYyWBax6HTsOtBNBukK+jUjZNBEop5+Xssf72ngCJ\nyb6NRZ1HG4uVUs7L2W0NJdGiq68jUTXQRKCUcl7uHojvDOFRvo5E1UATgVLKeTl7IKGnr6NQtdBE\noJRyVnmZNdR0Qg9fR6Jq4WgiEJFxIrJLRDJEZEYN2x8VkTQR2Soin4lIZyfjUUr5QN4hcBVpIvBj\njiUCEQkFZgLjgWRgkohU7y6wGUgxxlwGvAf8yal4lFI+kmPPPqZVQ37LyRLBUCDDGLPPGFMCLAQm\nuu9gjPnCGFNoL64BOjgYj1LKF3J2W381EfgtJxNBe+CQ23Kmva4204CPatogIqkiskFENmRnZzdg\niEopx+XshujmENPS15GoWvhFY7GITAZSgP9X03ZjzGxjTIoxJqVVq1beDU4pdWkqegzpcBJ+y8lE\ncBjo6LbcwV5XhYiMAf4buMkYU1x9u1KqkcvZrQ3Ffs7JRLAe6CEiXUQkArgDWOy+g4gMBP6BlQSO\nOxiLUsoXzp6CM8ehpSYCf+ZYIjDGuICHgE+AncC7xpgdIvK0iNxk7/b/gKbAIhHZIiKLa/k4pVRj\nlKs9hhoDRwedM8YsBZZWW/eE2/sxTp5fKeVj2mOoUfCLxmKlVIDK2Q0h4dBcnxX1Z5oIlFLOydlj\njTgaGu7rSFQdNBEopZyTs0d7DDUCOjGNUsHk4Br4bqX3zndiH/S+wXvnUxdFE4FSwWTJY5C1zXvn\nk1DoPMp751MXRROBUsGivMyaIGbEQzDmd945pwiEhHrnXOqiaSJQKlhUDAfdqheE6v/66hxtLFYq\nWFRMIK9P+apqNBEoFSz04S5VC00ESgWLnN0Q3QKa6HDQqipNBEoFC51AXtVCE4FSwUIf7lK10ESg\nVDA4e9IaDloTgaqBJgKlgoFOIK/qoIlAqWCgPYZUHTQRKBUMKoaDjtfhoNX5NBEoFQxy9kDLbvpE\nsaqRJgKlgkGu9hhStdNEoFSgKyu1hoPWoSVULTQRKBXoTu6Hcpc2FKtaaSJQKtBpjyFVD00ESgW6\nykTQ3bdxKL+liUCpQJeTAU3bQFQzX0ei/JT2JVPK3509CR8+AsWnL+74I5shsW/DxqQCiiYCpfzd\njg8g7d/QbqA1B/CFatEVBk5u+LhUwNBEoJS/S18KzbvA/V9YcwAr1cC0jUApf1aUD999Cb0naBJQ\njtFEoJQ/y/gUykqsRKCUQzQRKOXPdi2FmJbQcZivI1EBTBOBUv6qrBR2L4Oe4yHkIhqJlfKQJgKl\n/NX+r6E4T6uFlOMcTQQiMk5EdolIhojMqGF7pIi8Y29fKyJJTsajVKOSvgTCoqHr1b6ORAU4xxKB\niIQCM4HxQDIwSUSSq+02DThpjOkO/A14zql4lGpUjLHaB7pfBxExvo5GBTgnnyMYCmQYY/YBiMhC\nYCKQ5rbPROAp+/17wEsiIsYY0+DRbJoPq19q8I9VyhHlZZB/GK75b19HooKAk4mgPXDIbTkTqN71\noXIfY4xLRPKAlkCO+04ikgqkAnTq1OnioolpAa16XdyxSvlCx2GQfJOvo1BBoFE8WWyMmQ3MBkhJ\nSbm40kLvCdroppRSNXCysfgw0NFtuYO9rsZ9RCQMaAbkOhiTUkqpapxMBOuBHiLSRUQigDuAxdX2\nWQzca7+/BfjckfYBpZRStXKsasiu838I+AQIBeYaY3aIyNPABmPMYmAOMF9EMoATWMlCKaWUFzna\nRmCMWQosrbbuCbf3RcCtTsaglFKqbvpksVJKBTlNBEopFeQ0ESilVJDTRKCUUkFOGltvTRHJBg5c\n5OEJVHtqOUgE43UH4zVDcF53MF4zXPh1dzbGtKppQ6NLBJdCRDYYY1J8HYe3BeN1B+M1Q3BedzBe\nMzTsdWvVkFJKBTlNBEopFeSCLRHM9nUAPhKM1x2M1wzBed3BeM3QgNcdVG0ESimlzhdsJQKllFLV\naCJQSqkgFzSJQETGicguEckQkRm+jscJItJRRL4QkTQR2SEij9jrW4jIchHZY/9t7utYG5qIhIrI\nZhH5j73cRUTW2t/3O/ZQ6AFFROJF5D0RSReRnSIyIki+65/b/763i8jbIhIVaN+3iMwVkeMist1t\nXY3frVhesK99q4gMutDzBUUiEJFQYCYwHkgGJolIsm+jcoQL+IUxJhkYDvzEvs4ZwGfGmB7AZ/Zy\noHkE2Om2/BzwN2NMd+AkMM0nUTnreeBjY0xvYADW9Qf0dy0i7YGHgRRjTD+sIe7vIPC+79eBcdXW\n1fbdjgd62K9U4OULPVlQJAJgKJBhjNlnjCkBFgITfRxTgzPGHDXGbLLfF2DdGNpjXes8e7d5wA98\nE6EzRKQDMAF41V4W4FrgPXuXQLzmZsBVWHN6YIwpMcacIsC/a1sYEG3PahgDHCXAvm9jzFdYc7S4\nq+27nQi8YSxrgHgRaXsh5wuWRNAeOOS2nGmvC1gikgQMBNYCicaYo/amY0Cij8Jyyt+BXwHl9nJL\n4JQxxmUvB+L33QXIBl6zq8ReFZEmBPh3bYw5DPwZOIiVAPKAjQT+9w21f7eXfH8LlkQQVESkKfA+\n8DNjTL77Nnsq0IDpMywiNwLHjTEbfR2Ll4UBg4CXjTEDgTNUqwYKtO8awK4Xn4iVCNsBTTi/CiXg\nNfR3GyyJ4DDQ0W25g70u4IhIOFYSWGCM+ae9OquiqGj/Pe6r+BwwCrhJRPZjVfldi1V3Hm9XHUBg\nft+ZQKYxZq29/B5WYgjk7xpgDPCdMSbbGFMK/BPr30Cgf99Q+3d7yfe3YEkE64Eeds+CCKzGpcU+\njqnB2XXjc4Cdxpi/um1aDNxrv78X+Le3Y3OKMeY3xpgOxpgkrO/1c2PMXcAXwC32bgF1zQDGmGPA\nIRHpZa+6DkgjgL9r20FguIjE2P/eK647oL9vW23f7WLgHrv30HAgz60KyTPGmKB4ATcAu4G9wH/7\nOh6HrvEKrOLiVmCL/boBq878M2AP8CnQwtexOnT9VwP/sd93BdYBGcAiINLX8TlwvZcDG+zv+19A\n82D4roHfAenAdmA+EBlo3zfwNlYbSClW6W9abd8tIFi9IvcC27B6VF3Q+XSICaWUCnLBUjWklFKq\nFpoIlFIqyGkiUEqpIKeJQCmlgpwmAqWUCnKaCJSqRkTKRGSL26vBBm4TkST3ESWV8gdh9e+iVNA5\na4y53NdBKOUtWiJQykMisl9E/iQi20RknYh0t9cnicjn9ljwn4lIJ3t9ooh8ICLf2q+R9keFisgr\n9pj6y0Qk2mcXpRSaCJSqSXS1qqHb3bblGWP6Ay9hjXoK8CIwzxhzGbAAeMFe/wLwpTFmANY4QDvs\n9T2AmcaYvsAp4GaHr0epOumTxUpVIyKnjTFNa1i/H7jWGLPPHtzvmDGmpYjkAG2NMaX2+qPGmAQR\nyQY6GGOK3T4jCVhurMlFEJFfA+HGmGecvzKlaqYlAqUujKnl/YUodntfhrbVKR/TRKDUhbnd7e9q\n+/03WCOfAtwFrLTffwY8CJVzKjfzVpBKXQj9JaLU+aJFZIvb8sfGmIoupM1FZCvWr/pJ9rqfYs0U\n9kusWcOm2usfAWaLyDSsX/4PYo0oqZRf0TYCpTxktxGkGGNyfB2LUg1Jq4aUUirIaYlAKaWCnJYI\nlFIqyGkiUEqpIKeJQCmlgpwmAqWUCnKaCJRSKsj9fxhVCwbcf/05AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "# Evaluate on test data\n",
            "40/40 [==============================] - 0s 2ms/sample - loss: 0.7096 - acc: 0.6500\n",
            "test loss, test acc: [0.7096120703034103, 0.65]\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:107: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:111: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "(1, 3)\n",
            "Finding labels in training data\n",
            "Finding labels in testing data\n",
            "[]\n",
            "(40, 12, 1536)\n",
            "(40,)\n",
            "(0, 12, 1536)\n",
            "(0,)\n",
            "[]\n",
            "X_train shape: (40, 1, 12, 1536)\n",
            "40 train samples\n",
            "0 val samples\n",
            "(20, 12, 1536)\n",
            "(20, 1)\n",
            "X_train shape: (20, 1, 12, 1536)\n",
            "20 train samples\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 40 samples\n",
            "Epoch 1/100\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-335fd5d35f32>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    149\u001b[0m       history = model.fit(X_train, Y_train, batch_size = 16, epochs = 100, \n\u001b[1;32m    150\u001b[0m                       \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m                       callbacks=[checkpointer], class_weight = class_weights)\n\u001b[0m\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m       \u001b[0;31m# Plot training & validation accuracy values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m   def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    673\u001b[0m         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m         \u001b[0mvalidation_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m    676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m   def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    438\u001b[0m           \u001b[0mvalidation_in_fit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m           \u001b[0mprepared_feed_values_from_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m           steps_name='validation_steps')\n\u001b[0m\u001b[1;32m    441\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_results\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m         \u001b[0mval_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mval_results\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    409\u001b[0m           \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 411\u001b[0;31m     \u001b[0maggregator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    412\u001b[0m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maggregator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m     \u001b[0mepoch_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mfinalize\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    136\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mfinalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Empty training data.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_samples\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Empty training data."
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kd6FTGs7vBI3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "df_accl_all = pd.DataFrame({'12': acc_all[:, 0], '13': acc_all[:, 1],'14': acc_all[:, 2], '23': acc_all[:, 3],'24': acc_all[:, 4], '34': acc_all[:, 5]})\n",
        "df_accl_all.to_csv (r'EEG_Deep/df_accl_all.csv', index = None, header=True) #Don't forget to add '.csv' at the end of the path\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}