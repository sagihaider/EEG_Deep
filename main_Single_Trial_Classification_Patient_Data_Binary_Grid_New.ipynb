{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "main_Single_Trial_Classification_Patient_Data_Binary_Grid",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sagihaider/EEG_Deep/blob/master/main_Single_Trial_Classification_Patient_Data_Binary_Grid_New.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X110PE9MjrlE",
        "colab_type": "code",
        "outputId": "154bfeda-729b-4406-88ce-118c37e3ee7f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "!git clone https://github.com/sagihaider/EEG_Deep.git"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'EEG_Deep' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ioLzMoiS60Qf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import scipy.io as spio\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from importlib.machinery import SourceFileLoader\n",
        "\n",
        "# EEGNet-specific imports\n",
        "from EEG_Deep.EEGModels import EEGNet, ShallowConvNet, DeepConvNet\n",
        "from tensorflow.keras import utils as np_utils\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras import backend as K\n",
        "K.set_image_data_format('channels_first')\n",
        "\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# tools for plotting confusion matrices\n",
        "from matplotlib import pyplot as plt\n",
        "from scipy.signal import butter, lfilter"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mo80Jg_Pn5lP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Band-pass Filter\n",
        "def butter_bandpass_filter(data, lowcut, highcut, fs, order=4):\n",
        "    nyq = 0.5 * fs\n",
        "    low = lowcut / nyq\n",
        "    high = highcut / nyq\n",
        "    b, a = butter(order, [low, high], btype='band')\n",
        "    y = lfilter(b, a, data)\n",
        "    return y\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZjtnE_6RSpum",
        "colab_type": "code",
        "outputId": "c9b1f022-baed-4717-8360-4593c87357bb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from numpy import zeros\n",
        "K.clear_session()\n",
        "\n",
        "X_tr = np.empty([80, 12, 4096])\n",
        "X_ts = np.empty([40, 12, 4096])\n",
        "result=[]\n",
        "\n",
        "h_cut = [24]\n",
        "drop_out = [0.25,0.5]\n",
        "k_len = [32, 64, 128]\n",
        "n_epochs = 300\n",
        "\n",
        "outfname = 'accuray_epochs' + str(n_epochs) + '_filter_' + str(h_cut) + '_patient_data.npy'\n",
        "\n",
        "nsub = 10\n",
        "nfilt = len(h_cut)\n",
        "ndrop = len(drop_out)\n",
        "nkl = len(k_len)\n",
        "acc_sub = zeros([nsub, nfilt,ndrop,nkl])\n",
        "\n",
        "for sub_idx, x in enumerate(range(1,11)):\n",
        "    for h_indx, h in enumerate(h_cut):\n",
        "        fName = 'EEG_Deep/Data2A/parsed_P0' + str(x) + 'T.mat'  # Load Data\n",
        "        print(fName)\n",
        "        mat = spio.loadmat(fName)\n",
        "        r_X_tr = mat['RawEEGData']\n",
        "        y_tr = mat['Labels']\n",
        "        y_tr = y_tr.flatten() \n",
        "\n",
        "        print(np.shape(r_X_tr))\n",
        "        print(np.shape(y_tr))\n",
        "\n",
        "        for t in range(r_X_tr.shape[0]):\n",
        "            tril = r_X_tr[t,:,:]\n",
        "            #tril = tril.transpose()\n",
        "            tril_filtered = butter_bandpass_filter(tril, lowcut=4, highcut=h, fs=250, order=4)\n",
        "            # tril_filtered = tril_filtered.transpose()\n",
        "            X_tr[t,:,:] = tril_filtered \n",
        "\n",
        "            # split data of each subject in training and validation\n",
        "        X_train      = X_tr[0:60,:,2048:3584]\n",
        "        Y_train      = y_tr[0:60]\n",
        "        X_val       = X_tr[60:,:,2048:3584]\n",
        "        Y_val       = y_tr[60:]\n",
        "\n",
        "        print(np.shape(X_train))\n",
        "        print(np.shape(Y_train))\n",
        "        print(np.shape(X_val))\n",
        "        print(np.shape(Y_val))\n",
        "\n",
        "        # convert labels to one-hot encodings.\n",
        "        Y_train = np_utils.to_categorical(Y_train-1, num_classes=2)\n",
        "        Y_val = np_utils.to_categorical(Y_val-1, num_classes=2)\n",
        "\n",
        "        kernels, chans, samples = 1, 12, 1536\n",
        "        # convert data to NCHW (trials, kernels, channels, samples) format. Data \n",
        "        # contains 22 channels and 500 time-points. Set the number of kernels to 1.\n",
        "        X_train      = X_train.reshape(X_train.shape[0], kernels, chans, samples)\n",
        "        X_val       = X_val.reshape(X_val.shape[0], kernels, chans, samples)\n",
        "\n",
        "        print('X_train shape:', X_train.shape)\n",
        "        print(X_train.shape[0], 'train samples')\n",
        "        print(X_val.shape[0], 'val samples')\n",
        "\n",
        "        fName = 'EEG_Deep/Data2A/parsed_P0' + str(x) + 'E.mat'  # Load Data\n",
        "        print(fName)\n",
        "        mat = spio.loadmat(fName)\n",
        "        r_X_ts = mat['RawEEGData']\n",
        "        y_ts = mat['Labels']\n",
        "        y_ts = y_ts.flatten() \n",
        "\n",
        "        print(np.shape(r_X_ts))\n",
        "        print(np.shape(y_ts))\n",
        "\n",
        "        for t in range(r_X_ts.shape[0]):\n",
        "            tril = r_X_ts[t,:,:]\n",
        "            # tril = tril.transpose()\n",
        "            tril_filtered = butter_bandpass_filter(tril, lowcut=4, highcut=h, fs=250, order=4)\n",
        "            # tril_filtered = tril_filtered.transpose()\n",
        "            X_ts[t,:,:] = tril_filtered \n",
        "\n",
        "        X_test      = X_ts[:,:,2048:3584]\n",
        "        Y_test      = y_ts[:]\n",
        "        print(np.shape(X_test))\n",
        "        print(np.shape(Y_test))\n",
        "\n",
        "        #convert labels to one-hot encodings.\n",
        "        Y_test      = np_utils.to_categorical(Y_test-1)\n",
        "\n",
        "        # convert data to NCHW (trials, kernels, channels, samples) format. Data \n",
        "        # contains 22 channels and 500 time-points. Set the number of kernels to 1.\n",
        "        X_test      = X_test.reshape(X_test.shape[0], kernels, chans, samples)\n",
        "\n",
        "        print('X_train shape:', X_test.shape)\n",
        "        print(X_test.shape[0], 'test samples')\n",
        "\n",
        "        kernels, chans, samples = 1, 12, 1536\n",
        "        # convert data to NCHW (trials, kernels, channels, samples) format. Data \n",
        "        # contains 22 channels and 500 time-points. Set the number of kernels to 1.\n",
        "        X_train = X_train.reshape(X_train.shape[0], kernels, chans, samples)\n",
        "        X_val   = X_val.reshape(X_val.shape[0], kernels, chans, samples)\n",
        "\n",
        "        print('X_train shape:', X_train.shape)\n",
        "        print(X_train.shape[0], 'train samples')\n",
        "        print(X_val.shape[0], 'val samples')\n",
        "\n",
        "        X_test      = X_ts[:,:,2048:3584]\n",
        "        Y_test      = y_ts[:]\n",
        "        print(np.shape(X_test))\n",
        "        print(np.shape(Y_test))\n",
        "\n",
        "        #convert labels to one-hot encodings.\n",
        "        Y_test      = np_utils.to_categorical(Y_test-1, num_classes=2)\n",
        "\n",
        "        # convert data to NCHW (trials, kernels, channels, samples) format. Data \n",
        "        # contains 22 channels and 500 time-points. Set the number of kernels to 1.\n",
        "        X_test      = X_test.reshape(X_test.shape[0], kernels, chans, samples)\n",
        "\n",
        "        print('X_train shape:', X_test.shape)\n",
        "        print(X_test.shape[0], 'train samples')\n",
        "\n",
        "        # acc_sub = zeros([rows, cols])\n",
        "        for id_d, d in enumerate(drop_out):\n",
        "            for id_kl, kl in enumerate(k_len):\n",
        "              print(id_kl, id_d)\n",
        "              # configure the EEGNet-8,2,16 model with kernel length of 32 samples (other \n",
        "              # model configurations may do better, but this is a good starting point)\n",
        "              model = EEGNet(nb_classes = 2, Chans = 12, Samples = 1536,\n",
        "                             dropoutRate = d, kernLength = kl, F1 = 8,D = 2, F2 = 16,\n",
        "                             norm_rate = 0.25, dropoutType = 'Dropout')\n",
        "\n",
        "              # compile the model and set the optimizers\n",
        "              model.compile(loss='categorical_crossentropy', optimizer='adam', \n",
        "                            metrics = ['accuracy'])\n",
        "\n",
        "              # count number of parameters in the model\n",
        "              numParams    = model.count_params() \n",
        "\n",
        "              # set a valid path for your system to record model checkpoints\n",
        "              checkpointer = ModelCheckpoint(filepath='/tmp/checkpoint.h5', verbose=1, save_best_only=True)\n",
        "\n",
        "              # the syntax is {class_1:weight_1, class_2:weight_2,...}. Here just setting\n",
        "              # the weights all to be 1\n",
        "              class_weights = {0:1, 1:1}\n",
        "\n",
        "              history = model.fit(X_train, Y_train, batch_size = 16, epochs = n_epochs, \n",
        "                                  verbose = 2, validation_data=(X_val, Y_val),\n",
        "                                  callbacks=[checkpointer], class_weight = class_weights)\n",
        "\n",
        "              print('\\n# Evaluate on test data')\n",
        "              results = model.evaluate(X_test, Y_test, batch_size=1)\n",
        "              print('test loss, test acc:', results)\n",
        "\n",
        "              acc_sub[sub_idx,h_indx,id_d, id_kl] = results[1]\n",
        "\n",
        "              from keras import backend as K \n",
        "              # Do some code, e.g. train and save model\n",
        "              K.clear_session()\n",
        "              \n",
        "np.save(outfname, acc_sub)\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "EEG_Deep/Data2A/parsed_P01T.mat\n",
            "(80, 12, 4096)\n",
            "(80,)\n",
            "(60, 12, 1536)\n",
            "(60,)\n",
            "(20, 12, 1536)\n",
            "(20,)\n",
            "X_train shape: (60, 1, 12, 1536)\n",
            "60 train samples\n",
            "20 val samples\n",
            "EEG_Deep/Data2A/parsed_P01E.mat\n",
            "(40, 12, 4096)\n",
            "(40,)\n",
            "(40, 12, 1536)\n",
            "(40,)\n",
            "X_train shape: (40, 1, 12, 1536)\n",
            "40 test samples\n",
            "X_train shape: (60, 1, 12, 1536)\n",
            "60 train samples\n",
            "20 val samples\n",
            "(40, 12, 1536)\n",
            "(40,)\n",
            "X_train shape: (40, 1, 12, 1536)\n",
            "40 train samples\n",
            "0 0\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "Train on 60 samples, validate on 20 samples\n",
            "Epoch 1/300\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.76121, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 6s - loss: 0.8017 - acc: 0.5333 - val_loss: 0.7612 - val_acc: 0.0000e+00\n",
            "Epoch 2/300\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.76121\n",
            "60/60 - 0s - loss: 0.6103 - acc: 0.6667 - val_loss: 0.8429 - val_acc: 0.0000e+00\n",
            "Epoch 3/300\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.76121\n",
            "60/60 - 0s - loss: 0.5917 - acc: 0.6667 - val_loss: 0.9006 - val_acc: 0.0000e+00\n",
            "Epoch 4/300\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.76121\n",
            "60/60 - 0s - loss: 0.5514 - acc: 0.6667 - val_loss: 0.9390 - val_acc: 0.0000e+00\n",
            "Epoch 5/300\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.76121\n",
            "60/60 - 0s - loss: 0.5242 - acc: 0.6667 - val_loss: 0.9717 - val_acc: 0.0000e+00\n",
            "Epoch 6/300\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.76121\n",
            "60/60 - 0s - loss: 0.5204 - acc: 0.6667 - val_loss: 1.0020 - val_acc: 0.0000e+00\n",
            "Epoch 7/300\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.76121\n",
            "60/60 - 0s - loss: 0.4841 - acc: 0.6833 - val_loss: 1.0417 - val_acc: 0.0000e+00\n",
            "Epoch 8/300\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.76121\n",
            "60/60 - 0s - loss: 0.4965 - acc: 0.7000 - val_loss: 1.0857 - val_acc: 0.0000e+00\n",
            "Epoch 9/300\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.76121\n",
            "60/60 - 0s - loss: 0.4776 - acc: 0.7000 - val_loss: 1.1465 - val_acc: 0.0000e+00\n",
            "Epoch 10/300\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.76121\n",
            "60/60 - 0s - loss: 0.4464 - acc: 0.7500 - val_loss: 1.2157 - val_acc: 0.0000e+00\n",
            "Epoch 11/300\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.76121\n",
            "60/60 - 0s - loss: 0.4378 - acc: 0.8333 - val_loss: 1.3016 - val_acc: 0.0000e+00\n",
            "Epoch 12/300\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.76121\n",
            "60/60 - 0s - loss: 0.4287 - acc: 0.8333 - val_loss: 1.3888 - val_acc: 0.0000e+00\n",
            "Epoch 13/300\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.76121\n",
            "60/60 - 0s - loss: 0.4001 - acc: 0.9000 - val_loss: 1.4877 - val_acc: 0.0000e+00\n",
            "Epoch 14/300\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.76121\n",
            "60/60 - 0s - loss: 0.4114 - acc: 0.8500 - val_loss: 1.6032 - val_acc: 0.0000e+00\n",
            "Epoch 15/300\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.76121\n",
            "60/60 - 0s - loss: 0.3898 - acc: 0.9167 - val_loss: 1.7351 - val_acc: 0.0000e+00\n",
            "Epoch 16/300\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.76121\n",
            "60/60 - 0s - loss: 0.3804 - acc: 0.9167 - val_loss: 1.8770 - val_acc: 0.0000e+00\n",
            "Epoch 17/300\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.76121\n",
            "60/60 - 0s - loss: 0.3434 - acc: 0.9500 - val_loss: 2.0491 - val_acc: 0.0000e+00\n",
            "Epoch 18/300\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.76121\n",
            "60/60 - 0s - loss: 0.3434 - acc: 0.8833 - val_loss: 2.2174 - val_acc: 0.0000e+00\n",
            "Epoch 19/300\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.76121\n",
            "60/60 - 0s - loss: 0.3186 - acc: 0.9333 - val_loss: 2.3819 - val_acc: 0.0000e+00\n",
            "Epoch 20/300\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.76121\n",
            "60/60 - 0s - loss: 0.3528 - acc: 0.9000 - val_loss: 2.5573 - val_acc: 0.0000e+00\n",
            "Epoch 21/300\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.76121\n",
            "60/60 - 0s - loss: 0.3093 - acc: 0.9500 - val_loss: 2.7040 - val_acc: 0.0000e+00\n",
            "Epoch 22/300\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.76121\n",
            "60/60 - 0s - loss: 0.2979 - acc: 0.9500 - val_loss: 2.8465 - val_acc: 0.0000e+00\n",
            "Epoch 23/300\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.76121\n",
            "60/60 - 0s - loss: 0.2918 - acc: 0.9167 - val_loss: 2.9973 - val_acc: 0.0000e+00\n",
            "Epoch 24/300\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.76121\n",
            "60/60 - 0s - loss: 0.2853 - acc: 0.9000 - val_loss: 3.1488 - val_acc: 0.0000e+00\n",
            "Epoch 25/300\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.76121\n",
            "60/60 - 0s - loss: 0.2861 - acc: 0.9333 - val_loss: 3.2892 - val_acc: 0.0000e+00\n",
            "Epoch 26/300\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.76121\n",
            "60/60 - 0s - loss: 0.2808 - acc: 0.9167 - val_loss: 3.4199 - val_acc: 0.0000e+00\n",
            "Epoch 27/300\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.76121\n",
            "60/60 - 0s - loss: 0.3120 - acc: 0.8833 - val_loss: 3.5364 - val_acc: 0.0000e+00\n",
            "Epoch 28/300\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.76121\n",
            "60/60 - 0s - loss: 0.2747 - acc: 0.9333 - val_loss: 3.6791 - val_acc: 0.0000e+00\n",
            "Epoch 29/300\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.76121\n",
            "60/60 - 0s - loss: 0.2633 - acc: 0.9167 - val_loss: 3.7934 - val_acc: 0.0000e+00\n",
            "Epoch 30/300\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.76121\n",
            "60/60 - 0s - loss: 0.2818 - acc: 0.9167 - val_loss: 3.8980 - val_acc: 0.0000e+00\n",
            "Epoch 31/300\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.76121\n",
            "60/60 - 0s - loss: 0.2609 - acc: 0.9333 - val_loss: 3.9974 - val_acc: 0.0000e+00\n",
            "Epoch 32/300\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.76121\n",
            "60/60 - 0s - loss: 0.2496 - acc: 0.9500 - val_loss: 4.1400 - val_acc: 0.0000e+00\n",
            "Epoch 33/300\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.76121\n",
            "60/60 - 0s - loss: 0.2520 - acc: 0.9000 - val_loss: 4.2160 - val_acc: 0.0000e+00\n",
            "Epoch 34/300\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.76121\n",
            "60/60 - 0s - loss: 0.2334 - acc: 0.9500 - val_loss: 4.3175 - val_acc: 0.0000e+00\n",
            "Epoch 35/300\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.76121\n",
            "60/60 - 0s - loss: 0.2592 - acc: 0.8667 - val_loss: 4.3658 - val_acc: 0.0000e+00\n",
            "Epoch 36/300\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.76121\n",
            "60/60 - 0s - loss: 0.2692 - acc: 0.9333 - val_loss: 4.3978 - val_acc: 0.0000e+00\n",
            "Epoch 37/300\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.76121\n",
            "60/60 - 0s - loss: 0.2539 - acc: 0.9000 - val_loss: 4.4353 - val_acc: 0.0000e+00\n",
            "Epoch 38/300\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.76121\n",
            "60/60 - 0s - loss: 0.2101 - acc: 0.9500 - val_loss: 4.5011 - val_acc: 0.0000e+00\n",
            "Epoch 39/300\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.76121\n",
            "60/60 - 0s - loss: 0.2344 - acc: 0.9500 - val_loss: 4.5709 - val_acc: 0.0000e+00\n",
            "Epoch 40/300\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.76121\n",
            "60/60 - 0s - loss: 0.2639 - acc: 0.9167 - val_loss: 4.5929 - val_acc: 0.0000e+00\n",
            "Epoch 41/300\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.76121\n",
            "60/60 - 0s - loss: 0.2490 - acc: 0.8500 - val_loss: 4.5232 - val_acc: 0.0000e+00\n",
            "Epoch 42/300\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.76121\n",
            "60/60 - 0s - loss: 0.1909 - acc: 0.9667 - val_loss: 4.5053 - val_acc: 0.0000e+00\n",
            "Epoch 43/300\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.76121\n",
            "60/60 - 0s - loss: 0.2442 - acc: 0.9333 - val_loss: 4.5078 - val_acc: 0.0000e+00\n",
            "Epoch 44/300\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.76121\n",
            "60/60 - 0s - loss: 0.2053 - acc: 0.9333 - val_loss: 4.5239 - val_acc: 0.0000e+00\n",
            "Epoch 45/300\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.76121\n",
            "60/60 - 0s - loss: 0.2132 - acc: 0.9667 - val_loss: 4.5749 - val_acc: 0.0000e+00\n",
            "Epoch 46/300\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.76121\n",
            "60/60 - 0s - loss: 0.2324 - acc: 0.9167 - val_loss: 4.6351 - val_acc: 0.0000e+00\n",
            "Epoch 47/300\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.76121\n",
            "60/60 - 0s - loss: 0.2042 - acc: 0.9500 - val_loss: 4.6730 - val_acc: 0.0000e+00\n",
            "Epoch 48/300\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.76121\n",
            "60/60 - 0s - loss: 0.2101 - acc: 0.9500 - val_loss: 4.6941 - val_acc: 0.0000e+00\n",
            "Epoch 49/300\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.76121\n",
            "60/60 - 0s - loss: 0.1993 - acc: 0.9500 - val_loss: 4.7299 - val_acc: 0.0000e+00\n",
            "Epoch 50/300\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.76121\n",
            "60/60 - 0s - loss: 0.2141 - acc: 0.9500 - val_loss: 4.7220 - val_acc: 0.0500\n",
            "Epoch 51/300\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.76121\n",
            "60/60 - 0s - loss: 0.2068 - acc: 0.9333 - val_loss: 4.6779 - val_acc: 0.0500\n",
            "Epoch 52/300\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.76121\n",
            "60/60 - 0s - loss: 0.2000 - acc: 0.9333 - val_loss: 4.6769 - val_acc: 0.0500\n",
            "Epoch 53/300\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.76121\n",
            "60/60 - 0s - loss: 0.2229 - acc: 0.9500 - val_loss: 4.6427 - val_acc: 0.0500\n",
            "Epoch 54/300\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.76121\n",
            "60/60 - 0s - loss: 0.1975 - acc: 0.9500 - val_loss: 4.6632 - val_acc: 0.1000\n",
            "Epoch 55/300\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.76121\n",
            "60/60 - 0s - loss: 0.1761 - acc: 0.9667 - val_loss: 4.7171 - val_acc: 0.2000\n",
            "Epoch 56/300\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.76121\n",
            "60/60 - 0s - loss: 0.1838 - acc: 0.9667 - val_loss: 4.7574 - val_acc: 0.2000\n",
            "Epoch 57/300\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.76121\n",
            "60/60 - 0s - loss: 0.1806 - acc: 0.9667 - val_loss: 4.8106 - val_acc: 0.2000\n",
            "Epoch 58/300\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.76121\n",
            "60/60 - 0s - loss: 0.2031 - acc: 0.9333 - val_loss: 4.8263 - val_acc: 0.2000\n",
            "Epoch 59/300\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.76121\n",
            "60/60 - 0s - loss: 0.2132 - acc: 0.9333 - val_loss: 4.7921 - val_acc: 0.2000\n",
            "Epoch 60/300\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.76121\n",
            "60/60 - 0s - loss: 0.2124 - acc: 0.9667 - val_loss: 4.6816 - val_acc: 0.3000\n",
            "Epoch 61/300\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.76121\n",
            "60/60 - 0s - loss: 0.1831 - acc: 0.9833 - val_loss: 4.6128 - val_acc: 0.3000\n",
            "Epoch 62/300\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.76121\n",
            "60/60 - 0s - loss: 0.1851 - acc: 0.9500 - val_loss: 4.5849 - val_acc: 0.3000\n",
            "Epoch 63/300\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.76121\n",
            "60/60 - 0s - loss: 0.1878 - acc: 0.9667 - val_loss: 4.6039 - val_acc: 0.3000\n",
            "Epoch 64/300\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.76121\n",
            "60/60 - 0s - loss: 0.1900 - acc: 1.0000 - val_loss: 4.6663 - val_acc: 0.3000\n",
            "Epoch 65/300\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.76121\n",
            "60/60 - 0s - loss: 0.1706 - acc: 0.9833 - val_loss: 4.6462 - val_acc: 0.3000\n",
            "Epoch 66/300\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.76121\n",
            "60/60 - 0s - loss: 0.2040 - acc: 0.9500 - val_loss: 4.5959 - val_acc: 0.4500\n",
            "Epoch 67/300\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.76121\n",
            "60/60 - 0s - loss: 0.1508 - acc: 1.0000 - val_loss: 4.5875 - val_acc: 0.4500\n",
            "Epoch 68/300\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.76121\n",
            "60/60 - 0s - loss: 0.1634 - acc: 0.9667 - val_loss: 4.5408 - val_acc: 0.4500\n",
            "Epoch 69/300\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.76121\n",
            "60/60 - 0s - loss: 0.1986 - acc: 0.9833 - val_loss: 4.5076 - val_acc: 0.4500\n",
            "Epoch 70/300\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.76121\n",
            "60/60 - 0s - loss: 0.1730 - acc: 1.0000 - val_loss: 4.4421 - val_acc: 0.4500\n",
            "Epoch 71/300\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.76121\n",
            "60/60 - 0s - loss: 0.1767 - acc: 0.9667 - val_loss: 4.3675 - val_acc: 0.5000\n",
            "Epoch 72/300\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.76121\n",
            "60/60 - 0s - loss: 0.1716 - acc: 1.0000 - val_loss: 4.2636 - val_acc: 0.5000\n",
            "Epoch 73/300\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.76121\n",
            "60/60 - 0s - loss: 0.1488 - acc: 0.9667 - val_loss: 4.1943 - val_acc: 0.5000\n",
            "Epoch 74/300\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.76121\n",
            "60/60 - 0s - loss: 0.1739 - acc: 1.0000 - val_loss: 4.1327 - val_acc: 0.5000\n",
            "Epoch 75/300\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.76121\n",
            "60/60 - 0s - loss: 0.1544 - acc: 1.0000 - val_loss: 4.1091 - val_acc: 0.5000\n",
            "Epoch 76/300\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.76121\n",
            "60/60 - 0s - loss: 0.1703 - acc: 1.0000 - val_loss: 4.0816 - val_acc: 0.5000\n",
            "Epoch 77/300\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.76121\n",
            "60/60 - 0s - loss: 0.1589 - acc: 0.9833 - val_loss: 4.0930 - val_acc: 0.5000\n",
            "Epoch 78/300\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.76121\n",
            "60/60 - 0s - loss: 0.1507 - acc: 1.0000 - val_loss: 4.0868 - val_acc: 0.5000\n",
            "Epoch 79/300\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.76121\n",
            "60/60 - 0s - loss: 0.1402 - acc: 0.9667 - val_loss: 4.0899 - val_acc: 0.5000\n",
            "Epoch 80/300\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.76121\n",
            "60/60 - 0s - loss: 0.1528 - acc: 0.9833 - val_loss: 4.0331 - val_acc: 0.5000\n",
            "Epoch 81/300\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.76121\n",
            "60/60 - 0s - loss: 0.1524 - acc: 0.9500 - val_loss: 3.9331 - val_acc: 0.5000\n",
            "Epoch 82/300\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-b527a2a340fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    146\u001b[0m               history = model.fit(X_train, Y_train, batch_size = 16, epochs = n_epochs, \n\u001b[1;32m    147\u001b[0m                                   \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m                                   callbacks=[checkpointer], class_weight = class_weights)\n\u001b[0m\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m               \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n# Evaluate on test data'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m   def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    673\u001b[0m         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m         \u001b[0mvalidation_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m    676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m   def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3475\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3476\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3477\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3478\u001b[0m     output_structure = nest.pack_sequence_as(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kd6FTGs7vBI3",
        "colab_type": "code",
        "outputId": "207558d7-2dc6-4750-e7aa-95800b374dcb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "print(acc_sub.shape)\n",
        "print(acc_sub[:,0,0,0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10, 2, 2, 3)\n",
            "[0.5        0.5        0.5        0.5        0.5        0.5\n",
            " 0.5        0.5        0.57499999 0.85000002]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}