{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "main_Single_Trial_Classification_Patient_Data_Binary_Grid",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sagihaider/EEG_Deep/blob/master/main_Single_Trial_Classification_Patient_Data_Binary_Grid_New.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X110PE9MjrlE",
        "colab_type": "code",
        "outputId": "010eb606-101a-4f92-92f6-79ec4de3e77e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        }
      },
      "source": [
        "!git clone https://github.com/sagihaider/EEG_Deep.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'EEG_Deep'...\n",
            "remote: Enumerating objects: 43, done.\u001b[K\n",
            "remote: Counting objects: 100% (43/43), done.\u001b[K\n",
            "remote: Compressing objects: 100% (43/43), done.\u001b[K\n",
            "remote: Total 356 (delta 25), reused 0 (delta 0), pack-reused 313\u001b[K\n",
            "Receiving objects: 100% (356/356), 1.69 GiB | 43.83 MiB/s, done.\n",
            "Resolving deltas: 100% (153/153), done.\n",
            "Checking out files: 100% (96/96), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ioLzMoiS60Qf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 65
        },
        "outputId": "c092f979-ff3d-49f3-de0e-68ae10998186"
      },
      "source": [
        "import scipy.io as spio\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from importlib.machinery import SourceFileLoader\n",
        "\n",
        "# EEGNet-specific imports\n",
        "from EEG_Deep.EEGModels import EEGNet, ShallowConvNet, DeepConvNet\n",
        "from tensorflow.keras import utils as np_utils\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras import backend as K\n",
        "K.set_image_data_format('channels_first')\n",
        "\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# tools for plotting confusion matrices\n",
        "from matplotlib import pyplot as plt\n",
        "from scipy.signal import butter, lfilter"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mo80Jg_Pn5lP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Band-pass Filter\n",
        "def butter_bandpass_filter(data, lowcut, highcut, fs, order=4):\n",
        "    nyq = 0.5 * fs\n",
        "    low = lowcut / nyq\n",
        "    high = highcut / nyq\n",
        "    b, a = butter(order, [low, high], btype='band')\n",
        "    y = lfilter(b, a, data)\n",
        "    return y\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZjtnE_6RSpum",
        "colab_type": "code",
        "outputId": "0544f055-96fb-432a-d922-047c4da2bac0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from numpy import zeros\n",
        "K.clear_session()\n",
        "cols = 2\n",
        "rows = 3\n",
        "ndim = 10\n",
        "acc_all = zeros([rows, cols, ndim])\n",
        "loss_all = zeros([rows, cols])\n",
        "X_tr = np.empty([80, 12, 4096])\n",
        "X_ts = np.empty([40, 12, 4096])\n",
        "result=[]\n",
        "\n",
        "h_cut = [30,40]\n",
        "drop_out = [0.25,0.5]\n",
        "k_len = [32, 64, 128]\n",
        "n_epochs = 5\n",
        "\n",
        "for x in range(1,3):\n",
        "    for h in h_cut:\n",
        "        fName = 'EEG_Deep/Data2A/parsed_P0' + str(x) + 'T.mat'  # Load Data\n",
        "        print(fName)\n",
        "        mat = spio.loadmat(fName)\n",
        "        r_X_tr = mat['RawEEGData']\n",
        "        y_tr = mat['Labels']\n",
        "        y_tr = y_tr.flatten() \n",
        "\n",
        "        print(np.shape(r_X_tr))\n",
        "        print(np.shape(y_tr))\n",
        "\n",
        "        for t in range(r_X_tr.shape[0]):\n",
        "            tril = r_X_tr[t,:,:]\n",
        "            #tril = tril.transpose()\n",
        "            tril_filtered = butter_bandpass_filter(tril, lowcut=4, highcut=h, fs=250, order=4)\n",
        "            # tril_filtered = tril_filtered.transpose()\n",
        "            X_tr[t,:,:] = tril_filtered \n",
        "\n",
        "            # split data of each subject in training and validation\n",
        "        X_train      = X_tr[0:60,:,2048:3584]\n",
        "        Y_train      = y_tr[0:60]\n",
        "        X_val       = X_tr[60:,:,2048:3584]\n",
        "        Y_val       = y_tr[60:]\n",
        "\n",
        "        print(np.shape(X_train))\n",
        "        print(np.shape(Y_train))\n",
        "        print(np.shape(X_val))\n",
        "        print(np.shape(Y_val))\n",
        "\n",
        "        # convert labels to one-hot encodings.\n",
        "        Y_train = np_utils.to_categorical(Y_train-1, num_classes=2)\n",
        "        Y_val = np_utils.to_categorical(Y_val-1, num_classes=2)\n",
        "\n",
        "        kernels, chans, samples = 1, 12, 1536\n",
        "        # convert data to NCHW (trials, kernels, channels, samples) format. Data \n",
        "        # contains 22 channels and 500 time-points. Set the number of kernels to 1.\n",
        "        X_train      = X_train.reshape(X_train.shape[0], kernels, chans, samples)\n",
        "        X_val       = X_val.reshape(X_val.shape[0], kernels, chans, samples)\n",
        "\n",
        "        print('X_train shape:', X_train.shape)\n",
        "        print(X_train.shape[0], 'train samples')\n",
        "        print(X_val.shape[0], 'val samples')\n",
        "\n",
        "        fName = 'EEG_Deep/Data2A/parsed_P0' + str(x) + 'E.mat'  # Load Data\n",
        "        print(fName)\n",
        "        mat = spio.loadmat(fName)\n",
        "        r_X_ts = mat['RawEEGData']\n",
        "        y_ts = mat['Labels']\n",
        "        y_ts = y_ts.flatten() \n",
        "\n",
        "        print(np.shape(r_X_ts))\n",
        "        print(np.shape(y_ts))\n",
        "\n",
        "        for t in range(r_X_ts.shape[0]):\n",
        "            tril = r_X_ts[t,:,:]\n",
        "            # tril = tril.transpose()\n",
        "            tril_filtered = butter_bandpass_filter(tril, lowcut=4, highcut=h, fs=250, order=4)\n",
        "            # tril_filtered = tril_filtered.transpose()\n",
        "            X_ts[t,:,:] = tril_filtered \n",
        "\n",
        "        X_test      = X_ts[:,:,2048:3584]\n",
        "        Y_test      = y_ts[:]\n",
        "        print(np.shape(X_test))\n",
        "        print(np.shape(Y_test))\n",
        "\n",
        "        #convert labels to one-hot encodings.\n",
        "        Y_test      = np_utils.to_categorical(Y_test-1)\n",
        "\n",
        "        # convert data to NCHW (trials, kernels, channels, samples) format. Data \n",
        "        # contains 22 channels and 500 time-points. Set the number of kernels to 1.\n",
        "        X_test      = X_test.reshape(X_test.shape[0], kernels, chans, samples)\n",
        "\n",
        "        print('X_train shape:', X_test.shape)\n",
        "        print(X_test.shape[0], 'test samples')\n",
        "\n",
        "        kernels, chans, samples = 1, 12, 1536\n",
        "        # convert data to NCHW (trials, kernels, channels, samples) format. Data \n",
        "        # contains 22 channels and 500 time-points. Set the number of kernels to 1.\n",
        "        X_train = X_train.reshape(X_train.shape[0], kernels, chans, samples)\n",
        "        X_val   = X_val.reshape(X_val.shape[0], kernels, chans, samples)\n",
        "\n",
        "        print('X_train shape:', X_train.shape)\n",
        "        print(X_train.shape[0], 'train samples')\n",
        "        print(X_val.shape[0], 'val samples')\n",
        "\n",
        "        X_test      = X_ts[:,:,2048:3584]\n",
        "        Y_test      = y_ts[:]\n",
        "        print(np.shape(X_test))\n",
        "        print(np.shape(Y_test))\n",
        "\n",
        "        #convert labels to one-hot encodings.\n",
        "        Y_test      = np_utils.to_categorical(Y_test-1, num_classes=2)\n",
        "\n",
        "        # convert data to NCHW (trials, kernels, channels, samples) format. Data \n",
        "        # contains 22 channels and 500 time-points. Set the number of kernels to 1.\n",
        "        X_test      = X_test.reshape(X_test.shape[0], kernels, chans, samples)\n",
        "\n",
        "        print('X_train shape:', X_test.shape)\n",
        "        print(X_test.shape[0], 'train samples')\n",
        "\n",
        "        for id_d, d in enumerate(drop_out):\n",
        "            for id_kl, kl in enumerate(k_len):\n",
        "              print(id_kl, id_d)\n",
        "              # configure the EEGNet-8,2,16 model with kernel length of 32 samples (other \n",
        "              # model configurations may do better, but this is a good starting point)\n",
        "              model = EEGNet(nb_classes = 2, Chans = 12, Samples = 1536,\n",
        "                             dropoutRate = d, kernLength = kl, F1 = 8,D = 2, F2 = 16,\n",
        "                             norm_rate = 0.25, dropoutType = 'Dropout')\n",
        "\n",
        "              # compile the model and set the optimizers\n",
        "              model.compile(loss='categorical_crossentropy', optimizer='adam', \n",
        "                            metrics = ['accuracy'])\n",
        "\n",
        "              # count number of parameters in the model\n",
        "              numParams    = model.count_params() \n",
        "\n",
        "              # set a valid path for your system to record model checkpoints\n",
        "              checkpointer = ModelCheckpoint(filepath='/tmp/checkpoint.h5', verbose=1, save_best_only=True)\n",
        "\n",
        "              # the syntax is {class_1:weight_1, class_2:weight_2,...}. Here just setting\n",
        "              # the weights all to be 1\n",
        "              class_weights = {0:1, 1:1}\n",
        "\n",
        "              history = model.fit(X_train, Y_train, batch_size = 16, epochs = n_epochs, \n",
        "                                  verbose = 2, validation_data=(X_val, Y_val),\n",
        "                                  callbacks=[checkpointer], class_weight = class_weights)\n",
        "\n",
        "              print('\\n# Evaluate on test data')\n",
        "              results = model.evaluate(X_test, Y_test, batch_size=1)\n",
        "              print('test loss, test acc:', results)\n",
        "\n",
        "              # acc_all[x - 1, 0] = results[0]\n",
        "              acc_all[id_d,id_kl, x - 1] = results[1]\n",
        "              print(acc_all)\n",
        "              from keras import backend as K \n",
        "              # Do some code, e.g. train and save model\n",
        "              K.clear_session()\n",
        "\n",
        "        # out_f_name = 'df_accl_Patient_h' + str(h) + '_d' + str(d) + '_k' + str(kl) +'_epochs_' + str(n_epochs) + '.csv'\n",
        "        # print(out_f_name)\n",
        "        # df_accl_all = pd.DataFrame({'loss': acc_all[:, 0], 'acc': acc_all[:, 1]})\n",
        "        # print(df_accl_all)\n",
        "        # df_accl_all.to_csv (out_f_name, index = None, header=True) #Don't forget to add '.csv' at the end of the path"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "EEG_Deep/Data2A/parsed_P01T.mat\n",
            "(80, 12, 4096)\n",
            "(80,)\n",
            "(60, 12, 1536)\n",
            "(60,)\n",
            "(20, 12, 1536)\n",
            "(20,)\n",
            "X_train shape: (60, 1, 12, 1536)\n",
            "60 train samples\n",
            "20 val samples\n",
            "EEG_Deep/Data2A/parsed_P01E.mat\n",
            "(40, 12, 4096)\n",
            "(40,)\n",
            "(40, 12, 1536)\n",
            "(40,)\n",
            "X_train shape: (40, 1, 12, 1536)\n",
            "40 test samples\n",
            "X_train shape: (60, 1, 12, 1536)\n",
            "60 train samples\n",
            "20 val samples\n",
            "(40, 12, 1536)\n",
            "(40,)\n",
            "X_train shape: (40, 1, 12, 1536)\n",
            "40 train samples\n",
            "0 0\n",
            "Train on 60 samples, validate on 20 samples\n",
            "Epoch 1/5\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.77749, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 1s - loss: 0.6733 - acc: 0.6667 - val_loss: 0.7775 - val_acc: 0.0000e+00\n",
            "Epoch 2/5\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.77749\n",
            "60/60 - 0s - loss: 0.6033 - acc: 0.6667 - val_loss: 0.8724 - val_acc: 0.0000e+00\n",
            "Epoch 3/5\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.77749\n",
            "60/60 - 0s - loss: 0.5828 - acc: 0.6667 - val_loss: 0.8956 - val_acc: 0.0000e+00\n",
            "Epoch 4/5\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.77749\n",
            "60/60 - 0s - loss: 0.5672 - acc: 0.6667 - val_loss: 0.8949 - val_acc: 0.0000e+00\n",
            "Epoch 5/5\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.77749\n",
            "60/60 - 0s - loss: 0.5529 - acc: 0.6667 - val_loss: 0.8892 - val_acc: 0.0000e+00\n",
            "\n",
            "# Evaluate on test data\n",
            "40/40 [==============================] - 0s 2ms/sample - loss: 0.6882 - acc: 0.5000\n",
            "test loss, test acc: [0.6881801411509514, 0.5]\n",
            "[[[0.5 0.  0.  0.  0.  0.  0.  0.  0.  0. ]\n",
            "  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0. ]]\n",
            "\n",
            " [[0.  0.  0.  0.  0.  0.  0.  0.  0.  0. ]\n",
            "  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0. ]]\n",
            "\n",
            " [[0.  0.  0.  0.  0.  0.  0.  0.  0.  0. ]\n",
            "  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0. ]]]\n",
            "1 0\n",
            "Train on 60 samples, validate on 20 samples\n",
            "Epoch 1/5\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.81455, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 1s - loss: 0.6832 - acc: 0.5833 - val_loss: 0.8145 - val_acc: 0.0000e+00\n",
            "Epoch 2/5\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.81455\n",
            "60/60 - 0s - loss: 0.6008 - acc: 0.6667 - val_loss: 0.8949 - val_acc: 0.0000e+00\n",
            "Epoch 3/5\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.81455\n",
            "60/60 - 0s - loss: 0.5838 - acc: 0.6667 - val_loss: 0.9510 - val_acc: 0.0000e+00\n",
            "Epoch 4/5\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.81455\n",
            "60/60 - 0s - loss: 0.5543 - acc: 0.6667 - val_loss: 0.9891 - val_acc: 0.0000e+00\n",
            "Epoch 5/5\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.81455\n",
            "60/60 - 0s - loss: 0.5519 - acc: 0.6667 - val_loss: 1.0121 - val_acc: 0.0000e+00\n",
            "\n",
            "# Evaluate on test data\n",
            "40/40 [==============================] - 0s 2ms/sample - loss: 0.7003 - acc: 0.5000\n",
            "test loss, test acc: [0.7002917692065239, 0.5]\n",
            "[[[0.5 0.  0.  0.  0.  0.  0.  0.  0.  0. ]\n",
            "  [0.5 0.  0.  0.  0.  0.  0.  0.  0.  0. ]]\n",
            "\n",
            " [[0.  0.  0.  0.  0.  0.  0.  0.  0.  0. ]\n",
            "  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0. ]]\n",
            "\n",
            " [[0.  0.  0.  0.  0.  0.  0.  0.  0.  0. ]\n",
            "  [0.  0.  0.  0.  0.  0.  0.  0.  0.  0. ]]]\n",
            "2 0\n",
            "Train on 60 samples, validate on 20 samples\n",
            "Epoch 1/5\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.78999, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 1s - loss: 0.6886 - acc: 0.5667 - val_loss: 0.7900 - val_acc: 0.0000e+00\n",
            "Epoch 2/5\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.78999\n",
            "60/60 - 0s - loss: 0.6103 - acc: 0.6667 - val_loss: 0.8662 - val_acc: 0.0000e+00\n",
            "Epoch 3/5\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.78999\n",
            "60/60 - 0s - loss: 0.5820 - acc: 0.6667 - val_loss: 0.9174 - val_acc: 0.0000e+00\n",
            "Epoch 4/5\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.78999\n",
            "60/60 - 0s - loss: 0.5664 - acc: 0.6667 - val_loss: 0.9545 - val_acc: 0.0000e+00\n",
            "Epoch 5/5\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.78999\n",
            "60/60 - 0s - loss: 0.5309 - acc: 0.6667 - val_loss: 0.9939 - val_acc: 0.0000e+00\n",
            "\n",
            "# Evaluate on test data\n",
            "40/40 [==============================] - 0s 2ms/sample - loss: 0.6923 - acc: 0.5000\n",
            "test loss, test acc: [0.6923437923192978, 0.5]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-6a3b4b30a5b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m               \u001b[0;31m# acc_all[x - 1, 0] = results[0]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m               \u001b[0macc_all\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid_d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid_kl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m               \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc_all\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m               \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: index 2 is out of bounds for axis 1 with size 2"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kd6FTGs7vBI3",
        "colab_type": "code",
        "outputId": "3f35b1d7-95a8-42a8-f1c7-b9e524665465",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(model.metrics_names)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['loss', 'acc']\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}