{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "main_Single_Trial_Classification_PatientData_Binary",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sagihaider/EEG_Deep/blob/master/main_Single_Trial_Classification_PatientData_Binary.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X110PE9MjrlE",
        "colab_type": "code",
        "outputId": "2fa8b6f5-b6a3-4374-ae04-415fb7a8070e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        }
      },
      "source": [
        "!git clone https://github.com/sagihaider/EEG_Deep.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'EEG_Deep'...\n",
            "remote: Enumerating objects: 60, done.\u001b[K\n",
            "remote: Counting objects: 100% (60/60), done.\u001b[K\n",
            "remote: Compressing objects: 100% (55/55), done.\u001b[K\n",
            "remote: Total 213 (delta 31), reused 12 (delta 5), pack-reused 153\u001b[K\n",
            "Receiving objects: 100% (213/213), 859.08 MiB | 43.23 MiB/s, done.\n",
            "Resolving deltas: 100% (84/84), done.\n",
            "Checking out files: 100% (61/61), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ioLzMoiS60Qf",
        "colab_type": "code",
        "outputId": "7adb902c-913c-4725-8c26-9f01e25fb158",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 65
        }
      },
      "source": [
        "import scipy.io as spio\n",
        "import numpy as np\n",
        "from importlib.machinery import SourceFileLoader\n",
        "\n",
        "# EEGNet-specific imports\n",
        "from EEG_Deep.EEGModels import EEGNet, ShallowConvNet, DeepConvNet\n",
        "from tensorflow.keras import utils as np_utils\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras import backend as K\n",
        "K.set_image_data_format('channels_first')\n",
        "\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# tools for plotting confusion matrices\n",
        "from matplotlib import pyplot as plt\n",
        "from scipy.signal import butter, lfilter"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mo80Jg_Pn5lP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Band-pass Filter\n",
        "def butter_bandpass_filter(data, lowcut, highcut, fs, order=4):\n",
        "    nyq = 0.5 * fs\n",
        "    low = lowcut / nyq\n",
        "    high = highcut / nyq\n",
        "    b, a = butter(order, [low, high], btype='band')\n",
        "    y = lfilter(b, a, data)\n",
        "    return y\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZjtnE_6RSpum",
        "colab_type": "code",
        "outputId": "e516ef49-a38e-409a-d11e-e0dadbd88445",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from numpy import zeros\n",
        "K.clear_session()\n",
        "cols = 1\n",
        "rows = 10\n",
        "acc_all = zeros([rows, cols])\n",
        "loss_all = zeros([rows, cols])\n",
        "X_tr_c12 = np.empty([80, 12, 4096])\n",
        "X_ts_c12 = np.empty([80, 12, 4096])\n",
        "result=[]\n",
        "\n",
        "from itertools import combinations \n",
        "comb = combinations([1, 2], 2) \n",
        "  # Print the obtained combinations \n",
        "bincomb=[]\n",
        "for i in list(comb): \n",
        "    bincomb.append(i)\n",
        "\n",
        "for x in range(1,11):\n",
        "  fName = 'EEG_Deep/Data2A/parsed_P0' + str(x) + 'T.mat'  # Load Data\n",
        "  print(fName)\n",
        "  mat = spio.loadmat(fName)\n",
        "  r_X_tr = mat['RawEEGData']\n",
        "  r_y_tr = mat['Labels']\n",
        "\n",
        "  ### Filter Data ###\n",
        "  for t in range(r_X_tr.shape[0]):\n",
        "    tril = r_X_tr[t,:,:]\n",
        "    # tril = tril.transpose()\n",
        "    tril_filtered = butter_bandpass_filter(tril, \n",
        "                                              lowcut=8, \n",
        "                                              highcut=24, \n",
        "                                              fs=512,\n",
        "                                              order=4)\n",
        "    # tril_filtered = tril_filtered.transpose()\n",
        "    X_tr_c12[t,:,:] = tril_filtered\n",
        "\n",
        "  print(\"Filtering of Training Data Finished\")\n",
        "  ## Test Data Load \n",
        "\n",
        "  fName = 'EEG_Deep/Data2A/parsed_P0' + str(x) + 'E.mat'  # Load Data\n",
        "  print(fName)\n",
        "  mat = spio.loadmat(fName)\n",
        "  r_X_ts = mat['RawEEGData']\n",
        "  r_y_ts = mat['Labels']\n",
        "  for t in range(r_X_ts.shape[0]):\n",
        "    tril = r_X_ts[t,:,:]\n",
        "    # tril = tril.transpose()\n",
        "    tril_filtered = butter_bandpass_filter(tril, \n",
        "                                              lowcut=8, \n",
        "                                              highcut=24, \n",
        "                                              fs=512,\n",
        "                                              order=4)\n",
        "    # tril_filtered = tril_filtered.transpose()\n",
        "    X_ts_c12[t,:,:] = tril_filtered\n",
        "  \n",
        "  print(\"Filtering of Testing Data Finished\")    \n",
        "\n",
        "  for k, com in enumerate(bincomb):\n",
        "      print(com)\n",
        "      # # Find labels to specific class\n",
        "      print(\"Finding labels in training data\")\n",
        "      class1indx = list(np.where(r_y_tr == com[0]))\n",
        "      class2indx = list(np.where(r_y_tr == com[1]))\n",
        "      c1=list(class1indx[0])\n",
        "      c2=list(class2indx[0])\n",
        "      y_tr_c12 = c1 + c2\n",
        "      y_tr_c12.sort()\n",
        "      # print(y_tr_c12)\n",
        "      x_tr_12 = X_tr_c12[y_tr_c12,:,:]\n",
        "      y_tr_12 = r_y_tr[y_tr_c12]\n",
        "      # print(np.shape(x_tr_12))\n",
        "      # print(np.shape(y_tr_12))\n",
        "      # # Find labels to specific class\n",
        "      print(\"Finding labels in testing data\")\n",
        "      class1indx = list(np.where(r_y_ts == com[0]))\n",
        "      class2indx = list(np.where(r_y_ts == com[1]))\n",
        "      c1=list(class1indx[0])\n",
        "      c2=list(class2indx[0])\n",
        "      y_ts_c12 = c1 + c2\n",
        "      y_ts_c12.sort()\n",
        "      # print(y_ts_c12)\n",
        "      x_ts_12 = X_ts_c12[y_ts_c12,:,:]\n",
        "      y_ts_12 = r_y_ts[y_ts_c12]\n",
        "      # print(np.shape(x_ts_12))\n",
        "      # print(np.shape(y_ts_12))\n",
        "      del class1indx, class2indx, c1, c2\n",
        "\n",
        "      # shuffle the training data\n",
        "      indices = np.arange(x_tr_12.shape[0])\n",
        "      np.random.shuffle(indices)\n",
        "      x_tr_12 = x_tr_12[indices]\n",
        "      y_tr_12 = y_tr_12[indices]\n",
        "\n",
        "      # split data of each subject in training and validation\n",
        "      X_train = x_tr_12[0:60,:,2048:3584]\n",
        "      Y_train = y_tr_12[0:60].ravel()\n",
        "      X_val   = x_tr_12[60:,:,2048:3584]\n",
        "      Y_val   = y_tr_12[60:].ravel()\n",
        "      print(Y_val)\n",
        "      print(np.shape(X_train))\n",
        "      print(np.shape(Y_train))\n",
        "      print(np.shape(X_val))\n",
        "      print(np.shape(Y_val))\n",
        "  \n",
        "      # convert labels to one-hot encodings.\n",
        "      Y_train      = np_utils.to_categorical(Y_train-1, num_classes=2)\n",
        "      Y_val       = np_utils.to_categorical(Y_val-1, num_classes=2)\n",
        "      print(Y_val)\n",
        "\n",
        "      kernels, chans, samples = 1, 12, 1536\n",
        "      # convert data to NCHW (trials, kernels, channels, samples) format. Data \n",
        "      # contains 22 channels and 500 time-points. Set the number of kernels to 1.\n",
        "      X_train = X_train.reshape(X_train.shape[0], kernels, chans, samples)\n",
        "      X_val   = X_val.reshape(X_val.shape[0], kernels, chans, samples)\n",
        "   \n",
        "      print('X_train shape:', X_train.shape)\n",
        "      print(X_train.shape[0], 'train samples')\n",
        "      print(X_val.shape[0], 'val samples')\n",
        "\n",
        "      X_test      = x_ts_12[:,:,2048:3584]\n",
        "      Y_test      = y_ts_12[:]\n",
        "      print(np.shape(X_test))\n",
        "      print(np.shape(Y_test))\n",
        "\n",
        "      #convert labels to one-hot encodings.\n",
        "      Y_test      = np_utils.to_categorical(Y_test-1, num_classes=2)\n",
        "\n",
        "      # convert data to NCHW (trials, kernels, channels, samples) format. Data \n",
        "      # contains 22 channels and 500 time-points. Set the number of kernels to 1.\n",
        "      X_test      = X_test.reshape(X_test.shape[0], kernels, chans, samples)\n",
        "\n",
        "      print('X_train shape:', X_test.shape)\n",
        "      print(X_test.shape[0], 'train samples')\n",
        "\n",
        "      # configure the EEGNet-8,2,16 model with kernel length of 32 samples (other \n",
        "      # model configurations may do better, but this is a good starting point)\n",
        "      model = EEGNet(nb_classes = 2, Chans = 12, Samples = 1536,\n",
        "                     dropoutRate = 0.5, kernLength = 25, F1 = 8, \n",
        "                     D = 2, F2 = 16, norm_rate = 0.25, dropoutType = 'Dropout')\n",
        "      \n",
        "      # compile the model and set the optimizers\n",
        "      model.compile(loss='categorical_crossentropy', optimizer='adam', \n",
        "                    metrics = ['accuracy'])\n",
        "\n",
        "      # count number of parameters in the model\n",
        "      numParams    = model.count_params() \n",
        "\n",
        "      # set a valid path for your system to record model checkpoints\n",
        "      checkpointer = ModelCheckpoint(filepath='/tmp/checkpoint.h5', verbose=1,\n",
        "                                     save_best_only=True)\n",
        "  \n",
        "      # the syntax is {class_1:weight_1, class_2:weight_2,...}. Here just setting\n",
        "      # the weights all to be 1\n",
        "      class_weights = {0:1, 1:1}\n",
        "\n",
        "      history = model.fit(X_train, Y_train, batch_size = 16, epochs = 500, \n",
        "                      verbose = 2, validation_data=(X_val, Y_val),\n",
        "                      callbacks=[checkpointer], class_weight = class_weights)\n",
        "  \n",
        "      # Plot training & validation accuracy values\n",
        "      plt.plot(history.history['acc'])\n",
        "      plt.plot(history.history['val_acc'])\n",
        "      plt.title('Model accuracy')\n",
        "      plt.ylabel('Accuracy')\n",
        "      plt.xlabel('Epoch')\n",
        "      plt.legend(['Train', 'Test'], loc='upper left')\n",
        "      plt.show()\n",
        "      figName = 'Accuracy_A0' + str(x) + '.png'  \n",
        "      plt.savefig(figName)\n",
        "\n",
        "      print('\\n# Evaluate on test data')\n",
        "      results = model.evaluate(X_test, Y_test, batch_size=1)\n",
        "      print('test loss, test acc:', results)\n",
        "\n",
        "      loss_all[x - 1, k-1] = results[0]\n",
        "      acc_all[x - 1, k-1] = results[1]\n",
        "\n",
        "      from keras import backend as K \n",
        "      # Do some code, e.g. train and save model\n",
        "      K.clear_session()\n",
        "\n",
        "print(loss_all)\n",
        "print(acc_all)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "EEG_Deep/Data2A/parsed_P01T.mat\n",
            "Filtering of Training Data Finished\n",
            "EEG_Deep/Data2A/parsed_P01E.mat\n",
            "Filtering of Testing Data Finished\n",
            "(1, 2)\n",
            "Finding labels in training data\n",
            "Finding labels in testing data\n",
            "[1 2 1 1 1 2 2 2 2 1 1 1 2 2 1 1 2 1 2 1]\n",
            "(60, 12, 1536)\n",
            "(60,)\n",
            "(20, 12, 1536)\n",
            "(20,)\n",
            "[[1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]]\n",
            "X_train shape: (60, 1, 12, 1536)\n",
            "60 train samples\n",
            "20 val samples\n",
            "(40, 12, 1536)\n",
            "(40, 1)\n",
            "X_train shape: (40, 1, 12, 1536)\n",
            "40 train samples\n",
            "Train on 60 samples, validate on 20 samples\n",
            "Epoch 1/500\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.69254, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 1s - loss: 0.7523 - acc: 0.4500 - val_loss: 0.6925 - val_acc: 0.4500\n",
            "Epoch 2/500\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.69254\n",
            "60/60 - 0s - loss: 0.6918 - acc: 0.5333 - val_loss: 0.6929 - val_acc: 0.5000\n",
            "Epoch 3/500\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.69254\n",
            "60/60 - 0s - loss: 0.6772 - acc: 0.6333 - val_loss: 0.6931 - val_acc: 0.4000\n",
            "Epoch 4/500\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.69254\n",
            "60/60 - 0s - loss: 0.6541 - acc: 0.7000 - val_loss: 0.6927 - val_acc: 0.4500\n",
            "Epoch 5/500\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.69254 to 0.69231, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6602 - acc: 0.6667 - val_loss: 0.6923 - val_acc: 0.4500\n",
            "Epoch 6/500\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.69231 to 0.69144, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6524 - acc: 0.5833 - val_loss: 0.6914 - val_acc: 0.4500\n",
            "Epoch 7/500\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.69144 to 0.69103, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6261 - acc: 0.7500 - val_loss: 0.6910 - val_acc: 0.4500\n",
            "Epoch 8/500\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.69103\n",
            "60/60 - 0s - loss: 0.6165 - acc: 0.7333 - val_loss: 0.6911 - val_acc: 0.4500\n",
            "Epoch 9/500\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.69103 to 0.69072, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5995 - acc: 0.8000 - val_loss: 0.6907 - val_acc: 0.4500\n",
            "Epoch 10/500\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.69072 to 0.69010, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6229 - acc: 0.7667 - val_loss: 0.6901 - val_acc: 0.4500\n",
            "Epoch 11/500\n",
            "\n",
            "Epoch 00011: val_loss improved from 0.69010 to 0.68866, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6101 - acc: 0.7500 - val_loss: 0.6887 - val_acc: 0.4500\n",
            "Epoch 12/500\n",
            "\n",
            "Epoch 00012: val_loss improved from 0.68866 to 0.68684, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5814 - acc: 0.8500 - val_loss: 0.6868 - val_acc: 0.4500\n",
            "Epoch 13/500\n",
            "\n",
            "Epoch 00013: val_loss improved from 0.68684 to 0.68503, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5891 - acc: 0.8500 - val_loss: 0.6850 - val_acc: 0.4500\n",
            "Epoch 14/500\n",
            "\n",
            "Epoch 00014: val_loss improved from 0.68503 to 0.68338, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5806 - acc: 0.7833 - val_loss: 0.6834 - val_acc: 0.4500\n",
            "Epoch 15/500\n",
            "\n",
            "Epoch 00015: val_loss improved from 0.68338 to 0.68136, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5681 - acc: 0.9000 - val_loss: 0.6814 - val_acc: 0.4500\n",
            "Epoch 16/500\n",
            "\n",
            "Epoch 00016: val_loss improved from 0.68136 to 0.67837, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5560 - acc: 0.8667 - val_loss: 0.6784 - val_acc: 0.4500\n",
            "Epoch 17/500\n",
            "\n",
            "Epoch 00017: val_loss improved from 0.67837 to 0.67494, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5293 - acc: 0.8667 - val_loss: 0.6749 - val_acc: 0.4500\n",
            "Epoch 18/500\n",
            "\n",
            "Epoch 00018: val_loss improved from 0.67494 to 0.67131, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5258 - acc: 0.9000 - val_loss: 0.6713 - val_acc: 0.5000\n",
            "Epoch 19/500\n",
            "\n",
            "Epoch 00019: val_loss improved from 0.67131 to 0.66799, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5159 - acc: 0.9167 - val_loss: 0.6680 - val_acc: 0.5000\n",
            "Epoch 20/500\n",
            "\n",
            "Epoch 00020: val_loss improved from 0.66799 to 0.66531, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5218 - acc: 0.8667 - val_loss: 0.6653 - val_acc: 0.5000\n",
            "Epoch 21/500\n",
            "\n",
            "Epoch 00021: val_loss improved from 0.66531 to 0.66013, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5318 - acc: 0.8000 - val_loss: 0.6601 - val_acc: 0.5000\n",
            "Epoch 22/500\n",
            "\n",
            "Epoch 00022: val_loss improved from 0.66013 to 0.65544, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5016 - acc: 0.9000 - val_loss: 0.6554 - val_acc: 0.5000\n",
            "Epoch 23/500\n",
            "\n",
            "Epoch 00023: val_loss improved from 0.65544 to 0.65078, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4959 - acc: 0.8500 - val_loss: 0.6508 - val_acc: 0.5000\n",
            "Epoch 24/500\n",
            "\n",
            "Epoch 00024: val_loss improved from 0.65078 to 0.64552, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5210 - acc: 0.8000 - val_loss: 0.6455 - val_acc: 0.5000\n",
            "Epoch 25/500\n",
            "\n",
            "Epoch 00025: val_loss improved from 0.64552 to 0.64032, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4728 - acc: 0.9167 - val_loss: 0.6403 - val_acc: 0.4500\n",
            "Epoch 26/500\n",
            "\n",
            "Epoch 00026: val_loss improved from 0.64032 to 0.63768, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5070 - acc: 0.8500 - val_loss: 0.6377 - val_acc: 0.5000\n",
            "Epoch 27/500\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.63768\n",
            "60/60 - 0s - loss: 0.4792 - acc: 0.9000 - val_loss: 0.6406 - val_acc: 0.5000\n",
            "Epoch 28/500\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.63768\n",
            "60/60 - 0s - loss: 0.4705 - acc: 0.8667 - val_loss: 0.6401 - val_acc: 0.5000\n",
            "Epoch 29/500\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.63768\n",
            "60/60 - 0s - loss: 0.4680 - acc: 0.8667 - val_loss: 0.6391 - val_acc: 0.5000\n",
            "Epoch 30/500\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.63768\n",
            "60/60 - 0s - loss: 0.5013 - acc: 0.8000 - val_loss: 0.6428 - val_acc: 0.5000\n",
            "Epoch 31/500\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.63768\n",
            "60/60 - 0s - loss: 0.4817 - acc: 0.8667 - val_loss: 0.6440 - val_acc: 0.5000\n",
            "Epoch 32/500\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.63768\n",
            "60/60 - 0s - loss: 0.4489 - acc: 0.9333 - val_loss: 0.6478 - val_acc: 0.4500\n",
            "Epoch 33/500\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.63768\n",
            "60/60 - 0s - loss: 0.4341 - acc: 0.9333 - val_loss: 0.6463 - val_acc: 0.4500\n",
            "Epoch 34/500\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.63768\n",
            "60/60 - 0s - loss: 0.4609 - acc: 0.8833 - val_loss: 0.6439 - val_acc: 0.5000\n",
            "Epoch 35/500\n",
            "\n",
            "Epoch 00035: val_loss improved from 0.63768 to 0.63149, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4375 - acc: 0.8833 - val_loss: 0.6315 - val_acc: 0.5000\n",
            "Epoch 36/500\n",
            "\n",
            "Epoch 00036: val_loss improved from 0.63149 to 0.62686, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4146 - acc: 0.9167 - val_loss: 0.6269 - val_acc: 0.5000\n",
            "Epoch 37/500\n",
            "\n",
            "Epoch 00037: val_loss improved from 0.62686 to 0.61816, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4278 - acc: 0.8500 - val_loss: 0.6182 - val_acc: 0.5000\n",
            "Epoch 38/500\n",
            "\n",
            "Epoch 00038: val_loss improved from 0.61816 to 0.60654, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4287 - acc: 0.8833 - val_loss: 0.6065 - val_acc: 0.5500\n",
            "Epoch 39/500\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.60654\n",
            "60/60 - 0s - loss: 0.3894 - acc: 0.9000 - val_loss: 0.6114 - val_acc: 0.5500\n",
            "Epoch 40/500\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.60654\n",
            "60/60 - 0s - loss: 0.3769 - acc: 0.9333 - val_loss: 0.6178 - val_acc: 0.5500\n",
            "Epoch 41/500\n",
            "\n",
            "Epoch 00041: val_loss improved from 0.60654 to 0.60614, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3758 - acc: 0.8500 - val_loss: 0.6061 - val_acc: 0.5500\n",
            "Epoch 42/500\n",
            "\n",
            "Epoch 00042: val_loss improved from 0.60614 to 0.60246, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3897 - acc: 0.8667 - val_loss: 0.6025 - val_acc: 0.5500\n",
            "Epoch 43/500\n",
            "\n",
            "Epoch 00043: val_loss improved from 0.60246 to 0.58472, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3506 - acc: 0.9000 - val_loss: 0.5847 - val_acc: 0.6000\n",
            "Epoch 44/500\n",
            "\n",
            "Epoch 00044: val_loss improved from 0.58472 to 0.55332, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3836 - acc: 0.8833 - val_loss: 0.5533 - val_acc: 0.6000\n",
            "Epoch 45/500\n",
            "\n",
            "Epoch 00045: val_loss improved from 0.55332 to 0.51812, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3567 - acc: 0.9333 - val_loss: 0.5181 - val_acc: 0.6500\n",
            "Epoch 46/500\n",
            "\n",
            "Epoch 00046: val_loss improved from 0.51812 to 0.48895, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3703 - acc: 0.8500 - val_loss: 0.4890 - val_acc: 0.8000\n",
            "Epoch 47/500\n",
            "\n",
            "Epoch 00047: val_loss improved from 0.48895 to 0.47205, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3770 - acc: 0.8833 - val_loss: 0.4720 - val_acc: 0.8000\n",
            "Epoch 48/500\n",
            "\n",
            "Epoch 00048: val_loss improved from 0.47205 to 0.46588, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3504 - acc: 0.9000 - val_loss: 0.4659 - val_acc: 0.8000\n",
            "Epoch 49/500\n",
            "\n",
            "Epoch 00049: val_loss improved from 0.46588 to 0.46435, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3140 - acc: 0.9333 - val_loss: 0.4644 - val_acc: 0.8000\n",
            "Epoch 50/500\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.46435\n",
            "60/60 - 0s - loss: 0.3736 - acc: 0.8667 - val_loss: 0.4654 - val_acc: 0.8000\n",
            "Epoch 51/500\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.46435\n",
            "60/60 - 0s - loss: 0.3169 - acc: 0.9500 - val_loss: 0.4657 - val_acc: 0.8000\n",
            "Epoch 52/500\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.46435\n",
            "60/60 - 0s - loss: 0.3246 - acc: 0.9000 - val_loss: 0.4934 - val_acc: 0.7000\n",
            "Epoch 53/500\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.46435\n",
            "60/60 - 0s - loss: 0.3372 - acc: 0.9167 - val_loss: 0.5204 - val_acc: 0.7000\n",
            "Epoch 54/500\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.46435\n",
            "60/60 - 0s - loss: 0.3383 - acc: 0.9167 - val_loss: 0.5458 - val_acc: 0.7000\n",
            "Epoch 55/500\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.46435\n",
            "60/60 - 0s - loss: 0.2993 - acc: 0.9667 - val_loss: 0.5330 - val_acc: 0.7000\n",
            "Epoch 56/500\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.46435\n",
            "60/60 - 0s - loss: 0.3145 - acc: 0.9333 - val_loss: 0.5113 - val_acc: 0.7000\n",
            "Epoch 57/500\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.46435\n",
            "60/60 - 0s - loss: 0.3160 - acc: 0.9167 - val_loss: 0.5007 - val_acc: 0.7000\n",
            "Epoch 58/500\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.46435\n",
            "60/60 - 0s - loss: 0.3558 - acc: 0.8667 - val_loss: 0.4973 - val_acc: 0.7000\n",
            "Epoch 59/500\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.46435\n",
            "60/60 - 0s - loss: 0.2552 - acc: 0.9667 - val_loss: 0.4932 - val_acc: 0.7500\n",
            "Epoch 60/500\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.46435\n",
            "60/60 - 0s - loss: 0.3627 - acc: 0.8667 - val_loss: 0.4953 - val_acc: 0.7500\n",
            "Epoch 61/500\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.46435\n",
            "60/60 - 0s - loss: 0.3340 - acc: 0.8333 - val_loss: 0.4820 - val_acc: 0.7500\n",
            "Epoch 62/500\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.46435\n",
            "60/60 - 0s - loss: 0.2876 - acc: 0.9167 - val_loss: 0.4766 - val_acc: 0.7500\n",
            "Epoch 63/500\n",
            "\n",
            "Epoch 00063: val_loss improved from 0.46435 to 0.45083, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3441 - acc: 0.8833 - val_loss: 0.4508 - val_acc: 0.7500\n",
            "Epoch 64/500\n",
            "\n",
            "Epoch 00064: val_loss improved from 0.45083 to 0.42313, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3098 - acc: 0.9000 - val_loss: 0.4231 - val_acc: 0.8000\n",
            "Epoch 65/500\n",
            "\n",
            "Epoch 00065: val_loss improved from 0.42313 to 0.40247, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3394 - acc: 0.8667 - val_loss: 0.4025 - val_acc: 0.8000\n",
            "Epoch 66/500\n",
            "\n",
            "Epoch 00066: val_loss improved from 0.40247 to 0.38584, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3042 - acc: 0.9667 - val_loss: 0.3858 - val_acc: 0.8500\n",
            "Epoch 67/500\n",
            "\n",
            "Epoch 00067: val_loss improved from 0.38584 to 0.36784, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2714 - acc: 0.9000 - val_loss: 0.3678 - val_acc: 0.8500\n",
            "Epoch 68/500\n",
            "\n",
            "Epoch 00068: val_loss improved from 0.36784 to 0.35541, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2751 - acc: 0.9333 - val_loss: 0.3554 - val_acc: 0.9000\n",
            "Epoch 69/500\n",
            "\n",
            "Epoch 00069: val_loss improved from 0.35541 to 0.34370, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3226 - acc: 0.8833 - val_loss: 0.3437 - val_acc: 0.9000\n",
            "Epoch 70/500\n",
            "\n",
            "Epoch 00070: val_loss improved from 0.34370 to 0.33892, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2714 - acc: 0.9333 - val_loss: 0.3389 - val_acc: 0.9000\n",
            "Epoch 71/500\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.33892\n",
            "60/60 - 0s - loss: 0.3242 - acc: 0.9167 - val_loss: 0.3443 - val_acc: 0.9000\n",
            "Epoch 72/500\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.33892\n",
            "60/60 - 0s - loss: 0.2871 - acc: 0.9500 - val_loss: 0.3476 - val_acc: 0.9000\n",
            "Epoch 73/500\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.33892\n",
            "60/60 - 0s - loss: 0.2408 - acc: 0.9667 - val_loss: 0.3444 - val_acc: 0.9000\n",
            "Epoch 74/500\n",
            "\n",
            "Epoch 00074: val_loss improved from 0.33892 to 0.33457, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2397 - acc: 0.9667 - val_loss: 0.3346 - val_acc: 0.9000\n",
            "Epoch 75/500\n",
            "\n",
            "Epoch 00075: val_loss improved from 0.33457 to 0.32788, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2656 - acc: 0.9667 - val_loss: 0.3279 - val_acc: 0.9000\n",
            "Epoch 76/500\n",
            "\n",
            "Epoch 00076: val_loss improved from 0.32788 to 0.32046, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2635 - acc: 0.9667 - val_loss: 0.3205 - val_acc: 0.9000\n",
            "Epoch 77/500\n",
            "\n",
            "Epoch 00077: val_loss improved from 0.32046 to 0.30880, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2449 - acc: 0.9667 - val_loss: 0.3088 - val_acc: 0.9500\n",
            "Epoch 78/500\n",
            "\n",
            "Epoch 00078: val_loss improved from 0.30880 to 0.30251, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3251 - acc: 0.8500 - val_loss: 0.3025 - val_acc: 0.9000\n",
            "Epoch 79/500\n",
            "\n",
            "Epoch 00079: val_loss improved from 0.30251 to 0.29953, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2608 - acc: 0.9500 - val_loss: 0.2995 - val_acc: 0.9000\n",
            "Epoch 80/500\n",
            "\n",
            "Epoch 00080: val_loss improved from 0.29953 to 0.29539, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2298 - acc: 0.9333 - val_loss: 0.2954 - val_acc: 0.9000\n",
            "Epoch 81/500\n",
            "\n",
            "Epoch 00081: val_loss improved from 0.29539 to 0.29356, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2702 - acc: 0.9000 - val_loss: 0.2936 - val_acc: 0.9500\n",
            "Epoch 82/500\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.29356\n",
            "60/60 - 0s - loss: 0.2681 - acc: 0.9167 - val_loss: 0.3004 - val_acc: 1.0000\n",
            "Epoch 83/500\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.29356\n",
            "60/60 - 0s - loss: 0.2751 - acc: 0.8500 - val_loss: 0.3033 - val_acc: 0.9500\n",
            "Epoch 84/500\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.29356\n",
            "60/60 - 0s - loss: 0.2653 - acc: 0.9167 - val_loss: 0.3070 - val_acc: 0.9500\n",
            "Epoch 85/500\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.29356\n",
            "60/60 - 0s - loss: 0.2703 - acc: 0.9167 - val_loss: 0.3218 - val_acc: 0.9500\n",
            "Epoch 86/500\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.29356\n",
            "60/60 - 0s - loss: 0.2689 - acc: 0.9167 - val_loss: 0.3276 - val_acc: 0.9500\n",
            "Epoch 87/500\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.29356\n",
            "60/60 - 0s - loss: 0.2726 - acc: 0.9333 - val_loss: 0.3349 - val_acc: 0.9500\n",
            "Epoch 88/500\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.29356\n",
            "60/60 - 0s - loss: 0.2419 - acc: 0.9833 - val_loss: 0.3364 - val_acc: 0.9000\n",
            "Epoch 89/500\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.29356\n",
            "60/60 - 0s - loss: 0.2347 - acc: 0.9667 - val_loss: 0.3386 - val_acc: 0.9000\n",
            "Epoch 90/500\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.29356\n",
            "60/60 - 0s - loss: 0.2517 - acc: 0.9167 - val_loss: 0.3348 - val_acc: 0.9000\n",
            "Epoch 91/500\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.29356\n",
            "60/60 - 0s - loss: 0.2646 - acc: 0.9333 - val_loss: 0.3215 - val_acc: 0.9500\n",
            "Epoch 92/500\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.29356\n",
            "60/60 - 0s - loss: 0.2535 - acc: 0.9500 - val_loss: 0.3110 - val_acc: 0.9500\n",
            "Epoch 93/500\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.29356\n",
            "60/60 - 0s - loss: 0.2613 - acc: 0.9500 - val_loss: 0.3177 - val_acc: 0.9500\n",
            "Epoch 94/500\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.29356\n",
            "60/60 - 0s - loss: 0.2463 - acc: 0.9500 - val_loss: 0.3203 - val_acc: 0.9500\n",
            "Epoch 95/500\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.29356\n",
            "60/60 - 0s - loss: 0.2522 - acc: 0.9667 - val_loss: 0.3130 - val_acc: 0.9500\n",
            "Epoch 96/500\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.29356\n",
            "60/60 - 0s - loss: 0.2408 - acc: 0.9833 - val_loss: 0.3067 - val_acc: 0.9500\n",
            "Epoch 97/500\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.29356\n",
            "60/60 - 0s - loss: 0.2541 - acc: 0.9333 - val_loss: 0.2972 - val_acc: 0.9500\n",
            "Epoch 98/500\n",
            "\n",
            "Epoch 00098: val_loss improved from 0.29356 to 0.29293, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3040 - acc: 0.9333 - val_loss: 0.2929 - val_acc: 0.9500\n",
            "Epoch 99/500\n",
            "\n",
            "Epoch 00099: val_loss improved from 0.29293 to 0.28965, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2680 - acc: 0.9167 - val_loss: 0.2897 - val_acc: 0.9500\n",
            "Epoch 100/500\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.28965\n",
            "60/60 - 0s - loss: 0.2816 - acc: 0.9500 - val_loss: 0.3020 - val_acc: 0.9500\n",
            "Epoch 101/500\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.28965\n",
            "60/60 - 0s - loss: 0.2090 - acc: 0.9833 - val_loss: 0.3142 - val_acc: 0.9000\n",
            "Epoch 102/500\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.28965\n",
            "60/60 - 0s - loss: 0.2177 - acc: 0.9500 - val_loss: 0.3284 - val_acc: 0.9000\n",
            "Epoch 103/500\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.28965\n",
            "60/60 - 0s - loss: 0.2805 - acc: 0.9167 - val_loss: 0.3303 - val_acc: 0.9000\n",
            "Epoch 104/500\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.28965\n",
            "60/60 - 0s - loss: 0.2382 - acc: 0.9167 - val_loss: 0.3322 - val_acc: 0.9000\n",
            "Epoch 105/500\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 0.28965\n",
            "60/60 - 0s - loss: 0.2433 - acc: 0.9500 - val_loss: 0.3489 - val_acc: 0.8500\n",
            "Epoch 106/500\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 0.28965\n",
            "60/60 - 0s - loss: 0.2344 - acc: 0.9500 - val_loss: 0.3615 - val_acc: 0.8000\n",
            "Epoch 107/500\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.28965\n",
            "60/60 - 0s - loss: 0.2404 - acc: 0.9500 - val_loss: 0.3850 - val_acc: 0.6500\n",
            "Epoch 108/500\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 0.28965\n",
            "60/60 - 0s - loss: 0.2347 - acc: 0.9833 - val_loss: 0.4078 - val_acc: 0.6500\n",
            "Epoch 109/500\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.28965\n",
            "60/60 - 0s - loss: 0.2400 - acc: 0.9833 - val_loss: 0.3923 - val_acc: 0.6500\n",
            "Epoch 110/500\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.28965\n",
            "60/60 - 0s - loss: 0.2550 - acc: 0.9333 - val_loss: 0.3720 - val_acc: 0.7000\n",
            "Epoch 111/500\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 0.28965\n",
            "60/60 - 0s - loss: 0.2107 - acc: 0.9667 - val_loss: 0.3491 - val_acc: 0.7500\n",
            "Epoch 112/500\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 0.28965\n",
            "60/60 - 0s - loss: 0.2627 - acc: 0.9333 - val_loss: 0.3287 - val_acc: 0.8000\n",
            "Epoch 113/500\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.28965\n",
            "60/60 - 0s - loss: 0.2381 - acc: 0.9333 - val_loss: 0.3102 - val_acc: 0.9000\n",
            "Epoch 114/500\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.28965\n",
            "60/60 - 0s - loss: 0.2458 - acc: 0.9333 - val_loss: 0.3140 - val_acc: 0.9000\n",
            "Epoch 115/500\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.28965\n",
            "60/60 - 0s - loss: 0.2554 - acc: 0.9000 - val_loss: 0.3237 - val_acc: 0.9000\n",
            "Epoch 116/500\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.28965\n",
            "60/60 - 0s - loss: 0.2117 - acc: 0.9333 - val_loss: 0.3225 - val_acc: 0.9000\n",
            "Epoch 117/500\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.28965\n",
            "60/60 - 0s - loss: 0.2137 - acc: 0.9500 - val_loss: 0.3180 - val_acc: 0.9000\n",
            "Epoch 118/500\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.28965\n",
            "60/60 - 0s - loss: 0.1963 - acc: 0.9833 - val_loss: 0.3114 - val_acc: 0.9500\n",
            "Epoch 119/500\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 0.28965\n",
            "60/60 - 0s - loss: 0.2006 - acc: 0.9500 - val_loss: 0.3150 - val_acc: 0.9000\n",
            "Epoch 120/500\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.28965\n",
            "60/60 - 0s - loss: 0.1843 - acc: 0.9833 - val_loss: 0.3148 - val_acc: 0.9000\n",
            "Epoch 121/500\n",
            "\n",
            "Epoch 00121: val_loss improved from 0.28965 to 0.28427, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2631 - acc: 0.9167 - val_loss: 0.2843 - val_acc: 0.9500\n",
            "Epoch 122/500\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.28427\n",
            "60/60 - 0s - loss: 0.2364 - acc: 0.9333 - val_loss: 0.2894 - val_acc: 0.9500\n",
            "Epoch 123/500\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 0.28427\n",
            "60/60 - 0s - loss: 0.2235 - acc: 0.9833 - val_loss: 0.3012 - val_acc: 0.9000\n",
            "Epoch 124/500\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 0.28427\n",
            "60/60 - 0s - loss: 0.2150 - acc: 0.9333 - val_loss: 0.3116 - val_acc: 0.9000\n",
            "Epoch 125/500\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 0.28427\n",
            "60/60 - 0s - loss: 0.1886 - acc: 1.0000 - val_loss: 0.3106 - val_acc: 0.8500\n",
            "Epoch 126/500\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 0.28427\n",
            "60/60 - 0s - loss: 0.1844 - acc: 0.9500 - val_loss: 0.2979 - val_acc: 0.9500\n",
            "Epoch 127/500\n",
            "\n",
            "Epoch 00127: val_loss improved from 0.28427 to 0.27060, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1949 - acc: 0.9667 - val_loss: 0.2706 - val_acc: 1.0000\n",
            "Epoch 128/500\n",
            "\n",
            "Epoch 00128: val_loss improved from 0.27060 to 0.26251, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2087 - acc: 0.9500 - val_loss: 0.2625 - val_acc: 1.0000\n",
            "Epoch 129/500\n",
            "\n",
            "Epoch 00129: val_loss improved from 0.26251 to 0.25672, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1991 - acc: 0.9667 - val_loss: 0.2567 - val_acc: 1.0000\n",
            "Epoch 130/500\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 0.25672\n",
            "60/60 - 0s - loss: 0.2048 - acc: 0.9500 - val_loss: 0.2592 - val_acc: 1.0000\n",
            "Epoch 131/500\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 0.25672\n",
            "60/60 - 0s - loss: 0.1987 - acc: 0.9667 - val_loss: 0.2603 - val_acc: 1.0000\n",
            "Epoch 132/500\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 0.25672\n",
            "60/60 - 0s - loss: 0.2133 - acc: 0.9333 - val_loss: 0.2569 - val_acc: 1.0000\n",
            "Epoch 133/500\n",
            "\n",
            "Epoch 00133: val_loss improved from 0.25672 to 0.24961, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2016 - acc: 0.9833 - val_loss: 0.2496 - val_acc: 1.0000\n",
            "Epoch 134/500\n",
            "\n",
            "Epoch 00134: val_loss improved from 0.24961 to 0.24194, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1896 - acc: 0.9667 - val_loss: 0.2419 - val_acc: 1.0000\n",
            "Epoch 135/500\n",
            "\n",
            "Epoch 00135: val_loss improved from 0.24194 to 0.23840, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1810 - acc: 0.9333 - val_loss: 0.2384 - val_acc: 1.0000\n",
            "Epoch 136/500\n",
            "\n",
            "Epoch 00136: val_loss improved from 0.23840 to 0.23529, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1568 - acc: 0.9833 - val_loss: 0.2353 - val_acc: 1.0000\n",
            "Epoch 137/500\n",
            "\n",
            "Epoch 00137: val_loss improved from 0.23529 to 0.23079, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2150 - acc: 0.9333 - val_loss: 0.2308 - val_acc: 1.0000\n",
            "Epoch 138/500\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.23079\n",
            "60/60 - 0s - loss: 0.2393 - acc: 0.9167 - val_loss: 0.2313 - val_acc: 1.0000\n",
            "Epoch 139/500\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.23079\n",
            "60/60 - 0s - loss: 0.1872 - acc: 0.9500 - val_loss: 0.2405 - val_acc: 1.0000\n",
            "Epoch 140/500\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 0.23079\n",
            "60/60 - 0s - loss: 0.1975 - acc: 0.9500 - val_loss: 0.2403 - val_acc: 1.0000\n",
            "Epoch 141/500\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.23079\n",
            "60/60 - 0s - loss: 0.1987 - acc: 0.9333 - val_loss: 0.2374 - val_acc: 1.0000\n",
            "Epoch 142/500\n",
            "\n",
            "Epoch 00142: val_loss improved from 0.23079 to 0.22934, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1708 - acc: 0.9500 - val_loss: 0.2293 - val_acc: 1.0000\n",
            "Epoch 143/500\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 0.22934\n",
            "60/60 - 0s - loss: 0.2667 - acc: 0.8833 - val_loss: 0.2332 - val_acc: 1.0000\n",
            "Epoch 144/500\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 0.22934\n",
            "60/60 - 0s - loss: 0.1744 - acc: 0.9500 - val_loss: 0.2524 - val_acc: 0.9500\n",
            "Epoch 145/500\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.22934\n",
            "60/60 - 0s - loss: 0.1809 - acc: 0.9833 - val_loss: 0.2648 - val_acc: 0.9500\n",
            "Epoch 146/500\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.22934\n",
            "60/60 - 0s - loss: 0.2065 - acc: 0.9667 - val_loss: 0.2619 - val_acc: 0.9500\n",
            "Epoch 147/500\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.22934\n",
            "60/60 - 0s - loss: 0.2014 - acc: 0.9833 - val_loss: 0.2528 - val_acc: 0.9500\n",
            "Epoch 148/500\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 0.22934\n",
            "60/60 - 0s - loss: 0.1768 - acc: 0.9833 - val_loss: 0.2370 - val_acc: 1.0000\n",
            "Epoch 149/500\n",
            "\n",
            "Epoch 00149: val_loss improved from 0.22934 to 0.22704, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1969 - acc: 0.9500 - val_loss: 0.2270 - val_acc: 1.0000\n",
            "Epoch 150/500\n",
            "\n",
            "Epoch 00150: val_loss improved from 0.22704 to 0.21985, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1556 - acc: 0.9833 - val_loss: 0.2198 - val_acc: 1.0000\n",
            "Epoch 151/500\n",
            "\n",
            "Epoch 00151: val_loss improved from 0.21985 to 0.21478, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1625 - acc: 1.0000 - val_loss: 0.2148 - val_acc: 1.0000\n",
            "Epoch 152/500\n",
            "\n",
            "Epoch 00152: val_loss improved from 0.21478 to 0.21342, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1622 - acc: 0.9833 - val_loss: 0.2134 - val_acc: 1.0000\n",
            "Epoch 153/500\n",
            "\n",
            "Epoch 00153: val_loss improved from 0.21342 to 0.20795, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1672 - acc: 0.9833 - val_loss: 0.2080 - val_acc: 1.0000\n",
            "Epoch 154/500\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.20795\n",
            "60/60 - 0s - loss: 0.1669 - acc: 0.9333 - val_loss: 0.2091 - val_acc: 1.0000\n",
            "Epoch 155/500\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.20795\n",
            "60/60 - 0s - loss: 0.2532 - acc: 0.9167 - val_loss: 0.2233 - val_acc: 0.9500\n",
            "Epoch 156/500\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.20795\n",
            "60/60 - 0s - loss: 0.1197 - acc: 0.9667 - val_loss: 0.2298 - val_acc: 0.9500\n",
            "Epoch 157/500\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.20795\n",
            "60/60 - 0s - loss: 0.1402 - acc: 0.9833 - val_loss: 0.2270 - val_acc: 0.9500\n",
            "Epoch 158/500\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.20795\n",
            "60/60 - 0s - loss: 0.1662 - acc: 1.0000 - val_loss: 0.2259 - val_acc: 0.9500\n",
            "Epoch 159/500\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.20795\n",
            "60/60 - 0s - loss: 0.1455 - acc: 0.9833 - val_loss: 0.2235 - val_acc: 1.0000\n",
            "Epoch 160/500\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.20795\n",
            "60/60 - 0s - loss: 0.1792 - acc: 0.9500 - val_loss: 0.2234 - val_acc: 1.0000\n",
            "Epoch 161/500\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.20795\n",
            "60/60 - 0s - loss: 0.1362 - acc: 0.9833 - val_loss: 0.2252 - val_acc: 1.0000\n",
            "Epoch 162/500\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.20795\n",
            "60/60 - 0s - loss: 0.1558 - acc: 0.9500 - val_loss: 0.2236 - val_acc: 1.0000\n",
            "Epoch 163/500\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.20795\n",
            "60/60 - 0s - loss: 0.2080 - acc: 0.9500 - val_loss: 0.2164 - val_acc: 1.0000\n",
            "Epoch 164/500\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.20795\n",
            "60/60 - 0s - loss: 0.2469 - acc: 0.9333 - val_loss: 0.2215 - val_acc: 1.0000\n",
            "Epoch 165/500\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.20795\n",
            "60/60 - 0s - loss: 0.1782 - acc: 0.9500 - val_loss: 0.2323 - val_acc: 1.0000\n",
            "Epoch 166/500\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 0.20795\n",
            "60/60 - 0s - loss: 0.1590 - acc: 0.9833 - val_loss: 0.2397 - val_acc: 1.0000\n",
            "Epoch 167/500\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.20795\n",
            "60/60 - 0s - loss: 0.1717 - acc: 0.9833 - val_loss: 0.2381 - val_acc: 0.9500\n",
            "Epoch 168/500\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.20795\n",
            "60/60 - 0s - loss: 0.1698 - acc: 0.9833 - val_loss: 0.2279 - val_acc: 0.9500\n",
            "Epoch 169/500\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 0.20795\n",
            "60/60 - 0s - loss: 0.1931 - acc: 0.9667 - val_loss: 0.2183 - val_acc: 0.9500\n",
            "Epoch 170/500\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.20795\n",
            "60/60 - 0s - loss: 0.1757 - acc: 0.9667 - val_loss: 0.2165 - val_acc: 1.0000\n",
            "Epoch 171/500\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.20795\n",
            "60/60 - 0s - loss: 0.1820 - acc: 0.9833 - val_loss: 0.2162 - val_acc: 1.0000\n",
            "Epoch 172/500\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.20795\n",
            "60/60 - 0s - loss: 0.1542 - acc: 0.9833 - val_loss: 0.2226 - val_acc: 1.0000\n",
            "Epoch 173/500\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.20795\n",
            "60/60 - 0s - loss: 0.2134 - acc: 0.9667 - val_loss: 0.2272 - val_acc: 1.0000\n",
            "Epoch 174/500\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.20795\n",
            "60/60 - 0s - loss: 0.1656 - acc: 0.9667 - val_loss: 0.2416 - val_acc: 0.9500\n",
            "Epoch 175/500\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.20795\n",
            "60/60 - 0s - loss: 0.2217 - acc: 0.9500 - val_loss: 0.2636 - val_acc: 0.8500\n",
            "Epoch 176/500\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 0.20795\n",
            "60/60 - 0s - loss: 0.2093 - acc: 0.9333 - val_loss: 0.2857 - val_acc: 0.8000\n",
            "Epoch 177/500\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.20795\n",
            "60/60 - 0s - loss: 0.2434 - acc: 0.9000 - val_loss: 0.2842 - val_acc: 0.8000\n",
            "Epoch 178/500\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.20795\n",
            "60/60 - 0s - loss: 0.1772 - acc: 0.9500 - val_loss: 0.2688 - val_acc: 0.9500\n",
            "Epoch 179/500\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.20795\n",
            "60/60 - 0s - loss: 0.1669 - acc: 0.9833 - val_loss: 0.2602 - val_acc: 1.0000\n",
            "Epoch 180/500\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.20795\n",
            "60/60 - 0s - loss: 0.2294 - acc: 0.9333 - val_loss: 0.2363 - val_acc: 1.0000\n",
            "Epoch 181/500\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.20795\n",
            "60/60 - 0s - loss: 0.2505 - acc: 0.9333 - val_loss: 0.2287 - val_acc: 1.0000\n",
            "Epoch 182/500\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.20795\n",
            "60/60 - 0s - loss: 0.2266 - acc: 0.9000 - val_loss: 0.2258 - val_acc: 1.0000\n",
            "Epoch 183/500\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.20795\n",
            "60/60 - 0s - loss: 0.1529 - acc: 0.9667 - val_loss: 0.2155 - val_acc: 1.0000\n",
            "Epoch 184/500\n",
            "\n",
            "Epoch 00184: val_loss improved from 0.20795 to 0.20759, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1754 - acc: 0.9667 - val_loss: 0.2076 - val_acc: 1.0000\n",
            "Epoch 185/500\n",
            "\n",
            "Epoch 00185: val_loss improved from 0.20759 to 0.20538, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1964 - acc: 0.9333 - val_loss: 0.2054 - val_acc: 1.0000\n",
            "Epoch 186/500\n",
            "\n",
            "Epoch 00186: val_loss improved from 0.20538 to 0.20124, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1693 - acc: 0.9833 - val_loss: 0.2012 - val_acc: 1.0000\n",
            "Epoch 187/500\n",
            "\n",
            "Epoch 00187: val_loss improved from 0.20124 to 0.20028, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1882 - acc: 0.9833 - val_loss: 0.2003 - val_acc: 1.0000\n",
            "Epoch 188/500\n",
            "\n",
            "Epoch 00188: val_loss improved from 0.20028 to 0.19763, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1690 - acc: 0.9500 - val_loss: 0.1976 - val_acc: 1.0000\n",
            "Epoch 189/500\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.19763\n",
            "60/60 - 0s - loss: 0.1748 - acc: 0.9667 - val_loss: 0.1992 - val_acc: 0.9500\n",
            "Epoch 190/500\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.19763\n",
            "60/60 - 0s - loss: 0.1619 - acc: 0.9500 - val_loss: 0.2049 - val_acc: 0.9500\n",
            "Epoch 191/500\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.19763\n",
            "60/60 - 0s - loss: 0.1682 - acc: 0.9333 - val_loss: 0.2037 - val_acc: 0.9500\n",
            "Epoch 192/500\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 0.19763\n",
            "60/60 - 0s - loss: 0.1612 - acc: 0.9667 - val_loss: 0.2107 - val_acc: 0.9500\n",
            "Epoch 193/500\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.19763\n",
            "60/60 - 0s - loss: 0.2515 - acc: 0.8667 - val_loss: 0.2371 - val_acc: 0.9500\n",
            "Epoch 194/500\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.19763\n",
            "60/60 - 0s - loss: 0.1477 - acc: 0.9833 - val_loss: 0.2380 - val_acc: 0.9500\n",
            "Epoch 195/500\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.19763\n",
            "60/60 - 0s - loss: 0.1520 - acc: 0.9833 - val_loss: 0.2337 - val_acc: 0.9500\n",
            "Epoch 196/500\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.19763\n",
            "60/60 - 0s - loss: 0.1759 - acc: 0.9667 - val_loss: 0.2375 - val_acc: 0.9500\n",
            "Epoch 197/500\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.19763\n",
            "60/60 - 0s - loss: 0.1916 - acc: 0.9500 - val_loss: 0.2555 - val_acc: 0.9000\n",
            "Epoch 198/500\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.19763\n",
            "60/60 - 0s - loss: 0.2156 - acc: 0.9333 - val_loss: 0.2308 - val_acc: 0.9500\n",
            "Epoch 199/500\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.19763\n",
            "60/60 - 0s - loss: 0.1160 - acc: 0.9833 - val_loss: 0.2163 - val_acc: 1.0000\n",
            "Epoch 200/500\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.19763\n",
            "60/60 - 0s - loss: 0.1399 - acc: 0.9667 - val_loss: 0.2075 - val_acc: 1.0000\n",
            "Epoch 201/500\n",
            "\n",
            "Epoch 00201: val_loss did not improve from 0.19763\n",
            "60/60 - 0s - loss: 0.1629 - acc: 0.9667 - val_loss: 0.1996 - val_acc: 1.0000\n",
            "Epoch 202/500\n",
            "\n",
            "Epoch 00202: val_loss did not improve from 0.19763\n",
            "60/60 - 0s - loss: 0.1406 - acc: 1.0000 - val_loss: 0.1988 - val_acc: 1.0000\n",
            "Epoch 203/500\n",
            "\n",
            "Epoch 00203: val_loss did not improve from 0.19763\n",
            "60/60 - 0s - loss: 0.1267 - acc: 0.9833 - val_loss: 0.2026 - val_acc: 1.0000\n",
            "Epoch 204/500\n",
            "\n",
            "Epoch 00204: val_loss did not improve from 0.19763\n",
            "60/60 - 0s - loss: 0.1532 - acc: 0.9667 - val_loss: 0.2044 - val_acc: 1.0000\n",
            "Epoch 205/500\n",
            "\n",
            "Epoch 00205: val_loss did not improve from 0.19763\n",
            "60/60 - 0s - loss: 0.2076 - acc: 0.9000 - val_loss: 0.2214 - val_acc: 1.0000\n",
            "Epoch 206/500\n",
            "\n",
            "Epoch 00206: val_loss did not improve from 0.19763\n",
            "60/60 - 0s - loss: 0.1404 - acc: 1.0000 - val_loss: 0.2355 - val_acc: 1.0000\n",
            "Epoch 207/500\n",
            "\n",
            "Epoch 00207: val_loss did not improve from 0.19763\n",
            "60/60 - 0s - loss: 0.1666 - acc: 0.9833 - val_loss: 0.2456 - val_acc: 1.0000\n",
            "Epoch 208/500\n",
            "\n",
            "Epoch 00208: val_loss did not improve from 0.19763\n",
            "60/60 - 0s - loss: 0.1754 - acc: 0.9833 - val_loss: 0.2471 - val_acc: 1.0000\n",
            "Epoch 209/500\n",
            "\n",
            "Epoch 00209: val_loss did not improve from 0.19763\n",
            "60/60 - 0s - loss: 0.1772 - acc: 0.9667 - val_loss: 0.2493 - val_acc: 1.0000\n",
            "Epoch 210/500\n",
            "\n",
            "Epoch 00210: val_loss did not improve from 0.19763\n",
            "60/60 - 0s - loss: 0.1061 - acc: 0.9833 - val_loss: 0.2444 - val_acc: 1.0000\n",
            "Epoch 211/500\n",
            "\n",
            "Epoch 00211: val_loss did not improve from 0.19763\n",
            "60/60 - 0s - loss: 0.1782 - acc: 0.9167 - val_loss: 0.2279 - val_acc: 1.0000\n",
            "Epoch 212/500\n",
            "\n",
            "Epoch 00212: val_loss did not improve from 0.19763\n",
            "60/60 - 0s - loss: 0.1568 - acc: 0.9667 - val_loss: 0.2182 - val_acc: 1.0000\n",
            "Epoch 213/500\n",
            "\n",
            "Epoch 00213: val_loss did not improve from 0.19763\n",
            "60/60 - 0s - loss: 0.1582 - acc: 0.9667 - val_loss: 0.2064 - val_acc: 1.0000\n",
            "Epoch 214/500\n",
            "\n",
            "Epoch 00214: val_loss did not improve from 0.19763\n",
            "60/60 - 0s - loss: 0.1842 - acc: 0.9000 - val_loss: 0.1999 - val_acc: 1.0000\n",
            "Epoch 215/500\n",
            "\n",
            "Epoch 00215: val_loss did not improve from 0.19763\n",
            "60/60 - 0s - loss: 0.1731 - acc: 0.9667 - val_loss: 0.1991 - val_acc: 1.0000\n",
            "Epoch 216/500\n",
            "\n",
            "Epoch 00216: val_loss did not improve from 0.19763\n",
            "60/60 - 0s - loss: 0.1764 - acc: 0.9500 - val_loss: 0.1983 - val_acc: 1.0000\n",
            "Epoch 217/500\n",
            "\n",
            "Epoch 00217: val_loss did not improve from 0.19763\n",
            "60/60 - 0s - loss: 0.1504 - acc: 0.9667 - val_loss: 0.2034 - val_acc: 1.0000\n",
            "Epoch 218/500\n",
            "\n",
            "Epoch 00218: val_loss did not improve from 0.19763\n",
            "60/60 - 0s - loss: 0.1288 - acc: 1.0000 - val_loss: 0.2228 - val_acc: 1.0000\n",
            "Epoch 219/500\n",
            "\n",
            "Epoch 00219: val_loss did not improve from 0.19763\n",
            "60/60 - 0s - loss: 0.2247 - acc: 0.9333 - val_loss: 0.2490 - val_acc: 0.8500\n",
            "Epoch 220/500\n",
            "\n",
            "Epoch 00220: val_loss did not improve from 0.19763\n",
            "60/60 - 0s - loss: 0.1541 - acc: 0.9667 - val_loss: 0.2762 - val_acc: 0.8500\n",
            "Epoch 221/500\n",
            "\n",
            "Epoch 00221: val_loss did not improve from 0.19763\n",
            "60/60 - 0s - loss: 0.1419 - acc: 0.9833 - val_loss: 0.2827 - val_acc: 0.8500\n",
            "Epoch 222/500\n",
            "\n",
            "Epoch 00222: val_loss did not improve from 0.19763\n",
            "60/60 - 0s - loss: 0.1275 - acc: 0.9833 - val_loss: 0.2687 - val_acc: 0.8500\n",
            "Epoch 223/500\n",
            "\n",
            "Epoch 00223: val_loss did not improve from 0.19763\n",
            "60/60 - 0s - loss: 0.1308 - acc: 0.9667 - val_loss: 0.2554 - val_acc: 0.8500\n",
            "Epoch 224/500\n",
            "\n",
            "Epoch 00224: val_loss did not improve from 0.19763\n",
            "60/60 - 0s - loss: 0.1544 - acc: 0.9667 - val_loss: 0.2623 - val_acc: 0.8500\n",
            "Epoch 225/500\n",
            "\n",
            "Epoch 00225: val_loss did not improve from 0.19763\n",
            "60/60 - 0s - loss: 0.1576 - acc: 0.9667 - val_loss: 0.2433 - val_acc: 0.8500\n",
            "Epoch 226/500\n",
            "\n",
            "Epoch 00226: val_loss did not improve from 0.19763\n",
            "60/60 - 0s - loss: 0.1494 - acc: 0.9833 - val_loss: 0.2335 - val_acc: 0.9000\n",
            "Epoch 227/500\n",
            "\n",
            "Epoch 00227: val_loss did not improve from 0.19763\n",
            "60/60 - 0s - loss: 0.1158 - acc: 0.9833 - val_loss: 0.2329 - val_acc: 0.9000\n",
            "Epoch 228/500\n",
            "\n",
            "Epoch 00228: val_loss did not improve from 0.19763\n",
            "60/60 - 0s - loss: 0.1489 - acc: 0.9833 - val_loss: 0.2318 - val_acc: 0.9500\n",
            "Epoch 229/500\n",
            "\n",
            "Epoch 00229: val_loss did not improve from 0.19763\n",
            "60/60 - 0s - loss: 0.1500 - acc: 0.9833 - val_loss: 0.2329 - val_acc: 0.9500\n",
            "Epoch 230/500\n",
            "\n",
            "Epoch 00230: val_loss did not improve from 0.19763\n",
            "60/60 - 0s - loss: 0.1864 - acc: 0.9333 - val_loss: 0.2259 - val_acc: 0.9500\n",
            "Epoch 231/500\n",
            "\n",
            "Epoch 00231: val_loss did not improve from 0.19763\n",
            "60/60 - 0s - loss: 0.1811 - acc: 0.9667 - val_loss: 0.2149 - val_acc: 0.9500\n",
            "Epoch 232/500\n",
            "\n",
            "Epoch 00232: val_loss did not improve from 0.19763\n",
            "60/60 - 0s - loss: 0.1682 - acc: 0.9833 - val_loss: 0.2206 - val_acc: 0.9500\n",
            "Epoch 233/500\n",
            "\n",
            "Epoch 00233: val_loss did not improve from 0.19763\n",
            "60/60 - 0s - loss: 0.1713 - acc: 0.9500 - val_loss: 0.2231 - val_acc: 0.9500\n",
            "Epoch 234/500\n",
            "\n",
            "Epoch 00234: val_loss did not improve from 0.19763\n",
            "60/60 - 0s - loss: 0.1795 - acc: 0.9333 - val_loss: 0.2256 - val_acc: 0.9500\n",
            "Epoch 235/500\n",
            "\n",
            "Epoch 00235: val_loss did not improve from 0.19763\n",
            "60/60 - 0s - loss: 0.1621 - acc: 0.9667 - val_loss: 0.2219 - val_acc: 0.9500\n",
            "Epoch 236/500\n",
            "\n",
            "Epoch 00236: val_loss did not improve from 0.19763\n",
            "60/60 - 0s - loss: 0.1681 - acc: 0.9833 - val_loss: 0.2183 - val_acc: 0.9500\n",
            "Epoch 237/500\n",
            "\n",
            "Epoch 00237: val_loss did not improve from 0.19763\n",
            "60/60 - 0s - loss: 0.1040 - acc: 0.9833 - val_loss: 0.2094 - val_acc: 1.0000\n",
            "Epoch 238/500\n",
            "\n",
            "Epoch 00238: val_loss did not improve from 0.19763\n",
            "60/60 - 0s - loss: 0.1473 - acc: 0.9833 - val_loss: 0.2022 - val_acc: 1.0000\n",
            "Epoch 239/500\n",
            "\n",
            "Epoch 00239: val_loss improved from 0.19763 to 0.19533, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1747 - acc: 0.9667 - val_loss: 0.1953 - val_acc: 1.0000\n",
            "Epoch 240/500\n",
            "\n",
            "Epoch 00240: val_loss did not improve from 0.19533\n",
            "60/60 - 0s - loss: 0.1769 - acc: 0.9833 - val_loss: 0.1998 - val_acc: 1.0000\n",
            "Epoch 241/500\n",
            "\n",
            "Epoch 00241: val_loss did not improve from 0.19533\n",
            "60/60 - 0s - loss: 0.1908 - acc: 0.9333 - val_loss: 0.2166 - val_acc: 0.9500\n",
            "Epoch 242/500\n",
            "\n",
            "Epoch 00242: val_loss did not improve from 0.19533\n",
            "60/60 - 0s - loss: 0.1282 - acc: 1.0000 - val_loss: 0.2516 - val_acc: 0.9500\n",
            "Epoch 243/500\n",
            "\n",
            "Epoch 00243: val_loss did not improve from 0.19533\n",
            "60/60 - 0s - loss: 0.1458 - acc: 1.0000 - val_loss: 0.2929 - val_acc: 0.8500\n",
            "Epoch 244/500\n",
            "\n",
            "Epoch 00244: val_loss did not improve from 0.19533\n",
            "60/60 - 0s - loss: 0.2163 - acc: 0.9500 - val_loss: 0.3119 - val_acc: 0.8000\n",
            "Epoch 245/500\n",
            "\n",
            "Epoch 00245: val_loss did not improve from 0.19533\n",
            "60/60 - 0s - loss: 0.2175 - acc: 0.9667 - val_loss: 0.2593 - val_acc: 0.8500\n",
            "Epoch 246/500\n",
            "\n",
            "Epoch 00246: val_loss did not improve from 0.19533\n",
            "60/60 - 0s - loss: 0.1738 - acc: 0.9500 - val_loss: 0.2364 - val_acc: 0.9500\n",
            "Epoch 247/500\n",
            "\n",
            "Epoch 00247: val_loss did not improve from 0.19533\n",
            "60/60 - 0s - loss: 0.1392 - acc: 0.9833 - val_loss: 0.2198 - val_acc: 1.0000\n",
            "Epoch 248/500\n",
            "\n",
            "Epoch 00248: val_loss did not improve from 0.19533\n",
            "60/60 - 0s - loss: 0.1356 - acc: 1.0000 - val_loss: 0.2120 - val_acc: 1.0000\n",
            "Epoch 249/500\n",
            "\n",
            "Epoch 00249: val_loss did not improve from 0.19533\n",
            "60/60 - 0s - loss: 0.1684 - acc: 0.9667 - val_loss: 0.2072 - val_acc: 0.9500\n",
            "Epoch 250/500\n",
            "\n",
            "Epoch 00250: val_loss did not improve from 0.19533\n",
            "60/60 - 0s - loss: 0.2072 - acc: 0.9167 - val_loss: 0.2070 - val_acc: 1.0000\n",
            "Epoch 251/500\n",
            "\n",
            "Epoch 00251: val_loss did not improve from 0.19533\n",
            "60/60 - 0s - loss: 0.1674 - acc: 0.9667 - val_loss: 0.2094 - val_acc: 1.0000\n",
            "Epoch 252/500\n",
            "\n",
            "Epoch 00252: val_loss did not improve from 0.19533\n",
            "60/60 - 0s - loss: 0.1347 - acc: 0.9833 - val_loss: 0.2259 - val_acc: 1.0000\n",
            "Epoch 253/500\n",
            "\n",
            "Epoch 00253: val_loss did not improve from 0.19533\n",
            "60/60 - 0s - loss: 0.1388 - acc: 0.9667 - val_loss: 0.2425 - val_acc: 0.9500\n",
            "Epoch 254/500\n",
            "\n",
            "Epoch 00254: val_loss did not improve from 0.19533\n",
            "60/60 - 0s - loss: 0.1253 - acc: 0.9833 - val_loss: 0.2459 - val_acc: 0.9500\n",
            "Epoch 255/500\n",
            "\n",
            "Epoch 00255: val_loss did not improve from 0.19533\n",
            "60/60 - 0s - loss: 0.1576 - acc: 0.9833 - val_loss: 0.2320 - val_acc: 0.9500\n",
            "Epoch 256/500\n",
            "\n",
            "Epoch 00256: val_loss did not improve from 0.19533\n",
            "60/60 - 0s - loss: 0.1333 - acc: 1.0000 - val_loss: 0.2145 - val_acc: 0.9500\n",
            "Epoch 257/500\n",
            "\n",
            "Epoch 00257: val_loss did not improve from 0.19533\n",
            "60/60 - 0s - loss: 0.1201 - acc: 1.0000 - val_loss: 0.2117 - val_acc: 0.9500\n",
            "Epoch 258/500\n",
            "\n",
            "Epoch 00258: val_loss did not improve from 0.19533\n",
            "60/60 - 0s - loss: 0.1510 - acc: 1.0000 - val_loss: 0.2072 - val_acc: 1.0000\n",
            "Epoch 259/500\n",
            "\n",
            "Epoch 00259: val_loss did not improve from 0.19533\n",
            "60/60 - 0s - loss: 0.1557 - acc: 0.9833 - val_loss: 0.2124 - val_acc: 1.0000\n",
            "Epoch 260/500\n",
            "\n",
            "Epoch 00260: val_loss did not improve from 0.19533\n",
            "60/60 - 0s - loss: 0.1809 - acc: 0.9333 - val_loss: 0.2098 - val_acc: 1.0000\n",
            "Epoch 261/500\n",
            "\n",
            "Epoch 00261: val_loss did not improve from 0.19533\n",
            "60/60 - 0s - loss: 0.2073 - acc: 0.9167 - val_loss: 0.2115 - val_acc: 0.9500\n",
            "Epoch 262/500\n",
            "\n",
            "Epoch 00262: val_loss did not improve from 0.19533\n",
            "60/60 - 0s - loss: 0.1323 - acc: 0.9667 - val_loss: 0.2332 - val_acc: 0.9000\n",
            "Epoch 263/500\n",
            "\n",
            "Epoch 00263: val_loss did not improve from 0.19533\n",
            "60/60 - 0s - loss: 0.1189 - acc: 1.0000 - val_loss: 0.2515 - val_acc: 0.9000\n",
            "Epoch 264/500\n",
            "\n",
            "Epoch 00264: val_loss did not improve from 0.19533\n",
            "60/60 - 0s - loss: 0.2395 - acc: 0.9000 - val_loss: 0.2536 - val_acc: 0.9000\n",
            "Epoch 265/500\n",
            "\n",
            "Epoch 00265: val_loss did not improve from 0.19533\n",
            "60/60 - 0s - loss: 0.1067 - acc: 1.0000 - val_loss: 0.2400 - val_acc: 0.9500\n",
            "Epoch 266/500\n",
            "\n",
            "Epoch 00266: val_loss did not improve from 0.19533\n",
            "60/60 - 0s - loss: 0.1386 - acc: 0.9833 - val_loss: 0.2302 - val_acc: 0.9500\n",
            "Epoch 267/500\n",
            "\n",
            "Epoch 00267: val_loss did not improve from 0.19533\n",
            "60/60 - 0s - loss: 0.1070 - acc: 1.0000 - val_loss: 0.2192 - val_acc: 1.0000\n",
            "Epoch 268/500\n",
            "\n",
            "Epoch 00268: val_loss did not improve from 0.19533\n",
            "60/60 - 0s - loss: 0.1321 - acc: 0.9667 - val_loss: 0.2117 - val_acc: 1.0000\n",
            "Epoch 269/500\n",
            "\n",
            "Epoch 00269: val_loss did not improve from 0.19533\n",
            "60/60 - 0s - loss: 0.1794 - acc: 0.9667 - val_loss: 0.2131 - val_acc: 1.0000\n",
            "Epoch 270/500\n",
            "\n",
            "Epoch 00270: val_loss did not improve from 0.19533\n",
            "60/60 - 0s - loss: 0.1017 - acc: 1.0000 - val_loss: 0.2121 - val_acc: 1.0000\n",
            "Epoch 271/500\n",
            "\n",
            "Epoch 00271: val_loss did not improve from 0.19533\n",
            "60/60 - 0s - loss: 0.0795 - acc: 1.0000 - val_loss: 0.2045 - val_acc: 1.0000\n",
            "Epoch 272/500\n",
            "\n",
            "Epoch 00272: val_loss did not improve from 0.19533\n",
            "60/60 - 0s - loss: 0.1448 - acc: 0.9500 - val_loss: 0.2043 - val_acc: 1.0000\n",
            "Epoch 273/500\n",
            "\n",
            "Epoch 00273: val_loss did not improve from 0.19533\n",
            "60/60 - 0s - loss: 0.1101 - acc: 1.0000 - val_loss: 0.2017 - val_acc: 1.0000\n",
            "Epoch 274/500\n",
            "\n",
            "Epoch 00274: val_loss improved from 0.19533 to 0.19499, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1026 - acc: 1.0000 - val_loss: 0.1950 - val_acc: 1.0000\n",
            "Epoch 275/500\n",
            "\n",
            "Epoch 00275: val_loss improved from 0.19499 to 0.18569, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1251 - acc: 1.0000 - val_loss: 0.1857 - val_acc: 1.0000\n",
            "Epoch 276/500\n",
            "\n",
            "Epoch 00276: val_loss improved from 0.18569 to 0.18133, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.0977 - acc: 0.9833 - val_loss: 0.1813 - val_acc: 1.0000\n",
            "Epoch 277/500\n",
            "\n",
            "Epoch 00277: val_loss did not improve from 0.18133\n",
            "60/60 - 0s - loss: 0.1368 - acc: 1.0000 - val_loss: 0.1831 - val_acc: 0.9500\n",
            "Epoch 278/500\n",
            "\n",
            "Epoch 00278: val_loss did not improve from 0.18133\n",
            "60/60 - 0s - loss: 0.1558 - acc: 0.9667 - val_loss: 0.2032 - val_acc: 0.9500\n",
            "Epoch 279/500\n",
            "\n",
            "Epoch 00279: val_loss did not improve from 0.18133\n",
            "60/60 - 0s - loss: 0.0962 - acc: 0.9833 - val_loss: 0.2204 - val_acc: 0.9500\n",
            "Epoch 280/500\n",
            "\n",
            "Epoch 00280: val_loss did not improve from 0.18133\n",
            "60/60 - 0s - loss: 0.0987 - acc: 0.9833 - val_loss: 0.2277 - val_acc: 0.9500\n",
            "Epoch 281/500\n",
            "\n",
            "Epoch 00281: val_loss did not improve from 0.18133\n",
            "60/60 - 0s - loss: 0.1171 - acc: 0.9667 - val_loss: 0.2206 - val_acc: 0.9500\n",
            "Epoch 282/500\n",
            "\n",
            "Epoch 00282: val_loss did not improve from 0.18133\n",
            "60/60 - 0s - loss: 0.1096 - acc: 0.9833 - val_loss: 0.2139 - val_acc: 1.0000\n",
            "Epoch 283/500\n",
            "\n",
            "Epoch 00283: val_loss did not improve from 0.18133\n",
            "60/60 - 0s - loss: 0.1362 - acc: 0.9833 - val_loss: 0.2184 - val_acc: 0.9000\n",
            "Epoch 284/500\n",
            "\n",
            "Epoch 00284: val_loss did not improve from 0.18133\n",
            "60/60 - 0s - loss: 0.1443 - acc: 0.9667 - val_loss: 0.2204 - val_acc: 0.9000\n",
            "Epoch 285/500\n",
            "\n",
            "Epoch 00285: val_loss did not improve from 0.18133\n",
            "60/60 - 0s - loss: 0.1209 - acc: 0.9833 - val_loss: 0.2182 - val_acc: 0.9000\n",
            "Epoch 286/500\n",
            "\n",
            "Epoch 00286: val_loss did not improve from 0.18133\n",
            "60/60 - 0s - loss: 0.0912 - acc: 0.9833 - val_loss: 0.2114 - val_acc: 0.9000\n",
            "Epoch 287/500\n",
            "\n",
            "Epoch 00287: val_loss did not improve from 0.18133\n",
            "60/60 - 0s - loss: 0.1303 - acc: 1.0000 - val_loss: 0.2018 - val_acc: 0.9500\n",
            "Epoch 288/500\n",
            "\n",
            "Epoch 00288: val_loss did not improve from 0.18133\n",
            "60/60 - 0s - loss: 0.1767 - acc: 0.9333 - val_loss: 0.1912 - val_acc: 1.0000\n",
            "Epoch 289/500\n",
            "\n",
            "Epoch 00289: val_loss did not improve from 0.18133\n",
            "60/60 - 0s - loss: 0.1462 - acc: 0.9667 - val_loss: 0.1964 - val_acc: 0.9500\n",
            "Epoch 290/500\n",
            "\n",
            "Epoch 00290: val_loss did not improve from 0.18133\n",
            "60/60 - 0s - loss: 0.2047 - acc: 0.9000 - val_loss: 0.2251 - val_acc: 0.9000\n",
            "Epoch 291/500\n",
            "\n",
            "Epoch 00291: val_loss did not improve from 0.18133\n",
            "60/60 - 0s - loss: 0.1123 - acc: 0.9667 - val_loss: 0.2797 - val_acc: 0.8500\n",
            "Epoch 292/500\n",
            "\n",
            "Epoch 00292: val_loss did not improve from 0.18133\n",
            "60/60 - 0s - loss: 0.1266 - acc: 0.9833 - val_loss: 0.3401 - val_acc: 0.8500\n",
            "Epoch 293/500\n",
            "\n",
            "Epoch 00293: val_loss did not improve from 0.18133\n",
            "60/60 - 0s - loss: 0.1261 - acc: 1.0000 - val_loss: 0.3942 - val_acc: 0.8000\n",
            "Epoch 294/500\n",
            "\n",
            "Epoch 00294: val_loss did not improve from 0.18133\n",
            "60/60 - 0s - loss: 0.1462 - acc: 0.9833 - val_loss: 0.4333 - val_acc: 0.6500\n",
            "Epoch 295/500\n",
            "\n",
            "Epoch 00295: val_loss did not improve from 0.18133\n",
            "60/60 - 0s - loss: 0.1765 - acc: 0.9333 - val_loss: 0.3722 - val_acc: 0.8000\n",
            "Epoch 296/500\n",
            "\n",
            "Epoch 00296: val_loss did not improve from 0.18133\n",
            "60/60 - 0s - loss: 0.1319 - acc: 0.9667 - val_loss: 0.2991 - val_acc: 0.8500\n",
            "Epoch 297/500\n",
            "\n",
            "Epoch 00297: val_loss did not improve from 0.18133\n",
            "60/60 - 0s - loss: 0.1355 - acc: 0.9833 - val_loss: 0.2623 - val_acc: 0.9000\n",
            "Epoch 298/500\n",
            "\n",
            "Epoch 00298: val_loss did not improve from 0.18133\n",
            "60/60 - 0s - loss: 0.1720 - acc: 0.9667 - val_loss: 0.2384 - val_acc: 0.9000\n",
            "Epoch 299/500\n",
            "\n",
            "Epoch 00299: val_loss did not improve from 0.18133\n",
            "60/60 - 0s - loss: 0.1250 - acc: 0.9667 - val_loss: 0.2190 - val_acc: 0.9500\n",
            "Epoch 300/500\n",
            "\n",
            "Epoch 00300: val_loss did not improve from 0.18133\n",
            "60/60 - 0s - loss: 0.1214 - acc: 0.9833 - val_loss: 0.2153 - val_acc: 0.9500\n",
            "Epoch 301/500\n",
            "\n",
            "Epoch 00301: val_loss did not improve from 0.18133\n",
            "60/60 - 0s - loss: 0.1041 - acc: 0.9667 - val_loss: 0.2082 - val_acc: 0.9500\n",
            "Epoch 302/500\n",
            "\n",
            "Epoch 00302: val_loss did not improve from 0.18133\n",
            "60/60 - 0s - loss: 0.1143 - acc: 0.9833 - val_loss: 0.2180 - val_acc: 0.9500\n",
            "Epoch 303/500\n",
            "\n",
            "Epoch 00303: val_loss did not improve from 0.18133\n",
            "60/60 - 0s - loss: 0.1660 - acc: 0.9500 - val_loss: 0.2328 - val_acc: 0.9500\n",
            "Epoch 304/500\n",
            "\n",
            "Epoch 00304: val_loss did not improve from 0.18133\n",
            "60/60 - 0s - loss: 0.1616 - acc: 0.9500 - val_loss: 0.2400 - val_acc: 0.9000\n",
            "Epoch 305/500\n",
            "\n",
            "Epoch 00305: val_loss did not improve from 0.18133\n",
            "60/60 - 0s - loss: 0.1189 - acc: 0.9667 - val_loss: 0.2406 - val_acc: 0.9000\n",
            "Epoch 306/500\n",
            "\n",
            "Epoch 00306: val_loss did not improve from 0.18133\n",
            "60/60 - 0s - loss: 0.1301 - acc: 0.9833 - val_loss: 0.2380 - val_acc: 0.9000\n",
            "Epoch 307/500\n",
            "\n",
            "Epoch 00307: val_loss did not improve from 0.18133\n",
            "60/60 - 0s - loss: 0.1142 - acc: 0.9833 - val_loss: 0.2355 - val_acc: 0.9000\n",
            "Epoch 308/500\n",
            "\n",
            "Epoch 00308: val_loss did not improve from 0.18133\n",
            "60/60 - 0s - loss: 0.1180 - acc: 0.9667 - val_loss: 0.2338 - val_acc: 0.9000\n",
            "Epoch 309/500\n",
            "\n",
            "Epoch 00309: val_loss did not improve from 0.18133\n",
            "60/60 - 0s - loss: 0.0913 - acc: 1.0000 - val_loss: 0.2337 - val_acc: 0.9000\n",
            "Epoch 310/500\n",
            "\n",
            "Epoch 00310: val_loss did not improve from 0.18133\n",
            "60/60 - 0s - loss: 0.1117 - acc: 0.9833 - val_loss: 0.2349 - val_acc: 0.8500\n",
            "Epoch 311/500\n",
            "\n",
            "Epoch 00311: val_loss did not improve from 0.18133\n",
            "60/60 - 0s - loss: 0.1595 - acc: 0.9667 - val_loss: 0.2315 - val_acc: 0.8500\n",
            "Epoch 312/500\n",
            "\n",
            "Epoch 00312: val_loss did not improve from 0.18133\n",
            "60/60 - 0s - loss: 0.1208 - acc: 0.9833 - val_loss: 0.2336 - val_acc: 0.8500\n",
            "Epoch 313/500\n",
            "\n",
            "Epoch 00313: val_loss did not improve from 0.18133\n",
            "60/60 - 0s - loss: 0.1513 - acc: 0.9500 - val_loss: 0.2205 - val_acc: 0.9500\n",
            "Epoch 314/500\n",
            "\n",
            "Epoch 00314: val_loss did not improve from 0.18133\n",
            "60/60 - 0s - loss: 0.1152 - acc: 1.0000 - val_loss: 0.2028 - val_acc: 0.9500\n",
            "Epoch 315/500\n",
            "\n",
            "Epoch 00315: val_loss did not improve from 0.18133\n",
            "60/60 - 0s - loss: 0.1053 - acc: 1.0000 - val_loss: 0.1893 - val_acc: 1.0000\n",
            "Epoch 316/500\n",
            "\n",
            "Epoch 00316: val_loss improved from 0.18133 to 0.18041, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1327 - acc: 0.9833 - val_loss: 0.1804 - val_acc: 1.0000\n",
            "Epoch 317/500\n",
            "\n",
            "Epoch 00317: val_loss did not improve from 0.18041\n",
            "60/60 - 0s - loss: 0.0963 - acc: 1.0000 - val_loss: 0.1815 - val_acc: 1.0000\n",
            "Epoch 318/500\n",
            "\n",
            "Epoch 00318: val_loss did not improve from 0.18041\n",
            "60/60 - 0s - loss: 0.1615 - acc: 0.9667 - val_loss: 0.1940 - val_acc: 1.0000\n",
            "Epoch 319/500\n",
            "\n",
            "Epoch 00319: val_loss did not improve from 0.18041\n",
            "60/60 - 0s - loss: 0.0991 - acc: 0.9833 - val_loss: 0.2123 - val_acc: 0.9500\n",
            "Epoch 320/500\n",
            "\n",
            "Epoch 00320: val_loss did not improve from 0.18041\n",
            "60/60 - 0s - loss: 0.1195 - acc: 0.9833 - val_loss: 0.2283 - val_acc: 0.8500\n",
            "Epoch 321/500\n",
            "\n",
            "Epoch 00321: val_loss did not improve from 0.18041\n",
            "60/60 - 0s - loss: 0.1215 - acc: 0.9833 - val_loss: 0.2284 - val_acc: 0.8500\n",
            "Epoch 322/500\n",
            "\n",
            "Epoch 00322: val_loss did not improve from 0.18041\n",
            "60/60 - 0s - loss: 0.0998 - acc: 1.0000 - val_loss: 0.2259 - val_acc: 0.8500\n",
            "Epoch 323/500\n",
            "\n",
            "Epoch 00323: val_loss did not improve from 0.18041\n",
            "60/60 - 0s - loss: 0.0926 - acc: 0.9833 - val_loss: 0.2149 - val_acc: 0.9000\n",
            "Epoch 324/500\n",
            "\n",
            "Epoch 00324: val_loss did not improve from 0.18041\n",
            "60/60 - 0s - loss: 0.1104 - acc: 0.9833 - val_loss: 0.2066 - val_acc: 0.9500\n",
            "Epoch 325/500\n",
            "\n",
            "Epoch 00325: val_loss did not improve from 0.18041\n",
            "60/60 - 0s - loss: 0.1248 - acc: 0.9667 - val_loss: 0.2017 - val_acc: 0.9500\n",
            "Epoch 326/500\n",
            "\n",
            "Epoch 00326: val_loss did not improve from 0.18041\n",
            "60/60 - 0s - loss: 0.0833 - acc: 1.0000 - val_loss: 0.1894 - val_acc: 1.0000\n",
            "Epoch 327/500\n",
            "\n",
            "Epoch 00327: val_loss did not improve from 0.18041\n",
            "60/60 - 0s - loss: 0.2231 - acc: 0.9167 - val_loss: 0.1899 - val_acc: 0.9500\n",
            "Epoch 328/500\n",
            "\n",
            "Epoch 00328: val_loss did not improve from 0.18041\n",
            "60/60 - 0s - loss: 0.1212 - acc: 0.9833 - val_loss: 0.2198 - val_acc: 0.9000\n",
            "Epoch 329/500\n",
            "\n",
            "Epoch 00329: val_loss did not improve from 0.18041\n",
            "60/60 - 0s - loss: 0.1417 - acc: 0.9833 - val_loss: 0.2649 - val_acc: 0.8500\n",
            "Epoch 330/500\n",
            "\n",
            "Epoch 00330: val_loss did not improve from 0.18041\n",
            "60/60 - 0s - loss: 0.1141 - acc: 0.9833 - val_loss: 0.2764 - val_acc: 0.8500\n",
            "Epoch 331/500\n",
            "\n",
            "Epoch 00331: val_loss did not improve from 0.18041\n",
            "60/60 - 0s - loss: 0.1092 - acc: 1.0000 - val_loss: 0.2733 - val_acc: 0.8500\n",
            "Epoch 332/500\n",
            "\n",
            "Epoch 00332: val_loss did not improve from 0.18041\n",
            "60/60 - 0s - loss: 0.1452 - acc: 0.9500 - val_loss: 0.2479 - val_acc: 0.8500\n",
            "Epoch 333/500\n",
            "\n",
            "Epoch 00333: val_loss did not improve from 0.18041\n",
            "60/60 - 0s - loss: 0.0942 - acc: 1.0000 - val_loss: 0.2284 - val_acc: 0.9000\n",
            "Epoch 334/500\n",
            "\n",
            "Epoch 00334: val_loss did not improve from 0.18041\n",
            "60/60 - 0s - loss: 0.1261 - acc: 0.9833 - val_loss: 0.2023 - val_acc: 0.9500\n",
            "Epoch 335/500\n",
            "\n",
            "Epoch 00335: val_loss did not improve from 0.18041\n",
            "60/60 - 0s - loss: 0.1112 - acc: 0.9833 - val_loss: 0.1934 - val_acc: 0.9500\n",
            "Epoch 336/500\n",
            "\n",
            "Epoch 00336: val_loss did not improve from 0.18041\n",
            "60/60 - 0s - loss: 0.1117 - acc: 1.0000 - val_loss: 0.1955 - val_acc: 0.9500\n",
            "Epoch 337/500\n",
            "\n",
            "Epoch 00337: val_loss did not improve from 0.18041\n",
            "60/60 - 0s - loss: 0.0889 - acc: 0.9833 - val_loss: 0.1923 - val_acc: 1.0000\n",
            "Epoch 338/500\n",
            "\n",
            "Epoch 00338: val_loss did not improve from 0.18041\n",
            "60/60 - 0s - loss: 0.1456 - acc: 0.9667 - val_loss: 0.1864 - val_acc: 1.0000\n",
            "Epoch 339/500\n",
            "\n",
            "Epoch 00339: val_loss did not improve from 0.18041\n",
            "60/60 - 0s - loss: 0.0924 - acc: 1.0000 - val_loss: 0.1838 - val_acc: 1.0000\n",
            "Epoch 340/500\n",
            "\n",
            "Epoch 00340: val_loss did not improve from 0.18041\n",
            "60/60 - 0s - loss: 0.1369 - acc: 0.9667 - val_loss: 0.1899 - val_acc: 0.9500\n",
            "Epoch 341/500\n",
            "\n",
            "Epoch 00341: val_loss did not improve from 0.18041\n",
            "60/60 - 0s - loss: 0.1112 - acc: 0.9833 - val_loss: 0.2288 - val_acc: 0.9500\n",
            "Epoch 342/500\n",
            "\n",
            "Epoch 00342: val_loss did not improve from 0.18041\n",
            "60/60 - 0s - loss: 0.0948 - acc: 0.9833 - val_loss: 0.2293 - val_acc: 0.9500\n",
            "Epoch 343/500\n",
            "\n",
            "Epoch 00343: val_loss did not improve from 0.18041\n",
            "60/60 - 0s - loss: 0.1160 - acc: 1.0000 - val_loss: 0.2212 - val_acc: 0.9500\n",
            "Epoch 344/500\n",
            "\n",
            "Epoch 00344: val_loss did not improve from 0.18041\n",
            "60/60 - 0s - loss: 0.1483 - acc: 0.9500 - val_loss: 0.2240 - val_acc: 0.9500\n",
            "Epoch 345/500\n",
            "\n",
            "Epoch 00345: val_loss did not improve from 0.18041\n",
            "60/60 - 0s - loss: 0.1549 - acc: 0.9833 - val_loss: 0.2107 - val_acc: 0.9500\n",
            "Epoch 346/500\n",
            "\n",
            "Epoch 00346: val_loss did not improve from 0.18041\n",
            "60/60 - 0s - loss: 0.1123 - acc: 0.9667 - val_loss: 0.1972 - val_acc: 1.0000\n",
            "Epoch 347/500\n",
            "\n",
            "Epoch 00347: val_loss did not improve from 0.18041\n",
            "60/60 - 0s - loss: 0.1232 - acc: 0.9833 - val_loss: 0.2142 - val_acc: 0.9000\n",
            "Epoch 348/500\n",
            "\n",
            "Epoch 00348: val_loss did not improve from 0.18041\n",
            "60/60 - 0s - loss: 0.1479 - acc: 0.9667 - val_loss: 0.2337 - val_acc: 0.9000\n",
            "Epoch 349/500\n",
            "\n",
            "Epoch 00349: val_loss did not improve from 0.18041\n",
            "60/60 - 0s - loss: 0.1250 - acc: 1.0000 - val_loss: 0.2462 - val_acc: 0.8500\n",
            "Epoch 350/500\n",
            "\n",
            "Epoch 00350: val_loss did not improve from 0.18041\n",
            "60/60 - 0s - loss: 0.1376 - acc: 1.0000 - val_loss: 0.2593 - val_acc: 0.8500\n",
            "Epoch 351/500\n",
            "\n",
            "Epoch 00351: val_loss did not improve from 0.18041\n",
            "60/60 - 0s - loss: 0.0992 - acc: 1.0000 - val_loss: 0.2478 - val_acc: 0.8500\n",
            "Epoch 352/500\n",
            "\n",
            "Epoch 00352: val_loss did not improve from 0.18041\n",
            "60/60 - 0s - loss: 0.1808 - acc: 0.9000 - val_loss: 0.2391 - val_acc: 0.9000\n",
            "Epoch 353/500\n",
            "\n",
            "Epoch 00353: val_loss did not improve from 0.18041\n",
            "60/60 - 0s - loss: 0.1191 - acc: 0.9833 - val_loss: 0.2227 - val_acc: 0.9000\n",
            "Epoch 354/500\n",
            "\n",
            "Epoch 00354: val_loss did not improve from 0.18041\n",
            "60/60 - 0s - loss: 0.1406 - acc: 0.9667 - val_loss: 0.2088 - val_acc: 0.9500\n",
            "Epoch 355/500\n",
            "\n",
            "Epoch 00355: val_loss did not improve from 0.18041\n",
            "60/60 - 0s - loss: 0.1329 - acc: 0.9833 - val_loss: 0.1990 - val_acc: 0.9500\n",
            "Epoch 356/500\n",
            "\n",
            "Epoch 00356: val_loss did not improve from 0.18041\n",
            "60/60 - 0s - loss: 0.1734 - acc: 0.9500 - val_loss: 0.1936 - val_acc: 0.9500\n",
            "Epoch 357/500\n",
            "\n",
            "Epoch 00357: val_loss did not improve from 0.18041\n",
            "60/60 - 0s - loss: 0.1300 - acc: 0.9667 - val_loss: 0.1991 - val_acc: 0.9500\n",
            "Epoch 358/500\n",
            "\n",
            "Epoch 00358: val_loss did not improve from 0.18041\n",
            "60/60 - 0s - loss: 0.1089 - acc: 0.9833 - val_loss: 0.2060 - val_acc: 0.9500\n",
            "Epoch 359/500\n",
            "\n",
            "Epoch 00359: val_loss did not improve from 0.18041\n",
            "60/60 - 0s - loss: 0.1138 - acc: 0.9833 - val_loss: 0.2110 - val_acc: 1.0000\n",
            "Epoch 360/500\n",
            "\n",
            "Epoch 00360: val_loss did not improve from 0.18041\n",
            "60/60 - 0s - loss: 0.0968 - acc: 1.0000 - val_loss: 0.2099 - val_acc: 1.0000\n",
            "Epoch 361/500\n",
            "\n",
            "Epoch 00361: val_loss did not improve from 0.18041\n",
            "60/60 - 0s - loss: 0.1077 - acc: 0.9833 - val_loss: 0.2198 - val_acc: 1.0000\n",
            "Epoch 362/500\n",
            "\n",
            "Epoch 00362: val_loss did not improve from 0.18041\n",
            "60/60 - 0s - loss: 0.1028 - acc: 1.0000 - val_loss: 0.2307 - val_acc: 0.9000\n",
            "Epoch 363/500\n",
            "\n",
            "Epoch 00363: val_loss did not improve from 0.18041\n",
            "60/60 - 0s - loss: 0.1051 - acc: 0.9500 - val_loss: 0.2220 - val_acc: 0.9000\n",
            "Epoch 364/500\n",
            "\n",
            "Epoch 00364: val_loss did not improve from 0.18041\n",
            "60/60 - 0s - loss: 0.0855 - acc: 1.0000 - val_loss: 0.2042 - val_acc: 0.9500\n",
            "Epoch 365/500\n",
            "\n",
            "Epoch 00365: val_loss did not improve from 0.18041\n",
            "60/60 - 0s - loss: 0.1054 - acc: 0.9833 - val_loss: 0.1993 - val_acc: 0.9500\n",
            "Epoch 366/500\n",
            "\n",
            "Epoch 00366: val_loss did not improve from 0.18041\n",
            "60/60 - 0s - loss: 0.0914 - acc: 0.9833 - val_loss: 0.1963 - val_acc: 0.9500\n",
            "Epoch 367/500\n",
            "\n",
            "Epoch 00367: val_loss did not improve from 0.18041\n",
            "60/60 - 0s - loss: 0.1107 - acc: 1.0000 - val_loss: 0.1900 - val_acc: 0.9500\n",
            "Epoch 368/500\n",
            "\n",
            "Epoch 00368: val_loss did not improve from 0.18041\n",
            "60/60 - 0s - loss: 0.1486 - acc: 0.9667 - val_loss: 0.1918 - val_acc: 0.9500\n",
            "Epoch 369/500\n",
            "\n",
            "Epoch 00369: val_loss did not improve from 0.18041\n",
            "60/60 - 0s - loss: 0.1536 - acc: 0.9333 - val_loss: 0.2178 - val_acc: 0.9500\n",
            "Epoch 370/500\n",
            "\n",
            "Epoch 00370: val_loss did not improve from 0.18041\n",
            "60/60 - 0s - loss: 0.1055 - acc: 0.9833 - val_loss: 0.2270 - val_acc: 0.9000\n",
            "Epoch 371/500\n",
            "\n",
            "Epoch 00371: val_loss did not improve from 0.18041\n",
            "60/60 - 0s - loss: 0.1753 - acc: 0.9667 - val_loss: 0.2162 - val_acc: 0.9500\n",
            "Epoch 372/500\n",
            "\n",
            "Epoch 00372: val_loss did not improve from 0.18041\n",
            "60/60 - 0s - loss: 0.1132 - acc: 0.9833 - val_loss: 0.2322 - val_acc: 0.9000\n",
            "Epoch 373/500\n",
            "\n",
            "Epoch 00373: val_loss did not improve from 0.18041\n",
            "60/60 - 0s - loss: 0.1123 - acc: 0.9833 - val_loss: 0.2669 - val_acc: 0.9000\n",
            "Epoch 374/500\n",
            "\n",
            "Epoch 00374: val_loss did not improve from 0.18041\n",
            "60/60 - 0s - loss: 0.0946 - acc: 0.9667 - val_loss: 0.2950 - val_acc: 0.8500\n",
            "Epoch 375/500\n",
            "\n",
            "Epoch 00375: val_loss did not improve from 0.18041\n",
            "60/60 - 0s - loss: 0.1363 - acc: 0.9667 - val_loss: 0.3003 - val_acc: 0.8500\n",
            "Epoch 376/500\n",
            "\n",
            "Epoch 00376: val_loss did not improve from 0.18041\n",
            "60/60 - 0s - loss: 0.1107 - acc: 1.0000 - val_loss: 0.2812 - val_acc: 0.8500\n",
            "Epoch 377/500\n",
            "\n",
            "Epoch 00377: val_loss did not improve from 0.18041\n",
            "60/60 - 0s - loss: 0.1327 - acc: 0.9667 - val_loss: 0.2542 - val_acc: 0.9000\n",
            "Epoch 378/500\n",
            "\n",
            "Epoch 00378: val_loss did not improve from 0.18041\n",
            "60/60 - 0s - loss: 0.1024 - acc: 0.9833 - val_loss: 0.2288 - val_acc: 0.9000\n",
            "Epoch 379/500\n",
            "\n",
            "Epoch 00379: val_loss did not improve from 0.18041\n",
            "60/60 - 0s - loss: 0.1056 - acc: 1.0000 - val_loss: 0.2068 - val_acc: 0.9500\n",
            "Epoch 380/500\n",
            "\n",
            "Epoch 00380: val_loss did not improve from 0.18041\n",
            "60/60 - 0s - loss: 0.1071 - acc: 0.9833 - val_loss: 0.2010 - val_acc: 0.9500\n",
            "Epoch 381/500\n",
            "\n",
            "Epoch 00381: val_loss did not improve from 0.18041\n",
            "60/60 - 0s - loss: 0.1753 - acc: 0.9500 - val_loss: 0.1991 - val_acc: 0.9500\n",
            "Epoch 382/500\n",
            "\n",
            "Epoch 00382: val_loss did not improve from 0.18041\n",
            "60/60 - 0s - loss: 0.1369 - acc: 1.0000 - val_loss: 0.2060 - val_acc: 0.9500\n",
            "Epoch 383/500\n",
            "\n",
            "Epoch 00383: val_loss did not improve from 0.18041\n",
            "60/60 - 0s - loss: 0.0869 - acc: 1.0000 - val_loss: 0.2069 - val_acc: 0.9500\n",
            "Epoch 384/500\n",
            "\n",
            "Epoch 00384: val_loss did not improve from 0.18041\n",
            "60/60 - 0s - loss: 0.1216 - acc: 0.9667 - val_loss: 0.2084 - val_acc: 0.9500\n",
            "Epoch 385/500\n",
            "\n",
            "Epoch 00385: val_loss did not improve from 0.18041\n",
            "60/60 - 0s - loss: 0.1117 - acc: 0.9833 - val_loss: 0.1949 - val_acc: 0.9500\n",
            "Epoch 386/500\n",
            "\n",
            "Epoch 00386: val_loss did not improve from 0.18041\n",
            "60/60 - 0s - loss: 0.0919 - acc: 0.9833 - val_loss: 0.1853 - val_acc: 0.9500\n",
            "Epoch 387/500\n",
            "\n",
            "Epoch 00387: val_loss improved from 0.18041 to 0.17990, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.0880 - acc: 0.9833 - val_loss: 0.1799 - val_acc: 1.0000\n",
            "Epoch 388/500\n",
            "\n",
            "Epoch 00388: val_loss did not improve from 0.17990\n",
            "60/60 - 0s - loss: 0.1204 - acc: 0.9833 - val_loss: 0.1809 - val_acc: 1.0000\n",
            "Epoch 389/500\n",
            "\n",
            "Epoch 00389: val_loss did not improve from 0.17990\n",
            "60/60 - 0s - loss: 0.1373 - acc: 0.9667 - val_loss: 0.1837 - val_acc: 1.0000\n",
            "Epoch 390/500\n",
            "\n",
            "Epoch 00390: val_loss did not improve from 0.17990\n",
            "60/60 - 0s - loss: 0.1072 - acc: 0.9833 - val_loss: 0.1884 - val_acc: 0.9500\n",
            "Epoch 391/500\n",
            "\n",
            "Epoch 00391: val_loss did not improve from 0.17990\n",
            "60/60 - 0s - loss: 0.0826 - acc: 0.9667 - val_loss: 0.1973 - val_acc: 0.9500\n",
            "Epoch 392/500\n",
            "\n",
            "Epoch 00392: val_loss did not improve from 0.17990\n",
            "60/60 - 0s - loss: 0.1061 - acc: 1.0000 - val_loss: 0.2109 - val_acc: 0.9500\n",
            "Epoch 393/500\n",
            "\n",
            "Epoch 00393: val_loss did not improve from 0.17990\n",
            "60/60 - 0s - loss: 0.0996 - acc: 0.9833 - val_loss: 0.2143 - val_acc: 0.9500\n",
            "Epoch 394/500\n",
            "\n",
            "Epoch 00394: val_loss did not improve from 0.17990\n",
            "60/60 - 0s - loss: 0.1157 - acc: 0.9833 - val_loss: 0.2173 - val_acc: 0.9500\n",
            "Epoch 395/500\n",
            "\n",
            "Epoch 00395: val_loss did not improve from 0.17990\n",
            "60/60 - 0s - loss: 0.0819 - acc: 1.0000 - val_loss: 0.2143 - val_acc: 0.9500\n",
            "Epoch 396/500\n",
            "\n",
            "Epoch 00396: val_loss did not improve from 0.17990\n",
            "60/60 - 0s - loss: 0.1130 - acc: 0.9833 - val_loss: 0.2172 - val_acc: 1.0000\n",
            "Epoch 397/500\n",
            "\n",
            "Epoch 00397: val_loss did not improve from 0.17990\n",
            "60/60 - 0s - loss: 0.0860 - acc: 1.0000 - val_loss: 0.2327 - val_acc: 0.9500\n",
            "Epoch 398/500\n",
            "\n",
            "Epoch 00398: val_loss did not improve from 0.17990\n",
            "60/60 - 0s - loss: 0.0833 - acc: 0.9833 - val_loss: 0.2315 - val_acc: 0.9500\n",
            "Epoch 399/500\n",
            "\n",
            "Epoch 00399: val_loss did not improve from 0.17990\n",
            "60/60 - 0s - loss: 0.0932 - acc: 0.9833 - val_loss: 0.2188 - val_acc: 0.9500\n",
            "Epoch 400/500\n",
            "\n",
            "Epoch 00400: val_loss did not improve from 0.17990\n",
            "60/60 - 0s - loss: 0.1121 - acc: 0.9500 - val_loss: 0.2202 - val_acc: 0.9500\n",
            "Epoch 401/500\n",
            "\n",
            "Epoch 00401: val_loss did not improve from 0.17990\n",
            "60/60 - 0s - loss: 0.0843 - acc: 1.0000 - val_loss: 0.2170 - val_acc: 1.0000\n",
            "Epoch 402/500\n",
            "\n",
            "Epoch 00402: val_loss did not improve from 0.17990\n",
            "60/60 - 0s - loss: 0.1064 - acc: 1.0000 - val_loss: 0.2281 - val_acc: 0.9500\n",
            "Epoch 403/500\n",
            "\n",
            "Epoch 00403: val_loss did not improve from 0.17990\n",
            "60/60 - 0s - loss: 0.0705 - acc: 0.9833 - val_loss: 0.2362 - val_acc: 0.9000\n",
            "Epoch 404/500\n",
            "\n",
            "Epoch 00404: val_loss did not improve from 0.17990\n",
            "60/60 - 0s - loss: 0.0895 - acc: 1.0000 - val_loss: 0.2328 - val_acc: 0.9000\n",
            "Epoch 405/500\n",
            "\n",
            "Epoch 00405: val_loss did not improve from 0.17990\n",
            "60/60 - 0s - loss: 0.1021 - acc: 0.9667 - val_loss: 0.2209 - val_acc: 0.9500\n",
            "Epoch 406/500\n",
            "\n",
            "Epoch 00406: val_loss did not improve from 0.17990\n",
            "60/60 - 0s - loss: 0.0720 - acc: 1.0000 - val_loss: 0.2135 - val_acc: 0.9500\n",
            "Epoch 407/500\n",
            "\n",
            "Epoch 00407: val_loss did not improve from 0.17990\n",
            "60/60 - 0s - loss: 0.1151 - acc: 0.9833 - val_loss: 0.1951 - val_acc: 0.9500\n",
            "Epoch 408/500\n",
            "\n",
            "Epoch 00408: val_loss did not improve from 0.17990\n",
            "60/60 - 0s - loss: 0.1046 - acc: 1.0000 - val_loss: 0.1879 - val_acc: 0.9500\n",
            "Epoch 409/500\n",
            "\n",
            "Epoch 00409: val_loss did not improve from 0.17990\n",
            "60/60 - 0s - loss: 0.1036 - acc: 1.0000 - val_loss: 0.1929 - val_acc: 0.9500\n",
            "Epoch 410/500\n",
            "\n",
            "Epoch 00410: val_loss did not improve from 0.17990\n",
            "60/60 - 0s - loss: 0.1081 - acc: 0.9667 - val_loss: 0.1999 - val_acc: 0.9500\n",
            "Epoch 411/500\n",
            "\n",
            "Epoch 00411: val_loss did not improve from 0.17990\n",
            "60/60 - 0s - loss: 0.1239 - acc: 0.9833 - val_loss: 0.1979 - val_acc: 0.9500\n",
            "Epoch 412/500\n",
            "\n",
            "Epoch 00412: val_loss did not improve from 0.17990\n",
            "60/60 - 0s - loss: 0.1319 - acc: 0.9833 - val_loss: 0.1904 - val_acc: 0.9500\n",
            "Epoch 413/500\n",
            "\n",
            "Epoch 00413: val_loss did not improve from 0.17990\n",
            "60/60 - 0s - loss: 0.0964 - acc: 0.9833 - val_loss: 0.1821 - val_acc: 0.9500\n",
            "Epoch 414/500\n",
            "\n",
            "Epoch 00414: val_loss improved from 0.17990 to 0.17365, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.0835 - acc: 1.0000 - val_loss: 0.1737 - val_acc: 0.9500\n",
            "Epoch 415/500\n",
            "\n",
            "Epoch 00415: val_loss improved from 0.17365 to 0.16854, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1076 - acc: 0.9833 - val_loss: 0.1685 - val_acc: 0.9500\n",
            "Epoch 416/500\n",
            "\n",
            "Epoch 00416: val_loss did not improve from 0.16854\n",
            "60/60 - 0s - loss: 0.1181 - acc: 0.9833 - val_loss: 0.1771 - val_acc: 0.9500\n",
            "Epoch 417/500\n",
            "\n",
            "Epoch 00417: val_loss did not improve from 0.16854\n",
            "60/60 - 0s - loss: 0.0959 - acc: 1.0000 - val_loss: 0.1855 - val_acc: 0.9500\n",
            "Epoch 418/500\n",
            "\n",
            "Epoch 00418: val_loss did not improve from 0.16854\n",
            "60/60 - 0s - loss: 0.1154 - acc: 0.9500 - val_loss: 0.1894 - val_acc: 0.9500\n",
            "Epoch 419/500\n",
            "\n",
            "Epoch 00419: val_loss did not improve from 0.16854\n",
            "60/60 - 0s - loss: 0.0910 - acc: 0.9833 - val_loss: 0.2035 - val_acc: 1.0000\n",
            "Epoch 420/500\n",
            "\n",
            "Epoch 00420: val_loss did not improve from 0.16854\n",
            "60/60 - 0s - loss: 0.1393 - acc: 0.9667 - val_loss: 0.2199 - val_acc: 1.0000\n",
            "Epoch 421/500\n",
            "\n",
            "Epoch 00421: val_loss did not improve from 0.16854\n",
            "60/60 - 0s - loss: 0.0869 - acc: 0.9833 - val_loss: 0.2190 - val_acc: 0.9500\n",
            "Epoch 422/500\n",
            "\n",
            "Epoch 00422: val_loss did not improve from 0.16854\n",
            "60/60 - 0s - loss: 0.0877 - acc: 1.0000 - val_loss: 0.2140 - val_acc: 0.9500\n",
            "Epoch 423/500\n",
            "\n",
            "Epoch 00423: val_loss did not improve from 0.16854\n",
            "60/60 - 0s - loss: 0.0778 - acc: 1.0000 - val_loss: 0.2076 - val_acc: 0.9500\n",
            "Epoch 424/500\n",
            "\n",
            "Epoch 00424: val_loss did not improve from 0.16854\n",
            "60/60 - 0s - loss: 0.0777 - acc: 1.0000 - val_loss: 0.2101 - val_acc: 0.9500\n",
            "Epoch 425/500\n",
            "\n",
            "Epoch 00425: val_loss did not improve from 0.16854\n",
            "60/60 - 0s - loss: 0.0989 - acc: 0.9667 - val_loss: 0.2098 - val_acc: 0.9500\n",
            "Epoch 426/500\n",
            "\n",
            "Epoch 00426: val_loss did not improve from 0.16854\n",
            "60/60 - 0s - loss: 0.0827 - acc: 1.0000 - val_loss: 0.1988 - val_acc: 0.9500\n",
            "Epoch 427/500\n",
            "\n",
            "Epoch 00427: val_loss did not improve from 0.16854\n",
            "60/60 - 0s - loss: 0.0961 - acc: 0.9833 - val_loss: 0.1967 - val_acc: 0.9500\n",
            "Epoch 428/500\n",
            "\n",
            "Epoch 00428: val_loss did not improve from 0.16854\n",
            "60/60 - 0s - loss: 0.0604 - acc: 1.0000 - val_loss: 0.1937 - val_acc: 0.9500\n",
            "Epoch 429/500\n",
            "\n",
            "Epoch 00429: val_loss did not improve from 0.16854\n",
            "60/60 - 0s - loss: 0.0921 - acc: 0.9833 - val_loss: 0.1894 - val_acc: 0.9500\n",
            "Epoch 430/500\n",
            "\n",
            "Epoch 00430: val_loss did not improve from 0.16854\n",
            "60/60 - 0s - loss: 0.0947 - acc: 1.0000 - val_loss: 0.1847 - val_acc: 0.9500\n",
            "Epoch 431/500\n",
            "\n",
            "Epoch 00431: val_loss did not improve from 0.16854\n",
            "60/60 - 0s - loss: 0.0666 - acc: 1.0000 - val_loss: 0.1864 - val_acc: 0.9500\n",
            "Epoch 432/500\n",
            "\n",
            "Epoch 00432: val_loss did not improve from 0.16854\n",
            "60/60 - 0s - loss: 0.1238 - acc: 0.9667 - val_loss: 0.1910 - val_acc: 0.9500\n",
            "Epoch 433/500\n",
            "\n",
            "Epoch 00433: val_loss did not improve from 0.16854\n",
            "60/60 - 0s - loss: 0.0728 - acc: 1.0000 - val_loss: 0.1890 - val_acc: 0.9500\n",
            "Epoch 434/500\n",
            "\n",
            "Epoch 00434: val_loss did not improve from 0.16854\n",
            "60/60 - 0s - loss: 0.0892 - acc: 1.0000 - val_loss: 0.1866 - val_acc: 0.9500\n",
            "Epoch 435/500\n",
            "\n",
            "Epoch 00435: val_loss did not improve from 0.16854\n",
            "60/60 - 0s - loss: 0.0568 - acc: 1.0000 - val_loss: 0.1842 - val_acc: 0.9500\n",
            "Epoch 436/500\n",
            "\n",
            "Epoch 00436: val_loss did not improve from 0.16854\n",
            "60/60 - 0s - loss: 0.0924 - acc: 1.0000 - val_loss: 0.1840 - val_acc: 0.9000\n",
            "Epoch 437/500\n",
            "\n",
            "Epoch 00437: val_loss did not improve from 0.16854\n",
            "60/60 - 0s - loss: 0.0794 - acc: 1.0000 - val_loss: 0.1812 - val_acc: 1.0000\n",
            "Epoch 438/500\n",
            "\n",
            "Epoch 00438: val_loss did not improve from 0.16854\n",
            "60/60 - 0s - loss: 0.1425 - acc: 0.9667 - val_loss: 0.1872 - val_acc: 1.0000\n",
            "Epoch 439/500\n",
            "\n",
            "Epoch 00439: val_loss did not improve from 0.16854\n",
            "60/60 - 0s - loss: 0.0862 - acc: 1.0000 - val_loss: 0.1944 - val_acc: 0.9500\n",
            "Epoch 440/500\n",
            "\n",
            "Epoch 00440: val_loss did not improve from 0.16854\n",
            "60/60 - 0s - loss: 0.0850 - acc: 1.0000 - val_loss: 0.2095 - val_acc: 0.9500\n",
            "Epoch 441/500\n",
            "\n",
            "Epoch 00441: val_loss did not improve from 0.16854\n",
            "60/60 - 0s - loss: 0.0790 - acc: 0.9833 - val_loss: 0.2306 - val_acc: 0.9000\n",
            "Epoch 442/500\n",
            "\n",
            "Epoch 00442: val_loss did not improve from 0.16854\n",
            "60/60 - 0s - loss: 0.0888 - acc: 0.9833 - val_loss: 0.2452 - val_acc: 0.9000\n",
            "Epoch 443/500\n",
            "\n",
            "Epoch 00443: val_loss did not improve from 0.16854\n",
            "60/60 - 0s - loss: 0.0673 - acc: 1.0000 - val_loss: 0.2476 - val_acc: 0.9000\n",
            "Epoch 444/500\n",
            "\n",
            "Epoch 00444: val_loss did not improve from 0.16854\n",
            "60/60 - 0s - loss: 0.0678 - acc: 1.0000 - val_loss: 0.2216 - val_acc: 0.9500\n",
            "Epoch 445/500\n",
            "\n",
            "Epoch 00445: val_loss did not improve from 0.16854\n",
            "60/60 - 0s - loss: 0.0925 - acc: 0.9833 - val_loss: 0.2141 - val_acc: 0.9500\n",
            "Epoch 446/500\n",
            "\n",
            "Epoch 00446: val_loss did not improve from 0.16854\n",
            "60/60 - 0s - loss: 0.0809 - acc: 0.9833 - val_loss: 0.2000 - val_acc: 0.9500\n",
            "Epoch 447/500\n",
            "\n",
            "Epoch 00447: val_loss did not improve from 0.16854\n",
            "60/60 - 0s - loss: 0.0867 - acc: 0.9833 - val_loss: 0.1903 - val_acc: 0.9500\n",
            "Epoch 448/500\n",
            "\n",
            "Epoch 00448: val_loss did not improve from 0.16854\n",
            "60/60 - 0s - loss: 0.0633 - acc: 1.0000 - val_loss: 0.1801 - val_acc: 0.9500\n",
            "Epoch 449/500\n",
            "\n",
            "Epoch 00449: val_loss did not improve from 0.16854\n",
            "60/60 - 0s - loss: 0.0551 - acc: 1.0000 - val_loss: 0.1768 - val_acc: 0.9500\n",
            "Epoch 450/500\n",
            "\n",
            "Epoch 00450: val_loss did not improve from 0.16854\n",
            "60/60 - 0s - loss: 0.0682 - acc: 1.0000 - val_loss: 0.1830 - val_acc: 0.9500\n",
            "Epoch 451/500\n",
            "\n",
            "Epoch 00451: val_loss did not improve from 0.16854\n",
            "60/60 - 0s - loss: 0.0873 - acc: 0.9833 - val_loss: 0.1890 - val_acc: 0.9500\n",
            "Epoch 452/500\n",
            "\n",
            "Epoch 00452: val_loss did not improve from 0.16854\n",
            "60/60 - 0s - loss: 0.0919 - acc: 1.0000 - val_loss: 0.1862 - val_acc: 0.9500\n",
            "Epoch 453/500\n",
            "\n",
            "Epoch 00453: val_loss did not improve from 0.16854\n",
            "60/60 - 0s - loss: 0.0922 - acc: 1.0000 - val_loss: 0.1735 - val_acc: 0.9500\n",
            "Epoch 454/500\n",
            "\n",
            "Epoch 00454: val_loss improved from 0.16854 to 0.15967, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1130 - acc: 0.9833 - val_loss: 0.1597 - val_acc: 0.9500\n",
            "Epoch 455/500\n",
            "\n",
            "Epoch 00455: val_loss improved from 0.15967 to 0.14984, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.0779 - acc: 1.0000 - val_loss: 0.1498 - val_acc: 1.0000\n",
            "Epoch 456/500\n",
            "\n",
            "Epoch 00456: val_loss did not improve from 0.14984\n",
            "60/60 - 0s - loss: 0.0715 - acc: 1.0000 - val_loss: 0.1570 - val_acc: 0.9500\n",
            "Epoch 457/500\n",
            "\n",
            "Epoch 00457: val_loss did not improve from 0.14984\n",
            "60/60 - 0s - loss: 0.0717 - acc: 1.0000 - val_loss: 0.1646 - val_acc: 0.9000\n",
            "Epoch 458/500\n",
            "\n",
            "Epoch 00458: val_loss did not improve from 0.14984\n",
            "60/60 - 0s - loss: 0.1017 - acc: 0.9833 - val_loss: 0.1733 - val_acc: 0.9000\n",
            "Epoch 459/500\n",
            "\n",
            "Epoch 00459: val_loss did not improve from 0.14984\n",
            "60/60 - 0s - loss: 0.0861 - acc: 1.0000 - val_loss: 0.1767 - val_acc: 0.9000\n",
            "Epoch 460/500\n",
            "\n",
            "Epoch 00460: val_loss did not improve from 0.14984\n",
            "60/60 - 0s - loss: 0.0702 - acc: 1.0000 - val_loss: 0.1749 - val_acc: 0.9500\n",
            "Epoch 461/500\n",
            "\n",
            "Epoch 00461: val_loss did not improve from 0.14984\n",
            "60/60 - 0s - loss: 0.0511 - acc: 1.0000 - val_loss: 0.1881 - val_acc: 0.9500\n",
            "Epoch 462/500\n",
            "\n",
            "Epoch 00462: val_loss did not improve from 0.14984\n",
            "60/60 - 0s - loss: 0.0979 - acc: 0.9833 - val_loss: 0.1960 - val_acc: 0.9500\n",
            "Epoch 463/500\n",
            "\n",
            "Epoch 00463: val_loss did not improve from 0.14984\n",
            "60/60 - 0s - loss: 0.0737 - acc: 1.0000 - val_loss: 0.1912 - val_acc: 0.9500\n",
            "Epoch 464/500\n",
            "\n",
            "Epoch 00464: val_loss did not improve from 0.14984\n",
            "60/60 - 0s - loss: 0.0572 - acc: 0.9833 - val_loss: 0.1896 - val_acc: 0.9500\n",
            "Epoch 465/500\n",
            "\n",
            "Epoch 00465: val_loss did not improve from 0.14984\n",
            "60/60 - 0s - loss: 0.0762 - acc: 1.0000 - val_loss: 0.2019 - val_acc: 0.9000\n",
            "Epoch 466/500\n",
            "\n",
            "Epoch 00466: val_loss did not improve from 0.14984\n",
            "60/60 - 0s - loss: 0.0644 - acc: 1.0000 - val_loss: 0.2146 - val_acc: 0.9000\n",
            "Epoch 467/500\n",
            "\n",
            "Epoch 00467: val_loss did not improve from 0.14984\n",
            "60/60 - 0s - loss: 0.0792 - acc: 0.9833 - val_loss: 0.2267 - val_acc: 0.9000\n",
            "Epoch 468/500\n",
            "\n",
            "Epoch 00468: val_loss did not improve from 0.14984\n",
            "60/60 - 0s - loss: 0.0750 - acc: 1.0000 - val_loss: 0.2259 - val_acc: 0.9000\n",
            "Epoch 469/500\n",
            "\n",
            "Epoch 00469: val_loss did not improve from 0.14984\n",
            "60/60 - 0s - loss: 0.1149 - acc: 0.9833 - val_loss: 0.2289 - val_acc: 0.9000\n",
            "Epoch 470/500\n",
            "\n",
            "Epoch 00470: val_loss did not improve from 0.14984\n",
            "60/60 - 0s - loss: 0.0762 - acc: 1.0000 - val_loss: 0.2256 - val_acc: 0.9000\n",
            "Epoch 471/500\n",
            "\n",
            "Epoch 00471: val_loss did not improve from 0.14984\n",
            "60/60 - 0s - loss: 0.1267 - acc: 0.9667 - val_loss: 0.2257 - val_acc: 0.9000\n",
            "Epoch 472/500\n",
            "\n",
            "Epoch 00472: val_loss did not improve from 0.14984\n",
            "60/60 - 0s - loss: 0.0694 - acc: 1.0000 - val_loss: 0.2270 - val_acc: 0.9000\n",
            "Epoch 473/500\n",
            "\n",
            "Epoch 00473: val_loss did not improve from 0.14984\n",
            "60/60 - 0s - loss: 0.1089 - acc: 0.9667 - val_loss: 0.2281 - val_acc: 0.9000\n",
            "Epoch 474/500\n",
            "\n",
            "Epoch 00474: val_loss did not improve from 0.14984\n",
            "60/60 - 0s - loss: 0.0904 - acc: 0.9667 - val_loss: 0.2305 - val_acc: 0.8500\n",
            "Epoch 475/500\n",
            "\n",
            "Epoch 00475: val_loss did not improve from 0.14984\n",
            "60/60 - 0s - loss: 0.1139 - acc: 0.9833 - val_loss: 0.2291 - val_acc: 0.8500\n",
            "Epoch 476/500\n",
            "\n",
            "Epoch 00476: val_loss did not improve from 0.14984\n",
            "60/60 - 0s - loss: 0.0647 - acc: 1.0000 - val_loss: 0.2353 - val_acc: 0.9000\n",
            "Epoch 477/500\n",
            "\n",
            "Epoch 00477: val_loss did not improve from 0.14984\n",
            "60/60 - 0s - loss: 0.1013 - acc: 0.9833 - val_loss: 0.2330 - val_acc: 0.9000\n",
            "Epoch 478/500\n",
            "\n",
            "Epoch 00478: val_loss did not improve from 0.14984\n",
            "60/60 - 0s - loss: 0.0977 - acc: 1.0000 - val_loss: 0.2338 - val_acc: 0.9000\n",
            "Epoch 479/500\n",
            "\n",
            "Epoch 00479: val_loss did not improve from 0.14984\n",
            "60/60 - 0s - loss: 0.0947 - acc: 1.0000 - val_loss: 0.2331 - val_acc: 0.8500\n",
            "Epoch 480/500\n",
            "\n",
            "Epoch 00480: val_loss did not improve from 0.14984\n",
            "60/60 - 0s - loss: 0.1058 - acc: 0.9667 - val_loss: 0.2313 - val_acc: 0.9500\n",
            "Epoch 481/500\n",
            "\n",
            "Epoch 00481: val_loss did not improve from 0.14984\n",
            "60/60 - 0s - loss: 0.0647 - acc: 1.0000 - val_loss: 0.2257 - val_acc: 0.9500\n",
            "Epoch 482/500\n",
            "\n",
            "Epoch 00482: val_loss did not improve from 0.14984\n",
            "60/60 - 0s - loss: 0.0876 - acc: 0.9833 - val_loss: 0.2210 - val_acc: 0.9500\n",
            "Epoch 483/500\n",
            "\n",
            "Epoch 00483: val_loss did not improve from 0.14984\n",
            "60/60 - 0s - loss: 0.1259 - acc: 0.9667 - val_loss: 0.2129 - val_acc: 0.9500\n",
            "Epoch 484/500\n",
            "\n",
            "Epoch 00484: val_loss did not improve from 0.14984\n",
            "60/60 - 0s - loss: 0.0783 - acc: 1.0000 - val_loss: 0.2081 - val_acc: 0.9500\n",
            "Epoch 485/500\n",
            "\n",
            "Epoch 00485: val_loss did not improve from 0.14984\n",
            "60/60 - 0s - loss: 0.0833 - acc: 0.9833 - val_loss: 0.2057 - val_acc: 0.9500\n",
            "Epoch 486/500\n",
            "\n",
            "Epoch 00486: val_loss did not improve from 0.14984\n",
            "60/60 - 0s - loss: 0.0650 - acc: 1.0000 - val_loss: 0.2109 - val_acc: 1.0000\n",
            "Epoch 487/500\n",
            "\n",
            "Epoch 00487: val_loss did not improve from 0.14984\n",
            "60/60 - 0s - loss: 0.0763 - acc: 1.0000 - val_loss: 0.2090 - val_acc: 0.9500\n",
            "Epoch 488/500\n",
            "\n",
            "Epoch 00488: val_loss did not improve from 0.14984\n",
            "60/60 - 0s - loss: 0.0611 - acc: 1.0000 - val_loss: 0.2018 - val_acc: 0.9500\n",
            "Epoch 489/500\n",
            "\n",
            "Epoch 00489: val_loss did not improve from 0.14984\n",
            "60/60 - 0s - loss: 0.0691 - acc: 1.0000 - val_loss: 0.2073 - val_acc: 0.9500\n",
            "Epoch 490/500\n",
            "\n",
            "Epoch 00490: val_loss did not improve from 0.14984\n",
            "60/60 - 0s - loss: 0.0777 - acc: 0.9667 - val_loss: 0.2104 - val_acc: 0.9500\n",
            "Epoch 491/500\n",
            "\n",
            "Epoch 00491: val_loss did not improve from 0.14984\n",
            "60/60 - 0s - loss: 0.0703 - acc: 1.0000 - val_loss: 0.2129 - val_acc: 0.9500\n",
            "Epoch 492/500\n",
            "\n",
            "Epoch 00492: val_loss did not improve from 0.14984\n",
            "60/60 - 0s - loss: 0.0519 - acc: 0.9833 - val_loss: 0.2131 - val_acc: 0.9500\n",
            "Epoch 493/500\n",
            "\n",
            "Epoch 00493: val_loss did not improve from 0.14984\n",
            "60/60 - 0s - loss: 0.0819 - acc: 0.9833 - val_loss: 0.2389 - val_acc: 0.9000\n",
            "Epoch 494/500\n",
            "\n",
            "Epoch 00494: val_loss did not improve from 0.14984\n",
            "60/60 - 0s - loss: 0.0538 - acc: 1.0000 - val_loss: 0.2623 - val_acc: 0.8500\n",
            "Epoch 495/500\n",
            "\n",
            "Epoch 00495: val_loss did not improve from 0.14984\n",
            "60/60 - 0s - loss: 0.0583 - acc: 1.0000 - val_loss: 0.2757 - val_acc: 0.8500\n",
            "Epoch 496/500\n",
            "\n",
            "Epoch 00496: val_loss did not improve from 0.14984\n",
            "60/60 - 0s - loss: 0.1038 - acc: 0.9667 - val_loss: 0.2631 - val_acc: 0.8500\n",
            "Epoch 497/500\n",
            "\n",
            "Epoch 00497: val_loss did not improve from 0.14984\n",
            "60/60 - 0s - loss: 0.0791 - acc: 1.0000 - val_loss: 0.2544 - val_acc: 0.8500\n",
            "Epoch 498/500\n",
            "\n",
            "Epoch 00498: val_loss did not improve from 0.14984\n",
            "60/60 - 0s - loss: 0.0942 - acc: 0.9833 - val_loss: 0.2539 - val_acc: 0.8500\n",
            "Epoch 499/500\n",
            "\n",
            "Epoch 00499: val_loss did not improve from 0.14984\n",
            "60/60 - 0s - loss: 0.1124 - acc: 0.9333 - val_loss: 0.2425 - val_acc: 0.9000\n",
            "Epoch 500/500\n",
            "\n",
            "Epoch 00500: val_loss did not improve from 0.14984\n",
            "60/60 - 0s - loss: 0.1398 - acc: 0.9333 - val_loss: 0.2488 - val_acc: 0.9000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "# Evaluate on test data\n",
            "40/40 [==============================] - 0s 2ms/sample - loss: 0.7097 - acc: 0.6750\n",
            "test loss, test acc: [0.7097222997341305, 0.675]\n",
            "EEG_Deep/Data2A/parsed_P02T.mat\n",
            "Filtering of Training Data Finished\n",
            "EEG_Deep/Data2A/parsed_P02E.mat\n",
            "Filtering of Testing Data Finished\n",
            "(1, 2)\n",
            "Finding labels in training data\n",
            "Finding labels in testing data\n",
            "[1 1 2 1 1 1 1 2 2 2 2 2 1 2 2 1 1 1 1 2]\n",
            "(60, 12, 1536)\n",
            "(60,)\n",
            "(20, 12, 1536)\n",
            "(20,)\n",
            "[[1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]]\n",
            "X_train shape: (60, 1, 12, 1536)\n",
            "60 train samples\n",
            "20 val samples\n",
            "(40, 12, 1536)\n",
            "(40, 1)\n",
            "X_train shape: (40, 1, 12, 1536)\n",
            "40 train samples\n",
            "Train on 60 samples, validate on 20 samples\n",
            "Epoch 1/500\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.68869, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 1s - loss: 0.6977 - acc: 0.5000 - val_loss: 0.6887 - val_acc: 0.5000\n",
            "Epoch 2/500\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.68869\n",
            "60/60 - 0s - loss: 0.6681 - acc: 0.6667 - val_loss: 0.6887 - val_acc: 0.5000\n",
            "Epoch 3/500\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.68869\n",
            "60/60 - 0s - loss: 0.6415 - acc: 0.7333 - val_loss: 0.6890 - val_acc: 0.5000\n",
            "Epoch 4/500\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.68869\n",
            "60/60 - 0s - loss: 0.6111 - acc: 0.8500 - val_loss: 0.6890 - val_acc: 0.4500\n",
            "Epoch 5/500\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.68869\n",
            "60/60 - 0s - loss: 0.6110 - acc: 0.7500 - val_loss: 0.6889 - val_acc: 0.5000\n",
            "Epoch 6/500\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.68869\n",
            "60/60 - 0s - loss: 0.5833 - acc: 0.8500 - val_loss: 0.6888 - val_acc: 0.5000\n",
            "Epoch 7/500\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.68869 to 0.68820, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5773 - acc: 0.8167 - val_loss: 0.6882 - val_acc: 0.5000\n",
            "Epoch 8/500\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.68820 to 0.68676, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5606 - acc: 0.8500 - val_loss: 0.6868 - val_acc: 0.5000\n",
            "Epoch 9/500\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.68676 to 0.68458, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5573 - acc: 0.8500 - val_loss: 0.6846 - val_acc: 0.5000\n",
            "Epoch 10/500\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.68458 to 0.68295, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5289 - acc: 0.8833 - val_loss: 0.6830 - val_acc: 0.5000\n",
            "Epoch 11/500\n",
            "\n",
            "Epoch 00011: val_loss improved from 0.68295 to 0.68116, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5139 - acc: 0.9000 - val_loss: 0.6812 - val_acc: 0.5000\n",
            "Epoch 12/500\n",
            "\n",
            "Epoch 00012: val_loss improved from 0.68116 to 0.67806, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5118 - acc: 0.9167 - val_loss: 0.6781 - val_acc: 0.6000\n",
            "Epoch 13/500\n",
            "\n",
            "Epoch 00013: val_loss improved from 0.67806 to 0.67561, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5114 - acc: 0.8333 - val_loss: 0.6756 - val_acc: 0.6000\n",
            "Epoch 14/500\n",
            "\n",
            "Epoch 00014: val_loss improved from 0.67561 to 0.67326, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4788 - acc: 0.8833 - val_loss: 0.6733 - val_acc: 0.6000\n",
            "Epoch 15/500\n",
            "\n",
            "Epoch 00015: val_loss improved from 0.67326 to 0.67087, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4845 - acc: 0.8667 - val_loss: 0.6709 - val_acc: 0.5500\n",
            "Epoch 16/500\n",
            "\n",
            "Epoch 00016: val_loss improved from 0.67087 to 0.66857, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4674 - acc: 0.9500 - val_loss: 0.6686 - val_acc: 0.6500\n",
            "Epoch 17/500\n",
            "\n",
            "Epoch 00017: val_loss improved from 0.66857 to 0.66574, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4546 - acc: 0.8500 - val_loss: 0.6657 - val_acc: 0.6000\n",
            "Epoch 18/500\n",
            "\n",
            "Epoch 00018: val_loss improved from 0.66574 to 0.66429, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4522 - acc: 0.9000 - val_loss: 0.6643 - val_acc: 0.5500\n",
            "Epoch 19/500\n",
            "\n",
            "Epoch 00019: val_loss improved from 0.66429 to 0.66315, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4324 - acc: 0.9500 - val_loss: 0.6631 - val_acc: 0.5500\n",
            "Epoch 20/500\n",
            "\n",
            "Epoch 00020: val_loss improved from 0.66315 to 0.66140, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4484 - acc: 0.8500 - val_loss: 0.6614 - val_acc: 0.6000\n",
            "Epoch 21/500\n",
            "\n",
            "Epoch 00021: val_loss improved from 0.66140 to 0.65812, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4182 - acc: 0.9000 - val_loss: 0.6581 - val_acc: 0.6000\n",
            "Epoch 22/500\n",
            "\n",
            "Epoch 00022: val_loss improved from 0.65812 to 0.65567, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4182 - acc: 0.8667 - val_loss: 0.6557 - val_acc: 0.6000\n",
            "Epoch 23/500\n",
            "\n",
            "Epoch 00023: val_loss improved from 0.65567 to 0.65516, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4142 - acc: 0.9167 - val_loss: 0.6552 - val_acc: 0.6000\n",
            "Epoch 24/500\n",
            "\n",
            "Epoch 00024: val_loss improved from 0.65516 to 0.65263, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4074 - acc: 0.9333 - val_loss: 0.6526 - val_acc: 0.6000\n",
            "Epoch 25/500\n",
            "\n",
            "Epoch 00025: val_loss improved from 0.65263 to 0.65149, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3888 - acc: 0.9500 - val_loss: 0.6515 - val_acc: 0.6000\n",
            "Epoch 26/500\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.65149\n",
            "60/60 - 0s - loss: 0.3823 - acc: 0.9333 - val_loss: 0.6516 - val_acc: 0.6000\n",
            "Epoch 27/500\n",
            "\n",
            "Epoch 00027: val_loss improved from 0.65149 to 0.64908, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3867 - acc: 0.9167 - val_loss: 0.6491 - val_acc: 0.6000\n",
            "Epoch 28/500\n",
            "\n",
            "Epoch 00028: val_loss improved from 0.64908 to 0.64738, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3584 - acc: 0.9667 - val_loss: 0.6474 - val_acc: 0.6000\n",
            "Epoch 29/500\n",
            "\n",
            "Epoch 00029: val_loss improved from 0.64738 to 0.64546, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3775 - acc: 0.9500 - val_loss: 0.6455 - val_acc: 0.6000\n",
            "Epoch 30/500\n",
            "\n",
            "Epoch 00030: val_loss improved from 0.64546 to 0.64519, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3673 - acc: 0.9500 - val_loss: 0.6452 - val_acc: 0.6000\n",
            "Epoch 31/500\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.64519\n",
            "60/60 - 0s - loss: 0.3312 - acc: 0.9667 - val_loss: 0.6453 - val_acc: 0.6500\n",
            "Epoch 32/500\n",
            "\n",
            "Epoch 00032: val_loss improved from 0.64519 to 0.64251, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3572 - acc: 0.9167 - val_loss: 0.6425 - val_acc: 0.6500\n",
            "Epoch 33/500\n",
            "\n",
            "Epoch 00033: val_loss improved from 0.64251 to 0.64182, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3461 - acc: 0.9500 - val_loss: 0.6418 - val_acc: 0.6500\n",
            "Epoch 34/500\n",
            "\n",
            "Epoch 00034: val_loss improved from 0.64182 to 0.64050, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3401 - acc: 0.9833 - val_loss: 0.6405 - val_acc: 0.6500\n",
            "Epoch 35/500\n",
            "\n",
            "Epoch 00035: val_loss improved from 0.64050 to 0.63830, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3341 - acc: 0.9500 - val_loss: 0.6383 - val_acc: 0.6500\n",
            "Epoch 36/500\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.63830\n",
            "60/60 - 0s - loss: 0.3298 - acc: 0.9667 - val_loss: 0.6421 - val_acc: 0.6500\n",
            "Epoch 37/500\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.63830\n",
            "60/60 - 0s - loss: 0.3057 - acc: 0.9833 - val_loss: 0.6437 - val_acc: 0.7000\n",
            "Epoch 38/500\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.63830\n",
            "60/60 - 0s - loss: 0.3364 - acc: 0.9500 - val_loss: 0.6453 - val_acc: 0.6500\n",
            "Epoch 39/500\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.63830\n",
            "60/60 - 0s - loss: 0.2969 - acc: 0.9833 - val_loss: 0.6415 - val_acc: 0.7000\n",
            "Epoch 40/500\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.63830\n",
            "60/60 - 0s - loss: 0.2898 - acc: 1.0000 - val_loss: 0.6413 - val_acc: 0.7000\n",
            "Epoch 41/500\n",
            "\n",
            "Epoch 00041: val_loss improved from 0.63830 to 0.63754, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2799 - acc: 0.9833 - val_loss: 0.6375 - val_acc: 0.7000\n",
            "Epoch 42/500\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.63754\n",
            "60/60 - 0s - loss: 0.3022 - acc: 0.9833 - val_loss: 0.6376 - val_acc: 0.7000\n",
            "Epoch 43/500\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.63754\n",
            "60/60 - 0s - loss: 0.3225 - acc: 0.9500 - val_loss: 0.6446 - val_acc: 0.6500\n",
            "Epoch 44/500\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.63754\n",
            "60/60 - 0s - loss: 0.2713 - acc: 0.9667 - val_loss: 0.6510 - val_acc: 0.6000\n",
            "Epoch 45/500\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.63754\n",
            "60/60 - 0s - loss: 0.3089 - acc: 0.9333 - val_loss: 0.6576 - val_acc: 0.6000\n",
            "Epoch 46/500\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.63754\n",
            "60/60 - 0s - loss: 0.2503 - acc: 0.9667 - val_loss: 0.6646 - val_acc: 0.6000\n",
            "Epoch 47/500\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.63754\n",
            "60/60 - 0s - loss: 0.2567 - acc: 0.9500 - val_loss: 0.6700 - val_acc: 0.6000\n",
            "Epoch 48/500\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.63754\n",
            "60/60 - 0s - loss: 0.2967 - acc: 0.9500 - val_loss: 0.6726 - val_acc: 0.6000\n",
            "Epoch 49/500\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.63754\n",
            "60/60 - 0s - loss: 0.2420 - acc: 1.0000 - val_loss: 0.6580 - val_acc: 0.6500\n",
            "Epoch 50/500\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.63754\n",
            "60/60 - 0s - loss: 0.2490 - acc: 0.9667 - val_loss: 0.6520 - val_acc: 0.6500\n",
            "Epoch 51/500\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.63754\n",
            "60/60 - 0s - loss: 0.2465 - acc: 0.9833 - val_loss: 0.6551 - val_acc: 0.6500\n",
            "Epoch 52/500\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.63754\n",
            "60/60 - 0s - loss: 0.2672 - acc: 0.9333 - val_loss: 0.6545 - val_acc: 0.7000\n",
            "Epoch 53/500\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.63754\n",
            "60/60 - 0s - loss: 0.2533 - acc: 0.9667 - val_loss: 0.6531 - val_acc: 0.7500\n",
            "Epoch 54/500\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.63754\n",
            "60/60 - 0s - loss: 0.2478 - acc: 0.9833 - val_loss: 0.6504 - val_acc: 0.7500\n",
            "Epoch 55/500\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.63754\n",
            "60/60 - 0s - loss: 0.2602 - acc: 0.9333 - val_loss: 0.6442 - val_acc: 0.7500\n",
            "Epoch 56/500\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.63754\n",
            "60/60 - 0s - loss: 0.2018 - acc: 1.0000 - val_loss: 0.6457 - val_acc: 0.7500\n",
            "Epoch 57/500\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.63754\n",
            "60/60 - 0s - loss: 0.2171 - acc: 0.9833 - val_loss: 0.6532 - val_acc: 0.7500\n",
            "Epoch 58/500\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.63754\n",
            "60/60 - 0s - loss: 0.2019 - acc: 0.9833 - val_loss: 0.6465 - val_acc: 0.7500\n",
            "Epoch 59/500\n",
            "\n",
            "Epoch 00059: val_loss improved from 0.63754 to 0.63104, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2045 - acc: 0.9833 - val_loss: 0.6310 - val_acc: 0.7500\n",
            "Epoch 60/500\n",
            "\n",
            "Epoch 00060: val_loss improved from 0.63104 to 0.61828, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2348 - acc: 0.9500 - val_loss: 0.6183 - val_acc: 0.7500\n",
            "Epoch 61/500\n",
            "\n",
            "Epoch 00061: val_loss improved from 0.61828 to 0.61124, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2168 - acc: 0.9333 - val_loss: 0.6112 - val_acc: 0.7500\n",
            "Epoch 62/500\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.61124\n",
            "60/60 - 0s - loss: 0.1771 - acc: 0.9833 - val_loss: 0.6114 - val_acc: 0.7500\n",
            "Epoch 63/500\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.61124\n",
            "60/60 - 0s - loss: 0.2048 - acc: 0.9667 - val_loss: 0.6113 - val_acc: 0.7500\n",
            "Epoch 64/500\n",
            "\n",
            "Epoch 00064: val_loss improved from 0.61124 to 0.60334, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1748 - acc: 1.0000 - val_loss: 0.6033 - val_acc: 0.7500\n",
            "Epoch 65/500\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.60334\n",
            "60/60 - 0s - loss: 0.2601 - acc: 0.9167 - val_loss: 0.6141 - val_acc: 0.7500\n",
            "Epoch 66/500\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.60334\n",
            "60/60 - 0s - loss: 0.2523 - acc: 0.9000 - val_loss: 0.6091 - val_acc: 0.7500\n",
            "Epoch 67/500\n",
            "\n",
            "Epoch 00067: val_loss improved from 0.60334 to 0.60292, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2207 - acc: 0.9667 - val_loss: 0.6029 - val_acc: 0.7500\n",
            "Epoch 68/500\n",
            "\n",
            "Epoch 00068: val_loss improved from 0.60292 to 0.60270, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2119 - acc: 1.0000 - val_loss: 0.6027 - val_acc: 0.7500\n",
            "Epoch 69/500\n",
            "\n",
            "Epoch 00069: val_loss improved from 0.60270 to 0.59358, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1877 - acc: 0.9667 - val_loss: 0.5936 - val_acc: 0.8000\n",
            "Epoch 70/500\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.59358\n",
            "60/60 - 0s - loss: 0.1978 - acc: 0.9833 - val_loss: 0.5973 - val_acc: 0.8000\n",
            "Epoch 71/500\n",
            "\n",
            "Epoch 00071: val_loss improved from 0.59358 to 0.58904, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2243 - acc: 0.9667 - val_loss: 0.5890 - val_acc: 0.8000\n",
            "Epoch 72/500\n",
            "\n",
            "Epoch 00072: val_loss improved from 0.58904 to 0.56218, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1918 - acc: 0.9667 - val_loss: 0.5622 - val_acc: 0.8500\n",
            "Epoch 73/500\n",
            "\n",
            "Epoch 00073: val_loss improved from 0.56218 to 0.53252, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1849 - acc: 0.9667 - val_loss: 0.5325 - val_acc: 0.8500\n",
            "Epoch 74/500\n",
            "\n",
            "Epoch 00074: val_loss improved from 0.53252 to 0.52523, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2050 - acc: 0.9500 - val_loss: 0.5252 - val_acc: 0.8500\n",
            "Epoch 75/500\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.52523\n",
            "60/60 - 0s - loss: 0.2218 - acc: 0.9667 - val_loss: 0.5338 - val_acc: 0.8500\n",
            "Epoch 76/500\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.52523\n",
            "60/60 - 0s - loss: 0.1788 - acc: 1.0000 - val_loss: 0.5367 - val_acc: 0.8500\n",
            "Epoch 77/500\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.52523\n",
            "60/60 - 0s - loss: 0.1628 - acc: 1.0000 - val_loss: 0.5339 - val_acc: 0.8500\n",
            "Epoch 78/500\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.52523\n",
            "60/60 - 0s - loss: 0.1944 - acc: 0.9667 - val_loss: 0.5318 - val_acc: 0.8500\n",
            "Epoch 79/500\n",
            "\n",
            "Epoch 00079: val_loss improved from 0.52523 to 0.52331, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1941 - acc: 0.9667 - val_loss: 0.5233 - val_acc: 0.8500\n",
            "Epoch 80/500\n",
            "\n",
            "Epoch 00080: val_loss improved from 0.52331 to 0.51920, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1652 - acc: 0.9667 - val_loss: 0.5192 - val_acc: 0.8500\n",
            "Epoch 81/500\n",
            "\n",
            "Epoch 00081: val_loss improved from 0.51920 to 0.51790, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1668 - acc: 0.9667 - val_loss: 0.5179 - val_acc: 0.8500\n",
            "Epoch 82/500\n",
            "\n",
            "Epoch 00082: val_loss improved from 0.51790 to 0.51336, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1557 - acc: 0.9833 - val_loss: 0.5134 - val_acc: 0.8000\n",
            "Epoch 83/500\n",
            "\n",
            "Epoch 00083: val_loss improved from 0.51336 to 0.50901, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1764 - acc: 0.9833 - val_loss: 0.5090 - val_acc: 0.8000\n",
            "Epoch 84/500\n",
            "\n",
            "Epoch 00084: val_loss improved from 0.50901 to 0.49236, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1392 - acc: 0.9667 - val_loss: 0.4924 - val_acc: 0.8000\n",
            "Epoch 85/500\n",
            "\n",
            "Epoch 00085: val_loss improved from 0.49236 to 0.47982, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1489 - acc: 1.0000 - val_loss: 0.4798 - val_acc: 0.8000\n",
            "Epoch 86/500\n",
            "\n",
            "Epoch 00086: val_loss improved from 0.47982 to 0.47243, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1359 - acc: 0.9833 - val_loss: 0.4724 - val_acc: 0.7500\n",
            "Epoch 87/500\n",
            "\n",
            "Epoch 00087: val_loss improved from 0.47243 to 0.45767, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1944 - acc: 0.9667 - val_loss: 0.4577 - val_acc: 0.7500\n",
            "Epoch 88/500\n",
            "\n",
            "Epoch 00088: val_loss improved from 0.45767 to 0.45165, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1517 - acc: 0.9833 - val_loss: 0.4516 - val_acc: 0.7500\n",
            "Epoch 89/500\n",
            "\n",
            "Epoch 00089: val_loss improved from 0.45165 to 0.45161, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1281 - acc: 1.0000 - val_loss: 0.4516 - val_acc: 0.7500\n",
            "Epoch 90/500\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.1711 - acc: 0.9500 - val_loss: 0.4551 - val_acc: 0.7500\n",
            "Epoch 91/500\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.1591 - acc: 0.9833 - val_loss: 0.4554 - val_acc: 0.7000\n",
            "Epoch 92/500\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.1326 - acc: 0.9833 - val_loss: 0.4572 - val_acc: 0.7000\n",
            "Epoch 93/500\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.1137 - acc: 0.9833 - val_loss: 0.4579 - val_acc: 0.7000\n",
            "Epoch 94/500\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.1496 - acc: 0.9833 - val_loss: 0.4596 - val_acc: 0.7000\n",
            "Epoch 95/500\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.1065 - acc: 1.0000 - val_loss: 0.4631 - val_acc: 0.7500\n",
            "Epoch 96/500\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.1572 - acc: 0.9500 - val_loss: 0.4684 - val_acc: 0.7500\n",
            "Epoch 97/500\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.1661 - acc: 0.9667 - val_loss: 0.4672 - val_acc: 0.7500\n",
            "Epoch 98/500\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.1643 - acc: 0.9833 - val_loss: 0.4694 - val_acc: 0.7500\n",
            "Epoch 99/500\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.1606 - acc: 0.9833 - val_loss: 0.4742 - val_acc: 0.7500\n",
            "Epoch 100/500\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.1241 - acc: 0.9833 - val_loss: 0.4692 - val_acc: 0.7500\n",
            "Epoch 101/500\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.1351 - acc: 0.9833 - val_loss: 0.4650 - val_acc: 0.7500\n",
            "Epoch 102/500\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.1474 - acc: 0.9667 - val_loss: 0.4605 - val_acc: 0.7500\n",
            "Epoch 103/500\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.1074 - acc: 0.9833 - val_loss: 0.4628 - val_acc: 0.7500\n",
            "Epoch 104/500\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.1567 - acc: 0.9500 - val_loss: 0.4673 - val_acc: 0.7500\n",
            "Epoch 105/500\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.1767 - acc: 0.9667 - val_loss: 0.4780 - val_acc: 0.8000\n",
            "Epoch 106/500\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.1741 - acc: 0.9667 - val_loss: 0.4965 - val_acc: 0.8000\n",
            "Epoch 107/500\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.1349 - acc: 0.9833 - val_loss: 0.5121 - val_acc: 0.8000\n",
            "Epoch 108/500\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.1395 - acc: 1.0000 - val_loss: 0.5185 - val_acc: 0.8000\n",
            "Epoch 109/500\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.1481 - acc: 0.9667 - val_loss: 0.5039 - val_acc: 0.8000\n",
            "Epoch 110/500\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.1520 - acc: 0.9667 - val_loss: 0.4834 - val_acc: 0.7500\n",
            "Epoch 111/500\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.1490 - acc: 0.9333 - val_loss: 0.4741 - val_acc: 0.7500\n",
            "Epoch 112/500\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.1726 - acc: 0.9500 - val_loss: 0.4660 - val_acc: 0.7500\n",
            "Epoch 113/500\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.1492 - acc: 0.9833 - val_loss: 0.4691 - val_acc: 0.7500\n",
            "Epoch 114/500\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.1313 - acc: 0.9833 - val_loss: 0.4727 - val_acc: 0.7500\n",
            "Epoch 115/500\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.2013 - acc: 0.9333 - val_loss: 0.4740 - val_acc: 0.7500\n",
            "Epoch 116/500\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.1042 - acc: 0.9833 - val_loss: 0.4734 - val_acc: 0.7500\n",
            "Epoch 117/500\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.1271 - acc: 0.9667 - val_loss: 0.4718 - val_acc: 0.7500\n",
            "Epoch 118/500\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.1168 - acc: 1.0000 - val_loss: 0.4704 - val_acc: 0.7500\n",
            "Epoch 119/500\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0976 - acc: 1.0000 - val_loss: 0.4725 - val_acc: 0.8000\n",
            "Epoch 120/500\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.1034 - acc: 1.0000 - val_loss: 0.4643 - val_acc: 0.7500\n",
            "Epoch 121/500\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.1223 - acc: 1.0000 - val_loss: 0.4608 - val_acc: 0.7500\n",
            "Epoch 122/500\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.1567 - acc: 0.9667 - val_loss: 0.4735 - val_acc: 0.7500\n",
            "Epoch 123/500\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.1129 - acc: 1.0000 - val_loss: 0.4772 - val_acc: 0.7500\n",
            "Epoch 124/500\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.1284 - acc: 0.9833 - val_loss: 0.4671 - val_acc: 0.7500\n",
            "Epoch 125/500\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.1264 - acc: 0.9667 - val_loss: 0.4652 - val_acc: 0.7500\n",
            "Epoch 126/500\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.1213 - acc: 0.9833 - val_loss: 0.4645 - val_acc: 0.7500\n",
            "Epoch 127/500\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.1826 - acc: 0.9500 - val_loss: 0.4613 - val_acc: 0.7500\n",
            "Epoch 128/500\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.1623 - acc: 0.9667 - val_loss: 0.4646 - val_acc: 0.7500\n",
            "Epoch 129/500\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.1070 - acc: 1.0000 - val_loss: 0.4788 - val_acc: 0.7500\n",
            "Epoch 130/500\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.1144 - acc: 0.9833 - val_loss: 0.4975 - val_acc: 0.7500\n",
            "Epoch 131/500\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.1066 - acc: 1.0000 - val_loss: 0.5067 - val_acc: 0.7500\n",
            "Epoch 132/500\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.1696 - acc: 0.9667 - val_loss: 0.5010 - val_acc: 0.7500\n",
            "Epoch 133/500\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.1244 - acc: 0.9667 - val_loss: 0.4985 - val_acc: 0.7500\n",
            "Epoch 134/500\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0972 - acc: 1.0000 - val_loss: 0.4942 - val_acc: 0.7500\n",
            "Epoch 135/500\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.1048 - acc: 1.0000 - val_loss: 0.4932 - val_acc: 0.7500\n",
            "Epoch 136/500\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.1062 - acc: 1.0000 - val_loss: 0.4931 - val_acc: 0.7500\n",
            "Epoch 137/500\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.1044 - acc: 1.0000 - val_loss: 0.4935 - val_acc: 0.7500\n",
            "Epoch 138/500\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0735 - acc: 1.0000 - val_loss: 0.4925 - val_acc: 0.7500\n",
            "Epoch 139/500\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0856 - acc: 0.9833 - val_loss: 0.4943 - val_acc: 0.7500\n",
            "Epoch 140/500\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0935 - acc: 0.9833 - val_loss: 0.4987 - val_acc: 0.7500\n",
            "Epoch 141/500\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0799 - acc: 1.0000 - val_loss: 0.5000 - val_acc: 0.7500\n",
            "Epoch 142/500\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0838 - acc: 0.9833 - val_loss: 0.5011 - val_acc: 0.7500\n",
            "Epoch 143/500\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0973 - acc: 1.0000 - val_loss: 0.5004 - val_acc: 0.8000\n",
            "Epoch 144/500\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0854 - acc: 0.9833 - val_loss: 0.4929 - val_acc: 0.8500\n",
            "Epoch 145/500\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0773 - acc: 1.0000 - val_loss: 0.4856 - val_acc: 0.8500\n",
            "Epoch 146/500\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.1169 - acc: 1.0000 - val_loss: 0.4832 - val_acc: 0.8000\n",
            "Epoch 147/500\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.1031 - acc: 0.9833 - val_loss: 0.4928 - val_acc: 0.7500\n",
            "Epoch 148/500\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0904 - acc: 0.9833 - val_loss: 0.5017 - val_acc: 0.7500\n",
            "Epoch 149/500\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0962 - acc: 0.9833 - val_loss: 0.4977 - val_acc: 0.8000\n",
            "Epoch 150/500\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.1133 - acc: 0.9833 - val_loss: 0.5125 - val_acc: 0.8000\n",
            "Epoch 151/500\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0924 - acc: 0.9833 - val_loss: 0.5471 - val_acc: 0.7500\n",
            "Epoch 152/500\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0833 - acc: 0.9500 - val_loss: 0.5803 - val_acc: 0.7500\n",
            "Epoch 153/500\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0897 - acc: 1.0000 - val_loss: 0.6075 - val_acc: 0.7500\n",
            "Epoch 154/500\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0742 - acc: 1.0000 - val_loss: 0.5856 - val_acc: 0.7500\n",
            "Epoch 155/500\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0827 - acc: 0.9833 - val_loss: 0.5446 - val_acc: 0.7500\n",
            "Epoch 156/500\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0913 - acc: 1.0000 - val_loss: 0.5208 - val_acc: 0.8000\n",
            "Epoch 157/500\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.1105 - acc: 0.9667 - val_loss: 0.5139 - val_acc: 0.8000\n",
            "Epoch 158/500\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.1321 - acc: 0.9833 - val_loss: 0.5480 - val_acc: 0.7500\n",
            "Epoch 159/500\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.1017 - acc: 1.0000 - val_loss: 0.5493 - val_acc: 0.7500\n",
            "Epoch 160/500\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.1009 - acc: 0.9833 - val_loss: 0.5494 - val_acc: 0.7500\n",
            "Epoch 161/500\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0879 - acc: 1.0000 - val_loss: 0.5431 - val_acc: 0.7500\n",
            "Epoch 162/500\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.1162 - acc: 1.0000 - val_loss: 0.5327 - val_acc: 0.7500\n",
            "Epoch 163/500\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0997 - acc: 0.9833 - val_loss: 0.5305 - val_acc: 0.8000\n",
            "Epoch 164/500\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0889 - acc: 1.0000 - val_loss: 0.5223 - val_acc: 0.8000\n",
            "Epoch 165/500\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0792 - acc: 0.9833 - val_loss: 0.5222 - val_acc: 0.8000\n",
            "Epoch 166/500\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.1480 - acc: 0.9500 - val_loss: 0.5242 - val_acc: 0.8000\n",
            "Epoch 167/500\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0898 - acc: 0.9833 - val_loss: 0.5198 - val_acc: 0.8000\n",
            "Epoch 168/500\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.1024 - acc: 0.9833 - val_loss: 0.5264 - val_acc: 0.8000\n",
            "Epoch 169/500\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0780 - acc: 1.0000 - val_loss: 0.5285 - val_acc: 0.7500\n",
            "Epoch 170/500\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.1008 - acc: 0.9833 - val_loss: 0.5337 - val_acc: 0.7500\n",
            "Epoch 171/500\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0688 - acc: 1.0000 - val_loss: 0.5479 - val_acc: 0.7500\n",
            "Epoch 172/500\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0682 - acc: 1.0000 - val_loss: 0.5631 - val_acc: 0.7500\n",
            "Epoch 173/500\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0819 - acc: 1.0000 - val_loss: 0.5859 - val_acc: 0.7500\n",
            "Epoch 174/500\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0579 - acc: 1.0000 - val_loss: 0.5983 - val_acc: 0.7000\n",
            "Epoch 175/500\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0801 - acc: 0.9833 - val_loss: 0.6416 - val_acc: 0.7000\n",
            "Epoch 176/500\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0888 - acc: 1.0000 - val_loss: 0.6395 - val_acc: 0.7000\n",
            "Epoch 177/500\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0550 - acc: 1.0000 - val_loss: 0.6333 - val_acc: 0.7000\n",
            "Epoch 178/500\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.1302 - acc: 0.9500 - val_loss: 0.6383 - val_acc: 0.7000\n",
            "Epoch 179/500\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0814 - acc: 0.9833 - val_loss: 0.5857 - val_acc: 0.7500\n",
            "Epoch 180/500\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.1171 - acc: 0.9500 - val_loss: 0.5856 - val_acc: 0.7500\n",
            "Epoch 181/500\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.1271 - acc: 0.9833 - val_loss: 0.6038 - val_acc: 0.7500\n",
            "Epoch 182/500\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0577 - acc: 1.0000 - val_loss: 0.6220 - val_acc: 0.7000\n",
            "Epoch 183/500\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0692 - acc: 1.0000 - val_loss: 0.6379 - val_acc: 0.7000\n",
            "Epoch 184/500\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0701 - acc: 1.0000 - val_loss: 0.6550 - val_acc: 0.7000\n",
            "Epoch 185/500\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.1087 - acc: 0.9500 - val_loss: 0.6585 - val_acc: 0.7000\n",
            "Epoch 186/500\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0697 - acc: 1.0000 - val_loss: 0.6921 - val_acc: 0.6500\n",
            "Epoch 187/500\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.1010 - acc: 0.9667 - val_loss: 0.6726 - val_acc: 0.7000\n",
            "Epoch 188/500\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0680 - acc: 1.0000 - val_loss: 0.6106 - val_acc: 0.7000\n",
            "Epoch 189/500\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0626 - acc: 1.0000 - val_loss: 0.5816 - val_acc: 0.8000\n",
            "Epoch 190/500\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0641 - acc: 1.0000 - val_loss: 0.5670 - val_acc: 0.8000\n",
            "Epoch 191/500\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0761 - acc: 0.9833 - val_loss: 0.5809 - val_acc: 0.8000\n",
            "Epoch 192/500\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0837 - acc: 0.9833 - val_loss: 0.5939 - val_acc: 0.7500\n",
            "Epoch 193/500\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0637 - acc: 1.0000 - val_loss: 0.6302 - val_acc: 0.7500\n",
            "Epoch 194/500\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0607 - acc: 1.0000 - val_loss: 0.6477 - val_acc: 0.7000\n",
            "Epoch 195/500\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0868 - acc: 0.9833 - val_loss: 0.6503 - val_acc: 0.7000\n",
            "Epoch 196/500\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0577 - acc: 1.0000 - val_loss: 0.6513 - val_acc: 0.7000\n",
            "Epoch 197/500\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0604 - acc: 0.9833 - val_loss: 0.6604 - val_acc: 0.7000\n",
            "Epoch 198/500\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0746 - acc: 1.0000 - val_loss: 0.6550 - val_acc: 0.7000\n",
            "Epoch 199/500\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0529 - acc: 1.0000 - val_loss: 0.6313 - val_acc: 0.7000\n",
            "Epoch 200/500\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0808 - acc: 0.9833 - val_loss: 0.6108 - val_acc: 0.7500\n",
            "Epoch 201/500\n",
            "\n",
            "Epoch 00201: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0490 - acc: 1.0000 - val_loss: 0.6035 - val_acc: 0.7500\n",
            "Epoch 202/500\n",
            "\n",
            "Epoch 00202: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0686 - acc: 1.0000 - val_loss: 0.5939 - val_acc: 0.8000\n",
            "Epoch 203/500\n",
            "\n",
            "Epoch 00203: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0517 - acc: 1.0000 - val_loss: 0.5901 - val_acc: 0.8000\n",
            "Epoch 204/500\n",
            "\n",
            "Epoch 00204: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0417 - acc: 1.0000 - val_loss: 0.5935 - val_acc: 0.7500\n",
            "Epoch 205/500\n",
            "\n",
            "Epoch 00205: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0531 - acc: 1.0000 - val_loss: 0.6057 - val_acc: 0.7500\n",
            "Epoch 206/500\n",
            "\n",
            "Epoch 00206: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0694 - acc: 0.9833 - val_loss: 0.6043 - val_acc: 0.7500\n",
            "Epoch 207/500\n",
            "\n",
            "Epoch 00207: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0665 - acc: 0.9833 - val_loss: 0.6116 - val_acc: 0.7500\n",
            "Epoch 208/500\n",
            "\n",
            "Epoch 00208: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0462 - acc: 1.0000 - val_loss: 0.6360 - val_acc: 0.7500\n",
            "Epoch 209/500\n",
            "\n",
            "Epoch 00209: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0462 - acc: 1.0000 - val_loss: 0.6560 - val_acc: 0.7500\n",
            "Epoch 210/500\n",
            "\n",
            "Epoch 00210: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0749 - acc: 1.0000 - val_loss: 0.6297 - val_acc: 0.7500\n",
            "Epoch 211/500\n",
            "\n",
            "Epoch 00211: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0613 - acc: 1.0000 - val_loss: 0.6018 - val_acc: 0.7500\n",
            "Epoch 212/500\n",
            "\n",
            "Epoch 00212: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0963 - acc: 0.9667 - val_loss: 0.5702 - val_acc: 0.8500\n",
            "Epoch 213/500\n",
            "\n",
            "Epoch 00213: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0889 - acc: 0.9667 - val_loss: 0.5688 - val_acc: 0.8000\n",
            "Epoch 214/500\n",
            "\n",
            "Epoch 00214: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0783 - acc: 0.9833 - val_loss: 0.5607 - val_acc: 0.7500\n",
            "Epoch 215/500\n",
            "\n",
            "Epoch 00215: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0414 - acc: 1.0000 - val_loss: 0.5703 - val_acc: 0.7500\n",
            "Epoch 216/500\n",
            "\n",
            "Epoch 00216: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0550 - acc: 0.9833 - val_loss: 0.5948 - val_acc: 0.7500\n",
            "Epoch 217/500\n",
            "\n",
            "Epoch 00217: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0641 - acc: 0.9833 - val_loss: 0.5970 - val_acc: 0.7500\n",
            "Epoch 218/500\n",
            "\n",
            "Epoch 00218: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0678 - acc: 1.0000 - val_loss: 0.6038 - val_acc: 0.7500\n",
            "Epoch 219/500\n",
            "\n",
            "Epoch 00219: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0746 - acc: 1.0000 - val_loss: 0.5829 - val_acc: 0.7500\n",
            "Epoch 220/500\n",
            "\n",
            "Epoch 00220: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0565 - acc: 1.0000 - val_loss: 0.5566 - val_acc: 0.8000\n",
            "Epoch 221/500\n",
            "\n",
            "Epoch 00221: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0495 - acc: 1.0000 - val_loss: 0.5524 - val_acc: 0.8500\n",
            "Epoch 222/500\n",
            "\n",
            "Epoch 00222: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0610 - acc: 1.0000 - val_loss: 0.5522 - val_acc: 0.8500\n",
            "Epoch 223/500\n",
            "\n",
            "Epoch 00223: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0446 - acc: 1.0000 - val_loss: 0.5527 - val_acc: 0.8500\n",
            "Epoch 224/500\n",
            "\n",
            "Epoch 00224: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0535 - acc: 1.0000 - val_loss: 0.5531 - val_acc: 0.8500\n",
            "Epoch 225/500\n",
            "\n",
            "Epoch 00225: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0455 - acc: 1.0000 - val_loss: 0.5535 - val_acc: 0.8000\n",
            "Epoch 226/500\n",
            "\n",
            "Epoch 00226: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.1060 - acc: 0.9833 - val_loss: 0.5487 - val_acc: 0.8000\n",
            "Epoch 227/500\n",
            "\n",
            "Epoch 00227: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0402 - acc: 1.0000 - val_loss: 0.5414 - val_acc: 0.8000\n",
            "Epoch 228/500\n",
            "\n",
            "Epoch 00228: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.1184 - acc: 0.9833 - val_loss: 0.5212 - val_acc: 0.8000\n",
            "Epoch 229/500\n",
            "\n",
            "Epoch 00229: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0935 - acc: 0.9667 - val_loss: 0.5056 - val_acc: 0.8000\n",
            "Epoch 230/500\n",
            "\n",
            "Epoch 00230: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0718 - acc: 0.9833 - val_loss: 0.4980 - val_acc: 0.8500\n",
            "Epoch 231/500\n",
            "\n",
            "Epoch 00231: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0376 - acc: 1.0000 - val_loss: 0.4989 - val_acc: 0.9000\n",
            "Epoch 232/500\n",
            "\n",
            "Epoch 00232: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0823 - acc: 0.9667 - val_loss: 0.5119 - val_acc: 0.8500\n",
            "Epoch 233/500\n",
            "\n",
            "Epoch 00233: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0411 - acc: 1.0000 - val_loss: 0.5552 - val_acc: 0.8000\n",
            "Epoch 234/500\n",
            "\n",
            "Epoch 00234: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0506 - acc: 1.0000 - val_loss: 0.5854 - val_acc: 0.8000\n",
            "Epoch 235/500\n",
            "\n",
            "Epoch 00235: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0507 - acc: 1.0000 - val_loss: 0.5837 - val_acc: 0.8000\n",
            "Epoch 236/500\n",
            "\n",
            "Epoch 00236: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0537 - acc: 1.0000 - val_loss: 0.5811 - val_acc: 0.8000\n",
            "Epoch 237/500\n",
            "\n",
            "Epoch 00237: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0568 - acc: 0.9833 - val_loss: 0.5636 - val_acc: 0.8000\n",
            "Epoch 238/500\n",
            "\n",
            "Epoch 00238: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0354 - acc: 1.0000 - val_loss: 0.5547 - val_acc: 0.8000\n",
            "Epoch 239/500\n",
            "\n",
            "Epoch 00239: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0344 - acc: 1.0000 - val_loss: 0.5511 - val_acc: 0.8000\n",
            "Epoch 240/500\n",
            "\n",
            "Epoch 00240: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0568 - acc: 1.0000 - val_loss: 0.5513 - val_acc: 0.8000\n",
            "Epoch 241/500\n",
            "\n",
            "Epoch 00241: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0439 - acc: 1.0000 - val_loss: 0.5506 - val_acc: 0.8500\n",
            "Epoch 242/500\n",
            "\n",
            "Epoch 00242: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0665 - acc: 1.0000 - val_loss: 0.5507 - val_acc: 0.8500\n",
            "Epoch 243/500\n",
            "\n",
            "Epoch 00243: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0637 - acc: 1.0000 - val_loss: 0.5423 - val_acc: 0.8500\n",
            "Epoch 244/500\n",
            "\n",
            "Epoch 00244: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0586 - acc: 1.0000 - val_loss: 0.5353 - val_acc: 0.8000\n",
            "Epoch 245/500\n",
            "\n",
            "Epoch 00245: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0413 - acc: 1.0000 - val_loss: 0.5343 - val_acc: 0.8000\n",
            "Epoch 246/500\n",
            "\n",
            "Epoch 00246: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0419 - acc: 1.0000 - val_loss: 0.5369 - val_acc: 0.8000\n",
            "Epoch 247/500\n",
            "\n",
            "Epoch 00247: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0352 - acc: 1.0000 - val_loss: 0.5361 - val_acc: 0.8000\n",
            "Epoch 248/500\n",
            "\n",
            "Epoch 00248: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0893 - acc: 0.9667 - val_loss: 0.5506 - val_acc: 0.8000\n",
            "Epoch 249/500\n",
            "\n",
            "Epoch 00249: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0463 - acc: 0.9833 - val_loss: 0.5445 - val_acc: 0.8000\n",
            "Epoch 250/500\n",
            "\n",
            "Epoch 00250: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0646 - acc: 0.9833 - val_loss: 0.5345 - val_acc: 0.9000\n",
            "Epoch 251/500\n",
            "\n",
            "Epoch 00251: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0424 - acc: 1.0000 - val_loss: 0.5396 - val_acc: 0.9000\n",
            "Epoch 252/500\n",
            "\n",
            "Epoch 00252: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.1023 - acc: 0.9833 - val_loss: 0.5386 - val_acc: 0.9000\n",
            "Epoch 253/500\n",
            "\n",
            "Epoch 00253: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0691 - acc: 0.9833 - val_loss: 0.5380 - val_acc: 0.8000\n",
            "Epoch 254/500\n",
            "\n",
            "Epoch 00254: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0740 - acc: 0.9833 - val_loss: 0.5524 - val_acc: 0.8000\n",
            "Epoch 255/500\n",
            "\n",
            "Epoch 00255: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0481 - acc: 1.0000 - val_loss: 0.5563 - val_acc: 0.7500\n",
            "Epoch 256/500\n",
            "\n",
            "Epoch 00256: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0473 - acc: 1.0000 - val_loss: 0.5520 - val_acc: 0.7500\n",
            "Epoch 257/500\n",
            "\n",
            "Epoch 00257: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0573 - acc: 0.9833 - val_loss: 0.5445 - val_acc: 0.7500\n",
            "Epoch 258/500\n",
            "\n",
            "Epoch 00258: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0566 - acc: 1.0000 - val_loss: 0.5592 - val_acc: 0.7500\n",
            "Epoch 259/500\n",
            "\n",
            "Epoch 00259: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0961 - acc: 0.9667 - val_loss: 0.5256 - val_acc: 0.8000\n",
            "Epoch 260/500\n",
            "\n",
            "Epoch 00260: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0587 - acc: 0.9833 - val_loss: 0.5191 - val_acc: 0.8500\n",
            "Epoch 261/500\n",
            "\n",
            "Epoch 00261: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0537 - acc: 0.9833 - val_loss: 0.5137 - val_acc: 0.8000\n",
            "Epoch 262/500\n",
            "\n",
            "Epoch 00262: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0392 - acc: 1.0000 - val_loss: 0.5065 - val_acc: 0.8500\n",
            "Epoch 263/500\n",
            "\n",
            "Epoch 00263: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0592 - acc: 1.0000 - val_loss: 0.5118 - val_acc: 0.8500\n",
            "Epoch 264/500\n",
            "\n",
            "Epoch 00264: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0707 - acc: 0.9833 - val_loss: 0.5123 - val_acc: 0.8000\n",
            "Epoch 265/500\n",
            "\n",
            "Epoch 00265: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0464 - acc: 1.0000 - val_loss: 0.5016 - val_acc: 0.8500\n",
            "Epoch 266/500\n",
            "\n",
            "Epoch 00266: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0431 - acc: 1.0000 - val_loss: 0.4868 - val_acc: 0.9000\n",
            "Epoch 267/500\n",
            "\n",
            "Epoch 00267: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0611 - acc: 1.0000 - val_loss: 0.4776 - val_acc: 0.8500\n",
            "Epoch 268/500\n",
            "\n",
            "Epoch 00268: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0408 - acc: 1.0000 - val_loss: 0.4789 - val_acc: 0.8500\n",
            "Epoch 269/500\n",
            "\n",
            "Epoch 00269: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0915 - acc: 0.9667 - val_loss: 0.4834 - val_acc: 0.8500\n",
            "Epoch 270/500\n",
            "\n",
            "Epoch 00270: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0796 - acc: 0.9833 - val_loss: 0.5001 - val_acc: 0.8000\n",
            "Epoch 271/500\n",
            "\n",
            "Epoch 00271: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0720 - acc: 0.9833 - val_loss: 0.5017 - val_acc: 0.8000\n",
            "Epoch 272/500\n",
            "\n",
            "Epoch 00272: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0633 - acc: 1.0000 - val_loss: 0.4978 - val_acc: 0.7500\n",
            "Epoch 273/500\n",
            "\n",
            "Epoch 00273: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0347 - acc: 1.0000 - val_loss: 0.4915 - val_acc: 0.7500\n",
            "Epoch 274/500\n",
            "\n",
            "Epoch 00274: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0582 - acc: 1.0000 - val_loss: 0.4775 - val_acc: 0.8000\n",
            "Epoch 275/500\n",
            "\n",
            "Epoch 00275: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0432 - acc: 1.0000 - val_loss: 0.4792 - val_acc: 0.9000\n",
            "Epoch 276/500\n",
            "\n",
            "Epoch 00276: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0840 - acc: 0.9833 - val_loss: 0.5003 - val_acc: 0.9000\n",
            "Epoch 277/500\n",
            "\n",
            "Epoch 00277: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0638 - acc: 0.9833 - val_loss: 0.5326 - val_acc: 0.9000\n",
            "Epoch 278/500\n",
            "\n",
            "Epoch 00278: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0729 - acc: 1.0000 - val_loss: 0.5561 - val_acc: 0.8500\n",
            "Epoch 279/500\n",
            "\n",
            "Epoch 00279: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0399 - acc: 1.0000 - val_loss: 0.5892 - val_acc: 0.8000\n",
            "Epoch 280/500\n",
            "\n",
            "Epoch 00280: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0874 - acc: 0.9667 - val_loss: 0.6157 - val_acc: 0.8000\n",
            "Epoch 281/500\n",
            "\n",
            "Epoch 00281: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0267 - acc: 1.0000 - val_loss: 0.5803 - val_acc: 0.8000\n",
            "Epoch 282/500\n",
            "\n",
            "Epoch 00282: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0417 - acc: 1.0000 - val_loss: 0.5670 - val_acc: 0.8000\n",
            "Epoch 283/500\n",
            "\n",
            "Epoch 00283: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0403 - acc: 1.0000 - val_loss: 0.5556 - val_acc: 0.8000\n",
            "Epoch 284/500\n",
            "\n",
            "Epoch 00284: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0764 - acc: 0.9833 - val_loss: 0.5601 - val_acc: 0.8000\n",
            "Epoch 285/500\n",
            "\n",
            "Epoch 00285: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0930 - acc: 0.9833 - val_loss: 0.5409 - val_acc: 0.8000\n",
            "Epoch 286/500\n",
            "\n",
            "Epoch 00286: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0445 - acc: 1.0000 - val_loss: 0.5382 - val_acc: 0.7500\n",
            "Epoch 287/500\n",
            "\n",
            "Epoch 00287: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0474 - acc: 1.0000 - val_loss: 0.5255 - val_acc: 0.7500\n",
            "Epoch 288/500\n",
            "\n",
            "Epoch 00288: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0470 - acc: 1.0000 - val_loss: 0.5171 - val_acc: 0.8000\n",
            "Epoch 289/500\n",
            "\n",
            "Epoch 00289: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0672 - acc: 1.0000 - val_loss: 0.5209 - val_acc: 0.8500\n",
            "Epoch 290/500\n",
            "\n",
            "Epoch 00290: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0579 - acc: 0.9833 - val_loss: 0.5206 - val_acc: 0.9000\n",
            "Epoch 291/500\n",
            "\n",
            "Epoch 00291: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0456 - acc: 1.0000 - val_loss: 0.5182 - val_acc: 0.9000\n",
            "Epoch 292/500\n",
            "\n",
            "Epoch 00292: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0828 - acc: 0.9667 - val_loss: 0.5188 - val_acc: 0.9000\n",
            "Epoch 293/500\n",
            "\n",
            "Epoch 00293: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0391 - acc: 1.0000 - val_loss: 0.5270 - val_acc: 0.8500\n",
            "Epoch 294/500\n",
            "\n",
            "Epoch 00294: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0441 - acc: 1.0000 - val_loss: 0.5372 - val_acc: 0.8000\n",
            "Epoch 295/500\n",
            "\n",
            "Epoch 00295: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0296 - acc: 1.0000 - val_loss: 0.5389 - val_acc: 0.8000\n",
            "Epoch 296/500\n",
            "\n",
            "Epoch 00296: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0255 - acc: 1.0000 - val_loss: 0.5443 - val_acc: 0.8000\n",
            "Epoch 297/500\n",
            "\n",
            "Epoch 00297: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0462 - acc: 1.0000 - val_loss: 0.5322 - val_acc: 0.8500\n",
            "Epoch 298/500\n",
            "\n",
            "Epoch 00298: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0776 - acc: 0.9833 - val_loss: 0.5278 - val_acc: 0.8500\n",
            "Epoch 299/500\n",
            "\n",
            "Epoch 00299: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0410 - acc: 1.0000 - val_loss: 0.5300 - val_acc: 0.9000\n",
            "Epoch 300/500\n",
            "\n",
            "Epoch 00300: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0379 - acc: 1.0000 - val_loss: 0.5367 - val_acc: 0.9000\n",
            "Epoch 301/500\n",
            "\n",
            "Epoch 00301: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0493 - acc: 1.0000 - val_loss: 0.5343 - val_acc: 0.8500\n",
            "Epoch 302/500\n",
            "\n",
            "Epoch 00302: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0414 - acc: 1.0000 - val_loss: 0.5256 - val_acc: 0.8000\n",
            "Epoch 303/500\n",
            "\n",
            "Epoch 00303: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0356 - acc: 1.0000 - val_loss: 0.5185 - val_acc: 0.8000\n",
            "Epoch 304/500\n",
            "\n",
            "Epoch 00304: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0472 - acc: 0.9833 - val_loss: 0.5068 - val_acc: 0.8500\n",
            "Epoch 305/500\n",
            "\n",
            "Epoch 00305: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0670 - acc: 0.9833 - val_loss: 0.4941 - val_acc: 0.8500\n",
            "Epoch 306/500\n",
            "\n",
            "Epoch 00306: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0372 - acc: 1.0000 - val_loss: 0.4847 - val_acc: 0.8500\n",
            "Epoch 307/500\n",
            "\n",
            "Epoch 00307: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0327 - acc: 1.0000 - val_loss: 0.4872 - val_acc: 0.8000\n",
            "Epoch 308/500\n",
            "\n",
            "Epoch 00308: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0742 - acc: 0.9833 - val_loss: 0.5275 - val_acc: 0.7500\n",
            "Epoch 309/500\n",
            "\n",
            "Epoch 00309: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0431 - acc: 1.0000 - val_loss: 0.5922 - val_acc: 0.7500\n",
            "Epoch 310/500\n",
            "\n",
            "Epoch 00310: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0508 - acc: 1.0000 - val_loss: 0.6805 - val_acc: 0.7500\n",
            "Epoch 311/500\n",
            "\n",
            "Epoch 00311: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0411 - acc: 1.0000 - val_loss: 0.8409 - val_acc: 0.7500\n",
            "Epoch 312/500\n",
            "\n",
            "Epoch 00312: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0560 - acc: 1.0000 - val_loss: 0.9717 - val_acc: 0.7500\n",
            "Epoch 313/500\n",
            "\n",
            "Epoch 00313: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0409 - acc: 1.0000 - val_loss: 0.9732 - val_acc: 0.7500\n",
            "Epoch 314/500\n",
            "\n",
            "Epoch 00314: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0689 - acc: 1.0000 - val_loss: 0.8304 - val_acc: 0.7500\n",
            "Epoch 315/500\n",
            "\n",
            "Epoch 00315: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0444 - acc: 1.0000 - val_loss: 0.7726 - val_acc: 0.7500\n",
            "Epoch 316/500\n",
            "\n",
            "Epoch 00316: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.1033 - acc: 0.9833 - val_loss: 0.7638 - val_acc: 0.7500\n",
            "Epoch 317/500\n",
            "\n",
            "Epoch 00317: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0467 - acc: 1.0000 - val_loss: 0.7400 - val_acc: 0.7500\n",
            "Epoch 318/500\n",
            "\n",
            "Epoch 00318: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0473 - acc: 1.0000 - val_loss: 0.6548 - val_acc: 0.7500\n",
            "Epoch 319/500\n",
            "\n",
            "Epoch 00319: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0554 - acc: 1.0000 - val_loss: 0.5841 - val_acc: 0.8000\n",
            "Epoch 320/500\n",
            "\n",
            "Epoch 00320: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0346 - acc: 1.0000 - val_loss: 0.5723 - val_acc: 0.8000\n",
            "Epoch 321/500\n",
            "\n",
            "Epoch 00321: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0526 - acc: 0.9833 - val_loss: 0.5757 - val_acc: 0.8000\n",
            "Epoch 322/500\n",
            "\n",
            "Epoch 00322: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0494 - acc: 0.9833 - val_loss: 0.5707 - val_acc: 0.8000\n",
            "Epoch 323/500\n",
            "\n",
            "Epoch 00323: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0405 - acc: 1.0000 - val_loss: 0.5652 - val_acc: 0.8000\n",
            "Epoch 324/500\n",
            "\n",
            "Epoch 00324: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0443 - acc: 1.0000 - val_loss: 0.5791 - val_acc: 0.8000\n",
            "Epoch 325/500\n",
            "\n",
            "Epoch 00325: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0864 - acc: 0.9667 - val_loss: 0.6115 - val_acc: 0.8000\n",
            "Epoch 326/500\n",
            "\n",
            "Epoch 00326: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0389 - acc: 0.9833 - val_loss: 0.6097 - val_acc: 0.8000\n",
            "Epoch 327/500\n",
            "\n",
            "Epoch 00327: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0386 - acc: 1.0000 - val_loss: 0.5651 - val_acc: 0.8000\n",
            "Epoch 328/500\n",
            "\n",
            "Epoch 00328: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.1154 - acc: 0.9333 - val_loss: 0.5322 - val_acc: 0.9000\n",
            "Epoch 329/500\n",
            "\n",
            "Epoch 00329: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.1387 - acc: 0.9667 - val_loss: 0.5128 - val_acc: 0.8000\n",
            "Epoch 330/500\n",
            "\n",
            "Epoch 00330: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0448 - acc: 1.0000 - val_loss: 0.4834 - val_acc: 0.8000\n",
            "Epoch 331/500\n",
            "\n",
            "Epoch 00331: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0456 - acc: 1.0000 - val_loss: 0.4737 - val_acc: 0.8500\n",
            "Epoch 332/500\n",
            "\n",
            "Epoch 00332: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0576 - acc: 1.0000 - val_loss: 0.4862 - val_acc: 0.8000\n",
            "Epoch 333/500\n",
            "\n",
            "Epoch 00333: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0445 - acc: 1.0000 - val_loss: 0.4930 - val_acc: 0.8000\n",
            "Epoch 334/500\n",
            "\n",
            "Epoch 00334: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0511 - acc: 1.0000 - val_loss: 0.5046 - val_acc: 0.8000\n",
            "Epoch 335/500\n",
            "\n",
            "Epoch 00335: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0518 - acc: 1.0000 - val_loss: 0.5047 - val_acc: 0.8000\n",
            "Epoch 336/500\n",
            "\n",
            "Epoch 00336: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0662 - acc: 0.9833 - val_loss: 0.5040 - val_acc: 0.8500\n",
            "Epoch 337/500\n",
            "\n",
            "Epoch 00337: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0457 - acc: 1.0000 - val_loss: 0.4971 - val_acc: 0.8500\n",
            "Epoch 338/500\n",
            "\n",
            "Epoch 00338: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0451 - acc: 1.0000 - val_loss: 0.4907 - val_acc: 0.8500\n",
            "Epoch 339/500\n",
            "\n",
            "Epoch 00339: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0483 - acc: 0.9833 - val_loss: 0.4963 - val_acc: 0.8500\n",
            "Epoch 340/500\n",
            "\n",
            "Epoch 00340: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0376 - acc: 1.0000 - val_loss: 0.5269 - val_acc: 0.8500\n",
            "Epoch 341/500\n",
            "\n",
            "Epoch 00341: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0512 - acc: 0.9833 - val_loss: 0.5610 - val_acc: 0.8000\n",
            "Epoch 342/500\n",
            "\n",
            "Epoch 00342: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0557 - acc: 0.9833 - val_loss: 0.5909 - val_acc: 0.8000\n",
            "Epoch 343/500\n",
            "\n",
            "Epoch 00343: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0320 - acc: 1.0000 - val_loss: 0.6241 - val_acc: 0.8000\n",
            "Epoch 344/500\n",
            "\n",
            "Epoch 00344: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0777 - acc: 0.9833 - val_loss: 0.5746 - val_acc: 0.8000\n",
            "Epoch 345/500\n",
            "\n",
            "Epoch 00345: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0489 - acc: 1.0000 - val_loss: 0.5833 - val_acc: 0.8000\n",
            "Epoch 346/500\n",
            "\n",
            "Epoch 00346: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0456 - acc: 1.0000 - val_loss: 0.5818 - val_acc: 0.8000\n",
            "Epoch 347/500\n",
            "\n",
            "Epoch 00347: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0323 - acc: 1.0000 - val_loss: 0.5657 - val_acc: 0.8000\n",
            "Epoch 348/500\n",
            "\n",
            "Epoch 00348: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0657 - acc: 1.0000 - val_loss: 0.5343 - val_acc: 0.8000\n",
            "Epoch 349/500\n",
            "\n",
            "Epoch 00349: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0308 - acc: 1.0000 - val_loss: 0.5403 - val_acc: 0.8000\n",
            "Epoch 350/500\n",
            "\n",
            "Epoch 00350: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0364 - acc: 1.0000 - val_loss: 0.5677 - val_acc: 0.8000\n",
            "Epoch 351/500\n",
            "\n",
            "Epoch 00351: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0380 - acc: 1.0000 - val_loss: 0.6099 - val_acc: 0.7500\n",
            "Epoch 352/500\n",
            "\n",
            "Epoch 00352: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0371 - acc: 1.0000 - val_loss: 0.6439 - val_acc: 0.7500\n",
            "Epoch 353/500\n",
            "\n",
            "Epoch 00353: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0591 - acc: 1.0000 - val_loss: 0.6245 - val_acc: 0.7500\n",
            "Epoch 354/500\n",
            "\n",
            "Epoch 00354: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0410 - acc: 0.9833 - val_loss: 0.6047 - val_acc: 0.8000\n",
            "Epoch 355/500\n",
            "\n",
            "Epoch 00355: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0576 - acc: 1.0000 - val_loss: 0.5902 - val_acc: 0.8500\n",
            "Epoch 356/500\n",
            "\n",
            "Epoch 00356: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0416 - acc: 1.0000 - val_loss: 0.5996 - val_acc: 0.8500\n",
            "Epoch 357/500\n",
            "\n",
            "Epoch 00357: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0352 - acc: 1.0000 - val_loss: 0.5972 - val_acc: 0.8500\n",
            "Epoch 358/500\n",
            "\n",
            "Epoch 00358: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0371 - acc: 1.0000 - val_loss: 0.6242 - val_acc: 0.8000\n",
            "Epoch 359/500\n",
            "\n",
            "Epoch 00359: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0510 - acc: 1.0000 - val_loss: 0.6511 - val_acc: 0.8000\n",
            "Epoch 360/500\n",
            "\n",
            "Epoch 00360: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0432 - acc: 1.0000 - val_loss: 0.6568 - val_acc: 0.7500\n",
            "Epoch 361/500\n",
            "\n",
            "Epoch 00361: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0545 - acc: 0.9833 - val_loss: 0.5911 - val_acc: 0.8500\n",
            "Epoch 362/500\n",
            "\n",
            "Epoch 00362: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0290 - acc: 1.0000 - val_loss: 0.5757 - val_acc: 0.9000\n",
            "Epoch 363/500\n",
            "\n",
            "Epoch 00363: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0288 - acc: 1.0000 - val_loss: 0.5877 - val_acc: 0.8500\n",
            "Epoch 364/500\n",
            "\n",
            "Epoch 00364: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0540 - acc: 1.0000 - val_loss: 0.6044 - val_acc: 0.8500\n",
            "Epoch 365/500\n",
            "\n",
            "Epoch 00365: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0591 - acc: 0.9833 - val_loss: 0.6909 - val_acc: 0.8000\n",
            "Epoch 366/500\n",
            "\n",
            "Epoch 00366: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0541 - acc: 0.9833 - val_loss: 0.7618 - val_acc: 0.8000\n",
            "Epoch 367/500\n",
            "\n",
            "Epoch 00367: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0375 - acc: 1.0000 - val_loss: 0.8022 - val_acc: 0.7500\n",
            "Epoch 368/500\n",
            "\n",
            "Epoch 00368: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0452 - acc: 0.9833 - val_loss: 0.9121 - val_acc: 0.7500\n",
            "Epoch 369/500\n",
            "\n",
            "Epoch 00369: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.1118 - acc: 0.9667 - val_loss: 0.7443 - val_acc: 0.8000\n",
            "Epoch 370/500\n",
            "\n",
            "Epoch 00370: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0188 - acc: 1.0000 - val_loss: 0.6676 - val_acc: 0.7500\n",
            "Epoch 371/500\n",
            "\n",
            "Epoch 00371: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0465 - acc: 1.0000 - val_loss: 0.6682 - val_acc: 0.7500\n",
            "Epoch 372/500\n",
            "\n",
            "Epoch 00372: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0654 - acc: 0.9833 - val_loss: 0.7006 - val_acc: 0.7500\n",
            "Epoch 373/500\n",
            "\n",
            "Epoch 00373: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0709 - acc: 0.9667 - val_loss: 0.6699 - val_acc: 0.7500\n",
            "Epoch 374/500\n",
            "\n",
            "Epoch 00374: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0780 - acc: 0.9833 - val_loss: 0.5724 - val_acc: 0.7500\n",
            "Epoch 375/500\n",
            "\n",
            "Epoch 00375: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0339 - acc: 1.0000 - val_loss: 0.5421 - val_acc: 0.7500\n",
            "Epoch 376/500\n",
            "\n",
            "Epoch 00376: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0711 - acc: 1.0000 - val_loss: 0.5627 - val_acc: 0.7500\n",
            "Epoch 377/500\n",
            "\n",
            "Epoch 00377: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0407 - acc: 1.0000 - val_loss: 0.5731 - val_acc: 0.7500\n",
            "Epoch 378/500\n",
            "\n",
            "Epoch 00378: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0372 - acc: 1.0000 - val_loss: 0.5589 - val_acc: 0.7500\n",
            "Epoch 379/500\n",
            "\n",
            "Epoch 00379: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0565 - acc: 1.0000 - val_loss: 0.5600 - val_acc: 0.7500\n",
            "Epoch 380/500\n",
            "\n",
            "Epoch 00380: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0294 - acc: 1.0000 - val_loss: 0.5699 - val_acc: 0.7500\n",
            "Epoch 381/500\n",
            "\n",
            "Epoch 00381: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0711 - acc: 0.9833 - val_loss: 0.5499 - val_acc: 0.7500\n",
            "Epoch 382/500\n",
            "\n",
            "Epoch 00382: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0354 - acc: 1.0000 - val_loss: 0.5287 - val_acc: 0.7500\n",
            "Epoch 383/500\n",
            "\n",
            "Epoch 00383: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0466 - acc: 1.0000 - val_loss: 0.5179 - val_acc: 0.8000\n",
            "Epoch 384/500\n",
            "\n",
            "Epoch 00384: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0461 - acc: 1.0000 - val_loss: 0.5429 - val_acc: 0.7500\n",
            "Epoch 385/500\n",
            "\n",
            "Epoch 00385: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0325 - acc: 1.0000 - val_loss: 0.5428 - val_acc: 0.8000\n",
            "Epoch 386/500\n",
            "\n",
            "Epoch 00386: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0268 - acc: 1.0000 - val_loss: 0.5502 - val_acc: 0.8000\n",
            "Epoch 387/500\n",
            "\n",
            "Epoch 00387: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0348 - acc: 1.0000 - val_loss: 0.5568 - val_acc: 0.8500\n",
            "Epoch 388/500\n",
            "\n",
            "Epoch 00388: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0483 - acc: 1.0000 - val_loss: 0.5662 - val_acc: 0.8000\n",
            "Epoch 389/500\n",
            "\n",
            "Epoch 00389: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0541 - acc: 0.9833 - val_loss: 0.5755 - val_acc: 0.8000\n",
            "Epoch 390/500\n",
            "\n",
            "Epoch 00390: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0452 - acc: 1.0000 - val_loss: 0.6214 - val_acc: 0.8000\n",
            "Epoch 391/500\n",
            "\n",
            "Epoch 00391: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0241 - acc: 1.0000 - val_loss: 0.6379 - val_acc: 0.8000\n",
            "Epoch 392/500\n",
            "\n",
            "Epoch 00392: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0218 - acc: 1.0000 - val_loss: 0.6467 - val_acc: 0.8000\n",
            "Epoch 393/500\n",
            "\n",
            "Epoch 00393: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0489 - acc: 0.9833 - val_loss: 0.6579 - val_acc: 0.8000\n",
            "Epoch 394/500\n",
            "\n",
            "Epoch 00394: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0355 - acc: 1.0000 - val_loss: 0.6573 - val_acc: 0.7500\n",
            "Epoch 395/500\n",
            "\n",
            "Epoch 00395: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0289 - acc: 1.0000 - val_loss: 0.6473 - val_acc: 0.7500\n",
            "Epoch 396/500\n",
            "\n",
            "Epoch 00396: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0324 - acc: 1.0000 - val_loss: 0.6227 - val_acc: 0.7500\n",
            "Epoch 397/500\n",
            "\n",
            "Epoch 00397: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0396 - acc: 1.0000 - val_loss: 0.6366 - val_acc: 0.7500\n",
            "Epoch 398/500\n",
            "\n",
            "Epoch 00398: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0718 - acc: 0.9667 - val_loss: 0.6435 - val_acc: 0.7500\n",
            "Epoch 399/500\n",
            "\n",
            "Epoch 00399: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0232 - acc: 1.0000 - val_loss: 0.6254 - val_acc: 0.7500\n",
            "Epoch 400/500\n",
            "\n",
            "Epoch 00400: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.1152 - acc: 0.9500 - val_loss: 0.5590 - val_acc: 0.7500\n",
            "Epoch 401/500\n",
            "\n",
            "Epoch 00401: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0417 - acc: 1.0000 - val_loss: 0.5598 - val_acc: 0.7500\n",
            "Epoch 402/500\n",
            "\n",
            "Epoch 00402: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0450 - acc: 1.0000 - val_loss: 0.5731 - val_acc: 0.7500\n",
            "Epoch 403/500\n",
            "\n",
            "Epoch 00403: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0233 - acc: 1.0000 - val_loss: 0.6068 - val_acc: 0.7500\n",
            "Epoch 404/500\n",
            "\n",
            "Epoch 00404: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0307 - acc: 1.0000 - val_loss: 0.6303 - val_acc: 0.7500\n",
            "Epoch 405/500\n",
            "\n",
            "Epoch 00405: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0329 - acc: 1.0000 - val_loss: 0.6384 - val_acc: 0.7500\n",
            "Epoch 406/500\n",
            "\n",
            "Epoch 00406: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0584 - acc: 0.9667 - val_loss: 0.6373 - val_acc: 0.7500\n",
            "Epoch 407/500\n",
            "\n",
            "Epoch 00407: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0229 - acc: 1.0000 - val_loss: 0.6646 - val_acc: 0.7500\n",
            "Epoch 408/500\n",
            "\n",
            "Epoch 00408: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0625 - acc: 0.9833 - val_loss: 0.6886 - val_acc: 0.8000\n",
            "Epoch 409/500\n",
            "\n",
            "Epoch 00409: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0337 - acc: 1.0000 - val_loss: 0.6885 - val_acc: 0.8000\n",
            "Epoch 410/500\n",
            "\n",
            "Epoch 00410: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0365 - acc: 1.0000 - val_loss: 0.6671 - val_acc: 0.7500\n",
            "Epoch 411/500\n",
            "\n",
            "Epoch 00411: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0269 - acc: 1.0000 - val_loss: 0.6625 - val_acc: 0.7500\n",
            "Epoch 412/500\n",
            "\n",
            "Epoch 00412: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0318 - acc: 1.0000 - val_loss: 0.6540 - val_acc: 0.7500\n",
            "Epoch 413/500\n",
            "\n",
            "Epoch 00413: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0331 - acc: 1.0000 - val_loss: 0.6567 - val_acc: 0.7500\n",
            "Epoch 414/500\n",
            "\n",
            "Epoch 00414: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0291 - acc: 1.0000 - val_loss: 0.6517 - val_acc: 0.7500\n",
            "Epoch 415/500\n",
            "\n",
            "Epoch 00415: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0439 - acc: 1.0000 - val_loss: 0.6215 - val_acc: 0.7500\n",
            "Epoch 416/500\n",
            "\n",
            "Epoch 00416: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0335 - acc: 1.0000 - val_loss: 0.5602 - val_acc: 0.7500\n",
            "Epoch 417/500\n",
            "\n",
            "Epoch 00417: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0468 - acc: 1.0000 - val_loss: 0.5332 - val_acc: 0.8000\n",
            "Epoch 418/500\n",
            "\n",
            "Epoch 00418: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0285 - acc: 1.0000 - val_loss: 0.5245 - val_acc: 0.8000\n",
            "Epoch 419/500\n",
            "\n",
            "Epoch 00419: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0200 - acc: 1.0000 - val_loss: 0.5163 - val_acc: 0.8000\n",
            "Epoch 420/500\n",
            "\n",
            "Epoch 00420: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0186 - acc: 1.0000 - val_loss: 0.5136 - val_acc: 0.8500\n",
            "Epoch 421/500\n",
            "\n",
            "Epoch 00421: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0719 - acc: 0.9833 - val_loss: 0.5002 - val_acc: 0.8500\n",
            "Epoch 422/500\n",
            "\n",
            "Epoch 00422: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0430 - acc: 1.0000 - val_loss: 0.4962 - val_acc: 0.8500\n",
            "Epoch 423/500\n",
            "\n",
            "Epoch 00423: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0462 - acc: 1.0000 - val_loss: 0.4941 - val_acc: 0.8500\n",
            "Epoch 424/500\n",
            "\n",
            "Epoch 00424: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0276 - acc: 1.0000 - val_loss: 0.4904 - val_acc: 0.8500\n",
            "Epoch 425/500\n",
            "\n",
            "Epoch 00425: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0791 - acc: 0.9500 - val_loss: 0.4885 - val_acc: 0.8500\n",
            "Epoch 426/500\n",
            "\n",
            "Epoch 00426: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0383 - acc: 1.0000 - val_loss: 0.4895 - val_acc: 0.8500\n",
            "Epoch 427/500\n",
            "\n",
            "Epoch 00427: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0411 - acc: 0.9833 - val_loss: 0.5038 - val_acc: 0.8000\n",
            "Epoch 428/500\n",
            "\n",
            "Epoch 00428: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0272 - acc: 1.0000 - val_loss: 0.5182 - val_acc: 0.8000\n",
            "Epoch 429/500\n",
            "\n",
            "Epoch 00429: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0359 - acc: 1.0000 - val_loss: 0.5637 - val_acc: 0.8000\n",
            "Epoch 430/500\n",
            "\n",
            "Epoch 00430: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0310 - acc: 1.0000 - val_loss: 0.6032 - val_acc: 0.8000\n",
            "Epoch 431/500\n",
            "\n",
            "Epoch 00431: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0278 - acc: 1.0000 - val_loss: 0.6214 - val_acc: 0.8000\n",
            "Epoch 432/500\n",
            "\n",
            "Epoch 00432: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0609 - acc: 0.9833 - val_loss: 0.7068 - val_acc: 0.8000\n",
            "Epoch 433/500\n",
            "\n",
            "Epoch 00433: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0227 - acc: 1.0000 - val_loss: 0.7622 - val_acc: 0.7500\n",
            "Epoch 434/500\n",
            "\n",
            "Epoch 00434: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0467 - acc: 1.0000 - val_loss: 0.7805 - val_acc: 0.7500\n",
            "Epoch 435/500\n",
            "\n",
            "Epoch 00435: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0446 - acc: 1.0000 - val_loss: 0.7006 - val_acc: 0.8500\n",
            "Epoch 436/500\n",
            "\n",
            "Epoch 00436: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0536 - acc: 1.0000 - val_loss: 0.6647 - val_acc: 0.8000\n",
            "Epoch 437/500\n",
            "\n",
            "Epoch 00437: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0398 - acc: 0.9833 - val_loss: 0.6455 - val_acc: 0.8000\n",
            "Epoch 438/500\n",
            "\n",
            "Epoch 00438: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0663 - acc: 0.9667 - val_loss: 0.6552 - val_acc: 0.8000\n",
            "Epoch 439/500\n",
            "\n",
            "Epoch 00439: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0306 - acc: 1.0000 - val_loss: 0.6483 - val_acc: 0.8000\n",
            "Epoch 440/500\n",
            "\n",
            "Epoch 00440: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0392 - acc: 1.0000 - val_loss: 0.6427 - val_acc: 0.8500\n",
            "Epoch 441/500\n",
            "\n",
            "Epoch 00441: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0411 - acc: 1.0000 - val_loss: 0.6338 - val_acc: 0.8500\n",
            "Epoch 442/500\n",
            "\n",
            "Epoch 00442: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0542 - acc: 0.9833 - val_loss: 0.6246 - val_acc: 0.8500\n",
            "Epoch 443/500\n",
            "\n",
            "Epoch 00443: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0356 - acc: 1.0000 - val_loss: 0.6236 - val_acc: 0.8000\n",
            "Epoch 444/500\n",
            "\n",
            "Epoch 00444: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0361 - acc: 1.0000 - val_loss: 0.6245 - val_acc: 0.8000\n",
            "Epoch 445/500\n",
            "\n",
            "Epoch 00445: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0482 - acc: 1.0000 - val_loss: 0.6059 - val_acc: 0.8000\n",
            "Epoch 446/500\n",
            "\n",
            "Epoch 00446: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0317 - acc: 1.0000 - val_loss: 0.6104 - val_acc: 0.8000\n",
            "Epoch 447/500\n",
            "\n",
            "Epoch 00447: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0249 - acc: 1.0000 - val_loss: 0.5990 - val_acc: 0.8000\n",
            "Epoch 448/500\n",
            "\n",
            "Epoch 00448: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0403 - acc: 0.9833 - val_loss: 0.5898 - val_acc: 0.8000\n",
            "Epoch 449/500\n",
            "\n",
            "Epoch 00449: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0663 - acc: 0.9833 - val_loss: 0.5590 - val_acc: 0.8500\n",
            "Epoch 450/500\n",
            "\n",
            "Epoch 00450: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0262 - acc: 1.0000 - val_loss: 0.5459 - val_acc: 0.8500\n",
            "Epoch 451/500\n",
            "\n",
            "Epoch 00451: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0409 - acc: 1.0000 - val_loss: 0.5386 - val_acc: 0.8500\n",
            "Epoch 452/500\n",
            "\n",
            "Epoch 00452: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0401 - acc: 0.9833 - val_loss: 0.5326 - val_acc: 0.8500\n",
            "Epoch 453/500\n",
            "\n",
            "Epoch 00453: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0337 - acc: 1.0000 - val_loss: 0.5264 - val_acc: 0.8500\n",
            "Epoch 454/500\n",
            "\n",
            "Epoch 00454: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0434 - acc: 0.9833 - val_loss: 0.5092 - val_acc: 0.8500\n",
            "Epoch 455/500\n",
            "\n",
            "Epoch 00455: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0407 - acc: 1.0000 - val_loss: 0.5150 - val_acc: 0.8500\n",
            "Epoch 456/500\n",
            "\n",
            "Epoch 00456: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0191 - acc: 1.0000 - val_loss: 0.5315 - val_acc: 0.8500\n",
            "Epoch 457/500\n",
            "\n",
            "Epoch 00457: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0278 - acc: 1.0000 - val_loss: 0.5506 - val_acc: 0.8000\n",
            "Epoch 458/500\n",
            "\n",
            "Epoch 00458: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0324 - acc: 1.0000 - val_loss: 0.5833 - val_acc: 0.7500\n",
            "Epoch 459/500\n",
            "\n",
            "Epoch 00459: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0435 - acc: 1.0000 - val_loss: 0.6150 - val_acc: 0.7500\n",
            "Epoch 460/500\n",
            "\n",
            "Epoch 00460: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0360 - acc: 1.0000 - val_loss: 0.6076 - val_acc: 0.8000\n",
            "Epoch 461/500\n",
            "\n",
            "Epoch 00461: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0195 - acc: 1.0000 - val_loss: 0.6087 - val_acc: 0.7500\n",
            "Epoch 462/500\n",
            "\n",
            "Epoch 00462: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0194 - acc: 1.0000 - val_loss: 0.6139 - val_acc: 0.7500\n",
            "Epoch 463/500\n",
            "\n",
            "Epoch 00463: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0305 - acc: 1.0000 - val_loss: 0.6395 - val_acc: 0.7500\n",
            "Epoch 464/500\n",
            "\n",
            "Epoch 00464: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0282 - acc: 1.0000 - val_loss: 0.6903 - val_acc: 0.7500\n",
            "Epoch 465/500\n",
            "\n",
            "Epoch 00465: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0400 - acc: 1.0000 - val_loss: 0.7397 - val_acc: 0.7500\n",
            "Epoch 466/500\n",
            "\n",
            "Epoch 00466: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0543 - acc: 1.0000 - val_loss: 0.7959 - val_acc: 0.8000\n",
            "Epoch 467/500\n",
            "\n",
            "Epoch 00467: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0436 - acc: 1.0000 - val_loss: 0.8224 - val_acc: 0.7500\n",
            "Epoch 468/500\n",
            "\n",
            "Epoch 00468: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0323 - acc: 1.0000 - val_loss: 0.8234 - val_acc: 0.7500\n",
            "Epoch 469/500\n",
            "\n",
            "Epoch 00469: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0177 - acc: 1.0000 - val_loss: 0.7638 - val_acc: 0.8000\n",
            "Epoch 470/500\n",
            "\n",
            "Epoch 00470: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0224 - acc: 1.0000 - val_loss: 0.7262 - val_acc: 0.8000\n",
            "Epoch 471/500\n",
            "\n",
            "Epoch 00471: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0289 - acc: 1.0000 - val_loss: 0.6926 - val_acc: 0.8000\n",
            "Epoch 472/500\n",
            "\n",
            "Epoch 00472: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0201 - acc: 1.0000 - val_loss: 0.6701 - val_acc: 0.7500\n",
            "Epoch 473/500\n",
            "\n",
            "Epoch 00473: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0364 - acc: 1.0000 - val_loss: 0.6682 - val_acc: 0.7500\n",
            "Epoch 474/500\n",
            "\n",
            "Epoch 00474: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0281 - acc: 1.0000 - val_loss: 0.6639 - val_acc: 0.7500\n",
            "Epoch 475/500\n",
            "\n",
            "Epoch 00475: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0363 - acc: 1.0000 - val_loss: 0.6547 - val_acc: 0.7500\n",
            "Epoch 476/500\n",
            "\n",
            "Epoch 00476: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0180 - acc: 1.0000 - val_loss: 0.6240 - val_acc: 0.7500\n",
            "Epoch 477/500\n",
            "\n",
            "Epoch 00477: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0175 - acc: 1.0000 - val_loss: 0.5937 - val_acc: 0.7500\n",
            "Epoch 478/500\n",
            "\n",
            "Epoch 00478: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0441 - acc: 1.0000 - val_loss: 0.5930 - val_acc: 0.7500\n",
            "Epoch 479/500\n",
            "\n",
            "Epoch 00479: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0308 - acc: 1.0000 - val_loss: 0.5756 - val_acc: 0.8000\n",
            "Epoch 480/500\n",
            "\n",
            "Epoch 00480: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0131 - acc: 1.0000 - val_loss: 0.5677 - val_acc: 0.8000\n",
            "Epoch 481/500\n",
            "\n",
            "Epoch 00481: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0260 - acc: 1.0000 - val_loss: 0.5686 - val_acc: 0.8000\n",
            "Epoch 482/500\n",
            "\n",
            "Epoch 00482: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0208 - acc: 1.0000 - val_loss: 0.5864 - val_acc: 0.7500\n",
            "Epoch 483/500\n",
            "\n",
            "Epoch 00483: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0344 - acc: 1.0000 - val_loss: 0.6037 - val_acc: 0.7500\n",
            "Epoch 484/500\n",
            "\n",
            "Epoch 00484: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0397 - acc: 1.0000 - val_loss: 0.6045 - val_acc: 0.7500\n",
            "Epoch 485/500\n",
            "\n",
            "Epoch 00485: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0160 - acc: 1.0000 - val_loss: 0.6031 - val_acc: 0.8000\n",
            "Epoch 486/500\n",
            "\n",
            "Epoch 00486: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0081 - acc: 1.0000 - val_loss: 0.6054 - val_acc: 0.8000\n",
            "Epoch 487/500\n",
            "\n",
            "Epoch 00487: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0267 - acc: 1.0000 - val_loss: 0.6020 - val_acc: 0.8500\n",
            "Epoch 488/500\n",
            "\n",
            "Epoch 00488: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0341 - acc: 0.9833 - val_loss: 0.6067 - val_acc: 0.8500\n",
            "Epoch 489/500\n",
            "\n",
            "Epoch 00489: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0382 - acc: 1.0000 - val_loss: 0.6140 - val_acc: 0.9000\n",
            "Epoch 490/500\n",
            "\n",
            "Epoch 00490: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0261 - acc: 1.0000 - val_loss: 0.6356 - val_acc: 0.8000\n",
            "Epoch 491/500\n",
            "\n",
            "Epoch 00491: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0580 - acc: 0.9667 - val_loss: 0.6807 - val_acc: 0.7500\n",
            "Epoch 492/500\n",
            "\n",
            "Epoch 00492: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0229 - acc: 1.0000 - val_loss: 0.8474 - val_acc: 0.7500\n",
            "Epoch 493/500\n",
            "\n",
            "Epoch 00493: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0223 - acc: 1.0000 - val_loss: 1.0560 - val_acc: 0.7500\n",
            "Epoch 494/500\n",
            "\n",
            "Epoch 00494: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0473 - acc: 0.9833 - val_loss: 1.1358 - val_acc: 0.7500\n",
            "Epoch 495/500\n",
            "\n",
            "Epoch 00495: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0216 - acc: 1.0000 - val_loss: 1.1214 - val_acc: 0.7500\n",
            "Epoch 496/500\n",
            "\n",
            "Epoch 00496: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0247 - acc: 1.0000 - val_loss: 1.0652 - val_acc: 0.7500\n",
            "Epoch 497/500\n",
            "\n",
            "Epoch 00497: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0250 - acc: 1.0000 - val_loss: 0.9656 - val_acc: 0.7500\n",
            "Epoch 498/500\n",
            "\n",
            "Epoch 00498: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0390 - acc: 1.0000 - val_loss: 0.8409 - val_acc: 0.7500\n",
            "Epoch 499/500\n",
            "\n",
            "Epoch 00499: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0291 - acc: 1.0000 - val_loss: 0.8006 - val_acc: 0.7500\n",
            "Epoch 500/500\n",
            "\n",
            "Epoch 00500: val_loss did not improve from 0.45161\n",
            "60/60 - 0s - loss: 0.0343 - acc: 1.0000 - val_loss: 0.7798 - val_acc: 0.7500\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFwIzs8K5EJiZFc6FwMyscC4EZmaF+wd6ovzRW5tVUAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "# Evaluate on test data\n",
            "40/40 [==============================] - 0s 2ms/sample - loss: 0.5301 - acc: 0.8250\n",
            "test loss, test acc: [0.5300765404681442, 0.825]\n",
            "EEG_Deep/Data2A/parsed_P03T.mat\n",
            "Filtering of Training Data Finished\n",
            "EEG_Deep/Data2A/parsed_P03E.mat\n",
            "Filtering of Testing Data Finished\n",
            "(1, 2)\n",
            "Finding labels in training data\n",
            "Finding labels in testing data\n",
            "[2 2 2 1 1 2 1 1 1 2 1 1 2 2 1 1 2 2 2 2]\n",
            "(60, 12, 1536)\n",
            "(60,)\n",
            "(20, 12, 1536)\n",
            "(20,)\n",
            "[[0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]]\n",
            "X_train shape: (60, 1, 12, 1536)\n",
            "60 train samples\n",
            "20 val samples\n",
            "(40, 12, 1536)\n",
            "(40, 1)\n",
            "X_train shape: (40, 1, 12, 1536)\n",
            "40 train samples\n",
            "Train on 60 samples, validate on 20 samples\n",
            "Epoch 1/500\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.69283, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 1s - loss: 0.7317 - acc: 0.4333 - val_loss: 0.6928 - val_acc: 0.4000\n",
            "Epoch 2/500\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.69283 to 0.69231, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6752 - acc: 0.6500 - val_loss: 0.6923 - val_acc: 0.4000\n",
            "Epoch 3/500\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.69231 to 0.69202, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6735 - acc: 0.5833 - val_loss: 0.6920 - val_acc: 0.3000\n",
            "Epoch 4/500\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.69202 to 0.69184, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6529 - acc: 0.7167 - val_loss: 0.6918 - val_acc: 0.3500\n",
            "Epoch 5/500\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.69184 to 0.69125, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6424 - acc: 0.6167 - val_loss: 0.6913 - val_acc: 0.4000\n",
            "Epoch 6/500\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.69125 to 0.69077, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6322 - acc: 0.7667 - val_loss: 0.6908 - val_acc: 0.4000\n",
            "Epoch 7/500\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.69077\n",
            "60/60 - 0s - loss: 0.6428 - acc: 0.6833 - val_loss: 0.6908 - val_acc: 0.4500\n",
            "Epoch 8/500\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.69077\n",
            "60/60 - 0s - loss: 0.6051 - acc: 0.7667 - val_loss: 0.6908 - val_acc: 0.4500\n",
            "Epoch 9/500\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.69077\n",
            "60/60 - 0s - loss: 0.6018 - acc: 0.7500 - val_loss: 0.6910 - val_acc: 0.5000\n",
            "Epoch 10/500\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.69077\n",
            "60/60 - 0s - loss: 0.5990 - acc: 0.7667 - val_loss: 0.6913 - val_acc: 0.5000\n",
            "Epoch 11/500\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.69077\n",
            "60/60 - 0s - loss: 0.5855 - acc: 0.7500 - val_loss: 0.6911 - val_acc: 0.5000\n",
            "Epoch 12/500\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.69077\n",
            "60/60 - 0s - loss: 0.5814 - acc: 0.7833 - val_loss: 0.6910 - val_acc: 0.5000\n",
            "Epoch 13/500\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.69077\n",
            "60/60 - 0s - loss: 0.5855 - acc: 0.7500 - val_loss: 0.6914 - val_acc: 0.5000\n",
            "Epoch 14/500\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.69077\n",
            "60/60 - 0s - loss: 0.5814 - acc: 0.7667 - val_loss: 0.6920 - val_acc: 0.5000\n",
            "Epoch 15/500\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.69077\n",
            "60/60 - 0s - loss: 0.5560 - acc: 0.8167 - val_loss: 0.6922 - val_acc: 0.5000\n",
            "Epoch 16/500\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.69077\n",
            "60/60 - 0s - loss: 0.5653 - acc: 0.7667 - val_loss: 0.6924 - val_acc: 0.5000\n",
            "Epoch 17/500\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.69077\n",
            "60/60 - 0s - loss: 0.5505 - acc: 0.8333 - val_loss: 0.6927 - val_acc: 0.5000\n",
            "Epoch 18/500\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.69077\n",
            "60/60 - 0s - loss: 0.5571 - acc: 0.8667 - val_loss: 0.6928 - val_acc: 0.5000\n",
            "Epoch 19/500\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.69077\n",
            "60/60 - 0s - loss: 0.5606 - acc: 0.7833 - val_loss: 0.6928 - val_acc: 0.5000\n",
            "Epoch 20/500\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.69077\n",
            "60/60 - 0s - loss: 0.5323 - acc: 0.8667 - val_loss: 0.6922 - val_acc: 0.5000\n",
            "Epoch 21/500\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.69077\n",
            "60/60 - 0s - loss: 0.5260 - acc: 0.8833 - val_loss: 0.6917 - val_acc: 0.5000\n",
            "Epoch 22/500\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.69077\n",
            "60/60 - 0s - loss: 0.5403 - acc: 0.8667 - val_loss: 0.6913 - val_acc: 0.5000\n",
            "Epoch 23/500\n",
            "\n",
            "Epoch 00023: val_loss improved from 0.69077 to 0.69071, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4975 - acc: 0.8667 - val_loss: 0.6907 - val_acc: 0.5000\n",
            "Epoch 24/500\n",
            "\n",
            "Epoch 00024: val_loss improved from 0.69071 to 0.69009, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5122 - acc: 0.8333 - val_loss: 0.6901 - val_acc: 0.5500\n",
            "Epoch 25/500\n",
            "\n",
            "Epoch 00025: val_loss improved from 0.69009 to 0.68958, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5266 - acc: 0.8167 - val_loss: 0.6896 - val_acc: 0.5500\n",
            "Epoch 26/500\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.68958\n",
            "60/60 - 0s - loss: 0.5083 - acc: 0.8833 - val_loss: 0.6902 - val_acc: 0.5500\n",
            "Epoch 27/500\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.68958\n",
            "60/60 - 0s - loss: 0.4975 - acc: 0.8167 - val_loss: 0.6911 - val_acc: 0.5500\n",
            "Epoch 28/500\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.68958\n",
            "60/60 - 0s - loss: 0.4898 - acc: 0.8833 - val_loss: 0.6921 - val_acc: 0.5500\n",
            "Epoch 29/500\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.68958\n",
            "60/60 - 0s - loss: 0.4691 - acc: 0.9333 - val_loss: 0.6937 - val_acc: 0.5000\n",
            "Epoch 30/500\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.68958\n",
            "60/60 - 0s - loss: 0.4997 - acc: 0.8500 - val_loss: 0.6953 - val_acc: 0.5000\n",
            "Epoch 31/500\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.68958\n",
            "60/60 - 0s - loss: 0.4839 - acc: 0.9167 - val_loss: 0.6962 - val_acc: 0.5000\n",
            "Epoch 32/500\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.68958\n",
            "60/60 - 0s - loss: 0.4632 - acc: 0.9000 - val_loss: 0.6961 - val_acc: 0.5500\n",
            "Epoch 33/500\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.68958\n",
            "60/60 - 0s - loss: 0.4601 - acc: 0.8833 - val_loss: 0.6965 - val_acc: 0.5500\n",
            "Epoch 34/500\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.68958\n",
            "60/60 - 0s - loss: 0.4451 - acc: 0.9167 - val_loss: 0.6958 - val_acc: 0.5500\n",
            "Epoch 35/500\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.68958\n",
            "60/60 - 0s - loss: 0.4698 - acc: 0.8833 - val_loss: 0.6969 - val_acc: 0.5500\n",
            "Epoch 36/500\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.68958\n",
            "60/60 - 0s - loss: 0.4279 - acc: 0.9333 - val_loss: 0.6978 - val_acc: 0.5500\n",
            "Epoch 37/500\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.68958\n",
            "60/60 - 0s - loss: 0.4544 - acc: 0.9000 - val_loss: 0.6980 - val_acc: 0.5500\n",
            "Epoch 38/500\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.68958\n",
            "60/60 - 0s - loss: 0.4322 - acc: 0.9167 - val_loss: 0.6997 - val_acc: 0.5500\n",
            "Epoch 39/500\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.68958\n",
            "60/60 - 0s - loss: 0.4477 - acc: 0.8667 - val_loss: 0.7017 - val_acc: 0.5500\n",
            "Epoch 40/500\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.68958\n",
            "60/60 - 0s - loss: 0.4208 - acc: 0.8833 - val_loss: 0.7041 - val_acc: 0.5500\n",
            "Epoch 41/500\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.68958\n",
            "60/60 - 0s - loss: 0.4066 - acc: 0.9000 - val_loss: 0.7069 - val_acc: 0.5500\n",
            "Epoch 42/500\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.68958\n",
            "60/60 - 0s - loss: 0.4093 - acc: 0.9333 - val_loss: 0.7098 - val_acc: 0.5500\n",
            "Epoch 43/500\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.68958\n",
            "60/60 - 0s - loss: 0.4019 - acc: 0.8833 - val_loss: 0.7157 - val_acc: 0.5500\n",
            "Epoch 44/500\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.68958\n",
            "60/60 - 0s - loss: 0.4278 - acc: 0.8667 - val_loss: 0.7204 - val_acc: 0.5500\n",
            "Epoch 45/500\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.68958\n",
            "60/60 - 0s - loss: 0.4107 - acc: 0.8667 - val_loss: 0.7288 - val_acc: 0.5500\n",
            "Epoch 46/500\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.68958\n",
            "60/60 - 0s - loss: 0.3992 - acc: 0.9000 - val_loss: 0.7344 - val_acc: 0.5500\n",
            "Epoch 47/500\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.68958\n",
            "60/60 - 0s - loss: 0.3724 - acc: 0.9167 - val_loss: 0.7422 - val_acc: 0.5500\n",
            "Epoch 48/500\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.68958\n",
            "60/60 - 0s - loss: 0.3776 - acc: 0.9167 - val_loss: 0.7531 - val_acc: 0.5500\n",
            "Epoch 49/500\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.68958\n",
            "60/60 - 0s - loss: 0.3728 - acc: 0.9333 - val_loss: 0.7631 - val_acc: 0.5500\n",
            "Epoch 50/500\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.68958\n",
            "60/60 - 0s - loss: 0.3578 - acc: 0.9000 - val_loss: 0.7783 - val_acc: 0.5500\n",
            "Epoch 51/500\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.68958\n",
            "60/60 - 0s - loss: 0.3664 - acc: 0.9167 - val_loss: 0.7987 - val_acc: 0.5500\n",
            "Epoch 52/500\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.68958\n",
            "60/60 - 0s - loss: 0.3466 - acc: 0.9500 - val_loss: 0.8147 - val_acc: 0.5500\n",
            "Epoch 53/500\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.68958\n",
            "60/60 - 0s - loss: 0.3351 - acc: 0.9167 - val_loss: 0.8317 - val_acc: 0.5500\n",
            "Epoch 54/500\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.68958\n",
            "60/60 - 0s - loss: 0.3604 - acc: 0.8500 - val_loss: 0.8390 - val_acc: 0.5500\n",
            "Epoch 55/500\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.68958\n",
            "60/60 - 0s - loss: 0.4005 - acc: 0.8500 - val_loss: 0.8485 - val_acc: 0.5500\n",
            "Epoch 56/500\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.68958\n",
            "60/60 - 0s - loss: 0.3497 - acc: 0.8667 - val_loss: 0.8496 - val_acc: 0.5500\n",
            "Epoch 57/500\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.68958\n",
            "60/60 - 0s - loss: 0.3558 - acc: 0.9333 - val_loss: 0.8429 - val_acc: 0.5500\n",
            "Epoch 58/500\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.68958\n",
            "60/60 - 0s - loss: 0.3481 - acc: 0.9333 - val_loss: 0.8438 - val_acc: 0.5500\n",
            "Epoch 59/500\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.68958\n",
            "60/60 - 0s - loss: 0.3801 - acc: 0.9000 - val_loss: 0.8438 - val_acc: 0.5500\n",
            "Epoch 60/500\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.68958\n",
            "60/60 - 0s - loss: 0.3514 - acc: 0.9000 - val_loss: 0.8551 - val_acc: 0.5500\n",
            "Epoch 61/500\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.68958\n",
            "60/60 - 0s - loss: 0.3527 - acc: 0.9000 - val_loss: 0.8812 - val_acc: 0.5500\n",
            "Epoch 62/500\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.68958\n",
            "60/60 - 0s - loss: 0.3668 - acc: 0.8667 - val_loss: 0.8729 - val_acc: 0.5500\n",
            "Epoch 63/500\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.68958\n",
            "60/60 - 0s - loss: 0.3431 - acc: 0.8667 - val_loss: 0.8571 - val_acc: 0.5500\n",
            "Epoch 64/500\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.68958\n",
            "60/60 - 0s - loss: 0.3433 - acc: 0.9167 - val_loss: 0.8576 - val_acc: 0.5500\n",
            "Epoch 65/500\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.68958\n",
            "60/60 - 0s - loss: 0.3266 - acc: 0.8667 - val_loss: 0.8595 - val_acc: 0.5500\n",
            "Epoch 66/500\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.68958\n",
            "60/60 - 0s - loss: 0.3163 - acc: 0.9333 - val_loss: 0.8704 - val_acc: 0.5500\n",
            "Epoch 67/500\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.68958\n",
            "60/60 - 0s - loss: 0.3493 - acc: 0.9167 - val_loss: 0.8626 - val_acc: 0.5500\n",
            "Epoch 68/500\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.68958\n",
            "60/60 - 0s - loss: 0.3217 - acc: 0.9000 - val_loss: 0.8580 - val_acc: 0.5500\n",
            "Epoch 69/500\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.68958\n",
            "60/60 - 0s - loss: 0.3140 - acc: 0.9333 - val_loss: 0.8602 - val_acc: 0.5500\n",
            "Epoch 70/500\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.68958\n",
            "60/60 - 0s - loss: 0.3191 - acc: 0.9167 - val_loss: 0.8843 - val_acc: 0.5500\n",
            "Epoch 71/500\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.68958\n",
            "60/60 - 0s - loss: 0.3390 - acc: 0.8667 - val_loss: 0.9006 - val_acc: 0.5500\n",
            "Epoch 72/500\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.68958\n",
            "60/60 - 0s - loss: 0.3074 - acc: 0.9667 - val_loss: 0.9112 - val_acc: 0.5500\n",
            "Epoch 73/500\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.68958\n",
            "60/60 - 0s - loss: 0.3121 - acc: 0.9167 - val_loss: 0.9008 - val_acc: 0.5500\n",
            "Epoch 74/500\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.68958\n",
            "60/60 - 0s - loss: 0.2855 - acc: 0.9500 - val_loss: 0.8984 - val_acc: 0.5500\n",
            "Epoch 75/500\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.68958\n",
            "60/60 - 0s - loss: 0.3396 - acc: 0.9000 - val_loss: 0.8819 - val_acc: 0.5500\n",
            "Epoch 76/500\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.68958\n",
            "60/60 - 0s - loss: 0.3049 - acc: 0.9333 - val_loss: 0.8485 - val_acc: 0.5500\n",
            "Epoch 77/500\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.68958\n",
            "60/60 - 0s - loss: 0.3407 - acc: 0.9167 - val_loss: 0.8065 - val_acc: 0.5500\n",
            "Epoch 78/500\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.68958\n",
            "60/60 - 0s - loss: 0.3105 - acc: 0.9167 - val_loss: 0.7987 - val_acc: 0.5500\n",
            "Epoch 79/500\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.68958\n",
            "60/60 - 0s - loss: 0.2823 - acc: 0.9333 - val_loss: 0.8074 - val_acc: 0.5500\n",
            "Epoch 80/500\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.68958\n",
            "60/60 - 0s - loss: 0.2806 - acc: 0.9500 - val_loss: 0.7989 - val_acc: 0.5500\n",
            "Epoch 81/500\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.68958\n",
            "60/60 - 0s - loss: 0.2531 - acc: 0.9667 - val_loss: 0.8010 - val_acc: 0.5500\n",
            "Epoch 82/500\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.68958\n",
            "60/60 - 0s - loss: 0.2806 - acc: 0.9500 - val_loss: 0.8186 - val_acc: 0.5500\n",
            "Epoch 83/500\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.68958\n",
            "60/60 - 0s - loss: 0.2793 - acc: 0.9500 - val_loss: 0.8466 - val_acc: 0.5500\n",
            "Epoch 84/500\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.68958\n",
            "60/60 - 0s - loss: 0.2859 - acc: 0.9333 - val_loss: 0.8629 - val_acc: 0.5500\n",
            "Epoch 85/500\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.68958\n",
            "60/60 - 0s - loss: 0.3121 - acc: 0.9667 - val_loss: 0.8493 - val_acc: 0.5500\n",
            "Epoch 86/500\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.68958\n",
            "60/60 - 0s - loss: 0.2886 - acc: 0.9333 - val_loss: 0.8537 - val_acc: 0.5500\n",
            "Epoch 87/500\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.68958\n",
            "60/60 - 0s - loss: 0.2905 - acc: 0.9500 - val_loss: 0.8554 - val_acc: 0.5500\n",
            "Epoch 88/500\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.68958\n",
            "60/60 - 0s - loss: 0.2765 - acc: 0.9500 - val_loss: 0.8667 - val_acc: 0.5500\n",
            "Epoch 89/500\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.68958\n",
            "60/60 - 0s - loss: 0.3450 - acc: 0.9000 - val_loss: 0.8586 - val_acc: 0.5500\n",
            "Epoch 90/500\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.68958\n",
            "60/60 - 0s - loss: 0.3576 - acc: 0.8667 - val_loss: 0.8246 - val_acc: 0.5500\n",
            "Epoch 91/500\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.68958\n",
            "60/60 - 0s - loss: 0.2606 - acc: 0.9500 - val_loss: 0.8070 - val_acc: 0.5500\n",
            "Epoch 92/500\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.68958\n",
            "60/60 - 0s - loss: 0.3027 - acc: 0.9167 - val_loss: 0.8000 - val_acc: 0.5500\n",
            "Epoch 93/500\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.68958\n",
            "60/60 - 0s - loss: 0.2817 - acc: 0.9333 - val_loss: 0.7917 - val_acc: 0.5500\n",
            "Epoch 94/500\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.68958\n",
            "60/60 - 0s - loss: 0.3244 - acc: 0.9000 - val_loss: 0.7744 - val_acc: 0.5500\n",
            "Epoch 95/500\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.68958\n",
            "60/60 - 0s - loss: 0.2625 - acc: 0.9333 - val_loss: 0.7749 - val_acc: 0.5500\n",
            "Epoch 96/500\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.68958\n",
            "60/60 - 0s - loss: 0.2791 - acc: 0.9333 - val_loss: 0.7777 - val_acc: 0.5500\n",
            "Epoch 97/500\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.68958\n",
            "60/60 - 0s - loss: 0.2771 - acc: 0.9167 - val_loss: 0.7748 - val_acc: 0.6000\n",
            "Epoch 98/500\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.68958\n",
            "60/60 - 0s - loss: 0.2943 - acc: 0.9333 - val_loss: 0.7847 - val_acc: 0.6000\n",
            "Epoch 99/500\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.68958\n",
            "60/60 - 0s - loss: 0.2532 - acc: 0.9333 - val_loss: 0.8106 - val_acc: 0.6000\n",
            "Epoch 100/500\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.68958\n",
            "60/60 - 0s - loss: 0.2719 - acc: 0.9000 - val_loss: 0.8446 - val_acc: 0.6000\n",
            "Epoch 101/500\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.68958\n",
            "60/60 - 0s - loss: 0.2642 - acc: 0.9333 - val_loss: 0.8263 - val_acc: 0.6000\n",
            "Epoch 102/500\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.68958\n",
            "60/60 - 0s - loss: 0.3248 - acc: 0.9000 - val_loss: 0.7799 - val_acc: 0.6000\n",
            "Epoch 103/500\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.68958\n",
            "60/60 - 0s - loss: 0.2317 - acc: 0.9500 - val_loss: 0.7527 - val_acc: 0.6000\n",
            "Epoch 104/500\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.68958\n",
            "60/60 - 0s - loss: 0.2418 - acc: 0.9167 - val_loss: 0.7392 - val_acc: 0.6000\n",
            "Epoch 105/500\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 0.68958\n",
            "60/60 - 0s - loss: 0.2820 - acc: 0.8833 - val_loss: 0.7434 - val_acc: 0.6000\n",
            "Epoch 106/500\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 0.68958\n",
            "60/60 - 0s - loss: 0.2647 - acc: 0.9167 - val_loss: 0.7365 - val_acc: 0.6000\n",
            "Epoch 107/500\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.68958\n",
            "60/60 - 0s - loss: 0.2587 - acc: 0.9333 - val_loss: 0.7171 - val_acc: 0.6500\n",
            "Epoch 108/500\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 0.68958\n",
            "60/60 - 0s - loss: 0.3140 - acc: 0.9333 - val_loss: 0.7188 - val_acc: 0.6500\n",
            "Epoch 109/500\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.68958\n",
            "60/60 - 0s - loss: 0.2285 - acc: 0.9500 - val_loss: 0.7154 - val_acc: 0.6000\n",
            "Epoch 110/500\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.68958\n",
            "60/60 - 0s - loss: 0.2699 - acc: 0.9333 - val_loss: 0.7286 - val_acc: 0.6000\n",
            "Epoch 111/500\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 0.68958\n",
            "60/60 - 0s - loss: 0.2965 - acc: 0.9000 - val_loss: 0.7394 - val_acc: 0.6000\n",
            "Epoch 112/500\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 0.68958\n",
            "60/60 - 0s - loss: 0.2406 - acc: 0.9333 - val_loss: 0.7454 - val_acc: 0.6000\n",
            "Epoch 113/500\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.68958\n",
            "60/60 - 0s - loss: 0.2363 - acc: 0.9167 - val_loss: 0.7442 - val_acc: 0.6000\n",
            "Epoch 114/500\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.68958\n",
            "60/60 - 0s - loss: 0.2466 - acc: 0.9167 - val_loss: 0.7424 - val_acc: 0.6000\n",
            "Epoch 115/500\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.68958\n",
            "60/60 - 0s - loss: 0.2660 - acc: 0.9167 - val_loss: 0.7304 - val_acc: 0.6000\n",
            "Epoch 116/500\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.68958\n",
            "60/60 - 0s - loss: 0.2253 - acc: 0.9500 - val_loss: 0.7100 - val_acc: 0.6000\n",
            "Epoch 117/500\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.68958\n",
            "60/60 - 0s - loss: 0.2691 - acc: 0.9333 - val_loss: 0.6973 - val_acc: 0.6000\n",
            "Epoch 118/500\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.68958\n",
            "60/60 - 0s - loss: 0.2076 - acc: 0.9667 - val_loss: 0.6963 - val_acc: 0.6000\n",
            "Epoch 119/500\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 0.68958\n",
            "60/60 - 0s - loss: 0.2141 - acc: 0.9333 - val_loss: 0.7019 - val_acc: 0.7000\n",
            "Epoch 120/500\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.68958\n",
            "60/60 - 0s - loss: 0.2103 - acc: 0.9667 - val_loss: 0.7189 - val_acc: 0.7000\n",
            "Epoch 121/500\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.68958\n",
            "60/60 - 0s - loss: 0.2417 - acc: 0.9500 - val_loss: 0.7157 - val_acc: 0.7000\n",
            "Epoch 122/500\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.68958\n",
            "60/60 - 0s - loss: 0.2758 - acc: 0.9167 - val_loss: 0.7088 - val_acc: 0.7000\n",
            "Epoch 123/500\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 0.68958\n",
            "60/60 - 0s - loss: 0.2125 - acc: 0.9500 - val_loss: 0.7000 - val_acc: 0.7000\n",
            "Epoch 124/500\n",
            "\n",
            "Epoch 00124: val_loss improved from 0.68958 to 0.68688, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2253 - acc: 0.9667 - val_loss: 0.6869 - val_acc: 0.7000\n",
            "Epoch 125/500\n",
            "\n",
            "Epoch 00125: val_loss improved from 0.68688 to 0.67522, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2749 - acc: 0.9000 - val_loss: 0.6752 - val_acc: 0.7000\n",
            "Epoch 126/500\n",
            "\n",
            "Epoch 00126: val_loss improved from 0.67522 to 0.66486, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2810 - acc: 0.9167 - val_loss: 0.6649 - val_acc: 0.7000\n",
            "Epoch 127/500\n",
            "\n",
            "Epoch 00127: val_loss improved from 0.66486 to 0.63527, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2681 - acc: 0.8833 - val_loss: 0.6353 - val_acc: 0.7000\n",
            "Epoch 128/500\n",
            "\n",
            "Epoch 00128: val_loss improved from 0.63527 to 0.60585, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2688 - acc: 0.9167 - val_loss: 0.6058 - val_acc: 0.7000\n",
            "Epoch 129/500\n",
            "\n",
            "Epoch 00129: val_loss improved from 0.60585 to 0.58535, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2505 - acc: 0.9667 - val_loss: 0.5853 - val_acc: 0.7000\n",
            "Epoch 130/500\n",
            "\n",
            "Epoch 00130: val_loss improved from 0.58535 to 0.58512, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2182 - acc: 0.9500 - val_loss: 0.5851 - val_acc: 0.7000\n",
            "Epoch 131/500\n",
            "\n",
            "Epoch 00131: val_loss improved from 0.58512 to 0.58269, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2651 - acc: 0.9167 - val_loss: 0.5827 - val_acc: 0.7000\n",
            "Epoch 132/500\n",
            "\n",
            "Epoch 00132: val_loss improved from 0.58269 to 0.56426, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2720 - acc: 0.8833 - val_loss: 0.5643 - val_acc: 0.7000\n",
            "Epoch 133/500\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.56426\n",
            "60/60 - 0s - loss: 0.2304 - acc: 0.9167 - val_loss: 0.5692 - val_acc: 0.7000\n",
            "Epoch 134/500\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.56426\n",
            "60/60 - 0s - loss: 0.2308 - acc: 0.9333 - val_loss: 0.5789 - val_acc: 0.7000\n",
            "Epoch 135/500\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 0.56426\n",
            "60/60 - 0s - loss: 0.2740 - acc: 0.9333 - val_loss: 0.5815 - val_acc: 0.7000\n",
            "Epoch 136/500\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.56426\n",
            "60/60 - 0s - loss: 0.2460 - acc: 0.9667 - val_loss: 0.5785 - val_acc: 0.7000\n",
            "Epoch 137/500\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 0.56426\n",
            "60/60 - 0s - loss: 0.2599 - acc: 0.9333 - val_loss: 0.5694 - val_acc: 0.7000\n",
            "Epoch 138/500\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.56426\n",
            "60/60 - 0s - loss: 0.2067 - acc: 0.9667 - val_loss: 0.5682 - val_acc: 0.7000\n",
            "Epoch 139/500\n",
            "\n",
            "Epoch 00139: val_loss improved from 0.56426 to 0.55860, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2176 - acc: 0.9333 - val_loss: 0.5586 - val_acc: 0.7500\n",
            "Epoch 140/500\n",
            "\n",
            "Epoch 00140: val_loss improved from 0.55860 to 0.55227, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2508 - acc: 0.8833 - val_loss: 0.5523 - val_acc: 0.7500\n",
            "Epoch 141/500\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.55227\n",
            "60/60 - 0s - loss: 0.2138 - acc: 0.9667 - val_loss: 0.5624 - val_acc: 0.7000\n",
            "Epoch 142/500\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.55227\n",
            "60/60 - 0s - loss: 0.2351 - acc: 0.9500 - val_loss: 0.5712 - val_acc: 0.7000\n",
            "Epoch 143/500\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 0.55227\n",
            "60/60 - 0s - loss: 0.2056 - acc: 0.9667 - val_loss: 0.5791 - val_acc: 0.7000\n",
            "Epoch 144/500\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 0.55227\n",
            "60/60 - 0s - loss: 0.2806 - acc: 0.9167 - val_loss: 0.5858 - val_acc: 0.7000\n",
            "Epoch 145/500\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.55227\n",
            "60/60 - 0s - loss: 0.2329 - acc: 0.9500 - val_loss: 0.5623 - val_acc: 0.6500\n",
            "Epoch 146/500\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.55227\n",
            "60/60 - 0s - loss: 0.1887 - acc: 0.9667 - val_loss: 0.5638 - val_acc: 0.7000\n",
            "Epoch 147/500\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.55227\n",
            "60/60 - 0s - loss: 0.2044 - acc: 0.9833 - val_loss: 0.5566 - val_acc: 0.7000\n",
            "Epoch 148/500\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 0.55227\n",
            "60/60 - 0s - loss: 0.1971 - acc: 0.9500 - val_loss: 0.5598 - val_acc: 0.7000\n",
            "Epoch 149/500\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.55227\n",
            "60/60 - 0s - loss: 0.2508 - acc: 0.9333 - val_loss: 0.5640 - val_acc: 0.7000\n",
            "Epoch 150/500\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.55227\n",
            "60/60 - 0s - loss: 0.1943 - acc: 0.9667 - val_loss: 0.5585 - val_acc: 0.7000\n",
            "Epoch 151/500\n",
            "\n",
            "Epoch 00151: val_loss improved from 0.55227 to 0.54915, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2007 - acc: 0.9667 - val_loss: 0.5491 - val_acc: 0.7000\n",
            "Epoch 152/500\n",
            "\n",
            "Epoch 00152: val_loss improved from 0.54915 to 0.53088, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2144 - acc: 0.9333 - val_loss: 0.5309 - val_acc: 0.7000\n",
            "Epoch 153/500\n",
            "\n",
            "Epoch 00153: val_loss improved from 0.53088 to 0.52591, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1745 - acc: 0.9500 - val_loss: 0.5259 - val_acc: 0.7000\n",
            "Epoch 154/500\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.52591\n",
            "60/60 - 0s - loss: 0.2168 - acc: 0.9333 - val_loss: 0.5357 - val_acc: 0.7000\n",
            "Epoch 155/500\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.52591\n",
            "60/60 - 0s - loss: 0.2237 - acc: 0.9667 - val_loss: 0.5376 - val_acc: 0.7000\n",
            "Epoch 156/500\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.52591\n",
            "60/60 - 0s - loss: 0.1857 - acc: 0.9333 - val_loss: 0.5367 - val_acc: 0.7000\n",
            "Epoch 157/500\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.52591\n",
            "60/60 - 0s - loss: 0.1809 - acc: 0.9500 - val_loss: 0.5377 - val_acc: 0.7500\n",
            "Epoch 158/500\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.52591\n",
            "60/60 - 0s - loss: 0.1720 - acc: 0.9667 - val_loss: 0.5268 - val_acc: 0.7500\n",
            "Epoch 159/500\n",
            "\n",
            "Epoch 00159: val_loss improved from 0.52591 to 0.51762, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2293 - acc: 0.9333 - val_loss: 0.5176 - val_acc: 0.7500\n",
            "Epoch 160/500\n",
            "\n",
            "Epoch 00160: val_loss improved from 0.51762 to 0.50960, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1976 - acc: 0.9500 - val_loss: 0.5096 - val_acc: 0.7000\n",
            "Epoch 161/500\n",
            "\n",
            "Epoch 00161: val_loss improved from 0.50960 to 0.50956, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1942 - acc: 0.9500 - val_loss: 0.5096 - val_acc: 0.7000\n",
            "Epoch 162/500\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.50956\n",
            "60/60 - 0s - loss: 0.2511 - acc: 0.9000 - val_loss: 0.5130 - val_acc: 0.6500\n",
            "Epoch 163/500\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.50956\n",
            "60/60 - 0s - loss: 0.1971 - acc: 0.9667 - val_loss: 0.5102 - val_acc: 0.7000\n",
            "Epoch 164/500\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.50956\n",
            "60/60 - 0s - loss: 0.2239 - acc: 0.9333 - val_loss: 0.5202 - val_acc: 0.6500\n",
            "Epoch 165/500\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.50956\n",
            "60/60 - 0s - loss: 0.2178 - acc: 0.9167 - val_loss: 0.5351 - val_acc: 0.7000\n",
            "Epoch 166/500\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 0.50956\n",
            "60/60 - 0s - loss: 0.1884 - acc: 0.9167 - val_loss: 0.5373 - val_acc: 0.7000\n",
            "Epoch 167/500\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.50956\n",
            "60/60 - 0s - loss: 0.1863 - acc: 0.9667 - val_loss: 0.5292 - val_acc: 0.7000\n",
            "Epoch 168/500\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.50956\n",
            "60/60 - 0s - loss: 0.1744 - acc: 0.9833 - val_loss: 0.5220 - val_acc: 0.7000\n",
            "Epoch 169/500\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 0.50956\n",
            "60/60 - 0s - loss: 0.1468 - acc: 0.9667 - val_loss: 0.5225 - val_acc: 0.7000\n",
            "Epoch 170/500\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.50956\n",
            "60/60 - 0s - loss: 0.2117 - acc: 0.9500 - val_loss: 0.5341 - val_acc: 0.7000\n",
            "Epoch 171/500\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.50956\n",
            "60/60 - 0s - loss: 0.2549 - acc: 0.8833 - val_loss: 0.5206 - val_acc: 0.7000\n",
            "Epoch 172/500\n",
            "\n",
            "Epoch 00172: val_loss improved from 0.50956 to 0.50038, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1926 - acc: 0.9500 - val_loss: 0.5004 - val_acc: 0.7500\n",
            "Epoch 173/500\n",
            "\n",
            "Epoch 00173: val_loss improved from 0.50038 to 0.48300, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1844 - acc: 1.0000 - val_loss: 0.4830 - val_acc: 0.7500\n",
            "Epoch 174/500\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.48300\n",
            "60/60 - 0s - loss: 0.2173 - acc: 0.9333 - val_loss: 0.4879 - val_acc: 0.7000\n",
            "Epoch 175/500\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.48300\n",
            "60/60 - 0s - loss: 0.2037 - acc: 0.9667 - val_loss: 0.5159 - val_acc: 0.7000\n",
            "Epoch 176/500\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 0.48300\n",
            "60/60 - 0s - loss: 0.1798 - acc: 0.9667 - val_loss: 0.5404 - val_acc: 0.7000\n",
            "Epoch 177/500\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.48300\n",
            "60/60 - 0s - loss: 0.1858 - acc: 0.9667 - val_loss: 0.5458 - val_acc: 0.7000\n",
            "Epoch 178/500\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.48300\n",
            "60/60 - 0s - loss: 0.1894 - acc: 0.9500 - val_loss: 0.5332 - val_acc: 0.7000\n",
            "Epoch 179/500\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.48300\n",
            "60/60 - 0s - loss: 0.2177 - acc: 0.9500 - val_loss: 0.5304 - val_acc: 0.7000\n",
            "Epoch 180/500\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.48300\n",
            "60/60 - 0s - loss: 0.2066 - acc: 0.9000 - val_loss: 0.5245 - val_acc: 0.7000\n",
            "Epoch 181/500\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.48300\n",
            "60/60 - 0s - loss: 0.1806 - acc: 0.9333 - val_loss: 0.5250 - val_acc: 0.7000\n",
            "Epoch 182/500\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.48300\n",
            "60/60 - 0s - loss: 0.1903 - acc: 0.9500 - val_loss: 0.5039 - val_acc: 0.7000\n",
            "Epoch 183/500\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.48300\n",
            "60/60 - 0s - loss: 0.1898 - acc: 0.9667 - val_loss: 0.4997 - val_acc: 0.7000\n",
            "Epoch 184/500\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 0.48300\n",
            "60/60 - 0s - loss: 0.2490 - acc: 0.9000 - val_loss: 0.5062 - val_acc: 0.7000\n",
            "Epoch 185/500\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.48300\n",
            "60/60 - 0s - loss: 0.1955 - acc: 0.9500 - val_loss: 0.5090 - val_acc: 0.7000\n",
            "Epoch 186/500\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.48300\n",
            "60/60 - 0s - loss: 0.2133 - acc: 0.9167 - val_loss: 0.5107 - val_acc: 0.7000\n",
            "Epoch 187/500\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 0.48300\n",
            "60/60 - 0s - loss: 0.1599 - acc: 0.9667 - val_loss: 0.5039 - val_acc: 0.7000\n",
            "Epoch 188/500\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.48300\n",
            "60/60 - 0s - loss: 0.1458 - acc: 0.9667 - val_loss: 0.5054 - val_acc: 0.7000\n",
            "Epoch 189/500\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.48300\n",
            "60/60 - 0s - loss: 0.1632 - acc: 0.9667 - val_loss: 0.4946 - val_acc: 0.7000\n",
            "Epoch 190/500\n",
            "\n",
            "Epoch 00190: val_loss improved from 0.48300 to 0.47042, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1909 - acc: 0.9500 - val_loss: 0.4704 - val_acc: 0.7000\n",
            "Epoch 191/500\n",
            "\n",
            "Epoch 00191: val_loss improved from 0.47042 to 0.45078, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2108 - acc: 0.9333 - val_loss: 0.4508 - val_acc: 0.7000\n",
            "Epoch 192/500\n",
            "\n",
            "Epoch 00192: val_loss improved from 0.45078 to 0.44920, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1435 - acc: 0.9833 - val_loss: 0.4492 - val_acc: 0.7500\n",
            "Epoch 193/500\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.44920\n",
            "60/60 - 0s - loss: 0.2024 - acc: 0.9500 - val_loss: 0.4537 - val_acc: 0.7500\n",
            "Epoch 194/500\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.44920\n",
            "60/60 - 0s - loss: 0.1781 - acc: 0.9500 - val_loss: 0.4547 - val_acc: 0.7500\n",
            "Epoch 195/500\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.44920\n",
            "60/60 - 0s - loss: 0.1994 - acc: 0.9500 - val_loss: 0.4630 - val_acc: 0.7000\n",
            "Epoch 196/500\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.44920\n",
            "60/60 - 0s - loss: 0.1324 - acc: 0.9833 - val_loss: 0.4699 - val_acc: 0.7000\n",
            "Epoch 197/500\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.44920\n",
            "60/60 - 0s - loss: 0.1666 - acc: 0.9833 - val_loss: 0.4714 - val_acc: 0.7000\n",
            "Epoch 198/500\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.44920\n",
            "60/60 - 0s - loss: 0.1617 - acc: 0.9667 - val_loss: 0.4661 - val_acc: 0.7000\n",
            "Epoch 199/500\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.44920\n",
            "60/60 - 0s - loss: 0.1566 - acc: 0.9833 - val_loss: 0.4586 - val_acc: 0.7500\n",
            "Epoch 200/500\n",
            "\n",
            "Epoch 00200: val_loss improved from 0.44920 to 0.44603, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1565 - acc: 0.9667 - val_loss: 0.4460 - val_acc: 0.7500\n",
            "Epoch 201/500\n",
            "\n",
            "Epoch 00201: val_loss improved from 0.44603 to 0.44071, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1530 - acc: 0.9667 - val_loss: 0.4407 - val_acc: 0.7500\n",
            "Epoch 202/500\n",
            "\n",
            "Epoch 00202: val_loss did not improve from 0.44071\n",
            "60/60 - 0s - loss: 0.1546 - acc: 0.9833 - val_loss: 0.4427 - val_acc: 0.7500\n",
            "Epoch 203/500\n",
            "\n",
            "Epoch 00203: val_loss did not improve from 0.44071\n",
            "60/60 - 0s - loss: 0.1940 - acc: 0.9333 - val_loss: 0.4458 - val_acc: 0.7500\n",
            "Epoch 204/500\n",
            "\n",
            "Epoch 00204: val_loss did not improve from 0.44071\n",
            "60/60 - 0s - loss: 0.1362 - acc: 0.9833 - val_loss: 0.4644 - val_acc: 0.7500\n",
            "Epoch 205/500\n",
            "\n",
            "Epoch 00205: val_loss did not improve from 0.44071\n",
            "60/60 - 0s - loss: 0.1884 - acc: 0.9667 - val_loss: 0.4781 - val_acc: 0.7500\n",
            "Epoch 206/500\n",
            "\n",
            "Epoch 00206: val_loss did not improve from 0.44071\n",
            "60/60 - 0s - loss: 0.1333 - acc: 0.9667 - val_loss: 0.4867 - val_acc: 0.7500\n",
            "Epoch 207/500\n",
            "\n",
            "Epoch 00207: val_loss did not improve from 0.44071\n",
            "60/60 - 0s - loss: 0.1535 - acc: 0.9833 - val_loss: 0.4906 - val_acc: 0.7000\n",
            "Epoch 208/500\n",
            "\n",
            "Epoch 00208: val_loss did not improve from 0.44071\n",
            "60/60 - 0s - loss: 0.1467 - acc: 0.9667 - val_loss: 0.4893 - val_acc: 0.7000\n",
            "Epoch 209/500\n",
            "\n",
            "Epoch 00209: val_loss did not improve from 0.44071\n",
            "60/60 - 0s - loss: 0.1872 - acc: 0.9500 - val_loss: 0.4920 - val_acc: 0.7000\n",
            "Epoch 210/500\n",
            "\n",
            "Epoch 00210: val_loss did not improve from 0.44071\n",
            "60/60 - 0s - loss: 0.1733 - acc: 0.9500 - val_loss: 0.5060 - val_acc: 0.7000\n",
            "Epoch 211/500\n",
            "\n",
            "Epoch 00211: val_loss did not improve from 0.44071\n",
            "60/60 - 0s - loss: 0.1732 - acc: 0.9500 - val_loss: 0.5119 - val_acc: 0.7000\n",
            "Epoch 212/500\n",
            "\n",
            "Epoch 00212: val_loss did not improve from 0.44071\n",
            "60/60 - 0s - loss: 0.1784 - acc: 0.9500 - val_loss: 0.5110 - val_acc: 0.7000\n",
            "Epoch 213/500\n",
            "\n",
            "Epoch 00213: val_loss did not improve from 0.44071\n",
            "60/60 - 0s - loss: 0.1230 - acc: 0.9667 - val_loss: 0.5149 - val_acc: 0.7000\n",
            "Epoch 214/500\n",
            "\n",
            "Epoch 00214: val_loss did not improve from 0.44071\n",
            "60/60 - 0s - loss: 0.1432 - acc: 0.9833 - val_loss: 0.5156 - val_acc: 0.7000\n",
            "Epoch 215/500\n",
            "\n",
            "Epoch 00215: val_loss did not improve from 0.44071\n",
            "60/60 - 0s - loss: 0.1588 - acc: 0.9667 - val_loss: 0.5332 - val_acc: 0.7000\n",
            "Epoch 216/500\n",
            "\n",
            "Epoch 00216: val_loss did not improve from 0.44071\n",
            "60/60 - 0s - loss: 0.1292 - acc: 0.9833 - val_loss: 0.5374 - val_acc: 0.7000\n",
            "Epoch 217/500\n",
            "\n",
            "Epoch 00217: val_loss did not improve from 0.44071\n",
            "60/60 - 0s - loss: 0.1948 - acc: 0.9333 - val_loss: 0.5209 - val_acc: 0.7000\n",
            "Epoch 218/500\n",
            "\n",
            "Epoch 00218: val_loss did not improve from 0.44071\n",
            "60/60 - 0s - loss: 0.1733 - acc: 0.9333 - val_loss: 0.5023 - val_acc: 0.7000\n",
            "Epoch 219/500\n",
            "\n",
            "Epoch 00219: val_loss did not improve from 0.44071\n",
            "60/60 - 0s - loss: 0.1693 - acc: 0.9333 - val_loss: 0.4906 - val_acc: 0.7000\n",
            "Epoch 220/500\n",
            "\n",
            "Epoch 00220: val_loss did not improve from 0.44071\n",
            "60/60 - 0s - loss: 0.1510 - acc: 0.9500 - val_loss: 0.4995 - val_acc: 0.7000\n",
            "Epoch 221/500\n",
            "\n",
            "Epoch 00221: val_loss did not improve from 0.44071\n",
            "60/60 - 0s - loss: 0.1168 - acc: 0.9667 - val_loss: 0.5041 - val_acc: 0.7000\n",
            "Epoch 222/500\n",
            "\n",
            "Epoch 00222: val_loss did not improve from 0.44071\n",
            "60/60 - 0s - loss: 0.1390 - acc: 0.9833 - val_loss: 0.5115 - val_acc: 0.7000\n",
            "Epoch 223/500\n",
            "\n",
            "Epoch 00223: val_loss did not improve from 0.44071\n",
            "60/60 - 0s - loss: 0.1599 - acc: 0.9500 - val_loss: 0.5202 - val_acc: 0.7000\n",
            "Epoch 224/500\n",
            "\n",
            "Epoch 00224: val_loss did not improve from 0.44071\n",
            "60/60 - 0s - loss: 0.1721 - acc: 0.9667 - val_loss: 0.5472 - val_acc: 0.7000\n",
            "Epoch 225/500\n",
            "\n",
            "Epoch 00225: val_loss did not improve from 0.44071\n",
            "60/60 - 0s - loss: 0.2315 - acc: 0.9167 - val_loss: 0.5399 - val_acc: 0.7000\n",
            "Epoch 226/500\n",
            "\n",
            "Epoch 00226: val_loss did not improve from 0.44071\n",
            "60/60 - 0s - loss: 0.1383 - acc: 0.9833 - val_loss: 0.5200 - val_acc: 0.7000\n",
            "Epoch 227/500\n",
            "\n",
            "Epoch 00227: val_loss did not improve from 0.44071\n",
            "60/60 - 0s - loss: 0.1295 - acc: 0.9833 - val_loss: 0.5206 - val_acc: 0.7000\n",
            "Epoch 228/500\n",
            "\n",
            "Epoch 00228: val_loss did not improve from 0.44071\n",
            "60/60 - 0s - loss: 0.1521 - acc: 0.9667 - val_loss: 0.5174 - val_acc: 0.7000\n",
            "Epoch 229/500\n",
            "\n",
            "Epoch 00229: val_loss did not improve from 0.44071\n",
            "60/60 - 0s - loss: 0.1471 - acc: 0.9500 - val_loss: 0.5258 - val_acc: 0.7000\n",
            "Epoch 230/500\n",
            "\n",
            "Epoch 00230: val_loss did not improve from 0.44071\n",
            "60/60 - 0s - loss: 0.1557 - acc: 0.9667 - val_loss: 0.5419 - val_acc: 0.7000\n",
            "Epoch 231/500\n",
            "\n",
            "Epoch 00231: val_loss did not improve from 0.44071\n",
            "60/60 - 0s - loss: 0.1377 - acc: 0.9833 - val_loss: 0.5515 - val_acc: 0.7000\n",
            "Epoch 232/500\n",
            "\n",
            "Epoch 00232: val_loss did not improve from 0.44071\n",
            "60/60 - 0s - loss: 0.1411 - acc: 0.9667 - val_loss: 0.5766 - val_acc: 0.7000\n",
            "Epoch 233/500\n",
            "\n",
            "Epoch 00233: val_loss did not improve from 0.44071\n",
            "60/60 - 0s - loss: 0.1335 - acc: 0.9667 - val_loss: 0.5859 - val_acc: 0.7000\n",
            "Epoch 234/500\n",
            "\n",
            "Epoch 00234: val_loss did not improve from 0.44071\n",
            "60/60 - 0s - loss: 0.1231 - acc: 0.9833 - val_loss: 0.5854 - val_acc: 0.7000\n",
            "Epoch 235/500\n",
            "\n",
            "Epoch 00235: val_loss did not improve from 0.44071\n",
            "60/60 - 0s - loss: 0.1744 - acc: 0.9500 - val_loss: 0.5779 - val_acc: 0.7000\n",
            "Epoch 236/500\n",
            "\n",
            "Epoch 00236: val_loss did not improve from 0.44071\n",
            "60/60 - 0s - loss: 0.1678 - acc: 0.9500 - val_loss: 0.5186 - val_acc: 0.7000\n",
            "Epoch 237/500\n",
            "\n",
            "Epoch 00237: val_loss did not improve from 0.44071\n",
            "60/60 - 0s - loss: 0.1231 - acc: 0.9833 - val_loss: 0.4960 - val_acc: 0.7000\n",
            "Epoch 238/500\n",
            "\n",
            "Epoch 00238: val_loss did not improve from 0.44071\n",
            "60/60 - 0s - loss: 0.1297 - acc: 1.0000 - val_loss: 0.4881 - val_acc: 0.7000\n",
            "Epoch 239/500\n",
            "\n",
            "Epoch 00239: val_loss did not improve from 0.44071\n",
            "60/60 - 0s - loss: 0.2295 - acc: 0.9500 - val_loss: 0.4498 - val_acc: 0.7500\n",
            "Epoch 240/500\n",
            "\n",
            "Epoch 00240: val_loss improved from 0.44071 to 0.41923, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2237 - acc: 0.9000 - val_loss: 0.4192 - val_acc: 0.7500\n",
            "Epoch 241/500\n",
            "\n",
            "Epoch 00241: val_loss improved from 0.41923 to 0.41903, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1918 - acc: 0.9500 - val_loss: 0.4190 - val_acc: 0.7000\n",
            "Epoch 242/500\n",
            "\n",
            "Epoch 00242: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.1600 - acc: 0.9667 - val_loss: 0.4208 - val_acc: 0.7000\n",
            "Epoch 243/500\n",
            "\n",
            "Epoch 00243: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.1943 - acc: 0.9667 - val_loss: 0.4231 - val_acc: 0.7000\n",
            "Epoch 244/500\n",
            "\n",
            "Epoch 00244: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.1291 - acc: 1.0000 - val_loss: 0.4308 - val_acc: 0.7000\n",
            "Epoch 245/500\n",
            "\n",
            "Epoch 00245: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.1114 - acc: 1.0000 - val_loss: 0.4499 - val_acc: 0.7000\n",
            "Epoch 246/500\n",
            "\n",
            "Epoch 00246: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.1542 - acc: 0.9667 - val_loss: 0.4648 - val_acc: 0.7000\n",
            "Epoch 247/500\n",
            "\n",
            "Epoch 00247: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.1193 - acc: 1.0000 - val_loss: 0.4865 - val_acc: 0.7500\n",
            "Epoch 248/500\n",
            "\n",
            "Epoch 00248: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.1777 - acc: 0.9500 - val_loss: 0.4950 - val_acc: 0.7000\n",
            "Epoch 249/500\n",
            "\n",
            "Epoch 00249: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.1330 - acc: 0.9667 - val_loss: 0.5070 - val_acc: 0.7000\n",
            "Epoch 250/500\n",
            "\n",
            "Epoch 00250: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.1162 - acc: 0.9667 - val_loss: 0.5317 - val_acc: 0.7000\n",
            "Epoch 251/500\n",
            "\n",
            "Epoch 00251: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.1709 - acc: 0.9667 - val_loss: 0.5242 - val_acc: 0.7000\n",
            "Epoch 252/500\n",
            "\n",
            "Epoch 00252: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.1656 - acc: 0.9667 - val_loss: 0.5411 - val_acc: 0.7000\n",
            "Epoch 253/500\n",
            "\n",
            "Epoch 00253: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.1873 - acc: 0.9667 - val_loss: 0.5458 - val_acc: 0.7000\n",
            "Epoch 254/500\n",
            "\n",
            "Epoch 00254: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.1693 - acc: 0.9500 - val_loss: 0.5505 - val_acc: 0.7000\n",
            "Epoch 255/500\n",
            "\n",
            "Epoch 00255: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.1929 - acc: 0.9500 - val_loss: 0.5495 - val_acc: 0.7000\n",
            "Epoch 256/500\n",
            "\n",
            "Epoch 00256: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.1217 - acc: 0.9833 - val_loss: 0.5491 - val_acc: 0.7000\n",
            "Epoch 257/500\n",
            "\n",
            "Epoch 00257: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.1175 - acc: 1.0000 - val_loss: 0.5595 - val_acc: 0.7000\n",
            "Epoch 258/500\n",
            "\n",
            "Epoch 00258: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.1463 - acc: 0.9833 - val_loss: 0.5864 - val_acc: 0.7000\n",
            "Epoch 259/500\n",
            "\n",
            "Epoch 00259: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.1592 - acc: 0.9667 - val_loss: 0.6213 - val_acc: 0.7000\n",
            "Epoch 260/500\n",
            "\n",
            "Epoch 00260: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.1136 - acc: 0.9667 - val_loss: 0.6101 - val_acc: 0.7000\n",
            "Epoch 261/500\n",
            "\n",
            "Epoch 00261: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.1117 - acc: 0.9667 - val_loss: 0.5973 - val_acc: 0.7000\n",
            "Epoch 262/500\n",
            "\n",
            "Epoch 00262: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.1750 - acc: 0.9667 - val_loss: 0.5783 - val_acc: 0.7000\n",
            "Epoch 263/500\n",
            "\n",
            "Epoch 00263: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.1450 - acc: 0.9667 - val_loss: 0.5641 - val_acc: 0.7000\n",
            "Epoch 264/500\n",
            "\n",
            "Epoch 00264: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.1432 - acc: 0.9833 - val_loss: 0.5524 - val_acc: 0.7000\n",
            "Epoch 265/500\n",
            "\n",
            "Epoch 00265: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.1507 - acc: 0.9667 - val_loss: 0.5746 - val_acc: 0.7000\n",
            "Epoch 266/500\n",
            "\n",
            "Epoch 00266: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.1158 - acc: 0.9833 - val_loss: 0.5991 - val_acc: 0.7000\n",
            "Epoch 267/500\n",
            "\n",
            "Epoch 00267: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.1225 - acc: 0.9833 - val_loss: 0.5987 - val_acc: 0.7000\n",
            "Epoch 268/500\n",
            "\n",
            "Epoch 00268: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.1496 - acc: 0.9500 - val_loss: 0.5888 - val_acc: 0.7000\n",
            "Epoch 269/500\n",
            "\n",
            "Epoch 00269: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.0919 - acc: 0.9833 - val_loss: 0.5869 - val_acc: 0.7000\n",
            "Epoch 270/500\n",
            "\n",
            "Epoch 00270: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.1291 - acc: 0.9667 - val_loss: 0.5915 - val_acc: 0.7000\n",
            "Epoch 271/500\n",
            "\n",
            "Epoch 00271: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.0938 - acc: 0.9833 - val_loss: 0.5922 - val_acc: 0.7000\n",
            "Epoch 272/500\n",
            "\n",
            "Epoch 00272: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.1323 - acc: 0.9833 - val_loss: 0.5856 - val_acc: 0.7000\n",
            "Epoch 273/500\n",
            "\n",
            "Epoch 00273: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.1124 - acc: 0.9667 - val_loss: 0.5866 - val_acc: 0.7000\n",
            "Epoch 274/500\n",
            "\n",
            "Epoch 00274: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.1001 - acc: 1.0000 - val_loss: 0.5988 - val_acc: 0.7000\n",
            "Epoch 275/500\n",
            "\n",
            "Epoch 00275: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.1585 - acc: 0.9667 - val_loss: 0.6288 - val_acc: 0.7000\n",
            "Epoch 276/500\n",
            "\n",
            "Epoch 00276: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.1264 - acc: 0.9833 - val_loss: 0.6639 - val_acc: 0.7000\n",
            "Epoch 277/500\n",
            "\n",
            "Epoch 00277: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.1202 - acc: 0.9833 - val_loss: 0.6860 - val_acc: 0.7000\n",
            "Epoch 278/500\n",
            "\n",
            "Epoch 00278: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.1036 - acc: 0.9833 - val_loss: 0.7117 - val_acc: 0.7000\n",
            "Epoch 279/500\n",
            "\n",
            "Epoch 00279: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.1178 - acc: 0.9667 - val_loss: 0.7283 - val_acc: 0.7000\n",
            "Epoch 280/500\n",
            "\n",
            "Epoch 00280: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.1442 - acc: 0.9667 - val_loss: 0.6793 - val_acc: 0.7000\n",
            "Epoch 281/500\n",
            "\n",
            "Epoch 00281: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.1116 - acc: 0.9833 - val_loss: 0.6628 - val_acc: 0.7000\n",
            "Epoch 282/500\n",
            "\n",
            "Epoch 00282: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.0930 - acc: 1.0000 - val_loss: 0.6238 - val_acc: 0.7000\n",
            "Epoch 283/500\n",
            "\n",
            "Epoch 00283: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.1296 - acc: 0.9500 - val_loss: 0.6260 - val_acc: 0.7000\n",
            "Epoch 284/500\n",
            "\n",
            "Epoch 00284: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.1126 - acc: 1.0000 - val_loss: 0.6532 - val_acc: 0.7000\n",
            "Epoch 285/500\n",
            "\n",
            "Epoch 00285: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.1095 - acc: 0.9667 - val_loss: 0.6895 - val_acc: 0.7000\n",
            "Epoch 286/500\n",
            "\n",
            "Epoch 00286: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.1340 - acc: 0.9667 - val_loss: 0.6735 - val_acc: 0.7000\n",
            "Epoch 287/500\n",
            "\n",
            "Epoch 00287: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.2063 - acc: 0.9500 - val_loss: 0.6704 - val_acc: 0.7000\n",
            "Epoch 288/500\n",
            "\n",
            "Epoch 00288: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.1608 - acc: 0.9333 - val_loss: 0.6335 - val_acc: 0.7000\n",
            "Epoch 289/500\n",
            "\n",
            "Epoch 00289: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.1306 - acc: 0.9667 - val_loss: 0.6243 - val_acc: 0.7000\n",
            "Epoch 290/500\n",
            "\n",
            "Epoch 00290: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.1405 - acc: 0.9500 - val_loss: 0.6064 - val_acc: 0.7000\n",
            "Epoch 291/500\n",
            "\n",
            "Epoch 00291: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.1217 - acc: 0.9500 - val_loss: 0.6060 - val_acc: 0.7000\n",
            "Epoch 292/500\n",
            "\n",
            "Epoch 00292: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.1711 - acc: 0.9333 - val_loss: 0.6125 - val_acc: 0.7000\n",
            "Epoch 293/500\n",
            "\n",
            "Epoch 00293: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.1166 - acc: 0.9833 - val_loss: 0.6188 - val_acc: 0.7000\n",
            "Epoch 294/500\n",
            "\n",
            "Epoch 00294: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.1215 - acc: 0.9667 - val_loss: 0.6248 - val_acc: 0.7000\n",
            "Epoch 295/500\n",
            "\n",
            "Epoch 00295: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.0914 - acc: 0.9833 - val_loss: 0.6336 - val_acc: 0.7000\n",
            "Epoch 296/500\n",
            "\n",
            "Epoch 00296: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.1046 - acc: 1.0000 - val_loss: 0.6704 - val_acc: 0.7000\n",
            "Epoch 297/500\n",
            "\n",
            "Epoch 00297: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.1492 - acc: 0.9667 - val_loss: 0.6923 - val_acc: 0.7000\n",
            "Epoch 298/500\n",
            "\n",
            "Epoch 00298: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.1073 - acc: 0.9667 - val_loss: 0.6935 - val_acc: 0.7000\n",
            "Epoch 299/500\n",
            "\n",
            "Epoch 00299: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.1538 - acc: 0.9667 - val_loss: 0.6861 - val_acc: 0.7000\n",
            "Epoch 300/500\n",
            "\n",
            "Epoch 00300: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.1739 - acc: 0.9167 - val_loss: 0.6592 - val_acc: 0.7000\n",
            "Epoch 301/500\n",
            "\n",
            "Epoch 00301: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.2159 - acc: 0.9167 - val_loss: 0.6249 - val_acc: 0.7000\n",
            "Epoch 302/500\n",
            "\n",
            "Epoch 00302: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.1429 - acc: 0.9667 - val_loss: 0.5916 - val_acc: 0.7000\n",
            "Epoch 303/500\n",
            "\n",
            "Epoch 00303: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.2130 - acc: 0.9333 - val_loss: 0.5529 - val_acc: 0.7000\n",
            "Epoch 304/500\n",
            "\n",
            "Epoch 00304: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.1766 - acc: 0.9667 - val_loss: 0.5486 - val_acc: 0.7500\n",
            "Epoch 305/500\n",
            "\n",
            "Epoch 00305: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.1557 - acc: 0.9500 - val_loss: 0.5665 - val_acc: 0.7500\n",
            "Epoch 306/500\n",
            "\n",
            "Epoch 00306: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.1451 - acc: 0.9500 - val_loss: 0.5726 - val_acc: 0.7500\n",
            "Epoch 307/500\n",
            "\n",
            "Epoch 00307: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.1186 - acc: 1.0000 - val_loss: 0.5526 - val_acc: 0.7000\n",
            "Epoch 308/500\n",
            "\n",
            "Epoch 00308: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.1146 - acc: 1.0000 - val_loss: 0.5774 - val_acc: 0.7000\n",
            "Epoch 309/500\n",
            "\n",
            "Epoch 00309: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.1062 - acc: 0.9833 - val_loss: 0.5768 - val_acc: 0.7000\n",
            "Epoch 310/500\n",
            "\n",
            "Epoch 00310: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.1144 - acc: 0.9833 - val_loss: 0.5708 - val_acc: 0.7000\n",
            "Epoch 311/500\n",
            "\n",
            "Epoch 00311: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.1241 - acc: 0.9667 - val_loss: 0.5669 - val_acc: 0.7000\n",
            "Epoch 312/500\n",
            "\n",
            "Epoch 00312: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.1479 - acc: 0.9833 - val_loss: 0.5680 - val_acc: 0.7000\n",
            "Epoch 313/500\n",
            "\n",
            "Epoch 00313: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.0792 - acc: 1.0000 - val_loss: 0.5856 - val_acc: 0.7000\n",
            "Epoch 314/500\n",
            "\n",
            "Epoch 00314: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.1366 - acc: 0.9667 - val_loss: 0.6146 - val_acc: 0.7000\n",
            "Epoch 315/500\n",
            "\n",
            "Epoch 00315: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.0881 - acc: 1.0000 - val_loss: 0.6528 - val_acc: 0.7000\n",
            "Epoch 316/500\n",
            "\n",
            "Epoch 00316: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.1614 - acc: 0.9167 - val_loss: 0.6581 - val_acc: 0.7000\n",
            "Epoch 317/500\n",
            "\n",
            "Epoch 00317: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.1140 - acc: 0.9833 - val_loss: 0.6428 - val_acc: 0.7000\n",
            "Epoch 318/500\n",
            "\n",
            "Epoch 00318: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.1480 - acc: 1.0000 - val_loss: 0.6379 - val_acc: 0.7000\n",
            "Epoch 319/500\n",
            "\n",
            "Epoch 00319: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.1311 - acc: 0.9500 - val_loss: 0.6708 - val_acc: 0.7000\n",
            "Epoch 320/500\n",
            "\n",
            "Epoch 00320: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.1192 - acc: 0.9667 - val_loss: 0.7024 - val_acc: 0.7000\n",
            "Epoch 321/500\n",
            "\n",
            "Epoch 00321: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.0970 - acc: 0.9833 - val_loss: 0.7208 - val_acc: 0.7000\n",
            "Epoch 322/500\n",
            "\n",
            "Epoch 00322: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.1043 - acc: 0.9833 - val_loss: 0.7261 - val_acc: 0.7000\n",
            "Epoch 323/500\n",
            "\n",
            "Epoch 00323: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.1499 - acc: 0.9333 - val_loss: 0.7215 - val_acc: 0.7000\n",
            "Epoch 324/500\n",
            "\n",
            "Epoch 00324: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.0921 - acc: 0.9833 - val_loss: 0.7020 - val_acc: 0.7000\n",
            "Epoch 325/500\n",
            "\n",
            "Epoch 00325: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.1106 - acc: 1.0000 - val_loss: 0.6718 - val_acc: 0.7000\n",
            "Epoch 326/500\n",
            "\n",
            "Epoch 00326: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.0790 - acc: 0.9833 - val_loss: 0.6251 - val_acc: 0.7000\n",
            "Epoch 327/500\n",
            "\n",
            "Epoch 00327: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.0903 - acc: 1.0000 - val_loss: 0.6143 - val_acc: 0.7000\n",
            "Epoch 328/500\n",
            "\n",
            "Epoch 00328: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.1338 - acc: 0.9333 - val_loss: 0.5860 - val_acc: 0.7000\n",
            "Epoch 329/500\n",
            "\n",
            "Epoch 00329: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.1065 - acc: 0.9833 - val_loss: 0.5745 - val_acc: 0.7000\n",
            "Epoch 330/500\n",
            "\n",
            "Epoch 00330: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.0713 - acc: 1.0000 - val_loss: 0.5747 - val_acc: 0.7000\n",
            "Epoch 331/500\n",
            "\n",
            "Epoch 00331: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.1211 - acc: 0.9667 - val_loss: 0.5812 - val_acc: 0.7000\n",
            "Epoch 332/500\n",
            "\n",
            "Epoch 00332: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.1621 - acc: 0.9500 - val_loss: 0.5625 - val_acc: 0.7000\n",
            "Epoch 333/500\n",
            "\n",
            "Epoch 00333: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.1648 - acc: 0.9667 - val_loss: 0.5637 - val_acc: 0.7000\n",
            "Epoch 334/500\n",
            "\n",
            "Epoch 00334: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.0903 - acc: 0.9833 - val_loss: 0.5826 - val_acc: 0.7000\n",
            "Epoch 335/500\n",
            "\n",
            "Epoch 00335: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.1415 - acc: 0.9667 - val_loss: 0.6130 - val_acc: 0.7000\n",
            "Epoch 336/500\n",
            "\n",
            "Epoch 00336: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.0968 - acc: 1.0000 - val_loss: 0.6495 - val_acc: 0.6500\n",
            "Epoch 337/500\n",
            "\n",
            "Epoch 00337: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.1071 - acc: 1.0000 - val_loss: 0.6820 - val_acc: 0.6500\n",
            "Epoch 338/500\n",
            "\n",
            "Epoch 00338: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.1735 - acc: 0.9500 - val_loss: 0.6954 - val_acc: 0.6500\n",
            "Epoch 339/500\n",
            "\n",
            "Epoch 00339: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.0818 - acc: 1.0000 - val_loss: 0.6547 - val_acc: 0.6500\n",
            "Epoch 340/500\n",
            "\n",
            "Epoch 00340: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.1259 - acc: 0.9667 - val_loss: 0.5995 - val_acc: 0.7000\n",
            "Epoch 341/500\n",
            "\n",
            "Epoch 00341: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.1199 - acc: 0.9833 - val_loss: 0.5721 - val_acc: 0.7000\n",
            "Epoch 342/500\n",
            "\n",
            "Epoch 00342: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.0945 - acc: 0.9833 - val_loss: 0.5685 - val_acc: 0.7000\n",
            "Epoch 343/500\n",
            "\n",
            "Epoch 00343: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.0989 - acc: 1.0000 - val_loss: 0.5799 - val_acc: 0.7000\n",
            "Epoch 344/500\n",
            "\n",
            "Epoch 00344: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.1088 - acc: 1.0000 - val_loss: 0.5910 - val_acc: 0.7000\n",
            "Epoch 345/500\n",
            "\n",
            "Epoch 00345: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.1216 - acc: 0.9500 - val_loss: 0.5982 - val_acc: 0.7000\n",
            "Epoch 346/500\n",
            "\n",
            "Epoch 00346: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.0908 - acc: 1.0000 - val_loss: 0.5963 - val_acc: 0.7000\n",
            "Epoch 347/500\n",
            "\n",
            "Epoch 00347: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.1034 - acc: 0.9833 - val_loss: 0.6077 - val_acc: 0.7000\n",
            "Epoch 348/500\n",
            "\n",
            "Epoch 00348: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.0843 - acc: 0.9833 - val_loss: 0.6475 - val_acc: 0.7000\n",
            "Epoch 349/500\n",
            "\n",
            "Epoch 00349: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.0917 - acc: 1.0000 - val_loss: 0.6618 - val_acc: 0.7000\n",
            "Epoch 350/500\n",
            "\n",
            "Epoch 00350: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.0913 - acc: 1.0000 - val_loss: 0.6881 - val_acc: 0.7000\n",
            "Epoch 351/500\n",
            "\n",
            "Epoch 00351: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.1051 - acc: 0.9667 - val_loss: 0.7290 - val_acc: 0.6500\n",
            "Epoch 352/500\n",
            "\n",
            "Epoch 00352: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.0721 - acc: 1.0000 - val_loss: 0.7441 - val_acc: 0.6500\n",
            "Epoch 353/500\n",
            "\n",
            "Epoch 00353: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.1236 - acc: 1.0000 - val_loss: 0.6943 - val_acc: 0.6500\n",
            "Epoch 354/500\n",
            "\n",
            "Epoch 00354: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.1093 - acc: 1.0000 - val_loss: 0.6812 - val_acc: 0.7000\n",
            "Epoch 355/500\n",
            "\n",
            "Epoch 00355: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.1221 - acc: 0.9667 - val_loss: 0.6331 - val_acc: 0.7000\n",
            "Epoch 356/500\n",
            "\n",
            "Epoch 00356: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.1159 - acc: 0.9833 - val_loss: 0.6148 - val_acc: 0.7000\n",
            "Epoch 357/500\n",
            "\n",
            "Epoch 00357: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.0932 - acc: 0.9667 - val_loss: 0.5988 - val_acc: 0.7000\n",
            "Epoch 358/500\n",
            "\n",
            "Epoch 00358: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.0963 - acc: 0.9833 - val_loss: 0.6266 - val_acc: 0.7000\n",
            "Epoch 359/500\n",
            "\n",
            "Epoch 00359: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.0896 - acc: 0.9833 - val_loss: 0.6574 - val_acc: 0.7000\n",
            "Epoch 360/500\n",
            "\n",
            "Epoch 00360: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.0940 - acc: 0.9833 - val_loss: 0.6732 - val_acc: 0.7000\n",
            "Epoch 361/500\n",
            "\n",
            "Epoch 00361: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.1242 - acc: 0.9833 - val_loss: 0.6935 - val_acc: 0.7000\n",
            "Epoch 362/500\n",
            "\n",
            "Epoch 00362: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.1104 - acc: 0.9833 - val_loss: 0.7249 - val_acc: 0.7000\n",
            "Epoch 363/500\n",
            "\n",
            "Epoch 00363: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.0962 - acc: 0.9833 - val_loss: 0.7393 - val_acc: 0.7000\n",
            "Epoch 364/500\n",
            "\n",
            "Epoch 00364: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.1295 - acc: 0.9500 - val_loss: 0.7168 - val_acc: 0.7000\n",
            "Epoch 365/500\n",
            "\n",
            "Epoch 00365: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.0795 - acc: 1.0000 - val_loss: 0.7016 - val_acc: 0.7000\n",
            "Epoch 366/500\n",
            "\n",
            "Epoch 00366: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.1218 - acc: 1.0000 - val_loss: 0.6866 - val_acc: 0.7000\n",
            "Epoch 367/500\n",
            "\n",
            "Epoch 00367: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.0921 - acc: 1.0000 - val_loss: 0.6983 - val_acc: 0.7000\n",
            "Epoch 368/500\n",
            "\n",
            "Epoch 00368: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.0821 - acc: 0.9833 - val_loss: 0.7192 - val_acc: 0.6500\n",
            "Epoch 369/500\n",
            "\n",
            "Epoch 00369: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.1455 - acc: 0.9667 - val_loss: 0.7279 - val_acc: 0.7000\n",
            "Epoch 370/500\n",
            "\n",
            "Epoch 00370: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.0871 - acc: 1.0000 - val_loss: 0.7420 - val_acc: 0.7000\n",
            "Epoch 371/500\n",
            "\n",
            "Epoch 00371: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.0942 - acc: 0.9833 - val_loss: 0.7468 - val_acc: 0.7000\n",
            "Epoch 372/500\n",
            "\n",
            "Epoch 00372: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.0809 - acc: 1.0000 - val_loss: 0.7561 - val_acc: 0.7000\n",
            "Epoch 373/500\n",
            "\n",
            "Epoch 00373: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.1017 - acc: 0.9833 - val_loss: 0.7890 - val_acc: 0.6500\n",
            "Epoch 374/500\n",
            "\n",
            "Epoch 00374: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.0920 - acc: 0.9833 - val_loss: 0.8212 - val_acc: 0.6500\n",
            "Epoch 375/500\n",
            "\n",
            "Epoch 00375: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.0812 - acc: 1.0000 - val_loss: 0.8373 - val_acc: 0.6500\n",
            "Epoch 376/500\n",
            "\n",
            "Epoch 00376: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.1478 - acc: 0.9667 - val_loss: 0.8229 - val_acc: 0.7000\n",
            "Epoch 377/500\n",
            "\n",
            "Epoch 00377: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.1015 - acc: 0.9667 - val_loss: 0.8268 - val_acc: 0.7000\n",
            "Epoch 378/500\n",
            "\n",
            "Epoch 00378: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.1386 - acc: 0.9500 - val_loss: 0.8173 - val_acc: 0.7000\n",
            "Epoch 379/500\n",
            "\n",
            "Epoch 00379: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.0906 - acc: 0.9833 - val_loss: 0.7462 - val_acc: 0.6500\n",
            "Epoch 380/500\n",
            "\n",
            "Epoch 00380: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.1116 - acc: 0.9500 - val_loss: 0.6647 - val_acc: 0.7000\n",
            "Epoch 381/500\n",
            "\n",
            "Epoch 00381: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.1141 - acc: 1.0000 - val_loss: 0.6431 - val_acc: 0.7000\n",
            "Epoch 382/500\n",
            "\n",
            "Epoch 00382: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.1091 - acc: 1.0000 - val_loss: 0.6503 - val_acc: 0.7000\n",
            "Epoch 383/500\n",
            "\n",
            "Epoch 00383: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.1100 - acc: 0.9833 - val_loss: 0.6851 - val_acc: 0.7000\n",
            "Epoch 384/500\n",
            "\n",
            "Epoch 00384: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.0904 - acc: 0.9833 - val_loss: 0.7010 - val_acc: 0.7000\n",
            "Epoch 385/500\n",
            "\n",
            "Epoch 00385: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.0939 - acc: 0.9833 - val_loss: 0.7212 - val_acc: 0.6500\n",
            "Epoch 386/500\n",
            "\n",
            "Epoch 00386: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.1156 - acc: 0.9667 - val_loss: 0.7388 - val_acc: 0.6500\n",
            "Epoch 387/500\n",
            "\n",
            "Epoch 00387: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.0938 - acc: 0.9667 - val_loss: 0.7235 - val_acc: 0.6500\n",
            "Epoch 388/500\n",
            "\n",
            "Epoch 00388: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.1028 - acc: 0.9833 - val_loss: 0.6911 - val_acc: 0.6500\n",
            "Epoch 389/500\n",
            "\n",
            "Epoch 00389: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.0722 - acc: 0.9833 - val_loss: 0.6914 - val_acc: 0.6500\n",
            "Epoch 390/500\n",
            "\n",
            "Epoch 00390: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.0983 - acc: 1.0000 - val_loss: 0.6643 - val_acc: 0.7000\n",
            "Epoch 391/500\n",
            "\n",
            "Epoch 00391: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.1013 - acc: 0.9833 - val_loss: 0.6188 - val_acc: 0.7000\n",
            "Epoch 392/500\n",
            "\n",
            "Epoch 00392: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.1034 - acc: 0.9833 - val_loss: 0.6354 - val_acc: 0.7000\n",
            "Epoch 393/500\n",
            "\n",
            "Epoch 00393: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.0952 - acc: 0.9833 - val_loss: 0.6552 - val_acc: 0.7000\n",
            "Epoch 394/500\n",
            "\n",
            "Epoch 00394: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.0895 - acc: 0.9833 - val_loss: 0.6885 - val_acc: 0.7000\n",
            "Epoch 395/500\n",
            "\n",
            "Epoch 00395: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.0837 - acc: 0.9833 - val_loss: 0.7191 - val_acc: 0.7000\n",
            "Epoch 396/500\n",
            "\n",
            "Epoch 00396: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.1730 - acc: 0.9500 - val_loss: 0.7100 - val_acc: 0.7000\n",
            "Epoch 397/500\n",
            "\n",
            "Epoch 00397: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.0749 - acc: 1.0000 - val_loss: 0.7252 - val_acc: 0.6500\n",
            "Epoch 398/500\n",
            "\n",
            "Epoch 00398: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.0951 - acc: 1.0000 - val_loss: 0.7338 - val_acc: 0.6500\n",
            "Epoch 399/500\n",
            "\n",
            "Epoch 00399: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.0750 - acc: 1.0000 - val_loss: 0.7324 - val_acc: 0.6500\n",
            "Epoch 400/500\n",
            "\n",
            "Epoch 00400: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.1103 - acc: 1.0000 - val_loss: 0.7401 - val_acc: 0.6500\n",
            "Epoch 401/500\n",
            "\n",
            "Epoch 00401: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.0653 - acc: 1.0000 - val_loss: 0.7605 - val_acc: 0.6500\n",
            "Epoch 402/500\n",
            "\n",
            "Epoch 00402: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.0778 - acc: 0.9833 - val_loss: 0.7654 - val_acc: 0.6500\n",
            "Epoch 403/500\n",
            "\n",
            "Epoch 00403: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.0636 - acc: 1.0000 - val_loss: 0.7720 - val_acc: 0.7000\n",
            "Epoch 404/500\n",
            "\n",
            "Epoch 00404: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.0808 - acc: 0.9833 - val_loss: 0.7728 - val_acc: 0.7000\n",
            "Epoch 405/500\n",
            "\n",
            "Epoch 00405: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.0891 - acc: 0.9833 - val_loss: 0.8055 - val_acc: 0.6500\n",
            "Epoch 406/500\n",
            "\n",
            "Epoch 00406: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.0662 - acc: 1.0000 - val_loss: 0.8225 - val_acc: 0.6500\n",
            "Epoch 407/500\n",
            "\n",
            "Epoch 00407: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.0984 - acc: 0.9667 - val_loss: 0.8384 - val_acc: 0.6500\n",
            "Epoch 408/500\n",
            "\n",
            "Epoch 00408: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.0461 - acc: 1.0000 - val_loss: 0.8275 - val_acc: 0.6500\n",
            "Epoch 409/500\n",
            "\n",
            "Epoch 00409: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.0978 - acc: 0.9667 - val_loss: 0.8407 - val_acc: 0.6500\n",
            "Epoch 410/500\n",
            "\n",
            "Epoch 00410: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.1020 - acc: 1.0000 - val_loss: 0.8614 - val_acc: 0.6500\n",
            "Epoch 411/500\n",
            "\n",
            "Epoch 00411: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.0800 - acc: 0.9833 - val_loss: 0.8412 - val_acc: 0.6500\n",
            "Epoch 412/500\n",
            "\n",
            "Epoch 00412: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.0755 - acc: 1.0000 - val_loss: 0.8383 - val_acc: 0.6500\n",
            "Epoch 413/500\n",
            "\n",
            "Epoch 00413: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.0731 - acc: 1.0000 - val_loss: 0.8707 - val_acc: 0.6500\n",
            "Epoch 414/500\n",
            "\n",
            "Epoch 00414: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.0979 - acc: 0.9833 - val_loss: 0.8738 - val_acc: 0.6500\n",
            "Epoch 415/500\n",
            "\n",
            "Epoch 00415: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.0884 - acc: 1.0000 - val_loss: 0.8581 - val_acc: 0.6500\n",
            "Epoch 416/500\n",
            "\n",
            "Epoch 00416: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.1059 - acc: 0.9833 - val_loss: 0.8513 - val_acc: 0.6500\n",
            "Epoch 417/500\n",
            "\n",
            "Epoch 00417: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.1034 - acc: 1.0000 - val_loss: 0.8211 - val_acc: 0.6500\n",
            "Epoch 418/500\n",
            "\n",
            "Epoch 00418: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.0765 - acc: 1.0000 - val_loss: 0.8578 - val_acc: 0.6500\n",
            "Epoch 419/500\n",
            "\n",
            "Epoch 00419: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.0874 - acc: 1.0000 - val_loss: 0.8815 - val_acc: 0.6500\n",
            "Epoch 420/500\n",
            "\n",
            "Epoch 00420: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.1247 - acc: 0.9667 - val_loss: 0.9098 - val_acc: 0.6500\n",
            "Epoch 421/500\n",
            "\n",
            "Epoch 00421: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.0979 - acc: 0.9667 - val_loss: 0.9730 - val_acc: 0.6500\n",
            "Epoch 422/500\n",
            "\n",
            "Epoch 00422: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.0578 - acc: 1.0000 - val_loss: 1.0037 - val_acc: 0.6500\n",
            "Epoch 423/500\n",
            "\n",
            "Epoch 00423: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.0732 - acc: 1.0000 - val_loss: 1.0378 - val_acc: 0.6500\n",
            "Epoch 424/500\n",
            "\n",
            "Epoch 00424: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.0766 - acc: 0.9833 - val_loss: 1.0460 - val_acc: 0.6500\n",
            "Epoch 425/500\n",
            "\n",
            "Epoch 00425: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.0878 - acc: 0.9833 - val_loss: 1.0295 - val_acc: 0.6500\n",
            "Epoch 426/500\n",
            "\n",
            "Epoch 00426: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.0900 - acc: 1.0000 - val_loss: 1.0282 - val_acc: 0.6500\n",
            "Epoch 427/500\n",
            "\n",
            "Epoch 00427: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.0626 - acc: 1.0000 - val_loss: 1.0665 - val_acc: 0.6500\n",
            "Epoch 428/500\n",
            "\n",
            "Epoch 00428: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.1050 - acc: 0.9833 - val_loss: 1.0482 - val_acc: 0.6500\n",
            "Epoch 429/500\n",
            "\n",
            "Epoch 00429: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.1206 - acc: 0.9500 - val_loss: 0.9850 - val_acc: 0.6500\n",
            "Epoch 430/500\n",
            "\n",
            "Epoch 00430: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.1353 - acc: 0.9500 - val_loss: 0.9490 - val_acc: 0.6500\n",
            "Epoch 431/500\n",
            "\n",
            "Epoch 00431: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.0836 - acc: 0.9833 - val_loss: 0.9172 - val_acc: 0.6500\n",
            "Epoch 432/500\n",
            "\n",
            "Epoch 00432: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.1083 - acc: 0.9833 - val_loss: 0.8897 - val_acc: 0.6500\n",
            "Epoch 433/500\n",
            "\n",
            "Epoch 00433: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.0696 - acc: 1.0000 - val_loss: 0.8860 - val_acc: 0.6500\n",
            "Epoch 434/500\n",
            "\n",
            "Epoch 00434: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.0655 - acc: 1.0000 - val_loss: 0.9060 - val_acc: 0.6500\n",
            "Epoch 435/500\n",
            "\n",
            "Epoch 00435: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.1703 - acc: 0.9500 - val_loss: 0.9279 - val_acc: 0.6500\n",
            "Epoch 436/500\n",
            "\n",
            "Epoch 00436: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.0612 - acc: 1.0000 - val_loss: 0.9363 - val_acc: 0.6500\n",
            "Epoch 437/500\n",
            "\n",
            "Epoch 00437: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.1070 - acc: 0.9833 - val_loss: 0.9483 - val_acc: 0.6500\n",
            "Epoch 438/500\n",
            "\n",
            "Epoch 00438: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.1041 - acc: 0.9667 - val_loss: 0.9366 - val_acc: 0.6500\n",
            "Epoch 439/500\n",
            "\n",
            "Epoch 00439: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.0949 - acc: 0.9833 - val_loss: 0.9160 - val_acc: 0.6500\n",
            "Epoch 440/500\n",
            "\n",
            "Epoch 00440: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.0886 - acc: 1.0000 - val_loss: 0.9110 - val_acc: 0.6500\n",
            "Epoch 441/500\n",
            "\n",
            "Epoch 00441: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.1062 - acc: 0.9833 - val_loss: 0.8760 - val_acc: 0.6500\n",
            "Epoch 442/500\n",
            "\n",
            "Epoch 00442: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.0813 - acc: 1.0000 - val_loss: 0.8504 - val_acc: 0.6500\n",
            "Epoch 443/500\n",
            "\n",
            "Epoch 00443: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.1024 - acc: 0.9833 - val_loss: 0.8342 - val_acc: 0.6500\n",
            "Epoch 444/500\n",
            "\n",
            "Epoch 00444: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.1018 - acc: 0.9833 - val_loss: 0.7716 - val_acc: 0.6500\n",
            "Epoch 445/500\n",
            "\n",
            "Epoch 00445: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.0824 - acc: 1.0000 - val_loss: 0.7555 - val_acc: 0.6500\n",
            "Epoch 446/500\n",
            "\n",
            "Epoch 00446: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.0826 - acc: 0.9833 - val_loss: 0.7243 - val_acc: 0.6500\n",
            "Epoch 447/500\n",
            "\n",
            "Epoch 00447: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.0840 - acc: 0.9833 - val_loss: 0.7057 - val_acc: 0.6500\n",
            "Epoch 448/500\n",
            "\n",
            "Epoch 00448: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.0988 - acc: 0.9833 - val_loss: 0.7286 - val_acc: 0.6500\n",
            "Epoch 449/500\n",
            "\n",
            "Epoch 00449: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.1071 - acc: 1.0000 - val_loss: 0.7489 - val_acc: 0.6500\n",
            "Epoch 450/500\n",
            "\n",
            "Epoch 00450: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.0848 - acc: 0.9833 - val_loss: 0.7757 - val_acc: 0.6500\n",
            "Epoch 451/500\n",
            "\n",
            "Epoch 00451: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.1107 - acc: 0.9500 - val_loss: 0.7716 - val_acc: 0.6500\n",
            "Epoch 452/500\n",
            "\n",
            "Epoch 00452: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.0968 - acc: 0.9667 - val_loss: 0.7613 - val_acc: 0.6500\n",
            "Epoch 453/500\n",
            "\n",
            "Epoch 00453: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.0679 - acc: 1.0000 - val_loss: 0.7713 - val_acc: 0.6500\n",
            "Epoch 454/500\n",
            "\n",
            "Epoch 00454: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.0626 - acc: 1.0000 - val_loss: 0.7711 - val_acc: 0.6500\n",
            "Epoch 455/500\n",
            "\n",
            "Epoch 00455: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.0967 - acc: 0.9667 - val_loss: 0.8107 - val_acc: 0.6500\n",
            "Epoch 456/500\n",
            "\n",
            "Epoch 00456: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.1050 - acc: 0.9667 - val_loss: 0.7988 - val_acc: 0.6500\n",
            "Epoch 457/500\n",
            "\n",
            "Epoch 00457: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.0649 - acc: 0.9833 - val_loss: 0.7998 - val_acc: 0.6500\n",
            "Epoch 458/500\n",
            "\n",
            "Epoch 00458: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.0900 - acc: 0.9833 - val_loss: 0.7801 - val_acc: 0.6500\n",
            "Epoch 459/500\n",
            "\n",
            "Epoch 00459: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.1267 - acc: 0.9333 - val_loss: 0.7585 - val_acc: 0.6500\n",
            "Epoch 460/500\n",
            "\n",
            "Epoch 00460: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.0919 - acc: 0.9667 - val_loss: 0.7101 - val_acc: 0.7000\n",
            "Epoch 461/500\n",
            "\n",
            "Epoch 00461: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.1102 - acc: 0.9500 - val_loss: 0.6907 - val_acc: 0.7000\n",
            "Epoch 462/500\n",
            "\n",
            "Epoch 00462: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.1020 - acc: 0.9833 - val_loss: 0.7368 - val_acc: 0.6500\n",
            "Epoch 463/500\n",
            "\n",
            "Epoch 00463: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.0672 - acc: 1.0000 - val_loss: 0.7840 - val_acc: 0.6500\n",
            "Epoch 464/500\n",
            "\n",
            "Epoch 00464: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.1228 - acc: 0.9833 - val_loss: 0.7937 - val_acc: 0.6500\n",
            "Epoch 465/500\n",
            "\n",
            "Epoch 00465: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.0890 - acc: 0.9667 - val_loss: 0.7889 - val_acc: 0.6500\n",
            "Epoch 466/500\n",
            "\n",
            "Epoch 00466: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.0554 - acc: 1.0000 - val_loss: 0.8169 - val_acc: 0.6500\n",
            "Epoch 467/500\n",
            "\n",
            "Epoch 00467: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.0642 - acc: 1.0000 - val_loss: 0.8260 - val_acc: 0.6500\n",
            "Epoch 468/500\n",
            "\n",
            "Epoch 00468: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.1290 - acc: 0.9667 - val_loss: 0.8152 - val_acc: 0.6500\n",
            "Epoch 469/500\n",
            "\n",
            "Epoch 00469: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.1179 - acc: 0.9667 - val_loss: 0.7082 - val_acc: 0.7000\n",
            "Epoch 470/500\n",
            "\n",
            "Epoch 00470: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.0753 - acc: 0.9833 - val_loss: 0.6720 - val_acc: 0.7000\n",
            "Epoch 471/500\n",
            "\n",
            "Epoch 00471: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.1166 - acc: 0.9500 - val_loss: 0.6702 - val_acc: 0.7000\n",
            "Epoch 472/500\n",
            "\n",
            "Epoch 00472: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.0690 - acc: 0.9833 - val_loss: 0.6871 - val_acc: 0.7000\n",
            "Epoch 473/500\n",
            "\n",
            "Epoch 00473: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.0604 - acc: 1.0000 - val_loss: 0.6899 - val_acc: 0.7000\n",
            "Epoch 474/500\n",
            "\n",
            "Epoch 00474: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.1081 - acc: 0.9833 - val_loss: 0.7404 - val_acc: 0.7000\n",
            "Epoch 475/500\n",
            "\n",
            "Epoch 00475: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.0882 - acc: 0.9833 - val_loss: 0.7946 - val_acc: 0.7000\n",
            "Epoch 476/500\n",
            "\n",
            "Epoch 00476: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.0749 - acc: 1.0000 - val_loss: 0.8407 - val_acc: 0.6500\n",
            "Epoch 477/500\n",
            "\n",
            "Epoch 00477: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.1002 - acc: 0.9833 - val_loss: 0.9046 - val_acc: 0.6500\n",
            "Epoch 478/500\n",
            "\n",
            "Epoch 00478: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.0886 - acc: 0.9833 - val_loss: 0.8921 - val_acc: 0.6500\n",
            "Epoch 479/500\n",
            "\n",
            "Epoch 00479: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.1324 - acc: 0.9667 - val_loss: 0.8773 - val_acc: 0.6500\n",
            "Epoch 480/500\n",
            "\n",
            "Epoch 00480: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.0817 - acc: 0.9667 - val_loss: 0.8951 - val_acc: 0.6500\n",
            "Epoch 481/500\n",
            "\n",
            "Epoch 00481: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.0772 - acc: 0.9833 - val_loss: 0.8814 - val_acc: 0.6500\n",
            "Epoch 482/500\n",
            "\n",
            "Epoch 00482: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.0834 - acc: 0.9667 - val_loss: 0.8636 - val_acc: 0.6500\n",
            "Epoch 483/500\n",
            "\n",
            "Epoch 00483: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.0906 - acc: 0.9667 - val_loss: 0.8640 - val_acc: 0.6500\n",
            "Epoch 484/500\n",
            "\n",
            "Epoch 00484: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.0732 - acc: 1.0000 - val_loss: 0.8773 - val_acc: 0.6500\n",
            "Epoch 485/500\n",
            "\n",
            "Epoch 00485: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.0977 - acc: 1.0000 - val_loss: 0.8869 - val_acc: 0.6000\n",
            "Epoch 486/500\n",
            "\n",
            "Epoch 00486: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.0830 - acc: 0.9667 - val_loss: 0.9027 - val_acc: 0.6000\n",
            "Epoch 487/500\n",
            "\n",
            "Epoch 00487: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.0883 - acc: 1.0000 - val_loss: 0.9388 - val_acc: 0.6000\n",
            "Epoch 488/500\n",
            "\n",
            "Epoch 00488: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.0785 - acc: 1.0000 - val_loss: 0.9248 - val_acc: 0.6500\n",
            "Epoch 489/500\n",
            "\n",
            "Epoch 00489: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.0755 - acc: 0.9833 - val_loss: 0.8893 - val_acc: 0.6500\n",
            "Epoch 490/500\n",
            "\n",
            "Epoch 00490: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.0687 - acc: 1.0000 - val_loss: 0.8431 - val_acc: 0.6500\n",
            "Epoch 491/500\n",
            "\n",
            "Epoch 00491: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.0793 - acc: 0.9667 - val_loss: 0.8123 - val_acc: 0.6500\n",
            "Epoch 492/500\n",
            "\n",
            "Epoch 00492: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.0729 - acc: 0.9833 - val_loss: 0.7786 - val_acc: 0.7000\n",
            "Epoch 493/500\n",
            "\n",
            "Epoch 00493: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.0692 - acc: 1.0000 - val_loss: 0.7862 - val_acc: 0.7000\n",
            "Epoch 494/500\n",
            "\n",
            "Epoch 00494: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.0541 - acc: 1.0000 - val_loss: 0.8377 - val_acc: 0.6500\n",
            "Epoch 495/500\n",
            "\n",
            "Epoch 00495: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.0761 - acc: 0.9833 - val_loss: 0.8504 - val_acc: 0.7000\n",
            "Epoch 496/500\n",
            "\n",
            "Epoch 00496: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.0822 - acc: 0.9667 - val_loss: 0.8745 - val_acc: 0.7000\n",
            "Epoch 497/500\n",
            "\n",
            "Epoch 00497: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.0668 - acc: 0.9833 - val_loss: 0.9545 - val_acc: 0.6500\n",
            "Epoch 498/500\n",
            "\n",
            "Epoch 00498: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.1290 - acc: 0.9667 - val_loss: 0.9173 - val_acc: 0.6500\n",
            "Epoch 499/500\n",
            "\n",
            "Epoch 00499: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.0936 - acc: 0.9667 - val_loss: 0.8521 - val_acc: 0.6000\n",
            "Epoch 500/500\n",
            "\n",
            "Epoch 00500: val_loss did not improve from 0.41903\n",
            "60/60 - 0s - loss: 0.0883 - acc: 0.9667 - val_loss: 0.8239 - val_acc: 0.6000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAgAElEQVR4nOydeXxcVdn4v89MZrInbdJ0Tdt0ozsV\nWlpW2XfZBBRkERQrKq6g1t+LgLzu+rqAvChqEVBBllctiKAgQtmEFsrSQmnpmu5p06TNNtv5/XHv\nnblzMzOZpJkkzTzfz2c+M/fec+997kxynvMs5zlijEFRFEXJX3z9LYCiKIrSv6giUBRFyXNUESiK\nouQ5qggURVHyHFUEiqIoeY4qAkVRlDxHFYGSF4hInYgYESnIou1VIvJ8X8ilKAMBVQTKgENENohI\nSESGefa/bnfmdf0jmaIMTlQRKAOV9cClzoaIzAZK+k+cgUE2Fo2idBdVBMpA5T7gStf2x4F73Q1E\npFJE7hWRXSKyUURuFBGffcwvIj8WkQYRWQecneLc34rINhHZIiLfFhF/NoKJyEMisl1EmkTkORGZ\n6TpWLCL/Y8vTJCLPi0ixfexYEXlRRPaKyGYRucre/28RucZ1jSTXlG0FfU5E1gBr7H0/t6/RLCLL\nReQ4V3u/iPw/EXlfRPbZx8eKyB0i8j+eZ1kiIl/O5rmVwYsqAmWg8jJQISLT7Q76EuD3nja3A5XA\nROB4LMVxtX3sU8CHgMOAecBFnnN/B0SAyXab04BryI6/A1OA4cBrwB9cx34MzAWOBqqArwExERlv\nn3c7UAN8AFiR5f0AzgcWADPs7Vfta1QBfwQeEpEi+9hXsKyps4AK4BNAK3APcKlLWQ4DTrHPV/IZ\nY4y+9DWgXsAGrA7qRuB7wBnAP4ECwAB1gB8IATNc530a+Lf9+V/Ata5jp9nnFgAjgA6g2HX8UuAZ\n+/NVwPNZyjrEvm4l1sCqDZiTot03gD+nuca/gWtc20n3t69/UhdyNDr3BVYD56Vp9w5wqv35OuDx\n/v699dX/L/U3KgOZ+4DngAl43ELAMCAAbHTt2wiMsT+PBjZ7jjmMt8/dJiLOPp+nfUps6+Q7wMVY\nI/uYS55CoAh4P8WpY9Psz5Yk2UTkBuCTWM9psEb+TnA9073uAS7HUqyXAz8/AJmUQYK6hpQBizFm\nI1bQ+Czg/zyHG4AwVqfuMA7YYn/ehtUhuo85bMayCIYZY4bYrwpjzEy65mPAeVgWSyWWdQIgtkzt\nwKQU521Osx+gheRA+MgUbeJlgu14wNeAjwBDjTFDgCZbhq7u9XvgPBGZA0wH/pKmnZJHqCJQBjqf\nxHKLtLh3GmOiwIPAd0Sk3PbBf4VEHOFB4AsiUisiQ4FFrnO3Af8A/kdEKkTEJyKTROT4LOQpx1Ii\nu7E67++6rhsDFgM/EZHRdtD2KBEpxIojnCIiHxGRAhGpFpEP2KeuAD4sIiUiMtl+5q5kiAC7gAIR\nuQnLInD4DfDfIjJFLA4VkWpbxnqs+MJ9wCPGmLYsnlkZ5KgiUAY0xpj3jTHL0hz+PNZoeh3wPFbQ\nc7F97NfAk8AbWAFdr0VxJRAEVmH51x8GRmUh0r1YbqYt9rkve47fALyF1dnuAX4A+Iwxm7Asm+vt\n/SuAOfY5P8WKd+zAct38gcw8CTwBvGfL0k6y6+gnWIrwH0Az8Fug2HX8HmA2ljJQFMQYXZhGUfIJ\nEfkgluU03mgHoKAWgaLkFSISAL4I/EaVgOKgikBR8gQRmQ7sxXKB/ayfxVEGEOoaUhRFyXPUIlAU\nRclzDroJZcOGDTN1dXX9LYaiKMpBxfLlyxuMMTWpjh10iqCuro5ly9JlEyqKoiipEJGN6Y6pa0hR\nFCXPUUWgKIqS56giUBRFyXMOuhhBKsLhMPX19bS3t/e3KH1GUVERtbW1BAKB/hZFUZSDnEGhCOrr\n6ykvL6eurg5XWeFBizGG3bt3U19fz4QJE/pbHEVRDnJy5hoSkcUislNE3k5zXETkNhFZKyJvisjh\nPb1Xe3s71dXVeaEEAESE6urqvLKAFEXJHbmMEfwOa2WpdJyJtdzfFGAhcOeB3CxflIBDvj2voii5\nI2euIWPMcyJSl6HJecC9duGrl0VkiIiMsmvFK3lIfWMra3bs58Rpw/tblJQsXbOL2qElTBhWSigS\n4+4X1jOioggROO8DY5LatoejLFmxlYvm1uLzZVbaz6zeyeSaMsZWJdameezNrfhFmDS8jENGlHc6\nZ+veNlZubebUGSM6HXthbQMrNu/l6mPqKA74eXh5PeVFBcwcXclrmxo5YepwKout2NLL63bT2BKi\nvChAcdBPYYGPWWMqMcZw70sbKS0sIBYzXDS3lmff28XqHfsYVVnEkROrWbJiK5UlAXY2W5bpFUfV\nxa/rsGtfB/e/sokCv3DR3FqeX9NAJGY4dfoI/vnODi6eW4uI8PaWJv717k4umT+W4eXW0suxmOGh\n5Zs57wNjKAr4AXji7e2s2toEwJihxRgDp8wYwQtrG+gIxygtLCASi7F5T2v8+3x/534m1JQyZXg5\nT7+zk4vm1fLYG1uZVFPGKfb39+gbW2nY30F5UYAzZ41k4+5WNu5uwQBThpexa38HR06o5qHlmzn/\nsDG8sbmJVzfs4YqjxlNRlHjm1dv30dgaIhSJsXJrM1cfU0dRwM+m3a28v2s/1WVBIjHD4eOGJv3+\nq7fvo6IoQNQYCv0+TphWw4OvbmbBxGqOqKvK+PfTG/RnjGAMyTXU6+19nRSBiCzEshoYN26c93C/\ns3v3bk4++WQAtm/fjt/vp6bGmsD3yiuvEAwGu7zG1VdfzaJFi5g6dWpOZR3InPmzpezriLDh+2f3\ntyidMMZwxW9fIej38d53zmTZxj187+/vxo/PqR1C3bDS+Pbt/1rDHc+8T0VxgDNmpVpwLMHVd79K\nWWEBb3/rdAB2Nrdz3R9fjx9P9X38Zul67n1pA+99+8xOiuaLD6ygYX8HddWljK8u4asPv5l0/MOH\nj+EnH7HWxLnkLu9yCtb9Nu1p5eYlK+P7CgM+vvjAiozPMXpIMR8+vDZp319XbOEn/3wPgDv//T77\n2iNJx8dVlXDkxGp+9tR7PPXOTgr8wmdPmAzAP9/ZwdcfeYtNe1r56unTAFj0f2+ytzWcfOP/eyuj\nXAAi8MEpNTz73i5eXrebl9btjj9rOBrj8/cnvu+OSJT/+nNnj/btlx7G1x95i61721nyxlbWN7Qw\nekgRFxyWeObTf/YcAIUFPjoiMaaNLOfEacM5+/alSc/u/k2vvvvVTvcaVVnEtqZ2Jg8v46mvZLNe\n0oFxUASLjTF3AXcBzJs3b8BVyauurmbFCuuf5JZbbqGsrIwbbrghqY2zSLTPl9obd/fdd+dczoHO\nvg7rH8UYM+BcX07nE4paSxQ3eTqjSCyWtL2l0Vr4q7E1lPG67eEoAPs7Ep3Exj2tXcqzaU8LkZhh\nfyiSNCIFiNmFJDftaSXV1+h0SOForPNBm0bP823Z2/VCZt5zAFo6op3u6yYStWRts7+Hza5n39Ni\nfXfb9iZiYS0dET5zwiQOHVPJZ/7wWpcyORiTeAZHCYD1ve/Zn/wbbdqd+vvf1mSdv72pnaY261k7\nKSWbjoj13e5ts66d6tkzsa3JeuaG/R3dOq+n9Oc8gi0krylbS2K92UHB2rVrmTFjBpdddhkzZ85k\n27ZtLFy4kHnz5jFz5kxuvfXWeNtjjz2WFStWEIlEGDJkCIsWLWLOnDkcddRR7Ny5sx+fou8JZeig\n+ovNjcmdg9MRODj/+A7OaGXXvsz/yM3tnTuSzVkogs17rE7Jq5DAtXBxY2vKaw0rKwQs91JauTzP\nt7Gha5m85wC0hiMUFvioKEo95mwNRex3RxEkZHK+O0exhaMxwlFDScCf5Ebrigm2pZbqu9i8p7XT\nb7thd0undgBv1DfF5XGqNrv/DlJVck71+7j3R2OZx7V7W8O0dHRPifSE/rQIlgDXicgDwAKgqTfi\nA996dCWrtjYfsHBuZoyu4OZzslnXvDPvvvsu9957L/PmzQPg+9//PlVVVUQiEU488UQuuugiZsyY\nkXROU1MTxx9/PN///vf5yle+wuLFi1m0aFGqyw9KOiIxCgv8/S1GEk4HNbTEGn17FUFbKJq0vbO5\nwz4vcweaqvN0d4apMMbEO69UiiTRqbaSKjxRbnfKme7jfb51DfszylReVNDpHIDWjiglQT+jhxSz\nMsX/pXOO8/1tcn1fzucd9nfpPFdx0M/Yodkrgumjylnf0NJJWYP1HTmWh8OyDY0pr/PS+5Ylsb25\nPW7BuZ95V4rRe1NbJGVnv7mxlcqSSval+P1StZ02sqLLdgdCzhSBiNwPnAAME5F64GYgAGCM+SXw\nONYarmuBVuDqXMnSn0yaNCmuBADuv/9+fvvb3xKJRNi6dSurVq3qpAiKi4s588wzAZg7dy5Lly7t\nU5l7yurt+/jff6/lxxfPIeDPzth8cuV2Xt+0l0VnTovv6wjHoKjrc29ZspKTpg1HBH785Gqqywo5\noq6KjkiU1dv3cciIcooCfqpLg3zkCMv4XLpmF/e8uJECn/CTj86hJFjAX17fQn1jK5ctGM9n/rCc\n6aMquOG0qVz/4BtcefR47ntpI1NHWgHbqlIr3uPtgFs9isDpqJ33WMzwtUfeZNe+DlpDEcqLAsSM\n4dn3dsXP+dDtS5lUU5bxu9u4u4VFj7wVv5+3843FTNzNsnRNAy++31kTbGho4eJfvsiGNC6QDQ0t\nST5zgFfTdI4OFUUBfvfiBjbubknqEN/e0syYIcWMqypJqwh+/ORq3t2+D7ASBj7yy5f49PETeXh5\nPQCvbWrkQ7cvpbHFetaSYAGVJdlPpBxVWdxp34xRFaza1sw72/bx06feSzq2uyW1O89RGO/t2EfY\ndmm9Vd/EhXe+yEnThnPkxOpO5yzf1MhHfvVSp/1fuP91PnviZJZv3JNW7lljKnh7SzOf/f1r/NfZ\n0zl5eufEgN4il1lDl3Zx3ACf6+379nTknitKSxMBxDVr1vDzn/+cV155hSFDhnD55ZennAvgDi77\n/X4ikdybhr3BP1Zu568rtnL9qVMZV53diO3T9y0H4GunJ4LkHZFouuZxjDH87sUN/O7FDXzimAlx\ns/1f7ybcaH9/e3v8s6MIrvjtK/F9F6wZw+kzR/Lgss1s3N3K/AnVvLxuDy+v28Oc2iE8sXI7T6y0\nrvHeDquj8ttDbG8H7FUEjm/XGXVv2dsW79jS8faWZt7e0sxUT5ZQLGbiAeEfPbk6ycfd3Jb8t9Fu\nf3fHTRnG0jUNKUejz763K+Xo2OHrj7yZcv+EYaWsb7DcJkNLApwzZzT3vmQVtCwttCy4Z1bvYlhZ\nIXNqK3lnm9XxFweTXTlXHV1HfWMrT72zk+b2CL94Zi0Ah40bQllhAUvXNMS/34vn1tLYGqIjEuPt\nLdb1SoLWvW49bybv79zPW1uaeG3T3vj1j5syjM+fNIUn3t5ONBbjgsPG8Nvn1wNw1uyRRKKGC+fW\n8rk/vMaL7zcAML66hI0exXjmrJFUlQZpagsT9Ptoaguzaltz3H8PsGyjpSA37m5l8vCyTt/Zcy5F\nP3N0BZOHl/HPVTtY19DCDQ+90an9dSdOjn8fR08axttbmlnX0MIn71mW0ySKgyJYPFhobm6mvLyc\niooKtm3bxpNPPskZZ2SaanFwkcld0RU79iX+udrDXccI3HGEntwPoKzQdpE0ttLcFo4HbqGzS8dx\nXTgdvrcDbgsntjsiUdrDMUSsAGM4GqO+setg64iKQnY0d7DaVjoO+zoi8bTMoSXJGWhe15Ij36kz\nRtDYGop3nm7cSkAEvnXuTG76ayJDyOsqcfjRRYdy0S+t0e09n5hPgc8XVwTuoPCpM4bzvQ8fyq2P\nrmLxC+spCfoZO9QalZ8wtYZbzrUGa7NveZK9rmD64eOG8pVTD2HmzU+yesc+hpcX8qOL59jPFWHG\nTU8ClmIBuPKoOvt5oky98QmXnHMYWVnE/AlW2qWT3gowv66Kq46xZuNXFgfiv8s3z57BNfcmytv7\nBG679LBO1tnPn1rTyYKwZPLFlZdPIJXr/9bzZjJ3fBVPvL2Na3+fOtD9yWMn8NyaXbxZ38Sc2iEp\n2+QCLTrXhxx++OHMmDGDadOmceWVV3LMMcf0t0i9iuPTTeUr7vJc12gsG4ug1dXxZHM/dyfvvk8k\nGmPr3nb2dUSSRvUvr9+d1HarPQp0u2SckSkkd4SOkpg1upKYsYKy3oBkKuaNT50v7u7sh5YmK4JO\nloktR3HAT1EWcZbSYEGndnvTfJ/uUX1J0J/knnHL4bSrKLYUbdDvo9be555nUFkciGdXOdcsLSyg\n2n5G9/2KA/6kdm688STnvolt1z1dMlcUB+KZRCMrk32RoyqLU7ro3Nd2y9EejsV/p1ljKlPK6WR3\n1WaIb5QXFcS/o7FVnV1auUItgl7mlltuiX+ePHlyPK0UrNnA9913X8rznn/++fjnvXsTZu4ll1zC\nJZdcAiSyEnKVWunNenDu43ZNpDrHGGtkGc9kaQuze38HVaXB+DW8shtjkkbVm10dws59HYxuCcU7\nPfe5baEo+zrCVhzBJlXA1Ut9Yyvjq0uT9rWGomxrao+7T3a5rJJ0AUMny2VzYytjh5bER+8793Ww\n0z7fSSmcNaaCt7Y08faWZt7bvi/l9dzMHFPB397qnC+xa38HtfaI2v0b+cRKMwxHYxTYv0+LLV9J\nMLt/7eKgn8JAcoeXLtOpxs42ss4rSMoEcqe/Diu12jkdWiRm4sFdd6prRVGAt+3JYW6Za6tK2N0S\nYpxLEbj/5r0dbKdnCiQfL3JtJ93fpSCGVxQmnZOuE3YrMvfv3xaK0tQWRgSmj6zgzfqmpOPuczO5\nTQv8vriM3oD4pt2t1JQXxi2i3kQVwQAkFIny7vZ9jKsqYYjLFbBhdyutoQgTh5WyZud+YilGuT2h\nsSXEYf/9TwBOmT6ceXVVfP/v77Lq1tP5+1vbuf6hN3j5GyczsrKInfvamf+dpwE4cWoNz6y2fKDH\nTRkWz7P+7uPvUN/Yxo1nT2fL3jbufmEDly0Yx47mdn7z8SO4cvErPPfeLv72hWPjMrhdMc4Em19d\nMZfTZ47k0Te38a0lK3lh0Ukc98NnaNjfwZyxCbM5G4tg8562TiO81lA06b7fdLlH0vnQ28MxHnx1\nM+t2tXDajBHxf/Tbnl7DbU+vAWC0Pbo8tHYI97+ymc/9Mbt899n2SNLLwnuXEfT7qCoLsmBCIiBZ\nVRpk8QvrWdewn3+v3pV0TknQz6wxlXEfdjr8IllnaLkHAyUBf9y1Nm1kOT4RVtkxAWd07XRokViM\n2qHFBPwST10Fq2N0znFkBqirLuGNzXsZn6bDLA5k7rYyDZSqXBaV1zpxGFoSiKecenErkgnDSuO/\n//6OCLf/ay2lQT+ThpfiE5g0vDRJETiKxzvvw8uwsiBDSgIMKQkwbWR5PJD+wR89w7fPn8XlR47P\neH5PUEUwAHF85I2t4SRF4KSaOaOvtix86dngniz01Ds7eX6tFUDb3x7hsTe3AvBm/V5GVo5kgyuf\n/BlX57N0TUP8s+N3XdfQwh//swmwJvE4k4ecAJp7kk2qNMuVW5o4feZIXtvYyO6WEDubO+JB2Dfr\nE1bTvvYIFxw2hnM/MJpI1PD//vwWu/Z1MKyskK+cegj/789vsbmxNa4IPn38RH717DraQtGMLpvj\nD6nhjFkj+YZn5uorG6xMj29+aAYLPzgx7je/7sTJvPB+A6/bgctDRpRz1xVz2WmPsCfWlCIIl/7a\nms1763kzOWv2KDbubmF/R5TjptTwyGeOYvOeNm59bFXcV7+nJWS5mJramT3GUoBPX388e1pC/Pdj\nq1JOgCoO+vnGWdM4efpwZoyq4NxfvJD0O5cXFcS/f69FAHDFkeM5/7AxXHjniwD848sf7HR9EeHP\nnz2auupSYsawYXcL+9ojHDdlGJDoXMMRQ1HAzwMLj2JSTaKDvemcGZz586VJ1wT46ulTOWpiNafP\nTD0jO5VF8I8vf5B97dachVQ88pmjqG9sS/K7OxZNUcCXpAzv/cQCRngsBAe3a+nGD03n+Kk1vLut\nmXvsWElrOMplC8ZzaO0QxlaVcERdFd96dJV9n8Q9Hr72KLbsbaMkWECBX5g2sjyeFfW5kyZz0dyx\niAh//NSRvLdjH/WNbbSHo/G4R2+jimAA4gxoUk1QAcvU7k28E7gcRdQRiVFtj+Aa7NmXsTQypcLt\nslnf0NKpDo0TgA0W+Fi7KzlPvaKoIO4uqrc7a2eWbrDAR8g1Ym9qCzO0JMiJU60aRUvX7OLelzZy\n1uyRXDp/LN96dCWb97RSYM/qvnjuWH717DpaQ9GUbpDJw8tYu3M/c8cP5dL54/jRk6vZ0xLC7xOi\nMcN7O/YxY1QFY6tKkvzYnzh2AgYTVwSVxQHmjh/a6foOZ80exbCywqRR8tzxVcwdD9/+m9V5XH1M\nHXe/sMH1nUUYV1XCpJoyJtXA1BHlSUrYoSTop7DAz3FTrFIn8+qGsmVFQhFMHVEetxZSdZ7HThnG\nzNGJ3HVvvSPnnMNcNXOqy5I7T6fTdGYwe7+L6aOSc+Md91bt0BIumZ++lEwqRZCqHpMb53tNks/+\ne/T+Xc6uTW2ZQfJovnZoCZfOH8diOyMJrBnMpYUF8VTSq46uiysCN/Pqqpjn2eekuQ4vL4rXW6oq\nDaZMS+1tNFg8AEn40RP73GmAoQypfz0hnY+9IxKLm9KO/7urqfJuqzx51qV1n5jrOZwR79QRCfPX\nYfqoiriV4MQeHEXgTa/c78qqgcTI0hm11g4tZvOeNtsqEOqqSxCxOtVUZRgm11hpgI6f2HGBjKyw\n/jnf3b4vpQ95aEkgya/rDVp6yewikJTPWt/YltQRlgT98biFG29n6XWLHTIycd2iQOeOdezQkrSj\na8guTuU8XziW3d+rYzF2RW/5yLN11bjxKg3IHLMYaKVS0qGKoA+IGcP+btYaAav+SsQeTbnrwjgd\nbCgS5el3drB7fwdvb2lKeQ1jDM+vaYhbF7v2dbDSFaCLxgxPuPLt3bSHo3EF5Lh7MgVmC3zCmCGJ\nDtIrU8zAvS9tiG9vt9P6DhlR3km5jasqYdnGRrY1tcWzkR57c1u8vRd3p+uY+c772KoSVm5r4j/r\ndjN6SDEFfh/FAb8VI2hsZYIniDxpuLXtdOqOa3yU7fsORWIpZ7aKSJKFkKrTcBPM0NE6eHPTl21s\nTOoIi4MFNKf42yr2BIu9isCtYFJ1+GOrig+4E3PmFoQj2XXw2ZYWyTYQ3hXO75Pt5EdIrdxzEbzt\na1QR9AE7mttZ17A/5cgtJXanHTOGtTstl0kqKyAUNXzynmUc98Nn+NDtz3c6DvDapr1c/tv/8Mp6\ny6997i+e5+zbno8rhsXPr+eBVzenPLcjEounSzqB4EyB2REVRUlujlRFyG5xmcnb7ZRMp+N146Tg\n3fzXlfGZss6ErGkjOyuCOldnLp73WaMr2bynjdc27Y27O0qCflrDUTbvaevkCjh83FBKg/54J+y4\nMKaNStx3tEvhVZUG4x3rlBHWOUNLAmmDsKelKB3t5Qo7IDh1ZHmnMhFei8CLTxKlMBycPn1sVTHD\nywuZ6PLVu+W8eK5VSbPcNUo+fWZC3mMmZ++mqLazhz62IL2b56qj6+KfDxuXOW/euY6/i7Le2eL8\nzTi/85gh1neTCScj6cqjEn4mt2Jyf1cOQ0sCTEkx2WwgoTGCXqCrMtSOz70r03fx4sWcddZZlA5J\n/LM5oyTnfVRlcbxTdnA660g0RoFndONM2HF8/M6syKY2KxCdqapkRyRKWyi5pkqqyVvDywvZua+D\noaUBxgwtZsXmvZ3apMJ5Difd0M2VR43nZ0+9l7IsgdPZAjzxpeOoLi2kJsM/8PWnHcJlR47DGOLt\nioN+9uwP0bC/gynDy/jr547hvDteAOCkacNZcfNp8ZHi7ZceRmNrmPd37ef3L1vBb8ddBLDsv06J\nF5kbXl7EGzefRsCfvrP65eVz6WqM/IWTJ/O5EydR4PcxqrKYLXvbmDK8jDU79ydNuPMqgk8dN4Ev\nnDyl06jZUfwfP6qOq46u4y2XtVbkChb/4MJD+d6HZ8e33//uWbif5L5PLOhSdofioJ+13zkzY8d9\n8zkzuPHs6QCd/na9fOf8Wfz3ebOyvHvXnDFrJK/+1ylxpbn0ayd2eY6IdHom5zcYWVHELy+f2+mc\n5Tee2ksS5w61CHoBpwz1ihUruPbaa/nyl78c385mLQKHxYsXs337dlLFY0ORGD6ReMGwVKRyEaSr\nSeP43SsyuC86wrFO56eyCBwroKIowKiKLIoE2ThKKZUMIsKYocUpFVV1aSHDyuxJR0NLMioB51qj\nKosZPSQxSagkUBAvGzG2qiTpexWRJHdBgd9HTXlhUqfrdgf4fJLUMVQWBzK6L7zt08nsdIxOPv0x\nk61snN2uWj5et0RpYUHSaN7BcdMXBvwU+H1J8rktAp9Pkjpkv0+S0kazkd1Ngd+X0cXkPGdXSsBp\n21vWgENNeWH83j7Ps6bD+0zOb1AUSP2s2V63P1FFkGPuuecezj/teD5y+nF8+YvXEYvFiEQiXHHF\nFcyePZtZs2Zx22238ac//YkVK1bw0Y9+lGMWzCMcSp7mH4rECBb4MvozU3XSba6OPOLywTppk9EM\ngbyOSDTulnEmf6W6x5CSRNAtnV88lT98R3M7xQF/Wh9rSZp88eKgPz470zt5KFuKg37W2XVzxlaV\nZOV37solkyucwLTj1trtqp+frRxOtlfA7pDc56VKH1Wyx5mZ3Z1Yw0Bj8LmG/r4Itne9YlG3GDkb\nzvw+4WiMhv0djKwoQkQwxrC9uZ3q0kKCBT46wlH2d0QoLbX+6d586y3uf/Bh/vTYU4Ri8ONvXs/v\n//hHKoePZduOnbz1liXn3r17McESZh96G9/50U+YPuvQpLTGva0hQtEYQb8v44johofeYHJNGT6f\npQCGlRXy59etJR6a28Pc/q+18ba/em4dkNnn744RNLeH2bq3jf97rfOSEU7nXxjwpbUwJtWUxYuQ\nOTTsD1FdGkybnZJWQdhFzJuw2ZMAACAASURBVN7d3tzjkZa7Ixw7tCSrwK07ANuXAUInMD1ztBXL\n2OeaxdvV5CoHx8j02SNW9/eWKTtI6RonoUIVQZ6weU8r+zusFaFKCwtoC0ftssJRJtWUsb6hhZaO\nSLzg1GN/f5LXli/n/FOPAyAS6mDs2LGcPudo3l29mi984QucffbZnHbaaazZud9WNKFOue2b9rTi\nF4n7pUdUFFlT2j0jueUbG1meZiZpU1s4qRLiG5v38tk/vMa5c0YD1izhyuIAf1mxNd7G7RoyxlrX\nFazgaWNLKJ7141gEBT4fFxw+hqVrdvGJYyZw57PvA1atmQvn1vK3t7bx7rZm3t+VWPijOOhPSl88\nom4onzx2IpDorN2Tn5z9580ZHXcPebniqPEs22itJ5uO02eOZE9LiLrqUobZ68h2RUlSvZu++9c5\nZcYI3tnezJQRZVxyxFjOmj3KJUdCpsriQDzI7CXubrT7/5EVRZwxcySf+uBEgn4f584ZzUfmjU15\nrpKZqSPLOXXGCL50ypT+FqXHDD5FcOb3c3Zp72QqZ0zljAii9nGnVSQa4/yPXsZ1X/0vwKrVUhz0\ns2lPKw//43k2v/kSd9xxB4888ghfufXHKSsWOkSNiY84Rth++H3bs89/bmwJsa2pnc+dOIk/v7Yl\nXkStqS3MnNpK7r56PkCSImh3BYsB3t7aTMAvPPb5Y1mxuZEL77Rm1DodebBAqCgK8JuPHwHA0bZP\n2+Gs2aP4+sNvJimCEnvBdIcfXjQnPr3fGXUPLy9MUgTFQT+nzBgRX3jcy7CyQv5wzZEZv4+PH13H\nx10ZK5mCu+77uuXuK6aPquB/L7OCkN+/8NCkY245Hvz0UZ0mdTk4wWLHIvD7hF9ekQhs3nbpYb0q\ncz4RLPDx6yu908MOLg5eW6YfcPSAEw9yOm7T6d36cOzxJ/KPx/5C4x6rkmVDQwObNm1iz24rr//i\niy/m1ltv5bXXXsMYKCktpXV/+uJkXvdFd/K8V2/fR9Qu/uV232ze05rWneNYBM6kspVbmhg9pBi/\nT5JcEk7nUpBmPWY3BZ4OtzhYkKQI3J9L7VG3M8vSIdhPJrhbtp7GJnobt2WSSTnF4oog5yIpByGD\nzyLIId4Be6IEhG0JeBTClGkzufZLX+PTl55PLBajMBjkJ7f9gj2tEW7+6ucpKrCyDH7wgx9gjOG8\nj1zGLV/7IkVFRfzh0acJeDKOvB1gd+b7uAOj7o5/XUML00enXgbPSh+NMraqhD0tIdY1tHCsPcp3\nJgtBonPJxkfqjXGUBv0UujrVohTlhr2VIftrtmZ3KmD2FdkGsL0xAkVxo4qgGzgd/Nqd+xk9pDi+\n+HdHJMb2pnYMhs98ZRETa8rY1x6msTXEWRdczFkXXAxYueflRQG2NbXx4BPPMX1UBQU+4e2tzURi\nhtPPuYDTz7kg7f2DBcn/xEL3/6nHDi3ppFCGpLEIfvwPawGOkZVF8SqRTgaL203ilIv2dtipcDoi\nZ+5BUcBPURqLwLlHVWn2Kbh9RWnhwPjXccuRKW7hTO7KlH6s5C/6V9EtEjbBVk9++05XLfuYMZ2W\nLrT2JxeSi8YMQvricgG/j7CdLTS8ogi/r7NFUBzw0xaOJq2K9MvL5/Lejn385J+dV1IaVp7oVCcO\nK+WMWSP58OFj4vue+NJx7Nkf4mO/+U98n1MLHxJzBtydzmULxlNRFOCjR3QdbHQUwQWHjaGwwMdJ\n00ckWQRuReBYGO4893s+Mb/Le/SU+z91ZJdlIRwGSqZNTXkht543k6DflzGT6WtnTGXKiDJOmja8\nD6VTDhZyqghE5Azg54Af+I0x5vue4+OBxUANsAe43BiTeWHXfiTbwpuxmCEciRHw+4jGTNw/GzMm\nKSAcjZmMpnpFUQG7W0IMKy9MOyouCVqK4EOHjmaJndVzxqyRSa4bhwKfJPm2rz9tKmcfOiqpzbSR\nnd1E7ro6Tkfpvk5xwJ91jXTHGKkuC7Lwg5OA5DpK7olFzn53IPf4Q2qyuk9POGpS9uUTBlIxMWfJ\nxkwUBfxcmqGip5Lf5GxYIyJ+4A7gTGAGcKmIzPA0+zFwrzHmUOBW4Hs9vV+6UXVvkukO7m4hZhIT\nwBwlUFjgxxgwJFsEmco6ezL+ko/Z5zmjQG8gOdXINuCZEZlKWaRi1JBEsNap1Oj29WeTcePg5K+7\n64uliy04JTkO5vxsRTkYyOV/2HxgrTFmnTEmBDwAnOdpMwP4l/35mRTHs6KoqIjdu3fnXBlkurzb\nbRMzJj4BzKGwwIfxWAStoUjSbN909/OOPo0x7N69m6KioniA0NtZZpPnnu2odkhxwhpJVw4iW/zO\n8pdZ/FaOReDNNFIUpXfJpWtoDOAua1kPLPC0eQP4MJb76AKgXESqjTFJK4eLyEJgIcC4cZ3N29ra\nWurr69m1a1enY73J9r1tpKsb5/clRrntuwpoaotQUVRAKBqjPRyjpdBy4RQF/LSFohgDO7BG02H7\non4h6fqhkgCNrWFMYyE7PCP+oqIiamtrKQla7qDCAl984RTIHGA9/pAanl/bwNih2S2O7bYuuqqx\n3xXOQiYz0mQquXGWo3RWleqqMmRfUFUajK+joCiDhf4OFt8A/EJErgKeA7YAnaKsxpi7gLsA5s2b\n16krDgQCTJgwIbeSAhff/GTSIt0OR0+q5sX3E7rriiPHc9/L27jlnBlcMn8c+zsi3Pnv9/nTq9s5\nZfpwXt/cxN1XHcGn7l3G+oaWuJXwgwtnM3VkBefbVTDXf+8stja1J9X49+JkgQT8woqbTo1fq6o0\nyMvfOJkCv/DaxkYW3rc8fs41x03g7ENHJZVS9rLiplP5wK3WOsbuzj/bYGo6Tp0xghcXnZTx3g5n\nzR7FC4tOYsyQYlbcdOqAcBEt/dqJWS+goigHC7lUBFsAdxpJrb0vjjFmK5ZFgIiUARcaY7KrYdwP\nhNO4ccZXlyQpAqeqZmVJwEqPDFizZzsiUToiMQoLfEysKWNkZVHSLNvK4kBSho6IZFQCzjlgxQi8\nVSedRcSdd/d1u+qI3Wslp1vku6dkowQcnOd3y9OfDJS0UUXpTXI5xHoVmCIiE0QkCFwCLHE3EJFh\nIuLI8A2sDKIBSzRNDQj3LNtggY8ddg0e9xJ4hQV+wlErrdRJh/R2qoUBf7cnKjkWQdCf/rwD7bzd\nCiZT2WpFUQ5OcqYIjDER4DrgSeAd4EFjzEoRuVVEzrWbnQCsFpH3gBHAd3IlT3fpiET5zdJ1hCIx\nojHD4ufXpy1M5nTeQ0oCDC8vTFgErk7TKfXb3B6O56B710otLPDFS9pmizMTN1CQPqB6oIrAnSFU\n1ofF1hRF6Rty+l9tjHkceNyz7ybX54eBh3MpQ0+598WNfOfxdwj4fZQVFnDrY6uSjp8wtYaX1+3m\nlOkj4imcY4eWEDOG+kZrFq579OzMnm1qCzO60nJ3dLIICvzdLqvs5PNHM/ity4sCjK4s4mtnTOvW\ntW847RCeWW0F4G8+ZwZ/enVzknxXH1PHhoaWdKd3i3PmjB4wk7QUJd/Q4V0adtuZIXtTrLsLcPdV\nR8TTJu9+YT1glV+Ixkx8ecVki8DqsJvbIkyoti0CjyIo6sECIY4icBaQSYXfJ7z4jZO7fe3rTprC\ndSdZpXWvPmYCVx+THJC/+ZyZ3b5mOm7X6peK0m+oIkhDu92x7u8Ix+vtu0lVgGzs0JKkOEKSIrBH\nu81t4bibyKsI0i12ngnHGsmkCBRFUTKhtnga6u2lHDfvaeuyyqezctXYqhLGVSfKMbhdHY4vPxSN\nxeMAFZ4CYD1Zj9W5brsqAkVReohaBB46IlEu+/V/WGav9PXEyu1dnlPqWARVJcRcFoHbanArBacc\nhDdG0JP5s45VMZBq3yiKcnChisDD6u374kpgREUhO5o7eHn97oznLJhYzWdPmMSCCVVEY4aPLRjH\n9FHJM2fdbp9qe4nFueOHcun8cdSUBSkM+BlvWxO/vnIeBVlaB2fNGsmakyZzjb28o6IoSndRReCh\nzVU++uTpI3j0ja00Z1jgHax1BtwZOd+9YHanNoWuQLBTzbO8KMD3Pty57alplmBMRYHfx/WnTc26\nvaIoiheNEXhwB12Dfh8VRYGMawlni3t+wLiqkgwtFUVR+hZVBB7cC8oUFvh6paQCeCwCVQSKogwg\nVBF4cLuBAv7eUwTuJQJHeWr/KIqi9CcaI/DQ3J5QBMECX6eyy7/9+LysSih7GVVZzH2fnE9pYUHS\nKlyKoij9jSoCD01tyYrAaxFMrCljVGX21TPdHDcld8ssKoqi9BQdmnpobkusNxCwg8VutB6OoiiD\nDe3VPHRlEagiUBRlsKG9mgf3CmSFfh9DPEs+OiUdFEVRBgsaI/DgrtkTKBBqKpLjAWoRKIoy2NBe\nzUNHJLEcZdDvj88CdtCMH0VRBhvaq3lIsgj8krSGsKIoymBEFYGL//332viiMmAFizUmoCjKYEdj\nBC5++MTqpG2nXPSNZ09n9fZ9WiNIUZRBiSoCG2M6V5YL2vGAa47TEs+KogxecuoaEpEzRGS1iKwV\nkUUpjo8TkWdE5HUReVNEzsqlPJlwB4kdgpohpChKHpCznk5E/MAdwJnADOBSEZnhaXYj8KAx5jDg\nEuB/cyVPV7jXIXBQRaAoSj6Qy55uPrDWGLPOGBMCHgDO87QxgFPBrRLYmkN5MtKaYs3fgKaKKoqS\nB+SypxsDbHZt19v73NwCXC4i9cDjwOdTXUhEForIMhFZtmvXrlzISlso0mlfUBWBoih5QH/3dJcC\nvzPG1AJnAfeJSCeZjDF3GWPmGWPm1dTkpoJnawrXkM4iVhQlH8hlT7cFGOvarrX3ufkk8CCAMeYl\noAgYlkOZ0pJKEahrSFGUfCCXPd2rwBQRmSAiQaxg8BJPm03AyQAiMh1LEeTG95OBHc3tbGho6bQ/\noBaBoih5QM7mERhjIiJyHfAk4AcWG2NWisitwDJjzBLgeuDXIvJlrMDxVSZVQn+OWfDdp5O2h5QE\n2Nsa1hiBoih5QU4nlBljHscKArv33eT6vAo4Jpcy9IRbzpnJydOHa/qooih5gfZ0KRCB8qLeWbRe\nURRloKOKIAXRWJ97pxRFUfqNvKw1tLO5nUde20IoEqPAL52OqyJQFCWfyEtF8IUHXufldXtSHhtf\nXcJJ04b3sUSKoij9R14qAve6xG7KCgt49qsn9rE0iqIo/Utexgj80tkdBFBZrAFiRVHyj7xUBJJG\nEZQX5aWBpChKnpOXiqDAl1oR1FWX9rEkBzG734efzoZHv9h1212r4Wez4X+mw4YXUrdpb4Lb58Hv\nL8xeht9fBD+aDP+0p6Ys/Ym17X796oMQ6cj+mploWAs/nQWPfimx7/1/Wc8Wau2dexwI//mV65lD\n/S2NchCRl0Ngn0sRfGzBOMZVlSDAR48Ym/4kJZkdK6FpEyz/HZzz8y7avg17N1mft74OdSnmEDZu\ngN1rrJcx1mSOTBgDa58CDKx/ztq3Yan1Pv0c671hjbWvZRdU1mb5YBnYuRKaNsPyu+Gcn1n7nvh/\n1rPtWQcjZx34PQ4E51mdV6W32K+ipCY/FYGrj7nm2AlMrCnrP2EOVsLdGAG7R8vpznO3iYagoDDz\nNSPtWFVJXOeGWqFmGnzop9b2mw9ZnWNvjdYzXmcApBxn8z0rSgry0jXkd2kCLSPRQ0Kdi/Slxd0p\npTsv3NJ1m6T7p+j0wi0QdLn3giWdr30gZLpOdAC4YrL5nhUlBXnZC/pEFcEB4+50uqoT6HRK/sLs\nLIJsRrNh1zWd64daIVCSaON87guLYCDECEItUFBkfVaLQOkGedkLJlkEWmG0Z3hdOZkItwICJdXp\nO8yk0WwWnZjTprTGZRG0JqwASFgHvdUpuq/jDcYOhI433Gp9HzAwFJNy0JCXvaBfLYIDpzuuHGek\nHixN715xXyMbV47TpqzGihfEovZ9XK6huEXQS26STDIOBFdMqBVK7XWdessdpuQFedkLurOGdBWy\nHtIdV064xRqpB0tyYxE45zv3cYjHCHJgEXhlHBAWQYtaBEqPyMte0G0RpJtToHRBdzpuxyIIlPZi\njMCjCNqbIBbxWAT2516zCDLIOBA63pDLNaQWgdINulQEIvJ5ERnaF8L0Bet27WfZxkTBuXSzjJUu\n6I4rJ9xquYWCJb2YNWS3cVwhLfYKpzm1CDLI2N8dbyQEsXDi+xgIikk5aMjGIhgBvCoiD4rIGXKQ\n95wn/c+zNOwfAKl+BzvdsghabIugJHcWQUuD9d5XWUPO/U2sd+/RUxxFVOLECFQRKNnTpSIwxtwI\nTAF+C1wFrBGR74rIpBzLpgxkQq3gD1qfu4wR2Nk8wdLMMQLnet2ZRxBXBI5F4HIN+fxWOmWvzSNo\nBV8g+f7htsSx/sSRp7A8OaVWUbIgqxiBvaD8dvsVAYYCD4vIDzOdZ1sQq0VkrYgsSnH8pyKywn69\nJyJ7e/AMSn+QFJjMImsoWGZbBBmyhtyB32zuDylcQ556UZmUT3dJkrEl+b2/O17nOwuW2dlZahEo\n2dNliQkR+SJwJdAA/Ab4qjEmLCI+YA3wtTTn+YE7gFOBeiz30hJ7wXoAjDFfdrX/PHDYATyL0pc4\ngcnmLdllDQWyyBoqqbaul3XWkD03ARKKwO0agswB6u4SbrXSVfdtTS5r4RzrTxxF1JXlpSgpyKbW\nUBXwYWPMRvdOY0xMRD6U4bz5wFpjzDoAEXkAOA9Ylab9pcDNWcij5IKNL8E/v2nl42dD43qYcLz1\n+ZnvwYo/WoXiOvZ3brt3M9Qda41WI23wwGXW/pZdEA1bnxvWwKhDrY58+d2w5h+Z79+8xVYu5db2\nij9a750sghJY/Xe4qxcWHGrcABM+aH1+5rvwyl3W8wC892Tv3KOnOIrAicWsfvzA5Zl1IRx9Hbx2\nHyxbnLrN2AVwys3wpyugdXf6awVKLFddxz7LXXf+HbD2aet3K6qAaCShTBdcC3M+Cu3N8OCVVkaY\nw4iZcN4vrM/7dsAjn4RxR8JJNx7Ysx4oG16Ap26Boz4LMy+w9hkDf/mMVX3XoaQKPnJfclJDKv56\nHezdaF2jaiKce1vORIfsFMHfgXiajYhUANONMf8xxryT4bwxwGbXdj2wIFVDERkPTAD+leb4QmAh\nwLhx47IQWek26/4N9a/C5FOzaz/xBJj/aauq585Vicqfww6BIeOT20460epUiobAv78H7z7mOnYS\niB/GVcPsi2D8MVaF0q4oqYYxh8PQ8TDnY5ZSKaqA4TOS2x1xjdVJ9wYTT4AF9jM3bbH2HXIGlA2H\n5m29c4+eUlINI2bA6A9Yz9yVIu2Kra/D249YimDVX62y42PnJ7dpWG115IdfCWv/CSMPhbIRna/V\nsQ82Pm99rqiF5nqoXwYr/wxbX0u0G3cUbH8b3n3UUgQN78G6Z2D0YVYQfM/78Prv4dzbrUHH9jet\nv7sNS/tfEbz/NNS/Au/WJRRBNARv3A/VU2BoHbTstCrmdlWpNhaD1+9LbG9YalX4zWGeTjaK4E7g\ncNf2/hT7DpRLgIeNMSmHo8aYu4C7AObNmzcAyjwOQmJh8BXA5Q9377wpp8COVXDnUdb2/IUw/1Pp\n24+aA9veSGxf9rA1UjwQLrgz/bH5n8osT0+YfErvXq+3WbDQeh0If7rC6ojBGqmPnN35b+Opb8GL\ntyVG8id9Ew45rfO1GtbAL+ZZn2dfCC/83LJgvHGV8+6A//uUy+1mHz/t25ZF+fxPrVF3uC1zKnJ/\nkMpF6Mh3xDVw5LWWEvj9hV27ER0rM2lfBwSKekfWFGQTLBY7WAxYLiGyUyBbAHeB/1p7XyouAe7P\n4ppKroiGEhkx3SWYImUzHe4JXwVFB64ElNzgjjOEWlK7MoIl1iS+9r2J7VS4/yaSZoK3dm7nTjF2\n3p3zA57aUf0dl3GTKmkgHsD3yB9K4T51kyq+k2Oll40iWCciXxCRgP36IrAui/NeBaaIyAQRCWJ1\n9ku8jURkGlYW0kvdEVzpZaKRRPpmdwmkKP2cDvfxgtyNcJQDxJ3hFW5NreCd3z3VHA437t+8xDXh\nLdSaPPiIB7o9naoT93Gu4z0OljulP0lpEXgUWVz+LJIrstnXi2SjCK4FjsYazTt+/i7tTmNMBLgO\neBJ4B3jQGLNSRG4VkXNdTS8BHnBbHUo/EA2Bv4frFCVZBF0s99mVxaAMDNwZXqHWzkF4pw3A/p32\ndprf3v03UVRhDTjCLckpyE67jBaBZ6a4u9Ptb+sg7Pqu4vs8isxr0aQjpUWQ2+fr8j/fGLMTq7Pu\nNsaYx4HHPftu8mzf0pNr9wbPfvWE/rr1wCMW7rlFUFCc+NylRaDrQh8UBEotX3UsarkyMloEtiJI\np+QLXH9XjvvHiRFUjrXSccEaiLh9/50sAk/tKLdFEGqBwn5cadBx97jdPu5MLuhs0aS9Vt9bBNnM\nIygCPgnMBOK2vDHmEzmUK+dMqillvC5WnyAa7nmMwOcyLLuMEahFcFDgrtPkXefB28ZxDWWj5IOl\n1qu92bJC3RYBWMrFGyz2WgSpFEF/13rK5BqKWwRZ1r5K9Sw5tgiycQ3dB4wETgeexQr67sulUH2B\nOqI8RMPg76EicNNVZ9CVxaAMDJxOy+mwgylG206bdLO60103UAKttvLwKoKgJzYhvsT61d4igt0t\nXZ5LMrmG4hZBaec2qUh1PMeur2wUwWRjzDeBFmPMPcDZpJkPcDARU02QTDTUO4qgqxF/T91PSt/i\ndFqtGQLBTpuWXdZckGx+W2ddCkd5lKWwCGIRq5qqs9CQkz/vLSve3UKFucSRKdySGGXGLQL7u/MH\nLKs7m2q96a6fI7JRBPbUT/aKyCygEhieO5H6hpjqgWT6yiLgoC5emz90Gu2nihG4XEPB0uwmPAVK\nrVdLBosAEsHkTGXFu1u6PJe4q9FGOpL3ebPqsqnWm+76OSIbRXCXvR7BjVjpn6uAH+RUqj5ALQIP\nsQOIEbjRGMDgID7adyyCDFlDLbuy/929FkGnGIErxdJZ0Ch+zONaCbValggMAIug1ZqQ6ZbFXf/J\nIZBhuVaHlBZBP2YN2YXlmo0xjcBzwMScStOHqB7wEA31jtvG8ecqBzdZWQR2xxwNZR/7cWIEUXtN\nEGdegYOjgOJB6hRzVNwxhNIa2L+9f2MExliylI+EfdssBVBSlejQvVl1Pcka6moS2gGS0SKwZxGn\nrC56sKMWgYdopHdcQwf3ukWKQ9CjCFLGCLoxf8TB50/u3AvLk4+7M4OcBY0cCooASc4qGghLc4bb\nAJMoie62CAIlnbPqupxQZgfJ40jOLZ5sZhA9JSI3AH8C4t+2MWZP+lMGJo8sr49/zktF8PrvYf1z\ncNp3OgfpoiEIVPSPXMrAw+nYV/7Fek8V+wmk8N9nde0M5znb/7gRdqy0KtI6iFhyrPyzVQdpz3qo\ntWsYvfIbq5ppf+BU0HWU0uNfhaJKqyieV4EGS2HLcquWk5u5H4ehE+CZ78DWFXYarZ2c6Q/Ccz+C\n427IWb2hbBTBR+33z7n2GQ5CN9H1DyWKneVlsPhvN1iThKaeBTPPTz52oDGC8+9MLiaXjvkLYeML\nVlDt5Ju6bq/0D0PGQt1xVoxg7AKrqqwXnx9mXWR12NPOzny9D/8aNv/H+jz5ZNj0klWNtmqiVZRt\n7JHWseEzofYI675lI6y/VTezLoTNr1iF7CprYfbFlqXQuMHa11+MPBSO+JSVbrt/p/UqKO5czXfa\n2fD6H5Jl3fO+peTGHWVVfB12CMz6sFVZV3zw5kOwc6X1GjM3J+LLwVbZYd68eWbZsmU9Ordu0d/i\nn4eVFbLsxgFeRbK3uaXSej//TvjAx5KP/e9R1j/lJX/oe7kUJZ/51fGWNTHuSPjXf8ONO5Njbeue\nhXvPhY8/BhOO6/FtRGS5MWZeqmPZzCy+MtV+Y8y9PZZoAHCwKcBeJVUwKnoAJSYURek5ztKi4dbU\n8zHcAfQckY1r6AjX5yLgZOA14KBWBHkZI3BI9QfVWxPKFEXpHsFSy5XkrO3tTbjwltbIAdkUnfu8\ne1tEhgAP5EyiPiLvYgRuxZcqayHWS1lDiqJ0D6fiqncCnUMfWATZTCjz0oK1rORBTd5ZBGHXqkep\nUu0OZGEaRVF6jrMIkDdd1n0ccjpXIpsYwaNYWUJgKY4ZwIM5k6iPyDc90GWBLo0RKEr/4CwCFEpT\n5TXgmUiXA7KJEfzY9TkCbDTG1KdrfLCQdxZBqiX03PRWrSFFUbqHU38o3JJ6Yl6gmKSJdDkgG0Ww\nCdhmjGkHEJFiEakzxmzImVR9QL7pgZSLaruJqSJQlH4hUArRDujYB8VDOx8XSV65LQdkEyN4CHAv\nCBq19x3UnD5zRH+L0LdkKtlrjMYIFKW/iJfz2J153ef+zBoCCowxIWfDGBOyF6M/aPnSKVP47AmT\n+1uMvsXxL4qvs4kZi1rvGiNQlL7HXeAv7brP/W8R7HIvNi8i5wEN2VxcRM4QkdUislZEFqVp8xER\nWSUiK0Xkj9mJfWAMLy8iWNCThKmDGKfzL63pHHRyKkH2dPF6RVF6jtP5R9oyWASl/W4RXAv8QUR+\nYW/XAylnG7sRET9wB3Cqfc6rIrLEGLPK1WYK8A3gGGNMo4j0yYI3kVis60aDDafzL61JYRHYRbPU\nIlCUviepCF8Gi6CfJ5S9DxwpImX2draFsecDa40x6wBE5AHgPKyFbRw+Bdxhr3eAMWZnN2TvMZHo\nII8Uv/UwbH09ed/Od6z30mFQvxye/K/EMWeOgcYIFKXvSSrnnSFGsO4Z2PQfGNf7KwVnM4/gu8AP\njTF77e2hwPXGmBu7OHUMsNm1XU/ntY4Psa/5AuAHbjHGPJFChoXAQoBx48Z1JXKXRAf7tOK/fx3a\n99r1211UT4FDzrQUwfLfJR8rHgrDp/eZiIqi2FRPgbKR1oBs9AdStxm7wK66urp/FAFwpjHm/zkb\ntgvnLKylK3vj/lOAkw0AOwAAD+JJREFUE4Ba4DkRme0oHdc97wLuAqv6aE9u5C4yFx7srqHQfjjy\nM3Dat1MfP/LavpVHUZT0DB0PN6zO3OakG61XjsgmYuoXkXhNVBEpBrJZj3ALMNa1XWvvc1MPLDHG\nhI0x64H3sBRDr+O2AqKD2TUUi0KkPfsVoxRFyXuyUQR/AJ4WkU+KyDXAP4F7sjjvVWCKiEyw000v\nAZZ42vwFyxpARIZhuYrWZSl7t3B7g6KDeTaZk2LWnRWjFEXJa7IJFv9ARN4ATsGqOfQkMD6L8yIi\ncp3d3g8sNsasFJFbgWXGmCX2sdNEZBXWRLWvGmN29/xx0uMuKeEfzOvqOhlB6YJOiqIoHrJNHN+B\npQQuBtYDj2RzkjHmceBxz76bXJ8N8BX7lVMcReATuOa4g26Vzexx0kTTpaEpiqJ4SKsIROQQ4FL7\n1YC1eL0YY07sI9l6FSdG8I0zp1Mc9PezNDlELQJFUbpJJovgXWAp8CFjzFoAEflyn0iVA5wYgc83\niN1C4IoRqEWgKEp2ZAoWfxjYBjwjIr8WkZOBg7YXjcUSrqFBjTP7UC0CRVGyJK0iMMb8xRhzCTAN\neAb4EjBcRO4UkdP6SsDewskU8g92TaBZQ4qidJMu00eNMS3GmD8aY87BmgvwOvD1nEvWyySCxYNc\nEcRjBOoaUhQlO7pVgtMY02iMucsYc3KuBMoVzmTiQa8INGtIUZRukje1mGNx11A/C5JrQuoaUhSl\ne+RNAXonfVS6axG0NcLbj0A0Yi3qMvN8iEXgncfADMCaReuftd7VNaQoSpbkjSJwJhZ3e1bxmw/B\n37+a2G5tsAKyL97ee8L1NpXjoEDXFlAUJTvyRhE4WUO+7rqGOpqs9xvWwG2HQ3uzpQhKh8Pn/tO7\nQvYWwbL+lkBRlIOIvFEEPc4aCrVYC7aUDbcCsOEWq254YRmUVOVAUkVRlL4lfxRBrKeKoDUReA2W\nWNvhNvXBK4oyaMgfReDECLo7oSzckuj0A/YC0uFWzcpRFGXQMNiTKeNEe1piwmsRhG1FoCUcFEUZ\nJOSNIuhxjMDd6Qds11CoVSdsKYoyaMg7RdBt11CoJdHpB0stxRBuUYtAUZRBQ94ogmhPg8WdLIKW\nZHeRoijKQU7eKIIer0fQKUbQaisHdQ0pijI4yKOsoZ4Gi71ZQ62aNaQoyqAipxaBiJwhIqtFZK2I\nLEpx/CoR2SUiK+zXNbmSxZlH0O0SE+GWzllDGI0RKIoyaMiZRSAifuAO4FSgHnhVRJYYY1Z5mv7J\nGHNdruRwcEpMdLvoXMgTI3DQrCFFUQYJuXQNzQfWGmPWAYjIA8B5gFcR9Akm1YSytkbY+GLiYOez\nINKWnDXkoBaBoiiDhFwqgjHAZtd2PbAgRbsLReSDwHvAl40xm70NRGQhsBBg3LhxPRIm5YSy534M\nL/2i65PLRya/A5SP6pEciqIoA43+DhY/CtxvjOkQkU8D9wAneRsZY+4C7gKYN29euuF7RhLVR12a\noHU3lI2Eyx5Kf6KvAGqmWZ+nnwufewUQGDalJ2IoiqIMOHKpCLYAY13btfa+OMaY3a7N3wA/zJUw\nxqQIFodaoHgIjDo0u4uIQM3UHEinKIrSf+Qya+hVYIqITBCRIHAJsMTdQETc/pVzgXdyJUw01ZrF\nWjNIURQldxaBMSYiItcBTwJ+YLExZqWI3AosM8YsAb4gIucCEWAPcFWu5ImlWphGawYpiqLkNkZg\njHkceNyz7ybX528A38ilDA4p1yMIt1gxAkVRlDwm70pMJKWPas0gRVGU/FEE0VQlJrRmkKIoSv4o\nApNqPYJQi1oEiqLkPXmjCFKWodasIUVRlPxTBPEYQTQM0ZBmDSmKkvfkjSJwygnFDYJQi/WuFoGi\nKHlO3iiCqHepynCr9a4xAkVR8py8UQQxb4mJfdutd80aUhQlz8kfRRDzrEew5PPWe0l1P0mkKIoy\nMMgfReCdUBbpsGYVTzqx/4RSFEUZAOSNIui0HkG4FSafDD5//wmlKIoyAMgbRRDzrkcQatGMIUVR\nFPJREYgra0gzhhRFUfJJEVjvfhGIRqzJZJoxpCiK0u9LVfYZC4+byDXHTrCCxR3N1k61CBRFUfJH\nEfh8gg8nPmBPJtMYgaIoSv64hpKIzypW15CiKEp+KgKtM6QoihInPxWBWgSKoihx8lMROBaBKgJF\nUZTcKgIROUNEVovIWhFZlKHdhSJiRGReLuWJE9ZgsaIoikPOFIGI+IE7gDOBGcClIjIjRbty4IvA\nf3IlCwDhNmjdYy1M0NZo7VOLQFEUJacWwXxgrTFmnTEmBDwAnJei3X8DPwDacygLvHIX/HAC/OPG\nROXRwoqc3lJRFOVgIJeKYAyw2bVdb++LIyKHA2ONMX/LdCERWSgiy0Rk2a5du3omzYTjrfedq6z3\n8cdAWU3PrqUoijKI6LdgsYj4gJ8A13fV1hhzlzFmnjFmXk1NDzvv0R+AEbOhba+1Pfuinl1HURRl\nkJFLRbAFGOvarrX3OZQDs4B/i8gG4EhgSU4Dxv4AtNuKQOsMKYqiALlVBK8CU0RkgogEgUuAJc5B\nY0yTMWaYMabOGFMHvAyca4xZljOJ/AFob7I+a50hRVEUIIeKwBgTAa4DngTeAR40xqwUkVtF5Nxc\n3Tcj/mDCNaSpo4qiKECOi84ZYx4HHvfsuylN2xNyKQsAvgIwUeuzpo4qiqIA+Taz2B9MfFaLQFEU\nBcg7RRBIfFaLQFEUBchnRaAWgaIoCpBvisDntghUESiKokC+KYKkGIG6hhRFUSDvFIGdJOUPJj4r\niqLkOXmmCGyLQOMDiqIocfJTEWjGkKIoSpz8UgQ+2x2kFoGiKEqc/FIEcYtAFYGiKIpDnikCO300\nWNa/ciiKogwg8lMRqGtIURQlTn4pAmdCmbqGFEVR4uSXIoinj2rWkKIoikOeKQI7a0gtAkVRlDh5\npgh0QpmiKIqX/FIE8RiBuoYURVEc8ksRaNaQoihKJ/JTEWiMQFEUJU5OFYGInCEiq0VkrYgsSnH8\nWhF5S0RWiMjzIjIjl/Jo1pCiKEpncqYIRMQP3AGcCcwALk3R0f/RGDPbGPMB4IfAT3IlD5CoNaQW\ngaIoSpxcWgTzgbXGmHXGmBDwAHCeu4Exptm1WQqYHMqjFoGiKEoKcrk6yxhgs2u7HljgbSQinwO+\nAgSBk1JdSEQWAgsBxo0b13OJaufB0Z+H8Uf1/BqKoiiDjH4PFhtj7jDGTAK+DtyYps1dxph5xph5\nNTU1Pb9ZoBhO+7amjyqKorjIpSLYAox1bdfa+9LxAHB+DuVRFEVRUpBLRfAqMEVEJohIELgEWOJu\nICJTXJtnA2tyKI+iKIqSgpzFCIwxERG5DngS8AOLjTErReRWYJkxZglwnYicAoSBRuDjuZJHURRF\nSU0ug8UYYx4HHvfsu8n1+Yu5vL+iKIrSNf0eLFYURVH6F1UEiqIoeY4qAkVRlDxHFYGiKEqeI8bk\ntqpDbyMiu4CNPTx9GNDQi+IcDOgz5wf6zPnBgTzzeGNMyhm5B50iOBBEZJkxZl5/y9GX6DPnB/rM\n+UGunlldQ4qiKHmOKgJFUZQ8J98UwV39LUA/oM+cH+gz5wc5eea8ihEoiqIonck3i0BRFEXxoIpA\nURQlz8kbRSAiZ4jIahFZKyKL+lue3kJEFovIThF527WvSkT+KSJr7Peh9n4Rkdvs7+BNETm8/yTv\nOSIyVkSeEZFVIrJSRL5o7x+0zy0iRSLyioi8YT/zt+z9E0TkP/az/cku+Y6IFNrba+3jdf0pf08R\nEb+IvC4ij9nbg/p5AURkg4i8JSIrRGSZvS+nf9t5oQhExA/cAZwJzAAuFZEZ/StVr/E74AzPvkXA\n08aYKcDT9jZYzz/Ffi0E7uwjGXubCHC9MWYGcCT8//buKMSKKo7j+PdHWlmGlpZIWyxiEES2hYSW\nDyYUIdFLgolQxEIgEfZSIUFPvdRDltVDRUQPUhAliQ+laURQZFhqilkaQsnaaqQRhJj9ezj/uw2r\nhpt7d9yZ3weGe+Y/w+X8787eM3Nm7jk8kn/PJud9HFgUETcBfcDdkuYBzwKrI2I2ZSj3/ty/H/gt\n46tzv/FoJbCnst70fDvuiIi+ym8GuntsR0TjF2A+8FFlfRWwqu56jWJ+vcCuyvpeYGaWZwJ7s/wq\nsOx0+43nBfgAuLMteQOXAF9T5gA/AkzI+NBxTpkHZH6WJ+R+qrvuI8yzJ7/0FgEbADU530reB4Dp\nw2JdPbZbcUUAXA38VFn/OWNNNSMiBrJ8CJiR5cZ9DtkFcDPwJQ3PO7tJtgODwCZgP3A0Iv7KXap5\nDeWc248B08a2xufsBeAJ4O9cn0az8+0IYKOkbZIezlhXj+2uTkxj9YuIkNTIZ4QlTQbeAx6LiN8l\nDW1rYt4RcRLokzQVWAdcX3OVukbSPcBgRGyTtLDu+oyxBRFxUNJVwCZJ31U3duPYbssVwUHgmsp6\nT8aa6hdJMwHydTDjjfkcJE2kNAJrI+L9DDc+b4CIOAp8QukamSqpc0JXzWso59w+Bfh1jKt6Lm4H\n7pV0AHiH0j30Is3Nd0hEHMzXQUqDfytdPrbb0hB8BVyXTxxcCNwPrK+5Tt20nn/nf36Q0ofeiT+Q\nTxrMA45VLjfHDZVT/zeAPRHxfGVTY/OWdGVeCSBpEuWeyB5Kg7Akdxuec+ezWAJsiexEHg8iYlVE\n9EREL+X/dUtELKeh+XZIulTSZZ0ycBewi24f23XfGBnDGzCLge8p/apP1V2fUczrbWAAOEHpH+yn\n9I1uBn4APgauyH1FeXpqP/AtMLfu+v/PnBdQ+lF3AttzWdzkvIE5wDeZ8y7g6YzPArYC+4B3gYsy\nfnGu78vts+rO4RxyXwhsaEO+md+OXHZ3vqu6fWx7iAkzs5ZrS9eQmZmdgRsCM7OWc0NgZtZybgjM\nzFrODYGZWcu5ITAbRtLJHPmxs4zaaLWSelUZKdbsfOAhJsxO9WdE9NVdCbOx4isCs7OU48Q/l2PF\nb5U0O+O9krbkePCbJV2b8RmS1uUcAjsk3ZZvdYGk13NegY35S2Gz2rghMDvVpGFdQ0sr245FxI3A\ny5TRMQFeAt6KiDnAWmBNxtcAn0aZQ+AWyi9FoYwd/0pE3AAcBe7rcj5m/8m/LDYbRtIfETH5NPED\nlMlhfsxB7w5FxDRJRyhjwJ/I+EBETJd0GOiJiOOV9+gFNkWZYARJTwITI+KZ7mdmdnq+IjAbmThD\neSSOV8on8b06q5kbArORWVp5/SLLn1NGyARYDnyW5c3AChiaVGbKWFXSbCR8JmJ2qkk5E1jHhxHR\neYT0ckk7KWf1yzL2KPCmpMeBw8BDGV8JvCapn3Lmv4IyUqzZecX3CMzOUt4jmBsRR+qui9locteQ\nmVnL+YrAzKzlfEVgZtZybgjMzFrODYGZWcu5ITAzazk3BGZmLfcPhsnUGJQeoZEAAAAASUVORK5C\nYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "# Evaluate on test data\n",
            "40/40 [==============================] - 0s 2ms/sample - loss: 0.9766 - acc: 0.4750\n",
            "test loss, test acc: [0.9765959481708706, 0.475]\n",
            "EEG_Deep/Data2A/parsed_P04T.mat\n",
            "Filtering of Training Data Finished\n",
            "EEG_Deep/Data2A/parsed_P04E.mat\n",
            "Filtering of Testing Data Finished\n",
            "(1, 2)\n",
            "Finding labels in training data\n",
            "Finding labels in testing data\n",
            "[1 1 2 1 2 2 1 2 1 2 1 2 2 1 2 1 1 1 2 1]\n",
            "(60, 12, 1536)\n",
            "(60,)\n",
            "(20, 12, 1536)\n",
            "(20,)\n",
            "[[1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]]\n",
            "X_train shape: (60, 1, 12, 1536)\n",
            "60 train samples\n",
            "20 val samples\n",
            "(40, 12, 1536)\n",
            "(40, 1)\n",
            "X_train shape: (40, 1, 12, 1536)\n",
            "40 train samples\n",
            "Train on 60 samples, validate on 20 samples\n",
            "Epoch 1/500\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.68977, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 1s - loss: 0.7872 - acc: 0.4333 - val_loss: 0.6898 - val_acc: 0.6000\n",
            "Epoch 2/500\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.68977\n",
            "60/60 - 0s - loss: 0.6807 - acc: 0.5667 - val_loss: 0.6902 - val_acc: 0.6500\n",
            "Epoch 3/500\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.68977\n",
            "60/60 - 0s - loss: 0.6717 - acc: 0.6000 - val_loss: 0.6902 - val_acc: 0.6500\n",
            "Epoch 4/500\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.68977\n",
            "60/60 - 0s - loss: 0.6592 - acc: 0.6000 - val_loss: 0.6899 - val_acc: 0.6000\n",
            "Epoch 5/500\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.68977 to 0.68975, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6489 - acc: 0.6667 - val_loss: 0.6897 - val_acc: 0.6000\n",
            "Epoch 6/500\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.68975 to 0.68948, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6475 - acc: 0.7000 - val_loss: 0.6895 - val_acc: 0.6000\n",
            "Epoch 7/500\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.68948\n",
            "60/60 - 0s - loss: 0.6435 - acc: 0.7500 - val_loss: 0.6897 - val_acc: 0.6000\n",
            "Epoch 8/500\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.68948\n",
            "60/60 - 0s - loss: 0.6213 - acc: 0.7833 - val_loss: 0.6897 - val_acc: 0.6000\n",
            "Epoch 9/500\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.68948\n",
            "60/60 - 0s - loss: 0.6175 - acc: 0.7833 - val_loss: 0.6897 - val_acc: 0.6000\n",
            "Epoch 10/500\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.68948\n",
            "60/60 - 0s - loss: 0.6312 - acc: 0.7000 - val_loss: 0.6897 - val_acc: 0.5500\n",
            "Epoch 11/500\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.68948\n",
            "60/60 - 0s - loss: 0.6166 - acc: 0.7333 - val_loss: 0.6900 - val_acc: 0.5500\n",
            "Epoch 12/500\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.68948\n",
            "60/60 - 0s - loss: 0.5825 - acc: 0.8833 - val_loss: 0.6898 - val_acc: 0.6000\n",
            "Epoch 13/500\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.68948\n",
            "60/60 - 0s - loss: 0.6082 - acc: 0.7500 - val_loss: 0.6898 - val_acc: 0.6000\n",
            "Epoch 14/500\n",
            "\n",
            "Epoch 00014: val_loss improved from 0.68948 to 0.68923, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5758 - acc: 0.9000 - val_loss: 0.6892 - val_acc: 0.6000\n",
            "Epoch 15/500\n",
            "\n",
            "Epoch 00015: val_loss improved from 0.68923 to 0.68801, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5602 - acc: 0.9333 - val_loss: 0.6880 - val_acc: 0.6000\n",
            "Epoch 16/500\n",
            "\n",
            "Epoch 00016: val_loss improved from 0.68801 to 0.68734, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5745 - acc: 0.8333 - val_loss: 0.6873 - val_acc: 0.5500\n",
            "Epoch 17/500\n",
            "\n",
            "Epoch 00017: val_loss improved from 0.68734 to 0.68686, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5868 - acc: 0.8500 - val_loss: 0.6869 - val_acc: 0.6000\n",
            "Epoch 18/500\n",
            "\n",
            "Epoch 00018: val_loss improved from 0.68686 to 0.68644, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5564 - acc: 0.9000 - val_loss: 0.6864 - val_acc: 0.6000\n",
            "Epoch 19/500\n",
            "\n",
            "Epoch 00019: val_loss improved from 0.68644 to 0.68580, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5583 - acc: 0.8667 - val_loss: 0.6858 - val_acc: 0.6000\n",
            "Epoch 20/500\n",
            "\n",
            "Epoch 00020: val_loss improved from 0.68580 to 0.68530, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5759 - acc: 0.8333 - val_loss: 0.6853 - val_acc: 0.5000\n",
            "Epoch 21/500\n",
            "\n",
            "Epoch 00021: val_loss improved from 0.68530 to 0.68484, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5168 - acc: 0.8833 - val_loss: 0.6848 - val_acc: 0.5000\n",
            "Epoch 22/500\n",
            "\n",
            "Epoch 00022: val_loss improved from 0.68484 to 0.68441, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5298 - acc: 0.9000 - val_loss: 0.6844 - val_acc: 0.5000\n",
            "Epoch 23/500\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.68441\n",
            "60/60 - 0s - loss: 0.5185 - acc: 0.9167 - val_loss: 0.6845 - val_acc: 0.5000\n",
            "Epoch 24/500\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.68441\n",
            "60/60 - 0s - loss: 0.5279 - acc: 0.8833 - val_loss: 0.6848 - val_acc: 0.5000\n",
            "Epoch 25/500\n",
            "\n",
            "Epoch 00025: val_loss improved from 0.68441 to 0.68425, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5268 - acc: 0.8500 - val_loss: 0.6842 - val_acc: 0.5000\n",
            "Epoch 26/500\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.68425\n",
            "60/60 - 0s - loss: 0.4972 - acc: 0.9000 - val_loss: 0.6845 - val_acc: 0.4500\n",
            "Epoch 27/500\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.68425\n",
            "60/60 - 0s - loss: 0.5226 - acc: 0.8833 - val_loss: 0.6850 - val_acc: 0.4500\n",
            "Epoch 28/500\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.68425\n",
            "60/60 - 0s - loss: 0.4892 - acc: 0.8833 - val_loss: 0.6845 - val_acc: 0.4500\n",
            "Epoch 29/500\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.68425\n",
            "60/60 - 0s - loss: 0.4622 - acc: 0.9000 - val_loss: 0.6864 - val_acc: 0.4500\n",
            "Epoch 30/500\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.68425\n",
            "60/60 - 0s - loss: 0.4798 - acc: 0.8667 - val_loss: 0.6892 - val_acc: 0.4500\n",
            "Epoch 31/500\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.68425\n",
            "60/60 - 0s - loss: 0.4459 - acc: 0.9333 - val_loss: 0.6923 - val_acc: 0.4500\n",
            "Epoch 32/500\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.68425\n",
            "60/60 - 0s - loss: 0.4240 - acc: 0.9667 - val_loss: 0.6927 - val_acc: 0.4500\n",
            "Epoch 33/500\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.68425\n",
            "60/60 - 0s - loss: 0.4492 - acc: 0.8667 - val_loss: 0.6955 - val_acc: 0.4500\n",
            "Epoch 34/500\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.68425\n",
            "60/60 - 0s - loss: 0.4517 - acc: 0.8500 - val_loss: 0.7023 - val_acc: 0.4500\n",
            "Epoch 35/500\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.68425\n",
            "60/60 - 0s - loss: 0.4502 - acc: 0.8667 - val_loss: 0.7122 - val_acc: 0.4500\n",
            "Epoch 36/500\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.68425\n",
            "60/60 - 0s - loss: 0.3933 - acc: 0.9333 - val_loss: 0.7101 - val_acc: 0.4500\n",
            "Epoch 37/500\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.68425\n",
            "60/60 - 0s - loss: 0.4051 - acc: 0.9000 - val_loss: 0.7155 - val_acc: 0.4500\n",
            "Epoch 38/500\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.68425\n",
            "60/60 - 0s - loss: 0.3849 - acc: 0.8833 - val_loss: 0.7189 - val_acc: 0.4500\n",
            "Epoch 39/500\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.68425\n",
            "60/60 - 0s - loss: 0.4045 - acc: 0.9167 - val_loss: 0.7224 - val_acc: 0.4500\n",
            "Epoch 40/500\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.68425\n",
            "60/60 - 0s - loss: 0.3982 - acc: 0.8833 - val_loss: 0.7263 - val_acc: 0.4500\n",
            "Epoch 41/500\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.68425\n",
            "60/60 - 0s - loss: 0.3363 - acc: 0.9333 - val_loss: 0.7254 - val_acc: 0.4500\n",
            "Epoch 42/500\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.68425\n",
            "60/60 - 0s - loss: 0.3959 - acc: 0.8167 - val_loss: 0.7281 - val_acc: 0.4500\n",
            "Epoch 43/500\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.68425\n",
            "60/60 - 0s - loss: 0.3719 - acc: 0.9167 - val_loss: 0.7290 - val_acc: 0.4500\n",
            "Epoch 44/500\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.68425\n",
            "60/60 - 0s - loss: 0.3293 - acc: 0.9500 - val_loss: 0.7294 - val_acc: 0.4500\n",
            "Epoch 45/500\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.68425\n",
            "60/60 - 0s - loss: 0.3669 - acc: 0.8500 - val_loss: 0.7415 - val_acc: 0.4500\n",
            "Epoch 46/500\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.68425\n",
            "60/60 - 0s - loss: 0.3117 - acc: 0.9167 - val_loss: 0.7516 - val_acc: 0.4500\n",
            "Epoch 47/500\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.68425\n",
            "60/60 - 0s - loss: 0.3364 - acc: 0.9333 - val_loss: 0.7490 - val_acc: 0.4500\n",
            "Epoch 48/500\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.68425\n",
            "60/60 - 0s - loss: 0.2913 - acc: 0.9500 - val_loss: 0.7489 - val_acc: 0.4500\n",
            "Epoch 49/500\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.68425\n",
            "60/60 - 0s - loss: 0.3096 - acc: 0.9333 - val_loss: 0.7589 - val_acc: 0.4500\n",
            "Epoch 50/500\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.68425\n",
            "60/60 - 0s - loss: 0.3517 - acc: 0.8333 - val_loss: 0.7817 - val_acc: 0.5000\n",
            "Epoch 51/500\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.68425\n",
            "60/60 - 0s - loss: 0.2592 - acc: 0.9500 - val_loss: 0.8053 - val_acc: 0.5000\n",
            "Epoch 52/500\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.68425\n",
            "60/60 - 0s - loss: 0.3001 - acc: 0.9000 - val_loss: 0.8470 - val_acc: 0.5000\n",
            "Epoch 53/500\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.68425\n",
            "60/60 - 0s - loss: 0.3103 - acc: 0.9500 - val_loss: 0.8514 - val_acc: 0.5000\n",
            "Epoch 54/500\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.68425\n",
            "60/60 - 0s - loss: 0.3062 - acc: 0.9167 - val_loss: 0.8332 - val_acc: 0.5000\n",
            "Epoch 55/500\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.68425\n",
            "60/60 - 0s - loss: 0.3379 - acc: 0.8833 - val_loss: 0.7844 - val_acc: 0.5000\n",
            "Epoch 56/500\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.68425\n",
            "60/60 - 0s - loss: 0.3345 - acc: 0.9000 - val_loss: 0.7819 - val_acc: 0.5000\n",
            "Epoch 57/500\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.68425\n",
            "60/60 - 0s - loss: 0.2579 - acc: 0.9667 - val_loss: 0.7986 - val_acc: 0.5000\n",
            "Epoch 58/500\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.68425\n",
            "60/60 - 0s - loss: 0.3177 - acc: 0.9333 - val_loss: 0.7943 - val_acc: 0.5000\n",
            "Epoch 59/500\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.68425\n",
            "60/60 - 0s - loss: 0.3014 - acc: 0.9500 - val_loss: 0.7731 - val_acc: 0.5500\n",
            "Epoch 60/500\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.68425\n",
            "60/60 - 0s - loss: 0.2628 - acc: 0.9333 - val_loss: 0.7099 - val_acc: 0.6000\n",
            "Epoch 61/500\n",
            "\n",
            "Epoch 00061: val_loss improved from 0.68425 to 0.65395, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2440 - acc: 0.9833 - val_loss: 0.6540 - val_acc: 0.6000\n",
            "Epoch 62/500\n",
            "\n",
            "Epoch 00062: val_loss improved from 0.65395 to 0.63242, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2516 - acc: 0.9333 - val_loss: 0.6324 - val_acc: 0.6000\n",
            "Epoch 63/500\n",
            "\n",
            "Epoch 00063: val_loss improved from 0.63242 to 0.62758, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2340 - acc: 0.9500 - val_loss: 0.6276 - val_acc: 0.6000\n",
            "Epoch 64/500\n",
            "\n",
            "Epoch 00064: val_loss improved from 0.62758 to 0.62008, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2410 - acc: 0.9667 - val_loss: 0.6201 - val_acc: 0.6000\n",
            "Epoch 65/500\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.62008\n",
            "60/60 - 0s - loss: 0.2550 - acc: 0.9500 - val_loss: 0.6385 - val_acc: 0.6000\n",
            "Epoch 66/500\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.62008\n",
            "60/60 - 0s - loss: 0.2358 - acc: 0.9167 - val_loss: 0.6683 - val_acc: 0.6000\n",
            "Epoch 67/500\n",
            "\n",
            "Epoch 00067: val_loss improved from 0.62008 to 0.61725, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2242 - acc: 0.9333 - val_loss: 0.6172 - val_acc: 0.6000\n",
            "Epoch 68/500\n",
            "\n",
            "Epoch 00068: val_loss improved from 0.61725 to 0.56990, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2561 - acc: 0.9167 - val_loss: 0.5699 - val_acc: 0.6000\n",
            "Epoch 69/500\n",
            "\n",
            "Epoch 00069: val_loss improved from 0.56990 to 0.54337, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2377 - acc: 0.9000 - val_loss: 0.5434 - val_acc: 0.6000\n",
            "Epoch 70/500\n",
            "\n",
            "Epoch 00070: val_loss improved from 0.54337 to 0.50056, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2484 - acc: 0.9167 - val_loss: 0.5006 - val_acc: 0.6500\n",
            "Epoch 71/500\n",
            "\n",
            "Epoch 00071: val_loss improved from 0.50056 to 0.47348, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2378 - acc: 0.9667 - val_loss: 0.4735 - val_acc: 0.6500\n",
            "Epoch 72/500\n",
            "\n",
            "Epoch 00072: val_loss improved from 0.47348 to 0.44116, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2099 - acc: 0.9667 - val_loss: 0.4412 - val_acc: 0.6500\n",
            "Epoch 73/500\n",
            "\n",
            "Epoch 00073: val_loss improved from 0.44116 to 0.41859, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2184 - acc: 0.9667 - val_loss: 0.4186 - val_acc: 0.7000\n",
            "Epoch 74/500\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.41859\n",
            "60/60 - 0s - loss: 0.1952 - acc: 0.9667 - val_loss: 0.4301 - val_acc: 0.6500\n",
            "Epoch 75/500\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.41859\n",
            "60/60 - 0s - loss: 0.2623 - acc: 0.9167 - val_loss: 0.4526 - val_acc: 0.6500\n",
            "Epoch 76/500\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.41859\n",
            "60/60 - 0s - loss: 0.2473 - acc: 0.9333 - val_loss: 0.4599 - val_acc: 0.6500\n",
            "Epoch 77/500\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.41859\n",
            "60/60 - 0s - loss: 0.2168 - acc: 0.9500 - val_loss: 0.4765 - val_acc: 0.6500\n",
            "Epoch 78/500\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.41859\n",
            "60/60 - 0s - loss: 0.1968 - acc: 0.9833 - val_loss: 0.4885 - val_acc: 0.6500\n",
            "Epoch 79/500\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.41859\n",
            "60/60 - 0s - loss: 0.1905 - acc: 0.9667 - val_loss: 0.4452 - val_acc: 0.6500\n",
            "Epoch 80/500\n",
            "\n",
            "Epoch 00080: val_loss improved from 0.41859 to 0.39500, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2060 - acc: 0.9667 - val_loss: 0.3950 - val_acc: 0.7500\n",
            "Epoch 81/500\n",
            "\n",
            "Epoch 00081: val_loss improved from 0.39500 to 0.35683, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1923 - acc: 0.9500 - val_loss: 0.3568 - val_acc: 0.8000\n",
            "Epoch 82/500\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.35683\n",
            "60/60 - 0s - loss: 0.2398 - acc: 0.8833 - val_loss: 0.3608 - val_acc: 0.8000\n",
            "Epoch 83/500\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.35683\n",
            "60/60 - 0s - loss: 0.2184 - acc: 0.9500 - val_loss: 0.3661 - val_acc: 0.8000\n",
            "Epoch 84/500\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.35683\n",
            "60/60 - 0s - loss: 0.2086 - acc: 0.9333 - val_loss: 0.3623 - val_acc: 0.8000\n",
            "Epoch 85/500\n",
            "\n",
            "Epoch 00085: val_loss improved from 0.35683 to 0.34532, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2118 - acc: 0.9500 - val_loss: 0.3453 - val_acc: 0.8500\n",
            "Epoch 86/500\n",
            "\n",
            "Epoch 00086: val_loss improved from 0.34532 to 0.33291, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1815 - acc: 1.0000 - val_loss: 0.3329 - val_acc: 0.8500\n",
            "Epoch 87/500\n",
            "\n",
            "Epoch 00087: val_loss improved from 0.33291 to 0.30939, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1901 - acc: 0.9833 - val_loss: 0.3094 - val_acc: 0.8500\n",
            "Epoch 88/500\n",
            "\n",
            "Epoch 00088: val_loss improved from 0.30939 to 0.29257, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2013 - acc: 0.9500 - val_loss: 0.2926 - val_acc: 0.8500\n",
            "Epoch 89/500\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.29257\n",
            "60/60 - 0s - loss: 0.2091 - acc: 0.9333 - val_loss: 0.3124 - val_acc: 0.8500\n",
            "Epoch 90/500\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.29257\n",
            "60/60 - 0s - loss: 0.2067 - acc: 0.9333 - val_loss: 0.3334 - val_acc: 0.8000\n",
            "Epoch 91/500\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.29257\n",
            "60/60 - 0s - loss: 0.1741 - acc: 0.9333 - val_loss: 0.3375 - val_acc: 0.8000\n",
            "Epoch 92/500\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.29257\n",
            "60/60 - 0s - loss: 0.2344 - acc: 0.9333 - val_loss: 0.3278 - val_acc: 0.8000\n",
            "Epoch 93/500\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.29257\n",
            "60/60 - 0s - loss: 0.1869 - acc: 0.9500 - val_loss: 0.3276 - val_acc: 0.8000\n",
            "Epoch 94/500\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.29257\n",
            "60/60 - 0s - loss: 0.1843 - acc: 0.9667 - val_loss: 0.3386 - val_acc: 0.8000\n",
            "Epoch 95/500\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.29257\n",
            "60/60 - 0s - loss: 0.1985 - acc: 0.9500 - val_loss: 0.3226 - val_acc: 0.8000\n",
            "Epoch 96/500\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.29257\n",
            "60/60 - 0s - loss: 0.1742 - acc: 0.9500 - val_loss: 0.3495 - val_acc: 0.8000\n",
            "Epoch 97/500\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.29257\n",
            "60/60 - 0s - loss: 0.1738 - acc: 0.9667 - val_loss: 0.3361 - val_acc: 0.8000\n",
            "Epoch 98/500\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.29257\n",
            "60/60 - 0s - loss: 0.1853 - acc: 0.9667 - val_loss: 0.3380 - val_acc: 0.8000\n",
            "Epoch 99/500\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.29257\n",
            "60/60 - 0s - loss: 0.1587 - acc: 0.9833 - val_loss: 0.3082 - val_acc: 0.8000\n",
            "Epoch 100/500\n",
            "\n",
            "Epoch 00100: val_loss improved from 0.29257 to 0.26096, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1702 - acc: 0.9500 - val_loss: 0.2610 - val_acc: 0.8500\n",
            "Epoch 101/500\n",
            "\n",
            "Epoch 00101: val_loss improved from 0.26096 to 0.23666, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1797 - acc: 0.9667 - val_loss: 0.2367 - val_acc: 0.9500\n",
            "Epoch 102/500\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.23666\n",
            "60/60 - 0s - loss: 0.1472 - acc: 0.9833 - val_loss: 0.2486 - val_acc: 0.9500\n",
            "Epoch 103/500\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.23666\n",
            "60/60 - 0s - loss: 0.1774 - acc: 0.9500 - val_loss: 0.3015 - val_acc: 0.8000\n",
            "Epoch 104/500\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.23666\n",
            "60/60 - 0s - loss: 0.1856 - acc: 0.9000 - val_loss: 0.3218 - val_acc: 0.8000\n",
            "Epoch 105/500\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 0.23666\n",
            "60/60 - 0s - loss: 0.1792 - acc: 0.9500 - val_loss: 0.3240 - val_acc: 0.8000\n",
            "Epoch 106/500\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 0.23666\n",
            "60/60 - 0s - loss: 0.1616 - acc: 0.9667 - val_loss: 0.2862 - val_acc: 0.8500\n",
            "Epoch 107/500\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.23666\n",
            "60/60 - 0s - loss: 0.1848 - acc: 0.9667 - val_loss: 0.2539 - val_acc: 0.9000\n",
            "Epoch 108/500\n",
            "\n",
            "Epoch 00108: val_loss improved from 0.23666 to 0.22554, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2320 - acc: 0.9167 - val_loss: 0.2255 - val_acc: 0.9500\n",
            "Epoch 109/500\n",
            "\n",
            "Epoch 00109: val_loss improved from 0.22554 to 0.20393, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1731 - acc: 0.9833 - val_loss: 0.2039 - val_acc: 0.9500\n",
            "Epoch 110/500\n",
            "\n",
            "Epoch 00110: val_loss improved from 0.20393 to 0.19959, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2089 - acc: 0.9333 - val_loss: 0.1996 - val_acc: 0.9500\n",
            "Epoch 111/500\n",
            "\n",
            "Epoch 00111: val_loss improved from 0.19959 to 0.19709, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1271 - acc: 1.0000 - val_loss: 0.1971 - val_acc: 0.9500\n",
            "Epoch 112/500\n",
            "\n",
            "Epoch 00112: val_loss improved from 0.19709 to 0.19012, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2011 - acc: 0.9333 - val_loss: 0.1901 - val_acc: 0.9500\n",
            "Epoch 113/500\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.19012\n",
            "60/60 - 0s - loss: 0.1520 - acc: 0.9833 - val_loss: 0.1924 - val_acc: 1.0000\n",
            "Epoch 114/500\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.19012\n",
            "60/60 - 0s - loss: 0.2002 - acc: 0.9833 - val_loss: 0.2001 - val_acc: 1.0000\n",
            "Epoch 115/500\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.19012\n",
            "60/60 - 0s - loss: 0.1322 - acc: 0.9833 - val_loss: 0.2017 - val_acc: 1.0000\n",
            "Epoch 116/500\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.19012\n",
            "60/60 - 0s - loss: 0.1901 - acc: 0.9667 - val_loss: 0.2027 - val_acc: 1.0000\n",
            "Epoch 117/500\n",
            "\n",
            "Epoch 00117: val_loss improved from 0.19012 to 0.18896, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1634 - acc: 0.9667 - val_loss: 0.1890 - val_acc: 1.0000\n",
            "Epoch 118/500\n",
            "\n",
            "Epoch 00118: val_loss improved from 0.18896 to 0.17704, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1849 - acc: 0.9667 - val_loss: 0.1770 - val_acc: 1.0000\n",
            "Epoch 119/500\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 0.17704\n",
            "60/60 - 0s - loss: 0.1254 - acc: 0.9667 - val_loss: 0.1791 - val_acc: 1.0000\n",
            "Epoch 120/500\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.17704\n",
            "60/60 - 0s - loss: 0.1789 - acc: 0.9667 - val_loss: 0.1839 - val_acc: 1.0000\n",
            "Epoch 121/500\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.17704\n",
            "60/60 - 0s - loss: 0.1752 - acc: 0.9500 - val_loss: 0.1852 - val_acc: 1.0000\n",
            "Epoch 122/500\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.17704\n",
            "60/60 - 0s - loss: 0.1294 - acc: 0.9833 - val_loss: 0.1842 - val_acc: 1.0000\n",
            "Epoch 123/500\n",
            "\n",
            "Epoch 00123: val_loss improved from 0.17704 to 0.17206, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1445 - acc: 0.9667 - val_loss: 0.1721 - val_acc: 1.0000\n",
            "Epoch 124/500\n",
            "\n",
            "Epoch 00124: val_loss improved from 0.17206 to 0.16928, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1564 - acc: 0.9667 - val_loss: 0.1693 - val_acc: 1.0000\n",
            "Epoch 125/500\n",
            "\n",
            "Epoch 00125: val_loss improved from 0.16928 to 0.16425, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1258 - acc: 0.9833 - val_loss: 0.1642 - val_acc: 1.0000\n",
            "Epoch 126/500\n",
            "\n",
            "Epoch 00126: val_loss improved from 0.16425 to 0.16339, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2042 - acc: 0.9167 - val_loss: 0.1634 - val_acc: 1.0000\n",
            "Epoch 127/500\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 0.16339\n",
            "60/60 - 0s - loss: 0.1418 - acc: 0.9667 - val_loss: 0.1651 - val_acc: 1.0000\n",
            "Epoch 128/500\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 0.16339\n",
            "60/60 - 0s - loss: 0.1684 - acc: 0.9500 - val_loss: 0.1673 - val_acc: 1.0000\n",
            "Epoch 129/500\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.16339\n",
            "60/60 - 0s - loss: 0.1490 - acc: 0.9833 - val_loss: 0.1659 - val_acc: 1.0000\n",
            "Epoch 130/500\n",
            "\n",
            "Epoch 00130: val_loss improved from 0.16339 to 0.15963, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1420 - acc: 1.0000 - val_loss: 0.1596 - val_acc: 1.0000\n",
            "Epoch 131/500\n",
            "\n",
            "Epoch 00131: val_loss improved from 0.15963 to 0.15674, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1475 - acc: 0.9833 - val_loss: 0.1567 - val_acc: 1.0000\n",
            "Epoch 132/500\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 0.15674\n",
            "60/60 - 0s - loss: 0.1742 - acc: 0.9667 - val_loss: 0.1576 - val_acc: 1.0000\n",
            "Epoch 133/500\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.15674\n",
            "60/60 - 0s - loss: 0.1425 - acc: 0.9667 - val_loss: 0.1598 - val_acc: 0.9500\n",
            "Epoch 134/500\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.15674\n",
            "60/60 - 0s - loss: 0.2015 - acc: 0.9000 - val_loss: 0.1684 - val_acc: 0.9500\n",
            "Epoch 135/500\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 0.15674\n",
            "60/60 - 0s - loss: 0.1358 - acc: 0.9833 - val_loss: 0.1727 - val_acc: 0.9500\n",
            "Epoch 136/500\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.15674\n",
            "60/60 - 0s - loss: 0.1529 - acc: 0.9500 - val_loss: 0.1701 - val_acc: 0.9500\n",
            "Epoch 137/500\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 0.15674\n",
            "60/60 - 0s - loss: 0.1852 - acc: 0.9500 - val_loss: 0.1574 - val_acc: 0.9500\n",
            "Epoch 138/500\n",
            "\n",
            "Epoch 00138: val_loss improved from 0.15674 to 0.14953, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1525 - acc: 0.9833 - val_loss: 0.1495 - val_acc: 1.0000\n",
            "Epoch 139/500\n",
            "\n",
            "Epoch 00139: val_loss improved from 0.14953 to 0.14263, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1225 - acc: 0.9833 - val_loss: 0.1426 - val_acc: 1.0000\n",
            "Epoch 140/500\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 0.14263\n",
            "60/60 - 0s - loss: 0.1695 - acc: 0.9333 - val_loss: 0.1429 - val_acc: 1.0000\n",
            "Epoch 141/500\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.14263\n",
            "60/60 - 0s - loss: 0.1349 - acc: 0.9667 - val_loss: 0.1470 - val_acc: 1.0000\n",
            "Epoch 142/500\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.14263\n",
            "60/60 - 0s - loss: 0.1533 - acc: 0.9667 - val_loss: 0.1545 - val_acc: 1.0000\n",
            "Epoch 143/500\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 0.14263\n",
            "60/60 - 0s - loss: 0.1211 - acc: 0.9833 - val_loss: 0.1695 - val_acc: 1.0000\n",
            "Epoch 144/500\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 0.14263\n",
            "60/60 - 0s - loss: 0.1411 - acc: 0.9667 - val_loss: 0.1807 - val_acc: 1.0000\n",
            "Epoch 145/500\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.14263\n",
            "60/60 - 0s - loss: 0.1613 - acc: 0.9667 - val_loss: 0.1697 - val_acc: 1.0000\n",
            "Epoch 146/500\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.14263\n",
            "60/60 - 0s - loss: 0.1612 - acc: 0.9833 - val_loss: 0.1575 - val_acc: 1.0000\n",
            "Epoch 147/500\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.14263\n",
            "60/60 - 0s - loss: 0.1429 - acc: 0.9833 - val_loss: 0.1527 - val_acc: 1.0000\n",
            "Epoch 148/500\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 0.14263\n",
            "60/60 - 0s - loss: 0.1287 - acc: 0.9667 - val_loss: 0.1481 - val_acc: 1.0000\n",
            "Epoch 149/500\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.14263\n",
            "60/60 - 0s - loss: 0.1020 - acc: 1.0000 - val_loss: 0.1476 - val_acc: 1.0000\n",
            "Epoch 150/500\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.14263\n",
            "60/60 - 0s - loss: 0.1359 - acc: 0.9833 - val_loss: 0.1555 - val_acc: 1.0000\n",
            "Epoch 151/500\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.14263\n",
            "60/60 - 0s - loss: 0.1236 - acc: 0.9667 - val_loss: 0.1580 - val_acc: 1.0000\n",
            "Epoch 152/500\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.14263\n",
            "60/60 - 0s - loss: 0.1008 - acc: 1.0000 - val_loss: 0.1547 - val_acc: 0.9500\n",
            "Epoch 153/500\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 0.14263\n",
            "60/60 - 0s - loss: 0.1458 - acc: 0.9500 - val_loss: 0.1441 - val_acc: 1.0000\n",
            "Epoch 154/500\n",
            "\n",
            "Epoch 00154: val_loss improved from 0.14263 to 0.13877, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1580 - acc: 0.9667 - val_loss: 0.1388 - val_acc: 1.0000\n",
            "Epoch 155/500\n",
            "\n",
            "Epoch 00155: val_loss improved from 0.13877 to 0.13188, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1781 - acc: 0.9667 - val_loss: 0.1319 - val_acc: 1.0000\n",
            "Epoch 156/500\n",
            "\n",
            "Epoch 00156: val_loss improved from 0.13188 to 0.12339, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1528 - acc: 0.9833 - val_loss: 0.1234 - val_acc: 1.0000\n",
            "Epoch 157/500\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.1331 - acc: 0.9667 - val_loss: 0.1267 - val_acc: 1.0000\n",
            "Epoch 158/500\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.1484 - acc: 0.9500 - val_loss: 0.1336 - val_acc: 1.0000\n",
            "Epoch 159/500\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.1373 - acc: 0.9667 - val_loss: 0.1403 - val_acc: 1.0000\n",
            "Epoch 160/500\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.1633 - acc: 0.9667 - val_loss: 0.1502 - val_acc: 1.0000\n",
            "Epoch 161/500\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.1275 - acc: 1.0000 - val_loss: 0.1583 - val_acc: 1.0000\n",
            "Epoch 162/500\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.1485 - acc: 0.9833 - val_loss: 0.1624 - val_acc: 1.0000\n",
            "Epoch 163/500\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.1081 - acc: 0.9833 - val_loss: 0.1732 - val_acc: 1.0000\n",
            "Epoch 164/500\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.1213 - acc: 0.9667 - val_loss: 0.1707 - val_acc: 1.0000\n",
            "Epoch 165/500\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.1255 - acc: 0.9667 - val_loss: 0.1704 - val_acc: 1.0000\n",
            "Epoch 166/500\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.1136 - acc: 1.0000 - val_loss: 0.1625 - val_acc: 1.0000\n",
            "Epoch 167/500\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.1488 - acc: 0.9833 - val_loss: 0.1608 - val_acc: 1.0000\n",
            "Epoch 168/500\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.1614 - acc: 0.9667 - val_loss: 0.1611 - val_acc: 1.0000\n",
            "Epoch 169/500\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.1311 - acc: 1.0000 - val_loss: 0.1701 - val_acc: 0.9500\n",
            "Epoch 170/500\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.1992 - acc: 0.9333 - val_loss: 0.1814 - val_acc: 1.0000\n",
            "Epoch 171/500\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.1100 - acc: 1.0000 - val_loss: 0.1887 - val_acc: 1.0000\n",
            "Epoch 172/500\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.1096 - acc: 1.0000 - val_loss: 0.1930 - val_acc: 1.0000\n",
            "Epoch 173/500\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.1628 - acc: 0.9833 - val_loss: 0.2016 - val_acc: 1.0000\n",
            "Epoch 174/500\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.1153 - acc: 0.9833 - val_loss: 0.2051 - val_acc: 0.9500\n",
            "Epoch 175/500\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.1335 - acc: 0.9667 - val_loss: 0.1979 - val_acc: 0.9500\n",
            "Epoch 176/500\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.1221 - acc: 0.9667 - val_loss: 0.1744 - val_acc: 1.0000\n",
            "Epoch 177/500\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.1233 - acc: 0.9833 - val_loss: 0.1612 - val_acc: 1.0000\n",
            "Epoch 178/500\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.1247 - acc: 0.9833 - val_loss: 0.1565 - val_acc: 1.0000\n",
            "Epoch 179/500\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.1122 - acc: 0.9667 - val_loss: 0.1675 - val_acc: 1.0000\n",
            "Epoch 180/500\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0849 - acc: 0.9833 - val_loss: 0.1844 - val_acc: 0.9500\n",
            "Epoch 181/500\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.1278 - acc: 0.9833 - val_loss: 0.1953 - val_acc: 0.9500\n",
            "Epoch 182/500\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.1789 - acc: 0.9167 - val_loss: 0.1971 - val_acc: 0.9500\n",
            "Epoch 183/500\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.1146 - acc: 0.9833 - val_loss: 0.1845 - val_acc: 0.9500\n",
            "Epoch 184/500\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.1064 - acc: 1.0000 - val_loss: 0.1639 - val_acc: 1.0000\n",
            "Epoch 185/500\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.1136 - acc: 1.0000 - val_loss: 0.1460 - val_acc: 1.0000\n",
            "Epoch 186/500\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.1098 - acc: 0.9833 - val_loss: 0.1367 - val_acc: 1.0000\n",
            "Epoch 187/500\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.1013 - acc: 1.0000 - val_loss: 0.1277 - val_acc: 1.0000\n",
            "Epoch 188/500\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0868 - acc: 1.0000 - val_loss: 0.1238 - val_acc: 1.0000\n",
            "Epoch 189/500\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.1182 - acc: 0.9667 - val_loss: 0.1303 - val_acc: 1.0000\n",
            "Epoch 190/500\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.1587 - acc: 0.9167 - val_loss: 0.1605 - val_acc: 1.0000\n",
            "Epoch 191/500\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.1055 - acc: 0.9833 - val_loss: 0.1884 - val_acc: 0.9500\n",
            "Epoch 192/500\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.1329 - acc: 0.9667 - val_loss: 0.2201 - val_acc: 0.9000\n",
            "Epoch 193/500\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.1238 - acc: 0.9500 - val_loss: 0.2898 - val_acc: 0.8500\n",
            "Epoch 194/500\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0719 - acc: 1.0000 - val_loss: 0.3049 - val_acc: 0.8000\n",
            "Epoch 195/500\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.1017 - acc: 1.0000 - val_loss: 0.2803 - val_acc: 0.8500\n",
            "Epoch 196/500\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.1363 - acc: 0.9833 - val_loss: 0.2475 - val_acc: 0.9500\n",
            "Epoch 197/500\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0905 - acc: 1.0000 - val_loss: 0.2289 - val_acc: 0.9500\n",
            "Epoch 198/500\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.1035 - acc: 0.9833 - val_loss: 0.2117 - val_acc: 0.9500\n",
            "Epoch 199/500\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.1441 - acc: 0.9500 - val_loss: 0.2237 - val_acc: 0.9500\n",
            "Epoch 200/500\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0859 - acc: 1.0000 - val_loss: 0.2292 - val_acc: 0.9500\n",
            "Epoch 201/500\n",
            "\n",
            "Epoch 00201: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.1124 - acc: 1.0000 - val_loss: 0.2378 - val_acc: 0.9000\n",
            "Epoch 202/500\n",
            "\n",
            "Epoch 00202: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0873 - acc: 1.0000 - val_loss: 0.2357 - val_acc: 0.9000\n",
            "Epoch 203/500\n",
            "\n",
            "Epoch 00203: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.1319 - acc: 1.0000 - val_loss: 0.2115 - val_acc: 0.9500\n",
            "Epoch 204/500\n",
            "\n",
            "Epoch 00204: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.1302 - acc: 0.9833 - val_loss: 0.1857 - val_acc: 0.9500\n",
            "Epoch 205/500\n",
            "\n",
            "Epoch 00205: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.1122 - acc: 0.9667 - val_loss: 0.1591 - val_acc: 0.9500\n",
            "Epoch 206/500\n",
            "\n",
            "Epoch 00206: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.1342 - acc: 0.9833 - val_loss: 0.1581 - val_acc: 0.9500\n",
            "Epoch 207/500\n",
            "\n",
            "Epoch 00207: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0944 - acc: 0.9833 - val_loss: 0.1647 - val_acc: 0.9500\n",
            "Epoch 208/500\n",
            "\n",
            "Epoch 00208: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0828 - acc: 1.0000 - val_loss: 0.1886 - val_acc: 0.9500\n",
            "Epoch 209/500\n",
            "\n",
            "Epoch 00209: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0733 - acc: 1.0000 - val_loss: 0.2103 - val_acc: 0.9500\n",
            "Epoch 210/500\n",
            "\n",
            "Epoch 00210: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.1199 - acc: 0.9667 - val_loss: 0.2376 - val_acc: 0.9000\n",
            "Epoch 211/500\n",
            "\n",
            "Epoch 00211: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.1335 - acc: 0.9333 - val_loss: 0.2463 - val_acc: 0.9000\n",
            "Epoch 212/500\n",
            "\n",
            "Epoch 00212: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0830 - acc: 1.0000 - val_loss: 0.2353 - val_acc: 0.9000\n",
            "Epoch 213/500\n",
            "\n",
            "Epoch 00213: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0954 - acc: 0.9833 - val_loss: 0.2312 - val_acc: 0.9000\n",
            "Epoch 214/500\n",
            "\n",
            "Epoch 00214: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0932 - acc: 0.9833 - val_loss: 0.2179 - val_acc: 0.9500\n",
            "Epoch 215/500\n",
            "\n",
            "Epoch 00215: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.1043 - acc: 1.0000 - val_loss: 0.1913 - val_acc: 0.9500\n",
            "Epoch 216/500\n",
            "\n",
            "Epoch 00216: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0947 - acc: 0.9833 - val_loss: 0.1954 - val_acc: 0.9500\n",
            "Epoch 217/500\n",
            "\n",
            "Epoch 00217: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.1741 - acc: 0.9167 - val_loss: 0.1998 - val_acc: 0.9500\n",
            "Epoch 218/500\n",
            "\n",
            "Epoch 00218: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0973 - acc: 0.9833 - val_loss: 0.2300 - val_acc: 0.9500\n",
            "Epoch 219/500\n",
            "\n",
            "Epoch 00219: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0962 - acc: 0.9833 - val_loss: 0.2417 - val_acc: 0.9500\n",
            "Epoch 220/500\n",
            "\n",
            "Epoch 00220: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0922 - acc: 0.9667 - val_loss: 0.2204 - val_acc: 0.9500\n",
            "Epoch 221/500\n",
            "\n",
            "Epoch 00221: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.1074 - acc: 0.9833 - val_loss: 0.1839 - val_acc: 0.9500\n",
            "Epoch 222/500\n",
            "\n",
            "Epoch 00222: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0949 - acc: 0.9833 - val_loss: 0.1541 - val_acc: 1.0000\n",
            "Epoch 223/500\n",
            "\n",
            "Epoch 00223: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.1006 - acc: 0.9833 - val_loss: 0.1480 - val_acc: 1.0000\n",
            "Epoch 224/500\n",
            "\n",
            "Epoch 00224: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.1194 - acc: 0.9833 - val_loss: 0.1570 - val_acc: 0.9500\n",
            "Epoch 225/500\n",
            "\n",
            "Epoch 00225: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0715 - acc: 1.0000 - val_loss: 0.1694 - val_acc: 0.9000\n",
            "Epoch 226/500\n",
            "\n",
            "Epoch 00226: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.1175 - acc: 0.9833 - val_loss: 0.1733 - val_acc: 0.9000\n",
            "Epoch 227/500\n",
            "\n",
            "Epoch 00227: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.1209 - acc: 0.9667 - val_loss: 0.1695 - val_acc: 0.9500\n",
            "Epoch 228/500\n",
            "\n",
            "Epoch 00228: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0940 - acc: 1.0000 - val_loss: 0.1772 - val_acc: 0.9500\n",
            "Epoch 229/500\n",
            "\n",
            "Epoch 00229: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.1125 - acc: 0.9833 - val_loss: 0.1874 - val_acc: 0.9500\n",
            "Epoch 230/500\n",
            "\n",
            "Epoch 00230: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0871 - acc: 1.0000 - val_loss: 0.1841 - val_acc: 0.9500\n",
            "Epoch 231/500\n",
            "\n",
            "Epoch 00231: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0692 - acc: 1.0000 - val_loss: 0.1735 - val_acc: 0.9500\n",
            "Epoch 232/500\n",
            "\n",
            "Epoch 00232: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0733 - acc: 1.0000 - val_loss: 0.1714 - val_acc: 0.9500\n",
            "Epoch 233/500\n",
            "\n",
            "Epoch 00233: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0947 - acc: 0.9833 - val_loss: 0.1658 - val_acc: 1.0000\n",
            "Epoch 234/500\n",
            "\n",
            "Epoch 00234: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0620 - acc: 1.0000 - val_loss: 0.1585 - val_acc: 1.0000\n",
            "Epoch 235/500\n",
            "\n",
            "Epoch 00235: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.1144 - acc: 0.9500 - val_loss: 0.1560 - val_acc: 1.0000\n",
            "Epoch 236/500\n",
            "\n",
            "Epoch 00236: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.1104 - acc: 0.9667 - val_loss: 0.1623 - val_acc: 1.0000\n",
            "Epoch 237/500\n",
            "\n",
            "Epoch 00237: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0956 - acc: 0.9667 - val_loss: 0.1756 - val_acc: 0.9500\n",
            "Epoch 238/500\n",
            "\n",
            "Epoch 00238: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0899 - acc: 0.9833 - val_loss: 0.1865 - val_acc: 0.9500\n",
            "Epoch 239/500\n",
            "\n",
            "Epoch 00239: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0854 - acc: 0.9833 - val_loss: 0.1835 - val_acc: 0.9500\n",
            "Epoch 240/500\n",
            "\n",
            "Epoch 00240: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0879 - acc: 0.9833 - val_loss: 0.1767 - val_acc: 0.9500\n",
            "Epoch 241/500\n",
            "\n",
            "Epoch 00241: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.1024 - acc: 0.9500 - val_loss: 0.1743 - val_acc: 1.0000\n",
            "Epoch 242/500\n",
            "\n",
            "Epoch 00242: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0650 - acc: 1.0000 - val_loss: 0.1781 - val_acc: 0.9500\n",
            "Epoch 243/500\n",
            "\n",
            "Epoch 00243: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0735 - acc: 0.9833 - val_loss: 0.1843 - val_acc: 0.9500\n",
            "Epoch 244/500\n",
            "\n",
            "Epoch 00244: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.1695 - acc: 0.9167 - val_loss: 0.2106 - val_acc: 0.9500\n",
            "Epoch 245/500\n",
            "\n",
            "Epoch 00245: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0623 - acc: 1.0000 - val_loss: 0.2187 - val_acc: 0.9500\n",
            "Epoch 246/500\n",
            "\n",
            "Epoch 00246: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0797 - acc: 0.9833 - val_loss: 0.2269 - val_acc: 0.8500\n",
            "Epoch 247/500\n",
            "\n",
            "Epoch 00247: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0823 - acc: 1.0000 - val_loss: 0.2210 - val_acc: 0.8500\n",
            "Epoch 248/500\n",
            "\n",
            "Epoch 00248: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0815 - acc: 1.0000 - val_loss: 0.2138 - val_acc: 0.9000\n",
            "Epoch 249/500\n",
            "\n",
            "Epoch 00249: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0625 - acc: 1.0000 - val_loss: 0.2099 - val_acc: 0.9000\n",
            "Epoch 250/500\n",
            "\n",
            "Epoch 00250: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0962 - acc: 1.0000 - val_loss: 0.1850 - val_acc: 0.9500\n",
            "Epoch 251/500\n",
            "\n",
            "Epoch 00251: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.1019 - acc: 0.9833 - val_loss: 0.1655 - val_acc: 0.9500\n",
            "Epoch 252/500\n",
            "\n",
            "Epoch 00252: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0811 - acc: 0.9833 - val_loss: 0.1709 - val_acc: 0.9500\n",
            "Epoch 253/500\n",
            "\n",
            "Epoch 00253: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.1052 - acc: 0.9833 - val_loss: 0.1798 - val_acc: 0.9500\n",
            "Epoch 254/500\n",
            "\n",
            "Epoch 00254: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0725 - acc: 1.0000 - val_loss: 0.1901 - val_acc: 0.9500\n",
            "Epoch 255/500\n",
            "\n",
            "Epoch 00255: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.1030 - acc: 0.9833 - val_loss: 0.1914 - val_acc: 0.9500\n",
            "Epoch 256/500\n",
            "\n",
            "Epoch 00256: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.1549 - acc: 0.9833 - val_loss: 0.2068 - val_acc: 0.9500\n",
            "Epoch 257/500\n",
            "\n",
            "Epoch 00257: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0783 - acc: 1.0000 - val_loss: 0.2210 - val_acc: 0.9500\n",
            "Epoch 258/500\n",
            "\n",
            "Epoch 00258: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.1265 - acc: 0.9667 - val_loss: 0.2158 - val_acc: 0.9500\n",
            "Epoch 259/500\n",
            "\n",
            "Epoch 00259: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0880 - acc: 1.0000 - val_loss: 0.2097 - val_acc: 0.9500\n",
            "Epoch 260/500\n",
            "\n",
            "Epoch 00260: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0845 - acc: 0.9833 - val_loss: 0.2145 - val_acc: 0.9500\n",
            "Epoch 261/500\n",
            "\n",
            "Epoch 00261: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.1154 - acc: 0.9833 - val_loss: 0.2208 - val_acc: 0.9500\n",
            "Epoch 262/500\n",
            "\n",
            "Epoch 00262: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0800 - acc: 1.0000 - val_loss: 0.2058 - val_acc: 0.9500\n",
            "Epoch 263/500\n",
            "\n",
            "Epoch 00263: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0594 - acc: 1.0000 - val_loss: 0.1914 - val_acc: 1.0000\n",
            "Epoch 264/500\n",
            "\n",
            "Epoch 00264: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.1052 - acc: 0.9833 - val_loss: 0.1815 - val_acc: 1.0000\n",
            "Epoch 265/500\n",
            "\n",
            "Epoch 00265: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.1104 - acc: 0.9667 - val_loss: 0.1901 - val_acc: 1.0000\n",
            "Epoch 266/500\n",
            "\n",
            "Epoch 00266: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0839 - acc: 0.9833 - val_loss: 0.2011 - val_acc: 0.9500\n",
            "Epoch 267/500\n",
            "\n",
            "Epoch 00267: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.1065 - acc: 0.9667 - val_loss: 0.2029 - val_acc: 0.9500\n",
            "Epoch 268/500\n",
            "\n",
            "Epoch 00268: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0864 - acc: 0.9833 - val_loss: 0.2084 - val_acc: 0.9500\n",
            "Epoch 269/500\n",
            "\n",
            "Epoch 00269: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0462 - acc: 1.0000 - val_loss: 0.2052 - val_acc: 0.9500\n",
            "Epoch 270/500\n",
            "\n",
            "Epoch 00270: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0716 - acc: 1.0000 - val_loss: 0.2000 - val_acc: 0.9500\n",
            "Epoch 271/500\n",
            "\n",
            "Epoch 00271: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0765 - acc: 1.0000 - val_loss: 0.1841 - val_acc: 0.9500\n",
            "Epoch 272/500\n",
            "\n",
            "Epoch 00272: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.1088 - acc: 0.9667 - val_loss: 0.1715 - val_acc: 1.0000\n",
            "Epoch 273/500\n",
            "\n",
            "Epoch 00273: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.1118 - acc: 0.9667 - val_loss: 0.1711 - val_acc: 0.9000\n",
            "Epoch 274/500\n",
            "\n",
            "Epoch 00274: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0743 - acc: 1.0000 - val_loss: 0.1819 - val_acc: 0.9000\n",
            "Epoch 275/500\n",
            "\n",
            "Epoch 00275: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0555 - acc: 1.0000 - val_loss: 0.1884 - val_acc: 0.9000\n",
            "Epoch 276/500\n",
            "\n",
            "Epoch 00276: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0708 - acc: 1.0000 - val_loss: 0.1969 - val_acc: 0.8500\n",
            "Epoch 277/500\n",
            "\n",
            "Epoch 00277: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0515 - acc: 1.0000 - val_loss: 0.1976 - val_acc: 0.8500\n",
            "Epoch 278/500\n",
            "\n",
            "Epoch 00278: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0638 - acc: 1.0000 - val_loss: 0.1934 - val_acc: 0.9500\n",
            "Epoch 279/500\n",
            "\n",
            "Epoch 00279: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0820 - acc: 1.0000 - val_loss: 0.1799 - val_acc: 0.9500\n",
            "Epoch 280/500\n",
            "\n",
            "Epoch 00280: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0571 - acc: 1.0000 - val_loss: 0.1729 - val_acc: 0.9500\n",
            "Epoch 281/500\n",
            "\n",
            "Epoch 00281: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.1159 - acc: 0.9500 - val_loss: 0.1613 - val_acc: 0.9500\n",
            "Epoch 282/500\n",
            "\n",
            "Epoch 00282: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0594 - acc: 0.9833 - val_loss: 0.1594 - val_acc: 0.9500\n",
            "Epoch 283/500\n",
            "\n",
            "Epoch 00283: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0784 - acc: 0.9833 - val_loss: 0.1696 - val_acc: 1.0000\n",
            "Epoch 284/500\n",
            "\n",
            "Epoch 00284: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0518 - acc: 1.0000 - val_loss: 0.1838 - val_acc: 0.9500\n",
            "Epoch 285/500\n",
            "\n",
            "Epoch 00285: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0572 - acc: 1.0000 - val_loss: 0.1869 - val_acc: 0.9000\n",
            "Epoch 286/500\n",
            "\n",
            "Epoch 00286: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0599 - acc: 1.0000 - val_loss: 0.1928 - val_acc: 0.9000\n",
            "Epoch 287/500\n",
            "\n",
            "Epoch 00287: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0577 - acc: 1.0000 - val_loss: 0.1873 - val_acc: 0.9000\n",
            "Epoch 288/500\n",
            "\n",
            "Epoch 00288: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0629 - acc: 1.0000 - val_loss: 0.1848 - val_acc: 0.9500\n",
            "Epoch 289/500\n",
            "\n",
            "Epoch 00289: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0691 - acc: 0.9833 - val_loss: 0.1790 - val_acc: 0.9500\n",
            "Epoch 290/500\n",
            "\n",
            "Epoch 00290: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0468 - acc: 1.0000 - val_loss: 0.1839 - val_acc: 0.9500\n",
            "Epoch 291/500\n",
            "\n",
            "Epoch 00291: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0692 - acc: 0.9833 - val_loss: 0.1852 - val_acc: 0.9500\n",
            "Epoch 292/500\n",
            "\n",
            "Epoch 00292: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0621 - acc: 0.9833 - val_loss: 0.1909 - val_acc: 0.9500\n",
            "Epoch 293/500\n",
            "\n",
            "Epoch 00293: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0545 - acc: 1.0000 - val_loss: 0.2149 - val_acc: 0.9000\n",
            "Epoch 294/500\n",
            "\n",
            "Epoch 00294: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0485 - acc: 1.0000 - val_loss: 0.2212 - val_acc: 0.9000\n",
            "Epoch 295/500\n",
            "\n",
            "Epoch 00295: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0486 - acc: 1.0000 - val_loss: 0.1977 - val_acc: 0.9000\n",
            "Epoch 296/500\n",
            "\n",
            "Epoch 00296: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0490 - acc: 1.0000 - val_loss: 0.1844 - val_acc: 0.9000\n",
            "Epoch 297/500\n",
            "\n",
            "Epoch 00297: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0815 - acc: 1.0000 - val_loss: 0.1693 - val_acc: 0.9500\n",
            "Epoch 298/500\n",
            "\n",
            "Epoch 00298: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0748 - acc: 0.9667 - val_loss: 0.1593 - val_acc: 0.9500\n",
            "Epoch 299/500\n",
            "\n",
            "Epoch 00299: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0369 - acc: 1.0000 - val_loss: 0.1516 - val_acc: 1.0000\n",
            "Epoch 300/500\n",
            "\n",
            "Epoch 00300: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0610 - acc: 0.9833 - val_loss: 0.1489 - val_acc: 1.0000\n",
            "Epoch 301/500\n",
            "\n",
            "Epoch 00301: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0552 - acc: 0.9833 - val_loss: 0.1405 - val_acc: 1.0000\n",
            "Epoch 302/500\n",
            "\n",
            "Epoch 00302: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0953 - acc: 0.9833 - val_loss: 0.1382 - val_acc: 0.9500\n",
            "Epoch 303/500\n",
            "\n",
            "Epoch 00303: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0597 - acc: 1.0000 - val_loss: 0.1454 - val_acc: 0.9500\n",
            "Epoch 304/500\n",
            "\n",
            "Epoch 00304: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0411 - acc: 1.0000 - val_loss: 0.1521 - val_acc: 0.9500\n",
            "Epoch 305/500\n",
            "\n",
            "Epoch 00305: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0576 - acc: 0.9833 - val_loss: 0.1601 - val_acc: 0.9500\n",
            "Epoch 306/500\n",
            "\n",
            "Epoch 00306: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0351 - acc: 1.0000 - val_loss: 0.1643 - val_acc: 0.9500\n",
            "Epoch 307/500\n",
            "\n",
            "Epoch 00307: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0656 - acc: 0.9833 - val_loss: 0.1643 - val_acc: 0.9500\n",
            "Epoch 308/500\n",
            "\n",
            "Epoch 00308: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0740 - acc: 0.9833 - val_loss: 0.1582 - val_acc: 0.9500\n",
            "Epoch 309/500\n",
            "\n",
            "Epoch 00309: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0702 - acc: 1.0000 - val_loss: 0.1566 - val_acc: 0.9500\n",
            "Epoch 310/500\n",
            "\n",
            "Epoch 00310: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0528 - acc: 1.0000 - val_loss: 0.1586 - val_acc: 0.9500\n",
            "Epoch 311/500\n",
            "\n",
            "Epoch 00311: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0503 - acc: 1.0000 - val_loss: 0.1597 - val_acc: 0.9500\n",
            "Epoch 312/500\n",
            "\n",
            "Epoch 00312: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0679 - acc: 0.9833 - val_loss: 0.1610 - val_acc: 0.9500\n",
            "Epoch 313/500\n",
            "\n",
            "Epoch 00313: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0655 - acc: 0.9833 - val_loss: 0.1627 - val_acc: 0.9500\n",
            "Epoch 314/500\n",
            "\n",
            "Epoch 00314: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0611 - acc: 1.0000 - val_loss: 0.1684 - val_acc: 0.9000\n",
            "Epoch 315/500\n",
            "\n",
            "Epoch 00315: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0438 - acc: 1.0000 - val_loss: 0.1757 - val_acc: 0.9000\n",
            "Epoch 316/500\n",
            "\n",
            "Epoch 00316: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0579 - acc: 0.9833 - val_loss: 0.1852 - val_acc: 0.9000\n",
            "Epoch 317/500\n",
            "\n",
            "Epoch 00317: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0539 - acc: 1.0000 - val_loss: 0.1903 - val_acc: 0.9000\n",
            "Epoch 318/500\n",
            "\n",
            "Epoch 00318: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0688 - acc: 0.9833 - val_loss: 0.1938 - val_acc: 0.9000\n",
            "Epoch 319/500\n",
            "\n",
            "Epoch 00319: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0849 - acc: 0.9667 - val_loss: 0.1959 - val_acc: 0.9000\n",
            "Epoch 320/500\n",
            "\n",
            "Epoch 00320: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0416 - acc: 0.9833 - val_loss: 0.2027 - val_acc: 0.9000\n",
            "Epoch 321/500\n",
            "\n",
            "Epoch 00321: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0923 - acc: 0.9833 - val_loss: 0.2184 - val_acc: 0.9000\n",
            "Epoch 322/500\n",
            "\n",
            "Epoch 00322: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0553 - acc: 1.0000 - val_loss: 0.2322 - val_acc: 0.9000\n",
            "Epoch 323/500\n",
            "\n",
            "Epoch 00323: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0571 - acc: 1.0000 - val_loss: 0.2372 - val_acc: 0.9000\n",
            "Epoch 324/500\n",
            "\n",
            "Epoch 00324: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0511 - acc: 1.0000 - val_loss: 0.2374 - val_acc: 0.9000\n",
            "Epoch 325/500\n",
            "\n",
            "Epoch 00325: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0459 - acc: 1.0000 - val_loss: 0.2261 - val_acc: 0.9000\n",
            "Epoch 326/500\n",
            "\n",
            "Epoch 00326: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0493 - acc: 1.0000 - val_loss: 0.2170 - val_acc: 0.8500\n",
            "Epoch 327/500\n",
            "\n",
            "Epoch 00327: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0777 - acc: 0.9667 - val_loss: 0.2122 - val_acc: 0.8500\n",
            "Epoch 328/500\n",
            "\n",
            "Epoch 00328: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0467 - acc: 0.9833 - val_loss: 0.2318 - val_acc: 0.9000\n",
            "Epoch 329/500\n",
            "\n",
            "Epoch 00329: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0529 - acc: 1.0000 - val_loss: 0.2497 - val_acc: 0.8500\n",
            "Epoch 330/500\n",
            "\n",
            "Epoch 00330: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0569 - acc: 1.0000 - val_loss: 0.2380 - val_acc: 0.9000\n",
            "Epoch 331/500\n",
            "\n",
            "Epoch 00331: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0458 - acc: 1.0000 - val_loss: 0.2226 - val_acc: 0.9000\n",
            "Epoch 332/500\n",
            "\n",
            "Epoch 00332: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.1114 - acc: 0.9833 - val_loss: 0.1896 - val_acc: 0.9000\n",
            "Epoch 333/500\n",
            "\n",
            "Epoch 00333: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.1013 - acc: 0.9833 - val_loss: 0.1716 - val_acc: 0.9500\n",
            "Epoch 334/500\n",
            "\n",
            "Epoch 00334: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0724 - acc: 0.9833 - val_loss: 0.1755 - val_acc: 0.9500\n",
            "Epoch 335/500\n",
            "\n",
            "Epoch 00335: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0523 - acc: 1.0000 - val_loss: 0.1835 - val_acc: 0.9500\n",
            "Epoch 336/500\n",
            "\n",
            "Epoch 00336: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0645 - acc: 1.0000 - val_loss: 0.1860 - val_acc: 0.9500\n",
            "Epoch 337/500\n",
            "\n",
            "Epoch 00337: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0813 - acc: 0.9833 - val_loss: 0.1843 - val_acc: 0.9500\n",
            "Epoch 338/500\n",
            "\n",
            "Epoch 00338: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0996 - acc: 0.9667 - val_loss: 0.1947 - val_acc: 0.9000\n",
            "Epoch 339/500\n",
            "\n",
            "Epoch 00339: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0543 - acc: 1.0000 - val_loss: 0.1889 - val_acc: 0.9000\n",
            "Epoch 340/500\n",
            "\n",
            "Epoch 00340: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0636 - acc: 0.9833 - val_loss: 0.1858 - val_acc: 0.9000\n",
            "Epoch 341/500\n",
            "\n",
            "Epoch 00341: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0321 - acc: 1.0000 - val_loss: 0.1846 - val_acc: 0.9000\n",
            "Epoch 342/500\n",
            "\n",
            "Epoch 00342: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0487 - acc: 0.9833 - val_loss: 0.1863 - val_acc: 0.9000\n",
            "Epoch 343/500\n",
            "\n",
            "Epoch 00343: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0497 - acc: 1.0000 - val_loss: 0.1992 - val_acc: 0.9000\n",
            "Epoch 344/500\n",
            "\n",
            "Epoch 00344: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0570 - acc: 1.0000 - val_loss: 0.2071 - val_acc: 0.9000\n",
            "Epoch 345/500\n",
            "\n",
            "Epoch 00345: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0593 - acc: 1.0000 - val_loss: 0.1967 - val_acc: 0.8500\n",
            "Epoch 346/500\n",
            "\n",
            "Epoch 00346: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0365 - acc: 1.0000 - val_loss: 0.1854 - val_acc: 0.9000\n",
            "Epoch 347/500\n",
            "\n",
            "Epoch 00347: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0327 - acc: 1.0000 - val_loss: 0.1820 - val_acc: 0.9000\n",
            "Epoch 348/500\n",
            "\n",
            "Epoch 00348: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0826 - acc: 0.9833 - val_loss: 0.1814 - val_acc: 0.9000\n",
            "Epoch 349/500\n",
            "\n",
            "Epoch 00349: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0624 - acc: 0.9833 - val_loss: 0.2029 - val_acc: 0.8500\n",
            "Epoch 350/500\n",
            "\n",
            "Epoch 00350: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0624 - acc: 1.0000 - val_loss: 0.2242 - val_acc: 0.8500\n",
            "Epoch 351/500\n",
            "\n",
            "Epoch 00351: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0542 - acc: 1.0000 - val_loss: 0.2395 - val_acc: 0.8500\n",
            "Epoch 352/500\n",
            "\n",
            "Epoch 00352: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0513 - acc: 1.0000 - val_loss: 0.2367 - val_acc: 0.8500\n",
            "Epoch 353/500\n",
            "\n",
            "Epoch 00353: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0487 - acc: 1.0000 - val_loss: 0.2295 - val_acc: 0.8500\n",
            "Epoch 354/500\n",
            "\n",
            "Epoch 00354: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0509 - acc: 1.0000 - val_loss: 0.2157 - val_acc: 0.9000\n",
            "Epoch 355/500\n",
            "\n",
            "Epoch 00355: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0474 - acc: 1.0000 - val_loss: 0.1986 - val_acc: 0.9000\n",
            "Epoch 356/500\n",
            "\n",
            "Epoch 00356: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0442 - acc: 1.0000 - val_loss: 0.1880 - val_acc: 0.9000\n",
            "Epoch 357/500\n",
            "\n",
            "Epoch 00357: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0408 - acc: 1.0000 - val_loss: 0.1904 - val_acc: 0.9000\n",
            "Epoch 358/500\n",
            "\n",
            "Epoch 00358: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0455 - acc: 1.0000 - val_loss: 0.1906 - val_acc: 0.9000\n",
            "Epoch 359/500\n",
            "\n",
            "Epoch 00359: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.1140 - acc: 0.9667 - val_loss: 0.2107 - val_acc: 0.8500\n",
            "Epoch 360/500\n",
            "\n",
            "Epoch 00360: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0673 - acc: 0.9833 - val_loss: 0.2235 - val_acc: 0.9000\n",
            "Epoch 361/500\n",
            "\n",
            "Epoch 00361: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0685 - acc: 0.9833 - val_loss: 0.2418 - val_acc: 0.8500\n",
            "Epoch 362/500\n",
            "\n",
            "Epoch 00362: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0551 - acc: 1.0000 - val_loss: 0.2491 - val_acc: 0.8500\n",
            "Epoch 363/500\n",
            "\n",
            "Epoch 00363: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0728 - acc: 0.9833 - val_loss: 0.2706 - val_acc: 0.8500\n",
            "Epoch 364/500\n",
            "\n",
            "Epoch 00364: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0681 - acc: 1.0000 - val_loss: 0.2914 - val_acc: 0.8500\n",
            "Epoch 365/500\n",
            "\n",
            "Epoch 00365: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0966 - acc: 0.9833 - val_loss: 0.2763 - val_acc: 0.8000\n",
            "Epoch 366/500\n",
            "\n",
            "Epoch 00366: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0601 - acc: 1.0000 - val_loss: 0.2614 - val_acc: 0.8500\n",
            "Epoch 367/500\n",
            "\n",
            "Epoch 00367: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0677 - acc: 0.9833 - val_loss: 0.2470 - val_acc: 0.8500\n",
            "Epoch 368/500\n",
            "\n",
            "Epoch 00368: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0560 - acc: 1.0000 - val_loss: 0.2358 - val_acc: 0.8500\n",
            "Epoch 369/500\n",
            "\n",
            "Epoch 00369: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0665 - acc: 0.9833 - val_loss: 0.2276 - val_acc: 0.8500\n",
            "Epoch 370/500\n",
            "\n",
            "Epoch 00370: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0381 - acc: 1.0000 - val_loss: 0.2141 - val_acc: 0.9000\n",
            "Epoch 371/500\n",
            "\n",
            "Epoch 00371: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0666 - acc: 1.0000 - val_loss: 0.2111 - val_acc: 0.9000\n",
            "Epoch 372/500\n",
            "\n",
            "Epoch 00372: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0485 - acc: 1.0000 - val_loss: 0.1950 - val_acc: 0.9000\n",
            "Epoch 373/500\n",
            "\n",
            "Epoch 00373: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0626 - acc: 0.9833 - val_loss: 0.1789 - val_acc: 0.9000\n",
            "Epoch 374/500\n",
            "\n",
            "Epoch 00374: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0566 - acc: 0.9833 - val_loss: 0.1567 - val_acc: 0.9500\n",
            "Epoch 375/500\n",
            "\n",
            "Epoch 00375: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0552 - acc: 1.0000 - val_loss: 0.1657 - val_acc: 0.9000\n",
            "Epoch 376/500\n",
            "\n",
            "Epoch 00376: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0504 - acc: 0.9833 - val_loss: 0.1798 - val_acc: 0.9000\n",
            "Epoch 377/500\n",
            "\n",
            "Epoch 00377: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0735 - acc: 0.9833 - val_loss: 0.1773 - val_acc: 0.9000\n",
            "Epoch 378/500\n",
            "\n",
            "Epoch 00378: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0423 - acc: 1.0000 - val_loss: 0.1837 - val_acc: 0.9000\n",
            "Epoch 379/500\n",
            "\n",
            "Epoch 00379: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0653 - acc: 1.0000 - val_loss: 0.1761 - val_acc: 0.9000\n",
            "Epoch 380/500\n",
            "\n",
            "Epoch 00380: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0598 - acc: 0.9833 - val_loss: 0.1859 - val_acc: 0.9000\n",
            "Epoch 381/500\n",
            "\n",
            "Epoch 00381: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0565 - acc: 1.0000 - val_loss: 0.1890 - val_acc: 0.8500\n",
            "Epoch 382/500\n",
            "\n",
            "Epoch 00382: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0630 - acc: 0.9833 - val_loss: 0.1941 - val_acc: 0.8500\n",
            "Epoch 383/500\n",
            "\n",
            "Epoch 00383: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0680 - acc: 0.9833 - val_loss: 0.2010 - val_acc: 0.9500\n",
            "Epoch 384/500\n",
            "\n",
            "Epoch 00384: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0369 - acc: 1.0000 - val_loss: 0.2002 - val_acc: 0.9500\n",
            "Epoch 385/500\n",
            "\n",
            "Epoch 00385: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0413 - acc: 1.0000 - val_loss: 0.1990 - val_acc: 0.9000\n",
            "Epoch 386/500\n",
            "\n",
            "Epoch 00386: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0515 - acc: 1.0000 - val_loss: 0.1944 - val_acc: 0.9000\n",
            "Epoch 387/500\n",
            "\n",
            "Epoch 00387: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0605 - acc: 1.0000 - val_loss: 0.1943 - val_acc: 0.9000\n",
            "Epoch 388/500\n",
            "\n",
            "Epoch 00388: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0398 - acc: 1.0000 - val_loss: 0.1918 - val_acc: 0.9500\n",
            "Epoch 389/500\n",
            "\n",
            "Epoch 00389: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0386 - acc: 1.0000 - val_loss: 0.1895 - val_acc: 0.9500\n",
            "Epoch 390/500\n",
            "\n",
            "Epoch 00390: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0269 - acc: 1.0000 - val_loss: 0.1888 - val_acc: 0.9000\n",
            "Epoch 391/500\n",
            "\n",
            "Epoch 00391: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0479 - acc: 1.0000 - val_loss: 0.2004 - val_acc: 0.9000\n",
            "Epoch 392/500\n",
            "\n",
            "Epoch 00392: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0384 - acc: 1.0000 - val_loss: 0.2132 - val_acc: 0.9000\n",
            "Epoch 393/500\n",
            "\n",
            "Epoch 00393: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0643 - acc: 0.9833 - val_loss: 0.2235 - val_acc: 0.8500\n",
            "Epoch 394/500\n",
            "\n",
            "Epoch 00394: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0754 - acc: 0.9833 - val_loss: 0.2716 - val_acc: 0.8500\n",
            "Epoch 395/500\n",
            "\n",
            "Epoch 00395: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0402 - acc: 1.0000 - val_loss: 0.3011 - val_acc: 0.8500\n",
            "Epoch 396/500\n",
            "\n",
            "Epoch 00396: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0774 - acc: 1.0000 - val_loss: 0.2754 - val_acc: 0.8500\n",
            "Epoch 397/500\n",
            "\n",
            "Epoch 00397: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0672 - acc: 1.0000 - val_loss: 0.2454 - val_acc: 0.8500\n",
            "Epoch 398/500\n",
            "\n",
            "Epoch 00398: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0758 - acc: 0.9833 - val_loss: 0.2284 - val_acc: 0.9000\n",
            "Epoch 399/500\n",
            "\n",
            "Epoch 00399: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0572 - acc: 1.0000 - val_loss: 0.2099 - val_acc: 0.9000\n",
            "Epoch 400/500\n",
            "\n",
            "Epoch 00400: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0820 - acc: 0.9833 - val_loss: 0.2110 - val_acc: 0.9000\n",
            "Epoch 401/500\n",
            "\n",
            "Epoch 00401: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0500 - acc: 1.0000 - val_loss: 0.2388 - val_acc: 0.8500\n",
            "Epoch 402/500\n",
            "\n",
            "Epoch 00402: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0401 - acc: 1.0000 - val_loss: 0.2599 - val_acc: 0.8500\n",
            "Epoch 403/500\n",
            "\n",
            "Epoch 00403: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0763 - acc: 0.9833 - val_loss: 0.2417 - val_acc: 0.8000\n",
            "Epoch 404/500\n",
            "\n",
            "Epoch 00404: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0598 - acc: 1.0000 - val_loss: 0.2318 - val_acc: 0.8500\n",
            "Epoch 405/500\n",
            "\n",
            "Epoch 00405: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0589 - acc: 1.0000 - val_loss: 0.2314 - val_acc: 0.9000\n",
            "Epoch 406/500\n",
            "\n",
            "Epoch 00406: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0322 - acc: 1.0000 - val_loss: 0.2425 - val_acc: 0.8500\n",
            "Epoch 407/500\n",
            "\n",
            "Epoch 00407: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0696 - acc: 1.0000 - val_loss: 0.2492 - val_acc: 0.8500\n",
            "Epoch 408/500\n",
            "\n",
            "Epoch 00408: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0470 - acc: 1.0000 - val_loss: 0.2474 - val_acc: 0.8500\n",
            "Epoch 409/500\n",
            "\n",
            "Epoch 00409: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0479 - acc: 1.0000 - val_loss: 0.2465 - val_acc: 0.8500\n",
            "Epoch 410/500\n",
            "\n",
            "Epoch 00410: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0670 - acc: 1.0000 - val_loss: 0.2239 - val_acc: 0.8500\n",
            "Epoch 411/500\n",
            "\n",
            "Epoch 00411: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0789 - acc: 0.9667 - val_loss: 0.1981 - val_acc: 0.9000\n",
            "Epoch 412/500\n",
            "\n",
            "Epoch 00412: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0481 - acc: 1.0000 - val_loss: 0.1872 - val_acc: 0.9000\n",
            "Epoch 413/500\n",
            "\n",
            "Epoch 00413: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0593 - acc: 0.9833 - val_loss: 0.1817 - val_acc: 0.8500\n",
            "Epoch 414/500\n",
            "\n",
            "Epoch 00414: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0418 - acc: 1.0000 - val_loss: 0.1837 - val_acc: 0.8500\n",
            "Epoch 415/500\n",
            "\n",
            "Epoch 00415: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0318 - acc: 1.0000 - val_loss: 0.1930 - val_acc: 0.8500\n",
            "Epoch 416/500\n",
            "\n",
            "Epoch 00416: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0440 - acc: 1.0000 - val_loss: 0.1993 - val_acc: 0.8500\n",
            "Epoch 417/500\n",
            "\n",
            "Epoch 00417: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0665 - acc: 1.0000 - val_loss: 0.2259 - val_acc: 0.8500\n",
            "Epoch 418/500\n",
            "\n",
            "Epoch 00418: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0789 - acc: 0.9667 - val_loss: 0.2589 - val_acc: 0.8000\n",
            "Epoch 419/500\n",
            "\n",
            "Epoch 00419: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0377 - acc: 1.0000 - val_loss: 0.2639 - val_acc: 0.8500\n",
            "Epoch 420/500\n",
            "\n",
            "Epoch 00420: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0489 - acc: 1.0000 - val_loss: 0.2686 - val_acc: 0.8500\n",
            "Epoch 421/500\n",
            "\n",
            "Epoch 00421: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0521 - acc: 1.0000 - val_loss: 0.2726 - val_acc: 0.9000\n",
            "Epoch 422/500\n",
            "\n",
            "Epoch 00422: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0467 - acc: 1.0000 - val_loss: 0.2722 - val_acc: 0.9000\n",
            "Epoch 423/500\n",
            "\n",
            "Epoch 00423: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0584 - acc: 1.0000 - val_loss: 0.2694 - val_acc: 0.8500\n",
            "Epoch 424/500\n",
            "\n",
            "Epoch 00424: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0308 - acc: 1.0000 - val_loss: 0.2677 - val_acc: 0.8500\n",
            "Epoch 425/500\n",
            "\n",
            "Epoch 00425: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0537 - acc: 0.9833 - val_loss: 0.2807 - val_acc: 0.8000\n",
            "Epoch 426/500\n",
            "\n",
            "Epoch 00426: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0619 - acc: 0.9833 - val_loss: 0.2851 - val_acc: 0.8000\n",
            "Epoch 427/500\n",
            "\n",
            "Epoch 00427: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0579 - acc: 1.0000 - val_loss: 0.2728 - val_acc: 0.9000\n",
            "Epoch 428/500\n",
            "\n",
            "Epoch 00428: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0613 - acc: 0.9833 - val_loss: 0.2705 - val_acc: 0.9000\n",
            "Epoch 429/500\n",
            "\n",
            "Epoch 00429: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0238 - acc: 1.0000 - val_loss: 0.2675 - val_acc: 0.9000\n",
            "Epoch 430/500\n",
            "\n",
            "Epoch 00430: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0610 - acc: 0.9833 - val_loss: 0.2562 - val_acc: 0.9000\n",
            "Epoch 431/500\n",
            "\n",
            "Epoch 00431: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0412 - acc: 1.0000 - val_loss: 0.2516 - val_acc: 0.9000\n",
            "Epoch 432/500\n",
            "\n",
            "Epoch 00432: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0475 - acc: 1.0000 - val_loss: 0.2543 - val_acc: 0.9000\n",
            "Epoch 433/500\n",
            "\n",
            "Epoch 00433: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0557 - acc: 0.9833 - val_loss: 0.2645 - val_acc: 0.9000\n",
            "Epoch 434/500\n",
            "\n",
            "Epoch 00434: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0512 - acc: 1.0000 - val_loss: 0.2657 - val_acc: 0.8500\n",
            "Epoch 435/500\n",
            "\n",
            "Epoch 00435: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0420 - acc: 1.0000 - val_loss: 0.2541 - val_acc: 0.8500\n",
            "Epoch 436/500\n",
            "\n",
            "Epoch 00436: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0659 - acc: 0.9833 - val_loss: 0.2464 - val_acc: 0.8500\n",
            "Epoch 437/500\n",
            "\n",
            "Epoch 00437: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0607 - acc: 0.9833 - val_loss: 0.2278 - val_acc: 0.8500\n",
            "Epoch 438/500\n",
            "\n",
            "Epoch 00438: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0504 - acc: 1.0000 - val_loss: 0.2146 - val_acc: 0.9000\n",
            "Epoch 439/500\n",
            "\n",
            "Epoch 00439: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0654 - acc: 1.0000 - val_loss: 0.1869 - val_acc: 0.9000\n",
            "Epoch 440/500\n",
            "\n",
            "Epoch 00440: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0649 - acc: 0.9667 - val_loss: 0.1734 - val_acc: 0.9500\n",
            "Epoch 441/500\n",
            "\n",
            "Epoch 00441: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0449 - acc: 1.0000 - val_loss: 0.1812 - val_acc: 0.9000\n",
            "Epoch 442/500\n",
            "\n",
            "Epoch 00442: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0332 - acc: 1.0000 - val_loss: 0.1922 - val_acc: 0.9000\n",
            "Epoch 443/500\n",
            "\n",
            "Epoch 00443: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0637 - acc: 1.0000 - val_loss: 0.2094 - val_acc: 0.9000\n",
            "Epoch 444/500\n",
            "\n",
            "Epoch 00444: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0242 - acc: 1.0000 - val_loss: 0.2214 - val_acc: 0.8500\n",
            "Epoch 445/500\n",
            "\n",
            "Epoch 00445: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0546 - acc: 1.0000 - val_loss: 0.2361 - val_acc: 0.8500\n",
            "Epoch 446/500\n",
            "\n",
            "Epoch 00446: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0337 - acc: 1.0000 - val_loss: 0.2446 - val_acc: 0.8500\n",
            "Epoch 447/500\n",
            "\n",
            "Epoch 00447: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0508 - acc: 0.9833 - val_loss: 0.2375 - val_acc: 0.8500\n",
            "Epoch 448/500\n",
            "\n",
            "Epoch 00448: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0648 - acc: 1.0000 - val_loss: 0.2279 - val_acc: 0.9000\n",
            "Epoch 449/500\n",
            "\n",
            "Epoch 00449: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0519 - acc: 1.0000 - val_loss: 0.2093 - val_acc: 0.9000\n",
            "Epoch 450/500\n",
            "\n",
            "Epoch 00450: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0415 - acc: 1.0000 - val_loss: 0.1909 - val_acc: 0.8500\n",
            "Epoch 451/500\n",
            "\n",
            "Epoch 00451: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0435 - acc: 0.9833 - val_loss: 0.1776 - val_acc: 0.9000\n",
            "Epoch 452/500\n",
            "\n",
            "Epoch 00452: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0510 - acc: 1.0000 - val_loss: 0.1764 - val_acc: 0.9000\n",
            "Epoch 453/500\n",
            "\n",
            "Epoch 00453: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0347 - acc: 1.0000 - val_loss: 0.1815 - val_acc: 0.9000\n",
            "Epoch 454/500\n",
            "\n",
            "Epoch 00454: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0646 - acc: 0.9667 - val_loss: 0.1985 - val_acc: 0.9000\n",
            "Epoch 455/500\n",
            "\n",
            "Epoch 00455: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0345 - acc: 1.0000 - val_loss: 0.2088 - val_acc: 0.9000\n",
            "Epoch 456/500\n",
            "\n",
            "Epoch 00456: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0512 - acc: 1.0000 - val_loss: 0.2043 - val_acc: 0.9000\n",
            "Epoch 457/500\n",
            "\n",
            "Epoch 00457: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0392 - acc: 1.0000 - val_loss: 0.1954 - val_acc: 0.9000\n",
            "Epoch 458/500\n",
            "\n",
            "Epoch 00458: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0230 - acc: 1.0000 - val_loss: 0.1797 - val_acc: 0.9000\n",
            "Epoch 459/500\n",
            "\n",
            "Epoch 00459: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0652 - acc: 1.0000 - val_loss: 0.1711 - val_acc: 0.9000\n",
            "Epoch 460/500\n",
            "\n",
            "Epoch 00460: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0344 - acc: 1.0000 - val_loss: 0.1675 - val_acc: 0.9000\n",
            "Epoch 461/500\n",
            "\n",
            "Epoch 00461: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0363 - acc: 1.0000 - val_loss: 0.1646 - val_acc: 0.9000\n",
            "Epoch 462/500\n",
            "\n",
            "Epoch 00462: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0264 - acc: 1.0000 - val_loss: 0.1670 - val_acc: 0.8500\n",
            "Epoch 463/500\n",
            "\n",
            "Epoch 00463: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0419 - acc: 1.0000 - val_loss: 0.1562 - val_acc: 0.9500\n",
            "Epoch 464/500\n",
            "\n",
            "Epoch 00464: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0390 - acc: 1.0000 - val_loss: 0.1472 - val_acc: 0.9500\n",
            "Epoch 465/500\n",
            "\n",
            "Epoch 00465: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0821 - acc: 0.9667 - val_loss: 0.1635 - val_acc: 0.9000\n",
            "Epoch 466/500\n",
            "\n",
            "Epoch 00466: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0364 - acc: 1.0000 - val_loss: 0.1806 - val_acc: 0.9000\n",
            "Epoch 467/500\n",
            "\n",
            "Epoch 00467: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0508 - acc: 1.0000 - val_loss: 0.1942 - val_acc: 0.8500\n",
            "Epoch 468/500\n",
            "\n",
            "Epoch 00468: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0357 - acc: 1.0000 - val_loss: 0.1912 - val_acc: 0.9000\n",
            "Epoch 469/500\n",
            "\n",
            "Epoch 00469: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0303 - acc: 1.0000 - val_loss: 0.1845 - val_acc: 0.9000\n",
            "Epoch 470/500\n",
            "\n",
            "Epoch 00470: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0300 - acc: 1.0000 - val_loss: 0.1767 - val_acc: 0.9000\n",
            "Epoch 471/500\n",
            "\n",
            "Epoch 00471: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0445 - acc: 1.0000 - val_loss: 0.1824 - val_acc: 0.9000\n",
            "Epoch 472/500\n",
            "\n",
            "Epoch 00472: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0404 - acc: 1.0000 - val_loss: 0.1888 - val_acc: 0.9000\n",
            "Epoch 473/500\n",
            "\n",
            "Epoch 00473: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0505 - acc: 1.0000 - val_loss: 0.1980 - val_acc: 0.9000\n",
            "Epoch 474/500\n",
            "\n",
            "Epoch 00474: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0550 - acc: 0.9833 - val_loss: 0.2126 - val_acc: 0.9000\n",
            "Epoch 475/500\n",
            "\n",
            "Epoch 00475: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0463 - acc: 1.0000 - val_loss: 0.2119 - val_acc: 0.9000\n",
            "Epoch 476/500\n",
            "\n",
            "Epoch 00476: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0397 - acc: 1.0000 - val_loss: 0.2002 - val_acc: 0.9000\n",
            "Epoch 477/500\n",
            "\n",
            "Epoch 00477: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0336 - acc: 1.0000 - val_loss: 0.2012 - val_acc: 0.8500\n",
            "Epoch 478/500\n",
            "\n",
            "Epoch 00478: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0362 - acc: 1.0000 - val_loss: 0.2142 - val_acc: 0.8500\n",
            "Epoch 479/500\n",
            "\n",
            "Epoch 00479: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0459 - acc: 1.0000 - val_loss: 0.2181 - val_acc: 0.8500\n",
            "Epoch 480/500\n",
            "\n",
            "Epoch 00480: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0373 - acc: 1.0000 - val_loss: 0.2068 - val_acc: 0.8500\n",
            "Epoch 481/500\n",
            "\n",
            "Epoch 00481: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0877 - acc: 0.9500 - val_loss: 0.2083 - val_acc: 0.8500\n",
            "Epoch 482/500\n",
            "\n",
            "Epoch 00482: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0260 - acc: 1.0000 - val_loss: 0.2033 - val_acc: 0.8500\n",
            "Epoch 483/500\n",
            "\n",
            "Epoch 00483: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0403 - acc: 1.0000 - val_loss: 0.1899 - val_acc: 0.8500\n",
            "Epoch 484/500\n",
            "\n",
            "Epoch 00484: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0264 - acc: 1.0000 - val_loss: 0.1794 - val_acc: 0.9000\n",
            "Epoch 485/500\n",
            "\n",
            "Epoch 00485: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0854 - acc: 0.9833 - val_loss: 0.1734 - val_acc: 0.9000\n",
            "Epoch 486/500\n",
            "\n",
            "Epoch 00486: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0355 - acc: 1.0000 - val_loss: 0.1748 - val_acc: 0.9000\n",
            "Epoch 487/500\n",
            "\n",
            "Epoch 00487: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0506 - acc: 0.9833 - val_loss: 0.2054 - val_acc: 0.8500\n",
            "Epoch 488/500\n",
            "\n",
            "Epoch 00488: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0298 - acc: 1.0000 - val_loss: 0.2183 - val_acc: 0.8500\n",
            "Epoch 489/500\n",
            "\n",
            "Epoch 00489: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0389 - acc: 0.9833 - val_loss: 0.2268 - val_acc: 0.9000\n",
            "Epoch 490/500\n",
            "\n",
            "Epoch 00490: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0302 - acc: 1.0000 - val_loss: 0.2197 - val_acc: 0.9000\n",
            "Epoch 491/500\n",
            "\n",
            "Epoch 00491: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0262 - acc: 1.0000 - val_loss: 0.1977 - val_acc: 0.8500\n",
            "Epoch 492/500\n",
            "\n",
            "Epoch 00492: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0381 - acc: 1.0000 - val_loss: 0.1920 - val_acc: 0.8500\n",
            "Epoch 493/500\n",
            "\n",
            "Epoch 00493: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0470 - acc: 1.0000 - val_loss: 0.1855 - val_acc: 0.8500\n",
            "Epoch 494/500\n",
            "\n",
            "Epoch 00494: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0360 - acc: 1.0000 - val_loss: 0.1847 - val_acc: 0.8500\n",
            "Epoch 495/500\n",
            "\n",
            "Epoch 00495: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0978 - acc: 0.9667 - val_loss: 0.2145 - val_acc: 0.8500\n",
            "Epoch 496/500\n",
            "\n",
            "Epoch 00496: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0491 - acc: 1.0000 - val_loss: 0.2505 - val_acc: 0.8500\n",
            "Epoch 497/500\n",
            "\n",
            "Epoch 00497: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0574 - acc: 1.0000 - val_loss: 0.2641 - val_acc: 0.8500\n",
            "Epoch 498/500\n",
            "\n",
            "Epoch 00498: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0428 - acc: 1.0000 - val_loss: 0.2348 - val_acc: 0.8500\n",
            "Epoch 499/500\n",
            "\n",
            "Epoch 00499: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0438 - acc: 1.0000 - val_loss: 0.2220 - val_acc: 0.8500\n",
            "Epoch 500/500\n",
            "\n",
            "Epoch 00500: val_loss did not improve from 0.12339\n",
            "60/60 - 0s - loss: 0.0526 - acc: 0.9833 - val_loss: 0.2424 - val_acc: 0.8500\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "# Evaluate on test data\n",
            "40/40 [==============================] - 0s 2ms/sample - loss: 0.6505 - acc: 0.6750\n",
            "test loss, test acc: [0.6504768526821862, 0.675]\n",
            "EEG_Deep/Data2A/parsed_P05T.mat\n",
            "Filtering of Training Data Finished\n",
            "EEG_Deep/Data2A/parsed_P05E.mat\n",
            "Filtering of Testing Data Finished\n",
            "(1, 2)\n",
            "Finding labels in training data\n",
            "Finding labels in testing data\n",
            "[1 2 2 2 1 1 1 1 1 2 1 1 2 1 1 2 1 2 1 2]\n",
            "(60, 12, 1536)\n",
            "(60,)\n",
            "(20, 12, 1536)\n",
            "(20,)\n",
            "[[1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]]\n",
            "X_train shape: (60, 1, 12, 1536)\n",
            "60 train samples\n",
            "20 val samples\n",
            "(40, 12, 1536)\n",
            "(40, 1)\n",
            "X_train shape: (40, 1, 12, 1536)\n",
            "40 train samples\n",
            "Train on 60 samples, validate on 20 samples\n",
            "Epoch 1/500\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.69362, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 1s - loss: 0.7063 - acc: 0.5167 - val_loss: 0.6936 - val_acc: 0.4500\n",
            "Epoch 2/500\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.69362\n",
            "60/60 - 0s - loss: 0.6843 - acc: 0.5333 - val_loss: 0.6979 - val_acc: 0.3500\n",
            "Epoch 3/500\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.69362\n",
            "60/60 - 0s - loss: 0.6634 - acc: 0.5833 - val_loss: 0.7008 - val_acc: 0.4500\n",
            "Epoch 4/500\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.69362\n",
            "60/60 - 0s - loss: 0.6540 - acc: 0.6167 - val_loss: 0.7034 - val_acc: 0.4500\n",
            "Epoch 5/500\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.69362\n",
            "60/60 - 0s - loss: 0.6580 - acc: 0.6333 - val_loss: 0.7046 - val_acc: 0.4500\n",
            "Epoch 6/500\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.69362\n",
            "60/60 - 0s - loss: 0.6159 - acc: 0.7333 - val_loss: 0.7056 - val_acc: 0.4500\n",
            "Epoch 7/500\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.69362\n",
            "60/60 - 0s - loss: 0.6096 - acc: 0.6667 - val_loss: 0.7065 - val_acc: 0.4500\n",
            "Epoch 8/500\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.69362\n",
            "60/60 - 0s - loss: 0.5925 - acc: 0.7333 - val_loss: 0.7074 - val_acc: 0.4500\n",
            "Epoch 9/500\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.69362\n",
            "60/60 - 0s - loss: 0.5744 - acc: 0.8333 - val_loss: 0.7087 - val_acc: 0.4500\n",
            "Epoch 10/500\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.69362\n",
            "60/60 - 0s - loss: 0.5685 - acc: 0.8667 - val_loss: 0.7090 - val_acc: 0.4500\n",
            "Epoch 11/500\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.69362\n",
            "60/60 - 0s - loss: 0.5627 - acc: 0.9000 - val_loss: 0.7114 - val_acc: 0.4500\n",
            "Epoch 12/500\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.69362\n",
            "60/60 - 0s - loss: 0.5442 - acc: 0.9000 - val_loss: 0.7132 - val_acc: 0.4500\n",
            "Epoch 13/500\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.69362\n",
            "60/60 - 0s - loss: 0.5453 - acc: 0.8667 - val_loss: 0.7139 - val_acc: 0.4500\n",
            "Epoch 14/500\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.69362\n",
            "60/60 - 0s - loss: 0.5251 - acc: 0.8833 - val_loss: 0.7151 - val_acc: 0.4500\n",
            "Epoch 15/500\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.69362\n",
            "60/60 - 0s - loss: 0.5186 - acc: 0.8500 - val_loss: 0.7149 - val_acc: 0.4500\n",
            "Epoch 16/500\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.69362\n",
            "60/60 - 0s - loss: 0.5109 - acc: 0.9667 - val_loss: 0.7158 - val_acc: 0.4500\n",
            "Epoch 17/500\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.69362\n",
            "60/60 - 0s - loss: 0.5033 - acc: 0.9000 - val_loss: 0.7166 - val_acc: 0.4000\n",
            "Epoch 18/500\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.69362\n",
            "60/60 - 0s - loss: 0.5137 - acc: 0.9000 - val_loss: 0.7160 - val_acc: 0.4500\n",
            "Epoch 19/500\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.69362\n",
            "60/60 - 0s - loss: 0.4686 - acc: 0.9500 - val_loss: 0.7131 - val_acc: 0.4500\n",
            "Epoch 20/500\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.69362\n",
            "60/60 - 0s - loss: 0.4884 - acc: 0.9000 - val_loss: 0.7114 - val_acc: 0.5500\n",
            "Epoch 21/500\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.69362\n",
            "60/60 - 0s - loss: 0.4965 - acc: 0.9000 - val_loss: 0.7104 - val_acc: 0.5500\n",
            "Epoch 22/500\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.69362\n",
            "60/60 - 0s - loss: 0.4398 - acc: 0.9167 - val_loss: 0.7103 - val_acc: 0.5500\n",
            "Epoch 23/500\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.69362\n",
            "60/60 - 0s - loss: 0.4589 - acc: 0.9667 - val_loss: 0.7096 - val_acc: 0.5500\n",
            "Epoch 24/500\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.69362\n",
            "60/60 - 0s - loss: 0.4825 - acc: 0.8333 - val_loss: 0.7043 - val_acc: 0.5500\n",
            "Epoch 25/500\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.69362\n",
            "60/60 - 0s - loss: 0.4405 - acc: 0.9333 - val_loss: 0.6986 - val_acc: 0.5500\n",
            "Epoch 26/500\n",
            "\n",
            "Epoch 00026: val_loss improved from 0.69362 to 0.69346, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4261 - acc: 0.9167 - val_loss: 0.6935 - val_acc: 0.5500\n",
            "Epoch 27/500\n",
            "\n",
            "Epoch 00027: val_loss improved from 0.69346 to 0.68886, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4263 - acc: 0.9000 - val_loss: 0.6889 - val_acc: 0.5500\n",
            "Epoch 28/500\n",
            "\n",
            "Epoch 00028: val_loss improved from 0.68886 to 0.68538, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4307 - acc: 0.8500 - val_loss: 0.6854 - val_acc: 0.6000\n",
            "Epoch 29/500\n",
            "\n",
            "Epoch 00029: val_loss improved from 0.68538 to 0.68006, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3776 - acc: 0.9333 - val_loss: 0.6801 - val_acc: 0.6000\n",
            "Epoch 30/500\n",
            "\n",
            "Epoch 00030: val_loss improved from 0.68006 to 0.67138, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4101 - acc: 0.9500 - val_loss: 0.6714 - val_acc: 0.7500\n",
            "Epoch 31/500\n",
            "\n",
            "Epoch 00031: val_loss improved from 0.67138 to 0.66018, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3962 - acc: 0.9167 - val_loss: 0.6602 - val_acc: 0.7500\n",
            "Epoch 32/500\n",
            "\n",
            "Epoch 00032: val_loss improved from 0.66018 to 0.64867, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3461 - acc: 0.9333 - val_loss: 0.6487 - val_acc: 0.7500\n",
            "Epoch 33/500\n",
            "\n",
            "Epoch 00033: val_loss improved from 0.64867 to 0.64207, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3613 - acc: 0.9667 - val_loss: 0.6421 - val_acc: 0.7500\n",
            "Epoch 34/500\n",
            "\n",
            "Epoch 00034: val_loss improved from 0.64207 to 0.63704, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3441 - acc: 0.9333 - val_loss: 0.6370 - val_acc: 0.7500\n",
            "Epoch 35/500\n",
            "\n",
            "Epoch 00035: val_loss improved from 0.63704 to 0.62859, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3302 - acc: 0.9333 - val_loss: 0.6286 - val_acc: 0.7500\n",
            "Epoch 36/500\n",
            "\n",
            "Epoch 00036: val_loss improved from 0.62859 to 0.61604, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3241 - acc: 0.9000 - val_loss: 0.6160 - val_acc: 0.8000\n",
            "Epoch 37/500\n",
            "\n",
            "Epoch 00037: val_loss improved from 0.61604 to 0.59849, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2982 - acc: 0.9167 - val_loss: 0.5985 - val_acc: 0.8000\n",
            "Epoch 38/500\n",
            "\n",
            "Epoch 00038: val_loss improved from 0.59849 to 0.58127, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2845 - acc: 0.9833 - val_loss: 0.5813 - val_acc: 0.8000\n",
            "Epoch 39/500\n",
            "\n",
            "Epoch 00039: val_loss improved from 0.58127 to 0.56215, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3158 - acc: 0.9167 - val_loss: 0.5622 - val_acc: 0.8000\n",
            "Epoch 40/500\n",
            "\n",
            "Epoch 00040: val_loss improved from 0.56215 to 0.56132, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3087 - acc: 0.9167 - val_loss: 0.5613 - val_acc: 0.8000\n",
            "Epoch 41/500\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.56132\n",
            "60/60 - 0s - loss: 0.2535 - acc: 0.9667 - val_loss: 0.5791 - val_acc: 0.8000\n",
            "Epoch 42/500\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.56132\n",
            "60/60 - 0s - loss: 0.2750 - acc: 0.9333 - val_loss: 0.5894 - val_acc: 0.7500\n",
            "Epoch 43/500\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.56132\n",
            "60/60 - 0s - loss: 0.2183 - acc: 0.9833 - val_loss: 0.5845 - val_acc: 0.7500\n",
            "Epoch 44/500\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.56132\n",
            "60/60 - 0s - loss: 0.2785 - acc: 0.9500 - val_loss: 0.5667 - val_acc: 0.8000\n",
            "Epoch 45/500\n",
            "\n",
            "Epoch 00045: val_loss improved from 0.56132 to 0.53547, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2661 - acc: 0.9333 - val_loss: 0.5355 - val_acc: 0.8000\n",
            "Epoch 46/500\n",
            "\n",
            "Epoch 00046: val_loss improved from 0.53547 to 0.51054, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2534 - acc: 0.9667 - val_loss: 0.5105 - val_acc: 0.8000\n",
            "Epoch 47/500\n",
            "\n",
            "Epoch 00047: val_loss improved from 0.51054 to 0.48922, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2542 - acc: 0.9500 - val_loss: 0.4892 - val_acc: 0.8000\n",
            "Epoch 48/500\n",
            "\n",
            "Epoch 00048: val_loss improved from 0.48922 to 0.47332, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2537 - acc: 0.9500 - val_loss: 0.4733 - val_acc: 0.8000\n",
            "Epoch 49/500\n",
            "\n",
            "Epoch 00049: val_loss improved from 0.47332 to 0.45992, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2451 - acc: 0.9167 - val_loss: 0.4599 - val_acc: 0.8000\n",
            "Epoch 50/500\n",
            "\n",
            "Epoch 00050: val_loss improved from 0.45992 to 0.45794, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2791 - acc: 0.9333 - val_loss: 0.4579 - val_acc: 0.8000\n",
            "Epoch 51/500\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.45794\n",
            "60/60 - 0s - loss: 0.2190 - acc: 0.9833 - val_loss: 0.4600 - val_acc: 0.8000\n",
            "Epoch 52/500\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.45794\n",
            "60/60 - 0s - loss: 0.2355 - acc: 0.9833 - val_loss: 0.4636 - val_acc: 0.8000\n",
            "Epoch 53/500\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.45794\n",
            "60/60 - 0s - loss: 0.2154 - acc: 0.9500 - val_loss: 0.4646 - val_acc: 0.8000\n",
            "Epoch 54/500\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.45794\n",
            "60/60 - 0s - loss: 0.2483 - acc: 0.9333 - val_loss: 0.4714 - val_acc: 0.8000\n",
            "Epoch 55/500\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.45794\n",
            "60/60 - 0s - loss: 0.2721 - acc: 0.9333 - val_loss: 0.4722 - val_acc: 0.7500\n",
            "Epoch 56/500\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.45794\n",
            "60/60 - 0s - loss: 0.2014 - acc: 0.9667 - val_loss: 0.4723 - val_acc: 0.7500\n",
            "Epoch 57/500\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.45794\n",
            "60/60 - 0s - loss: 0.1987 - acc: 0.9667 - val_loss: 0.4632 - val_acc: 0.7500\n",
            "Epoch 58/500\n",
            "\n",
            "Epoch 00058: val_loss improved from 0.45794 to 0.43720, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2484 - acc: 0.9333 - val_loss: 0.4372 - val_acc: 0.7500\n",
            "Epoch 59/500\n",
            "\n",
            "Epoch 00059: val_loss improved from 0.43720 to 0.41942, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2423 - acc: 0.9167 - val_loss: 0.4194 - val_acc: 0.8000\n",
            "Epoch 60/500\n",
            "\n",
            "Epoch 00060: val_loss improved from 0.41942 to 0.41649, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2566 - acc: 0.9333 - val_loss: 0.4165 - val_acc: 0.8000\n",
            "Epoch 61/500\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.41649\n",
            "60/60 - 0s - loss: 0.2054 - acc: 0.9333 - val_loss: 0.4208 - val_acc: 0.8000\n",
            "Epoch 62/500\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.41649\n",
            "60/60 - 0s - loss: 0.1588 - acc: 1.0000 - val_loss: 0.4225 - val_acc: 0.8000\n",
            "Epoch 63/500\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.41649\n",
            "60/60 - 0s - loss: 0.2218 - acc: 0.9333 - val_loss: 0.4332 - val_acc: 0.8000\n",
            "Epoch 64/500\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.41649\n",
            "60/60 - 0s - loss: 0.1859 - acc: 0.9833 - val_loss: 0.4470 - val_acc: 0.7500\n",
            "Epoch 65/500\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.41649\n",
            "60/60 - 0s - loss: 0.1816 - acc: 0.9667 - val_loss: 0.4484 - val_acc: 0.7500\n",
            "Epoch 66/500\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.41649\n",
            "60/60 - 0s - loss: 0.2307 - acc: 0.9000 - val_loss: 0.4493 - val_acc: 0.7500\n",
            "Epoch 67/500\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.41649\n",
            "60/60 - 0s - loss: 0.1667 - acc: 0.9667 - val_loss: 0.4274 - val_acc: 0.7500\n",
            "Epoch 68/500\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.41649\n",
            "60/60 - 0s - loss: 0.2255 - acc: 0.9167 - val_loss: 0.4259 - val_acc: 0.7500\n",
            "Epoch 69/500\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.41649\n",
            "60/60 - 0s - loss: 0.2648 - acc: 0.9333 - val_loss: 0.4665 - val_acc: 0.7500\n",
            "Epoch 70/500\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.41649\n",
            "60/60 - 0s - loss: 0.1918 - acc: 0.9500 - val_loss: 0.5077 - val_acc: 0.7500\n",
            "Epoch 71/500\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.41649\n",
            "60/60 - 0s - loss: 0.2339 - acc: 0.9167 - val_loss: 0.5156 - val_acc: 0.7500\n",
            "Epoch 72/500\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.41649\n",
            "60/60 - 0s - loss: 0.2264 - acc: 0.9333 - val_loss: 0.4827 - val_acc: 0.7500\n",
            "Epoch 73/500\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.41649\n",
            "60/60 - 0s - loss: 0.1991 - acc: 0.9500 - val_loss: 0.4537 - val_acc: 0.8000\n",
            "Epoch 74/500\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.41649\n",
            "60/60 - 0s - loss: 0.2385 - acc: 0.9333 - val_loss: 0.4318 - val_acc: 0.8500\n",
            "Epoch 75/500\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.41649\n",
            "60/60 - 0s - loss: 0.2116 - acc: 0.9167 - val_loss: 0.4321 - val_acc: 0.8500\n",
            "Epoch 76/500\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.41649\n",
            "60/60 - 0s - loss: 0.2040 - acc: 0.9333 - val_loss: 0.4209 - val_acc: 0.8000\n",
            "Epoch 77/500\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.41649\n",
            "60/60 - 0s - loss: 0.2755 - acc: 0.8667 - val_loss: 0.4213 - val_acc: 0.8000\n",
            "Epoch 78/500\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.41649\n",
            "60/60 - 0s - loss: 0.2296 - acc: 0.9500 - val_loss: 0.4322 - val_acc: 0.8500\n",
            "Epoch 79/500\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.41649\n",
            "60/60 - 0s - loss: 0.1887 - acc: 0.9667 - val_loss: 0.4331 - val_acc: 0.8000\n",
            "Epoch 80/500\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.41649\n",
            "60/60 - 0s - loss: 0.1426 - acc: 1.0000 - val_loss: 0.4348 - val_acc: 0.8000\n",
            "Epoch 81/500\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.41649\n",
            "60/60 - 0s - loss: 0.2250 - acc: 0.9667 - val_loss: 0.4381 - val_acc: 0.8000\n",
            "Epoch 82/500\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.41649\n",
            "60/60 - 0s - loss: 0.2050 - acc: 0.9500 - val_loss: 0.4438 - val_acc: 0.8000\n",
            "Epoch 83/500\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.41649\n",
            "60/60 - 0s - loss: 0.1760 - acc: 0.9500 - val_loss: 0.4585 - val_acc: 0.8000\n",
            "Epoch 84/500\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.41649\n",
            "60/60 - 0s - loss: 0.1981 - acc: 0.9667 - val_loss: 0.4774 - val_acc: 0.8000\n",
            "Epoch 85/500\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.41649\n",
            "60/60 - 0s - loss: 0.1517 - acc: 1.0000 - val_loss: 0.4904 - val_acc: 0.7500\n",
            "Epoch 86/500\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.41649\n",
            "60/60 - 0s - loss: 0.1484 - acc: 0.9833 - val_loss: 0.5138 - val_acc: 0.7500\n",
            "Epoch 87/500\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.41649\n",
            "60/60 - 0s - loss: 0.1743 - acc: 0.9500 - val_loss: 0.5201 - val_acc: 0.7500\n",
            "Epoch 88/500\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.41649\n",
            "60/60 - 0s - loss: 0.2115 - acc: 0.9167 - val_loss: 0.5200 - val_acc: 0.7500\n",
            "Epoch 89/500\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.41649\n",
            "60/60 - 0s - loss: 0.1859 - acc: 0.9667 - val_loss: 0.5247 - val_acc: 0.7500\n",
            "Epoch 90/500\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.41649\n",
            "60/60 - 0s - loss: 0.1402 - acc: 0.9833 - val_loss: 0.5408 - val_acc: 0.7500\n",
            "Epoch 91/500\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.41649\n",
            "60/60 - 0s - loss: 0.1366 - acc: 0.9833 - val_loss: 0.5628 - val_acc: 0.7500\n",
            "Epoch 92/500\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.41649\n",
            "60/60 - 0s - loss: 0.1809 - acc: 0.9500 - val_loss: 0.5721 - val_acc: 0.7500\n",
            "Epoch 93/500\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.41649\n",
            "60/60 - 0s - loss: 0.1673 - acc: 0.9667 - val_loss: 0.5314 - val_acc: 0.7500\n",
            "Epoch 94/500\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.41649\n",
            "60/60 - 0s - loss: 0.1522 - acc: 0.9500 - val_loss: 0.4871 - val_acc: 0.7500\n",
            "Epoch 95/500\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.41649\n",
            "60/60 - 0s - loss: 0.1440 - acc: 0.9667 - val_loss: 0.4527 - val_acc: 0.8000\n",
            "Epoch 96/500\n",
            "\n",
            "Epoch 00096: val_loss improved from 0.41649 to 0.41604, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1162 - acc: 1.0000 - val_loss: 0.4160 - val_acc: 0.8500\n",
            "Epoch 97/500\n",
            "\n",
            "Epoch 00097: val_loss improved from 0.41604 to 0.39536, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1467 - acc: 0.9667 - val_loss: 0.3954 - val_acc: 0.9000\n",
            "Epoch 98/500\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.39536\n",
            "60/60 - 0s - loss: 0.1454 - acc: 0.9667 - val_loss: 0.3959 - val_acc: 0.9000\n",
            "Epoch 99/500\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.39536\n",
            "60/60 - 0s - loss: 0.1334 - acc: 0.9833 - val_loss: 0.4098 - val_acc: 0.9000\n",
            "Epoch 100/500\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.39536\n",
            "60/60 - 0s - loss: 0.2001 - acc: 0.9333 - val_loss: 0.4228 - val_acc: 0.8500\n",
            "Epoch 101/500\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.39536\n",
            "60/60 - 0s - loss: 0.1905 - acc: 0.9333 - val_loss: 0.4115 - val_acc: 0.8500\n",
            "Epoch 102/500\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.39536\n",
            "60/60 - 0s - loss: 0.1069 - acc: 1.0000 - val_loss: 0.4063 - val_acc: 0.8500\n",
            "Epoch 103/500\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.39536\n",
            "60/60 - 0s - loss: 0.1373 - acc: 0.9833 - val_loss: 0.4147 - val_acc: 0.8500\n",
            "Epoch 104/500\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.39536\n",
            "60/60 - 0s - loss: 0.1388 - acc: 0.9667 - val_loss: 0.4669 - val_acc: 0.8000\n",
            "Epoch 105/500\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 0.39536\n",
            "60/60 - 0s - loss: 0.1719 - acc: 0.9500 - val_loss: 0.5404 - val_acc: 0.8000\n",
            "Epoch 106/500\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 0.39536\n",
            "60/60 - 0s - loss: 0.1347 - acc: 0.9833 - val_loss: 0.5739 - val_acc: 0.7500\n",
            "Epoch 107/500\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.39536\n",
            "60/60 - 0s - loss: 0.1197 - acc: 0.9833 - val_loss: 0.5741 - val_acc: 0.7500\n",
            "Epoch 108/500\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 0.39536\n",
            "60/60 - 0s - loss: 0.1922 - acc: 0.9500 - val_loss: 0.5634 - val_acc: 0.8000\n",
            "Epoch 109/500\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.39536\n",
            "60/60 - 0s - loss: 0.1315 - acc: 0.9667 - val_loss: 0.5553 - val_acc: 0.7500\n",
            "Epoch 110/500\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.39536\n",
            "60/60 - 0s - loss: 0.1383 - acc: 0.9833 - val_loss: 0.5768 - val_acc: 0.7500\n",
            "Epoch 111/500\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 0.39536\n",
            "60/60 - 0s - loss: 0.1491 - acc: 0.9667 - val_loss: 0.5905 - val_acc: 0.7500\n",
            "Epoch 112/500\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 0.39536\n",
            "60/60 - 0s - loss: 0.1745 - acc: 0.9667 - val_loss: 0.6034 - val_acc: 0.7500\n",
            "Epoch 113/500\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.39536\n",
            "60/60 - 0s - loss: 0.1071 - acc: 0.9667 - val_loss: 0.5918 - val_acc: 0.8000\n",
            "Epoch 114/500\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.39536\n",
            "60/60 - 0s - loss: 0.1723 - acc: 0.9500 - val_loss: 0.5847 - val_acc: 0.8000\n",
            "Epoch 115/500\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.39536\n",
            "60/60 - 0s - loss: 0.1136 - acc: 1.0000 - val_loss: 0.5772 - val_acc: 0.8000\n",
            "Epoch 116/500\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.39536\n",
            "60/60 - 0s - loss: 0.1720 - acc: 0.9667 - val_loss: 0.6174 - val_acc: 0.8000\n",
            "Epoch 117/500\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.39536\n",
            "60/60 - 0s - loss: 0.1303 - acc: 1.0000 - val_loss: 0.6493 - val_acc: 0.7500\n",
            "Epoch 118/500\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.39536\n",
            "60/60 - 0s - loss: 0.1311 - acc: 0.9667 - val_loss: 0.6894 - val_acc: 0.7500\n",
            "Epoch 119/500\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 0.39536\n",
            "60/60 - 0s - loss: 0.1277 - acc: 1.0000 - val_loss: 0.7251 - val_acc: 0.7500\n",
            "Epoch 120/500\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.39536\n",
            "60/60 - 0s - loss: 0.1746 - acc: 0.9333 - val_loss: 0.7343 - val_acc: 0.7500\n",
            "Epoch 121/500\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.39536\n",
            "60/60 - 0s - loss: 0.0787 - acc: 1.0000 - val_loss: 0.7231 - val_acc: 0.7500\n",
            "Epoch 122/500\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.39536\n",
            "60/60 - 0s - loss: 0.1482 - acc: 0.9667 - val_loss: 0.6633 - val_acc: 0.7500\n",
            "Epoch 123/500\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 0.39536\n",
            "60/60 - 0s - loss: 0.1591 - acc: 0.9667 - val_loss: 0.5945 - val_acc: 0.7500\n",
            "Epoch 124/500\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 0.39536\n",
            "60/60 - 0s - loss: 0.1702 - acc: 0.9833 - val_loss: 0.5309 - val_acc: 0.8000\n",
            "Epoch 125/500\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 0.39536\n",
            "60/60 - 0s - loss: 0.1571 - acc: 0.9667 - val_loss: 0.4905 - val_acc: 0.8000\n",
            "Epoch 126/500\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 0.39536\n",
            "60/60 - 0s - loss: 0.1383 - acc: 0.9833 - val_loss: 0.4757 - val_acc: 0.8000\n",
            "Epoch 127/500\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 0.39536\n",
            "60/60 - 0s - loss: 0.0910 - acc: 1.0000 - val_loss: 0.4482 - val_acc: 0.8000\n",
            "Epoch 128/500\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 0.39536\n",
            "60/60 - 0s - loss: 0.1845 - acc: 0.9500 - val_loss: 0.4269 - val_acc: 0.8000\n",
            "Epoch 129/500\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.39536\n",
            "60/60 - 0s - loss: 0.1182 - acc: 0.9833 - val_loss: 0.4110 - val_acc: 0.8500\n",
            "Epoch 130/500\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 0.39536\n",
            "60/60 - 0s - loss: 0.1138 - acc: 0.9833 - val_loss: 0.4079 - val_acc: 0.8500\n",
            "Epoch 131/500\n",
            "\n",
            "Epoch 00131: val_loss improved from 0.39536 to 0.38070, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.0982 - acc: 1.0000 - val_loss: 0.3807 - val_acc: 0.8500\n",
            "Epoch 132/500\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 0.38070\n",
            "60/60 - 0s - loss: 0.1557 - acc: 0.9333 - val_loss: 0.3814 - val_acc: 0.8500\n",
            "Epoch 133/500\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.38070\n",
            "60/60 - 0s - loss: 0.1398 - acc: 0.9667 - val_loss: 0.3933 - val_acc: 0.8000\n",
            "Epoch 134/500\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.38070\n",
            "60/60 - 0s - loss: 0.1271 - acc: 1.0000 - val_loss: 0.4208 - val_acc: 0.9000\n",
            "Epoch 135/500\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 0.38070\n",
            "60/60 - 0s - loss: 0.1341 - acc: 0.9667 - val_loss: 0.4268 - val_acc: 0.9000\n",
            "Epoch 136/500\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.38070\n",
            "60/60 - 0s - loss: 0.1202 - acc: 1.0000 - val_loss: 0.4309 - val_acc: 0.8500\n",
            "Epoch 137/500\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 0.38070\n",
            "60/60 - 0s - loss: 0.1083 - acc: 0.9833 - val_loss: 0.4403 - val_acc: 0.8000\n",
            "Epoch 138/500\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.38070\n",
            "60/60 - 0s - loss: 0.0982 - acc: 1.0000 - val_loss: 0.4518 - val_acc: 0.8500\n",
            "Epoch 139/500\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.38070\n",
            "60/60 - 0s - loss: 0.1297 - acc: 0.9833 - val_loss: 0.4664 - val_acc: 0.8500\n",
            "Epoch 140/500\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 0.38070\n",
            "60/60 - 0s - loss: 0.1125 - acc: 0.9833 - val_loss: 0.4701 - val_acc: 0.8500\n",
            "Epoch 141/500\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.38070\n",
            "60/60 - 0s - loss: 0.1140 - acc: 0.9667 - val_loss: 0.4649 - val_acc: 0.8500\n",
            "Epoch 142/500\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.38070\n",
            "60/60 - 0s - loss: 0.1138 - acc: 0.9667 - val_loss: 0.4792 - val_acc: 0.8000\n",
            "Epoch 143/500\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 0.38070\n",
            "60/60 - 0s - loss: 0.0820 - acc: 1.0000 - val_loss: 0.4771 - val_acc: 0.8000\n",
            "Epoch 144/500\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 0.38070\n",
            "60/60 - 0s - loss: 0.1608 - acc: 0.9333 - val_loss: 0.4670 - val_acc: 0.8000\n",
            "Epoch 145/500\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.38070\n",
            "60/60 - 0s - loss: 0.1050 - acc: 1.0000 - val_loss: 0.4509 - val_acc: 0.8000\n",
            "Epoch 146/500\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.38070\n",
            "60/60 - 0s - loss: 0.1017 - acc: 1.0000 - val_loss: 0.4224 - val_acc: 0.9000\n",
            "Epoch 147/500\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.38070\n",
            "60/60 - 0s - loss: 0.0883 - acc: 1.0000 - val_loss: 0.3940 - val_acc: 0.9000\n",
            "Epoch 148/500\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 0.38070\n",
            "60/60 - 0s - loss: 0.1028 - acc: 1.0000 - val_loss: 0.3888 - val_acc: 0.9000\n",
            "Epoch 149/500\n",
            "\n",
            "Epoch 00149: val_loss improved from 0.38070 to 0.37871, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1217 - acc: 0.9667 - val_loss: 0.3787 - val_acc: 0.9000\n",
            "Epoch 150/500\n",
            "\n",
            "Epoch 00150: val_loss improved from 0.37871 to 0.37830, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1556 - acc: 0.9333 - val_loss: 0.3783 - val_acc: 0.9000\n",
            "Epoch 151/500\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.37830\n",
            "60/60 - 0s - loss: 0.1281 - acc: 0.9833 - val_loss: 0.3948 - val_acc: 0.9000\n",
            "Epoch 152/500\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.37830\n",
            "60/60 - 0s - loss: 0.1600 - acc: 0.9667 - val_loss: 0.3995 - val_acc: 0.9000\n",
            "Epoch 153/500\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 0.37830\n",
            "60/60 - 0s - loss: 0.0997 - acc: 1.0000 - val_loss: 0.4263 - val_acc: 0.9000\n",
            "Epoch 154/500\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.37830\n",
            "60/60 - 0s - loss: 0.0700 - acc: 1.0000 - val_loss: 0.4468 - val_acc: 0.9000\n",
            "Epoch 155/500\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.37830\n",
            "60/60 - 0s - loss: 0.1457 - acc: 0.9833 - val_loss: 0.4637 - val_acc: 0.9000\n",
            "Epoch 156/500\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.37830\n",
            "60/60 - 0s - loss: 0.1341 - acc: 0.9500 - val_loss: 0.4744 - val_acc: 0.8500\n",
            "Epoch 157/500\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.37830\n",
            "60/60 - 0s - loss: 0.1208 - acc: 0.9833 - val_loss: 0.4751 - val_acc: 0.8500\n",
            "Epoch 158/500\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.37830\n",
            "60/60 - 0s - loss: 0.0857 - acc: 0.9833 - val_loss: 0.4535 - val_acc: 0.8500\n",
            "Epoch 159/500\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.37830\n",
            "60/60 - 0s - loss: 0.0908 - acc: 1.0000 - val_loss: 0.4540 - val_acc: 0.8500\n",
            "Epoch 160/500\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.37830\n",
            "60/60 - 0s - loss: 0.0842 - acc: 1.0000 - val_loss: 0.4567 - val_acc: 0.8500\n",
            "Epoch 161/500\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.37830\n",
            "60/60 - 0s - loss: 0.1196 - acc: 0.9667 - val_loss: 0.4738 - val_acc: 0.9000\n",
            "Epoch 162/500\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.37830\n",
            "60/60 - 0s - loss: 0.1203 - acc: 0.9833 - val_loss: 0.4689 - val_acc: 0.8500\n",
            "Epoch 163/500\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.37830\n",
            "60/60 - 0s - loss: 0.0980 - acc: 1.0000 - val_loss: 0.4827 - val_acc: 0.9000\n",
            "Epoch 164/500\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.37830\n",
            "60/60 - 0s - loss: 0.0971 - acc: 1.0000 - val_loss: 0.5049 - val_acc: 0.9000\n",
            "Epoch 165/500\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.37830\n",
            "60/60 - 0s - loss: 0.1377 - acc: 0.9667 - val_loss: 0.4836 - val_acc: 0.9000\n",
            "Epoch 166/500\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 0.37830\n",
            "60/60 - 0s - loss: 0.1260 - acc: 0.9833 - val_loss: 0.4890 - val_acc: 0.9000\n",
            "Epoch 167/500\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.37830\n",
            "60/60 - 0s - loss: 0.1303 - acc: 0.9833 - val_loss: 0.5057 - val_acc: 0.8500\n",
            "Epoch 168/500\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.37830\n",
            "60/60 - 0s - loss: 0.1231 - acc: 0.9833 - val_loss: 0.5029 - val_acc: 0.9000\n",
            "Epoch 169/500\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 0.37830\n",
            "60/60 - 0s - loss: 0.0930 - acc: 1.0000 - val_loss: 0.5152 - val_acc: 0.9000\n",
            "Epoch 170/500\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.37830\n",
            "60/60 - 0s - loss: 0.0735 - acc: 1.0000 - val_loss: 0.5312 - val_acc: 0.9000\n",
            "Epoch 171/500\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.37830\n",
            "60/60 - 0s - loss: 0.0659 - acc: 1.0000 - val_loss: 0.5329 - val_acc: 0.8500\n",
            "Epoch 172/500\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.37830\n",
            "60/60 - 0s - loss: 0.0934 - acc: 0.9833 - val_loss: 0.5262 - val_acc: 0.8500\n",
            "Epoch 173/500\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.37830\n",
            "60/60 - 0s - loss: 0.1549 - acc: 0.9667 - val_loss: 0.5302 - val_acc: 0.8500\n",
            "Epoch 174/500\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.37830\n",
            "60/60 - 0s - loss: 0.0965 - acc: 0.9833 - val_loss: 0.5577 - val_acc: 0.8000\n",
            "Epoch 175/500\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.37830\n",
            "60/60 - 0s - loss: 0.1230 - acc: 1.0000 - val_loss: 0.5868 - val_acc: 0.8000\n",
            "Epoch 176/500\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 0.37830\n",
            "60/60 - 0s - loss: 0.1147 - acc: 0.9667 - val_loss: 0.5744 - val_acc: 0.8000\n",
            "Epoch 177/500\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.37830\n",
            "60/60 - 0s - loss: 0.1240 - acc: 0.9667 - val_loss: 0.5480 - val_acc: 0.8000\n",
            "Epoch 178/500\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.37830\n",
            "60/60 - 0s - loss: 0.0761 - acc: 1.0000 - val_loss: 0.5466 - val_acc: 0.8000\n",
            "Epoch 179/500\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.37830\n",
            "60/60 - 0s - loss: 0.0940 - acc: 1.0000 - val_loss: 0.5603 - val_acc: 0.8000\n",
            "Epoch 180/500\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.37830\n",
            "60/60 - 0s - loss: 0.1148 - acc: 0.9833 - val_loss: 0.5427 - val_acc: 0.8500\n",
            "Epoch 181/500\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.37830\n",
            "60/60 - 0s - loss: 0.1159 - acc: 1.0000 - val_loss: 0.5144 - val_acc: 0.8500\n",
            "Epoch 182/500\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.37830\n",
            "60/60 - 0s - loss: 0.1020 - acc: 0.9833 - val_loss: 0.5246 - val_acc: 0.8500\n",
            "Epoch 183/500\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.37830\n",
            "60/60 - 0s - loss: 0.1299 - acc: 0.9833 - val_loss: 0.5088 - val_acc: 0.8500\n",
            "Epoch 184/500\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 0.37830\n",
            "60/60 - 0s - loss: 0.0862 - acc: 1.0000 - val_loss: 0.5070 - val_acc: 0.8000\n",
            "Epoch 185/500\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.37830\n",
            "60/60 - 0s - loss: 0.1056 - acc: 1.0000 - val_loss: 0.5124 - val_acc: 0.8500\n",
            "Epoch 186/500\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.37830\n",
            "60/60 - 0s - loss: 0.1100 - acc: 1.0000 - val_loss: 0.5233 - val_acc: 0.8500\n",
            "Epoch 187/500\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 0.37830\n",
            "60/60 - 0s - loss: 0.0832 - acc: 1.0000 - val_loss: 0.5395 - val_acc: 0.8500\n",
            "Epoch 188/500\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.37830\n",
            "60/60 - 0s - loss: 0.1351 - acc: 0.9667 - val_loss: 0.5490 - val_acc: 0.8000\n",
            "Epoch 189/500\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.37830\n",
            "60/60 - 0s - loss: 0.0719 - acc: 1.0000 - val_loss: 0.5604 - val_acc: 0.8500\n",
            "Epoch 190/500\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.37830\n",
            "60/60 - 0s - loss: 0.0843 - acc: 0.9833 - val_loss: 0.5836 - val_acc: 0.8500\n",
            "Epoch 191/500\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.37830\n",
            "60/60 - 0s - loss: 0.0777 - acc: 1.0000 - val_loss: 0.6247 - val_acc: 0.8000\n",
            "Epoch 192/500\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 0.37830\n",
            "60/60 - 0s - loss: 0.0648 - acc: 1.0000 - val_loss: 0.6081 - val_acc: 0.8000\n",
            "Epoch 193/500\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.37830\n",
            "60/60 - 0s - loss: 0.0821 - acc: 0.9833 - val_loss: 0.5885 - val_acc: 0.8000\n",
            "Epoch 194/500\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.37830\n",
            "60/60 - 0s - loss: 0.0671 - acc: 1.0000 - val_loss: 0.5699 - val_acc: 0.8500\n",
            "Epoch 195/500\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.37830\n",
            "60/60 - 0s - loss: 0.0800 - acc: 0.9667 - val_loss: 0.5332 - val_acc: 0.8500\n",
            "Epoch 196/500\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.37830\n",
            "60/60 - 0s - loss: 0.0836 - acc: 0.9833 - val_loss: 0.5159 - val_acc: 0.8500\n",
            "Epoch 197/500\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.37830\n",
            "60/60 - 0s - loss: 0.1289 - acc: 0.9833 - val_loss: 0.5073 - val_acc: 0.8500\n",
            "Epoch 198/500\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.37830\n",
            "60/60 - 0s - loss: 0.1154 - acc: 1.0000 - val_loss: 0.5650 - val_acc: 0.8500\n",
            "Epoch 199/500\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.37830\n",
            "60/60 - 0s - loss: 0.1283 - acc: 0.9833 - val_loss: 0.5022 - val_acc: 0.9000\n",
            "Epoch 200/500\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.37830\n",
            "60/60 - 0s - loss: 0.0831 - acc: 1.0000 - val_loss: 0.4738 - val_acc: 0.9000\n",
            "Epoch 201/500\n",
            "\n",
            "Epoch 00201: val_loss did not improve from 0.37830\n",
            "60/60 - 0s - loss: 0.1262 - acc: 0.9833 - val_loss: 0.4787 - val_acc: 0.9000\n",
            "Epoch 202/500\n",
            "\n",
            "Epoch 00202: val_loss did not improve from 0.37830\n",
            "60/60 - 0s - loss: 0.0857 - acc: 0.9833 - val_loss: 0.4900 - val_acc: 0.8500\n",
            "Epoch 203/500\n",
            "\n",
            "Epoch 00203: val_loss did not improve from 0.37830\n",
            "60/60 - 0s - loss: 0.0710 - acc: 0.9833 - val_loss: 0.5194 - val_acc: 0.8000\n",
            "Epoch 204/500\n",
            "\n",
            "Epoch 00204: val_loss did not improve from 0.37830\n",
            "60/60 - 0s - loss: 0.0766 - acc: 0.9833 - val_loss: 0.5462 - val_acc: 0.8000\n",
            "Epoch 205/500\n",
            "\n",
            "Epoch 00205: val_loss did not improve from 0.37830\n",
            "60/60 - 0s - loss: 0.1191 - acc: 0.9667 - val_loss: 0.5580 - val_acc: 0.8000\n",
            "Epoch 206/500\n",
            "\n",
            "Epoch 00206: val_loss did not improve from 0.37830\n",
            "60/60 - 0s - loss: 0.1150 - acc: 0.9500 - val_loss: 0.5578 - val_acc: 0.8000\n",
            "Epoch 207/500\n",
            "\n",
            "Epoch 00207: val_loss did not improve from 0.37830\n",
            "60/60 - 0s - loss: 0.0871 - acc: 1.0000 - val_loss: 0.5501 - val_acc: 0.8500\n",
            "Epoch 208/500\n",
            "\n",
            "Epoch 00208: val_loss did not improve from 0.37830\n",
            "60/60 - 0s - loss: 0.1114 - acc: 0.9833 - val_loss: 0.5168 - val_acc: 0.8500\n",
            "Epoch 209/500\n",
            "\n",
            "Epoch 00209: val_loss did not improve from 0.37830\n",
            "60/60 - 0s - loss: 0.1248 - acc: 0.9667 - val_loss: 0.4875 - val_acc: 0.8500\n",
            "Epoch 210/500\n",
            "\n",
            "Epoch 00210: val_loss did not improve from 0.37830\n",
            "60/60 - 0s - loss: 0.0764 - acc: 1.0000 - val_loss: 0.5147 - val_acc: 0.8500\n",
            "Epoch 211/500\n",
            "\n",
            "Epoch 00211: val_loss did not improve from 0.37830\n",
            "60/60 - 0s - loss: 0.0742 - acc: 1.0000 - val_loss: 0.5337 - val_acc: 0.8500\n",
            "Epoch 212/500\n",
            "\n",
            "Epoch 00212: val_loss did not improve from 0.37830\n",
            "60/60 - 0s - loss: 0.0787 - acc: 1.0000 - val_loss: 0.5618 - val_acc: 0.8500\n",
            "Epoch 213/500\n",
            "\n",
            "Epoch 00213: val_loss did not improve from 0.37830\n",
            "60/60 - 0s - loss: 0.0958 - acc: 1.0000 - val_loss: 0.5610 - val_acc: 0.8500\n",
            "Epoch 214/500\n",
            "\n",
            "Epoch 00214: val_loss did not improve from 0.37830\n",
            "60/60 - 0s - loss: 0.0928 - acc: 0.9833 - val_loss: 0.5407 - val_acc: 0.8000\n",
            "Epoch 215/500\n",
            "\n",
            "Epoch 00215: val_loss did not improve from 0.37830\n",
            "60/60 - 0s - loss: 0.0947 - acc: 0.9833 - val_loss: 0.5258 - val_acc: 0.8000\n",
            "Epoch 216/500\n",
            "\n",
            "Epoch 00216: val_loss did not improve from 0.37830\n",
            "60/60 - 0s - loss: 0.0598 - acc: 1.0000 - val_loss: 0.4978 - val_acc: 0.8000\n",
            "Epoch 217/500\n",
            "\n",
            "Epoch 00217: val_loss did not improve from 0.37830\n",
            "60/60 - 0s - loss: 0.1210 - acc: 0.9667 - val_loss: 0.4879 - val_acc: 0.8500\n",
            "Epoch 218/500\n",
            "\n",
            "Epoch 00218: val_loss did not improve from 0.37830\n",
            "60/60 - 0s - loss: 0.0842 - acc: 0.9833 - val_loss: 0.4697 - val_acc: 0.8500\n",
            "Epoch 219/500\n",
            "\n",
            "Epoch 00219: val_loss did not improve from 0.37830\n",
            "60/60 - 0s - loss: 0.0685 - acc: 1.0000 - val_loss: 0.4586 - val_acc: 0.9000\n",
            "Epoch 220/500\n",
            "\n",
            "Epoch 00220: val_loss did not improve from 0.37830\n",
            "60/60 - 0s - loss: 0.1223 - acc: 0.9667 - val_loss: 0.4630 - val_acc: 0.8500\n",
            "Epoch 221/500\n",
            "\n",
            "Epoch 00221: val_loss did not improve from 0.37830\n",
            "60/60 - 0s - loss: 0.0757 - acc: 1.0000 - val_loss: 0.4684 - val_acc: 0.8500\n",
            "Epoch 222/500\n",
            "\n",
            "Epoch 00222: val_loss did not improve from 0.37830\n",
            "60/60 - 0s - loss: 0.0724 - acc: 1.0000 - val_loss: 0.4780 - val_acc: 0.8500\n",
            "Epoch 223/500\n",
            "\n",
            "Epoch 00223: val_loss did not improve from 0.37830\n",
            "60/60 - 0s - loss: 0.1262 - acc: 0.9333 - val_loss: 0.4799 - val_acc: 0.8500\n",
            "Epoch 224/500\n",
            "\n",
            "Epoch 00224: val_loss did not improve from 0.37830\n",
            "60/60 - 0s - loss: 0.0790 - acc: 1.0000 - val_loss: 0.4865 - val_acc: 0.8500\n",
            "Epoch 225/500\n",
            "\n",
            "Epoch 00225: val_loss did not improve from 0.37830\n",
            "60/60 - 0s - loss: 0.1244 - acc: 0.9833 - val_loss: 0.4897 - val_acc: 0.8500\n",
            "Epoch 226/500\n",
            "\n",
            "Epoch 00226: val_loss did not improve from 0.37830\n",
            "60/60 - 0s - loss: 0.0819 - acc: 1.0000 - val_loss: 0.4915 - val_acc: 0.8500\n",
            "Epoch 227/500\n",
            "\n",
            "Epoch 00227: val_loss did not improve from 0.37830\n",
            "60/60 - 0s - loss: 0.0942 - acc: 0.9667 - val_loss: 0.4995 - val_acc: 0.8500\n",
            "Epoch 228/500\n",
            "\n",
            "Epoch 00228: val_loss did not improve from 0.37830\n",
            "60/60 - 0s - loss: 0.0828 - acc: 0.9833 - val_loss: 0.5002 - val_acc: 0.8500\n",
            "Epoch 229/500\n",
            "\n",
            "Epoch 00229: val_loss did not improve from 0.37830\n",
            "60/60 - 0s - loss: 0.0871 - acc: 1.0000 - val_loss: 0.4842 - val_acc: 0.8500\n",
            "Epoch 230/500\n",
            "\n",
            "Epoch 00230: val_loss did not improve from 0.37830\n",
            "60/60 - 0s - loss: 0.0897 - acc: 1.0000 - val_loss: 0.5073 - val_acc: 0.8500\n",
            "Epoch 231/500\n",
            "\n",
            "Epoch 00231: val_loss did not improve from 0.37830\n",
            "60/60 - 0s - loss: 0.0798 - acc: 1.0000 - val_loss: 0.5073 - val_acc: 0.8500\n",
            "Epoch 232/500\n",
            "\n",
            "Epoch 00232: val_loss did not improve from 0.37830\n",
            "60/60 - 0s - loss: 0.0681 - acc: 1.0000 - val_loss: 0.4918 - val_acc: 0.8500\n",
            "Epoch 233/500\n",
            "\n",
            "Epoch 00233: val_loss did not improve from 0.37830\n",
            "60/60 - 0s - loss: 0.0784 - acc: 1.0000 - val_loss: 0.4753 - val_acc: 0.8500\n",
            "Epoch 234/500\n",
            "\n",
            "Epoch 00234: val_loss did not improve from 0.37830\n",
            "60/60 - 0s - loss: 0.1548 - acc: 0.9500 - val_loss: 0.4688 - val_acc: 0.8500\n",
            "Epoch 235/500\n",
            "\n",
            "Epoch 00235: val_loss did not improve from 0.37830\n",
            "60/60 - 0s - loss: 0.1117 - acc: 0.9833 - val_loss: 0.5212 - val_acc: 0.8000\n",
            "Epoch 236/500\n",
            "\n",
            "Epoch 00236: val_loss did not improve from 0.37830\n",
            "60/60 - 0s - loss: 0.0913 - acc: 1.0000 - val_loss: 0.5828 - val_acc: 0.8000\n",
            "Epoch 237/500\n",
            "\n",
            "Epoch 00237: val_loss did not improve from 0.37830\n",
            "60/60 - 0s - loss: 0.0968 - acc: 1.0000 - val_loss: 0.5957 - val_acc: 0.8000\n",
            "Epoch 238/500\n",
            "\n",
            "Epoch 00238: val_loss did not improve from 0.37830\n",
            "60/60 - 0s - loss: 0.0807 - acc: 1.0000 - val_loss: 0.5903 - val_acc: 0.8000\n",
            "Epoch 239/500\n",
            "\n",
            "Epoch 00239: val_loss did not improve from 0.37830\n",
            "60/60 - 0s - loss: 0.1510 - acc: 0.9667 - val_loss: 0.5875 - val_acc: 0.8000\n",
            "Epoch 240/500\n",
            "\n",
            "Epoch 00240: val_loss did not improve from 0.37830\n",
            "60/60 - 0s - loss: 0.0989 - acc: 0.9833 - val_loss: 0.5693 - val_acc: 0.8500\n",
            "Epoch 241/500\n",
            "\n",
            "Epoch 00241: val_loss did not improve from 0.37830\n",
            "60/60 - 0s - loss: 0.0881 - acc: 1.0000 - val_loss: 0.5491 - val_acc: 0.8000\n",
            "Epoch 242/500\n",
            "\n",
            "Epoch 00242: val_loss did not improve from 0.37830\n",
            "60/60 - 0s - loss: 0.0782 - acc: 1.0000 - val_loss: 0.5295 - val_acc: 0.8000\n",
            "Epoch 243/500\n",
            "\n",
            "Epoch 00243: val_loss did not improve from 0.37830\n",
            "60/60 - 0s - loss: 0.1381 - acc: 0.9667 - val_loss: 0.4803 - val_acc: 0.8500\n",
            "Epoch 244/500\n",
            "\n",
            "Epoch 00244: val_loss did not improve from 0.37830\n",
            "60/60 - 0s - loss: 0.1371 - acc: 0.9667 - val_loss: 0.4274 - val_acc: 0.8500\n",
            "Epoch 245/500\n",
            "\n",
            "Epoch 00245: val_loss did not improve from 0.37830\n",
            "60/60 - 0s - loss: 0.0768 - acc: 1.0000 - val_loss: 0.4262 - val_acc: 0.8500\n",
            "Epoch 246/500\n",
            "\n",
            "Epoch 00246: val_loss did not improve from 0.37830\n",
            "60/60 - 0s - loss: 0.0729 - acc: 0.9833 - val_loss: 0.4552 - val_acc: 0.8000\n",
            "Epoch 247/500\n",
            "\n",
            "Epoch 00247: val_loss did not improve from 0.37830\n",
            "60/60 - 0s - loss: 0.0703 - acc: 1.0000 - val_loss: 0.4704 - val_acc: 0.8000\n",
            "Epoch 248/500\n",
            "\n",
            "Epoch 00248: val_loss did not improve from 0.37830\n",
            "60/60 - 0s - loss: 0.1325 - acc: 0.9333 - val_loss: 0.4747 - val_acc: 0.8000\n",
            "Epoch 249/500\n",
            "\n",
            "Epoch 00249: val_loss did not improve from 0.37830\n",
            "60/60 - 0s - loss: 0.1056 - acc: 0.9667 - val_loss: 0.4890 - val_acc: 0.8000\n",
            "Epoch 250/500\n",
            "\n",
            "Epoch 00250: val_loss did not improve from 0.37830\n",
            "60/60 - 0s - loss: 0.0734 - acc: 1.0000 - val_loss: 0.5067 - val_acc: 0.8000\n",
            "Epoch 251/500\n",
            "\n",
            "Epoch 00251: val_loss did not improve from 0.37830\n",
            "60/60 - 0s - loss: 0.0741 - acc: 0.9833 - val_loss: 0.5087 - val_acc: 0.8000\n",
            "Epoch 252/500\n",
            "\n",
            "Epoch 00252: val_loss did not improve from 0.37830\n",
            "60/60 - 0s - loss: 0.1029 - acc: 0.9833 - val_loss: 0.4899 - val_acc: 0.8000\n",
            "Epoch 253/500\n",
            "\n",
            "Epoch 00253: val_loss did not improve from 0.37830\n",
            "60/60 - 0s - loss: 0.0920 - acc: 1.0000 - val_loss: 0.5052 - val_acc: 0.8000\n",
            "Epoch 254/500\n",
            "\n",
            "Epoch 00254: val_loss did not improve from 0.37830\n",
            "60/60 - 0s - loss: 0.0689 - acc: 1.0000 - val_loss: 0.4700 - val_acc: 0.8500\n",
            "Epoch 255/500\n",
            "\n",
            "Epoch 00255: val_loss did not improve from 0.37830\n",
            "60/60 - 0s - loss: 0.0768 - acc: 1.0000 - val_loss: 0.4490 - val_acc: 0.8500\n",
            "Epoch 256/500\n",
            "\n",
            "Epoch 00256: val_loss did not improve from 0.37830\n",
            "60/60 - 0s - loss: 0.0745 - acc: 0.9833 - val_loss: 0.4098 - val_acc: 0.8500\n",
            "Epoch 257/500\n",
            "\n",
            "Epoch 00257: val_loss did not improve from 0.37830\n",
            "60/60 - 0s - loss: 0.0808 - acc: 1.0000 - val_loss: 0.3935 - val_acc: 0.8500\n",
            "Epoch 258/500\n",
            "\n",
            "Epoch 00258: val_loss did not improve from 0.37830\n",
            "60/60 - 0s - loss: 0.0585 - acc: 1.0000 - val_loss: 0.3847 - val_acc: 0.8500\n",
            "Epoch 259/500\n",
            "\n",
            "Epoch 00259: val_loss did not improve from 0.37830\n",
            "60/60 - 0s - loss: 0.0635 - acc: 1.0000 - val_loss: 0.3992 - val_acc: 0.8500\n",
            "Epoch 260/500\n",
            "\n",
            "Epoch 00260: val_loss did not improve from 0.37830\n",
            "60/60 - 0s - loss: 0.0825 - acc: 0.9667 - val_loss: 0.4307 - val_acc: 0.8500\n",
            "Epoch 261/500\n",
            "\n",
            "Epoch 00261: val_loss did not improve from 0.37830\n",
            "60/60 - 0s - loss: 0.0810 - acc: 1.0000 - val_loss: 0.4554 - val_acc: 0.8500\n",
            "Epoch 262/500\n",
            "\n",
            "Epoch 00262: val_loss did not improve from 0.37830\n",
            "60/60 - 0s - loss: 0.1136 - acc: 0.9833 - val_loss: 0.4414 - val_acc: 0.8500\n",
            "Epoch 263/500\n",
            "\n",
            "Epoch 00263: val_loss did not improve from 0.37830\n",
            "60/60 - 0s - loss: 0.0801 - acc: 1.0000 - val_loss: 0.4806 - val_acc: 0.8500\n",
            "Epoch 264/500\n",
            "\n",
            "Epoch 00264: val_loss did not improve from 0.37830\n",
            "60/60 - 0s - loss: 0.0626 - acc: 1.0000 - val_loss: 0.4973 - val_acc: 0.8500\n",
            "Epoch 265/500\n",
            "\n",
            "Epoch 00265: val_loss did not improve from 0.37830\n",
            "60/60 - 0s - loss: 0.0685 - acc: 1.0000 - val_loss: 0.5052 - val_acc: 0.8500\n",
            "Epoch 266/500\n",
            "\n",
            "Epoch 00266: val_loss did not improve from 0.37830\n",
            "60/60 - 0s - loss: 0.0787 - acc: 0.9833 - val_loss: 0.5099 - val_acc: 0.8500\n",
            "Epoch 267/500\n",
            "\n",
            "Epoch 00267: val_loss did not improve from 0.37830\n",
            "60/60 - 0s - loss: 0.1024 - acc: 0.9833 - val_loss: 0.4863 - val_acc: 0.8500\n",
            "Epoch 268/500\n",
            "\n",
            "Epoch 00268: val_loss did not improve from 0.37830\n",
            "60/60 - 0s - loss: 0.0573 - acc: 1.0000 - val_loss: 0.4862 - val_acc: 0.8500\n",
            "Epoch 269/500\n",
            "\n",
            "Epoch 00269: val_loss did not improve from 0.37830\n",
            "60/60 - 0s - loss: 0.0559 - acc: 1.0000 - val_loss: 0.4858 - val_acc: 0.8000\n",
            "Epoch 270/500\n",
            "\n",
            "Epoch 00270: val_loss did not improve from 0.37830\n",
            "60/60 - 0s - loss: 0.1252 - acc: 0.9667 - val_loss: 0.4692 - val_acc: 0.8000\n",
            "Epoch 271/500\n",
            "\n",
            "Epoch 00271: val_loss did not improve from 0.37830\n",
            "60/60 - 0s - loss: 0.0798 - acc: 0.9667 - val_loss: 0.4106 - val_acc: 0.8500\n",
            "Epoch 272/500\n",
            "\n",
            "Epoch 00272: val_loss did not improve from 0.37830\n",
            "60/60 - 0s - loss: 0.0650 - acc: 1.0000 - val_loss: 0.4022 - val_acc: 0.8500\n",
            "Epoch 273/500\n",
            "\n",
            "Epoch 00273: val_loss did not improve from 0.37830\n",
            "60/60 - 0s - loss: 0.0738 - acc: 0.9833 - val_loss: 0.4053 - val_acc: 0.8500\n",
            "Epoch 274/500\n",
            "\n",
            "Epoch 00274: val_loss did not improve from 0.37830\n",
            "60/60 - 0s - loss: 0.0890 - acc: 1.0000 - val_loss: 0.4177 - val_acc: 0.8500\n",
            "Epoch 275/500\n",
            "\n",
            "Epoch 00275: val_loss did not improve from 0.37830\n",
            "60/60 - 0s - loss: 0.0827 - acc: 1.0000 - val_loss: 0.4226 - val_acc: 0.8500\n",
            "Epoch 276/500\n",
            "\n",
            "Epoch 00276: val_loss did not improve from 0.37830\n",
            "60/60 - 0s - loss: 0.1062 - acc: 0.9667 - val_loss: 0.3964 - val_acc: 0.8500\n",
            "Epoch 277/500\n",
            "\n",
            "Epoch 00277: val_loss improved from 0.37830 to 0.37630, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.0617 - acc: 1.0000 - val_loss: 0.3763 - val_acc: 0.8500\n",
            "Epoch 278/500\n",
            "\n",
            "Epoch 00278: val_loss improved from 0.37630 to 0.36437, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1120 - acc: 0.9667 - val_loss: 0.3644 - val_acc: 0.9000\n",
            "Epoch 279/500\n",
            "\n",
            "Epoch 00279: val_loss did not improve from 0.36437\n",
            "60/60 - 0s - loss: 0.0953 - acc: 0.9667 - val_loss: 0.3715 - val_acc: 0.8500\n",
            "Epoch 280/500\n",
            "\n",
            "Epoch 00280: val_loss did not improve from 0.36437\n",
            "60/60 - 0s - loss: 0.0602 - acc: 1.0000 - val_loss: 0.3754 - val_acc: 0.8500\n",
            "Epoch 281/500\n",
            "\n",
            "Epoch 00281: val_loss did not improve from 0.36437\n",
            "60/60 - 0s - loss: 0.0879 - acc: 1.0000 - val_loss: 0.3768 - val_acc: 0.9000\n",
            "Epoch 282/500\n",
            "\n",
            "Epoch 00282: val_loss did not improve from 0.36437\n",
            "60/60 - 0s - loss: 0.0729 - acc: 1.0000 - val_loss: 0.3833 - val_acc: 0.8500\n",
            "Epoch 283/500\n",
            "\n",
            "Epoch 00283: val_loss did not improve from 0.36437\n",
            "60/60 - 0s - loss: 0.1044 - acc: 1.0000 - val_loss: 0.3989 - val_acc: 0.8000\n",
            "Epoch 284/500\n",
            "\n",
            "Epoch 00284: val_loss did not improve from 0.36437\n",
            "60/60 - 0s - loss: 0.0684 - acc: 1.0000 - val_loss: 0.4113 - val_acc: 0.8000\n",
            "Epoch 285/500\n",
            "\n",
            "Epoch 00285: val_loss did not improve from 0.36437\n",
            "60/60 - 0s - loss: 0.0939 - acc: 0.9833 - val_loss: 0.4312 - val_acc: 0.8000\n",
            "Epoch 286/500\n",
            "\n",
            "Epoch 00286: val_loss did not improve from 0.36437\n",
            "60/60 - 0s - loss: 0.0805 - acc: 0.9833 - val_loss: 0.4282 - val_acc: 0.8500\n",
            "Epoch 287/500\n",
            "\n",
            "Epoch 00287: val_loss did not improve from 0.36437\n",
            "60/60 - 0s - loss: 0.0627 - acc: 1.0000 - val_loss: 0.4233 - val_acc: 0.8500\n",
            "Epoch 288/500\n",
            "\n",
            "Epoch 00288: val_loss did not improve from 0.36437\n",
            "60/60 - 0s - loss: 0.1097 - acc: 1.0000 - val_loss: 0.4105 - val_acc: 0.8500\n",
            "Epoch 289/500\n",
            "\n",
            "Epoch 00289: val_loss did not improve from 0.36437\n",
            "60/60 - 0s - loss: 0.0576 - acc: 1.0000 - val_loss: 0.4123 - val_acc: 0.8500\n",
            "Epoch 290/500\n",
            "\n",
            "Epoch 00290: val_loss did not improve from 0.36437\n",
            "60/60 - 0s - loss: 0.0531 - acc: 1.0000 - val_loss: 0.4210 - val_acc: 0.9000\n",
            "Epoch 291/500\n",
            "\n",
            "Epoch 00291: val_loss did not improve from 0.36437\n",
            "60/60 - 0s - loss: 0.0582 - acc: 0.9833 - val_loss: 0.4304 - val_acc: 0.9000\n",
            "Epoch 292/500\n",
            "\n",
            "Epoch 00292: val_loss did not improve from 0.36437\n",
            "60/60 - 0s - loss: 0.1100 - acc: 1.0000 - val_loss: 0.4420 - val_acc: 0.9000\n",
            "Epoch 293/500\n",
            "\n",
            "Epoch 00293: val_loss did not improve from 0.36437\n",
            "60/60 - 0s - loss: 0.0528 - acc: 1.0000 - val_loss: 0.4549 - val_acc: 0.9000\n",
            "Epoch 294/500\n",
            "\n",
            "Epoch 00294: val_loss did not improve from 0.36437\n",
            "60/60 - 0s - loss: 0.0718 - acc: 1.0000 - val_loss: 0.4650 - val_acc: 0.9000\n",
            "Epoch 295/500\n",
            "\n",
            "Epoch 00295: val_loss did not improve from 0.36437\n",
            "60/60 - 0s - loss: 0.0825 - acc: 0.9667 - val_loss: 0.4662 - val_acc: 0.9000\n",
            "Epoch 296/500\n",
            "\n",
            "Epoch 00296: val_loss did not improve from 0.36437\n",
            "60/60 - 0s - loss: 0.0578 - acc: 1.0000 - val_loss: 0.4555 - val_acc: 0.9000\n",
            "Epoch 297/500\n",
            "\n",
            "Epoch 00297: val_loss did not improve from 0.36437\n",
            "60/60 - 0s - loss: 0.0517 - acc: 1.0000 - val_loss: 0.4717 - val_acc: 0.8500\n",
            "Epoch 298/500\n",
            "\n",
            "Epoch 00298: val_loss did not improve from 0.36437\n",
            "60/60 - 0s - loss: 0.1476 - acc: 0.9333 - val_loss: 0.4986 - val_acc: 0.8500\n",
            "Epoch 299/500\n",
            "\n",
            "Epoch 00299: val_loss did not improve from 0.36437\n",
            "60/60 - 0s - loss: 0.0481 - acc: 1.0000 - val_loss: 0.4791 - val_acc: 0.8500\n",
            "Epoch 300/500\n",
            "\n",
            "Epoch 00300: val_loss did not improve from 0.36437\n",
            "60/60 - 0s - loss: 0.0652 - acc: 1.0000 - val_loss: 0.4681 - val_acc: 0.8500\n",
            "Epoch 301/500\n",
            "\n",
            "Epoch 00301: val_loss did not improve from 0.36437\n",
            "60/60 - 0s - loss: 0.0858 - acc: 0.9667 - val_loss: 0.4583 - val_acc: 0.8500\n",
            "Epoch 302/500\n",
            "\n",
            "Epoch 00302: val_loss did not improve from 0.36437\n",
            "60/60 - 0s - loss: 0.0541 - acc: 1.0000 - val_loss: 0.4313 - val_acc: 0.9000\n",
            "Epoch 303/500\n",
            "\n",
            "Epoch 00303: val_loss did not improve from 0.36437\n",
            "60/60 - 0s - loss: 0.0630 - acc: 1.0000 - val_loss: 0.4227 - val_acc: 0.9000\n",
            "Epoch 304/500\n",
            "\n",
            "Epoch 00304: val_loss did not improve from 0.36437\n",
            "60/60 - 0s - loss: 0.0618 - acc: 1.0000 - val_loss: 0.4199 - val_acc: 0.9000\n",
            "Epoch 305/500\n",
            "\n",
            "Epoch 00305: val_loss did not improve from 0.36437\n",
            "60/60 - 0s - loss: 0.0719 - acc: 1.0000 - val_loss: 0.4237 - val_acc: 0.9000\n",
            "Epoch 306/500\n",
            "\n",
            "Epoch 00306: val_loss did not improve from 0.36437\n",
            "60/60 - 0s - loss: 0.0573 - acc: 1.0000 - val_loss: 0.4253 - val_acc: 0.9000\n",
            "Epoch 307/500\n",
            "\n",
            "Epoch 00307: val_loss did not improve from 0.36437\n",
            "60/60 - 0s - loss: 0.0807 - acc: 1.0000 - val_loss: 0.4545 - val_acc: 0.9000\n",
            "Epoch 308/500\n",
            "\n",
            "Epoch 00308: val_loss did not improve from 0.36437\n",
            "60/60 - 0s - loss: 0.0628 - acc: 1.0000 - val_loss: 0.4881 - val_acc: 0.9000\n",
            "Epoch 309/500\n",
            "\n",
            "Epoch 00309: val_loss did not improve from 0.36437\n",
            "60/60 - 0s - loss: 0.0709 - acc: 1.0000 - val_loss: 0.5049 - val_acc: 0.9000\n",
            "Epoch 310/500\n",
            "\n",
            "Epoch 00310: val_loss did not improve from 0.36437\n",
            "60/60 - 0s - loss: 0.0766 - acc: 1.0000 - val_loss: 0.4932 - val_acc: 0.9000\n",
            "Epoch 311/500\n",
            "\n",
            "Epoch 00311: val_loss did not improve from 0.36437\n",
            "60/60 - 0s - loss: 0.1102 - acc: 0.9833 - val_loss: 0.4829 - val_acc: 0.9000\n",
            "Epoch 312/500\n",
            "\n",
            "Epoch 00312: val_loss did not improve from 0.36437\n",
            "60/60 - 0s - loss: 0.0612 - acc: 1.0000 - val_loss: 0.4721 - val_acc: 0.9000\n",
            "Epoch 313/500\n",
            "\n",
            "Epoch 00313: val_loss did not improve from 0.36437\n",
            "60/60 - 0s - loss: 0.0858 - acc: 0.9833 - val_loss: 0.4781 - val_acc: 0.9000\n",
            "Epoch 314/500\n",
            "\n",
            "Epoch 00314: val_loss did not improve from 0.36437\n",
            "60/60 - 0s - loss: 0.0735 - acc: 1.0000 - val_loss: 0.4947 - val_acc: 0.8500\n",
            "Epoch 315/500\n",
            "\n",
            "Epoch 00315: val_loss did not improve from 0.36437\n",
            "60/60 - 0s - loss: 0.0488 - acc: 1.0000 - val_loss: 0.5168 - val_acc: 0.8000\n",
            "Epoch 316/500\n",
            "\n",
            "Epoch 00316: val_loss did not improve from 0.36437\n",
            "60/60 - 0s - loss: 0.0647 - acc: 1.0000 - val_loss: 0.5360 - val_acc: 0.8000\n",
            "Epoch 317/500\n",
            "\n",
            "Epoch 00317: val_loss did not improve from 0.36437\n",
            "60/60 - 0s - loss: 0.0550 - acc: 1.0000 - val_loss: 0.5538 - val_acc: 0.8000\n",
            "Epoch 318/500\n",
            "\n",
            "Epoch 00318: val_loss did not improve from 0.36437\n",
            "60/60 - 0s - loss: 0.0754 - acc: 1.0000 - val_loss: 0.5471 - val_acc: 0.8000\n",
            "Epoch 319/500\n",
            "\n",
            "Epoch 00319: val_loss did not improve from 0.36437\n",
            "60/60 - 0s - loss: 0.0633 - acc: 1.0000 - val_loss: 0.5312 - val_acc: 0.8000\n",
            "Epoch 320/500\n",
            "\n",
            "Epoch 00320: val_loss did not improve from 0.36437\n",
            "60/60 - 0s - loss: 0.0552 - acc: 1.0000 - val_loss: 0.4899 - val_acc: 0.8500\n",
            "Epoch 321/500\n",
            "\n",
            "Epoch 00321: val_loss did not improve from 0.36437\n",
            "60/60 - 0s - loss: 0.0433 - acc: 1.0000 - val_loss: 0.4665 - val_acc: 0.8500\n",
            "Epoch 322/500\n",
            "\n",
            "Epoch 00322: val_loss did not improve from 0.36437\n",
            "60/60 - 0s - loss: 0.0870 - acc: 0.9833 - val_loss: 0.4495 - val_acc: 0.8500\n",
            "Epoch 323/500\n",
            "\n",
            "Epoch 00323: val_loss did not improve from 0.36437\n",
            "60/60 - 0s - loss: 0.0813 - acc: 0.9833 - val_loss: 0.4345 - val_acc: 0.8500\n",
            "Epoch 324/500\n",
            "\n",
            "Epoch 00324: val_loss did not improve from 0.36437\n",
            "60/60 - 0s - loss: 0.0594 - acc: 1.0000 - val_loss: 0.4257 - val_acc: 0.8500\n",
            "Epoch 325/500\n",
            "\n",
            "Epoch 00325: val_loss did not improve from 0.36437\n",
            "60/60 - 0s - loss: 0.0846 - acc: 0.9833 - val_loss: 0.4302 - val_acc: 0.8500\n",
            "Epoch 326/500\n",
            "\n",
            "Epoch 00326: val_loss did not improve from 0.36437\n",
            "60/60 - 0s - loss: 0.0841 - acc: 0.9667 - val_loss: 0.4234 - val_acc: 0.8500\n",
            "Epoch 327/500\n",
            "\n",
            "Epoch 00327: val_loss did not improve from 0.36437\n",
            "60/60 - 0s - loss: 0.0610 - acc: 1.0000 - val_loss: 0.4310 - val_acc: 0.8500\n",
            "Epoch 328/500\n",
            "\n",
            "Epoch 00328: val_loss did not improve from 0.36437\n",
            "60/60 - 0s - loss: 0.0790 - acc: 0.9667 - val_loss: 0.4624 - val_acc: 0.8500\n",
            "Epoch 329/500\n",
            "\n",
            "Epoch 00329: val_loss did not improve from 0.36437\n",
            "60/60 - 0s - loss: 0.0756 - acc: 1.0000 - val_loss: 0.5353 - val_acc: 0.8500\n",
            "Epoch 330/500\n",
            "\n",
            "Epoch 00330: val_loss did not improve from 0.36437\n",
            "60/60 - 0s - loss: 0.0777 - acc: 1.0000 - val_loss: 0.5531 - val_acc: 0.8500\n",
            "Epoch 331/500\n",
            "\n",
            "Epoch 00331: val_loss did not improve from 0.36437\n",
            "60/60 - 0s - loss: 0.0660 - acc: 1.0000 - val_loss: 0.5433 - val_acc: 0.8500\n",
            "Epoch 332/500\n",
            "\n",
            "Epoch 00332: val_loss did not improve from 0.36437\n",
            "60/60 - 0s - loss: 0.0490 - acc: 1.0000 - val_loss: 0.5258 - val_acc: 0.8500\n",
            "Epoch 333/500\n",
            "\n",
            "Epoch 00333: val_loss did not improve from 0.36437\n",
            "60/60 - 0s - loss: 0.0729 - acc: 0.9667 - val_loss: 0.4857 - val_acc: 0.9000\n",
            "Epoch 334/500\n",
            "\n",
            "Epoch 00334: val_loss did not improve from 0.36437\n",
            "60/60 - 0s - loss: 0.1445 - acc: 0.9667 - val_loss: 0.4381 - val_acc: 0.9000\n",
            "Epoch 335/500\n",
            "\n",
            "Epoch 00335: val_loss did not improve from 0.36437\n",
            "60/60 - 0s - loss: 0.1313 - acc: 0.9667 - val_loss: 0.4874 - val_acc: 0.9000\n",
            "Epoch 336/500\n",
            "\n",
            "Epoch 00336: val_loss did not improve from 0.36437\n",
            "60/60 - 0s - loss: 0.1030 - acc: 0.9667 - val_loss: 0.4556 - val_acc: 0.9000\n",
            "Epoch 337/500\n",
            "\n",
            "Epoch 00337: val_loss did not improve from 0.36437\n",
            "60/60 - 0s - loss: 0.0695 - acc: 1.0000 - val_loss: 0.4504 - val_acc: 0.9000\n",
            "Epoch 338/500\n",
            "\n",
            "Epoch 00338: val_loss did not improve from 0.36437\n",
            "60/60 - 0s - loss: 0.0638 - acc: 0.9833 - val_loss: 0.4455 - val_acc: 0.8500\n",
            "Epoch 339/500\n",
            "\n",
            "Epoch 00339: val_loss did not improve from 0.36437\n",
            "60/60 - 0s - loss: 0.0935 - acc: 1.0000 - val_loss: 0.4256 - val_acc: 0.8500\n",
            "Epoch 340/500\n",
            "\n",
            "Epoch 00340: val_loss did not improve from 0.36437\n",
            "60/60 - 0s - loss: 0.0924 - acc: 0.9833 - val_loss: 0.3747 - val_acc: 0.9000\n",
            "Epoch 341/500\n",
            "\n",
            "Epoch 00341: val_loss improved from 0.36437 to 0.33196, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.0387 - acc: 1.0000 - val_loss: 0.3320 - val_acc: 0.8500\n",
            "Epoch 342/500\n",
            "\n",
            "Epoch 00342: val_loss improved from 0.33196 to 0.31876, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.0883 - acc: 0.9833 - val_loss: 0.3188 - val_acc: 0.9000\n",
            "Epoch 343/500\n",
            "\n",
            "Epoch 00343: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.0839 - acc: 1.0000 - val_loss: 0.3302 - val_acc: 0.9000\n",
            "Epoch 344/500\n",
            "\n",
            "Epoch 00344: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.1409 - acc: 0.9500 - val_loss: 0.3606 - val_acc: 0.8500\n",
            "Epoch 345/500\n",
            "\n",
            "Epoch 00345: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.0712 - acc: 1.0000 - val_loss: 0.4012 - val_acc: 0.8500\n",
            "Epoch 346/500\n",
            "\n",
            "Epoch 00346: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.0597 - acc: 0.9833 - val_loss: 0.4222 - val_acc: 0.8500\n",
            "Epoch 347/500\n",
            "\n",
            "Epoch 00347: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.0728 - acc: 0.9833 - val_loss: 0.4329 - val_acc: 0.8500\n",
            "Epoch 348/500\n",
            "\n",
            "Epoch 00348: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.0806 - acc: 0.9833 - val_loss: 0.4382 - val_acc: 0.8500\n",
            "Epoch 349/500\n",
            "\n",
            "Epoch 00349: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.0843 - acc: 0.9833 - val_loss: 0.4624 - val_acc: 0.7500\n",
            "Epoch 350/500\n",
            "\n",
            "Epoch 00350: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.0858 - acc: 0.9833 - val_loss: 0.5096 - val_acc: 0.7000\n",
            "Epoch 351/500\n",
            "\n",
            "Epoch 00351: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.0888 - acc: 0.9833 - val_loss: 0.5352 - val_acc: 0.7500\n",
            "Epoch 352/500\n",
            "\n",
            "Epoch 00352: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.0537 - acc: 1.0000 - val_loss: 0.5501 - val_acc: 0.7500\n",
            "Epoch 353/500\n",
            "\n",
            "Epoch 00353: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.0851 - acc: 0.9833 - val_loss: 0.5568 - val_acc: 0.8000\n",
            "Epoch 354/500\n",
            "\n",
            "Epoch 00354: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.0853 - acc: 0.9833 - val_loss: 0.5177 - val_acc: 0.8500\n",
            "Epoch 355/500\n",
            "\n",
            "Epoch 00355: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.0556 - acc: 1.0000 - val_loss: 0.4778 - val_acc: 0.8500\n",
            "Epoch 356/500\n",
            "\n",
            "Epoch 00356: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.0695 - acc: 0.9667 - val_loss: 0.4572 - val_acc: 0.8000\n",
            "Epoch 357/500\n",
            "\n",
            "Epoch 00357: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.0815 - acc: 0.9833 - val_loss: 0.4724 - val_acc: 0.8000\n",
            "Epoch 358/500\n",
            "\n",
            "Epoch 00358: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.0610 - acc: 0.9833 - val_loss: 0.4792 - val_acc: 0.8000\n",
            "Epoch 359/500\n",
            "\n",
            "Epoch 00359: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.0570 - acc: 0.9833 - val_loss: 0.5189 - val_acc: 0.8500\n",
            "Epoch 360/500\n",
            "\n",
            "Epoch 00360: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.0390 - acc: 1.0000 - val_loss: 0.5441 - val_acc: 0.8500\n",
            "Epoch 361/500\n",
            "\n",
            "Epoch 00361: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.0527 - acc: 1.0000 - val_loss: 0.5314 - val_acc: 0.8500\n",
            "Epoch 362/500\n",
            "\n",
            "Epoch 00362: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.0769 - acc: 0.9833 - val_loss: 0.4980 - val_acc: 0.8500\n",
            "Epoch 363/500\n",
            "\n",
            "Epoch 00363: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.0862 - acc: 1.0000 - val_loss: 0.4725 - val_acc: 0.8500\n",
            "Epoch 364/500\n",
            "\n",
            "Epoch 00364: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.0457 - acc: 1.0000 - val_loss: 0.4480 - val_acc: 0.8500\n",
            "Epoch 365/500\n",
            "\n",
            "Epoch 00365: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.0627 - acc: 1.0000 - val_loss: 0.4601 - val_acc: 0.9000\n",
            "Epoch 366/500\n",
            "\n",
            "Epoch 00366: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.0578 - acc: 1.0000 - val_loss: 0.5052 - val_acc: 0.8000\n",
            "Epoch 367/500\n",
            "\n",
            "Epoch 00367: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.0482 - acc: 0.9833 - val_loss: 0.5555 - val_acc: 0.8000\n",
            "Epoch 368/500\n",
            "\n",
            "Epoch 00368: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.0621 - acc: 1.0000 - val_loss: 0.5439 - val_acc: 0.8000\n",
            "Epoch 369/500\n",
            "\n",
            "Epoch 00369: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.0679 - acc: 1.0000 - val_loss: 0.5188 - val_acc: 0.8000\n",
            "Epoch 370/500\n",
            "\n",
            "Epoch 00370: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.0502 - acc: 1.0000 - val_loss: 0.4663 - val_acc: 0.8500\n",
            "Epoch 371/500\n",
            "\n",
            "Epoch 00371: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.0700 - acc: 0.9667 - val_loss: 0.4210 - val_acc: 0.9000\n",
            "Epoch 372/500\n",
            "\n",
            "Epoch 00372: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.0468 - acc: 1.0000 - val_loss: 0.4132 - val_acc: 0.8500\n",
            "Epoch 373/500\n",
            "\n",
            "Epoch 00373: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.0516 - acc: 1.0000 - val_loss: 0.4347 - val_acc: 0.8500\n",
            "Epoch 374/500\n",
            "\n",
            "Epoch 00374: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.0551 - acc: 1.0000 - val_loss: 0.4610 - val_acc: 0.8500\n",
            "Epoch 375/500\n",
            "\n",
            "Epoch 00375: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.0626 - acc: 1.0000 - val_loss: 0.5028 - val_acc: 0.8000\n",
            "Epoch 376/500\n",
            "\n",
            "Epoch 00376: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.0682 - acc: 1.0000 - val_loss: 0.5360 - val_acc: 0.8000\n",
            "Epoch 377/500\n",
            "\n",
            "Epoch 00377: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.1145 - acc: 0.9667 - val_loss: 0.5574 - val_acc: 0.8500\n",
            "Epoch 378/500\n",
            "\n",
            "Epoch 00378: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.0566 - acc: 0.9833 - val_loss: 0.4450 - val_acc: 0.8500\n",
            "Epoch 379/500\n",
            "\n",
            "Epoch 00379: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.0515 - acc: 0.9833 - val_loss: 0.4160 - val_acc: 0.8000\n",
            "Epoch 380/500\n",
            "\n",
            "Epoch 00380: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.0452 - acc: 1.0000 - val_loss: 0.4218 - val_acc: 0.8500\n",
            "Epoch 381/500\n",
            "\n",
            "Epoch 00381: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.0559 - acc: 1.0000 - val_loss: 0.4406 - val_acc: 0.9000\n",
            "Epoch 382/500\n",
            "\n",
            "Epoch 00382: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.0777 - acc: 0.9833 - val_loss: 0.4806 - val_acc: 0.8500\n",
            "Epoch 383/500\n",
            "\n",
            "Epoch 00383: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.0552 - acc: 1.0000 - val_loss: 0.5193 - val_acc: 0.8500\n",
            "Epoch 384/500\n",
            "\n",
            "Epoch 00384: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.0703 - acc: 0.9833 - val_loss: 0.5694 - val_acc: 0.8500\n",
            "Epoch 385/500\n",
            "\n",
            "Epoch 00385: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.0549 - acc: 1.0000 - val_loss: 0.5772 - val_acc: 0.8000\n",
            "Epoch 386/500\n",
            "\n",
            "Epoch 00386: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.0546 - acc: 1.0000 - val_loss: 0.5679 - val_acc: 0.8000\n",
            "Epoch 387/500\n",
            "\n",
            "Epoch 00387: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.0675 - acc: 1.0000 - val_loss: 0.5576 - val_acc: 0.8000\n",
            "Epoch 388/500\n",
            "\n",
            "Epoch 00388: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.0584 - acc: 1.0000 - val_loss: 0.4943 - val_acc: 0.8500\n",
            "Epoch 389/500\n",
            "\n",
            "Epoch 00389: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.1086 - acc: 0.9667 - val_loss: 0.4406 - val_acc: 0.9000\n",
            "Epoch 390/500\n",
            "\n",
            "Epoch 00390: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.0608 - acc: 1.0000 - val_loss: 0.4486 - val_acc: 0.8500\n",
            "Epoch 391/500\n",
            "\n",
            "Epoch 00391: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.0580 - acc: 1.0000 - val_loss: 0.4694 - val_acc: 0.8500\n",
            "Epoch 392/500\n",
            "\n",
            "Epoch 00392: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.0467 - acc: 1.0000 - val_loss: 0.4813 - val_acc: 0.8500\n",
            "Epoch 393/500\n",
            "\n",
            "Epoch 00393: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.0799 - acc: 0.9833 - val_loss: 0.4750 - val_acc: 0.8500\n",
            "Epoch 394/500\n",
            "\n",
            "Epoch 00394: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.0627 - acc: 1.0000 - val_loss: 0.4505 - val_acc: 0.8500\n",
            "Epoch 395/500\n",
            "\n",
            "Epoch 00395: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.0392 - acc: 1.0000 - val_loss: 0.4491 - val_acc: 0.8500\n",
            "Epoch 396/500\n",
            "\n",
            "Epoch 00396: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.0716 - acc: 0.9667 - val_loss: 0.4495 - val_acc: 0.8500\n",
            "Epoch 397/500\n",
            "\n",
            "Epoch 00397: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.0487 - acc: 1.0000 - val_loss: 0.4562 - val_acc: 0.8500\n",
            "Epoch 398/500\n",
            "\n",
            "Epoch 00398: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.0293 - acc: 1.0000 - val_loss: 0.4711 - val_acc: 0.8500\n",
            "Epoch 399/500\n",
            "\n",
            "Epoch 00399: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.0371 - acc: 1.0000 - val_loss: 0.4751 - val_acc: 0.8500\n",
            "Epoch 400/500\n",
            "\n",
            "Epoch 00400: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.0417 - acc: 1.0000 - val_loss: 0.4813 - val_acc: 0.8500\n",
            "Epoch 401/500\n",
            "\n",
            "Epoch 00401: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.0676 - acc: 0.9833 - val_loss: 0.4942 - val_acc: 0.8500\n",
            "Epoch 402/500\n",
            "\n",
            "Epoch 00402: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.0484 - acc: 0.9833 - val_loss: 0.5186 - val_acc: 0.8500\n",
            "Epoch 403/500\n",
            "\n",
            "Epoch 00403: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.0502 - acc: 1.0000 - val_loss: 0.5268 - val_acc: 0.9000\n",
            "Epoch 404/500\n",
            "\n",
            "Epoch 00404: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.0547 - acc: 1.0000 - val_loss: 0.5520 - val_acc: 0.9000\n",
            "Epoch 405/500\n",
            "\n",
            "Epoch 00405: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.1195 - acc: 0.9500 - val_loss: 0.5986 - val_acc: 0.9000\n",
            "Epoch 406/500\n",
            "\n",
            "Epoch 00406: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.0597 - acc: 0.9833 - val_loss: 0.6648 - val_acc: 0.9000\n",
            "Epoch 407/500\n",
            "\n",
            "Epoch 00407: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.0503 - acc: 1.0000 - val_loss: 0.6797 - val_acc: 0.8500\n",
            "Epoch 408/500\n",
            "\n",
            "Epoch 00408: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.0662 - acc: 0.9833 - val_loss: 0.7094 - val_acc: 0.8500\n",
            "Epoch 409/500\n",
            "\n",
            "Epoch 00409: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.0648 - acc: 0.9833 - val_loss: 0.6939 - val_acc: 0.8500\n",
            "Epoch 410/500\n",
            "\n",
            "Epoch 00410: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.0638 - acc: 0.9833 - val_loss: 0.7011 - val_acc: 0.8500\n",
            "Epoch 411/500\n",
            "\n",
            "Epoch 00411: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.0531 - acc: 0.9833 - val_loss: 0.6763 - val_acc: 0.9000\n",
            "Epoch 412/500\n",
            "\n",
            "Epoch 00412: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.0942 - acc: 0.9833 - val_loss: 0.6864 - val_acc: 0.8500\n",
            "Epoch 413/500\n",
            "\n",
            "Epoch 00413: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.0454 - acc: 1.0000 - val_loss: 0.6679 - val_acc: 0.8000\n",
            "Epoch 414/500\n",
            "\n",
            "Epoch 00414: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.0516 - acc: 0.9833 - val_loss: 0.6207 - val_acc: 0.9000\n",
            "Epoch 415/500\n",
            "\n",
            "Epoch 00415: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.0601 - acc: 0.9833 - val_loss: 0.5097 - val_acc: 0.9000\n",
            "Epoch 416/500\n",
            "\n",
            "Epoch 00416: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.0607 - acc: 1.0000 - val_loss: 0.4684 - val_acc: 0.8000\n",
            "Epoch 417/500\n",
            "\n",
            "Epoch 00417: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.0532 - acc: 1.0000 - val_loss: 0.4706 - val_acc: 0.8000\n",
            "Epoch 418/500\n",
            "\n",
            "Epoch 00418: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.0422 - acc: 1.0000 - val_loss: 0.4751 - val_acc: 0.9000\n",
            "Epoch 419/500\n",
            "\n",
            "Epoch 00419: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.1495 - acc: 0.9333 - val_loss: 0.4808 - val_acc: 0.9000\n",
            "Epoch 420/500\n",
            "\n",
            "Epoch 00420: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.0849 - acc: 0.9833 - val_loss: 0.4340 - val_acc: 0.9000\n",
            "Epoch 421/500\n",
            "\n",
            "Epoch 00421: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.0845 - acc: 0.9833 - val_loss: 0.4594 - val_acc: 0.9000\n",
            "Epoch 422/500\n",
            "\n",
            "Epoch 00422: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.0647 - acc: 0.9833 - val_loss: 0.5118 - val_acc: 0.9000\n",
            "Epoch 423/500\n",
            "\n",
            "Epoch 00423: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.0723 - acc: 0.9833 - val_loss: 0.5370 - val_acc: 0.9000\n",
            "Epoch 424/500\n",
            "\n",
            "Epoch 00424: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.0640 - acc: 0.9833 - val_loss: 0.5673 - val_acc: 0.9000\n",
            "Epoch 425/500\n",
            "\n",
            "Epoch 00425: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.0641 - acc: 1.0000 - val_loss: 0.5911 - val_acc: 0.9000\n",
            "Epoch 426/500\n",
            "\n",
            "Epoch 00426: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.0402 - acc: 1.0000 - val_loss: 0.6003 - val_acc: 0.9000\n",
            "Epoch 427/500\n",
            "\n",
            "Epoch 00427: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.0522 - acc: 1.0000 - val_loss: 0.5908 - val_acc: 0.9000\n",
            "Epoch 428/500\n",
            "\n",
            "Epoch 00428: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.0653 - acc: 0.9833 - val_loss: 0.5570 - val_acc: 0.9000\n",
            "Epoch 429/500\n",
            "\n",
            "Epoch 00429: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.0548 - acc: 1.0000 - val_loss: 0.5043 - val_acc: 0.9000\n",
            "Epoch 430/500\n",
            "\n",
            "Epoch 00430: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.0516 - acc: 1.0000 - val_loss: 0.5040 - val_acc: 0.9000\n",
            "Epoch 431/500\n",
            "\n",
            "Epoch 00431: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.0446 - acc: 1.0000 - val_loss: 0.5524 - val_acc: 0.9000\n",
            "Epoch 432/500\n",
            "\n",
            "Epoch 00432: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.0538 - acc: 1.0000 - val_loss: 0.6008 - val_acc: 0.9000\n",
            "Epoch 433/500\n",
            "\n",
            "Epoch 00433: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.0627 - acc: 0.9833 - val_loss: 0.6213 - val_acc: 0.9000\n",
            "Epoch 434/500\n",
            "\n",
            "Epoch 00434: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.1558 - acc: 0.9167 - val_loss: 0.5834 - val_acc: 0.8500\n",
            "Epoch 435/500\n",
            "\n",
            "Epoch 00435: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.0484 - acc: 1.0000 - val_loss: 0.5649 - val_acc: 0.8500\n",
            "Epoch 436/500\n",
            "\n",
            "Epoch 00436: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.0420 - acc: 1.0000 - val_loss: 0.5422 - val_acc: 0.8000\n",
            "Epoch 437/500\n",
            "\n",
            "Epoch 00437: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.0421 - acc: 1.0000 - val_loss: 0.5619 - val_acc: 0.9000\n",
            "Epoch 438/500\n",
            "\n",
            "Epoch 00438: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.0455 - acc: 1.0000 - val_loss: 0.6212 - val_acc: 0.8500\n",
            "Epoch 439/500\n",
            "\n",
            "Epoch 00439: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.0649 - acc: 0.9833 - val_loss: 0.6969 - val_acc: 0.8500\n",
            "Epoch 440/500\n",
            "\n",
            "Epoch 00440: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.0511 - acc: 1.0000 - val_loss: 0.7546 - val_acc: 0.8000\n",
            "Epoch 441/500\n",
            "\n",
            "Epoch 00441: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.0523 - acc: 1.0000 - val_loss: 0.7614 - val_acc: 0.8000\n",
            "Epoch 442/500\n",
            "\n",
            "Epoch 00442: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.0637 - acc: 0.9833 - val_loss: 0.6593 - val_acc: 0.8000\n",
            "Epoch 443/500\n",
            "\n",
            "Epoch 00443: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.1046 - acc: 0.9833 - val_loss: 0.5749 - val_acc: 0.8500\n",
            "Epoch 444/500\n",
            "\n",
            "Epoch 00444: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.0472 - acc: 1.0000 - val_loss: 0.5502 - val_acc: 0.8500\n",
            "Epoch 445/500\n",
            "\n",
            "Epoch 00445: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.0922 - acc: 0.9833 - val_loss: 0.5559 - val_acc: 0.8500\n",
            "Epoch 446/500\n",
            "\n",
            "Epoch 00446: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.0790 - acc: 1.0000 - val_loss: 0.6574 - val_acc: 0.8500\n",
            "Epoch 447/500\n",
            "\n",
            "Epoch 00447: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.0760 - acc: 0.9833 - val_loss: 0.7053 - val_acc: 0.8000\n",
            "Epoch 448/500\n",
            "\n",
            "Epoch 00448: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.0715 - acc: 0.9833 - val_loss: 0.6484 - val_acc: 0.8000\n",
            "Epoch 449/500\n",
            "\n",
            "Epoch 00449: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.0520 - acc: 1.0000 - val_loss: 0.6491 - val_acc: 0.8000\n",
            "Epoch 450/500\n",
            "\n",
            "Epoch 00450: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.0544 - acc: 1.0000 - val_loss: 0.6259 - val_acc: 0.8000\n",
            "Epoch 451/500\n",
            "\n",
            "Epoch 00451: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.0338 - acc: 1.0000 - val_loss: 0.6120 - val_acc: 0.8000\n",
            "Epoch 452/500\n",
            "\n",
            "Epoch 00452: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.0595 - acc: 1.0000 - val_loss: 0.6145 - val_acc: 0.8500\n",
            "Epoch 453/500\n",
            "\n",
            "Epoch 00453: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.0429 - acc: 1.0000 - val_loss: 0.6163 - val_acc: 0.8500\n",
            "Epoch 454/500\n",
            "\n",
            "Epoch 00454: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.0381 - acc: 1.0000 - val_loss: 0.6251 - val_acc: 0.9000\n",
            "Epoch 455/500\n",
            "\n",
            "Epoch 00455: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.0634 - acc: 0.9833 - val_loss: 0.6050 - val_acc: 0.9000\n",
            "Epoch 456/500\n",
            "\n",
            "Epoch 00456: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.0691 - acc: 0.9833 - val_loss: 0.5501 - val_acc: 0.9000\n",
            "Epoch 457/500\n",
            "\n",
            "Epoch 00457: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.0409 - acc: 1.0000 - val_loss: 0.5474 - val_acc: 0.9000\n",
            "Epoch 458/500\n",
            "\n",
            "Epoch 00458: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.0360 - acc: 1.0000 - val_loss: 0.5657 - val_acc: 0.9000\n",
            "Epoch 459/500\n",
            "\n",
            "Epoch 00459: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.0411 - acc: 1.0000 - val_loss: 0.5489 - val_acc: 0.9000\n",
            "Epoch 460/500\n",
            "\n",
            "Epoch 00460: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.0419 - acc: 1.0000 - val_loss: 0.5457 - val_acc: 0.9000\n",
            "Epoch 461/500\n",
            "\n",
            "Epoch 00461: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.0453 - acc: 1.0000 - val_loss: 0.5348 - val_acc: 0.9000\n",
            "Epoch 462/500\n",
            "\n",
            "Epoch 00462: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.0488 - acc: 1.0000 - val_loss: 0.5324 - val_acc: 0.9000\n",
            "Epoch 463/500\n",
            "\n",
            "Epoch 00463: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.1038 - acc: 0.9500 - val_loss: 0.5083 - val_acc: 0.9000\n",
            "Epoch 464/500\n",
            "\n",
            "Epoch 00464: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.0477 - acc: 1.0000 - val_loss: 0.5102 - val_acc: 0.9000\n",
            "Epoch 465/500\n",
            "\n",
            "Epoch 00465: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.0875 - acc: 0.9667 - val_loss: 0.4748 - val_acc: 0.9000\n",
            "Epoch 466/500\n",
            "\n",
            "Epoch 00466: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.0484 - acc: 1.0000 - val_loss: 0.4730 - val_acc: 0.9000\n",
            "Epoch 467/500\n",
            "\n",
            "Epoch 00467: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.0487 - acc: 1.0000 - val_loss: 0.4823 - val_acc: 0.9000\n",
            "Epoch 468/500\n",
            "\n",
            "Epoch 00468: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.0443 - acc: 1.0000 - val_loss: 0.4939 - val_acc: 0.9000\n",
            "Epoch 469/500\n",
            "\n",
            "Epoch 00469: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.0363 - acc: 1.0000 - val_loss: 0.5153 - val_acc: 0.9000\n",
            "Epoch 470/500\n",
            "\n",
            "Epoch 00470: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.0326 - acc: 1.0000 - val_loss: 0.5131 - val_acc: 0.8500\n",
            "Epoch 471/500\n",
            "\n",
            "Epoch 00471: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.0299 - acc: 1.0000 - val_loss: 0.5199 - val_acc: 0.8500\n",
            "Epoch 472/500\n",
            "\n",
            "Epoch 00472: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.0774 - acc: 0.9667 - val_loss: 0.5208 - val_acc: 0.8500\n",
            "Epoch 473/500\n",
            "\n",
            "Epoch 00473: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.0343 - acc: 1.0000 - val_loss: 0.4785 - val_acc: 0.8500\n",
            "Epoch 474/500\n",
            "\n",
            "Epoch 00474: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.0459 - acc: 0.9833 - val_loss: 0.4524 - val_acc: 0.8500\n",
            "Epoch 475/500\n",
            "\n",
            "Epoch 00475: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.0344 - acc: 1.0000 - val_loss: 0.4358 - val_acc: 0.8500\n",
            "Epoch 476/500\n",
            "\n",
            "Epoch 00476: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.0438 - acc: 1.0000 - val_loss: 0.4258 - val_acc: 0.8500\n",
            "Epoch 477/500\n",
            "\n",
            "Epoch 00477: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.0605 - acc: 0.9833 - val_loss: 0.4171 - val_acc: 0.8500\n",
            "Epoch 478/500\n",
            "\n",
            "Epoch 00478: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.0282 - acc: 1.0000 - val_loss: 0.4314 - val_acc: 0.8500\n",
            "Epoch 479/500\n",
            "\n",
            "Epoch 00479: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.0633 - acc: 0.9833 - val_loss: 0.4438 - val_acc: 0.8500\n",
            "Epoch 480/500\n",
            "\n",
            "Epoch 00480: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.0900 - acc: 0.9667 - val_loss: 0.4287 - val_acc: 0.8500\n",
            "Epoch 481/500\n",
            "\n",
            "Epoch 00481: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.0359 - acc: 1.0000 - val_loss: 0.4440 - val_acc: 0.8500\n",
            "Epoch 482/500\n",
            "\n",
            "Epoch 00482: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.0272 - acc: 1.0000 - val_loss: 0.4573 - val_acc: 0.9000\n",
            "Epoch 483/500\n",
            "\n",
            "Epoch 00483: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.0486 - acc: 0.9833 - val_loss: 0.4521 - val_acc: 0.9000\n",
            "Epoch 484/500\n",
            "\n",
            "Epoch 00484: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.0525 - acc: 1.0000 - val_loss: 0.3796 - val_acc: 0.9000\n",
            "Epoch 485/500\n",
            "\n",
            "Epoch 00485: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.0555 - acc: 1.0000 - val_loss: 0.3410 - val_acc: 0.8500\n",
            "Epoch 486/500\n",
            "\n",
            "Epoch 00486: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.0380 - acc: 1.0000 - val_loss: 0.3354 - val_acc: 0.8500\n",
            "Epoch 487/500\n",
            "\n",
            "Epoch 00487: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.0332 - acc: 1.0000 - val_loss: 0.3398 - val_acc: 0.8500\n",
            "Epoch 488/500\n",
            "\n",
            "Epoch 00488: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.0721 - acc: 1.0000 - val_loss: 0.3436 - val_acc: 0.8500\n",
            "Epoch 489/500\n",
            "\n",
            "Epoch 00489: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.0345 - acc: 1.0000 - val_loss: 0.3450 - val_acc: 0.8500\n",
            "Epoch 490/500\n",
            "\n",
            "Epoch 00490: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.0364 - acc: 1.0000 - val_loss: 0.3425 - val_acc: 0.8500\n",
            "Epoch 491/500\n",
            "\n",
            "Epoch 00491: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.0437 - acc: 1.0000 - val_loss: 0.3371 - val_acc: 0.9000\n",
            "Epoch 492/500\n",
            "\n",
            "Epoch 00492: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.0388 - acc: 1.0000 - val_loss: 0.3592 - val_acc: 0.9000\n",
            "Epoch 493/500\n",
            "\n",
            "Epoch 00493: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.0732 - acc: 0.9833 - val_loss: 0.3673 - val_acc: 0.8500\n",
            "Epoch 494/500\n",
            "\n",
            "Epoch 00494: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.0488 - acc: 0.9833 - val_loss: 0.3883 - val_acc: 0.8500\n",
            "Epoch 495/500\n",
            "\n",
            "Epoch 00495: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.0226 - acc: 1.0000 - val_loss: 0.3892 - val_acc: 0.8500\n",
            "Epoch 496/500\n",
            "\n",
            "Epoch 00496: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.0460 - acc: 1.0000 - val_loss: 0.3943 - val_acc: 0.8500\n",
            "Epoch 497/500\n",
            "\n",
            "Epoch 00497: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.0421 - acc: 1.0000 - val_loss: 0.3854 - val_acc: 0.8000\n",
            "Epoch 498/500\n",
            "\n",
            "Epoch 00498: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.0754 - acc: 0.9833 - val_loss: 0.3810 - val_acc: 0.8000\n",
            "Epoch 499/500\n",
            "\n",
            "Epoch 00499: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.0476 - acc: 1.0000 - val_loss: 0.3386 - val_acc: 0.8000\n",
            "Epoch 500/500\n",
            "\n",
            "Epoch 00500: val_loss did not improve from 0.31876\n",
            "60/60 - 0s - loss: 0.0678 - acc: 0.9667 - val_loss: 0.3502 - val_acc: 0.8000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "# Evaluate on test data\n",
            "40/40 [==============================] - 0s 2ms/sample - loss: 0.9866 - acc: 0.6500\n",
            "test loss, test acc: [0.986615450532031, 0.65]\n",
            "EEG_Deep/Data2A/parsed_P06T.mat\n",
            "Filtering of Training Data Finished\n",
            "EEG_Deep/Data2A/parsed_P06E.mat\n",
            "Filtering of Testing Data Finished\n",
            "(1, 2)\n",
            "Finding labels in training data\n",
            "Finding labels in testing data\n",
            "[1 1 2 1 1 2 2 2 2 1 1 2 1 1 2 1 2 1 2 2]\n",
            "(60, 12, 1536)\n",
            "(60,)\n",
            "(20, 12, 1536)\n",
            "(20,)\n",
            "[[1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]]\n",
            "X_train shape: (60, 1, 12, 1536)\n",
            "60 train samples\n",
            "20 val samples\n",
            "(40, 12, 1536)\n",
            "(40, 1)\n",
            "X_train shape: (40, 1, 12, 1536)\n",
            "40 train samples\n",
            "Train on 60 samples, validate on 20 samples\n",
            "Epoch 1/500\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.69141, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 1s - loss: 0.6852 - acc: 0.5833 - val_loss: 0.6914 - val_acc: 0.5000\n",
            "Epoch 2/500\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.69141 to 0.69099, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6748 - acc: 0.5833 - val_loss: 0.6910 - val_acc: 0.5000\n",
            "Epoch 3/500\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.69099\n",
            "60/60 - 0s - loss: 0.6470 - acc: 0.7000 - val_loss: 0.6911 - val_acc: 0.5000\n",
            "Epoch 4/500\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.69099 to 0.69056, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6180 - acc: 0.7833 - val_loss: 0.6906 - val_acc: 0.5000\n",
            "Epoch 5/500\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.69056 to 0.69027, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6436 - acc: 0.6833 - val_loss: 0.6903 - val_acc: 0.5000\n",
            "Epoch 6/500\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.69027 to 0.68951, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5986 - acc: 0.8667 - val_loss: 0.6895 - val_acc: 0.5000\n",
            "Epoch 7/500\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.68951 to 0.68878, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5858 - acc: 0.8333 - val_loss: 0.6888 - val_acc: 0.5500\n",
            "Epoch 8/500\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.68878 to 0.68736, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5776 - acc: 0.8833 - val_loss: 0.6874 - val_acc: 0.5000\n",
            "Epoch 9/500\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.68736 to 0.68558, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5611 - acc: 0.8500 - val_loss: 0.6856 - val_acc: 0.5500\n",
            "Epoch 10/500\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.68558 to 0.68309, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5514 - acc: 0.8667 - val_loss: 0.6831 - val_acc: 0.5500\n",
            "Epoch 11/500\n",
            "\n",
            "Epoch 00011: val_loss improved from 0.68309 to 0.68044, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5281 - acc: 0.9167 - val_loss: 0.6804 - val_acc: 0.5500\n",
            "Epoch 12/500\n",
            "\n",
            "Epoch 00012: val_loss improved from 0.68044 to 0.67867, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5313 - acc: 0.9167 - val_loss: 0.6787 - val_acc: 0.6500\n",
            "Epoch 13/500\n",
            "\n",
            "Epoch 00013: val_loss improved from 0.67867 to 0.67642, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5333 - acc: 0.8167 - val_loss: 0.6764 - val_acc: 0.6500\n",
            "Epoch 14/500\n",
            "\n",
            "Epoch 00014: val_loss improved from 0.67642 to 0.67379, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5027 - acc: 0.8667 - val_loss: 0.6738 - val_acc: 0.6000\n",
            "Epoch 15/500\n",
            "\n",
            "Epoch 00015: val_loss improved from 0.67379 to 0.67034, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5172 - acc: 0.8500 - val_loss: 0.6703 - val_acc: 0.6000\n",
            "Epoch 16/500\n",
            "\n",
            "Epoch 00016: val_loss improved from 0.67034 to 0.66802, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4800 - acc: 0.9167 - val_loss: 0.6680 - val_acc: 0.6000\n",
            "Epoch 17/500\n",
            "\n",
            "Epoch 00017: val_loss improved from 0.66802 to 0.66589, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4883 - acc: 0.8667 - val_loss: 0.6659 - val_acc: 0.6000\n",
            "Epoch 18/500\n",
            "\n",
            "Epoch 00018: val_loss improved from 0.66589 to 0.66441, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4714 - acc: 0.9333 - val_loss: 0.6644 - val_acc: 0.6000\n",
            "Epoch 19/500\n",
            "\n",
            "Epoch 00019: val_loss improved from 0.66441 to 0.66105, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4480 - acc: 0.9167 - val_loss: 0.6610 - val_acc: 0.6000\n",
            "Epoch 20/500\n",
            "\n",
            "Epoch 00020: val_loss improved from 0.66105 to 0.65696, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4456 - acc: 0.9500 - val_loss: 0.6570 - val_acc: 0.6500\n",
            "Epoch 21/500\n",
            "\n",
            "Epoch 00021: val_loss improved from 0.65696 to 0.65300, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4458 - acc: 0.9167 - val_loss: 0.6530 - val_acc: 0.6500\n",
            "Epoch 22/500\n",
            "\n",
            "Epoch 00022: val_loss improved from 0.65300 to 0.64874, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4422 - acc: 0.9000 - val_loss: 0.6487 - val_acc: 0.6500\n",
            "Epoch 23/500\n",
            "\n",
            "Epoch 00023: val_loss improved from 0.64874 to 0.64464, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4308 - acc: 0.9667 - val_loss: 0.6446 - val_acc: 0.6500\n",
            "Epoch 24/500\n",
            "\n",
            "Epoch 00024: val_loss improved from 0.64464 to 0.64152, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4163 - acc: 0.9500 - val_loss: 0.6415 - val_acc: 0.6500\n",
            "Epoch 25/500\n",
            "\n",
            "Epoch 00025: val_loss improved from 0.64152 to 0.63788, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4332 - acc: 0.9333 - val_loss: 0.6379 - val_acc: 0.6500\n",
            "Epoch 26/500\n",
            "\n",
            "Epoch 00026: val_loss improved from 0.63788 to 0.63259, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4634 - acc: 0.8500 - val_loss: 0.6326 - val_acc: 0.7000\n",
            "Epoch 27/500\n",
            "\n",
            "Epoch 00027: val_loss improved from 0.63259 to 0.62690, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4146 - acc: 0.9333 - val_loss: 0.6269 - val_acc: 0.6500\n",
            "Epoch 28/500\n",
            "\n",
            "Epoch 00028: val_loss improved from 0.62690 to 0.62010, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4060 - acc: 0.9167 - val_loss: 0.6201 - val_acc: 0.6500\n",
            "Epoch 29/500\n",
            "\n",
            "Epoch 00029: val_loss improved from 0.62010 to 0.61469, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3768 - acc: 0.9833 - val_loss: 0.6147 - val_acc: 0.6500\n",
            "Epoch 30/500\n",
            "\n",
            "Epoch 00030: val_loss improved from 0.61469 to 0.61244, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3700 - acc: 0.9500 - val_loss: 0.6124 - val_acc: 0.7000\n",
            "Epoch 31/500\n",
            "\n",
            "Epoch 00031: val_loss improved from 0.61244 to 0.60983, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3952 - acc: 0.9333 - val_loss: 0.6098 - val_acc: 0.7000\n",
            "Epoch 32/500\n",
            "\n",
            "Epoch 00032: val_loss improved from 0.60983 to 0.60502, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3658 - acc: 0.9333 - val_loss: 0.6050 - val_acc: 0.7000\n",
            "Epoch 33/500\n",
            "\n",
            "Epoch 00033: val_loss improved from 0.60502 to 0.60296, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3436 - acc: 0.9667 - val_loss: 0.6030 - val_acc: 0.7000\n",
            "Epoch 34/500\n",
            "\n",
            "Epoch 00034: val_loss improved from 0.60296 to 0.60040, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3347 - acc: 0.9833 - val_loss: 0.6004 - val_acc: 0.7000\n",
            "Epoch 35/500\n",
            "\n",
            "Epoch 00035: val_loss improved from 0.60040 to 0.58985, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3939 - acc: 0.9333 - val_loss: 0.5898 - val_acc: 0.7000\n",
            "Epoch 36/500\n",
            "\n",
            "Epoch 00036: val_loss improved from 0.58985 to 0.57762, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3170 - acc: 0.9500 - val_loss: 0.5776 - val_acc: 0.7000\n",
            "Epoch 37/500\n",
            "\n",
            "Epoch 00037: val_loss improved from 0.57762 to 0.56579, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3064 - acc: 0.9833 - val_loss: 0.5658 - val_acc: 0.7000\n",
            "Epoch 38/500\n",
            "\n",
            "Epoch 00038: val_loss improved from 0.56579 to 0.55826, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2944 - acc: 0.9500 - val_loss: 0.5583 - val_acc: 0.7500\n",
            "Epoch 39/500\n",
            "\n",
            "Epoch 00039: val_loss improved from 0.55826 to 0.54690, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2415 - acc: 0.9833 - val_loss: 0.5469 - val_acc: 0.7500\n",
            "Epoch 40/500\n",
            "\n",
            "Epoch 00040: val_loss improved from 0.54690 to 0.54505, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2898 - acc: 0.9500 - val_loss: 0.5451 - val_acc: 0.7500\n",
            "Epoch 41/500\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.54505\n",
            "60/60 - 0s - loss: 0.2870 - acc: 0.9667 - val_loss: 0.5522 - val_acc: 0.7500\n",
            "Epoch 42/500\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.54505\n",
            "60/60 - 0s - loss: 0.2534 - acc: 0.9667 - val_loss: 0.5503 - val_acc: 0.7500\n",
            "Epoch 43/500\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.54505\n",
            "60/60 - 0s - loss: 0.2429 - acc: 0.9500 - val_loss: 0.5515 - val_acc: 0.7500\n",
            "Epoch 44/500\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.54505\n",
            "60/60 - 0s - loss: 0.2123 - acc: 1.0000 - val_loss: 0.5550 - val_acc: 0.7500\n",
            "Epoch 45/500\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.54505\n",
            "60/60 - 0s - loss: 0.2431 - acc: 0.9667 - val_loss: 0.5585 - val_acc: 0.7500\n",
            "Epoch 46/500\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.54505\n",
            "60/60 - 0s - loss: 0.2016 - acc: 1.0000 - val_loss: 0.5648 - val_acc: 0.7500\n",
            "Epoch 47/500\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.54505\n",
            "60/60 - 0s - loss: 0.2380 - acc: 0.9667 - val_loss: 0.5769 - val_acc: 0.7500\n",
            "Epoch 48/500\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.54505\n",
            "60/60 - 0s - loss: 0.2091 - acc: 0.9500 - val_loss: 0.5772 - val_acc: 0.7500\n",
            "Epoch 49/500\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.54505\n",
            "60/60 - 0s - loss: 0.2587 - acc: 0.9167 - val_loss: 0.5588 - val_acc: 0.7500\n",
            "Epoch 50/500\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.54505\n",
            "60/60 - 0s - loss: 0.1906 - acc: 0.9667 - val_loss: 0.5471 - val_acc: 0.7500\n",
            "Epoch 51/500\n",
            "\n",
            "Epoch 00051: val_loss improved from 0.54505 to 0.53934, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1859 - acc: 0.9667 - val_loss: 0.5393 - val_acc: 0.7500\n",
            "Epoch 52/500\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.53934\n",
            "60/60 - 0s - loss: 0.1848 - acc: 0.9833 - val_loss: 0.5411 - val_acc: 0.7500\n",
            "Epoch 53/500\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.53934\n",
            "60/60 - 0s - loss: 0.1825 - acc: 1.0000 - val_loss: 0.5462 - val_acc: 0.7500\n",
            "Epoch 54/500\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.53934\n",
            "60/60 - 0s - loss: 0.1563 - acc: 0.9667 - val_loss: 0.5440 - val_acc: 0.8000\n",
            "Epoch 55/500\n",
            "\n",
            "Epoch 00055: val_loss improved from 0.53934 to 0.53777, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1586 - acc: 0.9667 - val_loss: 0.5378 - val_acc: 0.8000\n",
            "Epoch 56/500\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.53777\n",
            "60/60 - 0s - loss: 0.1731 - acc: 1.0000 - val_loss: 0.5384 - val_acc: 0.8000\n",
            "Epoch 57/500\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.53777\n",
            "60/60 - 0s - loss: 0.1713 - acc: 0.9667 - val_loss: 0.5429 - val_acc: 0.8000\n",
            "Epoch 58/500\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.53777\n",
            "60/60 - 0s - loss: 0.1491 - acc: 0.9833 - val_loss: 0.5601 - val_acc: 0.8000\n",
            "Epoch 59/500\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.53777\n",
            "60/60 - 0s - loss: 0.1512 - acc: 0.9833 - val_loss: 0.5764 - val_acc: 0.8000\n",
            "Epoch 60/500\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.53777\n",
            "60/60 - 0s - loss: 0.1478 - acc: 0.9833 - val_loss: 0.5951 - val_acc: 0.7500\n",
            "Epoch 61/500\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.53777\n",
            "60/60 - 0s - loss: 0.1579 - acc: 1.0000 - val_loss: 0.6012 - val_acc: 0.7500\n",
            "Epoch 62/500\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.53777\n",
            "60/60 - 0s - loss: 0.1834 - acc: 0.9333 - val_loss: 0.5983 - val_acc: 0.7500\n",
            "Epoch 63/500\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.53777\n",
            "60/60 - 0s - loss: 0.1359 - acc: 0.9833 - val_loss: 0.5896 - val_acc: 0.8000\n",
            "Epoch 64/500\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.53777\n",
            "60/60 - 0s - loss: 0.1437 - acc: 0.9833 - val_loss: 0.5710 - val_acc: 0.8000\n",
            "Epoch 65/500\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.53777\n",
            "60/60 - 0s - loss: 0.1573 - acc: 0.9333 - val_loss: 0.5450 - val_acc: 0.8000\n",
            "Epoch 66/500\n",
            "\n",
            "Epoch 00066: val_loss improved from 0.53777 to 0.53137, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1632 - acc: 0.9667 - val_loss: 0.5314 - val_acc: 0.8000\n",
            "Epoch 67/500\n",
            "\n",
            "Epoch 00067: val_loss improved from 0.53137 to 0.52399, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1523 - acc: 0.9667 - val_loss: 0.5240 - val_acc: 0.8000\n",
            "Epoch 68/500\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.52399\n",
            "60/60 - 0s - loss: 0.1257 - acc: 0.9667 - val_loss: 0.5242 - val_acc: 0.8000\n",
            "Epoch 69/500\n",
            "\n",
            "Epoch 00069: val_loss improved from 0.52399 to 0.51775, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.0975 - acc: 1.0000 - val_loss: 0.5178 - val_acc: 0.8000\n",
            "Epoch 70/500\n",
            "\n",
            "Epoch 00070: val_loss improved from 0.51775 to 0.50938, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1210 - acc: 0.9667 - val_loss: 0.5094 - val_acc: 0.8000\n",
            "Epoch 71/500\n",
            "\n",
            "Epoch 00071: val_loss improved from 0.50938 to 0.50076, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.0973 - acc: 1.0000 - val_loss: 0.5008 - val_acc: 0.8000\n",
            "Epoch 72/500\n",
            "\n",
            "Epoch 00072: val_loss improved from 0.50076 to 0.49138, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1301 - acc: 0.9667 - val_loss: 0.4914 - val_acc: 0.8000\n",
            "Epoch 73/500\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.49138\n",
            "60/60 - 0s - loss: 0.1480 - acc: 0.9333 - val_loss: 0.5005 - val_acc: 0.8000\n",
            "Epoch 74/500\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.49138\n",
            "60/60 - 0s - loss: 0.1383 - acc: 0.9667 - val_loss: 0.4984 - val_acc: 0.8000\n",
            "Epoch 75/500\n",
            "\n",
            "Epoch 00075: val_loss improved from 0.49138 to 0.48765, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1203 - acc: 0.9833 - val_loss: 0.4877 - val_acc: 0.8000\n",
            "Epoch 76/500\n",
            "\n",
            "Epoch 00076: val_loss improved from 0.48765 to 0.47999, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.0946 - acc: 1.0000 - val_loss: 0.4800 - val_acc: 0.8000\n",
            "Epoch 77/500\n",
            "\n",
            "Epoch 00077: val_loss improved from 0.47999 to 0.47437, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1088 - acc: 0.9833 - val_loss: 0.4744 - val_acc: 0.8000\n",
            "Epoch 78/500\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.47437\n",
            "60/60 - 0s - loss: 0.0915 - acc: 1.0000 - val_loss: 0.4783 - val_acc: 0.8000\n",
            "Epoch 79/500\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.47437\n",
            "60/60 - 0s - loss: 0.1212 - acc: 0.9833 - val_loss: 0.4804 - val_acc: 0.8000\n",
            "Epoch 80/500\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.47437\n",
            "60/60 - 0s - loss: 0.1314 - acc: 0.9500 - val_loss: 0.4811 - val_acc: 0.8000\n",
            "Epoch 81/500\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.47437\n",
            "60/60 - 0s - loss: 0.1268 - acc: 0.9500 - val_loss: 0.4820 - val_acc: 0.8000\n",
            "Epoch 82/500\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.47437\n",
            "60/60 - 0s - loss: 0.1437 - acc: 0.9500 - val_loss: 0.4835 - val_acc: 0.8000\n",
            "Epoch 83/500\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.47437\n",
            "60/60 - 0s - loss: 0.1320 - acc: 0.9833 - val_loss: 0.4811 - val_acc: 0.8000\n",
            "Epoch 84/500\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.47437\n",
            "60/60 - 0s - loss: 0.2766 - acc: 0.8833 - val_loss: 0.4873 - val_acc: 0.8000\n",
            "Epoch 85/500\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.47437\n",
            "60/60 - 0s - loss: 0.0924 - acc: 1.0000 - val_loss: 0.4807 - val_acc: 0.8000\n",
            "Epoch 86/500\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.47437\n",
            "60/60 - 0s - loss: 0.1098 - acc: 0.9833 - val_loss: 0.4884 - val_acc: 0.8000\n",
            "Epoch 87/500\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.47437\n",
            "60/60 - 0s - loss: 0.0982 - acc: 1.0000 - val_loss: 0.4975 - val_acc: 0.8000\n",
            "Epoch 88/500\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.47437\n",
            "60/60 - 0s - loss: 0.0869 - acc: 1.0000 - val_loss: 0.5065 - val_acc: 0.8000\n",
            "Epoch 89/500\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.47437\n",
            "60/60 - 0s - loss: 0.1037 - acc: 1.0000 - val_loss: 0.5144 - val_acc: 0.8000\n",
            "Epoch 90/500\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.47437\n",
            "60/60 - 0s - loss: 0.0924 - acc: 1.0000 - val_loss: 0.5153 - val_acc: 0.8000\n",
            "Epoch 91/500\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.47437\n",
            "60/60 - 0s - loss: 0.1000 - acc: 0.9833 - val_loss: 0.5236 - val_acc: 0.8000\n",
            "Epoch 92/500\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.47437\n",
            "60/60 - 0s - loss: 0.1076 - acc: 0.9833 - val_loss: 0.5226 - val_acc: 0.8000\n",
            "Epoch 93/500\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.47437\n",
            "60/60 - 0s - loss: 0.0964 - acc: 1.0000 - val_loss: 0.5081 - val_acc: 0.8000\n",
            "Epoch 94/500\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.47437\n",
            "60/60 - 0s - loss: 0.0743 - acc: 1.0000 - val_loss: 0.4990 - val_acc: 0.8000\n",
            "Epoch 95/500\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.47437\n",
            "60/60 - 0s - loss: 0.0767 - acc: 1.0000 - val_loss: 0.4880 - val_acc: 0.8000\n",
            "Epoch 96/500\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.47437\n",
            "60/60 - 0s - loss: 0.0760 - acc: 1.0000 - val_loss: 0.4762 - val_acc: 0.8000\n",
            "Epoch 97/500\n",
            "\n",
            "Epoch 00097: val_loss improved from 0.47437 to 0.46962, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.0755 - acc: 1.0000 - val_loss: 0.4696 - val_acc: 0.8500\n",
            "Epoch 98/500\n",
            "\n",
            "Epoch 00098: val_loss improved from 0.46962 to 0.46177, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.0678 - acc: 1.0000 - val_loss: 0.4618 - val_acc: 0.8500\n",
            "Epoch 99/500\n",
            "\n",
            "Epoch 00099: val_loss improved from 0.46177 to 0.45609, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.0870 - acc: 0.9833 - val_loss: 0.4561 - val_acc: 0.8500\n",
            "Epoch 100/500\n",
            "\n",
            "Epoch 00100: val_loss improved from 0.45609 to 0.45544, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.0922 - acc: 0.9833 - val_loss: 0.4554 - val_acc: 0.8500\n",
            "Epoch 101/500\n",
            "\n",
            "Epoch 00101: val_loss improved from 0.45544 to 0.45260, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1142 - acc: 0.9833 - val_loss: 0.4526 - val_acc: 0.8500\n",
            "Epoch 102/500\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.45260\n",
            "60/60 - 0s - loss: 0.1236 - acc: 0.9667 - val_loss: 0.4568 - val_acc: 0.8500\n",
            "Epoch 103/500\n",
            "\n",
            "Epoch 00103: val_loss improved from 0.45260 to 0.45026, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.0917 - acc: 0.9833 - val_loss: 0.4503 - val_acc: 0.8500\n",
            "Epoch 104/500\n",
            "\n",
            "Epoch 00104: val_loss improved from 0.45026 to 0.42814, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1216 - acc: 0.9833 - val_loss: 0.4281 - val_acc: 0.8500\n",
            "Epoch 105/500\n",
            "\n",
            "Epoch 00105: val_loss improved from 0.42814 to 0.41311, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.0823 - acc: 1.0000 - val_loss: 0.4131 - val_acc: 0.8500\n",
            "Epoch 106/500\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 0.41311\n",
            "60/60 - 0s - loss: 0.1158 - acc: 0.9667 - val_loss: 0.4161 - val_acc: 0.8500\n",
            "Epoch 107/500\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.41311\n",
            "60/60 - 0s - loss: 0.1016 - acc: 1.0000 - val_loss: 0.4245 - val_acc: 0.8500\n",
            "Epoch 108/500\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 0.41311\n",
            "60/60 - 0s - loss: 0.0638 - acc: 1.0000 - val_loss: 0.4292 - val_acc: 0.8500\n",
            "Epoch 109/500\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.41311\n",
            "60/60 - 0s - loss: 0.1204 - acc: 0.9667 - val_loss: 0.4330 - val_acc: 0.8500\n",
            "Epoch 110/500\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.41311\n",
            "60/60 - 0s - loss: 0.1490 - acc: 0.9167 - val_loss: 0.4469 - val_acc: 0.8500\n",
            "Epoch 111/500\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 0.41311\n",
            "60/60 - 0s - loss: 0.0965 - acc: 1.0000 - val_loss: 0.4711 - val_acc: 0.8500\n",
            "Epoch 112/500\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 0.41311\n",
            "60/60 - 0s - loss: 0.0988 - acc: 0.9833 - val_loss: 0.4688 - val_acc: 0.8500\n",
            "Epoch 113/500\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.41311\n",
            "60/60 - 0s - loss: 0.0842 - acc: 1.0000 - val_loss: 0.4683 - val_acc: 0.8500\n",
            "Epoch 114/500\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.41311\n",
            "60/60 - 0s - loss: 0.0900 - acc: 1.0000 - val_loss: 0.4690 - val_acc: 0.8500\n",
            "Epoch 115/500\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.41311\n",
            "60/60 - 0s - loss: 0.1179 - acc: 0.9500 - val_loss: 0.4761 - val_acc: 0.8500\n",
            "Epoch 116/500\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.41311\n",
            "60/60 - 0s - loss: 0.0997 - acc: 0.9833 - val_loss: 0.4837 - val_acc: 0.8500\n",
            "Epoch 117/500\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.41311\n",
            "60/60 - 0s - loss: 0.1082 - acc: 0.9833 - val_loss: 0.4809 - val_acc: 0.8500\n",
            "Epoch 118/500\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.41311\n",
            "60/60 - 0s - loss: 0.1006 - acc: 0.9833 - val_loss: 0.4716 - val_acc: 0.8500\n",
            "Epoch 119/500\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 0.41311\n",
            "60/60 - 0s - loss: 0.0922 - acc: 0.9833 - val_loss: 0.4588 - val_acc: 0.8500\n",
            "Epoch 120/500\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.41311\n",
            "60/60 - 0s - loss: 0.0753 - acc: 1.0000 - val_loss: 0.4449 - val_acc: 0.8500\n",
            "Epoch 121/500\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.41311\n",
            "60/60 - 0s - loss: 0.0961 - acc: 1.0000 - val_loss: 0.4379 - val_acc: 0.8500\n",
            "Epoch 122/500\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.41311\n",
            "60/60 - 0s - loss: 0.0692 - acc: 1.0000 - val_loss: 0.4369 - val_acc: 0.8500\n",
            "Epoch 123/500\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 0.41311\n",
            "60/60 - 0s - loss: 0.0947 - acc: 1.0000 - val_loss: 0.4342 - val_acc: 0.8500\n",
            "Epoch 124/500\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 0.41311\n",
            "60/60 - 0s - loss: 0.0896 - acc: 0.9833 - val_loss: 0.4317 - val_acc: 0.8500\n",
            "Epoch 125/500\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 0.41311\n",
            "60/60 - 0s - loss: 0.0796 - acc: 0.9833 - val_loss: 0.4321 - val_acc: 0.8500\n",
            "Epoch 126/500\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 0.41311\n",
            "60/60 - 0s - loss: 0.0724 - acc: 0.9667 - val_loss: 0.4198 - val_acc: 0.8500\n",
            "Epoch 127/500\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 0.41311\n",
            "60/60 - 0s - loss: 0.0688 - acc: 0.9833 - val_loss: 0.4150 - val_acc: 0.8500\n",
            "Epoch 128/500\n",
            "\n",
            "Epoch 00128: val_loss improved from 0.41311 to 0.41088, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.0745 - acc: 0.9833 - val_loss: 0.4109 - val_acc: 0.8500\n",
            "Epoch 129/500\n",
            "\n",
            "Epoch 00129: val_loss improved from 0.41088 to 0.40984, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.0597 - acc: 1.0000 - val_loss: 0.4098 - val_acc: 0.8500\n",
            "Epoch 130/500\n",
            "\n",
            "Epoch 00130: val_loss improved from 0.40984 to 0.40922, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.0654 - acc: 1.0000 - val_loss: 0.4092 - val_acc: 0.8500\n",
            "Epoch 131/500\n",
            "\n",
            "Epoch 00131: val_loss improved from 0.40922 to 0.40713, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.0746 - acc: 1.0000 - val_loss: 0.4071 - val_acc: 0.8500\n",
            "Epoch 132/500\n",
            "\n",
            "Epoch 00132: val_loss improved from 0.40713 to 0.39781, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.0981 - acc: 0.9833 - val_loss: 0.3978 - val_acc: 0.8500\n",
            "Epoch 133/500\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.39781\n",
            "60/60 - 0s - loss: 0.0684 - acc: 1.0000 - val_loss: 0.3987 - val_acc: 0.8500\n",
            "Epoch 134/500\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.39781\n",
            "60/60 - 0s - loss: 0.0515 - acc: 1.0000 - val_loss: 0.4053 - val_acc: 0.8500\n",
            "Epoch 135/500\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 0.39781\n",
            "60/60 - 0s - loss: 0.1141 - acc: 0.9500 - val_loss: 0.4227 - val_acc: 0.8500\n",
            "Epoch 136/500\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.39781\n",
            "60/60 - 0s - loss: 0.0719 - acc: 1.0000 - val_loss: 0.4378 - val_acc: 0.8500\n",
            "Epoch 137/500\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 0.39781\n",
            "60/60 - 0s - loss: 0.0575 - acc: 1.0000 - val_loss: 0.4465 - val_acc: 0.8500\n",
            "Epoch 138/500\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.39781\n",
            "60/60 - 0s - loss: 0.0642 - acc: 0.9833 - val_loss: 0.4433 - val_acc: 0.8500\n",
            "Epoch 139/500\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.39781\n",
            "60/60 - 0s - loss: 0.0751 - acc: 0.9833 - val_loss: 0.4393 - val_acc: 0.8500\n",
            "Epoch 140/500\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 0.39781\n",
            "60/60 - 0s - loss: 0.0844 - acc: 0.9833 - val_loss: 0.4313 - val_acc: 0.8500\n",
            "Epoch 141/500\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.39781\n",
            "60/60 - 0s - loss: 0.0892 - acc: 0.9833 - val_loss: 0.4254 - val_acc: 0.8500\n",
            "Epoch 142/500\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.39781\n",
            "60/60 - 0s - loss: 0.0739 - acc: 0.9833 - val_loss: 0.4186 - val_acc: 0.8500\n",
            "Epoch 143/500\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 0.39781\n",
            "60/60 - 0s - loss: 0.0890 - acc: 0.9667 - val_loss: 0.3992 - val_acc: 0.8500\n",
            "Epoch 144/500\n",
            "\n",
            "Epoch 00144: val_loss improved from 0.39781 to 0.38195, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1767 - acc: 0.9167 - val_loss: 0.3820 - val_acc: 0.8500\n",
            "Epoch 145/500\n",
            "\n",
            "Epoch 00145: val_loss improved from 0.38195 to 0.36635, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.0831 - acc: 0.9833 - val_loss: 0.3664 - val_acc: 0.8500\n",
            "Epoch 146/500\n",
            "\n",
            "Epoch 00146: val_loss improved from 0.36635 to 0.36048, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.0730 - acc: 1.0000 - val_loss: 0.3605 - val_acc: 0.8500\n",
            "Epoch 147/500\n",
            "\n",
            "Epoch 00147: val_loss improved from 0.36048 to 0.35450, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.0969 - acc: 0.9667 - val_loss: 0.3545 - val_acc: 0.8500\n",
            "Epoch 148/500\n",
            "\n",
            "Epoch 00148: val_loss improved from 0.35450 to 0.35302, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.0761 - acc: 1.0000 - val_loss: 0.3530 - val_acc: 0.8500\n",
            "Epoch 149/500\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.35302\n",
            "60/60 - 0s - loss: 0.1106 - acc: 0.9667 - val_loss: 0.3551 - val_acc: 0.8500\n",
            "Epoch 150/500\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.35302\n",
            "60/60 - 0s - loss: 0.0922 - acc: 0.9833 - val_loss: 0.3645 - val_acc: 0.8500\n",
            "Epoch 151/500\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.35302\n",
            "60/60 - 0s - loss: 0.1239 - acc: 0.9833 - val_loss: 0.3717 - val_acc: 0.8500\n",
            "Epoch 152/500\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.35302\n",
            "60/60 - 0s - loss: 0.0594 - acc: 1.0000 - val_loss: 0.3788 - val_acc: 0.8500\n",
            "Epoch 153/500\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 0.35302\n",
            "60/60 - 0s - loss: 0.0812 - acc: 1.0000 - val_loss: 0.3855 - val_acc: 0.8500\n",
            "Epoch 154/500\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.35302\n",
            "60/60 - 0s - loss: 0.0784 - acc: 0.9833 - val_loss: 0.3936 - val_acc: 0.8500\n",
            "Epoch 155/500\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.35302\n",
            "60/60 - 0s - loss: 0.0801 - acc: 1.0000 - val_loss: 0.3928 - val_acc: 0.8500\n",
            "Epoch 156/500\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.35302\n",
            "60/60 - 0s - loss: 0.0614 - acc: 0.9833 - val_loss: 0.3861 - val_acc: 0.8500\n",
            "Epoch 157/500\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.35302\n",
            "60/60 - 0s - loss: 0.0604 - acc: 0.9833 - val_loss: 0.3766 - val_acc: 0.8500\n",
            "Epoch 158/500\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.35302\n",
            "60/60 - 0s - loss: 0.1096 - acc: 0.9833 - val_loss: 0.3766 - val_acc: 0.8500\n",
            "Epoch 159/500\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.35302\n",
            "60/60 - 0s - loss: 0.0623 - acc: 1.0000 - val_loss: 0.3730 - val_acc: 0.8500\n",
            "Epoch 160/500\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.35302\n",
            "60/60 - 0s - loss: 0.0457 - acc: 1.0000 - val_loss: 0.3725 - val_acc: 0.8500\n",
            "Epoch 161/500\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.35302\n",
            "60/60 - 0s - loss: 0.0868 - acc: 1.0000 - val_loss: 0.3773 - val_acc: 0.8500\n",
            "Epoch 162/500\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.35302\n",
            "60/60 - 0s - loss: 0.0740 - acc: 0.9833 - val_loss: 0.3903 - val_acc: 0.8500\n",
            "Epoch 163/500\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.35302\n",
            "60/60 - 0s - loss: 0.1156 - acc: 0.9500 - val_loss: 0.3996 - val_acc: 0.8500\n",
            "Epoch 164/500\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.35302\n",
            "60/60 - 0s - loss: 0.0652 - acc: 1.0000 - val_loss: 0.4012 - val_acc: 0.8500\n",
            "Epoch 165/500\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.35302\n",
            "60/60 - 0s - loss: 0.0587 - acc: 1.0000 - val_loss: 0.4010 - val_acc: 0.8500\n",
            "Epoch 166/500\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 0.35302\n",
            "60/60 - 0s - loss: 0.0704 - acc: 0.9833 - val_loss: 0.4021 - val_acc: 0.8500\n",
            "Epoch 167/500\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.35302\n",
            "60/60 - 0s - loss: 0.0745 - acc: 1.0000 - val_loss: 0.4011 - val_acc: 0.8500\n",
            "Epoch 168/500\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.35302\n",
            "60/60 - 0s - loss: 0.0447 - acc: 1.0000 - val_loss: 0.3989 - val_acc: 0.8500\n",
            "Epoch 169/500\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 0.35302\n",
            "60/60 - 0s - loss: 0.0845 - acc: 0.9667 - val_loss: 0.3904 - val_acc: 0.8500\n",
            "Epoch 170/500\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.35302\n",
            "60/60 - 0s - loss: 0.1021 - acc: 0.9833 - val_loss: 0.3792 - val_acc: 0.8500\n",
            "Epoch 171/500\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.35302\n",
            "60/60 - 0s - loss: 0.1031 - acc: 0.9667 - val_loss: 0.3641 - val_acc: 0.8500\n",
            "Epoch 172/500\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.35302\n",
            "60/60 - 0s - loss: 0.0652 - acc: 0.9833 - val_loss: 0.3533 - val_acc: 0.8500\n",
            "Epoch 173/500\n",
            "\n",
            "Epoch 00173: val_loss improved from 0.35302 to 0.34770, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.0679 - acc: 0.9833 - val_loss: 0.3477 - val_acc: 0.8500\n",
            "Epoch 174/500\n",
            "\n",
            "Epoch 00174: val_loss improved from 0.34770 to 0.34238, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.0657 - acc: 1.0000 - val_loss: 0.3424 - val_acc: 0.8500\n",
            "Epoch 175/500\n",
            "\n",
            "Epoch 00175: val_loss improved from 0.34238 to 0.34033, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.0714 - acc: 1.0000 - val_loss: 0.3403 - val_acc: 0.8500\n",
            "Epoch 176/500\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 0.34033\n",
            "60/60 - 0s - loss: 0.0670 - acc: 1.0000 - val_loss: 0.3441 - val_acc: 0.8500\n",
            "Epoch 177/500\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.34033\n",
            "60/60 - 0s - loss: 0.0462 - acc: 1.0000 - val_loss: 0.3543 - val_acc: 0.8500\n",
            "Epoch 178/500\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.34033\n",
            "60/60 - 0s - loss: 0.0979 - acc: 0.9667 - val_loss: 0.3658 - val_acc: 0.8500\n",
            "Epoch 179/500\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.34033\n",
            "60/60 - 0s - loss: 0.0707 - acc: 1.0000 - val_loss: 0.3817 - val_acc: 0.8500\n",
            "Epoch 180/500\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.34033\n",
            "60/60 - 0s - loss: 0.0713 - acc: 1.0000 - val_loss: 0.3853 - val_acc: 0.8500\n",
            "Epoch 181/500\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.34033\n",
            "60/60 - 0s - loss: 0.0852 - acc: 0.9833 - val_loss: 0.3948 - val_acc: 0.8500\n",
            "Epoch 182/500\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.34033\n",
            "60/60 - 0s - loss: 0.0619 - acc: 1.0000 - val_loss: 0.4004 - val_acc: 0.8500\n",
            "Epoch 183/500\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.34033\n",
            "60/60 - 0s - loss: 0.0604 - acc: 1.0000 - val_loss: 0.4040 - val_acc: 0.8500\n",
            "Epoch 184/500\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 0.34033\n",
            "60/60 - 0s - loss: 0.0721 - acc: 0.9833 - val_loss: 0.4041 - val_acc: 0.8500\n",
            "Epoch 185/500\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.34033\n",
            "60/60 - 0s - loss: 0.0904 - acc: 0.9833 - val_loss: 0.4063 - val_acc: 0.8500\n",
            "Epoch 186/500\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.34033\n",
            "60/60 - 0s - loss: 0.0781 - acc: 0.9833 - val_loss: 0.4016 - val_acc: 0.8500\n",
            "Epoch 187/500\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 0.34033\n",
            "60/60 - 0s - loss: 0.0673 - acc: 1.0000 - val_loss: 0.3991 - val_acc: 0.8500\n",
            "Epoch 188/500\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.34033\n",
            "60/60 - 0s - loss: 0.0462 - acc: 1.0000 - val_loss: 0.4020 - val_acc: 0.8500\n",
            "Epoch 189/500\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.34033\n",
            "60/60 - 0s - loss: 0.0474 - acc: 1.0000 - val_loss: 0.4041 - val_acc: 0.8500\n",
            "Epoch 190/500\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.34033\n",
            "60/60 - 0s - loss: 0.0427 - acc: 1.0000 - val_loss: 0.4074 - val_acc: 0.8500\n",
            "Epoch 191/500\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.34033\n",
            "60/60 - 0s - loss: 0.0629 - acc: 0.9833 - val_loss: 0.4043 - val_acc: 0.8500\n",
            "Epoch 192/500\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 0.34033\n",
            "60/60 - 0s - loss: 0.1198 - acc: 0.9333 - val_loss: 0.4129 - val_acc: 0.8500\n",
            "Epoch 193/500\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.34033\n",
            "60/60 - 0s - loss: 0.0460 - acc: 1.0000 - val_loss: 0.4161 - val_acc: 0.8500\n",
            "Epoch 194/500\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.34033\n",
            "60/60 - 0s - loss: 0.0448 - acc: 1.0000 - val_loss: 0.4165 - val_acc: 0.8500\n",
            "Epoch 195/500\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.34033\n",
            "60/60 - 0s - loss: 0.1041 - acc: 0.9500 - val_loss: 0.4087 - val_acc: 0.8500\n",
            "Epoch 196/500\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.34033\n",
            "60/60 - 0s - loss: 0.0550 - acc: 0.9833 - val_loss: 0.4163 - val_acc: 0.8500\n",
            "Epoch 197/500\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.34033\n",
            "60/60 - 0s - loss: 0.0920 - acc: 1.0000 - val_loss: 0.4258 - val_acc: 0.8500\n",
            "Epoch 198/500\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.34033\n",
            "60/60 - 0s - loss: 0.0578 - acc: 1.0000 - val_loss: 0.4332 - val_acc: 0.8500\n",
            "Epoch 199/500\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.34033\n",
            "60/60 - 0s - loss: 0.0548 - acc: 1.0000 - val_loss: 0.4433 - val_acc: 0.8500\n",
            "Epoch 200/500\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.34033\n",
            "60/60 - 0s - loss: 0.0519 - acc: 1.0000 - val_loss: 0.4525 - val_acc: 0.8500\n",
            "Epoch 201/500\n",
            "\n",
            "Epoch 00201: val_loss did not improve from 0.34033\n",
            "60/60 - 0s - loss: 0.0592 - acc: 0.9833 - val_loss: 0.4634 - val_acc: 0.8500\n",
            "Epoch 202/500\n",
            "\n",
            "Epoch 00202: val_loss did not improve from 0.34033\n",
            "60/60 - 0s - loss: 0.0518 - acc: 0.9833 - val_loss: 0.4561 - val_acc: 0.8500\n",
            "Epoch 203/500\n",
            "\n",
            "Epoch 00203: val_loss did not improve from 0.34033\n",
            "60/60 - 0s - loss: 0.0532 - acc: 1.0000 - val_loss: 0.4429 - val_acc: 0.8500\n",
            "Epoch 204/500\n",
            "\n",
            "Epoch 00204: val_loss did not improve from 0.34033\n",
            "60/60 - 0s - loss: 0.0727 - acc: 1.0000 - val_loss: 0.4320 - val_acc: 0.8500\n",
            "Epoch 205/500\n",
            "\n",
            "Epoch 00205: val_loss did not improve from 0.34033\n",
            "60/60 - 0s - loss: 0.0592 - acc: 1.0000 - val_loss: 0.4209 - val_acc: 0.8500\n",
            "Epoch 206/500\n",
            "\n",
            "Epoch 00206: val_loss did not improve from 0.34033\n",
            "60/60 - 0s - loss: 0.0466 - acc: 1.0000 - val_loss: 0.4051 - val_acc: 0.8500\n",
            "Epoch 207/500\n",
            "\n",
            "Epoch 00207: val_loss did not improve from 0.34033\n",
            "60/60 - 0s - loss: 0.0552 - acc: 1.0000 - val_loss: 0.3916 - val_acc: 0.8500\n",
            "Epoch 208/500\n",
            "\n",
            "Epoch 00208: val_loss did not improve from 0.34033\n",
            "60/60 - 0s - loss: 0.0394 - acc: 1.0000 - val_loss: 0.3828 - val_acc: 0.8500\n",
            "Epoch 209/500\n",
            "\n",
            "Epoch 00209: val_loss did not improve from 0.34033\n",
            "60/60 - 0s - loss: 0.0598 - acc: 0.9833 - val_loss: 0.3739 - val_acc: 0.8500\n",
            "Epoch 210/500\n",
            "\n",
            "Epoch 00210: val_loss did not improve from 0.34033\n",
            "60/60 - 0s - loss: 0.0596 - acc: 0.9833 - val_loss: 0.3740 - val_acc: 0.8500\n",
            "Epoch 211/500\n",
            "\n",
            "Epoch 00211: val_loss did not improve from 0.34033\n",
            "60/60 - 0s - loss: 0.0551 - acc: 0.9833 - val_loss: 0.3791 - val_acc: 0.8500\n",
            "Epoch 212/500\n",
            "\n",
            "Epoch 00212: val_loss did not improve from 0.34033\n",
            "60/60 - 0s - loss: 0.0520 - acc: 1.0000 - val_loss: 0.3919 - val_acc: 0.8500\n",
            "Epoch 213/500\n",
            "\n",
            "Epoch 00213: val_loss did not improve from 0.34033\n",
            "60/60 - 0s - loss: 0.0521 - acc: 1.0000 - val_loss: 0.4042 - val_acc: 0.8500\n",
            "Epoch 214/500\n",
            "\n",
            "Epoch 00214: val_loss did not improve from 0.34033\n",
            "60/60 - 0s - loss: 0.0677 - acc: 0.9833 - val_loss: 0.4008 - val_acc: 0.8500\n",
            "Epoch 215/500\n",
            "\n",
            "Epoch 00215: val_loss did not improve from 0.34033\n",
            "60/60 - 0s - loss: 0.0569 - acc: 0.9833 - val_loss: 0.3888 - val_acc: 0.8500\n",
            "Epoch 216/500\n",
            "\n",
            "Epoch 00216: val_loss did not improve from 0.34033\n",
            "60/60 - 0s - loss: 0.0814 - acc: 0.9833 - val_loss: 0.3881 - val_acc: 0.8500\n",
            "Epoch 217/500\n",
            "\n",
            "Epoch 00217: val_loss did not improve from 0.34033\n",
            "60/60 - 0s - loss: 0.0678 - acc: 0.9833 - val_loss: 0.3878 - val_acc: 0.8500\n",
            "Epoch 218/500\n",
            "\n",
            "Epoch 00218: val_loss did not improve from 0.34033\n",
            "60/60 - 0s - loss: 0.0513 - acc: 0.9833 - val_loss: 0.3869 - val_acc: 0.8500\n",
            "Epoch 219/500\n",
            "\n",
            "Epoch 00219: val_loss did not improve from 0.34033\n",
            "60/60 - 0s - loss: 0.0565 - acc: 1.0000 - val_loss: 0.3804 - val_acc: 0.8500\n",
            "Epoch 220/500\n",
            "\n",
            "Epoch 00220: val_loss did not improve from 0.34033\n",
            "60/60 - 0s - loss: 0.0336 - acc: 1.0000 - val_loss: 0.3832 - val_acc: 0.8500\n",
            "Epoch 221/500\n",
            "\n",
            "Epoch 00221: val_loss did not improve from 0.34033\n",
            "60/60 - 0s - loss: 0.0863 - acc: 1.0000 - val_loss: 0.3854 - val_acc: 0.8500\n",
            "Epoch 222/500\n",
            "\n",
            "Epoch 00222: val_loss did not improve from 0.34033\n",
            "60/60 - 0s - loss: 0.0410 - acc: 1.0000 - val_loss: 0.3904 - val_acc: 0.8500\n",
            "Epoch 223/500\n",
            "\n",
            "Epoch 00223: val_loss did not improve from 0.34033\n",
            "60/60 - 0s - loss: 0.0889 - acc: 0.9833 - val_loss: 0.3947 - val_acc: 0.8500\n",
            "Epoch 224/500\n",
            "\n",
            "Epoch 00224: val_loss did not improve from 0.34033\n",
            "60/60 - 0s - loss: 0.0435 - acc: 1.0000 - val_loss: 0.3954 - val_acc: 0.8500\n",
            "Epoch 225/500\n",
            "\n",
            "Epoch 00225: val_loss did not improve from 0.34033\n",
            "60/60 - 0s - loss: 0.0665 - acc: 0.9667 - val_loss: 0.3976 - val_acc: 0.8500\n",
            "Epoch 226/500\n",
            "\n",
            "Epoch 00226: val_loss did not improve from 0.34033\n",
            "60/60 - 0s - loss: 0.0694 - acc: 0.9667 - val_loss: 0.3968 - val_acc: 0.8500\n",
            "Epoch 227/500\n",
            "\n",
            "Epoch 00227: val_loss did not improve from 0.34033\n",
            "60/60 - 0s - loss: 0.0564 - acc: 0.9833 - val_loss: 0.3968 - val_acc: 0.8500\n",
            "Epoch 228/500\n",
            "\n",
            "Epoch 00228: val_loss did not improve from 0.34033\n",
            "60/60 - 0s - loss: 0.0976 - acc: 0.9833 - val_loss: 0.3963 - val_acc: 0.8500\n",
            "Epoch 229/500\n",
            "\n",
            "Epoch 00229: val_loss did not improve from 0.34033\n",
            "60/60 - 0s - loss: 0.0425 - acc: 1.0000 - val_loss: 0.3957 - val_acc: 0.8500\n",
            "Epoch 230/500\n",
            "\n",
            "Epoch 00230: val_loss did not improve from 0.34033\n",
            "60/60 - 0s - loss: 0.0822 - acc: 0.9833 - val_loss: 0.3977 - val_acc: 0.8500\n",
            "Epoch 231/500\n",
            "\n",
            "Epoch 00231: val_loss did not improve from 0.34033\n",
            "60/60 - 0s - loss: 0.0706 - acc: 1.0000 - val_loss: 0.3958 - val_acc: 0.8500\n",
            "Epoch 232/500\n",
            "\n",
            "Epoch 00232: val_loss did not improve from 0.34033\n",
            "60/60 - 0s - loss: 0.0619 - acc: 1.0000 - val_loss: 0.3897 - val_acc: 0.8500\n",
            "Epoch 233/500\n",
            "\n",
            "Epoch 00233: val_loss did not improve from 0.34033\n",
            "60/60 - 0s - loss: 0.0576 - acc: 1.0000 - val_loss: 0.3881 - val_acc: 0.8500\n",
            "Epoch 234/500\n",
            "\n",
            "Epoch 00234: val_loss did not improve from 0.34033\n",
            "60/60 - 0s - loss: 0.0649 - acc: 0.9833 - val_loss: 0.3883 - val_acc: 0.8500\n",
            "Epoch 235/500\n",
            "\n",
            "Epoch 00235: val_loss did not improve from 0.34033\n",
            "60/60 - 0s - loss: 0.0392 - acc: 1.0000 - val_loss: 0.3801 - val_acc: 0.8500\n",
            "Epoch 236/500\n",
            "\n",
            "Epoch 00236: val_loss did not improve from 0.34033\n",
            "60/60 - 0s - loss: 0.0506 - acc: 0.9833 - val_loss: 0.3714 - val_acc: 0.8500\n",
            "Epoch 237/500\n",
            "\n",
            "Epoch 00237: val_loss did not improve from 0.34033\n",
            "60/60 - 0s - loss: 0.0487 - acc: 0.9833 - val_loss: 0.3714 - val_acc: 0.8500\n",
            "Epoch 238/500\n",
            "\n",
            "Epoch 00238: val_loss did not improve from 0.34033\n",
            "60/60 - 0s - loss: 0.0396 - acc: 1.0000 - val_loss: 0.3791 - val_acc: 0.9000\n",
            "Epoch 239/500\n",
            "\n",
            "Epoch 00239: val_loss did not improve from 0.34033\n",
            "60/60 - 0s - loss: 0.0861 - acc: 1.0000 - val_loss: 0.3790 - val_acc: 0.8500\n",
            "Epoch 240/500\n",
            "\n",
            "Epoch 00240: val_loss did not improve from 0.34033\n",
            "60/60 - 0s - loss: 0.0522 - acc: 1.0000 - val_loss: 0.3808 - val_acc: 0.8500\n",
            "Epoch 241/500\n",
            "\n",
            "Epoch 00241: val_loss did not improve from 0.34033\n",
            "60/60 - 0s - loss: 0.0638 - acc: 1.0000 - val_loss: 0.3779 - val_acc: 0.8500\n",
            "Epoch 242/500\n",
            "\n",
            "Epoch 00242: val_loss did not improve from 0.34033\n",
            "60/60 - 0s - loss: 0.0662 - acc: 0.9833 - val_loss: 0.3770 - val_acc: 0.8500\n",
            "Epoch 243/500\n",
            "\n",
            "Epoch 00243: val_loss did not improve from 0.34033\n",
            "60/60 - 0s - loss: 0.0400 - acc: 1.0000 - val_loss: 0.3792 - val_acc: 0.8500\n",
            "Epoch 244/500\n",
            "\n",
            "Epoch 00244: val_loss did not improve from 0.34033\n",
            "60/60 - 0s - loss: 0.0842 - acc: 0.9833 - val_loss: 0.3756 - val_acc: 0.8500\n",
            "Epoch 245/500\n",
            "\n",
            "Epoch 00245: val_loss did not improve from 0.34033\n",
            "60/60 - 0s - loss: 0.0591 - acc: 1.0000 - val_loss: 0.3616 - val_acc: 0.8500\n",
            "Epoch 246/500\n",
            "\n",
            "Epoch 00246: val_loss did not improve from 0.34033\n",
            "60/60 - 0s - loss: 0.0553 - acc: 1.0000 - val_loss: 0.3532 - val_acc: 0.8500\n",
            "Epoch 247/500\n",
            "\n",
            "Epoch 00247: val_loss did not improve from 0.34033\n",
            "60/60 - 0s - loss: 0.1059 - acc: 0.9667 - val_loss: 0.3508 - val_acc: 0.8500\n",
            "Epoch 248/500\n",
            "\n",
            "Epoch 00248: val_loss did not improve from 0.34033\n",
            "60/60 - 0s - loss: 0.0586 - acc: 1.0000 - val_loss: 0.3427 - val_acc: 0.8500\n",
            "Epoch 249/500\n",
            "\n",
            "Epoch 00249: val_loss improved from 0.34033 to 0.33967, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.0468 - acc: 1.0000 - val_loss: 0.3397 - val_acc: 0.9000\n",
            "Epoch 250/500\n",
            "\n",
            "Epoch 00250: val_loss improved from 0.33967 to 0.33660, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.0618 - acc: 1.0000 - val_loss: 0.3366 - val_acc: 0.9000\n",
            "Epoch 251/500\n",
            "\n",
            "Epoch 00251: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0860 - acc: 0.9667 - val_loss: 0.3393 - val_acc: 0.9000\n",
            "Epoch 252/500\n",
            "\n",
            "Epoch 00252: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0655 - acc: 1.0000 - val_loss: 0.3625 - val_acc: 0.9000\n",
            "Epoch 253/500\n",
            "\n",
            "Epoch 00253: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0409 - acc: 1.0000 - val_loss: 0.3718 - val_acc: 0.9000\n",
            "Epoch 254/500\n",
            "\n",
            "Epoch 00254: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0496 - acc: 1.0000 - val_loss: 0.3605 - val_acc: 0.8500\n",
            "Epoch 255/500\n",
            "\n",
            "Epoch 00255: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0594 - acc: 1.0000 - val_loss: 0.3528 - val_acc: 0.8500\n",
            "Epoch 256/500\n",
            "\n",
            "Epoch 00256: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0545 - acc: 1.0000 - val_loss: 0.3453 - val_acc: 0.8500\n",
            "Epoch 257/500\n",
            "\n",
            "Epoch 00257: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0776 - acc: 0.9833 - val_loss: 0.3428 - val_acc: 0.8500\n",
            "Epoch 258/500\n",
            "\n",
            "Epoch 00258: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0464 - acc: 1.0000 - val_loss: 0.3518 - val_acc: 0.8500\n",
            "Epoch 259/500\n",
            "\n",
            "Epoch 00259: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0344 - acc: 1.0000 - val_loss: 0.3547 - val_acc: 0.8500\n",
            "Epoch 260/500\n",
            "\n",
            "Epoch 00260: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0454 - acc: 1.0000 - val_loss: 0.3601 - val_acc: 0.8500\n",
            "Epoch 261/500\n",
            "\n",
            "Epoch 00261: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0354 - acc: 1.0000 - val_loss: 0.3653 - val_acc: 0.8500\n",
            "Epoch 262/500\n",
            "\n",
            "Epoch 00262: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0263 - acc: 1.0000 - val_loss: 0.3681 - val_acc: 0.8500\n",
            "Epoch 263/500\n",
            "\n",
            "Epoch 00263: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0685 - acc: 0.9833 - val_loss: 0.3606 - val_acc: 0.8500\n",
            "Epoch 264/500\n",
            "\n",
            "Epoch 00264: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0439 - acc: 1.0000 - val_loss: 0.3543 - val_acc: 0.8500\n",
            "Epoch 265/500\n",
            "\n",
            "Epoch 00265: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0414 - acc: 1.0000 - val_loss: 0.3552 - val_acc: 0.8500\n",
            "Epoch 266/500\n",
            "\n",
            "Epoch 00266: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0407 - acc: 1.0000 - val_loss: 0.3560 - val_acc: 0.8500\n",
            "Epoch 267/500\n",
            "\n",
            "Epoch 00267: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0340 - acc: 1.0000 - val_loss: 0.3593 - val_acc: 0.8500\n",
            "Epoch 268/500\n",
            "\n",
            "Epoch 00268: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0311 - acc: 1.0000 - val_loss: 0.3636 - val_acc: 0.8500\n",
            "Epoch 269/500\n",
            "\n",
            "Epoch 00269: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0353 - acc: 1.0000 - val_loss: 0.3705 - val_acc: 0.8500\n",
            "Epoch 270/500\n",
            "\n",
            "Epoch 00270: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0693 - acc: 0.9833 - val_loss: 0.3680 - val_acc: 0.8500\n",
            "Epoch 271/500\n",
            "\n",
            "Epoch 00271: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0349 - acc: 1.0000 - val_loss: 0.3599 - val_acc: 0.8500\n",
            "Epoch 272/500\n",
            "\n",
            "Epoch 00272: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0432 - acc: 1.0000 - val_loss: 0.3574 - val_acc: 0.8500\n",
            "Epoch 273/500\n",
            "\n",
            "Epoch 00273: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0594 - acc: 1.0000 - val_loss: 0.3647 - val_acc: 0.8500\n",
            "Epoch 274/500\n",
            "\n",
            "Epoch 00274: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0381 - acc: 0.9833 - val_loss: 0.3812 - val_acc: 0.8500\n",
            "Epoch 275/500\n",
            "\n",
            "Epoch 00275: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0337 - acc: 1.0000 - val_loss: 0.3885 - val_acc: 0.8500\n",
            "Epoch 276/500\n",
            "\n",
            "Epoch 00276: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0343 - acc: 1.0000 - val_loss: 0.3935 - val_acc: 0.8500\n",
            "Epoch 277/500\n",
            "\n",
            "Epoch 00277: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0402 - acc: 1.0000 - val_loss: 0.3949 - val_acc: 0.8500\n",
            "Epoch 278/500\n",
            "\n",
            "Epoch 00278: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0403 - acc: 1.0000 - val_loss: 0.3899 - val_acc: 0.8500\n",
            "Epoch 279/500\n",
            "\n",
            "Epoch 00279: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0403 - acc: 1.0000 - val_loss: 0.3857 - val_acc: 0.8500\n",
            "Epoch 280/500\n",
            "\n",
            "Epoch 00280: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0834 - acc: 0.9833 - val_loss: 0.3760 - val_acc: 0.8500\n",
            "Epoch 281/500\n",
            "\n",
            "Epoch 00281: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0677 - acc: 0.9833 - val_loss: 0.3648 - val_acc: 0.8500\n",
            "Epoch 282/500\n",
            "\n",
            "Epoch 00282: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0526 - acc: 0.9833 - val_loss: 0.3584 - val_acc: 0.8500\n",
            "Epoch 283/500\n",
            "\n",
            "Epoch 00283: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0438 - acc: 0.9833 - val_loss: 0.3735 - val_acc: 0.8500\n",
            "Epoch 284/500\n",
            "\n",
            "Epoch 00284: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0477 - acc: 1.0000 - val_loss: 0.3832 - val_acc: 0.8000\n",
            "Epoch 285/500\n",
            "\n",
            "Epoch 00285: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0598 - acc: 1.0000 - val_loss: 0.3872 - val_acc: 0.8000\n",
            "Epoch 286/500\n",
            "\n",
            "Epoch 00286: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0639 - acc: 0.9833 - val_loss: 0.3778 - val_acc: 0.8000\n",
            "Epoch 287/500\n",
            "\n",
            "Epoch 00287: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0502 - acc: 1.0000 - val_loss: 0.3767 - val_acc: 0.8000\n",
            "Epoch 288/500\n",
            "\n",
            "Epoch 00288: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0473 - acc: 1.0000 - val_loss: 0.3697 - val_acc: 0.8500\n",
            "Epoch 289/500\n",
            "\n",
            "Epoch 00289: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0412 - acc: 1.0000 - val_loss: 0.3663 - val_acc: 0.8500\n",
            "Epoch 290/500\n",
            "\n",
            "Epoch 00290: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0703 - acc: 0.9833 - val_loss: 0.3635 - val_acc: 0.8500\n",
            "Epoch 291/500\n",
            "\n",
            "Epoch 00291: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0497 - acc: 1.0000 - val_loss: 0.3611 - val_acc: 0.8500\n",
            "Epoch 292/500\n",
            "\n",
            "Epoch 00292: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0410 - acc: 1.0000 - val_loss: 0.3693 - val_acc: 0.8500\n",
            "Epoch 293/500\n",
            "\n",
            "Epoch 00293: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0381 - acc: 1.0000 - val_loss: 0.3774 - val_acc: 0.9000\n",
            "Epoch 294/500\n",
            "\n",
            "Epoch 00294: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0428 - acc: 0.9833 - val_loss: 0.3823 - val_acc: 0.8500\n",
            "Epoch 295/500\n",
            "\n",
            "Epoch 00295: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0413 - acc: 1.0000 - val_loss: 0.3844 - val_acc: 0.8500\n",
            "Epoch 296/500\n",
            "\n",
            "Epoch 00296: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0321 - acc: 1.0000 - val_loss: 0.3848 - val_acc: 0.9000\n",
            "Epoch 297/500\n",
            "\n",
            "Epoch 00297: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0306 - acc: 1.0000 - val_loss: 0.3828 - val_acc: 0.9000\n",
            "Epoch 298/500\n",
            "\n",
            "Epoch 00298: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0395 - acc: 1.0000 - val_loss: 0.3756 - val_acc: 0.8500\n",
            "Epoch 299/500\n",
            "\n",
            "Epoch 00299: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0470 - acc: 1.0000 - val_loss: 0.3692 - val_acc: 0.8500\n",
            "Epoch 300/500\n",
            "\n",
            "Epoch 00300: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0387 - acc: 1.0000 - val_loss: 0.3715 - val_acc: 0.8500\n",
            "Epoch 301/500\n",
            "\n",
            "Epoch 00301: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0442 - acc: 1.0000 - val_loss: 0.3747 - val_acc: 0.8500\n",
            "Epoch 302/500\n",
            "\n",
            "Epoch 00302: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0402 - acc: 1.0000 - val_loss: 0.3816 - val_acc: 0.8500\n",
            "Epoch 303/500\n",
            "\n",
            "Epoch 00303: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0308 - acc: 1.0000 - val_loss: 0.3948 - val_acc: 0.8500\n",
            "Epoch 304/500\n",
            "\n",
            "Epoch 00304: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0421 - acc: 1.0000 - val_loss: 0.4027 - val_acc: 0.8500\n",
            "Epoch 305/500\n",
            "\n",
            "Epoch 00305: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0749 - acc: 0.9833 - val_loss: 0.3918 - val_acc: 0.8500\n",
            "Epoch 306/500\n",
            "\n",
            "Epoch 00306: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0640 - acc: 0.9833 - val_loss: 0.3888 - val_acc: 0.8500\n",
            "Epoch 307/500\n",
            "\n",
            "Epoch 00307: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0276 - acc: 1.0000 - val_loss: 0.3847 - val_acc: 0.8500\n",
            "Epoch 308/500\n",
            "\n",
            "Epoch 00308: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0527 - acc: 1.0000 - val_loss: 0.3886 - val_acc: 0.8500\n",
            "Epoch 309/500\n",
            "\n",
            "Epoch 00309: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0467 - acc: 1.0000 - val_loss: 0.3854 - val_acc: 0.8500\n",
            "Epoch 310/500\n",
            "\n",
            "Epoch 00310: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0617 - acc: 0.9833 - val_loss: 0.3887 - val_acc: 0.8500\n",
            "Epoch 311/500\n",
            "\n",
            "Epoch 00311: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0284 - acc: 1.0000 - val_loss: 0.3893 - val_acc: 0.8500\n",
            "Epoch 312/500\n",
            "\n",
            "Epoch 00312: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0432 - acc: 1.0000 - val_loss: 0.3866 - val_acc: 0.8500\n",
            "Epoch 313/500\n",
            "\n",
            "Epoch 00313: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0287 - acc: 1.0000 - val_loss: 0.3863 - val_acc: 0.8500\n",
            "Epoch 314/500\n",
            "\n",
            "Epoch 00314: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0364 - acc: 1.0000 - val_loss: 0.3890 - val_acc: 0.8500\n",
            "Epoch 315/500\n",
            "\n",
            "Epoch 00315: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0346 - acc: 1.0000 - val_loss: 0.3905 - val_acc: 0.8500\n",
            "Epoch 316/500\n",
            "\n",
            "Epoch 00316: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0242 - acc: 1.0000 - val_loss: 0.3789 - val_acc: 0.8500\n",
            "Epoch 317/500\n",
            "\n",
            "Epoch 00317: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0467 - acc: 1.0000 - val_loss: 0.3788 - val_acc: 0.8500\n",
            "Epoch 318/500\n",
            "\n",
            "Epoch 00318: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0413 - acc: 1.0000 - val_loss: 0.3884 - val_acc: 0.8500\n",
            "Epoch 319/500\n",
            "\n",
            "Epoch 00319: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0349 - acc: 1.0000 - val_loss: 0.3928 - val_acc: 0.8500\n",
            "Epoch 320/500\n",
            "\n",
            "Epoch 00320: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0520 - acc: 0.9833 - val_loss: 0.3995 - val_acc: 0.8500\n",
            "Epoch 321/500\n",
            "\n",
            "Epoch 00321: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0516 - acc: 0.9833 - val_loss: 0.4215 - val_acc: 0.8500\n",
            "Epoch 322/500\n",
            "\n",
            "Epoch 00322: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0378 - acc: 1.0000 - val_loss: 0.4504 - val_acc: 0.8500\n",
            "Epoch 323/500\n",
            "\n",
            "Epoch 00323: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0565 - acc: 1.0000 - val_loss: 0.4661 - val_acc: 0.8500\n",
            "Epoch 324/500\n",
            "\n",
            "Epoch 00324: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0391 - acc: 1.0000 - val_loss: 0.4596 - val_acc: 0.8500\n",
            "Epoch 325/500\n",
            "\n",
            "Epoch 00325: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0509 - acc: 0.9833 - val_loss: 0.4342 - val_acc: 0.8500\n",
            "Epoch 326/500\n",
            "\n",
            "Epoch 00326: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0327 - acc: 1.0000 - val_loss: 0.4107 - val_acc: 0.8500\n",
            "Epoch 327/500\n",
            "\n",
            "Epoch 00327: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0513 - acc: 1.0000 - val_loss: 0.3935 - val_acc: 0.8500\n",
            "Epoch 328/500\n",
            "\n",
            "Epoch 00328: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0370 - acc: 1.0000 - val_loss: 0.3796 - val_acc: 0.8500\n",
            "Epoch 329/500\n",
            "\n",
            "Epoch 00329: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0287 - acc: 1.0000 - val_loss: 0.3763 - val_acc: 0.8500\n",
            "Epoch 330/500\n",
            "\n",
            "Epoch 00330: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0245 - acc: 1.0000 - val_loss: 0.3774 - val_acc: 0.8500\n",
            "Epoch 331/500\n",
            "\n",
            "Epoch 00331: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0194 - acc: 1.0000 - val_loss: 0.3717 - val_acc: 0.8500\n",
            "Epoch 332/500\n",
            "\n",
            "Epoch 00332: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0283 - acc: 1.0000 - val_loss: 0.3689 - val_acc: 0.8500\n",
            "Epoch 333/500\n",
            "\n",
            "Epoch 00333: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0441 - acc: 1.0000 - val_loss: 0.3718 - val_acc: 0.8500\n",
            "Epoch 334/500\n",
            "\n",
            "Epoch 00334: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0412 - acc: 1.0000 - val_loss: 0.3693 - val_acc: 0.8500\n",
            "Epoch 335/500\n",
            "\n",
            "Epoch 00335: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0563 - acc: 0.9833 - val_loss: 0.3768 - val_acc: 0.8500\n",
            "Epoch 336/500\n",
            "\n",
            "Epoch 00336: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0414 - acc: 1.0000 - val_loss: 0.3814 - val_acc: 0.8500\n",
            "Epoch 337/500\n",
            "\n",
            "Epoch 00337: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0439 - acc: 1.0000 - val_loss: 0.3870 - val_acc: 0.8500\n",
            "Epoch 338/500\n",
            "\n",
            "Epoch 00338: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0263 - acc: 1.0000 - val_loss: 0.3996 - val_acc: 0.8500\n",
            "Epoch 339/500\n",
            "\n",
            "Epoch 00339: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0423 - acc: 1.0000 - val_loss: 0.3975 - val_acc: 0.8500\n",
            "Epoch 340/500\n",
            "\n",
            "Epoch 00340: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0658 - acc: 0.9833 - val_loss: 0.3905 - val_acc: 0.8500\n",
            "Epoch 341/500\n",
            "\n",
            "Epoch 00341: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0510 - acc: 1.0000 - val_loss: 0.3873 - val_acc: 0.8500\n",
            "Epoch 342/500\n",
            "\n",
            "Epoch 00342: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0415 - acc: 1.0000 - val_loss: 0.3875 - val_acc: 0.8500\n",
            "Epoch 343/500\n",
            "\n",
            "Epoch 00343: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0299 - acc: 1.0000 - val_loss: 0.3870 - val_acc: 0.8500\n",
            "Epoch 344/500\n",
            "\n",
            "Epoch 00344: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0457 - acc: 1.0000 - val_loss: 0.3786 - val_acc: 0.8500\n",
            "Epoch 345/500\n",
            "\n",
            "Epoch 00345: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0326 - acc: 1.0000 - val_loss: 0.3742 - val_acc: 0.8500\n",
            "Epoch 346/500\n",
            "\n",
            "Epoch 00346: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0579 - acc: 1.0000 - val_loss: 0.3678 - val_acc: 0.8500\n",
            "Epoch 347/500\n",
            "\n",
            "Epoch 00347: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0490 - acc: 1.0000 - val_loss: 0.3822 - val_acc: 0.9000\n",
            "Epoch 348/500\n",
            "\n",
            "Epoch 00348: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0384 - acc: 1.0000 - val_loss: 0.3867 - val_acc: 0.9000\n",
            "Epoch 349/500\n",
            "\n",
            "Epoch 00349: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0526 - acc: 1.0000 - val_loss: 0.3872 - val_acc: 0.8500\n",
            "Epoch 350/500\n",
            "\n",
            "Epoch 00350: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0342 - acc: 1.0000 - val_loss: 0.3938 - val_acc: 0.8500\n",
            "Epoch 351/500\n",
            "\n",
            "Epoch 00351: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0366 - acc: 1.0000 - val_loss: 0.4074 - val_acc: 0.8500\n",
            "Epoch 352/500\n",
            "\n",
            "Epoch 00352: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0247 - acc: 1.0000 - val_loss: 0.4272 - val_acc: 0.8500\n",
            "Epoch 353/500\n",
            "\n",
            "Epoch 00353: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0238 - acc: 1.0000 - val_loss: 0.4427 - val_acc: 0.8500\n",
            "Epoch 354/500\n",
            "\n",
            "Epoch 00354: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0289 - acc: 1.0000 - val_loss: 0.4586 - val_acc: 0.8500\n",
            "Epoch 355/500\n",
            "\n",
            "Epoch 00355: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0294 - acc: 1.0000 - val_loss: 0.4659 - val_acc: 0.8500\n",
            "Epoch 356/500\n",
            "\n",
            "Epoch 00356: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0728 - acc: 0.9667 - val_loss: 0.4675 - val_acc: 0.8500\n",
            "Epoch 357/500\n",
            "\n",
            "Epoch 00357: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0260 - acc: 1.0000 - val_loss: 0.4605 - val_acc: 0.8500\n",
            "Epoch 358/500\n",
            "\n",
            "Epoch 00358: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0303 - acc: 1.0000 - val_loss: 0.4577 - val_acc: 0.8500\n",
            "Epoch 359/500\n",
            "\n",
            "Epoch 00359: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0582 - acc: 0.9833 - val_loss: 0.4608 - val_acc: 0.8500\n",
            "Epoch 360/500\n",
            "\n",
            "Epoch 00360: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0268 - acc: 1.0000 - val_loss: 0.4586 - val_acc: 0.8500\n",
            "Epoch 361/500\n",
            "\n",
            "Epoch 00361: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0337 - acc: 1.0000 - val_loss: 0.4477 - val_acc: 0.8500\n",
            "Epoch 362/500\n",
            "\n",
            "Epoch 00362: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0494 - acc: 1.0000 - val_loss: 0.4384 - val_acc: 0.8500\n",
            "Epoch 363/500\n",
            "\n",
            "Epoch 00363: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0412 - acc: 1.0000 - val_loss: 0.4285 - val_acc: 0.8500\n",
            "Epoch 364/500\n",
            "\n",
            "Epoch 00364: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0305 - acc: 1.0000 - val_loss: 0.4192 - val_acc: 0.8500\n",
            "Epoch 365/500\n",
            "\n",
            "Epoch 00365: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0430 - acc: 1.0000 - val_loss: 0.4117 - val_acc: 0.8500\n",
            "Epoch 366/500\n",
            "\n",
            "Epoch 00366: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0587 - acc: 0.9833 - val_loss: 0.4060 - val_acc: 0.8500\n",
            "Epoch 367/500\n",
            "\n",
            "Epoch 00367: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0617 - acc: 0.9833 - val_loss: 0.4103 - val_acc: 0.8500\n",
            "Epoch 368/500\n",
            "\n",
            "Epoch 00368: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0379 - acc: 1.0000 - val_loss: 0.4234 - val_acc: 0.8500\n",
            "Epoch 369/500\n",
            "\n",
            "Epoch 00369: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0262 - acc: 1.0000 - val_loss: 0.4303 - val_acc: 0.8500\n",
            "Epoch 370/500\n",
            "\n",
            "Epoch 00370: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0252 - acc: 1.0000 - val_loss: 0.4311 - val_acc: 0.8500\n",
            "Epoch 371/500\n",
            "\n",
            "Epoch 00371: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0275 - acc: 1.0000 - val_loss: 0.4347 - val_acc: 0.8500\n",
            "Epoch 372/500\n",
            "\n",
            "Epoch 00372: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0326 - acc: 1.0000 - val_loss: 0.4329 - val_acc: 0.8500\n",
            "Epoch 373/500\n",
            "\n",
            "Epoch 00373: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0240 - acc: 1.0000 - val_loss: 0.4321 - val_acc: 0.8500\n",
            "Epoch 374/500\n",
            "\n",
            "Epoch 00374: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0330 - acc: 1.0000 - val_loss: 0.4369 - val_acc: 0.8500\n",
            "Epoch 375/500\n",
            "\n",
            "Epoch 00375: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0215 - acc: 1.0000 - val_loss: 0.4377 - val_acc: 0.8500\n",
            "Epoch 376/500\n",
            "\n",
            "Epoch 00376: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0279 - acc: 1.0000 - val_loss: 0.4383 - val_acc: 0.8500\n",
            "Epoch 377/500\n",
            "\n",
            "Epoch 00377: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0264 - acc: 1.0000 - val_loss: 0.4338 - val_acc: 0.8500\n",
            "Epoch 378/500\n",
            "\n",
            "Epoch 00378: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0595 - acc: 0.9667 - val_loss: 0.4286 - val_acc: 0.8500\n",
            "Epoch 379/500\n",
            "\n",
            "Epoch 00379: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0302 - acc: 1.0000 - val_loss: 0.4266 - val_acc: 0.8500\n",
            "Epoch 380/500\n",
            "\n",
            "Epoch 00380: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0296 - acc: 1.0000 - val_loss: 0.4241 - val_acc: 0.8500\n",
            "Epoch 381/500\n",
            "\n",
            "Epoch 00381: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0439 - acc: 1.0000 - val_loss: 0.4126 - val_acc: 0.8500\n",
            "Epoch 382/500\n",
            "\n",
            "Epoch 00382: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0327 - acc: 1.0000 - val_loss: 0.3961 - val_acc: 0.8500\n",
            "Epoch 383/500\n",
            "\n",
            "Epoch 00383: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0318 - acc: 1.0000 - val_loss: 0.3832 - val_acc: 0.8500\n",
            "Epoch 384/500\n",
            "\n",
            "Epoch 00384: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0254 - acc: 1.0000 - val_loss: 0.3790 - val_acc: 0.8500\n",
            "Epoch 385/500\n",
            "\n",
            "Epoch 00385: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0350 - acc: 1.0000 - val_loss: 0.3787 - val_acc: 0.8500\n",
            "Epoch 386/500\n",
            "\n",
            "Epoch 00386: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0332 - acc: 1.0000 - val_loss: 0.3759 - val_acc: 0.8500\n",
            "Epoch 387/500\n",
            "\n",
            "Epoch 00387: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0334 - acc: 1.0000 - val_loss: 0.3721 - val_acc: 0.8500\n",
            "Epoch 388/500\n",
            "\n",
            "Epoch 00388: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0153 - acc: 1.0000 - val_loss: 0.3659 - val_acc: 0.8500\n",
            "Epoch 389/500\n",
            "\n",
            "Epoch 00389: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0205 - acc: 1.0000 - val_loss: 0.3595 - val_acc: 0.9000\n",
            "Epoch 390/500\n",
            "\n",
            "Epoch 00390: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0199 - acc: 1.0000 - val_loss: 0.3557 - val_acc: 0.9000\n",
            "Epoch 391/500\n",
            "\n",
            "Epoch 00391: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0380 - acc: 1.0000 - val_loss: 0.3584 - val_acc: 0.9000\n",
            "Epoch 392/500\n",
            "\n",
            "Epoch 00392: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0553 - acc: 0.9667 - val_loss: 0.3635 - val_acc: 0.8500\n",
            "Epoch 393/500\n",
            "\n",
            "Epoch 00393: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0201 - acc: 1.0000 - val_loss: 0.3611 - val_acc: 0.8500\n",
            "Epoch 394/500\n",
            "\n",
            "Epoch 00394: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0162 - acc: 1.0000 - val_loss: 0.3656 - val_acc: 0.8500\n",
            "Epoch 395/500\n",
            "\n",
            "Epoch 00395: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0287 - acc: 1.0000 - val_loss: 0.3733 - val_acc: 0.8500\n",
            "Epoch 396/500\n",
            "\n",
            "Epoch 00396: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0547 - acc: 1.0000 - val_loss: 0.3903 - val_acc: 0.8500\n",
            "Epoch 397/500\n",
            "\n",
            "Epoch 00397: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0273 - acc: 1.0000 - val_loss: 0.4198 - val_acc: 0.8500\n",
            "Epoch 398/500\n",
            "\n",
            "Epoch 00398: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0463 - acc: 0.9833 - val_loss: 0.4345 - val_acc: 0.8500\n",
            "Epoch 399/500\n",
            "\n",
            "Epoch 00399: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0288 - acc: 1.0000 - val_loss: 0.4335 - val_acc: 0.8500\n",
            "Epoch 400/500\n",
            "\n",
            "Epoch 00400: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0325 - acc: 1.0000 - val_loss: 0.4286 - val_acc: 0.8500\n",
            "Epoch 401/500\n",
            "\n",
            "Epoch 00401: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0262 - acc: 1.0000 - val_loss: 0.4177 - val_acc: 0.8500\n",
            "Epoch 402/500\n",
            "\n",
            "Epoch 00402: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0359 - acc: 1.0000 - val_loss: 0.4118 - val_acc: 0.8500\n",
            "Epoch 403/500\n",
            "\n",
            "Epoch 00403: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0426 - acc: 1.0000 - val_loss: 0.4120 - val_acc: 0.8500\n",
            "Epoch 404/500\n",
            "\n",
            "Epoch 00404: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0254 - acc: 1.0000 - val_loss: 0.4099 - val_acc: 0.8500\n",
            "Epoch 405/500\n",
            "\n",
            "Epoch 00405: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0206 - acc: 1.0000 - val_loss: 0.4078 - val_acc: 0.8500\n",
            "Epoch 406/500\n",
            "\n",
            "Epoch 00406: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0305 - acc: 1.0000 - val_loss: 0.4060 - val_acc: 0.8500\n",
            "Epoch 407/500\n",
            "\n",
            "Epoch 00407: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0238 - acc: 1.0000 - val_loss: 0.4049 - val_acc: 0.8500\n",
            "Epoch 408/500\n",
            "\n",
            "Epoch 00408: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0476 - acc: 1.0000 - val_loss: 0.4048 - val_acc: 0.8500\n",
            "Epoch 409/500\n",
            "\n",
            "Epoch 00409: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0270 - acc: 1.0000 - val_loss: 0.3930 - val_acc: 0.8500\n",
            "Epoch 410/500\n",
            "\n",
            "Epoch 00410: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0265 - acc: 1.0000 - val_loss: 0.3850 - val_acc: 0.9000\n",
            "Epoch 411/500\n",
            "\n",
            "Epoch 00411: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0721 - acc: 0.9667 - val_loss: 0.3586 - val_acc: 0.9000\n",
            "Epoch 412/500\n",
            "\n",
            "Epoch 00412: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0397 - acc: 1.0000 - val_loss: 0.3384 - val_acc: 0.9000\n",
            "Epoch 413/500\n",
            "\n",
            "Epoch 00413: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0702 - acc: 0.9833 - val_loss: 0.3473 - val_acc: 0.8500\n",
            "Epoch 414/500\n",
            "\n",
            "Epoch 00414: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0402 - acc: 1.0000 - val_loss: 0.3495 - val_acc: 0.9000\n",
            "Epoch 415/500\n",
            "\n",
            "Epoch 00415: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0300 - acc: 1.0000 - val_loss: 0.3546 - val_acc: 0.9000\n",
            "Epoch 416/500\n",
            "\n",
            "Epoch 00416: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0239 - acc: 1.0000 - val_loss: 0.3645 - val_acc: 0.9000\n",
            "Epoch 417/500\n",
            "\n",
            "Epoch 00417: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0905 - acc: 0.9667 - val_loss: 0.3953 - val_acc: 0.9000\n",
            "Epoch 418/500\n",
            "\n",
            "Epoch 00418: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0333 - acc: 1.0000 - val_loss: 0.4266 - val_acc: 0.9000\n",
            "Epoch 419/500\n",
            "\n",
            "Epoch 00419: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0752 - acc: 0.9833 - val_loss: 0.4324 - val_acc: 0.8500\n",
            "Epoch 420/500\n",
            "\n",
            "Epoch 00420: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0469 - acc: 1.0000 - val_loss: 0.4446 - val_acc: 0.8500\n",
            "Epoch 421/500\n",
            "\n",
            "Epoch 00421: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0499 - acc: 0.9833 - val_loss: 0.4442 - val_acc: 0.8500\n",
            "Epoch 422/500\n",
            "\n",
            "Epoch 00422: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0236 - acc: 1.0000 - val_loss: 0.4468 - val_acc: 0.8500\n",
            "Epoch 423/500\n",
            "\n",
            "Epoch 00423: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0555 - acc: 1.0000 - val_loss: 0.4441 - val_acc: 0.8500\n",
            "Epoch 424/500\n",
            "\n",
            "Epoch 00424: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0464 - acc: 0.9833 - val_loss: 0.4470 - val_acc: 0.8500\n",
            "Epoch 425/500\n",
            "\n",
            "Epoch 00425: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.1045 - acc: 0.9667 - val_loss: 0.4413 - val_acc: 0.8500\n",
            "Epoch 426/500\n",
            "\n",
            "Epoch 00426: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0286 - acc: 1.0000 - val_loss: 0.4360 - val_acc: 0.8500\n",
            "Epoch 427/500\n",
            "\n",
            "Epoch 00427: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0381 - acc: 0.9833 - val_loss: 0.4338 - val_acc: 0.8500\n",
            "Epoch 428/500\n",
            "\n",
            "Epoch 00428: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0374 - acc: 1.0000 - val_loss: 0.4322 - val_acc: 0.8500\n",
            "Epoch 429/500\n",
            "\n",
            "Epoch 00429: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0287 - acc: 1.0000 - val_loss: 0.4326 - val_acc: 0.8500\n",
            "Epoch 430/500\n",
            "\n",
            "Epoch 00430: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0501 - acc: 0.9833 - val_loss: 0.4368 - val_acc: 0.8500\n",
            "Epoch 431/500\n",
            "\n",
            "Epoch 00431: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0368 - acc: 0.9833 - val_loss: 0.4311 - val_acc: 0.8500\n",
            "Epoch 432/500\n",
            "\n",
            "Epoch 00432: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0361 - acc: 1.0000 - val_loss: 0.4292 - val_acc: 0.8500\n",
            "Epoch 433/500\n",
            "\n",
            "Epoch 00433: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0190 - acc: 1.0000 - val_loss: 0.4272 - val_acc: 0.8500\n",
            "Epoch 434/500\n",
            "\n",
            "Epoch 00434: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0268 - acc: 1.0000 - val_loss: 0.4337 - val_acc: 0.8500\n",
            "Epoch 435/500\n",
            "\n",
            "Epoch 00435: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0292 - acc: 1.0000 - val_loss: 0.4398 - val_acc: 0.8500\n",
            "Epoch 436/500\n",
            "\n",
            "Epoch 00436: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0474 - acc: 0.9833 - val_loss: 0.4715 - val_acc: 0.8500\n",
            "Epoch 437/500\n",
            "\n",
            "Epoch 00437: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0226 - acc: 1.0000 - val_loss: 0.4929 - val_acc: 0.8500\n",
            "Epoch 438/500\n",
            "\n",
            "Epoch 00438: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0190 - acc: 1.0000 - val_loss: 0.4965 - val_acc: 0.8500\n",
            "Epoch 439/500\n",
            "\n",
            "Epoch 00439: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0256 - acc: 1.0000 - val_loss: 0.4921 - val_acc: 0.8500\n",
            "Epoch 440/500\n",
            "\n",
            "Epoch 00440: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0295 - acc: 1.0000 - val_loss: 0.4711 - val_acc: 0.8500\n",
            "Epoch 441/500\n",
            "\n",
            "Epoch 00441: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0192 - acc: 1.0000 - val_loss: 0.4508 - val_acc: 0.9000\n",
            "Epoch 442/500\n",
            "\n",
            "Epoch 00442: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0256 - acc: 1.0000 - val_loss: 0.4401 - val_acc: 0.9000\n",
            "Epoch 443/500\n",
            "\n",
            "Epoch 00443: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0453 - acc: 0.9667 - val_loss: 0.4242 - val_acc: 0.9000\n",
            "Epoch 444/500\n",
            "\n",
            "Epoch 00444: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0371 - acc: 0.9833 - val_loss: 0.4117 - val_acc: 0.9000\n",
            "Epoch 445/500\n",
            "\n",
            "Epoch 00445: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0296 - acc: 1.0000 - val_loss: 0.4134 - val_acc: 0.9000\n",
            "Epoch 446/500\n",
            "\n",
            "Epoch 00446: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0252 - acc: 1.0000 - val_loss: 0.4159 - val_acc: 0.9000\n",
            "Epoch 447/500\n",
            "\n",
            "Epoch 00447: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0379 - acc: 1.0000 - val_loss: 0.4155 - val_acc: 0.9000\n",
            "Epoch 448/500\n",
            "\n",
            "Epoch 00448: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0198 - acc: 1.0000 - val_loss: 0.4078 - val_acc: 0.9000\n",
            "Epoch 449/500\n",
            "\n",
            "Epoch 00449: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0214 - acc: 1.0000 - val_loss: 0.3949 - val_acc: 0.9000\n",
            "Epoch 450/500\n",
            "\n",
            "Epoch 00450: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0451 - acc: 1.0000 - val_loss: 0.3784 - val_acc: 0.9000\n",
            "Epoch 451/500\n",
            "\n",
            "Epoch 00451: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0323 - acc: 1.0000 - val_loss: 0.3688 - val_acc: 0.9000\n",
            "Epoch 452/500\n",
            "\n",
            "Epoch 00452: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0352 - acc: 1.0000 - val_loss: 0.3609 - val_acc: 0.9000\n",
            "Epoch 453/500\n",
            "\n",
            "Epoch 00453: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0485 - acc: 0.9833 - val_loss: 0.3649 - val_acc: 0.9000\n",
            "Epoch 454/500\n",
            "\n",
            "Epoch 00454: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0242 - acc: 1.0000 - val_loss: 0.3856 - val_acc: 0.9000\n",
            "Epoch 455/500\n",
            "\n",
            "Epoch 00455: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0695 - acc: 0.9833 - val_loss: 0.3858 - val_acc: 0.8500\n",
            "Epoch 456/500\n",
            "\n",
            "Epoch 00456: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0372 - acc: 1.0000 - val_loss: 0.3744 - val_acc: 0.8000\n",
            "Epoch 457/500\n",
            "\n",
            "Epoch 00457: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0329 - acc: 1.0000 - val_loss: 0.3689 - val_acc: 0.8500\n",
            "Epoch 458/500\n",
            "\n",
            "Epoch 00458: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0343 - acc: 1.0000 - val_loss: 0.3793 - val_acc: 0.8500\n",
            "Epoch 459/500\n",
            "\n",
            "Epoch 00459: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0222 - acc: 1.0000 - val_loss: 0.3900 - val_acc: 0.8500\n",
            "Epoch 460/500\n",
            "\n",
            "Epoch 00460: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0213 - acc: 1.0000 - val_loss: 0.4021 - val_acc: 0.8500\n",
            "Epoch 461/500\n",
            "\n",
            "Epoch 00461: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0277 - acc: 0.9833 - val_loss: 0.4111 - val_acc: 0.8500\n",
            "Epoch 462/500\n",
            "\n",
            "Epoch 00462: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0191 - acc: 1.0000 - val_loss: 0.4162 - val_acc: 0.8500\n",
            "Epoch 463/500\n",
            "\n",
            "Epoch 00463: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0354 - acc: 1.0000 - val_loss: 0.4017 - val_acc: 0.8500\n",
            "Epoch 464/500\n",
            "\n",
            "Epoch 00464: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0270 - acc: 1.0000 - val_loss: 0.3877 - val_acc: 0.8500\n",
            "Epoch 465/500\n",
            "\n",
            "Epoch 00465: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0152 - acc: 1.0000 - val_loss: 0.3755 - val_acc: 0.9000\n",
            "Epoch 466/500\n",
            "\n",
            "Epoch 00466: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0300 - acc: 1.0000 - val_loss: 0.3718 - val_acc: 0.9000\n",
            "Epoch 467/500\n",
            "\n",
            "Epoch 00467: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0405 - acc: 0.9833 - val_loss: 0.3682 - val_acc: 0.9000\n",
            "Epoch 468/500\n",
            "\n",
            "Epoch 00468: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0228 - acc: 1.0000 - val_loss: 0.3620 - val_acc: 0.9000\n",
            "Epoch 469/500\n",
            "\n",
            "Epoch 00469: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0423 - acc: 1.0000 - val_loss: 0.3664 - val_acc: 0.9000\n",
            "Epoch 470/500\n",
            "\n",
            "Epoch 00470: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0379 - acc: 1.0000 - val_loss: 0.3746 - val_acc: 0.9000\n",
            "Epoch 471/500\n",
            "\n",
            "Epoch 00471: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0316 - acc: 1.0000 - val_loss: 0.3795 - val_acc: 0.8500\n",
            "Epoch 472/500\n",
            "\n",
            "Epoch 00472: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0165 - acc: 1.0000 - val_loss: 0.3852 - val_acc: 0.8500\n",
            "Epoch 473/500\n",
            "\n",
            "Epoch 00473: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0170 - acc: 1.0000 - val_loss: 0.3891 - val_acc: 0.8500\n",
            "Epoch 474/500\n",
            "\n",
            "Epoch 00474: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0184 - acc: 1.0000 - val_loss: 0.3945 - val_acc: 0.8500\n",
            "Epoch 475/500\n",
            "\n",
            "Epoch 00475: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0340 - acc: 1.0000 - val_loss: 0.4063 - val_acc: 0.8500\n",
            "Epoch 476/500\n",
            "\n",
            "Epoch 00476: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0359 - acc: 1.0000 - val_loss: 0.4248 - val_acc: 0.8500\n",
            "Epoch 477/500\n",
            "\n",
            "Epoch 00477: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0238 - acc: 1.0000 - val_loss: 0.4349 - val_acc: 0.8500\n",
            "Epoch 478/500\n",
            "\n",
            "Epoch 00478: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0127 - acc: 1.0000 - val_loss: 0.4353 - val_acc: 0.8500\n",
            "Epoch 479/500\n",
            "\n",
            "Epoch 00479: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0177 - acc: 1.0000 - val_loss: 0.4310 - val_acc: 0.8500\n",
            "Epoch 480/500\n",
            "\n",
            "Epoch 00480: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0260 - acc: 1.0000 - val_loss: 0.4283 - val_acc: 0.8500\n",
            "Epoch 481/500\n",
            "\n",
            "Epoch 00481: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0222 - acc: 1.0000 - val_loss: 0.4237 - val_acc: 0.8500\n",
            "Epoch 482/500\n",
            "\n",
            "Epoch 00482: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0525 - acc: 0.9833 - val_loss: 0.4368 - val_acc: 0.8500\n",
            "Epoch 483/500\n",
            "\n",
            "Epoch 00483: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0296 - acc: 1.0000 - val_loss: 0.4486 - val_acc: 0.8500\n",
            "Epoch 484/500\n",
            "\n",
            "Epoch 00484: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0165 - acc: 1.0000 - val_loss: 0.4616 - val_acc: 0.8500\n",
            "Epoch 485/500\n",
            "\n",
            "Epoch 00485: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0303 - acc: 1.0000 - val_loss: 0.4643 - val_acc: 0.8500\n",
            "Epoch 486/500\n",
            "\n",
            "Epoch 00486: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0152 - acc: 1.0000 - val_loss: 0.4678 - val_acc: 0.8500\n",
            "Epoch 487/500\n",
            "\n",
            "Epoch 00487: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0375 - acc: 0.9833 - val_loss: 0.4658 - val_acc: 0.8500\n",
            "Epoch 488/500\n",
            "\n",
            "Epoch 00488: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0318 - acc: 0.9833 - val_loss: 0.4596 - val_acc: 0.8500\n",
            "Epoch 489/500\n",
            "\n",
            "Epoch 00489: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0193 - acc: 1.0000 - val_loss: 0.4483 - val_acc: 0.8500\n",
            "Epoch 490/500\n",
            "\n",
            "Epoch 00490: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0147 - acc: 1.0000 - val_loss: 0.4329 - val_acc: 0.8500\n",
            "Epoch 491/500\n",
            "\n",
            "Epoch 00491: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0320 - acc: 1.0000 - val_loss: 0.4093 - val_acc: 0.8500\n",
            "Epoch 492/500\n",
            "\n",
            "Epoch 00492: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0507 - acc: 1.0000 - val_loss: 0.3930 - val_acc: 0.8500\n",
            "Epoch 493/500\n",
            "\n",
            "Epoch 00493: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0248 - acc: 1.0000 - val_loss: 0.3887 - val_acc: 0.8500\n",
            "Epoch 494/500\n",
            "\n",
            "Epoch 00494: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0404 - acc: 1.0000 - val_loss: 0.3998 - val_acc: 0.8500\n",
            "Epoch 495/500\n",
            "\n",
            "Epoch 00495: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0266 - acc: 1.0000 - val_loss: 0.4219 - val_acc: 0.9000\n",
            "Epoch 496/500\n",
            "\n",
            "Epoch 00496: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0375 - acc: 1.0000 - val_loss: 0.4391 - val_acc: 0.9000\n",
            "Epoch 497/500\n",
            "\n",
            "Epoch 00497: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0163 - acc: 1.0000 - val_loss: 0.4536 - val_acc: 0.8500\n",
            "Epoch 498/500\n",
            "\n",
            "Epoch 00498: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0347 - acc: 1.0000 - val_loss: 0.4702 - val_acc: 0.8500\n",
            "Epoch 499/500\n",
            "\n",
            "Epoch 00499: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0218 - acc: 1.0000 - val_loss: 0.4722 - val_acc: 0.8500\n",
            "Epoch 500/500\n",
            "\n",
            "Epoch 00500: val_loss did not improve from 0.33660\n",
            "60/60 - 0s - loss: 0.0334 - acc: 1.0000 - val_loss: 0.4863 - val_acc: 0.8500\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nc1QRKIqi+Jz/Dy3lXckvUguxAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "# Evaluate on test data\n",
            "40/40 [==============================] - 0s 2ms/sample - loss: 1.2655 - acc: 0.6750\n",
            "test loss, test acc: [1.265494425839006, 0.675]\n",
            "EEG_Deep/Data2A/parsed_P07T.mat\n",
            "Filtering of Training Data Finished\n",
            "EEG_Deep/Data2A/parsed_P07E.mat\n",
            "Filtering of Testing Data Finished\n",
            "(1, 2)\n",
            "Finding labels in training data\n",
            "Finding labels in testing data\n",
            "[1 1 1 1 1 1 1 2 1 2 1 1 1 1 2 2 1 2 2 2]\n",
            "(60, 12, 1536)\n",
            "(60,)\n",
            "(20, 12, 1536)\n",
            "(20,)\n",
            "[[1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]]\n",
            "X_train shape: (60, 1, 12, 1536)\n",
            "60 train samples\n",
            "20 val samples\n",
            "(40, 12, 1536)\n",
            "(40, 1)\n",
            "X_train shape: (40, 1, 12, 1536)\n",
            "40 train samples\n",
            "Train on 60 samples, validate on 20 samples\n",
            "Epoch 1/500\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.70050, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 1s - loss: 0.7005 - acc: 0.6000 - val_loss: 0.7005 - val_acc: 0.3500\n",
            "Epoch 2/500\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.70050\n",
            "60/60 - 0s - loss: 0.6670 - acc: 0.5667 - val_loss: 0.7017 - val_acc: 0.3500\n",
            "Epoch 3/500\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.70050\n",
            "60/60 - 0s - loss: 0.6585 - acc: 0.5667 - val_loss: 0.7027 - val_acc: 0.3500\n",
            "Epoch 4/500\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.70050\n",
            "60/60 - 0s - loss: 0.6498 - acc: 0.5667 - val_loss: 0.7026 - val_acc: 0.4000\n",
            "Epoch 5/500\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.70050\n",
            "60/60 - 0s - loss: 0.6268 - acc: 0.5667 - val_loss: 0.7020 - val_acc: 0.4000\n",
            "Epoch 6/500\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.70050\n",
            "60/60 - 0s - loss: 0.6179 - acc: 0.5833 - val_loss: 0.7013 - val_acc: 0.4500\n",
            "Epoch 7/500\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.70050 to 0.70042, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6150 - acc: 0.6500 - val_loss: 0.7004 - val_acc: 0.4500\n",
            "Epoch 8/500\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.70042 to 0.69952, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5924 - acc: 0.7167 - val_loss: 0.6995 - val_acc: 0.4500\n",
            "Epoch 9/500\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.69952 to 0.69918, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5585 - acc: 0.7667 - val_loss: 0.6992 - val_acc: 0.5000\n",
            "Epoch 10/500\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.69918 to 0.69847, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5647 - acc: 0.7833 - val_loss: 0.6985 - val_acc: 0.5000\n",
            "Epoch 11/500\n",
            "\n",
            "Epoch 00011: val_loss improved from 0.69847 to 0.69786, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5719 - acc: 0.8167 - val_loss: 0.6979 - val_acc: 0.5000\n",
            "Epoch 12/500\n",
            "\n",
            "Epoch 00012: val_loss improved from 0.69786 to 0.69777, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5723 - acc: 0.8167 - val_loss: 0.6978 - val_acc: 0.4500\n",
            "Epoch 13/500\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.69777\n",
            "60/60 - 0s - loss: 0.5334 - acc: 0.8833 - val_loss: 0.6982 - val_acc: 0.4500\n",
            "Epoch 14/500\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.69777\n",
            "60/60 - 0s - loss: 0.5409 - acc: 0.8167 - val_loss: 0.6980 - val_acc: 0.4500\n",
            "Epoch 15/500\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.69777\n",
            "60/60 - 0s - loss: 0.5156 - acc: 0.8500 - val_loss: 0.6978 - val_acc: 0.4000\n",
            "Epoch 16/500\n",
            "\n",
            "Epoch 00016: val_loss improved from 0.69777 to 0.69765, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5231 - acc: 0.7833 - val_loss: 0.6977 - val_acc: 0.4000\n",
            "Epoch 17/500\n",
            "\n",
            "Epoch 00017: val_loss improved from 0.69765 to 0.69634, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5248 - acc: 0.8667 - val_loss: 0.6963 - val_acc: 0.4000\n",
            "Epoch 18/500\n",
            "\n",
            "Epoch 00018: val_loss improved from 0.69634 to 0.69592, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4998 - acc: 0.8667 - val_loss: 0.6959 - val_acc: 0.4000\n",
            "Epoch 19/500\n",
            "\n",
            "Epoch 00019: val_loss improved from 0.69592 to 0.69470, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4897 - acc: 0.8667 - val_loss: 0.6947 - val_acc: 0.4000\n",
            "Epoch 20/500\n",
            "\n",
            "Epoch 00020: val_loss improved from 0.69470 to 0.69447, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4900 - acc: 0.8500 - val_loss: 0.6945 - val_acc: 0.4000\n",
            "Epoch 21/500\n",
            "\n",
            "Epoch 00021: val_loss improved from 0.69447 to 0.69287, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4786 - acc: 0.8667 - val_loss: 0.6929 - val_acc: 0.4000\n",
            "Epoch 22/500\n",
            "\n",
            "Epoch 00022: val_loss improved from 0.69287 to 0.68924, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4734 - acc: 0.8833 - val_loss: 0.6892 - val_acc: 0.5000\n",
            "Epoch 23/500\n",
            "\n",
            "Epoch 00023: val_loss improved from 0.68924 to 0.68584, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4536 - acc: 0.9167 - val_loss: 0.6858 - val_acc: 0.5000\n",
            "Epoch 24/500\n",
            "\n",
            "Epoch 00024: val_loss improved from 0.68584 to 0.68361, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4622 - acc: 0.8667 - val_loss: 0.6836 - val_acc: 0.5000\n",
            "Epoch 25/500\n",
            "\n",
            "Epoch 00025: val_loss improved from 0.68361 to 0.68260, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4217 - acc: 0.9500 - val_loss: 0.6826 - val_acc: 0.5000\n",
            "Epoch 26/500\n",
            "\n",
            "Epoch 00026: val_loss improved from 0.68260 to 0.67991, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4357 - acc: 0.9000 - val_loss: 0.6799 - val_acc: 0.5500\n",
            "Epoch 27/500\n",
            "\n",
            "Epoch 00027: val_loss improved from 0.67991 to 0.67954, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4734 - acc: 0.8333 - val_loss: 0.6795 - val_acc: 0.5500\n",
            "Epoch 28/500\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.4330 - acc: 0.9333 - val_loss: 0.6821 - val_acc: 0.5500\n",
            "Epoch 29/500\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.4275 - acc: 0.8833 - val_loss: 0.6871 - val_acc: 0.5500\n",
            "Epoch 30/500\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.4157 - acc: 0.9167 - val_loss: 0.6891 - val_acc: 0.6000\n",
            "Epoch 31/500\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.4010 - acc: 0.9333 - val_loss: 0.6918 - val_acc: 0.6000\n",
            "Epoch 32/500\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.4243 - acc: 0.9333 - val_loss: 0.6910 - val_acc: 0.6000\n",
            "Epoch 33/500\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.4043 - acc: 0.9333 - val_loss: 0.6926 - val_acc: 0.6000\n",
            "Epoch 34/500\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.4118 - acc: 0.8833 - val_loss: 0.6958 - val_acc: 0.6000\n",
            "Epoch 35/500\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.3801 - acc: 0.9333 - val_loss: 0.7025 - val_acc: 0.6000\n",
            "Epoch 36/500\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.4136 - acc: 0.9333 - val_loss: 0.7057 - val_acc: 0.6000\n",
            "Epoch 37/500\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.3975 - acc: 0.9167 - val_loss: 0.7090 - val_acc: 0.6000\n",
            "Epoch 38/500\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.3872 - acc: 0.9167 - val_loss: 0.7140 - val_acc: 0.6000\n",
            "Epoch 39/500\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.3715 - acc: 0.9500 - val_loss: 0.7154 - val_acc: 0.6000\n",
            "Epoch 40/500\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.3584 - acc: 0.9000 - val_loss: 0.7169 - val_acc: 0.6000\n",
            "Epoch 41/500\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.3889 - acc: 0.9000 - val_loss: 0.7182 - val_acc: 0.6000\n",
            "Epoch 42/500\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.3899 - acc: 0.9000 - val_loss: 0.7204 - val_acc: 0.5500\n",
            "Epoch 43/500\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.3648 - acc: 0.9167 - val_loss: 0.7229 - val_acc: 0.5500\n",
            "Epoch 44/500\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.3417 - acc: 0.9167 - val_loss: 0.7268 - val_acc: 0.5500\n",
            "Epoch 45/500\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.3543 - acc: 0.9167 - val_loss: 0.7258 - val_acc: 0.5500\n",
            "Epoch 46/500\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.3538 - acc: 0.9333 - val_loss: 0.7301 - val_acc: 0.5500\n",
            "Epoch 47/500\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.3314 - acc: 0.9667 - val_loss: 0.7327 - val_acc: 0.5500\n",
            "Epoch 48/500\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.3571 - acc: 0.9500 - val_loss: 0.7327 - val_acc: 0.5500\n",
            "Epoch 49/500\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.3244 - acc: 0.9333 - val_loss: 0.7401 - val_acc: 0.5500\n",
            "Epoch 50/500\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.3168 - acc: 0.9833 - val_loss: 0.7428 - val_acc: 0.5500\n",
            "Epoch 51/500\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.3537 - acc: 0.9500 - val_loss: 0.7435 - val_acc: 0.5500\n",
            "Epoch 52/500\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.3428 - acc: 0.9667 - val_loss: 0.7469 - val_acc: 0.5500\n",
            "Epoch 53/500\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.3215 - acc: 0.9333 - val_loss: 0.7478 - val_acc: 0.5500\n",
            "Epoch 54/500\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.3223 - acc: 0.9667 - val_loss: 0.7486 - val_acc: 0.5500\n",
            "Epoch 55/500\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.3292 - acc: 0.9333 - val_loss: 0.7455 - val_acc: 0.5500\n",
            "Epoch 56/500\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.3185 - acc: 0.9500 - val_loss: 0.7434 - val_acc: 0.5500\n",
            "Epoch 57/500\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.3373 - acc: 0.9167 - val_loss: 0.7445 - val_acc: 0.5500\n",
            "Epoch 58/500\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.3186 - acc: 0.9667 - val_loss: 0.7496 - val_acc: 0.5500\n",
            "Epoch 59/500\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.3383 - acc: 0.9333 - val_loss: 0.7589 - val_acc: 0.5500\n",
            "Epoch 60/500\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.3203 - acc: 0.9667 - val_loss: 0.7628 - val_acc: 0.5500\n",
            "Epoch 61/500\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.3220 - acc: 0.9667 - val_loss: 0.7620 - val_acc: 0.5500\n",
            "Epoch 62/500\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.3132 - acc: 0.9500 - val_loss: 0.7525 - val_acc: 0.5500\n",
            "Epoch 63/500\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.3009 - acc: 0.9833 - val_loss: 0.7443 - val_acc: 0.5500\n",
            "Epoch 64/500\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.2852 - acc: 0.9833 - val_loss: 0.7455 - val_acc: 0.5500\n",
            "Epoch 65/500\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.2796 - acc: 0.9833 - val_loss: 0.7501 - val_acc: 0.5500\n",
            "Epoch 66/500\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.2912 - acc: 0.9667 - val_loss: 0.7606 - val_acc: 0.5500\n",
            "Epoch 67/500\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.2941 - acc: 0.9500 - val_loss: 0.7649 - val_acc: 0.5000\n",
            "Epoch 68/500\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.2821 - acc: 0.9500 - val_loss: 0.7768 - val_acc: 0.5000\n",
            "Epoch 69/500\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.3108 - acc: 0.9500 - val_loss: 0.7822 - val_acc: 0.5500\n",
            "Epoch 70/500\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.2879 - acc: 0.9667 - val_loss: 0.7865 - val_acc: 0.5500\n",
            "Epoch 71/500\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.2977 - acc: 0.9500 - val_loss: 0.7806 - val_acc: 0.5500\n",
            "Epoch 72/500\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.2886 - acc: 0.9667 - val_loss: 0.7715 - val_acc: 0.5000\n",
            "Epoch 73/500\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.2804 - acc: 0.9833 - val_loss: 0.7720 - val_acc: 0.5000\n",
            "Epoch 74/500\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.2836 - acc: 0.9667 - val_loss: 0.7704 - val_acc: 0.5000\n",
            "Epoch 75/500\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.2570 - acc: 0.9833 - val_loss: 0.7655 - val_acc: 0.5000\n",
            "Epoch 76/500\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.2406 - acc: 0.9833 - val_loss: 0.7582 - val_acc: 0.5500\n",
            "Epoch 77/500\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.2721 - acc: 0.9667 - val_loss: 0.7551 - val_acc: 0.5500\n",
            "Epoch 78/500\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.2913 - acc: 0.9167 - val_loss: 0.7563 - val_acc: 0.5500\n",
            "Epoch 79/500\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.2686 - acc: 0.9500 - val_loss: 0.7587 - val_acc: 0.5500\n",
            "Epoch 80/500\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.2777 - acc: 0.9667 - val_loss: 0.7660 - val_acc: 0.5500\n",
            "Epoch 81/500\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.2668 - acc: 0.9500 - val_loss: 0.7567 - val_acc: 0.5500\n",
            "Epoch 82/500\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.2920 - acc: 0.9500 - val_loss: 0.7479 - val_acc: 0.6000\n",
            "Epoch 83/500\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.2583 - acc: 0.9833 - val_loss: 0.7577 - val_acc: 0.5000\n",
            "Epoch 84/500\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.2572 - acc: 1.0000 - val_loss: 0.7691 - val_acc: 0.5000\n",
            "Epoch 85/500\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.2566 - acc: 0.9667 - val_loss: 0.7752 - val_acc: 0.5000\n",
            "Epoch 86/500\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.2256 - acc: 0.9833 - val_loss: 0.7729 - val_acc: 0.5000\n",
            "Epoch 87/500\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.2610 - acc: 1.0000 - val_loss: 0.7823 - val_acc: 0.5000\n",
            "Epoch 88/500\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.2232 - acc: 0.9833 - val_loss: 0.7799 - val_acc: 0.5000\n",
            "Epoch 89/500\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.2372 - acc: 1.0000 - val_loss: 0.7759 - val_acc: 0.5000\n",
            "Epoch 90/500\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.2370 - acc: 0.9833 - val_loss: 0.7745 - val_acc: 0.5000\n",
            "Epoch 91/500\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.2671 - acc: 0.9500 - val_loss: 0.7811 - val_acc: 0.5000\n",
            "Epoch 92/500\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.2454 - acc: 0.9833 - val_loss: 0.7732 - val_acc: 0.5000\n",
            "Epoch 93/500\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.2558 - acc: 0.9667 - val_loss: 0.7698 - val_acc: 0.5000\n",
            "Epoch 94/500\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.2415 - acc: 0.9667 - val_loss: 0.7812 - val_acc: 0.5000\n",
            "Epoch 95/500\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.2633 - acc: 0.9667 - val_loss: 0.7844 - val_acc: 0.5000\n",
            "Epoch 96/500\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.2600 - acc: 0.9667 - val_loss: 0.7785 - val_acc: 0.5000\n",
            "Epoch 97/500\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.2790 - acc: 0.9500 - val_loss: 0.7757 - val_acc: 0.5000\n",
            "Epoch 98/500\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.2360 - acc: 1.0000 - val_loss: 0.7800 - val_acc: 0.5500\n",
            "Epoch 99/500\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.2600 - acc: 0.9667 - val_loss: 0.7753 - val_acc: 0.6000\n",
            "Epoch 100/500\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.2459 - acc: 0.9667 - val_loss: 0.7690 - val_acc: 0.6000\n",
            "Epoch 101/500\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.2457 - acc: 0.9667 - val_loss: 0.7498 - val_acc: 0.6000\n",
            "Epoch 102/500\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.2412 - acc: 0.9833 - val_loss: 0.7514 - val_acc: 0.6500\n",
            "Epoch 103/500\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.2468 - acc: 0.9667 - val_loss: 0.7398 - val_acc: 0.7500\n",
            "Epoch 104/500\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.2304 - acc: 0.9833 - val_loss: 0.7430 - val_acc: 0.7000\n",
            "Epoch 105/500\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.2446 - acc: 0.9833 - val_loss: 0.7503 - val_acc: 0.6500\n",
            "Epoch 106/500\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.2148 - acc: 1.0000 - val_loss: 0.7515 - val_acc: 0.6500\n",
            "Epoch 107/500\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.2110 - acc: 0.9833 - val_loss: 0.7475 - val_acc: 0.6500\n",
            "Epoch 108/500\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.2139 - acc: 0.9667 - val_loss: 0.7499 - val_acc: 0.6000\n",
            "Epoch 109/500\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.2055 - acc: 1.0000 - val_loss: 0.7455 - val_acc: 0.6000\n",
            "Epoch 110/500\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.2017 - acc: 1.0000 - val_loss: 0.7412 - val_acc: 0.6000\n",
            "Epoch 111/500\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.2195 - acc: 1.0000 - val_loss: 0.7479 - val_acc: 0.6000\n",
            "Epoch 112/500\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.2276 - acc: 0.9667 - val_loss: 0.7448 - val_acc: 0.6000\n",
            "Epoch 113/500\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.2163 - acc: 1.0000 - val_loss: 0.7420 - val_acc: 0.6000\n",
            "Epoch 114/500\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.2480 - acc: 0.9833 - val_loss: 0.7437 - val_acc: 0.6000\n",
            "Epoch 115/500\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1881 - acc: 0.9833 - val_loss: 0.7536 - val_acc: 0.6000\n",
            "Epoch 116/500\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.2241 - acc: 1.0000 - val_loss: 0.7578 - val_acc: 0.6000\n",
            "Epoch 117/500\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.2023 - acc: 1.0000 - val_loss: 0.7719 - val_acc: 0.6500\n",
            "Epoch 118/500\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.2232 - acc: 0.9833 - val_loss: 0.7770 - val_acc: 0.6500\n",
            "Epoch 119/500\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.2293 - acc: 0.9667 - val_loss: 0.7672 - val_acc: 0.6500\n",
            "Epoch 120/500\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.2181 - acc: 0.9833 - val_loss: 0.7535 - val_acc: 0.6500\n",
            "Epoch 121/500\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.2047 - acc: 0.9833 - val_loss: 0.7517 - val_acc: 0.6500\n",
            "Epoch 122/500\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.2328 - acc: 0.9667 - val_loss: 0.7647 - val_acc: 0.6000\n",
            "Epoch 123/500\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1794 - acc: 1.0000 - val_loss: 0.7571 - val_acc: 0.6000\n",
            "Epoch 124/500\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.2210 - acc: 0.9833 - val_loss: 0.7498 - val_acc: 0.5500\n",
            "Epoch 125/500\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.2431 - acc: 0.9667 - val_loss: 0.7550 - val_acc: 0.6000\n",
            "Epoch 126/500\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.2161 - acc: 0.9833 - val_loss: 0.7509 - val_acc: 0.6000\n",
            "Epoch 127/500\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.2272 - acc: 1.0000 - val_loss: 0.7415 - val_acc: 0.6500\n",
            "Epoch 128/500\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1902 - acc: 1.0000 - val_loss: 0.7448 - val_acc: 0.6500\n",
            "Epoch 129/500\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1886 - acc: 1.0000 - val_loss: 0.7516 - val_acc: 0.6500\n",
            "Epoch 130/500\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1941 - acc: 1.0000 - val_loss: 0.7597 - val_acc: 0.6500\n",
            "Epoch 131/500\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1988 - acc: 1.0000 - val_loss: 0.7565 - val_acc: 0.6500\n",
            "Epoch 132/500\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1895 - acc: 0.9833 - val_loss: 0.7504 - val_acc: 0.7000\n",
            "Epoch 133/500\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1847 - acc: 1.0000 - val_loss: 0.7558 - val_acc: 0.7000\n",
            "Epoch 134/500\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1903 - acc: 1.0000 - val_loss: 0.7462 - val_acc: 0.7500\n",
            "Epoch 135/500\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1981 - acc: 0.9667 - val_loss: 0.7286 - val_acc: 0.7000\n",
            "Epoch 136/500\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.2048 - acc: 0.9667 - val_loss: 0.7318 - val_acc: 0.7000\n",
            "Epoch 137/500\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1943 - acc: 0.9833 - val_loss: 0.7471 - val_acc: 0.7000\n",
            "Epoch 138/500\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1962 - acc: 1.0000 - val_loss: 0.7468 - val_acc: 0.6500\n",
            "Epoch 139/500\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.2124 - acc: 0.9667 - val_loss: 0.7358 - val_acc: 0.6000\n",
            "Epoch 140/500\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1937 - acc: 0.9833 - val_loss: 0.7398 - val_acc: 0.6500\n",
            "Epoch 141/500\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1883 - acc: 1.0000 - val_loss: 0.7357 - val_acc: 0.6500\n",
            "Epoch 142/500\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.2026 - acc: 0.9833 - val_loss: 0.7239 - val_acc: 0.6500\n",
            "Epoch 143/500\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1960 - acc: 1.0000 - val_loss: 0.7127 - val_acc: 0.7000\n",
            "Epoch 144/500\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1838 - acc: 0.9833 - val_loss: 0.7141 - val_acc: 0.6500\n",
            "Epoch 145/500\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.2038 - acc: 0.9500 - val_loss: 0.7052 - val_acc: 0.7000\n",
            "Epoch 146/500\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1641 - acc: 1.0000 - val_loss: 0.6880 - val_acc: 0.7000\n",
            "Epoch 147/500\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.2096 - acc: 0.9667 - val_loss: 0.6949 - val_acc: 0.7000\n",
            "Epoch 148/500\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1968 - acc: 0.9667 - val_loss: 0.7250 - val_acc: 0.6000\n",
            "Epoch 149/500\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1792 - acc: 1.0000 - val_loss: 0.7304 - val_acc: 0.6500\n",
            "Epoch 150/500\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1838 - acc: 1.0000 - val_loss: 0.7414 - val_acc: 0.6000\n",
            "Epoch 151/500\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.2013 - acc: 0.9833 - val_loss: 0.7299 - val_acc: 0.6000\n",
            "Epoch 152/500\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1678 - acc: 0.9833 - val_loss: 0.7302 - val_acc: 0.6000\n",
            "Epoch 153/500\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1958 - acc: 0.9833 - val_loss: 0.7368 - val_acc: 0.6000\n",
            "Epoch 154/500\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1968 - acc: 0.9667 - val_loss: 0.7278 - val_acc: 0.6000\n",
            "Epoch 155/500\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1826 - acc: 0.9833 - val_loss: 0.7265 - val_acc: 0.6500\n",
            "Epoch 156/500\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1489 - acc: 1.0000 - val_loss: 0.7223 - val_acc: 0.6500\n",
            "Epoch 157/500\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1721 - acc: 1.0000 - val_loss: 0.7172 - val_acc: 0.6500\n",
            "Epoch 158/500\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1880 - acc: 0.9833 - val_loss: 0.7301 - val_acc: 0.6500\n",
            "Epoch 159/500\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1862 - acc: 0.9500 - val_loss: 0.7348 - val_acc: 0.6500\n",
            "Epoch 160/500\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1820 - acc: 0.9833 - val_loss: 0.7122 - val_acc: 0.7000\n",
            "Epoch 161/500\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1855 - acc: 0.9833 - val_loss: 0.6958 - val_acc: 0.7000\n",
            "Epoch 162/500\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1824 - acc: 1.0000 - val_loss: 0.6952 - val_acc: 0.7000\n",
            "Epoch 163/500\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1970 - acc: 0.9833 - val_loss: 0.7127 - val_acc: 0.7000\n",
            "Epoch 164/500\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1643 - acc: 1.0000 - val_loss: 0.7381 - val_acc: 0.6500\n",
            "Epoch 165/500\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1520 - acc: 1.0000 - val_loss: 0.7469 - val_acc: 0.6500\n",
            "Epoch 166/500\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1934 - acc: 0.9833 - val_loss: 0.7468 - val_acc: 0.6500\n",
            "Epoch 167/500\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1610 - acc: 0.9833 - val_loss: 0.7260 - val_acc: 0.6500\n",
            "Epoch 168/500\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1652 - acc: 1.0000 - val_loss: 0.7137 - val_acc: 0.6500\n",
            "Epoch 169/500\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1585 - acc: 1.0000 - val_loss: 0.6929 - val_acc: 0.6000\n",
            "Epoch 170/500\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1774 - acc: 0.9833 - val_loss: 0.6981 - val_acc: 0.6000\n",
            "Epoch 171/500\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1855 - acc: 0.9667 - val_loss: 0.7222 - val_acc: 0.6500\n",
            "Epoch 172/500\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1669 - acc: 0.9833 - val_loss: 0.7327 - val_acc: 0.6500\n",
            "Epoch 173/500\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1551 - acc: 0.9833 - val_loss: 0.7378 - val_acc: 0.6500\n",
            "Epoch 174/500\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1995 - acc: 0.9500 - val_loss: 0.7526 - val_acc: 0.6500\n",
            "Epoch 175/500\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1657 - acc: 0.9833 - val_loss: 0.7565 - val_acc: 0.6000\n",
            "Epoch 176/500\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1599 - acc: 0.9833 - val_loss: 0.7764 - val_acc: 0.6000\n",
            "Epoch 177/500\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1475 - acc: 0.9833 - val_loss: 0.7935 - val_acc: 0.5500\n",
            "Epoch 178/500\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1716 - acc: 0.9833 - val_loss: 0.8039 - val_acc: 0.6000\n",
            "Epoch 179/500\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1426 - acc: 1.0000 - val_loss: 0.8112 - val_acc: 0.6000\n",
            "Epoch 180/500\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1688 - acc: 0.9833 - val_loss: 0.8229 - val_acc: 0.6500\n",
            "Epoch 181/500\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1810 - acc: 0.9833 - val_loss: 0.8140 - val_acc: 0.6500\n",
            "Epoch 182/500\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1534 - acc: 0.9833 - val_loss: 0.7893 - val_acc: 0.6000\n",
            "Epoch 183/500\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1965 - acc: 0.9833 - val_loss: 0.7793 - val_acc: 0.6000\n",
            "Epoch 184/500\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1718 - acc: 1.0000 - val_loss: 0.7832 - val_acc: 0.6000\n",
            "Epoch 185/500\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1530 - acc: 1.0000 - val_loss: 0.7868 - val_acc: 0.6000\n",
            "Epoch 186/500\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1735 - acc: 0.9833 - val_loss: 0.8272 - val_acc: 0.6000\n",
            "Epoch 187/500\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1642 - acc: 1.0000 - val_loss: 0.8497 - val_acc: 0.6000\n",
            "Epoch 188/500\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1524 - acc: 0.9833 - val_loss: 0.8589 - val_acc: 0.6000\n",
            "Epoch 189/500\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1738 - acc: 0.9833 - val_loss: 0.8257 - val_acc: 0.5500\n",
            "Epoch 190/500\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1461 - acc: 0.9833 - val_loss: 0.7848 - val_acc: 0.5500\n",
            "Epoch 191/500\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1533 - acc: 0.9667 - val_loss: 0.7791 - val_acc: 0.5500\n",
            "Epoch 192/500\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1441 - acc: 1.0000 - val_loss: 0.7765 - val_acc: 0.6000\n",
            "Epoch 193/500\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1457 - acc: 1.0000 - val_loss: 0.7897 - val_acc: 0.5500\n",
            "Epoch 194/500\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1527 - acc: 0.9833 - val_loss: 0.8036 - val_acc: 0.5500\n",
            "Epoch 195/500\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1533 - acc: 1.0000 - val_loss: 0.8015 - val_acc: 0.5500\n",
            "Epoch 196/500\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1616 - acc: 0.9833 - val_loss: 0.8016 - val_acc: 0.6000\n",
            "Epoch 197/500\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1607 - acc: 0.9833 - val_loss: 0.7941 - val_acc: 0.6000\n",
            "Epoch 198/500\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1536 - acc: 0.9833 - val_loss: 0.7858 - val_acc: 0.6000\n",
            "Epoch 199/500\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1756 - acc: 0.9833 - val_loss: 0.7843 - val_acc: 0.6000\n",
            "Epoch 200/500\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1415 - acc: 0.9667 - val_loss: 0.7899 - val_acc: 0.6000\n",
            "Epoch 201/500\n",
            "\n",
            "Epoch 00201: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1382 - acc: 1.0000 - val_loss: 0.7889 - val_acc: 0.6000\n",
            "Epoch 202/500\n",
            "\n",
            "Epoch 00202: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1639 - acc: 0.9667 - val_loss: 0.7830 - val_acc: 0.6000\n",
            "Epoch 203/500\n",
            "\n",
            "Epoch 00203: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1576 - acc: 1.0000 - val_loss: 0.7635 - val_acc: 0.6500\n",
            "Epoch 204/500\n",
            "\n",
            "Epoch 00204: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1488 - acc: 1.0000 - val_loss: 0.7522 - val_acc: 0.7000\n",
            "Epoch 205/500\n",
            "\n",
            "Epoch 00205: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1623 - acc: 0.9667 - val_loss: 0.7395 - val_acc: 0.7000\n",
            "Epoch 206/500\n",
            "\n",
            "Epoch 00206: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1317 - acc: 0.9833 - val_loss: 0.7372 - val_acc: 0.7000\n",
            "Epoch 207/500\n",
            "\n",
            "Epoch 00207: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1431 - acc: 1.0000 - val_loss: 0.7623 - val_acc: 0.6500\n",
            "Epoch 208/500\n",
            "\n",
            "Epoch 00208: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1393 - acc: 0.9833 - val_loss: 0.7691 - val_acc: 0.6500\n",
            "Epoch 209/500\n",
            "\n",
            "Epoch 00209: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1592 - acc: 0.9667 - val_loss: 0.7837 - val_acc: 0.6500\n",
            "Epoch 210/500\n",
            "\n",
            "Epoch 00210: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1298 - acc: 1.0000 - val_loss: 0.7905 - val_acc: 0.6500\n",
            "Epoch 211/500\n",
            "\n",
            "Epoch 00211: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1438 - acc: 1.0000 - val_loss: 0.7915 - val_acc: 0.6500\n",
            "Epoch 212/500\n",
            "\n",
            "Epoch 00212: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1569 - acc: 1.0000 - val_loss: 0.8043 - val_acc: 0.6500\n",
            "Epoch 213/500\n",
            "\n",
            "Epoch 00213: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1592 - acc: 1.0000 - val_loss: 0.8415 - val_acc: 0.6500\n",
            "Epoch 214/500\n",
            "\n",
            "Epoch 00214: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1378 - acc: 1.0000 - val_loss: 0.8779 - val_acc: 0.5500\n",
            "Epoch 215/500\n",
            "\n",
            "Epoch 00215: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1231 - acc: 1.0000 - val_loss: 0.9154 - val_acc: 0.5500\n",
            "Epoch 216/500\n",
            "\n",
            "Epoch 00216: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1369 - acc: 0.9833 - val_loss: 0.9246 - val_acc: 0.5500\n",
            "Epoch 217/500\n",
            "\n",
            "Epoch 00217: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1397 - acc: 1.0000 - val_loss: 0.8830 - val_acc: 0.6000\n",
            "Epoch 218/500\n",
            "\n",
            "Epoch 00218: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1331 - acc: 1.0000 - val_loss: 0.8812 - val_acc: 0.6500\n",
            "Epoch 219/500\n",
            "\n",
            "Epoch 00219: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1326 - acc: 0.9833 - val_loss: 0.8535 - val_acc: 0.6000\n",
            "Epoch 220/500\n",
            "\n",
            "Epoch 00220: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1274 - acc: 0.9833 - val_loss: 0.8336 - val_acc: 0.6500\n",
            "Epoch 221/500\n",
            "\n",
            "Epoch 00221: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1544 - acc: 1.0000 - val_loss: 0.8055 - val_acc: 0.6000\n",
            "Epoch 222/500\n",
            "\n",
            "Epoch 00222: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1642 - acc: 0.9500 - val_loss: 0.7883 - val_acc: 0.6500\n",
            "Epoch 223/500\n",
            "\n",
            "Epoch 00223: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1362 - acc: 0.9833 - val_loss: 0.7659 - val_acc: 0.6500\n",
            "Epoch 224/500\n",
            "\n",
            "Epoch 00224: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1525 - acc: 0.9833 - val_loss: 0.7556 - val_acc: 0.6000\n",
            "Epoch 225/500\n",
            "\n",
            "Epoch 00225: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1394 - acc: 1.0000 - val_loss: 0.7754 - val_acc: 0.6500\n",
            "Epoch 226/500\n",
            "\n",
            "Epoch 00226: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1223 - acc: 1.0000 - val_loss: 0.7784 - val_acc: 0.6000\n",
            "Epoch 227/500\n",
            "\n",
            "Epoch 00227: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1470 - acc: 1.0000 - val_loss: 0.7626 - val_acc: 0.6000\n",
            "Epoch 228/500\n",
            "\n",
            "Epoch 00228: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1337 - acc: 1.0000 - val_loss: 0.7491 - val_acc: 0.6000\n",
            "Epoch 229/500\n",
            "\n",
            "Epoch 00229: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1421 - acc: 1.0000 - val_loss: 0.7413 - val_acc: 0.6000\n",
            "Epoch 230/500\n",
            "\n",
            "Epoch 00230: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1369 - acc: 0.9833 - val_loss: 0.7510 - val_acc: 0.6000\n",
            "Epoch 231/500\n",
            "\n",
            "Epoch 00231: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1236 - acc: 1.0000 - val_loss: 0.7597 - val_acc: 0.6500\n",
            "Epoch 232/500\n",
            "\n",
            "Epoch 00232: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1391 - acc: 1.0000 - val_loss: 0.7600 - val_acc: 0.6500\n",
            "Epoch 233/500\n",
            "\n",
            "Epoch 00233: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1101 - acc: 0.9833 - val_loss: 0.7575 - val_acc: 0.6000\n",
            "Epoch 234/500\n",
            "\n",
            "Epoch 00234: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1335 - acc: 1.0000 - val_loss: 0.7451 - val_acc: 0.6500\n",
            "Epoch 235/500\n",
            "\n",
            "Epoch 00235: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1735 - acc: 0.9833 - val_loss: 0.7636 - val_acc: 0.6000\n",
            "Epoch 236/500\n",
            "\n",
            "Epoch 00236: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1331 - acc: 1.0000 - val_loss: 0.7743 - val_acc: 0.6500\n",
            "Epoch 237/500\n",
            "\n",
            "Epoch 00237: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1299 - acc: 1.0000 - val_loss: 0.7841 - val_acc: 0.6000\n",
            "Epoch 238/500\n",
            "\n",
            "Epoch 00238: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1450 - acc: 0.9833 - val_loss: 0.7896 - val_acc: 0.6000\n",
            "Epoch 239/500\n",
            "\n",
            "Epoch 00239: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1239 - acc: 0.9833 - val_loss: 0.7751 - val_acc: 0.6500\n",
            "Epoch 240/500\n",
            "\n",
            "Epoch 00240: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1326 - acc: 1.0000 - val_loss: 0.7738 - val_acc: 0.6500\n",
            "Epoch 241/500\n",
            "\n",
            "Epoch 00241: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1247 - acc: 0.9667 - val_loss: 0.7825 - val_acc: 0.6500\n",
            "Epoch 242/500\n",
            "\n",
            "Epoch 00242: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1405 - acc: 1.0000 - val_loss: 0.7693 - val_acc: 0.6500\n",
            "Epoch 243/500\n",
            "\n",
            "Epoch 00243: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1400 - acc: 0.9833 - val_loss: 0.7784 - val_acc: 0.6500\n",
            "Epoch 244/500\n",
            "\n",
            "Epoch 00244: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1058 - acc: 1.0000 - val_loss: 0.7645 - val_acc: 0.6000\n",
            "Epoch 245/500\n",
            "\n",
            "Epoch 00245: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1077 - acc: 1.0000 - val_loss: 0.7383 - val_acc: 0.6000\n",
            "Epoch 246/500\n",
            "\n",
            "Epoch 00246: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1333 - acc: 1.0000 - val_loss: 0.7079 - val_acc: 0.6000\n",
            "Epoch 247/500\n",
            "\n",
            "Epoch 00247: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1008 - acc: 1.0000 - val_loss: 0.7195 - val_acc: 0.6000\n",
            "Epoch 248/500\n",
            "\n",
            "Epoch 00248: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1062 - acc: 1.0000 - val_loss: 0.7404 - val_acc: 0.6500\n",
            "Epoch 249/500\n",
            "\n",
            "Epoch 00249: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1336 - acc: 0.9833 - val_loss: 0.7369 - val_acc: 0.6500\n",
            "Epoch 250/500\n",
            "\n",
            "Epoch 00250: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1256 - acc: 0.9833 - val_loss: 0.7297 - val_acc: 0.6500\n",
            "Epoch 251/500\n",
            "\n",
            "Epoch 00251: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1459 - acc: 0.9833 - val_loss: 0.7278 - val_acc: 0.6500\n",
            "Epoch 252/500\n",
            "\n",
            "Epoch 00252: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1133 - acc: 1.0000 - val_loss: 0.7711 - val_acc: 0.6000\n",
            "Epoch 253/500\n",
            "\n",
            "Epoch 00253: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1002 - acc: 1.0000 - val_loss: 0.8010 - val_acc: 0.6000\n",
            "Epoch 254/500\n",
            "\n",
            "Epoch 00254: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1233 - acc: 1.0000 - val_loss: 0.8125 - val_acc: 0.6000\n",
            "Epoch 255/500\n",
            "\n",
            "Epoch 00255: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1230 - acc: 1.0000 - val_loss: 0.8071 - val_acc: 0.6500\n",
            "Epoch 256/500\n",
            "\n",
            "Epoch 00256: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1235 - acc: 1.0000 - val_loss: 0.8400 - val_acc: 0.6000\n",
            "Epoch 257/500\n",
            "\n",
            "Epoch 00257: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1381 - acc: 0.9667 - val_loss: 0.8136 - val_acc: 0.6500\n",
            "Epoch 258/500\n",
            "\n",
            "Epoch 00258: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1348 - acc: 1.0000 - val_loss: 0.8074 - val_acc: 0.6500\n",
            "Epoch 259/500\n",
            "\n",
            "Epoch 00259: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1215 - acc: 1.0000 - val_loss: 0.8191 - val_acc: 0.6500\n",
            "Epoch 260/500\n",
            "\n",
            "Epoch 00260: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1595 - acc: 0.9833 - val_loss: 0.8089 - val_acc: 0.6000\n",
            "Epoch 261/500\n",
            "\n",
            "Epoch 00261: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1257 - acc: 0.9833 - val_loss: 0.7673 - val_acc: 0.6000\n",
            "Epoch 262/500\n",
            "\n",
            "Epoch 00262: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.0993 - acc: 1.0000 - val_loss: 0.7450 - val_acc: 0.6000\n",
            "Epoch 263/500\n",
            "\n",
            "Epoch 00263: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1207 - acc: 0.9833 - val_loss: 0.7465 - val_acc: 0.6000\n",
            "Epoch 264/500\n",
            "\n",
            "Epoch 00264: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1333 - acc: 1.0000 - val_loss: 0.7703 - val_acc: 0.6000\n",
            "Epoch 265/500\n",
            "\n",
            "Epoch 00265: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1163 - acc: 1.0000 - val_loss: 0.7508 - val_acc: 0.6000\n",
            "Epoch 266/500\n",
            "\n",
            "Epoch 00266: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1294 - acc: 1.0000 - val_loss: 0.7451 - val_acc: 0.5500\n",
            "Epoch 267/500\n",
            "\n",
            "Epoch 00267: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1125 - acc: 1.0000 - val_loss: 0.7501 - val_acc: 0.5500\n",
            "Epoch 268/500\n",
            "\n",
            "Epoch 00268: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1125 - acc: 0.9833 - val_loss: 0.7506 - val_acc: 0.6000\n",
            "Epoch 269/500\n",
            "\n",
            "Epoch 00269: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.0954 - acc: 1.0000 - val_loss: 0.7554 - val_acc: 0.6500\n",
            "Epoch 270/500\n",
            "\n",
            "Epoch 00270: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1420 - acc: 0.9667 - val_loss: 0.7509 - val_acc: 0.6500\n",
            "Epoch 271/500\n",
            "\n",
            "Epoch 00271: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.0990 - acc: 1.0000 - val_loss: 0.7483 - val_acc: 0.6500\n",
            "Epoch 272/500\n",
            "\n",
            "Epoch 00272: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1194 - acc: 1.0000 - val_loss: 0.7462 - val_acc: 0.6500\n",
            "Epoch 273/500\n",
            "\n",
            "Epoch 00273: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1359 - acc: 1.0000 - val_loss: 0.7449 - val_acc: 0.6000\n",
            "Epoch 274/500\n",
            "\n",
            "Epoch 00274: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1154 - acc: 0.9833 - val_loss: 0.7734 - val_acc: 0.6000\n",
            "Epoch 275/500\n",
            "\n",
            "Epoch 00275: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1181 - acc: 0.9667 - val_loss: 0.8016 - val_acc: 0.6000\n",
            "Epoch 276/500\n",
            "\n",
            "Epoch 00276: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.0934 - acc: 1.0000 - val_loss: 0.8059 - val_acc: 0.6000\n",
            "Epoch 277/500\n",
            "\n",
            "Epoch 00277: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1082 - acc: 1.0000 - val_loss: 0.8385 - val_acc: 0.6000\n",
            "Epoch 278/500\n",
            "\n",
            "Epoch 00278: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1031 - acc: 1.0000 - val_loss: 0.8547 - val_acc: 0.6000\n",
            "Epoch 279/500\n",
            "\n",
            "Epoch 00279: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1089 - acc: 1.0000 - val_loss: 0.8504 - val_acc: 0.6000\n",
            "Epoch 280/500\n",
            "\n",
            "Epoch 00280: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1004 - acc: 1.0000 - val_loss: 0.8445 - val_acc: 0.6000\n",
            "Epoch 281/500\n",
            "\n",
            "Epoch 00281: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1154 - acc: 1.0000 - val_loss: 0.8508 - val_acc: 0.6000\n",
            "Epoch 282/500\n",
            "\n",
            "Epoch 00282: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1112 - acc: 0.9833 - val_loss: 0.8469 - val_acc: 0.6500\n",
            "Epoch 283/500\n",
            "\n",
            "Epoch 00283: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1065 - acc: 0.9833 - val_loss: 0.8530 - val_acc: 0.6500\n",
            "Epoch 284/500\n",
            "\n",
            "Epoch 00284: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1086 - acc: 1.0000 - val_loss: 0.8211 - val_acc: 0.6000\n",
            "Epoch 285/500\n",
            "\n",
            "Epoch 00285: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1091 - acc: 1.0000 - val_loss: 0.8079 - val_acc: 0.6500\n",
            "Epoch 286/500\n",
            "\n",
            "Epoch 00286: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1102 - acc: 0.9667 - val_loss: 0.7775 - val_acc: 0.6500\n",
            "Epoch 287/500\n",
            "\n",
            "Epoch 00287: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.0978 - acc: 1.0000 - val_loss: 0.7382 - val_acc: 0.6000\n",
            "Epoch 288/500\n",
            "\n",
            "Epoch 00288: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1067 - acc: 1.0000 - val_loss: 0.7287 - val_acc: 0.6000\n",
            "Epoch 289/500\n",
            "\n",
            "Epoch 00289: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.0963 - acc: 1.0000 - val_loss: 0.7386 - val_acc: 0.6000\n",
            "Epoch 290/500\n",
            "\n",
            "Epoch 00290: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1141 - acc: 1.0000 - val_loss: 0.7382 - val_acc: 0.6000\n",
            "Epoch 291/500\n",
            "\n",
            "Epoch 00291: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.0960 - acc: 1.0000 - val_loss: 0.7252 - val_acc: 0.6000\n",
            "Epoch 292/500\n",
            "\n",
            "Epoch 00292: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1132 - acc: 1.0000 - val_loss: 0.6978 - val_acc: 0.6000\n",
            "Epoch 293/500\n",
            "\n",
            "Epoch 00293: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.0978 - acc: 1.0000 - val_loss: 0.6800 - val_acc: 0.6000\n",
            "Epoch 294/500\n",
            "\n",
            "Epoch 00294: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1139 - acc: 0.9833 - val_loss: 0.6811 - val_acc: 0.6000\n",
            "Epoch 295/500\n",
            "\n",
            "Epoch 00295: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1007 - acc: 1.0000 - val_loss: 0.7132 - val_acc: 0.6000\n",
            "Epoch 296/500\n",
            "\n",
            "Epoch 00296: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.0842 - acc: 1.0000 - val_loss: 0.7509 - val_acc: 0.6000\n",
            "Epoch 297/500\n",
            "\n",
            "Epoch 00297: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1157 - acc: 1.0000 - val_loss: 0.7689 - val_acc: 0.6000\n",
            "Epoch 298/500\n",
            "\n",
            "Epoch 00298: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1203 - acc: 1.0000 - val_loss: 0.7469 - val_acc: 0.6000\n",
            "Epoch 299/500\n",
            "\n",
            "Epoch 00299: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.0948 - acc: 1.0000 - val_loss: 0.7222 - val_acc: 0.6000\n",
            "Epoch 300/500\n",
            "\n",
            "Epoch 00300: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.0952 - acc: 1.0000 - val_loss: 0.6947 - val_acc: 0.6000\n",
            "Epoch 301/500\n",
            "\n",
            "Epoch 00301: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1051 - acc: 1.0000 - val_loss: 0.6997 - val_acc: 0.6000\n",
            "Epoch 302/500\n",
            "\n",
            "Epoch 00302: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1152 - acc: 1.0000 - val_loss: 0.7362 - val_acc: 0.6000\n",
            "Epoch 303/500\n",
            "\n",
            "Epoch 00303: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1137 - acc: 0.9833 - val_loss: 0.8016 - val_acc: 0.7000\n",
            "Epoch 304/500\n",
            "\n",
            "Epoch 00304: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.0954 - acc: 1.0000 - val_loss: 0.8147 - val_acc: 0.6000\n",
            "Epoch 305/500\n",
            "\n",
            "Epoch 00305: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1092 - acc: 0.9833 - val_loss: 0.8463 - val_acc: 0.6000\n",
            "Epoch 306/500\n",
            "\n",
            "Epoch 00306: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1495 - acc: 0.9667 - val_loss: 0.8589 - val_acc: 0.6000\n",
            "Epoch 307/500\n",
            "\n",
            "Epoch 00307: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.0986 - acc: 0.9833 - val_loss: 0.8164 - val_acc: 0.6500\n",
            "Epoch 308/500\n",
            "\n",
            "Epoch 00308: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1089 - acc: 1.0000 - val_loss: 0.7873 - val_acc: 0.6000\n",
            "Epoch 309/500\n",
            "\n",
            "Epoch 00309: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1316 - acc: 0.9833 - val_loss: 0.8239 - val_acc: 0.6500\n",
            "Epoch 310/500\n",
            "\n",
            "Epoch 00310: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1006 - acc: 1.0000 - val_loss: 0.8840 - val_acc: 0.6500\n",
            "Epoch 311/500\n",
            "\n",
            "Epoch 00311: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1239 - acc: 1.0000 - val_loss: 0.9136 - val_acc: 0.6000\n",
            "Epoch 312/500\n",
            "\n",
            "Epoch 00312: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.0956 - acc: 1.0000 - val_loss: 0.9307 - val_acc: 0.6500\n",
            "Epoch 313/500\n",
            "\n",
            "Epoch 00313: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1285 - acc: 0.9833 - val_loss: 0.8819 - val_acc: 0.6000\n",
            "Epoch 314/500\n",
            "\n",
            "Epoch 00314: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.0991 - acc: 1.0000 - val_loss: 0.8463 - val_acc: 0.6000\n",
            "Epoch 315/500\n",
            "\n",
            "Epoch 00315: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1065 - acc: 1.0000 - val_loss: 0.7864 - val_acc: 0.6000\n",
            "Epoch 316/500\n",
            "\n",
            "Epoch 00316: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.0956 - acc: 1.0000 - val_loss: 0.7667 - val_acc: 0.6500\n",
            "Epoch 317/500\n",
            "\n",
            "Epoch 00317: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1024 - acc: 1.0000 - val_loss: 0.7658 - val_acc: 0.6500\n",
            "Epoch 318/500\n",
            "\n",
            "Epoch 00318: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.0767 - acc: 1.0000 - val_loss: 0.7889 - val_acc: 0.6000\n",
            "Epoch 319/500\n",
            "\n",
            "Epoch 00319: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1084 - acc: 1.0000 - val_loss: 0.8067 - val_acc: 0.6000\n",
            "Epoch 320/500\n",
            "\n",
            "Epoch 00320: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.0968 - acc: 0.9833 - val_loss: 0.8095 - val_acc: 0.5500\n",
            "Epoch 321/500\n",
            "\n",
            "Epoch 00321: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1017 - acc: 0.9833 - val_loss: 0.8147 - val_acc: 0.5500\n",
            "Epoch 322/500\n",
            "\n",
            "Epoch 00322: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.0837 - acc: 1.0000 - val_loss: 0.8203 - val_acc: 0.6000\n",
            "Epoch 323/500\n",
            "\n",
            "Epoch 00323: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1148 - acc: 1.0000 - val_loss: 0.7982 - val_acc: 0.6500\n",
            "Epoch 324/500\n",
            "\n",
            "Epoch 00324: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1059 - acc: 1.0000 - val_loss: 0.7388 - val_acc: 0.6000\n",
            "Epoch 325/500\n",
            "\n",
            "Epoch 00325: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.0994 - acc: 1.0000 - val_loss: 0.7572 - val_acc: 0.6500\n",
            "Epoch 326/500\n",
            "\n",
            "Epoch 00326: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1069 - acc: 1.0000 - val_loss: 0.8033 - val_acc: 0.7000\n",
            "Epoch 327/500\n",
            "\n",
            "Epoch 00327: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.0890 - acc: 1.0000 - val_loss: 0.8372 - val_acc: 0.6000\n",
            "Epoch 328/500\n",
            "\n",
            "Epoch 00328: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.0968 - acc: 1.0000 - val_loss: 0.9048 - val_acc: 0.6000\n",
            "Epoch 329/500\n",
            "\n",
            "Epoch 00329: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1037 - acc: 1.0000 - val_loss: 0.9260 - val_acc: 0.6000\n",
            "Epoch 330/500\n",
            "\n",
            "Epoch 00330: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.0902 - acc: 1.0000 - val_loss: 0.8680 - val_acc: 0.6000\n",
            "Epoch 331/500\n",
            "\n",
            "Epoch 00331: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.0874 - acc: 1.0000 - val_loss: 0.8276 - val_acc: 0.6000\n",
            "Epoch 332/500\n",
            "\n",
            "Epoch 00332: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1024 - acc: 1.0000 - val_loss: 0.8175 - val_acc: 0.6000\n",
            "Epoch 333/500\n",
            "\n",
            "Epoch 00333: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1107 - acc: 1.0000 - val_loss: 0.8692 - val_acc: 0.6500\n",
            "Epoch 334/500\n",
            "\n",
            "Epoch 00334: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1023 - acc: 1.0000 - val_loss: 0.8521 - val_acc: 0.6000\n",
            "Epoch 335/500\n",
            "\n",
            "Epoch 00335: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.0871 - acc: 1.0000 - val_loss: 0.8266 - val_acc: 0.6500\n",
            "Epoch 336/500\n",
            "\n",
            "Epoch 00336: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.0956 - acc: 1.0000 - val_loss: 0.8272 - val_acc: 0.6500\n",
            "Epoch 337/500\n",
            "\n",
            "Epoch 00337: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1014 - acc: 0.9833 - val_loss: 0.8533 - val_acc: 0.6000\n",
            "Epoch 338/500\n",
            "\n",
            "Epoch 00338: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.0887 - acc: 1.0000 - val_loss: 0.8879 - val_acc: 0.6000\n",
            "Epoch 339/500\n",
            "\n",
            "Epoch 00339: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.0824 - acc: 1.0000 - val_loss: 0.8842 - val_acc: 0.6000\n",
            "Epoch 340/500\n",
            "\n",
            "Epoch 00340: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1279 - acc: 0.9667 - val_loss: 0.8885 - val_acc: 0.6000\n",
            "Epoch 341/500\n",
            "\n",
            "Epoch 00341: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.0939 - acc: 1.0000 - val_loss: 0.8930 - val_acc: 0.6000\n",
            "Epoch 342/500\n",
            "\n",
            "Epoch 00342: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1001 - acc: 0.9833 - val_loss: 0.8551 - val_acc: 0.6000\n",
            "Epoch 343/500\n",
            "\n",
            "Epoch 00343: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1065 - acc: 0.9833 - val_loss: 0.8336 - val_acc: 0.6000\n",
            "Epoch 344/500\n",
            "\n",
            "Epoch 00344: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1141 - acc: 0.9833 - val_loss: 0.8962 - val_acc: 0.5500\n",
            "Epoch 345/500\n",
            "\n",
            "Epoch 00345: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.0980 - acc: 1.0000 - val_loss: 0.8603 - val_acc: 0.5500\n",
            "Epoch 346/500\n",
            "\n",
            "Epoch 00346: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.0944 - acc: 0.9833 - val_loss: 0.7947 - val_acc: 0.6500\n",
            "Epoch 347/500\n",
            "\n",
            "Epoch 00347: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.0809 - acc: 1.0000 - val_loss: 0.7423 - val_acc: 0.6500\n",
            "Epoch 348/500\n",
            "\n",
            "Epoch 00348: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1154 - acc: 0.9833 - val_loss: 0.7191 - val_acc: 0.6500\n",
            "Epoch 349/500\n",
            "\n",
            "Epoch 00349: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.0797 - acc: 1.0000 - val_loss: 0.7225 - val_acc: 0.6500\n",
            "Epoch 350/500\n",
            "\n",
            "Epoch 00350: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.0926 - acc: 1.0000 - val_loss: 0.7738 - val_acc: 0.7000\n",
            "Epoch 351/500\n",
            "\n",
            "Epoch 00351: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.0913 - acc: 1.0000 - val_loss: 0.7896 - val_acc: 0.6500\n",
            "Epoch 352/500\n",
            "\n",
            "Epoch 00352: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.0945 - acc: 1.0000 - val_loss: 0.7674 - val_acc: 0.6500\n",
            "Epoch 353/500\n",
            "\n",
            "Epoch 00353: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.0953 - acc: 1.0000 - val_loss: 0.7432 - val_acc: 0.7000\n",
            "Epoch 354/500\n",
            "\n",
            "Epoch 00354: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.0924 - acc: 1.0000 - val_loss: 0.7257 - val_acc: 0.7000\n",
            "Epoch 355/500\n",
            "\n",
            "Epoch 00355: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1232 - acc: 0.9833 - val_loss: 0.7203 - val_acc: 0.7000\n",
            "Epoch 356/500\n",
            "\n",
            "Epoch 00356: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.0881 - acc: 1.0000 - val_loss: 0.7331 - val_acc: 0.7000\n",
            "Epoch 357/500\n",
            "\n",
            "Epoch 00357: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.0950 - acc: 1.0000 - val_loss: 0.7759 - val_acc: 0.7000\n",
            "Epoch 358/500\n",
            "\n",
            "Epoch 00358: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1081 - acc: 0.9833 - val_loss: 0.7469 - val_acc: 0.6500\n",
            "Epoch 359/500\n",
            "\n",
            "Epoch 00359: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.0835 - acc: 1.0000 - val_loss: 0.7250 - val_acc: 0.6500\n",
            "Epoch 360/500\n",
            "\n",
            "Epoch 00360: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1242 - acc: 0.9833 - val_loss: 0.6973 - val_acc: 0.7000\n",
            "Epoch 361/500\n",
            "\n",
            "Epoch 00361: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.0923 - acc: 0.9833 - val_loss: 0.7269 - val_acc: 0.7000\n",
            "Epoch 362/500\n",
            "\n",
            "Epoch 00362: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.0852 - acc: 1.0000 - val_loss: 0.7772 - val_acc: 0.6500\n",
            "Epoch 363/500\n",
            "\n",
            "Epoch 00363: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.0804 - acc: 0.9833 - val_loss: 0.8068 - val_acc: 0.6500\n",
            "Epoch 364/500\n",
            "\n",
            "Epoch 00364: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.0890 - acc: 1.0000 - val_loss: 0.8004 - val_acc: 0.6500\n",
            "Epoch 365/500\n",
            "\n",
            "Epoch 00365: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.0877 - acc: 0.9833 - val_loss: 0.7975 - val_acc: 0.6500\n",
            "Epoch 366/500\n",
            "\n",
            "Epoch 00366: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.0963 - acc: 1.0000 - val_loss: 0.7931 - val_acc: 0.6500\n",
            "Epoch 367/500\n",
            "\n",
            "Epoch 00367: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.0831 - acc: 1.0000 - val_loss: 0.7946 - val_acc: 0.6500\n",
            "Epoch 368/500\n",
            "\n",
            "Epoch 00368: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1100 - acc: 1.0000 - val_loss: 0.8002 - val_acc: 0.6500\n",
            "Epoch 369/500\n",
            "\n",
            "Epoch 00369: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.0948 - acc: 1.0000 - val_loss: 0.7902 - val_acc: 0.6500\n",
            "Epoch 370/500\n",
            "\n",
            "Epoch 00370: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.0904 - acc: 1.0000 - val_loss: 0.7940 - val_acc: 0.6500\n",
            "Epoch 371/500\n",
            "\n",
            "Epoch 00371: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.0712 - acc: 1.0000 - val_loss: 0.8302 - val_acc: 0.6500\n",
            "Epoch 372/500\n",
            "\n",
            "Epoch 00372: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.0911 - acc: 0.9833 - val_loss: 0.8928 - val_acc: 0.6000\n",
            "Epoch 373/500\n",
            "\n",
            "Epoch 00373: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.0719 - acc: 1.0000 - val_loss: 0.9284 - val_acc: 0.6000\n",
            "Epoch 374/500\n",
            "\n",
            "Epoch 00374: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1115 - acc: 1.0000 - val_loss: 0.8543 - val_acc: 0.6000\n",
            "Epoch 375/500\n",
            "\n",
            "Epoch 00375: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.0909 - acc: 0.9833 - val_loss: 0.8036 - val_acc: 0.6500\n",
            "Epoch 376/500\n",
            "\n",
            "Epoch 00376: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.0797 - acc: 1.0000 - val_loss: 0.8328 - val_acc: 0.6500\n",
            "Epoch 377/500\n",
            "\n",
            "Epoch 00377: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.0858 - acc: 1.0000 - val_loss: 0.8220 - val_acc: 0.6500\n",
            "Epoch 378/500\n",
            "\n",
            "Epoch 00378: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.0776 - acc: 1.0000 - val_loss: 0.8218 - val_acc: 0.6500\n",
            "Epoch 379/500\n",
            "\n",
            "Epoch 00379: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.0887 - acc: 1.0000 - val_loss: 0.8449 - val_acc: 0.6500\n",
            "Epoch 380/500\n",
            "\n",
            "Epoch 00380: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1095 - acc: 0.9833 - val_loss: 0.8447 - val_acc: 0.6000\n",
            "Epoch 381/500\n",
            "\n",
            "Epoch 00381: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.0869 - acc: 1.0000 - val_loss: 0.8347 - val_acc: 0.6000\n",
            "Epoch 382/500\n",
            "\n",
            "Epoch 00382: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.0924 - acc: 1.0000 - val_loss: 0.8281 - val_acc: 0.6000\n",
            "Epoch 383/500\n",
            "\n",
            "Epoch 00383: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.0739 - acc: 1.0000 - val_loss: 0.8113 - val_acc: 0.6000\n",
            "Epoch 384/500\n",
            "\n",
            "Epoch 00384: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.0879 - acc: 1.0000 - val_loss: 0.7747 - val_acc: 0.6500\n",
            "Epoch 385/500\n",
            "\n",
            "Epoch 00385: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.0791 - acc: 1.0000 - val_loss: 0.7509 - val_acc: 0.7000\n",
            "Epoch 386/500\n",
            "\n",
            "Epoch 00386: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1219 - acc: 0.9667 - val_loss: 0.7796 - val_acc: 0.6500\n",
            "Epoch 387/500\n",
            "\n",
            "Epoch 00387: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.0653 - acc: 1.0000 - val_loss: 0.8186 - val_acc: 0.6500\n",
            "Epoch 388/500\n",
            "\n",
            "Epoch 00388: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.0807 - acc: 1.0000 - val_loss: 0.8290 - val_acc: 0.6500\n",
            "Epoch 389/500\n",
            "\n",
            "Epoch 00389: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.0854 - acc: 1.0000 - val_loss: 0.8195 - val_acc: 0.6000\n",
            "Epoch 390/500\n",
            "\n",
            "Epoch 00390: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.0869 - acc: 1.0000 - val_loss: 0.8021 - val_acc: 0.7000\n",
            "Epoch 391/500\n",
            "\n",
            "Epoch 00391: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1031 - acc: 1.0000 - val_loss: 0.7972 - val_acc: 0.7000\n",
            "Epoch 392/500\n",
            "\n",
            "Epoch 00392: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.0932 - acc: 1.0000 - val_loss: 0.8458 - val_acc: 0.6500\n",
            "Epoch 393/500\n",
            "\n",
            "Epoch 00393: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.0658 - acc: 1.0000 - val_loss: 0.8553 - val_acc: 0.7000\n",
            "Epoch 394/500\n",
            "\n",
            "Epoch 00394: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.0952 - acc: 1.0000 - val_loss: 0.8637 - val_acc: 0.6500\n",
            "Epoch 395/500\n",
            "\n",
            "Epoch 00395: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.0977 - acc: 1.0000 - val_loss: 0.8790 - val_acc: 0.6000\n",
            "Epoch 396/500\n",
            "\n",
            "Epoch 00396: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.0888 - acc: 1.0000 - val_loss: 0.8206 - val_acc: 0.6000\n",
            "Epoch 397/500\n",
            "\n",
            "Epoch 00397: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.0851 - acc: 1.0000 - val_loss: 0.7973 - val_acc: 0.6500\n",
            "Epoch 398/500\n",
            "\n",
            "Epoch 00398: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.0934 - acc: 1.0000 - val_loss: 0.7870 - val_acc: 0.6500\n",
            "Epoch 399/500\n",
            "\n",
            "Epoch 00399: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.0863 - acc: 0.9833 - val_loss: 0.7917 - val_acc: 0.6500\n",
            "Epoch 400/500\n",
            "\n",
            "Epoch 00400: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.0719 - acc: 1.0000 - val_loss: 0.8012 - val_acc: 0.6500\n",
            "Epoch 401/500\n",
            "\n",
            "Epoch 00401: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.0717 - acc: 1.0000 - val_loss: 0.7940 - val_acc: 0.6500\n",
            "Epoch 402/500\n",
            "\n",
            "Epoch 00402: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.0760 - acc: 1.0000 - val_loss: 0.7808 - val_acc: 0.6000\n",
            "Epoch 403/500\n",
            "\n",
            "Epoch 00403: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.0902 - acc: 0.9833 - val_loss: 0.7722 - val_acc: 0.6000\n",
            "Epoch 404/500\n",
            "\n",
            "Epoch 00404: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.1033 - acc: 1.0000 - val_loss: 0.7445 - val_acc: 0.6000\n",
            "Epoch 405/500\n",
            "\n",
            "Epoch 00405: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.0592 - acc: 1.0000 - val_loss: 0.8029 - val_acc: 0.6000\n",
            "Epoch 406/500\n",
            "\n",
            "Epoch 00406: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.0983 - acc: 0.9833 - val_loss: 0.7729 - val_acc: 0.6000\n",
            "Epoch 407/500\n",
            "\n",
            "Epoch 00407: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.0761 - acc: 1.0000 - val_loss: 0.7386 - val_acc: 0.7000\n",
            "Epoch 408/500\n",
            "\n",
            "Epoch 00408: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.0747 - acc: 1.0000 - val_loss: 0.7300 - val_acc: 0.7000\n",
            "Epoch 409/500\n",
            "\n",
            "Epoch 00409: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.0872 - acc: 0.9833 - val_loss: 0.7261 - val_acc: 0.7000\n",
            "Epoch 410/500\n",
            "\n",
            "Epoch 00410: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.0762 - acc: 1.0000 - val_loss: 0.7043 - val_acc: 0.7000\n",
            "Epoch 411/500\n",
            "\n",
            "Epoch 00411: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.0695 - acc: 1.0000 - val_loss: 0.6926 - val_acc: 0.7000\n",
            "Epoch 412/500\n",
            "\n",
            "Epoch 00412: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.0955 - acc: 0.9833 - val_loss: 0.7233 - val_acc: 0.7000\n",
            "Epoch 413/500\n",
            "\n",
            "Epoch 00413: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.0850 - acc: 0.9833 - val_loss: 0.7623 - val_acc: 0.6500\n",
            "Epoch 414/500\n",
            "\n",
            "Epoch 00414: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.0725 - acc: 1.0000 - val_loss: 0.8395 - val_acc: 0.6000\n",
            "Epoch 415/500\n",
            "\n",
            "Epoch 00415: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.0681 - acc: 1.0000 - val_loss: 0.8793 - val_acc: 0.5000\n",
            "Epoch 416/500\n",
            "\n",
            "Epoch 00416: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.0770 - acc: 1.0000 - val_loss: 0.8639 - val_acc: 0.6000\n",
            "Epoch 417/500\n",
            "\n",
            "Epoch 00417: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.0955 - acc: 1.0000 - val_loss: 0.8008 - val_acc: 0.6500\n",
            "Epoch 418/500\n",
            "\n",
            "Epoch 00418: val_loss did not improve from 0.67954\n",
            "60/60 - 0s - loss: 0.0741 - acc: 0.9833 - val_loss: 0.6839 - val_acc: 0.6500\n",
            "Epoch 419/500\n",
            "\n",
            "Epoch 00419: val_loss improved from 0.67954 to 0.63318, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1061 - acc: 0.9833 - val_loss: 0.6332 - val_acc: 0.7000\n",
            "Epoch 420/500\n",
            "\n",
            "Epoch 00420: val_loss did not improve from 0.63318\n",
            "60/60 - 0s - loss: 0.0760 - acc: 1.0000 - val_loss: 0.6369 - val_acc: 0.7000\n",
            "Epoch 421/500\n",
            "\n",
            "Epoch 00421: val_loss did not improve from 0.63318\n",
            "60/60 - 0s - loss: 0.0872 - acc: 0.9833 - val_loss: 0.6943 - val_acc: 0.7000\n",
            "Epoch 422/500\n",
            "\n",
            "Epoch 00422: val_loss did not improve from 0.63318\n",
            "60/60 - 0s - loss: 0.0804 - acc: 1.0000 - val_loss: 0.7481 - val_acc: 0.6500\n",
            "Epoch 423/500\n",
            "\n",
            "Epoch 00423: val_loss did not improve from 0.63318\n",
            "60/60 - 0s - loss: 0.0787 - acc: 1.0000 - val_loss: 0.7764 - val_acc: 0.6000\n",
            "Epoch 424/500\n",
            "\n",
            "Epoch 00424: val_loss did not improve from 0.63318\n",
            "60/60 - 0s - loss: 0.0709 - acc: 1.0000 - val_loss: 0.7937 - val_acc: 0.6500\n",
            "Epoch 425/500\n",
            "\n",
            "Epoch 00425: val_loss did not improve from 0.63318\n",
            "60/60 - 0s - loss: 0.0877 - acc: 1.0000 - val_loss: 0.8340 - val_acc: 0.6000\n",
            "Epoch 426/500\n",
            "\n",
            "Epoch 00426: val_loss did not improve from 0.63318\n",
            "60/60 - 0s - loss: 0.0783 - acc: 1.0000 - val_loss: 0.8994 - val_acc: 0.6000\n",
            "Epoch 427/500\n",
            "\n",
            "Epoch 00427: val_loss did not improve from 0.63318\n",
            "60/60 - 0s - loss: 0.0903 - acc: 0.9667 - val_loss: 0.8914 - val_acc: 0.5500\n",
            "Epoch 428/500\n",
            "\n",
            "Epoch 00428: val_loss did not improve from 0.63318\n",
            "60/60 - 0s - loss: 0.0567 - acc: 1.0000 - val_loss: 0.8327 - val_acc: 0.6500\n",
            "Epoch 429/500\n",
            "\n",
            "Epoch 00429: val_loss did not improve from 0.63318\n",
            "60/60 - 0s - loss: 0.0834 - acc: 1.0000 - val_loss: 0.8018 - val_acc: 0.7000\n",
            "Epoch 430/500\n",
            "\n",
            "Epoch 00430: val_loss did not improve from 0.63318\n",
            "60/60 - 0s - loss: 0.0726 - acc: 1.0000 - val_loss: 0.7871 - val_acc: 0.7000\n",
            "Epoch 431/500\n",
            "\n",
            "Epoch 00431: val_loss did not improve from 0.63318\n",
            "60/60 - 0s - loss: 0.0720 - acc: 1.0000 - val_loss: 0.7973 - val_acc: 0.7000\n",
            "Epoch 432/500\n",
            "\n",
            "Epoch 00432: val_loss did not improve from 0.63318\n",
            "60/60 - 0s - loss: 0.0807 - acc: 0.9833 - val_loss: 0.7875 - val_acc: 0.7000\n",
            "Epoch 433/500\n",
            "\n",
            "Epoch 00433: val_loss did not improve from 0.63318\n",
            "60/60 - 0s - loss: 0.0954 - acc: 0.9833 - val_loss: 0.8003 - val_acc: 0.7000\n",
            "Epoch 434/500\n",
            "\n",
            "Epoch 00434: val_loss did not improve from 0.63318\n",
            "60/60 - 0s - loss: 0.0690 - acc: 1.0000 - val_loss: 0.8203 - val_acc: 0.6500\n",
            "Epoch 435/500\n",
            "\n",
            "Epoch 00435: val_loss did not improve from 0.63318\n",
            "60/60 - 0s - loss: 0.0745 - acc: 1.0000 - val_loss: 0.8237 - val_acc: 0.6500\n",
            "Epoch 436/500\n",
            "\n",
            "Epoch 00436: val_loss did not improve from 0.63318\n",
            "60/60 - 0s - loss: 0.0829 - acc: 1.0000 - val_loss: 0.8367 - val_acc: 0.5500\n",
            "Epoch 437/500\n",
            "\n",
            "Epoch 00437: val_loss did not improve from 0.63318\n",
            "60/60 - 0s - loss: 0.0671 - acc: 1.0000 - val_loss: 0.7999 - val_acc: 0.5500\n",
            "Epoch 438/500\n",
            "\n",
            "Epoch 00438: val_loss did not improve from 0.63318\n",
            "60/60 - 0s - loss: 0.0734 - acc: 1.0000 - val_loss: 0.7809 - val_acc: 0.6000\n",
            "Epoch 439/500\n",
            "\n",
            "Epoch 00439: val_loss did not improve from 0.63318\n",
            "60/60 - 0s - loss: 0.0687 - acc: 1.0000 - val_loss: 0.7869 - val_acc: 0.6000\n",
            "Epoch 440/500\n",
            "\n",
            "Epoch 00440: val_loss did not improve from 0.63318\n",
            "60/60 - 0s - loss: 0.0807 - acc: 0.9833 - val_loss: 0.8179 - val_acc: 0.6000\n",
            "Epoch 441/500\n",
            "\n",
            "Epoch 00441: val_loss did not improve from 0.63318\n",
            "60/60 - 0s - loss: 0.0812 - acc: 0.9833 - val_loss: 0.8418 - val_acc: 0.5500\n",
            "Epoch 442/500\n",
            "\n",
            "Epoch 00442: val_loss did not improve from 0.63318\n",
            "60/60 - 0s - loss: 0.0673 - acc: 1.0000 - val_loss: 0.8199 - val_acc: 0.6000\n",
            "Epoch 443/500\n",
            "\n",
            "Epoch 00443: val_loss did not improve from 0.63318\n",
            "60/60 - 0s - loss: 0.0462 - acc: 1.0000 - val_loss: 0.8054 - val_acc: 0.6000\n",
            "Epoch 444/500\n",
            "\n",
            "Epoch 00444: val_loss did not improve from 0.63318\n",
            "60/60 - 0s - loss: 0.0721 - acc: 1.0000 - val_loss: 0.7702 - val_acc: 0.6000\n",
            "Epoch 445/500\n",
            "\n",
            "Epoch 00445: val_loss did not improve from 0.63318\n",
            "60/60 - 0s - loss: 0.0595 - acc: 1.0000 - val_loss: 0.7743 - val_acc: 0.6000\n",
            "Epoch 446/500\n",
            "\n",
            "Epoch 00446: val_loss did not improve from 0.63318\n",
            "60/60 - 0s - loss: 0.0576 - acc: 0.9833 - val_loss: 0.7715 - val_acc: 0.6500\n",
            "Epoch 447/500\n",
            "\n",
            "Epoch 00447: val_loss did not improve from 0.63318\n",
            "60/60 - 0s - loss: 0.0780 - acc: 1.0000 - val_loss: 0.7716 - val_acc: 0.6500\n",
            "Epoch 448/500\n",
            "\n",
            "Epoch 00448: val_loss did not improve from 0.63318\n",
            "60/60 - 0s - loss: 0.0661 - acc: 1.0000 - val_loss: 0.8191 - val_acc: 0.5500\n",
            "Epoch 449/500\n",
            "\n",
            "Epoch 00449: val_loss did not improve from 0.63318\n",
            "60/60 - 0s - loss: 0.0693 - acc: 1.0000 - val_loss: 0.8102 - val_acc: 0.6500\n",
            "Epoch 450/500\n",
            "\n",
            "Epoch 00450: val_loss did not improve from 0.63318\n",
            "60/60 - 0s - loss: 0.0679 - acc: 1.0000 - val_loss: 0.7670 - val_acc: 0.6500\n",
            "Epoch 451/500\n",
            "\n",
            "Epoch 00451: val_loss did not improve from 0.63318\n",
            "60/60 - 0s - loss: 0.0669 - acc: 1.0000 - val_loss: 0.7266 - val_acc: 0.6500\n",
            "Epoch 452/500\n",
            "\n",
            "Epoch 00452: val_loss did not improve from 0.63318\n",
            "60/60 - 0s - loss: 0.0760 - acc: 0.9833 - val_loss: 0.7359 - val_acc: 0.6500\n",
            "Epoch 453/500\n",
            "\n",
            "Epoch 00453: val_loss did not improve from 0.63318\n",
            "60/60 - 0s - loss: 0.0877 - acc: 0.9833 - val_loss: 0.8068 - val_acc: 0.6000\n",
            "Epoch 454/500\n",
            "\n",
            "Epoch 00454: val_loss did not improve from 0.63318\n",
            "60/60 - 0s - loss: 0.0612 - acc: 1.0000 - val_loss: 0.8349 - val_acc: 0.6000\n",
            "Epoch 455/500\n",
            "\n",
            "Epoch 00455: val_loss did not improve from 0.63318\n",
            "60/60 - 0s - loss: 0.0905 - acc: 1.0000 - val_loss: 0.8779 - val_acc: 0.6000\n",
            "Epoch 456/500\n",
            "\n",
            "Epoch 00456: val_loss did not improve from 0.63318\n",
            "60/60 - 0s - loss: 0.0701 - acc: 1.0000 - val_loss: 0.9358 - val_acc: 0.6000\n",
            "Epoch 457/500\n",
            "\n",
            "Epoch 00457: val_loss did not improve from 0.63318\n",
            "60/60 - 0s - loss: 0.0789 - acc: 0.9833 - val_loss: 0.8975 - val_acc: 0.5500\n",
            "Epoch 458/500\n",
            "\n",
            "Epoch 00458: val_loss did not improve from 0.63318\n",
            "60/60 - 0s - loss: 0.0640 - acc: 1.0000 - val_loss: 0.8642 - val_acc: 0.5500\n",
            "Epoch 459/500\n",
            "\n",
            "Epoch 00459: val_loss did not improve from 0.63318\n",
            "60/60 - 0s - loss: 0.0667 - acc: 1.0000 - val_loss: 0.8396 - val_acc: 0.6000\n",
            "Epoch 460/500\n",
            "\n",
            "Epoch 00460: val_loss did not improve from 0.63318\n",
            "60/60 - 0s - loss: 0.0645 - acc: 1.0000 - val_loss: 0.8234 - val_acc: 0.6000\n",
            "Epoch 461/500\n",
            "\n",
            "Epoch 00461: val_loss did not improve from 0.63318\n",
            "60/60 - 0s - loss: 0.0694 - acc: 1.0000 - val_loss: 0.7949 - val_acc: 0.6000\n",
            "Epoch 462/500\n",
            "\n",
            "Epoch 00462: val_loss did not improve from 0.63318\n",
            "60/60 - 0s - loss: 0.0557 - acc: 1.0000 - val_loss: 0.7584 - val_acc: 0.6000\n",
            "Epoch 463/500\n",
            "\n",
            "Epoch 00463: val_loss did not improve from 0.63318\n",
            "60/60 - 0s - loss: 0.0648 - acc: 1.0000 - val_loss: 0.7515 - val_acc: 0.6500\n",
            "Epoch 464/500\n",
            "\n",
            "Epoch 00464: val_loss did not improve from 0.63318\n",
            "60/60 - 0s - loss: 0.0522 - acc: 1.0000 - val_loss: 0.7391 - val_acc: 0.6500\n",
            "Epoch 465/500\n",
            "\n",
            "Epoch 00465: val_loss did not improve from 0.63318\n",
            "60/60 - 0s - loss: 0.0655 - acc: 0.9833 - val_loss: 0.7479 - val_acc: 0.6500\n",
            "Epoch 466/500\n",
            "\n",
            "Epoch 00466: val_loss did not improve from 0.63318\n",
            "60/60 - 0s - loss: 0.0584 - acc: 1.0000 - val_loss: 0.7438 - val_acc: 0.6500\n",
            "Epoch 467/500\n",
            "\n",
            "Epoch 00467: val_loss did not improve from 0.63318\n",
            "60/60 - 0s - loss: 0.0663 - acc: 1.0000 - val_loss: 0.7182 - val_acc: 0.7000\n",
            "Epoch 468/500\n",
            "\n",
            "Epoch 00468: val_loss did not improve from 0.63318\n",
            "60/60 - 0s - loss: 0.0968 - acc: 0.9833 - val_loss: 0.7181 - val_acc: 0.7000\n",
            "Epoch 469/500\n",
            "\n",
            "Epoch 00469: val_loss did not improve from 0.63318\n",
            "60/60 - 0s - loss: 0.0585 - acc: 1.0000 - val_loss: 0.7120 - val_acc: 0.7000\n",
            "Epoch 470/500\n",
            "\n",
            "Epoch 00470: val_loss did not improve from 0.63318\n",
            "60/60 - 0s - loss: 0.0686 - acc: 1.0000 - val_loss: 0.7154 - val_acc: 0.7000\n",
            "Epoch 471/500\n",
            "\n",
            "Epoch 00471: val_loss did not improve from 0.63318\n",
            "60/60 - 0s - loss: 0.0535 - acc: 1.0000 - val_loss: 0.7401 - val_acc: 0.6500\n",
            "Epoch 472/500\n",
            "\n",
            "Epoch 00472: val_loss did not improve from 0.63318\n",
            "60/60 - 0s - loss: 0.0539 - acc: 1.0000 - val_loss: 0.7724 - val_acc: 0.7000\n",
            "Epoch 473/500\n",
            "\n",
            "Epoch 00473: val_loss did not improve from 0.63318\n",
            "60/60 - 0s - loss: 0.0743 - acc: 1.0000 - val_loss: 0.7703 - val_acc: 0.7000\n",
            "Epoch 474/500\n",
            "\n",
            "Epoch 00474: val_loss did not improve from 0.63318\n",
            "60/60 - 0s - loss: 0.0511 - acc: 1.0000 - val_loss: 0.7195 - val_acc: 0.6500\n",
            "Epoch 475/500\n",
            "\n",
            "Epoch 00475: val_loss did not improve from 0.63318\n",
            "60/60 - 0s - loss: 0.0653 - acc: 0.9833 - val_loss: 0.6870 - val_acc: 0.6500\n",
            "Epoch 476/500\n",
            "\n",
            "Epoch 00476: val_loss did not improve from 0.63318\n",
            "60/60 - 0s - loss: 0.1095 - acc: 0.9833 - val_loss: 0.6591 - val_acc: 0.6500\n",
            "Epoch 477/500\n",
            "\n",
            "Epoch 00477: val_loss did not improve from 0.63318\n",
            "60/60 - 0s - loss: 0.0572 - acc: 1.0000 - val_loss: 0.6915 - val_acc: 0.6500\n",
            "Epoch 478/500\n",
            "\n",
            "Epoch 00478: val_loss did not improve from 0.63318\n",
            "60/60 - 0s - loss: 0.0806 - acc: 0.9833 - val_loss: 0.6844 - val_acc: 0.7000\n",
            "Epoch 479/500\n",
            "\n",
            "Epoch 00479: val_loss did not improve from 0.63318\n",
            "60/60 - 0s - loss: 0.0876 - acc: 1.0000 - val_loss: 0.6726 - val_acc: 0.6500\n",
            "Epoch 480/500\n",
            "\n",
            "Epoch 00480: val_loss did not improve from 0.63318\n",
            "60/60 - 0s - loss: 0.0530 - acc: 1.0000 - val_loss: 0.6799 - val_acc: 0.7000\n",
            "Epoch 481/500\n",
            "\n",
            "Epoch 00481: val_loss did not improve from 0.63318\n",
            "60/60 - 0s - loss: 0.0945 - acc: 1.0000 - val_loss: 0.6888 - val_acc: 0.7000\n",
            "Epoch 482/500\n",
            "\n",
            "Epoch 00482: val_loss did not improve from 0.63318\n",
            "60/60 - 0s - loss: 0.0563 - acc: 1.0000 - val_loss: 0.6843 - val_acc: 0.7000\n",
            "Epoch 483/500\n",
            "\n",
            "Epoch 00483: val_loss did not improve from 0.63318\n",
            "60/60 - 0s - loss: 0.0510 - acc: 1.0000 - val_loss: 0.6970 - val_acc: 0.7000\n",
            "Epoch 484/500\n",
            "\n",
            "Epoch 00484: val_loss did not improve from 0.63318\n",
            "60/60 - 0s - loss: 0.0534 - acc: 1.0000 - val_loss: 0.6969 - val_acc: 0.7000\n",
            "Epoch 485/500\n",
            "\n",
            "Epoch 00485: val_loss did not improve from 0.63318\n",
            "60/60 - 0s - loss: 0.0518 - acc: 1.0000 - val_loss: 0.6938 - val_acc: 0.7000\n",
            "Epoch 486/500\n",
            "\n",
            "Epoch 00486: val_loss did not improve from 0.63318\n",
            "60/60 - 0s - loss: 0.0767 - acc: 1.0000 - val_loss: 0.7098 - val_acc: 0.7000\n",
            "Epoch 487/500\n",
            "\n",
            "Epoch 00487: val_loss did not improve from 0.63318\n",
            "60/60 - 0s - loss: 0.0603 - acc: 1.0000 - val_loss: 0.7416 - val_acc: 0.7000\n",
            "Epoch 488/500\n",
            "\n",
            "Epoch 00488: val_loss did not improve from 0.63318\n",
            "60/60 - 0s - loss: 0.0570 - acc: 1.0000 - val_loss: 0.7613 - val_acc: 0.6500\n",
            "Epoch 489/500\n",
            "\n",
            "Epoch 00489: val_loss did not improve from 0.63318\n",
            "60/60 - 0s - loss: 0.0624 - acc: 1.0000 - val_loss: 0.7449 - val_acc: 0.6500\n",
            "Epoch 490/500\n",
            "\n",
            "Epoch 00490: val_loss did not improve from 0.63318\n",
            "60/60 - 0s - loss: 0.0702 - acc: 1.0000 - val_loss: 0.7769 - val_acc: 0.6500\n",
            "Epoch 491/500\n",
            "\n",
            "Epoch 00491: val_loss did not improve from 0.63318\n",
            "60/60 - 0s - loss: 0.0726 - acc: 1.0000 - val_loss: 0.7184 - val_acc: 0.6500\n",
            "Epoch 492/500\n",
            "\n",
            "Epoch 00492: val_loss did not improve from 0.63318\n",
            "60/60 - 0s - loss: 0.0727 - acc: 1.0000 - val_loss: 0.6920 - val_acc: 0.6500\n",
            "Epoch 493/500\n",
            "\n",
            "Epoch 00493: val_loss did not improve from 0.63318\n",
            "60/60 - 0s - loss: 0.0757 - acc: 1.0000 - val_loss: 0.7393 - val_acc: 0.6500\n",
            "Epoch 494/500\n",
            "\n",
            "Epoch 00494: val_loss did not improve from 0.63318\n",
            "60/60 - 0s - loss: 0.0455 - acc: 1.0000 - val_loss: 0.7584 - val_acc: 0.6500\n",
            "Epoch 495/500\n",
            "\n",
            "Epoch 00495: val_loss did not improve from 0.63318\n",
            "60/60 - 0s - loss: 0.0994 - acc: 0.9667 - val_loss: 0.7749 - val_acc: 0.6500\n",
            "Epoch 496/500\n",
            "\n",
            "Epoch 00496: val_loss did not improve from 0.63318\n",
            "60/60 - 0s - loss: 0.0686 - acc: 1.0000 - val_loss: 0.7364 - val_acc: 0.6500\n",
            "Epoch 497/500\n",
            "\n",
            "Epoch 00497: val_loss did not improve from 0.63318\n",
            "60/60 - 0s - loss: 0.0754 - acc: 0.9833 - val_loss: 0.7169 - val_acc: 0.7000\n",
            "Epoch 498/500\n",
            "\n",
            "Epoch 00498: val_loss did not improve from 0.63318\n",
            "60/60 - 0s - loss: 0.0875 - acc: 0.9833 - val_loss: 0.7843 - val_acc: 0.6500\n",
            "Epoch 499/500\n",
            "\n",
            "Epoch 00499: val_loss did not improve from 0.63318\n",
            "60/60 - 0s - loss: 0.0697 - acc: 1.0000 - val_loss: 0.8294 - val_acc: 0.7000\n",
            "Epoch 500/500\n",
            "\n",
            "Epoch 00500: val_loss did not improve from 0.63318\n",
            "60/60 - 0s - loss: 0.0768 - acc: 1.0000 - val_loss: 0.7644 - val_acc: 0.7000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n9y+9W9Y4EbBdZwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "# Evaluate on test data\n",
            "40/40 [==============================] - 0s 2ms/sample - loss: 0.4580 - acc: 0.7750\n",
            "test loss, test acc: [0.45802246627863497, 0.775]\n",
            "EEG_Deep/Data2A/parsed_P08T.mat\n",
            "Filtering of Training Data Finished\n",
            "EEG_Deep/Data2A/parsed_P08E.mat\n",
            "Filtering of Testing Data Finished\n",
            "(1, 2)\n",
            "Finding labels in training data\n",
            "Finding labels in testing data\n",
            "[1 2 1 2 2 1 2 1 2 2 2 1 1 1 1 1 1 1 1 1]\n",
            "(60, 12, 1536)\n",
            "(60,)\n",
            "(20, 12, 1536)\n",
            "(20,)\n",
            "[[1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]]\n",
            "X_train shape: (60, 1, 12, 1536)\n",
            "60 train samples\n",
            "20 val samples\n",
            "(40, 12, 1536)\n",
            "(40, 1)\n",
            "X_train shape: (40, 1, 12, 1536)\n",
            "40 train samples\n",
            "Train on 60 samples, validate on 20 samples\n",
            "Epoch 1/500\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.70890, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 1s - loss: 0.7431 - acc: 0.4833 - val_loss: 0.7089 - val_acc: 0.3000\n",
            "Epoch 2/500\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.70890\n",
            "60/60 - 0s - loss: 0.6678 - acc: 0.6667 - val_loss: 0.7176 - val_acc: 0.3500\n",
            "Epoch 3/500\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.70890\n",
            "60/60 - 0s - loss: 0.6539 - acc: 0.6333 - val_loss: 0.7210 - val_acc: 0.3500\n",
            "Epoch 4/500\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.70890\n",
            "60/60 - 0s - loss: 0.6572 - acc: 0.6500 - val_loss: 0.7253 - val_acc: 0.3500\n",
            "Epoch 5/500\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.70890\n",
            "60/60 - 0s - loss: 0.6378 - acc: 0.6000 - val_loss: 0.7273 - val_acc: 0.3500\n",
            "Epoch 6/500\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.70890\n",
            "60/60 - 0s - loss: 0.6145 - acc: 0.6667 - val_loss: 0.7290 - val_acc: 0.2500\n",
            "Epoch 7/500\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.70890\n",
            "60/60 - 0s - loss: 0.5875 - acc: 0.6833 - val_loss: 0.7315 - val_acc: 0.2500\n",
            "Epoch 8/500\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.70890\n",
            "60/60 - 0s - loss: 0.5609 - acc: 0.7833 - val_loss: 0.7337 - val_acc: 0.2500\n",
            "Epoch 9/500\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.70890\n",
            "60/60 - 0s - loss: 0.5565 - acc: 0.8000 - val_loss: 0.7364 - val_acc: 0.2500\n",
            "Epoch 10/500\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.70890\n",
            "60/60 - 0s - loss: 0.5557 - acc: 0.8500 - val_loss: 0.7387 - val_acc: 0.2000\n",
            "Epoch 11/500\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.70890\n",
            "60/60 - 0s - loss: 0.5411 - acc: 0.8167 - val_loss: 0.7415 - val_acc: 0.2500\n",
            "Epoch 12/500\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.70890\n",
            "60/60 - 0s - loss: 0.5227 - acc: 0.8500 - val_loss: 0.7455 - val_acc: 0.2500\n",
            "Epoch 13/500\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.70890\n",
            "60/60 - 0s - loss: 0.5121 - acc: 0.8833 - val_loss: 0.7496 - val_acc: 0.2500\n",
            "Epoch 14/500\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.70890\n",
            "60/60 - 0s - loss: 0.5128 - acc: 0.8333 - val_loss: 0.7521 - val_acc: 0.2500\n",
            "Epoch 15/500\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.70890\n",
            "60/60 - 0s - loss: 0.5016 - acc: 0.8500 - val_loss: 0.7553 - val_acc: 0.2500\n",
            "Epoch 16/500\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.70890\n",
            "60/60 - 0s - loss: 0.4897 - acc: 0.7833 - val_loss: 0.7584 - val_acc: 0.2500\n",
            "Epoch 17/500\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.70890\n",
            "60/60 - 0s - loss: 0.5131 - acc: 0.8167 - val_loss: 0.7622 - val_acc: 0.3000\n",
            "Epoch 18/500\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.70890\n",
            "60/60 - 0s - loss: 0.4811 - acc: 0.8833 - val_loss: 0.7642 - val_acc: 0.3000\n",
            "Epoch 19/500\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.70890\n",
            "60/60 - 0s - loss: 0.4628 - acc: 0.9167 - val_loss: 0.7688 - val_acc: 0.3000\n",
            "Epoch 20/500\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.70890\n",
            "60/60 - 0s - loss: 0.4561 - acc: 0.8833 - val_loss: 0.7715 - val_acc: 0.3000\n",
            "Epoch 21/500\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.70890\n",
            "60/60 - 0s - loss: 0.4695 - acc: 0.8833 - val_loss: 0.7741 - val_acc: 0.3000\n",
            "Epoch 22/500\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.70890\n",
            "60/60 - 0s - loss: 0.4412 - acc: 0.8833 - val_loss: 0.7763 - val_acc: 0.3000\n",
            "Epoch 23/500\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.70890\n",
            "60/60 - 0s - loss: 0.4570 - acc: 0.8333 - val_loss: 0.7772 - val_acc: 0.3000\n",
            "Epoch 24/500\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.70890\n",
            "60/60 - 0s - loss: 0.4658 - acc: 0.8500 - val_loss: 0.7807 - val_acc: 0.3000\n",
            "Epoch 25/500\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.70890\n",
            "60/60 - 0s - loss: 0.4121 - acc: 0.9167 - val_loss: 0.7827 - val_acc: 0.3000\n",
            "Epoch 26/500\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.70890\n",
            "60/60 - 0s - loss: 0.4405 - acc: 0.9000 - val_loss: 0.7827 - val_acc: 0.3000\n",
            "Epoch 27/500\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.70890\n",
            "60/60 - 0s - loss: 0.4443 - acc: 0.9333 - val_loss: 0.7833 - val_acc: 0.3000\n",
            "Epoch 28/500\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.70890\n",
            "60/60 - 0s - loss: 0.4546 - acc: 0.8333 - val_loss: 0.7802 - val_acc: 0.3000\n",
            "Epoch 29/500\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.70890\n",
            "60/60 - 0s - loss: 0.4161 - acc: 0.9167 - val_loss: 0.7796 - val_acc: 0.3000\n",
            "Epoch 30/500\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.70890\n",
            "60/60 - 0s - loss: 0.4464 - acc: 0.8500 - val_loss: 0.7803 - val_acc: 0.3000\n",
            "Epoch 31/500\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.70890\n",
            "60/60 - 0s - loss: 0.4285 - acc: 0.8500 - val_loss: 0.7786 - val_acc: 0.3000\n",
            "Epoch 32/500\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.70890\n",
            "60/60 - 0s - loss: 0.4598 - acc: 0.8833 - val_loss: 0.7774 - val_acc: 0.3000\n",
            "Epoch 33/500\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.70890\n",
            "60/60 - 0s - loss: 0.4009 - acc: 0.9333 - val_loss: 0.7787 - val_acc: 0.3000\n",
            "Epoch 34/500\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.70890\n",
            "60/60 - 0s - loss: 0.4025 - acc: 0.9000 - val_loss: 0.7786 - val_acc: 0.3000\n",
            "Epoch 35/500\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.70890\n",
            "60/60 - 0s - loss: 0.4114 - acc: 0.8833 - val_loss: 0.7779 - val_acc: 0.3000\n",
            "Epoch 36/500\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.70890\n",
            "60/60 - 0s - loss: 0.3823 - acc: 0.9667 - val_loss: 0.7764 - val_acc: 0.3000\n",
            "Epoch 37/500\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.70890\n",
            "60/60 - 0s - loss: 0.4086 - acc: 0.9000 - val_loss: 0.7785 - val_acc: 0.3000\n",
            "Epoch 38/500\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.70890\n",
            "60/60 - 0s - loss: 0.4010 - acc: 0.9167 - val_loss: 0.7775 - val_acc: 0.3000\n",
            "Epoch 39/500\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.70890\n",
            "60/60 - 0s - loss: 0.4042 - acc: 0.9333 - val_loss: 0.7764 - val_acc: 0.3000\n",
            "Epoch 40/500\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.70890\n",
            "60/60 - 0s - loss: 0.3804 - acc: 0.9333 - val_loss: 0.7734 - val_acc: 0.3500\n",
            "Epoch 41/500\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.70890\n",
            "60/60 - 0s - loss: 0.3692 - acc: 0.9500 - val_loss: 0.7757 - val_acc: 0.3000\n",
            "Epoch 42/500\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.70890\n",
            "60/60 - 0s - loss: 0.3737 - acc: 0.8833 - val_loss: 0.7756 - val_acc: 0.3000\n",
            "Epoch 43/500\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.70890\n",
            "60/60 - 0s - loss: 0.3655 - acc: 0.9333 - val_loss: 0.7709 - val_acc: 0.2500\n",
            "Epoch 44/500\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.70890\n",
            "60/60 - 0s - loss: 0.3604 - acc: 0.9500 - val_loss: 0.7682 - val_acc: 0.2500\n",
            "Epoch 45/500\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.70890\n",
            "60/60 - 0s - loss: 0.3667 - acc: 0.9333 - val_loss: 0.7637 - val_acc: 0.3000\n",
            "Epoch 46/500\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.70890\n",
            "60/60 - 0s - loss: 0.3728 - acc: 0.9333 - val_loss: 0.7632 - val_acc: 0.3000\n",
            "Epoch 47/500\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.70890\n",
            "60/60 - 0s - loss: 0.3878 - acc: 0.8667 - val_loss: 0.7634 - val_acc: 0.3000\n",
            "Epoch 48/500\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.70890\n",
            "60/60 - 0s - loss: 0.3793 - acc: 0.8833 - val_loss: 0.7588 - val_acc: 0.3000\n",
            "Epoch 49/500\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.70890\n",
            "60/60 - 0s - loss: 0.3359 - acc: 0.9667 - val_loss: 0.7588 - val_acc: 0.4500\n",
            "Epoch 50/500\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.70890\n",
            "60/60 - 0s - loss: 0.3416 - acc: 0.9333 - val_loss: 0.7606 - val_acc: 0.4500\n",
            "Epoch 51/500\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.70890\n",
            "60/60 - 0s - loss: 0.3257 - acc: 0.9333 - val_loss: 0.7635 - val_acc: 0.4000\n",
            "Epoch 52/500\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.70890\n",
            "60/60 - 0s - loss: 0.3694 - acc: 0.9167 - val_loss: 0.7605 - val_acc: 0.4000\n",
            "Epoch 53/500\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.70890\n",
            "60/60 - 0s - loss: 0.3283 - acc: 0.9333 - val_loss: 0.7593 - val_acc: 0.3500\n",
            "Epoch 54/500\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.70890\n",
            "60/60 - 0s - loss: 0.3247 - acc: 0.9500 - val_loss: 0.7552 - val_acc: 0.4500\n",
            "Epoch 55/500\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.70890\n",
            "60/60 - 0s - loss: 0.3530 - acc: 0.9000 - val_loss: 0.7566 - val_acc: 0.4500\n",
            "Epoch 56/500\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.70890\n",
            "60/60 - 0s - loss: 0.3075 - acc: 1.0000 - val_loss: 0.7538 - val_acc: 0.4500\n",
            "Epoch 57/500\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.70890\n",
            "60/60 - 0s - loss: 0.3364 - acc: 0.9667 - val_loss: 0.7519 - val_acc: 0.5000\n",
            "Epoch 58/500\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.70890\n",
            "60/60 - 0s - loss: 0.3032 - acc: 0.9500 - val_loss: 0.7573 - val_acc: 0.5000\n",
            "Epoch 59/500\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.70890\n",
            "60/60 - 0s - loss: 0.3107 - acc: 0.9833 - val_loss: 0.7611 - val_acc: 0.4500\n",
            "Epoch 60/500\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.70890\n",
            "60/60 - 0s - loss: 0.2987 - acc: 0.9500 - val_loss: 0.7675 - val_acc: 0.4000\n",
            "Epoch 61/500\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.70890\n",
            "60/60 - 0s - loss: 0.3357 - acc: 0.9500 - val_loss: 0.7609 - val_acc: 0.5000\n",
            "Epoch 62/500\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.70890\n",
            "60/60 - 0s - loss: 0.2935 - acc: 0.9667 - val_loss: 0.7622 - val_acc: 0.5000\n",
            "Epoch 63/500\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.70890\n",
            "60/60 - 0s - loss: 0.2803 - acc: 0.9500 - val_loss: 0.7588 - val_acc: 0.5500\n",
            "Epoch 64/500\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.70890\n",
            "60/60 - 0s - loss: 0.2907 - acc: 0.9667 - val_loss: 0.7587 - val_acc: 0.6000\n",
            "Epoch 65/500\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.70890\n",
            "60/60 - 0s - loss: 0.2973 - acc: 0.9667 - val_loss: 0.7587 - val_acc: 0.5500\n",
            "Epoch 66/500\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.70890\n",
            "60/60 - 0s - loss: 0.2941 - acc: 1.0000 - val_loss: 0.7545 - val_acc: 0.6000\n",
            "Epoch 67/500\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.70890\n",
            "60/60 - 0s - loss: 0.3208 - acc: 0.9667 - val_loss: 0.7484 - val_acc: 0.6000\n",
            "Epoch 68/500\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.70890\n",
            "60/60 - 0s - loss: 0.3008 - acc: 0.9667 - val_loss: 0.7469 - val_acc: 0.6000\n",
            "Epoch 69/500\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.70890\n",
            "60/60 - 0s - loss: 0.2999 - acc: 0.9333 - val_loss: 0.7464 - val_acc: 0.6000\n",
            "Epoch 70/500\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.70890\n",
            "60/60 - 0s - loss: 0.3087 - acc: 0.9333 - val_loss: 0.7424 - val_acc: 0.6500\n",
            "Epoch 71/500\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.70890\n",
            "60/60 - 0s - loss: 0.2927 - acc: 0.9500 - val_loss: 0.7371 - val_acc: 0.6500\n",
            "Epoch 72/500\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.70890\n",
            "60/60 - 0s - loss: 0.2523 - acc: 0.9667 - val_loss: 0.7328 - val_acc: 0.6500\n",
            "Epoch 73/500\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.70890\n",
            "60/60 - 0s - loss: 0.2700 - acc: 1.0000 - val_loss: 0.7242 - val_acc: 0.6500\n",
            "Epoch 74/500\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.70890\n",
            "60/60 - 0s - loss: 0.2801 - acc: 0.9500 - val_loss: 0.7166 - val_acc: 0.6500\n",
            "Epoch 75/500\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.70890\n",
            "60/60 - 0s - loss: 0.2732 - acc: 0.9833 - val_loss: 0.7104 - val_acc: 0.6500\n",
            "Epoch 76/500\n",
            "\n",
            "Epoch 00076: val_loss improved from 0.70890 to 0.70578, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2629 - acc: 0.9500 - val_loss: 0.7058 - val_acc: 0.6500\n",
            "Epoch 77/500\n",
            "\n",
            "Epoch 00077: val_loss improved from 0.70578 to 0.70574, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2780 - acc: 0.9500 - val_loss: 0.7057 - val_acc: 0.7000\n",
            "Epoch 78/500\n",
            "\n",
            "Epoch 00078: val_loss improved from 0.70574 to 0.70413, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2975 - acc: 0.9500 - val_loss: 0.7041 - val_acc: 0.7000\n",
            "Epoch 79/500\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.2674 - acc: 0.9667 - val_loss: 0.7066 - val_acc: 0.7000\n",
            "Epoch 80/500\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.2451 - acc: 0.9667 - val_loss: 0.7143 - val_acc: 0.7000\n",
            "Epoch 81/500\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.2618 - acc: 0.9667 - val_loss: 0.7217 - val_acc: 0.7000\n",
            "Epoch 82/500\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.2168 - acc: 0.9667 - val_loss: 0.7244 - val_acc: 0.7000\n",
            "Epoch 83/500\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.2262 - acc: 0.9833 - val_loss: 0.7262 - val_acc: 0.7000\n",
            "Epoch 84/500\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.2759 - acc: 0.9500 - val_loss: 0.7291 - val_acc: 0.7000\n",
            "Epoch 85/500\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.2421 - acc: 0.9667 - val_loss: 0.7347 - val_acc: 0.7000\n",
            "Epoch 86/500\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.2807 - acc: 0.9667 - val_loss: 0.7389 - val_acc: 0.7000\n",
            "Epoch 87/500\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.2626 - acc: 0.9833 - val_loss: 0.7398 - val_acc: 0.7000\n",
            "Epoch 88/500\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.2428 - acc: 0.9500 - val_loss: 0.7404 - val_acc: 0.6500\n",
            "Epoch 89/500\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.2375 - acc: 0.9667 - val_loss: 0.7434 - val_acc: 0.6500\n",
            "Epoch 90/500\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.2265 - acc: 0.9833 - val_loss: 0.7518 - val_acc: 0.6500\n",
            "Epoch 91/500\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.2597 - acc: 0.9833 - val_loss: 0.7627 - val_acc: 0.6500\n",
            "Epoch 92/500\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.2816 - acc: 0.9333 - val_loss: 0.7700 - val_acc: 0.6500\n",
            "Epoch 93/500\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.2418 - acc: 0.9500 - val_loss: 0.7679 - val_acc: 0.6500\n",
            "Epoch 94/500\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.2482 - acc: 0.9500 - val_loss: 0.7627 - val_acc: 0.6500\n",
            "Epoch 95/500\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.2356 - acc: 0.9500 - val_loss: 0.7540 - val_acc: 0.6500\n",
            "Epoch 96/500\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.2378 - acc: 0.9500 - val_loss: 0.7490 - val_acc: 0.6500\n",
            "Epoch 97/500\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.2423 - acc: 0.9500 - val_loss: 0.7507 - val_acc: 0.6500\n",
            "Epoch 98/500\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.2473 - acc: 0.9333 - val_loss: 0.7551 - val_acc: 0.6500\n",
            "Epoch 99/500\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.2420 - acc: 0.9333 - val_loss: 0.7580 - val_acc: 0.7000\n",
            "Epoch 100/500\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.2224 - acc: 0.9667 - val_loss: 0.7724 - val_acc: 0.7000\n",
            "Epoch 101/500\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.2439 - acc: 0.9667 - val_loss: 0.7776 - val_acc: 0.7000\n",
            "Epoch 102/500\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.2197 - acc: 0.9833 - val_loss: 0.7788 - val_acc: 0.7000\n",
            "Epoch 103/500\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.2184 - acc: 0.9500 - val_loss: 0.7778 - val_acc: 0.7000\n",
            "Epoch 104/500\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.2234 - acc: 1.0000 - val_loss: 0.7846 - val_acc: 0.7000\n",
            "Epoch 105/500\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.2122 - acc: 1.0000 - val_loss: 0.7931 - val_acc: 0.7000\n",
            "Epoch 106/500\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.1966 - acc: 0.9833 - val_loss: 0.7954 - val_acc: 0.7000\n",
            "Epoch 107/500\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.1962 - acc: 1.0000 - val_loss: 0.8023 - val_acc: 0.7000\n",
            "Epoch 108/500\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.2318 - acc: 0.9333 - val_loss: 0.8224 - val_acc: 0.7000\n",
            "Epoch 109/500\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.2295 - acc: 0.9667 - val_loss: 0.8208 - val_acc: 0.7000\n",
            "Epoch 110/500\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.2090 - acc: 0.9667 - val_loss: 0.8021 - val_acc: 0.7000\n",
            "Epoch 111/500\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.2016 - acc: 0.9667 - val_loss: 0.7775 - val_acc: 0.7000\n",
            "Epoch 112/500\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.2244 - acc: 0.9667 - val_loss: 0.7627 - val_acc: 0.7000\n",
            "Epoch 113/500\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.1847 - acc: 0.9833 - val_loss: 0.7544 - val_acc: 0.7000\n",
            "Epoch 114/500\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.2503 - acc: 0.9833 - val_loss: 0.7555 - val_acc: 0.6500\n",
            "Epoch 115/500\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.1873 - acc: 1.0000 - val_loss: 0.7614 - val_acc: 0.6500\n",
            "Epoch 116/500\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.2112 - acc: 0.9667 - val_loss: 0.7777 - val_acc: 0.6500\n",
            "Epoch 117/500\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.2233 - acc: 0.9500 - val_loss: 0.7894 - val_acc: 0.6500\n",
            "Epoch 118/500\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.2004 - acc: 0.9667 - val_loss: 0.7989 - val_acc: 0.6500\n",
            "Epoch 119/500\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.2154 - acc: 0.9833 - val_loss: 0.8179 - val_acc: 0.6500\n",
            "Epoch 120/500\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.1741 - acc: 0.9667 - val_loss: 0.8389 - val_acc: 0.6500\n",
            "Epoch 121/500\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.1929 - acc: 0.9667 - val_loss: 0.8423 - val_acc: 0.6500\n",
            "Epoch 122/500\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.2594 - acc: 0.9167 - val_loss: 0.8117 - val_acc: 0.7000\n",
            "Epoch 123/500\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.1750 - acc: 0.9833 - val_loss: 0.7723 - val_acc: 0.7500\n",
            "Epoch 124/500\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.2078 - acc: 0.9500 - val_loss: 0.7556 - val_acc: 0.7000\n",
            "Epoch 125/500\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.1975 - acc: 0.9833 - val_loss: 0.7663 - val_acc: 0.7000\n",
            "Epoch 126/500\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.2190 - acc: 0.9667 - val_loss: 0.7739 - val_acc: 0.7500\n",
            "Epoch 127/500\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.1842 - acc: 0.9667 - val_loss: 0.7778 - val_acc: 0.7500\n",
            "Epoch 128/500\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.1980 - acc: 0.9667 - val_loss: 0.7817 - val_acc: 0.7500\n",
            "Epoch 129/500\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.1938 - acc: 1.0000 - val_loss: 0.7781 - val_acc: 0.7500\n",
            "Epoch 130/500\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.2001 - acc: 0.9833 - val_loss: 0.7694 - val_acc: 0.7500\n",
            "Epoch 131/500\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.1770 - acc: 0.9833 - val_loss: 0.7594 - val_acc: 0.7500\n",
            "Epoch 132/500\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.1977 - acc: 0.9500 - val_loss: 0.7486 - val_acc: 0.7500\n",
            "Epoch 133/500\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.1866 - acc: 1.0000 - val_loss: 0.7444 - val_acc: 0.7500\n",
            "Epoch 134/500\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.1917 - acc: 0.9500 - val_loss: 0.7511 - val_acc: 0.7500\n",
            "Epoch 135/500\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.1554 - acc: 0.9833 - val_loss: 0.7587 - val_acc: 0.7500\n",
            "Epoch 136/500\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.1663 - acc: 0.9833 - val_loss: 0.7847 - val_acc: 0.7500\n",
            "Epoch 137/500\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.1841 - acc: 0.9667 - val_loss: 0.8085 - val_acc: 0.7000\n",
            "Epoch 138/500\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.2013 - acc: 0.9500 - val_loss: 0.8102 - val_acc: 0.7000\n",
            "Epoch 139/500\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.2025 - acc: 0.9667 - val_loss: 0.7876 - val_acc: 0.7500\n",
            "Epoch 140/500\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.1832 - acc: 0.9667 - val_loss: 0.7678 - val_acc: 0.7500\n",
            "Epoch 141/500\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.1784 - acc: 0.9833 - val_loss: 0.7549 - val_acc: 0.7000\n",
            "Epoch 142/500\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.1810 - acc: 0.9667 - val_loss: 0.7488 - val_acc: 0.7000\n",
            "Epoch 143/500\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.1622 - acc: 1.0000 - val_loss: 0.7400 - val_acc: 0.7000\n",
            "Epoch 144/500\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.1682 - acc: 1.0000 - val_loss: 0.7409 - val_acc: 0.7000\n",
            "Epoch 145/500\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.1639 - acc: 1.0000 - val_loss: 0.7540 - val_acc: 0.7000\n",
            "Epoch 146/500\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.1818 - acc: 0.9667 - val_loss: 0.7494 - val_acc: 0.7500\n",
            "Epoch 147/500\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.1493 - acc: 1.0000 - val_loss: 0.7459 - val_acc: 0.7500\n",
            "Epoch 148/500\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.1729 - acc: 0.9833 - val_loss: 0.7390 - val_acc: 0.7500\n",
            "Epoch 149/500\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.1746 - acc: 0.9833 - val_loss: 0.7416 - val_acc: 0.7500\n",
            "Epoch 150/500\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.1704 - acc: 0.9833 - val_loss: 0.7456 - val_acc: 0.7500\n",
            "Epoch 151/500\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.1607 - acc: 0.9833 - val_loss: 0.7467 - val_acc: 0.7000\n",
            "Epoch 152/500\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.1669 - acc: 1.0000 - val_loss: 0.7640 - val_acc: 0.7000\n",
            "Epoch 153/500\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.1477 - acc: 0.9667 - val_loss: 0.7930 - val_acc: 0.7500\n",
            "Epoch 154/500\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.1408 - acc: 1.0000 - val_loss: 0.8196 - val_acc: 0.7500\n",
            "Epoch 155/500\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.1660 - acc: 1.0000 - val_loss: 0.8431 - val_acc: 0.7500\n",
            "Epoch 156/500\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.1422 - acc: 0.9833 - val_loss: 0.8370 - val_acc: 0.7000\n",
            "Epoch 157/500\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.1574 - acc: 1.0000 - val_loss: 0.8099 - val_acc: 0.7500\n",
            "Epoch 158/500\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.1941 - acc: 0.9167 - val_loss: 0.7772 - val_acc: 0.7500\n",
            "Epoch 159/500\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.1414 - acc: 1.0000 - val_loss: 0.7883 - val_acc: 0.7500\n",
            "Epoch 160/500\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.1621 - acc: 1.0000 - val_loss: 0.7783 - val_acc: 0.7000\n",
            "Epoch 161/500\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.1624 - acc: 0.9500 - val_loss: 0.7789 - val_acc: 0.7000\n",
            "Epoch 162/500\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.1770 - acc: 0.9667 - val_loss: 0.7793 - val_acc: 0.7000\n",
            "Epoch 163/500\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.1501 - acc: 0.9667 - val_loss: 0.7866 - val_acc: 0.7000\n",
            "Epoch 164/500\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.1326 - acc: 0.9833 - val_loss: 0.7885 - val_acc: 0.7000\n",
            "Epoch 165/500\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.1474 - acc: 0.9667 - val_loss: 0.7838 - val_acc: 0.7000\n",
            "Epoch 166/500\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.1332 - acc: 0.9833 - val_loss: 0.7832 - val_acc: 0.7000\n",
            "Epoch 167/500\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.1410 - acc: 0.9667 - val_loss: 0.7863 - val_acc: 0.7000\n",
            "Epoch 168/500\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.1315 - acc: 0.9833 - val_loss: 0.8026 - val_acc: 0.7000\n",
            "Epoch 169/500\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.1367 - acc: 0.9833 - val_loss: 0.8195 - val_acc: 0.7000\n",
            "Epoch 170/500\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.1401 - acc: 0.9667 - val_loss: 0.8244 - val_acc: 0.7000\n",
            "Epoch 171/500\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.1229 - acc: 1.0000 - val_loss: 0.8222 - val_acc: 0.7000\n",
            "Epoch 172/500\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.1936 - acc: 0.9500 - val_loss: 0.8210 - val_acc: 0.7000\n",
            "Epoch 173/500\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.1289 - acc: 1.0000 - val_loss: 0.8028 - val_acc: 0.7000\n",
            "Epoch 174/500\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.1751 - acc: 0.9500 - val_loss: 0.8187 - val_acc: 0.7000\n",
            "Epoch 175/500\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.1332 - acc: 0.9833 - val_loss: 0.8282 - val_acc: 0.7000\n",
            "Epoch 176/500\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.1238 - acc: 0.9833 - val_loss: 0.8305 - val_acc: 0.7000\n",
            "Epoch 177/500\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.1397 - acc: 1.0000 - val_loss: 0.8206 - val_acc: 0.7000\n",
            "Epoch 178/500\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.1356 - acc: 1.0000 - val_loss: 0.8202 - val_acc: 0.7000\n",
            "Epoch 179/500\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.1284 - acc: 0.9833 - val_loss: 0.8065 - val_acc: 0.7000\n",
            "Epoch 180/500\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.1199 - acc: 0.9833 - val_loss: 0.7789 - val_acc: 0.7500\n",
            "Epoch 181/500\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.1202 - acc: 1.0000 - val_loss: 0.7729 - val_acc: 0.7500\n",
            "Epoch 182/500\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.1338 - acc: 0.9833 - val_loss: 0.7590 - val_acc: 0.7500\n",
            "Epoch 183/500\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.1399 - acc: 1.0000 - val_loss: 0.7375 - val_acc: 0.7500\n",
            "Epoch 184/500\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.1307 - acc: 0.9833 - val_loss: 0.7387 - val_acc: 0.7000\n",
            "Epoch 185/500\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.1149 - acc: 1.0000 - val_loss: 0.7427 - val_acc: 0.7000\n",
            "Epoch 186/500\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.1573 - acc: 0.9667 - val_loss: 0.7388 - val_acc: 0.7000\n",
            "Epoch 187/500\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.1363 - acc: 1.0000 - val_loss: 0.7763 - val_acc: 0.7000\n",
            "Epoch 188/500\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.1448 - acc: 0.9500 - val_loss: 0.8092 - val_acc: 0.7000\n",
            "Epoch 189/500\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0999 - acc: 1.0000 - val_loss: 0.8281 - val_acc: 0.7000\n",
            "Epoch 190/500\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.1455 - acc: 1.0000 - val_loss: 0.8405 - val_acc: 0.7000\n",
            "Epoch 191/500\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.1301 - acc: 0.9667 - val_loss: 0.8425 - val_acc: 0.7000\n",
            "Epoch 192/500\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.1138 - acc: 1.0000 - val_loss: 0.8602 - val_acc: 0.7000\n",
            "Epoch 193/500\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.1137 - acc: 1.0000 - val_loss: 0.8611 - val_acc: 0.7000\n",
            "Epoch 194/500\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.1321 - acc: 0.9833 - val_loss: 0.8618 - val_acc: 0.7000\n",
            "Epoch 195/500\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.1234 - acc: 0.9667 - val_loss: 0.8294 - val_acc: 0.7000\n",
            "Epoch 196/500\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0999 - acc: 1.0000 - val_loss: 0.8186 - val_acc: 0.7000\n",
            "Epoch 197/500\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.1290 - acc: 0.9833 - val_loss: 0.8245 - val_acc: 0.7000\n",
            "Epoch 198/500\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.1155 - acc: 1.0000 - val_loss: 0.8298 - val_acc: 0.7000\n",
            "Epoch 199/500\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.1200 - acc: 1.0000 - val_loss: 0.8327 - val_acc: 0.7000\n",
            "Epoch 200/500\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.1243 - acc: 0.9667 - val_loss: 0.8595 - val_acc: 0.7000\n",
            "Epoch 201/500\n",
            "\n",
            "Epoch 00201: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.1224 - acc: 1.0000 - val_loss: 0.8550 - val_acc: 0.7000\n",
            "Epoch 202/500\n",
            "\n",
            "Epoch 00202: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.1099 - acc: 0.9833 - val_loss: 0.8328 - val_acc: 0.7000\n",
            "Epoch 203/500\n",
            "\n",
            "Epoch 00203: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.1557 - acc: 0.9667 - val_loss: 0.7925 - val_acc: 0.7000\n",
            "Epoch 204/500\n",
            "\n",
            "Epoch 00204: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.1038 - acc: 1.0000 - val_loss: 0.7829 - val_acc: 0.7000\n",
            "Epoch 205/500\n",
            "\n",
            "Epoch 00205: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.1294 - acc: 0.9833 - val_loss: 0.7927 - val_acc: 0.7500\n",
            "Epoch 206/500\n",
            "\n",
            "Epoch 00206: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0927 - acc: 0.9833 - val_loss: 0.8327 - val_acc: 0.7500\n",
            "Epoch 207/500\n",
            "\n",
            "Epoch 00207: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.1227 - acc: 0.9833 - val_loss: 0.8593 - val_acc: 0.7000\n",
            "Epoch 208/500\n",
            "\n",
            "Epoch 00208: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.1146 - acc: 1.0000 - val_loss: 0.8526 - val_acc: 0.7000\n",
            "Epoch 209/500\n",
            "\n",
            "Epoch 00209: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.1049 - acc: 0.9833 - val_loss: 0.8134 - val_acc: 0.7500\n",
            "Epoch 210/500\n",
            "\n",
            "Epoch 00210: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0970 - acc: 1.0000 - val_loss: 0.7894 - val_acc: 0.7500\n",
            "Epoch 211/500\n",
            "\n",
            "Epoch 00211: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.1180 - acc: 1.0000 - val_loss: 0.7897 - val_acc: 0.7500\n",
            "Epoch 212/500\n",
            "\n",
            "Epoch 00212: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.1311 - acc: 0.9833 - val_loss: 0.8047 - val_acc: 0.7500\n",
            "Epoch 213/500\n",
            "\n",
            "Epoch 00213: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0940 - acc: 1.0000 - val_loss: 0.8210 - val_acc: 0.7500\n",
            "Epoch 214/500\n",
            "\n",
            "Epoch 00214: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.1163 - acc: 0.9833 - val_loss: 0.8019 - val_acc: 0.7500\n",
            "Epoch 215/500\n",
            "\n",
            "Epoch 00215: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.1075 - acc: 1.0000 - val_loss: 0.7696 - val_acc: 0.7000\n",
            "Epoch 216/500\n",
            "\n",
            "Epoch 00216: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.1257 - acc: 1.0000 - val_loss: 0.7589 - val_acc: 0.7000\n",
            "Epoch 217/500\n",
            "\n",
            "Epoch 00217: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.1135 - acc: 0.9833 - val_loss: 0.7598 - val_acc: 0.7000\n",
            "Epoch 218/500\n",
            "\n",
            "Epoch 00218: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.1017 - acc: 1.0000 - val_loss: 0.7691 - val_acc: 0.7000\n",
            "Epoch 219/500\n",
            "\n",
            "Epoch 00219: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.1145 - acc: 0.9833 - val_loss: 0.7693 - val_acc: 0.7000\n",
            "Epoch 220/500\n",
            "\n",
            "Epoch 00220: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.1022 - acc: 1.0000 - val_loss: 0.7648 - val_acc: 0.7000\n",
            "Epoch 221/500\n",
            "\n",
            "Epoch 00221: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.1042 - acc: 1.0000 - val_loss: 0.7819 - val_acc: 0.7000\n",
            "Epoch 222/500\n",
            "\n",
            "Epoch 00222: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0981 - acc: 1.0000 - val_loss: 0.8116 - val_acc: 0.7000\n",
            "Epoch 223/500\n",
            "\n",
            "Epoch 00223: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.1641 - acc: 0.9500 - val_loss: 0.8151 - val_acc: 0.7000\n",
            "Epoch 224/500\n",
            "\n",
            "Epoch 00224: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.1209 - acc: 1.0000 - val_loss: 0.8086 - val_acc: 0.6500\n",
            "Epoch 225/500\n",
            "\n",
            "Epoch 00225: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.1069 - acc: 0.9833 - val_loss: 0.8126 - val_acc: 0.6500\n",
            "Epoch 226/500\n",
            "\n",
            "Epoch 00226: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.1262 - acc: 0.9833 - val_loss: 0.8177 - val_acc: 0.7000\n",
            "Epoch 227/500\n",
            "\n",
            "Epoch 00227: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.1025 - acc: 0.9833 - val_loss: 0.8179 - val_acc: 0.7000\n",
            "Epoch 228/500\n",
            "\n",
            "Epoch 00228: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.1091 - acc: 0.9833 - val_loss: 0.8063 - val_acc: 0.7000\n",
            "Epoch 229/500\n",
            "\n",
            "Epoch 00229: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.1257 - acc: 0.9833 - val_loss: 0.7975 - val_acc: 0.7000\n",
            "Epoch 230/500\n",
            "\n",
            "Epoch 00230: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.1056 - acc: 0.9833 - val_loss: 0.7810 - val_acc: 0.7500\n",
            "Epoch 231/500\n",
            "\n",
            "Epoch 00231: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.1026 - acc: 1.0000 - val_loss: 0.7751 - val_acc: 0.7500\n",
            "Epoch 232/500\n",
            "\n",
            "Epoch 00232: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0859 - acc: 1.0000 - val_loss: 0.7738 - val_acc: 0.7500\n",
            "Epoch 233/500\n",
            "\n",
            "Epoch 00233: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.1167 - acc: 1.0000 - val_loss: 0.7620 - val_acc: 0.7500\n",
            "Epoch 234/500\n",
            "\n",
            "Epoch 00234: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.1129 - acc: 1.0000 - val_loss: 0.7731 - val_acc: 0.7000\n",
            "Epoch 235/500\n",
            "\n",
            "Epoch 00235: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0963 - acc: 0.9833 - val_loss: 0.7819 - val_acc: 0.7000\n",
            "Epoch 236/500\n",
            "\n",
            "Epoch 00236: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0853 - acc: 1.0000 - val_loss: 0.7750 - val_acc: 0.7000\n",
            "Epoch 237/500\n",
            "\n",
            "Epoch 00237: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0879 - acc: 0.9833 - val_loss: 0.7562 - val_acc: 0.7000\n",
            "Epoch 238/500\n",
            "\n",
            "Epoch 00238: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0906 - acc: 1.0000 - val_loss: 0.7541 - val_acc: 0.7000\n",
            "Epoch 239/500\n",
            "\n",
            "Epoch 00239: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.1012 - acc: 1.0000 - val_loss: 0.7608 - val_acc: 0.7500\n",
            "Epoch 240/500\n",
            "\n",
            "Epoch 00240: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0989 - acc: 0.9833 - val_loss: 0.7895 - val_acc: 0.7000\n",
            "Epoch 241/500\n",
            "\n",
            "Epoch 00241: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.1124 - acc: 0.9833 - val_loss: 0.7958 - val_acc: 0.7000\n",
            "Epoch 242/500\n",
            "\n",
            "Epoch 00242: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0836 - acc: 1.0000 - val_loss: 0.7874 - val_acc: 0.7000\n",
            "Epoch 243/500\n",
            "\n",
            "Epoch 00243: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0904 - acc: 1.0000 - val_loss: 0.7820 - val_acc: 0.7000\n",
            "Epoch 244/500\n",
            "\n",
            "Epoch 00244: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0947 - acc: 0.9833 - val_loss: 0.7630 - val_acc: 0.7000\n",
            "Epoch 245/500\n",
            "\n",
            "Epoch 00245: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0942 - acc: 0.9833 - val_loss: 0.7629 - val_acc: 0.7500\n",
            "Epoch 246/500\n",
            "\n",
            "Epoch 00246: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.1027 - acc: 0.9833 - val_loss: 0.7676 - val_acc: 0.7000\n",
            "Epoch 247/500\n",
            "\n",
            "Epoch 00247: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.1014 - acc: 0.9833 - val_loss: 0.7558 - val_acc: 0.7500\n",
            "Epoch 248/500\n",
            "\n",
            "Epoch 00248: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.1455 - acc: 0.9500 - val_loss: 0.7496 - val_acc: 0.7000\n",
            "Epoch 249/500\n",
            "\n",
            "Epoch 00249: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.1125 - acc: 1.0000 - val_loss: 0.7810 - val_acc: 0.7000\n",
            "Epoch 250/500\n",
            "\n",
            "Epoch 00250: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.1308 - acc: 0.9500 - val_loss: 0.8589 - val_acc: 0.6500\n",
            "Epoch 251/500\n",
            "\n",
            "Epoch 00251: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.1315 - acc: 0.9833 - val_loss: 0.8418 - val_acc: 0.6000\n",
            "Epoch 252/500\n",
            "\n",
            "Epoch 00252: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.1165 - acc: 1.0000 - val_loss: 0.7949 - val_acc: 0.7500\n",
            "Epoch 253/500\n",
            "\n",
            "Epoch 00253: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0859 - acc: 1.0000 - val_loss: 0.7620 - val_acc: 0.7500\n",
            "Epoch 254/500\n",
            "\n",
            "Epoch 00254: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.1069 - acc: 1.0000 - val_loss: 0.7536 - val_acc: 0.7000\n",
            "Epoch 255/500\n",
            "\n",
            "Epoch 00255: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0721 - acc: 1.0000 - val_loss: 0.7566 - val_acc: 0.7000\n",
            "Epoch 256/500\n",
            "\n",
            "Epoch 00256: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.1004 - acc: 1.0000 - val_loss: 0.7631 - val_acc: 0.7500\n",
            "Epoch 257/500\n",
            "\n",
            "Epoch 00257: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0970 - acc: 1.0000 - val_loss: 0.7621 - val_acc: 0.7500\n",
            "Epoch 258/500\n",
            "\n",
            "Epoch 00258: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0866 - acc: 1.0000 - val_loss: 0.7606 - val_acc: 0.7500\n",
            "Epoch 259/500\n",
            "\n",
            "Epoch 00259: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0773 - acc: 1.0000 - val_loss: 0.7599 - val_acc: 0.7500\n",
            "Epoch 260/500\n",
            "\n",
            "Epoch 00260: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.1030 - acc: 1.0000 - val_loss: 0.7607 - val_acc: 0.7500\n",
            "Epoch 261/500\n",
            "\n",
            "Epoch 00261: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0731 - acc: 1.0000 - val_loss: 0.7630 - val_acc: 0.7000\n",
            "Epoch 262/500\n",
            "\n",
            "Epoch 00262: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0739 - acc: 1.0000 - val_loss: 0.7665 - val_acc: 0.7000\n",
            "Epoch 263/500\n",
            "\n",
            "Epoch 00263: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.1189 - acc: 0.9667 - val_loss: 0.7620 - val_acc: 0.7000\n",
            "Epoch 264/500\n",
            "\n",
            "Epoch 00264: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0776 - acc: 1.0000 - val_loss: 0.7598 - val_acc: 0.7000\n",
            "Epoch 265/500\n",
            "\n",
            "Epoch 00265: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0885 - acc: 0.9667 - val_loss: 0.7660 - val_acc: 0.7500\n",
            "Epoch 266/500\n",
            "\n",
            "Epoch 00266: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0684 - acc: 1.0000 - val_loss: 0.7821 - val_acc: 0.7000\n",
            "Epoch 267/500\n",
            "\n",
            "Epoch 00267: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0935 - acc: 0.9667 - val_loss: 0.7972 - val_acc: 0.7000\n",
            "Epoch 268/500\n",
            "\n",
            "Epoch 00268: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0866 - acc: 0.9833 - val_loss: 0.7951 - val_acc: 0.7000\n",
            "Epoch 269/500\n",
            "\n",
            "Epoch 00269: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0634 - acc: 1.0000 - val_loss: 0.7900 - val_acc: 0.7500\n",
            "Epoch 270/500\n",
            "\n",
            "Epoch 00270: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0798 - acc: 1.0000 - val_loss: 0.7907 - val_acc: 0.7500\n",
            "Epoch 271/500\n",
            "\n",
            "Epoch 00271: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.1076 - acc: 0.9833 - val_loss: 0.7867 - val_acc: 0.7000\n",
            "Epoch 272/500\n",
            "\n",
            "Epoch 00272: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0834 - acc: 0.9833 - val_loss: 0.7810 - val_acc: 0.7000\n",
            "Epoch 273/500\n",
            "\n",
            "Epoch 00273: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0957 - acc: 1.0000 - val_loss: 0.7738 - val_acc: 0.7000\n",
            "Epoch 274/500\n",
            "\n",
            "Epoch 00274: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0788 - acc: 1.0000 - val_loss: 0.7661 - val_acc: 0.7000\n",
            "Epoch 275/500\n",
            "\n",
            "Epoch 00275: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.1198 - acc: 0.9833 - val_loss: 0.7699 - val_acc: 0.7500\n",
            "Epoch 276/500\n",
            "\n",
            "Epoch 00276: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0777 - acc: 1.0000 - val_loss: 0.7705 - val_acc: 0.7500\n",
            "Epoch 277/500\n",
            "\n",
            "Epoch 00277: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0643 - acc: 1.0000 - val_loss: 0.7715 - val_acc: 0.7500\n",
            "Epoch 278/500\n",
            "\n",
            "Epoch 00278: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0924 - acc: 0.9667 - val_loss: 0.7768 - val_acc: 0.7000\n",
            "Epoch 279/500\n",
            "\n",
            "Epoch 00279: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.1234 - acc: 0.9667 - val_loss: 0.7970 - val_acc: 0.7000\n",
            "Epoch 280/500\n",
            "\n",
            "Epoch 00280: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.1026 - acc: 0.9667 - val_loss: 0.8206 - val_acc: 0.7000\n",
            "Epoch 281/500\n",
            "\n",
            "Epoch 00281: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.1025 - acc: 0.9833 - val_loss: 0.8165 - val_acc: 0.7000\n",
            "Epoch 282/500\n",
            "\n",
            "Epoch 00282: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.1010 - acc: 1.0000 - val_loss: 0.8218 - val_acc: 0.7000\n",
            "Epoch 283/500\n",
            "\n",
            "Epoch 00283: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.1141 - acc: 0.9833 - val_loss: 0.8227 - val_acc: 0.7000\n",
            "Epoch 284/500\n",
            "\n",
            "Epoch 00284: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0621 - acc: 1.0000 - val_loss: 0.8380 - val_acc: 0.7000\n",
            "Epoch 285/500\n",
            "\n",
            "Epoch 00285: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0702 - acc: 1.0000 - val_loss: 0.8423 - val_acc: 0.7000\n",
            "Epoch 286/500\n",
            "\n",
            "Epoch 00286: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0766 - acc: 0.9833 - val_loss: 0.8320 - val_acc: 0.7000\n",
            "Epoch 287/500\n",
            "\n",
            "Epoch 00287: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0699 - acc: 1.0000 - val_loss: 0.8179 - val_acc: 0.7000\n",
            "Epoch 288/500\n",
            "\n",
            "Epoch 00288: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0886 - acc: 0.9833 - val_loss: 0.8055 - val_acc: 0.7000\n",
            "Epoch 289/500\n",
            "\n",
            "Epoch 00289: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0832 - acc: 1.0000 - val_loss: 0.8041 - val_acc: 0.7500\n",
            "Epoch 290/500\n",
            "\n",
            "Epoch 00290: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0885 - acc: 1.0000 - val_loss: 0.7941 - val_acc: 0.7500\n",
            "Epoch 291/500\n",
            "\n",
            "Epoch 00291: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0887 - acc: 1.0000 - val_loss: 0.7845 - val_acc: 0.8000\n",
            "Epoch 292/500\n",
            "\n",
            "Epoch 00292: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0855 - acc: 1.0000 - val_loss: 0.7762 - val_acc: 0.7000\n",
            "Epoch 293/500\n",
            "\n",
            "Epoch 00293: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0765 - acc: 1.0000 - val_loss: 0.8108 - val_acc: 0.7000\n",
            "Epoch 294/500\n",
            "\n",
            "Epoch 00294: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0949 - acc: 1.0000 - val_loss: 0.8386 - val_acc: 0.6500\n",
            "Epoch 295/500\n",
            "\n",
            "Epoch 00295: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.1136 - acc: 0.9667 - val_loss: 0.8592 - val_acc: 0.6500\n",
            "Epoch 296/500\n",
            "\n",
            "Epoch 00296: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0860 - acc: 0.9833 - val_loss: 0.9714 - val_acc: 0.6500\n",
            "Epoch 297/500\n",
            "\n",
            "Epoch 00297: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.1004 - acc: 0.9833 - val_loss: 1.0634 - val_acc: 0.6000\n",
            "Epoch 298/500\n",
            "\n",
            "Epoch 00298: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0847 - acc: 1.0000 - val_loss: 0.9510 - val_acc: 0.6500\n",
            "Epoch 299/500\n",
            "\n",
            "Epoch 00299: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.1153 - acc: 0.9667 - val_loss: 0.8489 - val_acc: 0.7000\n",
            "Epoch 300/500\n",
            "\n",
            "Epoch 00300: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0944 - acc: 1.0000 - val_loss: 0.8183 - val_acc: 0.7500\n",
            "Epoch 301/500\n",
            "\n",
            "Epoch 00301: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0935 - acc: 1.0000 - val_loss: 0.8356 - val_acc: 0.8000\n",
            "Epoch 302/500\n",
            "\n",
            "Epoch 00302: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0804 - acc: 1.0000 - val_loss: 0.8413 - val_acc: 0.7500\n",
            "Epoch 303/500\n",
            "\n",
            "Epoch 00303: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0951 - acc: 0.9833 - val_loss: 0.8292 - val_acc: 0.7000\n",
            "Epoch 304/500\n",
            "\n",
            "Epoch 00304: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0939 - acc: 1.0000 - val_loss: 0.8355 - val_acc: 0.7500\n",
            "Epoch 305/500\n",
            "\n",
            "Epoch 00305: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0885 - acc: 0.9833 - val_loss: 0.8452 - val_acc: 0.7500\n",
            "Epoch 306/500\n",
            "\n",
            "Epoch 00306: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0559 - acc: 1.0000 - val_loss: 0.8492 - val_acc: 0.7000\n",
            "Epoch 307/500\n",
            "\n",
            "Epoch 00307: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0841 - acc: 1.0000 - val_loss: 0.8529 - val_acc: 0.7000\n",
            "Epoch 308/500\n",
            "\n",
            "Epoch 00308: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0834 - acc: 1.0000 - val_loss: 0.8724 - val_acc: 0.7000\n",
            "Epoch 309/500\n",
            "\n",
            "Epoch 00309: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0861 - acc: 1.0000 - val_loss: 0.9114 - val_acc: 0.7000\n",
            "Epoch 310/500\n",
            "\n",
            "Epoch 00310: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0695 - acc: 1.0000 - val_loss: 0.9009 - val_acc: 0.7000\n",
            "Epoch 311/500\n",
            "\n",
            "Epoch 00311: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0690 - acc: 1.0000 - val_loss: 0.8774 - val_acc: 0.7000\n",
            "Epoch 312/500\n",
            "\n",
            "Epoch 00312: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.1232 - acc: 0.9833 - val_loss: 0.8574 - val_acc: 0.7000\n",
            "Epoch 313/500\n",
            "\n",
            "Epoch 00313: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0738 - acc: 1.0000 - val_loss: 0.8549 - val_acc: 0.7000\n",
            "Epoch 314/500\n",
            "\n",
            "Epoch 00314: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0648 - acc: 0.9833 - val_loss: 0.8593 - val_acc: 0.7000\n",
            "Epoch 315/500\n",
            "\n",
            "Epoch 00315: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.1040 - acc: 0.9833 - val_loss: 0.8140 - val_acc: 0.7000\n",
            "Epoch 316/500\n",
            "\n",
            "Epoch 00316: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.1105 - acc: 0.9667 - val_loss: 0.7856 - val_acc: 0.7500\n",
            "Epoch 317/500\n",
            "\n",
            "Epoch 00317: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.1101 - acc: 0.9833 - val_loss: 0.7789 - val_acc: 0.7500\n",
            "Epoch 318/500\n",
            "\n",
            "Epoch 00318: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.1063 - acc: 0.9833 - val_loss: 0.7858 - val_acc: 0.7500\n",
            "Epoch 319/500\n",
            "\n",
            "Epoch 00319: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0921 - acc: 0.9833 - val_loss: 0.7780 - val_acc: 0.7000\n",
            "Epoch 320/500\n",
            "\n",
            "Epoch 00320: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0632 - acc: 1.0000 - val_loss: 0.7737 - val_acc: 0.7000\n",
            "Epoch 321/500\n",
            "\n",
            "Epoch 00321: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0677 - acc: 1.0000 - val_loss: 0.7994 - val_acc: 0.7000\n",
            "Epoch 322/500\n",
            "\n",
            "Epoch 00322: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0862 - acc: 1.0000 - val_loss: 0.8248 - val_acc: 0.7000\n",
            "Epoch 323/500\n",
            "\n",
            "Epoch 00323: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0849 - acc: 0.9833 - val_loss: 0.8651 - val_acc: 0.7500\n",
            "Epoch 324/500\n",
            "\n",
            "Epoch 00324: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0706 - acc: 1.0000 - val_loss: 0.8936 - val_acc: 0.7500\n",
            "Epoch 325/500\n",
            "\n",
            "Epoch 00325: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0963 - acc: 0.9667 - val_loss: 0.8354 - val_acc: 0.7000\n",
            "Epoch 326/500\n",
            "\n",
            "Epoch 00326: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0531 - acc: 1.0000 - val_loss: 0.7764 - val_acc: 0.7000\n",
            "Epoch 327/500\n",
            "\n",
            "Epoch 00327: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0657 - acc: 1.0000 - val_loss: 0.7600 - val_acc: 0.7500\n",
            "Epoch 328/500\n",
            "\n",
            "Epoch 00328: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0783 - acc: 0.9667 - val_loss: 0.7691 - val_acc: 0.7500\n",
            "Epoch 329/500\n",
            "\n",
            "Epoch 00329: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0732 - acc: 1.0000 - val_loss: 0.7972 - val_acc: 0.7000\n",
            "Epoch 330/500\n",
            "\n",
            "Epoch 00330: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0898 - acc: 1.0000 - val_loss: 0.8284 - val_acc: 0.7000\n",
            "Epoch 331/500\n",
            "\n",
            "Epoch 00331: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0674 - acc: 1.0000 - val_loss: 0.8306 - val_acc: 0.7000\n",
            "Epoch 332/500\n",
            "\n",
            "Epoch 00332: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.1116 - acc: 0.9500 - val_loss: 0.8056 - val_acc: 0.7000\n",
            "Epoch 333/500\n",
            "\n",
            "Epoch 00333: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0688 - acc: 0.9833 - val_loss: 0.8052 - val_acc: 0.7000\n",
            "Epoch 334/500\n",
            "\n",
            "Epoch 00334: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0806 - acc: 1.0000 - val_loss: 0.8037 - val_acc: 0.7000\n",
            "Epoch 335/500\n",
            "\n",
            "Epoch 00335: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0691 - acc: 1.0000 - val_loss: 0.7982 - val_acc: 0.7000\n",
            "Epoch 336/500\n",
            "\n",
            "Epoch 00336: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0630 - acc: 1.0000 - val_loss: 0.7974 - val_acc: 0.7000\n",
            "Epoch 337/500\n",
            "\n",
            "Epoch 00337: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0943 - acc: 0.9667 - val_loss: 0.8083 - val_acc: 0.7000\n",
            "Epoch 338/500\n",
            "\n",
            "Epoch 00338: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0462 - acc: 1.0000 - val_loss: 0.8357 - val_acc: 0.7000\n",
            "Epoch 339/500\n",
            "\n",
            "Epoch 00339: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.1213 - acc: 0.9667 - val_loss: 0.8628 - val_acc: 0.7000\n",
            "Epoch 340/500\n",
            "\n",
            "Epoch 00340: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0767 - acc: 0.9833 - val_loss: 0.8443 - val_acc: 0.7000\n",
            "Epoch 341/500\n",
            "\n",
            "Epoch 00341: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0811 - acc: 0.9667 - val_loss: 0.8148 - val_acc: 0.7000\n",
            "Epoch 342/500\n",
            "\n",
            "Epoch 00342: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0711 - acc: 1.0000 - val_loss: 0.8155 - val_acc: 0.7000\n",
            "Epoch 343/500\n",
            "\n",
            "Epoch 00343: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0671 - acc: 1.0000 - val_loss: 0.7915 - val_acc: 0.7000\n",
            "Epoch 344/500\n",
            "\n",
            "Epoch 00344: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0851 - acc: 0.9833 - val_loss: 0.7775 - val_acc: 0.7500\n",
            "Epoch 345/500\n",
            "\n",
            "Epoch 00345: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0804 - acc: 1.0000 - val_loss: 0.7755 - val_acc: 0.7500\n",
            "Epoch 346/500\n",
            "\n",
            "Epoch 00346: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0817 - acc: 1.0000 - val_loss: 0.7865 - val_acc: 0.7500\n",
            "Epoch 347/500\n",
            "\n",
            "Epoch 00347: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0757 - acc: 1.0000 - val_loss: 0.7964 - val_acc: 0.7500\n",
            "Epoch 348/500\n",
            "\n",
            "Epoch 00348: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0770 - acc: 1.0000 - val_loss: 0.8088 - val_acc: 0.7000\n",
            "Epoch 349/500\n",
            "\n",
            "Epoch 00349: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0785 - acc: 1.0000 - val_loss: 0.8445 - val_acc: 0.7000\n",
            "Epoch 350/500\n",
            "\n",
            "Epoch 00350: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.1128 - acc: 0.9667 - val_loss: 0.8783 - val_acc: 0.6000\n",
            "Epoch 351/500\n",
            "\n",
            "Epoch 00351: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0707 - acc: 1.0000 - val_loss: 0.8845 - val_acc: 0.6000\n",
            "Epoch 352/500\n",
            "\n",
            "Epoch 00352: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0576 - acc: 1.0000 - val_loss: 0.9022 - val_acc: 0.6000\n",
            "Epoch 353/500\n",
            "\n",
            "Epoch 00353: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0588 - acc: 1.0000 - val_loss: 0.9130 - val_acc: 0.6500\n",
            "Epoch 354/500\n",
            "\n",
            "Epoch 00354: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0638 - acc: 1.0000 - val_loss: 0.9011 - val_acc: 0.7000\n",
            "Epoch 355/500\n",
            "\n",
            "Epoch 00355: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0613 - acc: 1.0000 - val_loss: 0.8672 - val_acc: 0.7000\n",
            "Epoch 356/500\n",
            "\n",
            "Epoch 00356: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0448 - acc: 1.0000 - val_loss: 0.8508 - val_acc: 0.7000\n",
            "Epoch 357/500\n",
            "\n",
            "Epoch 00357: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0563 - acc: 0.9833 - val_loss: 0.8397 - val_acc: 0.7000\n",
            "Epoch 358/500\n",
            "\n",
            "Epoch 00358: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0656 - acc: 1.0000 - val_loss: 0.8186 - val_acc: 0.7500\n",
            "Epoch 359/500\n",
            "\n",
            "Epoch 00359: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0585 - acc: 1.0000 - val_loss: 0.8017 - val_acc: 0.7500\n",
            "Epoch 360/500\n",
            "\n",
            "Epoch 00360: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0667 - acc: 1.0000 - val_loss: 0.7938 - val_acc: 0.7000\n",
            "Epoch 361/500\n",
            "\n",
            "Epoch 00361: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0492 - acc: 1.0000 - val_loss: 0.8094 - val_acc: 0.7000\n",
            "Epoch 362/500\n",
            "\n",
            "Epoch 00362: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0466 - acc: 1.0000 - val_loss: 0.8100 - val_acc: 0.7000\n",
            "Epoch 363/500\n",
            "\n",
            "Epoch 00363: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0617 - acc: 1.0000 - val_loss: 0.8011 - val_acc: 0.7000\n",
            "Epoch 364/500\n",
            "\n",
            "Epoch 00364: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0649 - acc: 0.9833 - val_loss: 0.7958 - val_acc: 0.7500\n",
            "Epoch 365/500\n",
            "\n",
            "Epoch 00365: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0652 - acc: 1.0000 - val_loss: 0.8073 - val_acc: 0.7500\n",
            "Epoch 366/500\n",
            "\n",
            "Epoch 00366: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0654 - acc: 1.0000 - val_loss: 0.8194 - val_acc: 0.7500\n",
            "Epoch 367/500\n",
            "\n",
            "Epoch 00367: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0604 - acc: 1.0000 - val_loss: 0.8303 - val_acc: 0.7000\n",
            "Epoch 368/500\n",
            "\n",
            "Epoch 00368: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0666 - acc: 0.9833 - val_loss: 0.8411 - val_acc: 0.7000\n",
            "Epoch 369/500\n",
            "\n",
            "Epoch 00369: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0534 - acc: 1.0000 - val_loss: 0.8248 - val_acc: 0.7000\n",
            "Epoch 370/500\n",
            "\n",
            "Epoch 00370: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0542 - acc: 1.0000 - val_loss: 0.8187 - val_acc: 0.7000\n",
            "Epoch 371/500\n",
            "\n",
            "Epoch 00371: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0576 - acc: 1.0000 - val_loss: 0.8166 - val_acc: 0.7500\n",
            "Epoch 372/500\n",
            "\n",
            "Epoch 00372: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0569 - acc: 1.0000 - val_loss: 0.8199 - val_acc: 0.8000\n",
            "Epoch 373/500\n",
            "\n",
            "Epoch 00373: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0562 - acc: 1.0000 - val_loss: 0.8214 - val_acc: 0.8000\n",
            "Epoch 374/500\n",
            "\n",
            "Epoch 00374: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0562 - acc: 0.9833 - val_loss: 0.8125 - val_acc: 0.8000\n",
            "Epoch 375/500\n",
            "\n",
            "Epoch 00375: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0665 - acc: 0.9833 - val_loss: 0.7953 - val_acc: 0.8000\n",
            "Epoch 376/500\n",
            "\n",
            "Epoch 00376: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0792 - acc: 0.9833 - val_loss: 0.7808 - val_acc: 0.8000\n",
            "Epoch 377/500\n",
            "\n",
            "Epoch 00377: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0629 - acc: 1.0000 - val_loss: 0.7680 - val_acc: 0.7500\n",
            "Epoch 378/500\n",
            "\n",
            "Epoch 00378: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0680 - acc: 1.0000 - val_loss: 0.7737 - val_acc: 0.7000\n",
            "Epoch 379/500\n",
            "\n",
            "Epoch 00379: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0542 - acc: 1.0000 - val_loss: 0.7947 - val_acc: 0.7500\n",
            "Epoch 380/500\n",
            "\n",
            "Epoch 00380: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0590 - acc: 0.9833 - val_loss: 0.8138 - val_acc: 0.7500\n",
            "Epoch 381/500\n",
            "\n",
            "Epoch 00381: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0480 - acc: 1.0000 - val_loss: 0.8323 - val_acc: 0.7500\n",
            "Epoch 382/500\n",
            "\n",
            "Epoch 00382: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0490 - acc: 1.0000 - val_loss: 0.8433 - val_acc: 0.7500\n",
            "Epoch 383/500\n",
            "\n",
            "Epoch 00383: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0762 - acc: 1.0000 - val_loss: 0.8548 - val_acc: 0.7000\n",
            "Epoch 384/500\n",
            "\n",
            "Epoch 00384: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0544 - acc: 0.9833 - val_loss: 0.8335 - val_acc: 0.7000\n",
            "Epoch 385/500\n",
            "\n",
            "Epoch 00385: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0606 - acc: 1.0000 - val_loss: 0.8469 - val_acc: 0.7000\n",
            "Epoch 386/500\n",
            "\n",
            "Epoch 00386: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0607 - acc: 1.0000 - val_loss: 0.8593 - val_acc: 0.7000\n",
            "Epoch 387/500\n",
            "\n",
            "Epoch 00387: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0690 - acc: 1.0000 - val_loss: 0.8729 - val_acc: 0.7000\n",
            "Epoch 388/500\n",
            "\n",
            "Epoch 00388: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0675 - acc: 0.9833 - val_loss: 0.9060 - val_acc: 0.7000\n",
            "Epoch 389/500\n",
            "\n",
            "Epoch 00389: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0760 - acc: 0.9833 - val_loss: 0.9610 - val_acc: 0.7000\n",
            "Epoch 390/500\n",
            "\n",
            "Epoch 00390: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0852 - acc: 0.9833 - val_loss: 1.0341 - val_acc: 0.7000\n",
            "Epoch 391/500\n",
            "\n",
            "Epoch 00391: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0747 - acc: 0.9667 - val_loss: 1.0237 - val_acc: 0.7000\n",
            "Epoch 392/500\n",
            "\n",
            "Epoch 00392: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0738 - acc: 1.0000 - val_loss: 0.9610 - val_acc: 0.7000\n",
            "Epoch 393/500\n",
            "\n",
            "Epoch 00393: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0589 - acc: 1.0000 - val_loss: 0.8992 - val_acc: 0.7000\n",
            "Epoch 394/500\n",
            "\n",
            "Epoch 00394: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0577 - acc: 1.0000 - val_loss: 0.8705 - val_acc: 0.7000\n",
            "Epoch 395/500\n",
            "\n",
            "Epoch 00395: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0958 - acc: 0.9833 - val_loss: 0.8426 - val_acc: 0.7000\n",
            "Epoch 396/500\n",
            "\n",
            "Epoch 00396: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0761 - acc: 0.9833 - val_loss: 0.8026 - val_acc: 0.7500\n",
            "Epoch 397/500\n",
            "\n",
            "Epoch 00397: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0508 - acc: 1.0000 - val_loss: 0.7965 - val_acc: 0.7500\n",
            "Epoch 398/500\n",
            "\n",
            "Epoch 00398: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0629 - acc: 1.0000 - val_loss: 0.8071 - val_acc: 0.7000\n",
            "Epoch 399/500\n",
            "\n",
            "Epoch 00399: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0692 - acc: 0.9833 - val_loss: 0.8260 - val_acc: 0.7000\n",
            "Epoch 400/500\n",
            "\n",
            "Epoch 00400: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0652 - acc: 0.9833 - val_loss: 0.8694 - val_acc: 0.7500\n",
            "Epoch 401/500\n",
            "\n",
            "Epoch 00401: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0506 - acc: 1.0000 - val_loss: 0.9024 - val_acc: 0.7500\n",
            "Epoch 402/500\n",
            "\n",
            "Epoch 00402: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0623 - acc: 0.9833 - val_loss: 0.9112 - val_acc: 0.7500\n",
            "Epoch 403/500\n",
            "\n",
            "Epoch 00403: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0634 - acc: 0.9833 - val_loss: 0.8942 - val_acc: 0.7000\n",
            "Epoch 404/500\n",
            "\n",
            "Epoch 00404: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0656 - acc: 0.9833 - val_loss: 0.8791 - val_acc: 0.7000\n",
            "Epoch 405/500\n",
            "\n",
            "Epoch 00405: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0639 - acc: 1.0000 - val_loss: 0.9104 - val_acc: 0.7000\n",
            "Epoch 406/500\n",
            "\n",
            "Epoch 00406: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0431 - acc: 1.0000 - val_loss: 0.9516 - val_acc: 0.6500\n",
            "Epoch 407/500\n",
            "\n",
            "Epoch 00407: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0570 - acc: 1.0000 - val_loss: 0.9708 - val_acc: 0.7000\n",
            "Epoch 408/500\n",
            "\n",
            "Epoch 00408: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0535 - acc: 0.9833 - val_loss: 0.9545 - val_acc: 0.6500\n",
            "Epoch 409/500\n",
            "\n",
            "Epoch 00409: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0395 - acc: 1.0000 - val_loss: 0.9231 - val_acc: 0.6500\n",
            "Epoch 410/500\n",
            "\n",
            "Epoch 00410: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0636 - acc: 1.0000 - val_loss: 0.9258 - val_acc: 0.6500\n",
            "Epoch 411/500\n",
            "\n",
            "Epoch 00411: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0529 - acc: 1.0000 - val_loss: 0.9508 - val_acc: 0.6500\n",
            "Epoch 412/500\n",
            "\n",
            "Epoch 00412: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0636 - acc: 0.9833 - val_loss: 0.9818 - val_acc: 0.7000\n",
            "Epoch 413/500\n",
            "\n",
            "Epoch 00413: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0621 - acc: 0.9833 - val_loss: 0.9811 - val_acc: 0.7000\n",
            "Epoch 414/500\n",
            "\n",
            "Epoch 00414: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0562 - acc: 1.0000 - val_loss: 0.9545 - val_acc: 0.6500\n",
            "Epoch 415/500\n",
            "\n",
            "Epoch 00415: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0626 - acc: 0.9833 - val_loss: 0.9254 - val_acc: 0.7000\n",
            "Epoch 416/500\n",
            "\n",
            "Epoch 00416: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0582 - acc: 1.0000 - val_loss: 0.9106 - val_acc: 0.7000\n",
            "Epoch 417/500\n",
            "\n",
            "Epoch 00417: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0530 - acc: 1.0000 - val_loss: 0.8722 - val_acc: 0.7000\n",
            "Epoch 418/500\n",
            "\n",
            "Epoch 00418: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0849 - acc: 0.9833 - val_loss: 0.8433 - val_acc: 0.7000\n",
            "Epoch 419/500\n",
            "\n",
            "Epoch 00419: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0830 - acc: 0.9833 - val_loss: 0.8218 - val_acc: 0.7000\n",
            "Epoch 420/500\n",
            "\n",
            "Epoch 00420: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0565 - acc: 0.9833 - val_loss: 0.7982 - val_acc: 0.7000\n",
            "Epoch 421/500\n",
            "\n",
            "Epoch 00421: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0772 - acc: 1.0000 - val_loss: 0.8060 - val_acc: 0.7000\n",
            "Epoch 422/500\n",
            "\n",
            "Epoch 00422: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0514 - acc: 1.0000 - val_loss: 0.8237 - val_acc: 0.7000\n",
            "Epoch 423/500\n",
            "\n",
            "Epoch 00423: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0634 - acc: 1.0000 - val_loss: 0.8559 - val_acc: 0.7500\n",
            "Epoch 424/500\n",
            "\n",
            "Epoch 00424: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0529 - acc: 1.0000 - val_loss: 0.8919 - val_acc: 0.7000\n",
            "Epoch 425/500\n",
            "\n",
            "Epoch 00425: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0627 - acc: 0.9833 - val_loss: 0.9063 - val_acc: 0.7000\n",
            "Epoch 426/500\n",
            "\n",
            "Epoch 00426: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0573 - acc: 1.0000 - val_loss: 0.8564 - val_acc: 0.7500\n",
            "Epoch 427/500\n",
            "\n",
            "Epoch 00427: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0522 - acc: 1.0000 - val_loss: 0.8226 - val_acc: 0.7000\n",
            "Epoch 428/500\n",
            "\n",
            "Epoch 00428: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0471 - acc: 1.0000 - val_loss: 0.8249 - val_acc: 0.7000\n",
            "Epoch 429/500\n",
            "\n",
            "Epoch 00429: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0865 - acc: 1.0000 - val_loss: 0.8382 - val_acc: 0.7000\n",
            "Epoch 430/500\n",
            "\n",
            "Epoch 00430: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0634 - acc: 1.0000 - val_loss: 0.8523 - val_acc: 0.7000\n",
            "Epoch 431/500\n",
            "\n",
            "Epoch 00431: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0543 - acc: 0.9833 - val_loss: 0.8461 - val_acc: 0.7000\n",
            "Epoch 432/500\n",
            "\n",
            "Epoch 00432: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0506 - acc: 0.9833 - val_loss: 0.8447 - val_acc: 0.7000\n",
            "Epoch 433/500\n",
            "\n",
            "Epoch 00433: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0398 - acc: 1.0000 - val_loss: 0.8404 - val_acc: 0.7000\n",
            "Epoch 434/500\n",
            "\n",
            "Epoch 00434: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0579 - acc: 0.9833 - val_loss: 0.8543 - val_acc: 0.7000\n",
            "Epoch 435/500\n",
            "\n",
            "Epoch 00435: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0618 - acc: 1.0000 - val_loss: 0.8808 - val_acc: 0.7000\n",
            "Epoch 436/500\n",
            "\n",
            "Epoch 00436: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0551 - acc: 1.0000 - val_loss: 0.9159 - val_acc: 0.7000\n",
            "Epoch 437/500\n",
            "\n",
            "Epoch 00437: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0442 - acc: 1.0000 - val_loss: 0.9563 - val_acc: 0.7500\n",
            "Epoch 438/500\n",
            "\n",
            "Epoch 00438: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0384 - acc: 1.0000 - val_loss: 0.9826 - val_acc: 0.7500\n",
            "Epoch 439/500\n",
            "\n",
            "Epoch 00439: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0449 - acc: 0.9833 - val_loss: 1.0113 - val_acc: 0.7500\n",
            "Epoch 440/500\n",
            "\n",
            "Epoch 00440: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0653 - acc: 1.0000 - val_loss: 0.9937 - val_acc: 0.7500\n",
            "Epoch 441/500\n",
            "\n",
            "Epoch 00441: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0854 - acc: 0.9833 - val_loss: 1.0192 - val_acc: 0.7000\n",
            "Epoch 442/500\n",
            "\n",
            "Epoch 00442: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0487 - acc: 1.0000 - val_loss: 0.9848 - val_acc: 0.7000\n",
            "Epoch 443/500\n",
            "\n",
            "Epoch 00443: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0618 - acc: 1.0000 - val_loss: 0.9017 - val_acc: 0.7000\n",
            "Epoch 444/500\n",
            "\n",
            "Epoch 00444: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0678 - acc: 1.0000 - val_loss: 0.9182 - val_acc: 0.7000\n",
            "Epoch 445/500\n",
            "\n",
            "Epoch 00445: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0768 - acc: 1.0000 - val_loss: 0.9171 - val_acc: 0.7000\n",
            "Epoch 446/500\n",
            "\n",
            "Epoch 00446: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0531 - acc: 1.0000 - val_loss: 0.9144 - val_acc: 0.7000\n",
            "Epoch 447/500\n",
            "\n",
            "Epoch 00447: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0601 - acc: 1.0000 - val_loss: 0.9192 - val_acc: 0.7500\n",
            "Epoch 448/500\n",
            "\n",
            "Epoch 00448: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0435 - acc: 1.0000 - val_loss: 0.9137 - val_acc: 0.7000\n",
            "Epoch 449/500\n",
            "\n",
            "Epoch 00449: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0509 - acc: 1.0000 - val_loss: 0.9369 - val_acc: 0.7500\n",
            "Epoch 450/500\n",
            "\n",
            "Epoch 00450: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0676 - acc: 0.9833 - val_loss: 0.9544 - val_acc: 0.7500\n",
            "Epoch 451/500\n",
            "\n",
            "Epoch 00451: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0552 - acc: 1.0000 - val_loss: 0.9420 - val_acc: 0.7500\n",
            "Epoch 452/500\n",
            "\n",
            "Epoch 00452: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0569 - acc: 0.9833 - val_loss: 0.9184 - val_acc: 0.7500\n",
            "Epoch 453/500\n",
            "\n",
            "Epoch 00453: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0694 - acc: 1.0000 - val_loss: 0.9142 - val_acc: 0.7500\n",
            "Epoch 454/500\n",
            "\n",
            "Epoch 00454: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0428 - acc: 1.0000 - val_loss: 0.9097 - val_acc: 0.7000\n",
            "Epoch 455/500\n",
            "\n",
            "Epoch 00455: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0315 - acc: 1.0000 - val_loss: 0.8970 - val_acc: 0.7000\n",
            "Epoch 456/500\n",
            "\n",
            "Epoch 00456: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0703 - acc: 0.9833 - val_loss: 0.9149 - val_acc: 0.7000\n",
            "Epoch 457/500\n",
            "\n",
            "Epoch 00457: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0412 - acc: 1.0000 - val_loss: 0.9286 - val_acc: 0.7000\n",
            "Epoch 458/500\n",
            "\n",
            "Epoch 00458: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0536 - acc: 1.0000 - val_loss: 0.9646 - val_acc: 0.7000\n",
            "Epoch 459/500\n",
            "\n",
            "Epoch 00459: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0768 - acc: 0.9833 - val_loss: 1.0015 - val_acc: 0.7000\n",
            "Epoch 460/500\n",
            "\n",
            "Epoch 00460: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0490 - acc: 1.0000 - val_loss: 0.9973 - val_acc: 0.7000\n",
            "Epoch 461/500\n",
            "\n",
            "Epoch 00461: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0529 - acc: 1.0000 - val_loss: 0.9730 - val_acc: 0.7000\n",
            "Epoch 462/500\n",
            "\n",
            "Epoch 00462: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0630 - acc: 1.0000 - val_loss: 0.9502 - val_acc: 0.7000\n",
            "Epoch 463/500\n",
            "\n",
            "Epoch 00463: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0666 - acc: 0.9833 - val_loss: 0.9328 - val_acc: 0.7000\n",
            "Epoch 464/500\n",
            "\n",
            "Epoch 00464: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0736 - acc: 1.0000 - val_loss: 0.9303 - val_acc: 0.7000\n",
            "Epoch 465/500\n",
            "\n",
            "Epoch 00465: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0461 - acc: 1.0000 - val_loss: 0.9264 - val_acc: 0.7000\n",
            "Epoch 466/500\n",
            "\n",
            "Epoch 00466: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0459 - acc: 1.0000 - val_loss: 0.9114 - val_acc: 0.7000\n",
            "Epoch 467/500\n",
            "\n",
            "Epoch 00467: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0367 - acc: 1.0000 - val_loss: 0.9046 - val_acc: 0.7000\n",
            "Epoch 468/500\n",
            "\n",
            "Epoch 00468: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0459 - acc: 1.0000 - val_loss: 0.9065 - val_acc: 0.7000\n",
            "Epoch 469/500\n",
            "\n",
            "Epoch 00469: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0431 - acc: 1.0000 - val_loss: 0.8895 - val_acc: 0.7000\n",
            "Epoch 470/500\n",
            "\n",
            "Epoch 00470: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0649 - acc: 1.0000 - val_loss: 0.8981 - val_acc: 0.7000\n",
            "Epoch 471/500\n",
            "\n",
            "Epoch 00471: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0543 - acc: 0.9833 - val_loss: 0.9277 - val_acc: 0.7000\n",
            "Epoch 472/500\n",
            "\n",
            "Epoch 00472: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0322 - acc: 1.0000 - val_loss: 0.9731 - val_acc: 0.7000\n",
            "Epoch 473/500\n",
            "\n",
            "Epoch 00473: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0507 - acc: 0.9833 - val_loss: 0.9570 - val_acc: 0.7000\n",
            "Epoch 474/500\n",
            "\n",
            "Epoch 00474: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0831 - acc: 0.9833 - val_loss: 0.9436 - val_acc: 0.7000\n",
            "Epoch 475/500\n",
            "\n",
            "Epoch 00475: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0404 - acc: 1.0000 - val_loss: 0.9190 - val_acc: 0.7000\n",
            "Epoch 476/500\n",
            "\n",
            "Epoch 00476: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0991 - acc: 0.9667 - val_loss: 0.8846 - val_acc: 0.7000\n",
            "Epoch 477/500\n",
            "\n",
            "Epoch 00477: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0470 - acc: 1.0000 - val_loss: 0.8731 - val_acc: 0.7000\n",
            "Epoch 478/500\n",
            "\n",
            "Epoch 00478: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0372 - acc: 1.0000 - val_loss: 0.8992 - val_acc: 0.7000\n",
            "Epoch 479/500\n",
            "\n",
            "Epoch 00479: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0531 - acc: 1.0000 - val_loss: 0.9370 - val_acc: 0.7000\n",
            "Epoch 480/500\n",
            "\n",
            "Epoch 00480: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0587 - acc: 1.0000 - val_loss: 0.9704 - val_acc: 0.6000\n",
            "Epoch 481/500\n",
            "\n",
            "Epoch 00481: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0504 - acc: 1.0000 - val_loss: 0.9629 - val_acc: 0.6000\n",
            "Epoch 482/500\n",
            "\n",
            "Epoch 00482: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0744 - acc: 0.9833 - val_loss: 0.9283 - val_acc: 0.6500\n",
            "Epoch 483/500\n",
            "\n",
            "Epoch 00483: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0527 - acc: 1.0000 - val_loss: 0.9305 - val_acc: 0.6500\n",
            "Epoch 484/500\n",
            "\n",
            "Epoch 00484: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0450 - acc: 1.0000 - val_loss: 0.9250 - val_acc: 0.6500\n",
            "Epoch 485/500\n",
            "\n",
            "Epoch 00485: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0871 - acc: 0.9500 - val_loss: 0.8896 - val_acc: 0.7000\n",
            "Epoch 486/500\n",
            "\n",
            "Epoch 00486: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0444 - acc: 0.9833 - val_loss: 0.8679 - val_acc: 0.7000\n",
            "Epoch 487/500\n",
            "\n",
            "Epoch 00487: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0394 - acc: 1.0000 - val_loss: 0.8660 - val_acc: 0.7000\n",
            "Epoch 488/500\n",
            "\n",
            "Epoch 00488: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0469 - acc: 1.0000 - val_loss: 0.8836 - val_acc: 0.7000\n",
            "Epoch 489/500\n",
            "\n",
            "Epoch 00489: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0426 - acc: 1.0000 - val_loss: 0.9147 - val_acc: 0.7000\n",
            "Epoch 490/500\n",
            "\n",
            "Epoch 00490: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0567 - acc: 1.0000 - val_loss: 0.9522 - val_acc: 0.7000\n",
            "Epoch 491/500\n",
            "\n",
            "Epoch 00491: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0565 - acc: 0.9833 - val_loss: 1.0426 - val_acc: 0.6500\n",
            "Epoch 492/500\n",
            "\n",
            "Epoch 00492: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0365 - acc: 1.0000 - val_loss: 1.1117 - val_acc: 0.6000\n",
            "Epoch 493/500\n",
            "\n",
            "Epoch 00493: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.1070 - acc: 0.9667 - val_loss: 1.1569 - val_acc: 0.5500\n",
            "Epoch 494/500\n",
            "\n",
            "Epoch 00494: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0753 - acc: 0.9833 - val_loss: 1.1528 - val_acc: 0.5500\n",
            "Epoch 495/500\n",
            "\n",
            "Epoch 00495: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0593 - acc: 0.9833 - val_loss: 1.0701 - val_acc: 0.6000\n",
            "Epoch 496/500\n",
            "\n",
            "Epoch 00496: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0544 - acc: 1.0000 - val_loss: 1.0056 - val_acc: 0.6500\n",
            "Epoch 497/500\n",
            "\n",
            "Epoch 00497: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0434 - acc: 1.0000 - val_loss: 0.9549 - val_acc: 0.7000\n",
            "Epoch 498/500\n",
            "\n",
            "Epoch 00498: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0843 - acc: 0.9833 - val_loss: 0.9165 - val_acc: 0.7000\n",
            "Epoch 499/500\n",
            "\n",
            "Epoch 00499: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0686 - acc: 0.9833 - val_loss: 0.8982 - val_acc: 0.7000\n",
            "Epoch 500/500\n",
            "\n",
            "Epoch 00500: val_loss did not improve from 0.70413\n",
            "60/60 - 0s - loss: 0.0853 - acc: 0.9833 - val_loss: 0.8861 - val_acc: 0.7000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUMKQEKIAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "# Evaluate on test data\n",
            "40/40 [==============================] - 0s 2ms/sample - loss: 0.9464 - acc: 0.6250\n",
            "test loss, test acc: [0.9463524913639049, 0.625]\n",
            "EEG_Deep/Data2A/parsed_P09T.mat\n",
            "Filtering of Training Data Finished\n",
            "EEG_Deep/Data2A/parsed_P09E.mat\n",
            "Filtering of Testing Data Finished\n",
            "(1, 2)\n",
            "Finding labels in training data\n",
            "Finding labels in testing data\n",
            "[1 2 1 2 1 2 1 1 1 1 2 1 1 2 1 2 2 1 1 1]\n",
            "(60, 12, 1536)\n",
            "(60,)\n",
            "(20, 12, 1536)\n",
            "(20,)\n",
            "[[1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]]\n",
            "X_train shape: (60, 1, 12, 1536)\n",
            "60 train samples\n",
            "20 val samples\n",
            "(40, 12, 1536)\n",
            "(40, 1)\n",
            "X_train shape: (40, 1, 12, 1536)\n",
            "40 train samples\n",
            "Train on 60 samples, validate on 20 samples\n",
            "Epoch 1/500\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.69218, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 1s - loss: 0.7718 - acc: 0.3833 - val_loss: 0.6922 - val_acc: 0.5000\n",
            "Epoch 2/500\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.69218\n",
            "60/60 - 0s - loss: 0.6903 - acc: 0.5667 - val_loss: 0.6940 - val_acc: 0.5000\n",
            "Epoch 3/500\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.69218\n",
            "60/60 - 0s - loss: 0.6585 - acc: 0.6000 - val_loss: 0.6966 - val_acc: 0.4500\n",
            "Epoch 4/500\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.69218\n",
            "60/60 - 0s - loss: 0.6596 - acc: 0.6000 - val_loss: 0.6984 - val_acc: 0.4000\n",
            "Epoch 5/500\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.69218\n",
            "60/60 - 0s - loss: 0.6327 - acc: 0.6500 - val_loss: 0.6995 - val_acc: 0.3500\n",
            "Epoch 6/500\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.69218\n",
            "60/60 - 0s - loss: 0.6287 - acc: 0.6167 - val_loss: 0.6999 - val_acc: 0.3500\n",
            "Epoch 7/500\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.69218\n",
            "60/60 - 0s - loss: 0.6092 - acc: 0.7500 - val_loss: 0.7010 - val_acc: 0.3500\n",
            "Epoch 8/500\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.69218\n",
            "60/60 - 0s - loss: 0.5925 - acc: 0.7667 - val_loss: 0.7018 - val_acc: 0.3500\n",
            "Epoch 9/500\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.69218\n",
            "60/60 - 0s - loss: 0.5993 - acc: 0.7333 - val_loss: 0.7029 - val_acc: 0.3500\n",
            "Epoch 10/500\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.69218\n",
            "60/60 - 0s - loss: 0.5796 - acc: 0.8333 - val_loss: 0.7034 - val_acc: 0.3500\n",
            "Epoch 11/500\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.69218\n",
            "60/60 - 0s - loss: 0.5636 - acc: 0.8333 - val_loss: 0.7059 - val_acc: 0.3500\n",
            "Epoch 12/500\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.69218\n",
            "60/60 - 0s - loss: 0.5267 - acc: 0.8667 - val_loss: 0.7107 - val_acc: 0.3000\n",
            "Epoch 13/500\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.69218\n",
            "60/60 - 0s - loss: 0.5459 - acc: 0.8500 - val_loss: 0.7174 - val_acc: 0.3000\n",
            "Epoch 14/500\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.69218\n",
            "60/60 - 0s - loss: 0.5274 - acc: 0.8833 - val_loss: 0.7235 - val_acc: 0.3000\n",
            "Epoch 15/500\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.69218\n",
            "60/60 - 0s - loss: 0.5374 - acc: 0.8333 - val_loss: 0.7275 - val_acc: 0.3000\n",
            "Epoch 16/500\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.69218\n",
            "60/60 - 0s - loss: 0.5365 - acc: 0.8667 - val_loss: 0.7312 - val_acc: 0.3000\n",
            "Epoch 17/500\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.69218\n",
            "60/60 - 0s - loss: 0.4969 - acc: 0.8667 - val_loss: 0.7356 - val_acc: 0.3000\n",
            "Epoch 18/500\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.69218\n",
            "60/60 - 0s - loss: 0.4875 - acc: 0.8500 - val_loss: 0.7388 - val_acc: 0.3000\n",
            "Epoch 19/500\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.69218\n",
            "60/60 - 0s - loss: 0.4783 - acc: 0.9000 - val_loss: 0.7407 - val_acc: 0.3000\n",
            "Epoch 20/500\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.69218\n",
            "60/60 - 0s - loss: 0.4871 - acc: 0.8333 - val_loss: 0.7457 - val_acc: 0.3000\n",
            "Epoch 21/500\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.69218\n",
            "60/60 - 0s - loss: 0.4356 - acc: 0.8833 - val_loss: 0.7488 - val_acc: 0.3000\n",
            "Epoch 22/500\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.69218\n",
            "60/60 - 0s - loss: 0.4502 - acc: 0.8667 - val_loss: 0.7547 - val_acc: 0.3000\n",
            "Epoch 23/500\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.69218\n",
            "60/60 - 0s - loss: 0.4629 - acc: 0.8500 - val_loss: 0.7618 - val_acc: 0.3000\n",
            "Epoch 24/500\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.69218\n",
            "60/60 - 0s - loss: 0.4205 - acc: 0.9167 - val_loss: 0.7650 - val_acc: 0.3000\n",
            "Epoch 25/500\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.69218\n",
            "60/60 - 0s - loss: 0.4302 - acc: 0.8667 - val_loss: 0.7724 - val_acc: 0.3000\n",
            "Epoch 26/500\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.69218\n",
            "60/60 - 0s - loss: 0.4446 - acc: 0.8500 - val_loss: 0.7821 - val_acc: 0.3500\n",
            "Epoch 27/500\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.69218\n",
            "60/60 - 0s - loss: 0.4319 - acc: 0.8833 - val_loss: 0.7892 - val_acc: 0.3500\n",
            "Epoch 28/500\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.69218\n",
            "60/60 - 0s - loss: 0.4316 - acc: 0.8500 - val_loss: 0.8056 - val_acc: 0.3500\n",
            "Epoch 29/500\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.69218\n",
            "60/60 - 0s - loss: 0.4377 - acc: 0.8667 - val_loss: 0.8138 - val_acc: 0.3500\n",
            "Epoch 30/500\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.69218\n",
            "60/60 - 0s - loss: 0.3991 - acc: 0.9167 - val_loss: 0.8202 - val_acc: 0.3500\n",
            "Epoch 31/500\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.69218\n",
            "60/60 - 0s - loss: 0.4355 - acc: 0.8667 - val_loss: 0.8207 - val_acc: 0.3500\n",
            "Epoch 32/500\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.69218\n",
            "60/60 - 0s - loss: 0.4102 - acc: 0.9167 - val_loss: 0.8271 - val_acc: 0.3500\n",
            "Epoch 33/500\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.69218\n",
            "60/60 - 0s - loss: 0.3742 - acc: 0.9167 - val_loss: 0.8418 - val_acc: 0.3500\n",
            "Epoch 34/500\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.69218\n",
            "60/60 - 0s - loss: 0.3775 - acc: 0.9167 - val_loss: 0.8555 - val_acc: 0.3500\n",
            "Epoch 35/500\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.69218\n",
            "60/60 - 0s - loss: 0.3387 - acc: 0.9333 - val_loss: 0.8762 - val_acc: 0.3500\n",
            "Epoch 36/500\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.69218\n",
            "60/60 - 0s - loss: 0.3849 - acc: 0.9000 - val_loss: 0.8784 - val_acc: 0.3500\n",
            "Epoch 37/500\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.69218\n",
            "60/60 - 0s - loss: 0.3247 - acc: 0.9333 - val_loss: 0.8794 - val_acc: 0.3500\n",
            "Epoch 38/500\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.69218\n",
            "60/60 - 0s - loss: 0.3585 - acc: 0.9167 - val_loss: 0.8739 - val_acc: 0.3500\n",
            "Epoch 39/500\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.69218\n",
            "60/60 - 0s - loss: 0.3835 - acc: 0.8667 - val_loss: 0.8690 - val_acc: 0.3500\n",
            "Epoch 40/500\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.69218\n",
            "60/60 - 0s - loss: 0.3600 - acc: 0.8833 - val_loss: 0.8534 - val_acc: 0.3500\n",
            "Epoch 41/500\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.69218\n",
            "60/60 - 0s - loss: 0.3625 - acc: 0.8833 - val_loss: 0.8481 - val_acc: 0.3500\n",
            "Epoch 42/500\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.69218\n",
            "60/60 - 0s - loss: 0.3047 - acc: 0.9167 - val_loss: 0.8472 - val_acc: 0.3500\n",
            "Epoch 43/500\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.69218\n",
            "60/60 - 0s - loss: 0.3465 - acc: 0.8833 - val_loss: 0.8615 - val_acc: 0.3500\n",
            "Epoch 44/500\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.69218\n",
            "60/60 - 0s - loss: 0.3433 - acc: 0.9333 - val_loss: 0.8773 - val_acc: 0.3500\n",
            "Epoch 45/500\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.69218\n",
            "60/60 - 0s - loss: 0.3087 - acc: 0.9333 - val_loss: 0.9059 - val_acc: 0.3500\n",
            "Epoch 46/500\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.69218\n",
            "60/60 - 0s - loss: 0.3200 - acc: 0.9333 - val_loss: 0.9329 - val_acc: 0.3500\n",
            "Epoch 47/500\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.69218\n",
            "60/60 - 0s - loss: 0.3386 - acc: 0.9167 - val_loss: 0.9424 - val_acc: 0.3500\n",
            "Epoch 48/500\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.69218\n",
            "60/60 - 0s - loss: 0.3321 - acc: 0.9000 - val_loss: 0.9451 - val_acc: 0.3500\n",
            "Epoch 49/500\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.69218\n",
            "60/60 - 0s - loss: 0.3028 - acc: 0.9167 - val_loss: 0.9456 - val_acc: 0.3500\n",
            "Epoch 50/500\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.69218\n",
            "60/60 - 0s - loss: 0.2867 - acc: 0.9333 - val_loss: 0.9328 - val_acc: 0.3500\n",
            "Epoch 51/500\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.69218\n",
            "60/60 - 0s - loss: 0.3026 - acc: 0.9000 - val_loss: 0.9196 - val_acc: 0.3500\n",
            "Epoch 52/500\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.69218\n",
            "60/60 - 0s - loss: 0.3186 - acc: 0.9500 - val_loss: 0.9226 - val_acc: 0.3500\n",
            "Epoch 53/500\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.69218\n",
            "60/60 - 0s - loss: 0.3008 - acc: 0.9000 - val_loss: 0.9190 - val_acc: 0.4000\n",
            "Epoch 54/500\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.69218\n",
            "60/60 - 0s - loss: 0.3336 - acc: 0.8833 - val_loss: 0.9092 - val_acc: 0.4000\n",
            "Epoch 55/500\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.69218\n",
            "60/60 - 0s - loss: 0.2878 - acc: 0.8833 - val_loss: 0.8922 - val_acc: 0.4000\n",
            "Epoch 56/500\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.69218\n",
            "60/60 - 0s - loss: 0.2829 - acc: 0.9500 - val_loss: 0.8762 - val_acc: 0.4000\n",
            "Epoch 57/500\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.69218\n",
            "60/60 - 0s - loss: 0.3260 - acc: 0.9000 - val_loss: 0.8811 - val_acc: 0.4000\n",
            "Epoch 58/500\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.69218\n",
            "60/60 - 0s - loss: 0.2672 - acc: 0.9167 - val_loss: 0.8937 - val_acc: 0.4000\n",
            "Epoch 59/500\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.69218\n",
            "60/60 - 0s - loss: 0.3092 - acc: 0.9167 - val_loss: 0.8956 - val_acc: 0.4000\n",
            "Epoch 60/500\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.69218\n",
            "60/60 - 0s - loss: 0.2859 - acc: 0.9000 - val_loss: 0.8934 - val_acc: 0.4000\n",
            "Epoch 61/500\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.69218\n",
            "60/60 - 0s - loss: 0.2954 - acc: 0.9333 - val_loss: 0.9016 - val_acc: 0.4000\n",
            "Epoch 62/500\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.69218\n",
            "60/60 - 0s - loss: 0.2423 - acc: 0.9833 - val_loss: 0.8970 - val_acc: 0.4000\n",
            "Epoch 63/500\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.69218\n",
            "60/60 - 0s - loss: 0.2868 - acc: 0.9333 - val_loss: 0.8935 - val_acc: 0.4000\n",
            "Epoch 64/500\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.69218\n",
            "60/60 - 0s - loss: 0.2481 - acc: 0.9333 - val_loss: 0.8971 - val_acc: 0.4500\n",
            "Epoch 65/500\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.69218\n",
            "60/60 - 0s - loss: 0.2833 - acc: 0.9333 - val_loss: 0.9222 - val_acc: 0.4000\n",
            "Epoch 66/500\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.69218\n",
            "60/60 - 0s - loss: 0.2366 - acc: 0.9667 - val_loss: 0.9162 - val_acc: 0.4500\n",
            "Epoch 67/500\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.69218\n",
            "60/60 - 0s - loss: 0.2457 - acc: 0.9667 - val_loss: 0.8902 - val_acc: 0.5000\n",
            "Epoch 68/500\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.69218\n",
            "60/60 - 0s - loss: 0.2591 - acc: 0.9333 - val_loss: 0.8797 - val_acc: 0.4500\n",
            "Epoch 69/500\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.69218\n",
            "60/60 - 0s - loss: 0.2522 - acc: 0.9500 - val_loss: 0.8819 - val_acc: 0.4500\n",
            "Epoch 70/500\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.69218\n",
            "60/60 - 0s - loss: 0.2818 - acc: 0.9333 - val_loss: 0.8717 - val_acc: 0.4500\n",
            "Epoch 71/500\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.69218\n",
            "60/60 - 0s - loss: 0.2367 - acc: 0.9500 - val_loss: 0.8506 - val_acc: 0.4500\n",
            "Epoch 72/500\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.69218\n",
            "60/60 - 0s - loss: 0.2427 - acc: 0.9667 - val_loss: 0.8516 - val_acc: 0.4500\n",
            "Epoch 73/500\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.69218\n",
            "60/60 - 0s - loss: 0.2537 - acc: 0.9000 - val_loss: 0.8832 - val_acc: 0.4500\n",
            "Epoch 74/500\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.69218\n",
            "60/60 - 0s - loss: 0.2456 - acc: 0.9500 - val_loss: 0.9385 - val_acc: 0.4500\n",
            "Epoch 75/500\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.69218\n",
            "60/60 - 0s - loss: 0.2741 - acc: 0.9333 - val_loss: 0.9961 - val_acc: 0.4000\n",
            "Epoch 76/500\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.69218\n",
            "60/60 - 0s - loss: 0.2374 - acc: 0.9500 - val_loss: 1.0461 - val_acc: 0.4000\n",
            "Epoch 77/500\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.69218\n",
            "60/60 - 0s - loss: 0.2494 - acc: 0.9667 - val_loss: 1.0745 - val_acc: 0.4000\n",
            "Epoch 78/500\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.69218\n",
            "60/60 - 0s - loss: 0.2329 - acc: 0.9167 - val_loss: 1.1301 - val_acc: 0.4000\n",
            "Epoch 79/500\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.69218\n",
            "60/60 - 0s - loss: 0.2396 - acc: 0.9667 - val_loss: 1.1360 - val_acc: 0.4000\n",
            "Epoch 80/500\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.69218\n",
            "60/60 - 0s - loss: 0.2242 - acc: 0.9167 - val_loss: 1.1789 - val_acc: 0.4000\n",
            "Epoch 81/500\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.69218\n",
            "60/60 - 0s - loss: 0.2502 - acc: 0.9667 - val_loss: 1.1650 - val_acc: 0.4000\n",
            "Epoch 82/500\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.69218\n",
            "60/60 - 0s - loss: 0.2628 - acc: 0.9167 - val_loss: 1.1582 - val_acc: 0.4000\n",
            "Epoch 83/500\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.69218\n",
            "60/60 - 0s - loss: 0.2177 - acc: 0.9667 - val_loss: 1.1645 - val_acc: 0.4000\n",
            "Epoch 84/500\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.69218\n",
            "60/60 - 0s - loss: 0.2248 - acc: 0.9333 - val_loss: 1.1544 - val_acc: 0.3500\n",
            "Epoch 85/500\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.69218\n",
            "60/60 - 0s - loss: 0.2231 - acc: 0.9667 - val_loss: 1.1811 - val_acc: 0.3500\n",
            "Epoch 86/500\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.69218\n",
            "60/60 - 0s - loss: 0.2390 - acc: 0.9500 - val_loss: 1.1789 - val_acc: 0.3500\n",
            "Epoch 87/500\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.69218\n",
            "60/60 - 0s - loss: 0.2033 - acc: 0.9833 - val_loss: 1.1584 - val_acc: 0.3500\n",
            "Epoch 88/500\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.69218\n",
            "60/60 - 0s - loss: 0.2448 - acc: 0.9167 - val_loss: 1.1883 - val_acc: 0.3500\n",
            "Epoch 89/500\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.69218\n",
            "60/60 - 0s - loss: 0.2549 - acc: 0.9500 - val_loss: 1.2503 - val_acc: 0.3500\n",
            "Epoch 90/500\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.69218\n",
            "60/60 - 0s - loss: 0.2197 - acc: 0.9500 - val_loss: 1.2846 - val_acc: 0.3500\n",
            "Epoch 91/500\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.69218\n",
            "60/60 - 0s - loss: 0.2131 - acc: 0.9333 - val_loss: 1.2753 - val_acc: 0.4000\n",
            "Epoch 92/500\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.69218\n",
            "60/60 - 0s - loss: 0.2433 - acc: 0.9333 - val_loss: 1.2200 - val_acc: 0.4500\n",
            "Epoch 93/500\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.69218\n",
            "60/60 - 0s - loss: 0.2618 - acc: 0.9167 - val_loss: 1.1289 - val_acc: 0.5000\n",
            "Epoch 94/500\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.69218\n",
            "60/60 - 0s - loss: 0.2094 - acc: 0.9833 - val_loss: 1.0852 - val_acc: 0.5000\n",
            "Epoch 95/500\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.69218\n",
            "60/60 - 0s - loss: 0.1856 - acc: 0.9667 - val_loss: 1.0567 - val_acc: 0.5000\n",
            "Epoch 96/500\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.69218\n",
            "60/60 - 0s - loss: 0.2283 - acc: 0.9500 - val_loss: 1.0544 - val_acc: 0.5000\n",
            "Epoch 97/500\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.69218\n",
            "60/60 - 0s - loss: 0.2135 - acc: 0.9833 - val_loss: 0.9903 - val_acc: 0.5000\n",
            "Epoch 98/500\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.69218\n",
            "60/60 - 0s - loss: 0.2130 - acc: 0.9667 - val_loss: 0.9278 - val_acc: 0.5000\n",
            "Epoch 99/500\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.69218\n",
            "60/60 - 0s - loss: 0.2536 - acc: 0.9333 - val_loss: 0.8963 - val_acc: 0.5000\n",
            "Epoch 100/500\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.69218\n",
            "60/60 - 0s - loss: 0.1899 - acc: 0.9833 - val_loss: 0.8712 - val_acc: 0.5500\n",
            "Epoch 101/500\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.69218\n",
            "60/60 - 0s - loss: 0.2118 - acc: 0.9333 - val_loss: 0.8580 - val_acc: 0.5000\n",
            "Epoch 102/500\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.69218\n",
            "60/60 - 0s - loss: 0.2444 - acc: 0.9000 - val_loss: 0.8599 - val_acc: 0.5500\n",
            "Epoch 103/500\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.69218\n",
            "60/60 - 0s - loss: 0.2114 - acc: 0.9333 - val_loss: 0.8598 - val_acc: 0.5000\n",
            "Epoch 104/500\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.69218\n",
            "60/60 - 0s - loss: 0.2080 - acc: 0.9667 - val_loss: 0.8737 - val_acc: 0.5000\n",
            "Epoch 105/500\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 0.69218\n",
            "60/60 - 0s - loss: 0.2562 - acc: 0.8833 - val_loss: 0.8501 - val_acc: 0.5000\n",
            "Epoch 106/500\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 0.69218\n",
            "60/60 - 0s - loss: 0.1939 - acc: 0.9667 - val_loss: 0.8414 - val_acc: 0.5000\n",
            "Epoch 107/500\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.69218\n",
            "60/60 - 0s - loss: 0.2225 - acc: 0.9333 - val_loss: 0.8566 - val_acc: 0.5000\n",
            "Epoch 108/500\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 0.69218\n",
            "60/60 - 0s - loss: 0.2426 - acc: 0.9500 - val_loss: 0.8705 - val_acc: 0.5000\n",
            "Epoch 109/500\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.69218\n",
            "60/60 - 0s - loss: 0.1745 - acc: 0.9667 - val_loss: 0.8959 - val_acc: 0.5000\n",
            "Epoch 110/500\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.69218\n",
            "60/60 - 0s - loss: 0.1909 - acc: 0.9500 - val_loss: 0.8882 - val_acc: 0.5000\n",
            "Epoch 111/500\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 0.69218\n",
            "60/60 - 0s - loss: 0.2332 - acc: 0.9667 - val_loss: 0.8625 - val_acc: 0.5000\n",
            "Epoch 112/500\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 0.69218\n",
            "60/60 - 0s - loss: 0.1722 - acc: 1.0000 - val_loss: 0.8215 - val_acc: 0.5500\n",
            "Epoch 113/500\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.69218\n",
            "60/60 - 0s - loss: 0.2187 - acc: 0.9500 - val_loss: 0.8101 - val_acc: 0.5500\n",
            "Epoch 114/500\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.69218\n",
            "60/60 - 0s - loss: 0.1826 - acc: 0.9833 - val_loss: 0.7976 - val_acc: 0.5500\n",
            "Epoch 115/500\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.69218\n",
            "60/60 - 0s - loss: 0.2248 - acc: 0.9500 - val_loss: 0.7669 - val_acc: 0.5500\n",
            "Epoch 116/500\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.69218\n",
            "60/60 - 0s - loss: 0.2547 - acc: 0.9333 - val_loss: 0.7182 - val_acc: 0.5500\n",
            "Epoch 117/500\n",
            "\n",
            "Epoch 00117: val_loss improved from 0.69218 to 0.68641, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1710 - acc: 0.9667 - val_loss: 0.6864 - val_acc: 0.5500\n",
            "Epoch 118/500\n",
            "\n",
            "Epoch 00118: val_loss improved from 0.68641 to 0.67063, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1782 - acc: 0.9667 - val_loss: 0.6706 - val_acc: 0.6500\n",
            "Epoch 119/500\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.2153 - acc: 0.9667 - val_loss: 0.6814 - val_acc: 0.6500\n",
            "Epoch 120/500\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1683 - acc: 0.9500 - val_loss: 0.7134 - val_acc: 0.5500\n",
            "Epoch 121/500\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.2435 - acc: 0.9333 - val_loss: 0.7411 - val_acc: 0.5500\n",
            "Epoch 122/500\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.2064 - acc: 0.9500 - val_loss: 0.7740 - val_acc: 0.5500\n",
            "Epoch 123/500\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1616 - acc: 1.0000 - val_loss: 0.8056 - val_acc: 0.5000\n",
            "Epoch 124/500\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1977 - acc: 0.9500 - val_loss: 0.8460 - val_acc: 0.4500\n",
            "Epoch 125/500\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1810 - acc: 0.9667 - val_loss: 0.8764 - val_acc: 0.4500\n",
            "Epoch 126/500\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1819 - acc: 0.9833 - val_loss: 0.8892 - val_acc: 0.4500\n",
            "Epoch 127/500\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1766 - acc: 0.9833 - val_loss: 0.8716 - val_acc: 0.4500\n",
            "Epoch 128/500\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1950 - acc: 0.9333 - val_loss: 0.8444 - val_acc: 0.5000\n",
            "Epoch 129/500\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1879 - acc: 0.9667 - val_loss: 0.8095 - val_acc: 0.5000\n",
            "Epoch 130/500\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.2216 - acc: 0.9000 - val_loss: 0.7993 - val_acc: 0.5000\n",
            "Epoch 131/500\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1521 - acc: 1.0000 - val_loss: 0.8071 - val_acc: 0.5000\n",
            "Epoch 132/500\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.2614 - acc: 0.9167 - val_loss: 0.8030 - val_acc: 0.6000\n",
            "Epoch 133/500\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1474 - acc: 0.9667 - val_loss: 0.8064 - val_acc: 0.6000\n",
            "Epoch 134/500\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1874 - acc: 0.9333 - val_loss: 0.8074 - val_acc: 0.6000\n",
            "Epoch 135/500\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1535 - acc: 0.9833 - val_loss: 0.7606 - val_acc: 0.6000\n",
            "Epoch 136/500\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1798 - acc: 0.9500 - val_loss: 0.7456 - val_acc: 0.6000\n",
            "Epoch 137/500\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1967 - acc: 0.9667 - val_loss: 0.7374 - val_acc: 0.6000\n",
            "Epoch 138/500\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1863 - acc: 0.9667 - val_loss: 0.7436 - val_acc: 0.6500\n",
            "Epoch 139/500\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1762 - acc: 0.9500 - val_loss: 0.7471 - val_acc: 0.6500\n",
            "Epoch 140/500\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1456 - acc: 1.0000 - val_loss: 0.7598 - val_acc: 0.6000\n",
            "Epoch 141/500\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1875 - acc: 0.9667 - val_loss: 0.7909 - val_acc: 0.5500\n",
            "Epoch 142/500\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1578 - acc: 0.9667 - val_loss: 0.8220 - val_acc: 0.5000\n",
            "Epoch 143/500\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1830 - acc: 0.9500 - val_loss: 0.8756 - val_acc: 0.4500\n",
            "Epoch 144/500\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1592 - acc: 0.9500 - val_loss: 0.9060 - val_acc: 0.4500\n",
            "Epoch 145/500\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1663 - acc: 0.9833 - val_loss: 0.9175 - val_acc: 0.5000\n",
            "Epoch 146/500\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1645 - acc: 0.9500 - val_loss: 0.9118 - val_acc: 0.5000\n",
            "Epoch 147/500\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.2029 - acc: 0.9833 - val_loss: 0.8856 - val_acc: 0.4500\n",
            "Epoch 148/500\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1498 - acc: 0.9667 - val_loss: 0.9070 - val_acc: 0.4500\n",
            "Epoch 149/500\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1638 - acc: 0.9833 - val_loss: 0.9129 - val_acc: 0.4500\n",
            "Epoch 150/500\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1693 - acc: 0.9833 - val_loss: 0.8960 - val_acc: 0.4500\n",
            "Epoch 151/500\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1738 - acc: 0.9667 - val_loss: 0.8952 - val_acc: 0.4500\n",
            "Epoch 152/500\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1322 - acc: 0.9667 - val_loss: 0.8975 - val_acc: 0.5000\n",
            "Epoch 153/500\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1567 - acc: 0.9667 - val_loss: 0.8900 - val_acc: 0.5000\n",
            "Epoch 154/500\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1665 - acc: 0.9667 - val_loss: 0.8602 - val_acc: 0.6000\n",
            "Epoch 155/500\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1688 - acc: 0.9833 - val_loss: 0.8855 - val_acc: 0.5500\n",
            "Epoch 156/500\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.2046 - acc: 0.9167 - val_loss: 0.9155 - val_acc: 0.5500\n",
            "Epoch 157/500\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1444 - acc: 1.0000 - val_loss: 0.9219 - val_acc: 0.5500\n",
            "Epoch 158/500\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1701 - acc: 0.9667 - val_loss: 0.9079 - val_acc: 0.6000\n",
            "Epoch 159/500\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1489 - acc: 0.9833 - val_loss: 0.8678 - val_acc: 0.6000\n",
            "Epoch 160/500\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1405 - acc: 1.0000 - val_loss: 0.8849 - val_acc: 0.5500\n",
            "Epoch 161/500\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1134 - acc: 1.0000 - val_loss: 0.8826 - val_acc: 0.5500\n",
            "Epoch 162/500\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1972 - acc: 0.9500 - val_loss: 0.8707 - val_acc: 0.6000\n",
            "Epoch 163/500\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1516 - acc: 0.9833 - val_loss: 0.8893 - val_acc: 0.5500\n",
            "Epoch 164/500\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1275 - acc: 1.0000 - val_loss: 0.9807 - val_acc: 0.5500\n",
            "Epoch 165/500\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1779 - acc: 0.9667 - val_loss: 0.9754 - val_acc: 0.5500\n",
            "Epoch 166/500\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1602 - acc: 0.9500 - val_loss: 0.9296 - val_acc: 0.5500\n",
            "Epoch 167/500\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1539 - acc: 0.9667 - val_loss: 0.8790 - val_acc: 0.5500\n",
            "Epoch 168/500\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1568 - acc: 0.9833 - val_loss: 0.8692 - val_acc: 0.5500\n",
            "Epoch 169/500\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1605 - acc: 0.9500 - val_loss: 0.8780 - val_acc: 0.5500\n",
            "Epoch 170/500\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1739 - acc: 0.9667 - val_loss: 0.8265 - val_acc: 0.5500\n",
            "Epoch 171/500\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1180 - acc: 1.0000 - val_loss: 0.7935 - val_acc: 0.6000\n",
            "Epoch 172/500\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1316 - acc: 0.9833 - val_loss: 0.7577 - val_acc: 0.6500\n",
            "Epoch 173/500\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1464 - acc: 0.9833 - val_loss: 0.7445 - val_acc: 0.6500\n",
            "Epoch 174/500\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1197 - acc: 0.9833 - val_loss: 0.7532 - val_acc: 0.6500\n",
            "Epoch 175/500\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.2032 - acc: 0.9500 - val_loss: 0.8029 - val_acc: 0.6000\n",
            "Epoch 176/500\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1247 - acc: 0.9667 - val_loss: 0.8133 - val_acc: 0.6500\n",
            "Epoch 177/500\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1814 - acc: 0.9333 - val_loss: 0.8370 - val_acc: 0.6000\n",
            "Epoch 178/500\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1191 - acc: 0.9833 - val_loss: 0.9159 - val_acc: 0.5000\n",
            "Epoch 179/500\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1244 - acc: 1.0000 - val_loss: 0.9929 - val_acc: 0.5000\n",
            "Epoch 180/500\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1395 - acc: 0.9833 - val_loss: 1.0184 - val_acc: 0.5000\n",
            "Epoch 181/500\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1363 - acc: 0.9833 - val_loss: 1.0047 - val_acc: 0.5000\n",
            "Epoch 182/500\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1569 - acc: 0.9667 - val_loss: 0.9418 - val_acc: 0.5000\n",
            "Epoch 183/500\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1288 - acc: 1.0000 - val_loss: 0.9264 - val_acc: 0.5000\n",
            "Epoch 184/500\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1194 - acc: 1.0000 - val_loss: 0.9229 - val_acc: 0.5000\n",
            "Epoch 185/500\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1733 - acc: 0.9333 - val_loss: 0.9541 - val_acc: 0.5000\n",
            "Epoch 186/500\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1727 - acc: 0.9500 - val_loss: 0.9686 - val_acc: 0.5000\n",
            "Epoch 187/500\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1386 - acc: 0.9833 - val_loss: 1.0969 - val_acc: 0.5000\n",
            "Epoch 188/500\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1101 - acc: 1.0000 - val_loss: 1.1831 - val_acc: 0.4500\n",
            "Epoch 189/500\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1549 - acc: 0.9833 - val_loss: 1.2922 - val_acc: 0.4500\n",
            "Epoch 190/500\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1571 - acc: 0.9500 - val_loss: 1.1999 - val_acc: 0.5000\n",
            "Epoch 191/500\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1434 - acc: 0.9667 - val_loss: 1.0504 - val_acc: 0.5000\n",
            "Epoch 192/500\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.2037 - acc: 0.9167 - val_loss: 0.9095 - val_acc: 0.5500\n",
            "Epoch 193/500\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1668 - acc: 0.9667 - val_loss: 0.8490 - val_acc: 0.5500\n",
            "Epoch 194/500\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1253 - acc: 1.0000 - val_loss: 0.8366 - val_acc: 0.5500\n",
            "Epoch 195/500\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1314 - acc: 0.9833 - val_loss: 0.7956 - val_acc: 0.5500\n",
            "Epoch 196/500\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1550 - acc: 0.9667 - val_loss: 0.7934 - val_acc: 0.5000\n",
            "Epoch 197/500\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1191 - acc: 1.0000 - val_loss: 0.7722 - val_acc: 0.5500\n",
            "Epoch 198/500\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1378 - acc: 1.0000 - val_loss: 0.7848 - val_acc: 0.6500\n",
            "Epoch 199/500\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1479 - acc: 0.9833 - val_loss: 0.8342 - val_acc: 0.5500\n",
            "Epoch 200/500\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1430 - acc: 0.9833 - val_loss: 0.9297 - val_acc: 0.5000\n",
            "Epoch 201/500\n",
            "\n",
            "Epoch 00201: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1641 - acc: 0.9333 - val_loss: 1.0139 - val_acc: 0.5000\n",
            "Epoch 202/500\n",
            "\n",
            "Epoch 00202: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1509 - acc: 0.9833 - val_loss: 1.0121 - val_acc: 0.5000\n",
            "Epoch 203/500\n",
            "\n",
            "Epoch 00203: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1509 - acc: 0.9333 - val_loss: 1.1097 - val_acc: 0.5000\n",
            "Epoch 204/500\n",
            "\n",
            "Epoch 00204: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1160 - acc: 0.9833 - val_loss: 1.1011 - val_acc: 0.5000\n",
            "Epoch 205/500\n",
            "\n",
            "Epoch 00205: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1206 - acc: 1.0000 - val_loss: 1.0200 - val_acc: 0.5000\n",
            "Epoch 206/500\n",
            "\n",
            "Epoch 00206: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1260 - acc: 1.0000 - val_loss: 0.9924 - val_acc: 0.5000\n",
            "Epoch 207/500\n",
            "\n",
            "Epoch 00207: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1143 - acc: 1.0000 - val_loss: 1.0303 - val_acc: 0.5000\n",
            "Epoch 208/500\n",
            "\n",
            "Epoch 00208: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1526 - acc: 0.9500 - val_loss: 1.0068 - val_acc: 0.5000\n",
            "Epoch 209/500\n",
            "\n",
            "Epoch 00209: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0938 - acc: 0.9833 - val_loss: 0.9417 - val_acc: 0.5000\n",
            "Epoch 210/500\n",
            "\n",
            "Epoch 00210: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1494 - acc: 0.9667 - val_loss: 0.8818 - val_acc: 0.5500\n",
            "Epoch 211/500\n",
            "\n",
            "Epoch 00211: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0917 - acc: 1.0000 - val_loss: 0.8521 - val_acc: 0.5000\n",
            "Epoch 212/500\n",
            "\n",
            "Epoch 00212: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1132 - acc: 0.9833 - val_loss: 0.8704 - val_acc: 0.5000\n",
            "Epoch 213/500\n",
            "\n",
            "Epoch 00213: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1613 - acc: 0.9667 - val_loss: 0.8950 - val_acc: 0.5000\n",
            "Epoch 214/500\n",
            "\n",
            "Epoch 00214: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1887 - acc: 0.9500 - val_loss: 0.9654 - val_acc: 0.5000\n",
            "Epoch 215/500\n",
            "\n",
            "Epoch 00215: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1166 - acc: 1.0000 - val_loss: 0.9972 - val_acc: 0.5000\n",
            "Epoch 216/500\n",
            "\n",
            "Epoch 00216: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1330 - acc: 0.9667 - val_loss: 1.0064 - val_acc: 0.5000\n",
            "Epoch 217/500\n",
            "\n",
            "Epoch 00217: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1337 - acc: 0.9500 - val_loss: 0.9945 - val_acc: 0.5500\n",
            "Epoch 218/500\n",
            "\n",
            "Epoch 00218: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1429 - acc: 1.0000 - val_loss: 0.9777 - val_acc: 0.5500\n",
            "Epoch 219/500\n",
            "\n",
            "Epoch 00219: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0998 - acc: 1.0000 - val_loss: 0.9838 - val_acc: 0.5500\n",
            "Epoch 220/500\n",
            "\n",
            "Epoch 00220: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1119 - acc: 1.0000 - val_loss: 0.9810 - val_acc: 0.6000\n",
            "Epoch 221/500\n",
            "\n",
            "Epoch 00221: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1421 - acc: 0.9333 - val_loss: 0.9155 - val_acc: 0.6000\n",
            "Epoch 222/500\n",
            "\n",
            "Epoch 00222: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1049 - acc: 1.0000 - val_loss: 0.9026 - val_acc: 0.6000\n",
            "Epoch 223/500\n",
            "\n",
            "Epoch 00223: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1422 - acc: 0.9667 - val_loss: 0.9469 - val_acc: 0.6000\n",
            "Epoch 224/500\n",
            "\n",
            "Epoch 00224: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1112 - acc: 0.9833 - val_loss: 0.9594 - val_acc: 0.6000\n",
            "Epoch 225/500\n",
            "\n",
            "Epoch 00225: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1125 - acc: 0.9833 - val_loss: 0.9865 - val_acc: 0.6000\n",
            "Epoch 226/500\n",
            "\n",
            "Epoch 00226: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1968 - acc: 0.9167 - val_loss: 0.9055 - val_acc: 0.6500\n",
            "Epoch 227/500\n",
            "\n",
            "Epoch 00227: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1259 - acc: 0.9833 - val_loss: 0.8451 - val_acc: 0.6500\n",
            "Epoch 228/500\n",
            "\n",
            "Epoch 00228: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0960 - acc: 1.0000 - val_loss: 0.8185 - val_acc: 0.6500\n",
            "Epoch 229/500\n",
            "\n",
            "Epoch 00229: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1441 - acc: 1.0000 - val_loss: 0.8046 - val_acc: 0.6000\n",
            "Epoch 230/500\n",
            "\n",
            "Epoch 00230: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1539 - acc: 0.9500 - val_loss: 0.8264 - val_acc: 0.6500\n",
            "Epoch 231/500\n",
            "\n",
            "Epoch 00231: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1026 - acc: 0.9833 - val_loss: 0.8652 - val_acc: 0.6500\n",
            "Epoch 232/500\n",
            "\n",
            "Epoch 00232: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1435 - acc: 0.9667 - val_loss: 0.8894 - val_acc: 0.6000\n",
            "Epoch 233/500\n",
            "\n",
            "Epoch 00233: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0880 - acc: 1.0000 - val_loss: 0.9292 - val_acc: 0.6000\n",
            "Epoch 234/500\n",
            "\n",
            "Epoch 00234: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1013 - acc: 1.0000 - val_loss: 0.9716 - val_acc: 0.5000\n",
            "Epoch 235/500\n",
            "\n",
            "Epoch 00235: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1025 - acc: 1.0000 - val_loss: 1.0699 - val_acc: 0.5000\n",
            "Epoch 236/500\n",
            "\n",
            "Epoch 00236: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1096 - acc: 0.9667 - val_loss: 1.1785 - val_acc: 0.5000\n",
            "Epoch 237/500\n",
            "\n",
            "Epoch 00237: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1158 - acc: 1.0000 - val_loss: 1.2441 - val_acc: 0.5500\n",
            "Epoch 238/500\n",
            "\n",
            "Epoch 00238: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1026 - acc: 1.0000 - val_loss: 1.2611 - val_acc: 0.5000\n",
            "Epoch 239/500\n",
            "\n",
            "Epoch 00239: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1216 - acc: 0.9667 - val_loss: 1.1467 - val_acc: 0.5500\n",
            "Epoch 240/500\n",
            "\n",
            "Epoch 00240: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1090 - acc: 1.0000 - val_loss: 1.0795 - val_acc: 0.5500\n",
            "Epoch 241/500\n",
            "\n",
            "Epoch 00241: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1210 - acc: 0.9833 - val_loss: 1.0555 - val_acc: 0.5000\n",
            "Epoch 242/500\n",
            "\n",
            "Epoch 00242: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1802 - acc: 0.9333 - val_loss: 1.0573 - val_acc: 0.4500\n",
            "Epoch 243/500\n",
            "\n",
            "Epoch 00243: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1476 - acc: 0.9500 - val_loss: 1.1129 - val_acc: 0.4500\n",
            "Epoch 244/500\n",
            "\n",
            "Epoch 00244: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1105 - acc: 1.0000 - val_loss: 1.1037 - val_acc: 0.4500\n",
            "Epoch 245/500\n",
            "\n",
            "Epoch 00245: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1646 - acc: 0.9333 - val_loss: 1.1066 - val_acc: 0.4500\n",
            "Epoch 246/500\n",
            "\n",
            "Epoch 00246: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1396 - acc: 0.9667 - val_loss: 1.1390 - val_acc: 0.4500\n",
            "Epoch 247/500\n",
            "\n",
            "Epoch 00247: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1083 - acc: 1.0000 - val_loss: 1.1481 - val_acc: 0.4000\n",
            "Epoch 248/500\n",
            "\n",
            "Epoch 00248: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1654 - acc: 0.9500 - val_loss: 1.1544 - val_acc: 0.4000\n",
            "Epoch 249/500\n",
            "\n",
            "Epoch 00249: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1217 - acc: 1.0000 - val_loss: 1.0807 - val_acc: 0.4500\n",
            "Epoch 250/500\n",
            "\n",
            "Epoch 00250: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1184 - acc: 0.9833 - val_loss: 1.0258 - val_acc: 0.5000\n",
            "Epoch 251/500\n",
            "\n",
            "Epoch 00251: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1040 - acc: 0.9833 - val_loss: 0.9775 - val_acc: 0.4500\n",
            "Epoch 252/500\n",
            "\n",
            "Epoch 00252: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1174 - acc: 0.9833 - val_loss: 1.0184 - val_acc: 0.5000\n",
            "Epoch 253/500\n",
            "\n",
            "Epoch 00253: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1215 - acc: 1.0000 - val_loss: 1.1166 - val_acc: 0.4500\n",
            "Epoch 254/500\n",
            "\n",
            "Epoch 00254: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0817 - acc: 1.0000 - val_loss: 1.1168 - val_acc: 0.4500\n",
            "Epoch 255/500\n",
            "\n",
            "Epoch 00255: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1256 - acc: 0.9667 - val_loss: 1.1270 - val_acc: 0.4500\n",
            "Epoch 256/500\n",
            "\n",
            "Epoch 00256: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1020 - acc: 1.0000 - val_loss: 1.1801 - val_acc: 0.4500\n",
            "Epoch 257/500\n",
            "\n",
            "Epoch 00257: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1035 - acc: 1.0000 - val_loss: 1.2635 - val_acc: 0.4000\n",
            "Epoch 258/500\n",
            "\n",
            "Epoch 00258: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1380 - acc: 0.9667 - val_loss: 1.3250 - val_acc: 0.4000\n",
            "Epoch 259/500\n",
            "\n",
            "Epoch 00259: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0998 - acc: 1.0000 - val_loss: 1.2937 - val_acc: 0.4000\n",
            "Epoch 260/500\n",
            "\n",
            "Epoch 00260: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1228 - acc: 1.0000 - val_loss: 1.2832 - val_acc: 0.4000\n",
            "Epoch 261/500\n",
            "\n",
            "Epoch 00261: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0879 - acc: 1.0000 - val_loss: 1.3379 - val_acc: 0.4000\n",
            "Epoch 262/500\n",
            "\n",
            "Epoch 00262: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0837 - acc: 1.0000 - val_loss: 1.4179 - val_acc: 0.4000\n",
            "Epoch 263/500\n",
            "\n",
            "Epoch 00263: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1049 - acc: 0.9833 - val_loss: 1.4634 - val_acc: 0.4000\n",
            "Epoch 264/500\n",
            "\n",
            "Epoch 00264: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0762 - acc: 0.9833 - val_loss: 1.4800 - val_acc: 0.4000\n",
            "Epoch 265/500\n",
            "\n",
            "Epoch 00265: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0966 - acc: 1.0000 - val_loss: 1.4699 - val_acc: 0.4000\n",
            "Epoch 266/500\n",
            "\n",
            "Epoch 00266: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1088 - acc: 0.9833 - val_loss: 1.4334 - val_acc: 0.4000\n",
            "Epoch 267/500\n",
            "\n",
            "Epoch 00267: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0959 - acc: 0.9833 - val_loss: 1.3864 - val_acc: 0.5000\n",
            "Epoch 268/500\n",
            "\n",
            "Epoch 00268: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0945 - acc: 0.9833 - val_loss: 1.4411 - val_acc: 0.4500\n",
            "Epoch 269/500\n",
            "\n",
            "Epoch 00269: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1098 - acc: 0.9667 - val_loss: 1.3882 - val_acc: 0.5000\n",
            "Epoch 270/500\n",
            "\n",
            "Epoch 00270: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1029 - acc: 0.9667 - val_loss: 1.2642 - val_acc: 0.4500\n",
            "Epoch 271/500\n",
            "\n",
            "Epoch 00271: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1202 - acc: 0.9833 - val_loss: 1.1877 - val_acc: 0.4500\n",
            "Epoch 272/500\n",
            "\n",
            "Epoch 00272: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0987 - acc: 1.0000 - val_loss: 1.1519 - val_acc: 0.4500\n",
            "Epoch 273/500\n",
            "\n",
            "Epoch 00273: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1165 - acc: 0.9833 - val_loss: 1.1482 - val_acc: 0.4500\n",
            "Epoch 274/500\n",
            "\n",
            "Epoch 00274: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1689 - acc: 0.9667 - val_loss: 1.1547 - val_acc: 0.5000\n",
            "Epoch 275/500\n",
            "\n",
            "Epoch 00275: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1075 - acc: 0.9667 - val_loss: 1.0677 - val_acc: 0.5500\n",
            "Epoch 276/500\n",
            "\n",
            "Epoch 00276: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1110 - acc: 0.9833 - val_loss: 1.0534 - val_acc: 0.6000\n",
            "Epoch 277/500\n",
            "\n",
            "Epoch 00277: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0972 - acc: 1.0000 - val_loss: 1.0488 - val_acc: 0.6000\n",
            "Epoch 278/500\n",
            "\n",
            "Epoch 00278: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1221 - acc: 0.9500 - val_loss: 1.0719 - val_acc: 0.5500\n",
            "Epoch 279/500\n",
            "\n",
            "Epoch 00279: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0884 - acc: 1.0000 - val_loss: 1.1716 - val_acc: 0.5500\n",
            "Epoch 280/500\n",
            "\n",
            "Epoch 00280: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1535 - acc: 0.9500 - val_loss: 1.3010 - val_acc: 0.4500\n",
            "Epoch 281/500\n",
            "\n",
            "Epoch 00281: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0760 - acc: 1.0000 - val_loss: 1.3332 - val_acc: 0.4500\n",
            "Epoch 282/500\n",
            "\n",
            "Epoch 00282: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1099 - acc: 0.9833 - val_loss: 1.2912 - val_acc: 0.4500\n",
            "Epoch 283/500\n",
            "\n",
            "Epoch 00283: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1124 - acc: 0.9500 - val_loss: 1.1625 - val_acc: 0.6000\n",
            "Epoch 284/500\n",
            "\n",
            "Epoch 00284: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1554 - acc: 0.9333 - val_loss: 1.1231 - val_acc: 0.6000\n",
            "Epoch 285/500\n",
            "\n",
            "Epoch 00285: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1137 - acc: 0.9833 - val_loss: 1.0185 - val_acc: 0.6500\n",
            "Epoch 286/500\n",
            "\n",
            "Epoch 00286: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0749 - acc: 1.0000 - val_loss: 0.9492 - val_acc: 0.6500\n",
            "Epoch 287/500\n",
            "\n",
            "Epoch 00287: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0944 - acc: 0.9833 - val_loss: 0.9508 - val_acc: 0.6500\n",
            "Epoch 288/500\n",
            "\n",
            "Epoch 00288: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0757 - acc: 1.0000 - val_loss: 0.9392 - val_acc: 0.6500\n",
            "Epoch 289/500\n",
            "\n",
            "Epoch 00289: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0875 - acc: 1.0000 - val_loss: 0.9430 - val_acc: 0.6500\n",
            "Epoch 290/500\n",
            "\n",
            "Epoch 00290: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1146 - acc: 0.9833 - val_loss: 0.9609 - val_acc: 0.6500\n",
            "Epoch 291/500\n",
            "\n",
            "Epoch 00291: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0864 - acc: 0.9833 - val_loss: 1.0121 - val_acc: 0.5500\n",
            "Epoch 292/500\n",
            "\n",
            "Epoch 00292: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0780 - acc: 1.0000 - val_loss: 1.0849 - val_acc: 0.5500\n",
            "Epoch 293/500\n",
            "\n",
            "Epoch 00293: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.2051 - acc: 0.9333 - val_loss: 1.2492 - val_acc: 0.4500\n",
            "Epoch 294/500\n",
            "\n",
            "Epoch 00294: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1028 - acc: 0.9833 - val_loss: 1.5370 - val_acc: 0.4000\n",
            "Epoch 295/500\n",
            "\n",
            "Epoch 00295: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0874 - acc: 0.9833 - val_loss: 1.7190 - val_acc: 0.4000\n",
            "Epoch 296/500\n",
            "\n",
            "Epoch 00296: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1412 - acc: 0.9500 - val_loss: 1.7162 - val_acc: 0.4000\n",
            "Epoch 297/500\n",
            "\n",
            "Epoch 00297: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0964 - acc: 0.9833 - val_loss: 1.5047 - val_acc: 0.4000\n",
            "Epoch 298/500\n",
            "\n",
            "Epoch 00298: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1439 - acc: 0.9500 - val_loss: 1.2062 - val_acc: 0.5000\n",
            "Epoch 299/500\n",
            "\n",
            "Epoch 00299: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1234 - acc: 0.9833 - val_loss: 1.0340 - val_acc: 0.5500\n",
            "Epoch 300/500\n",
            "\n",
            "Epoch 00300: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1128 - acc: 0.9833 - val_loss: 1.0137 - val_acc: 0.5000\n",
            "Epoch 301/500\n",
            "\n",
            "Epoch 00301: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1059 - acc: 1.0000 - val_loss: 1.0439 - val_acc: 0.4500\n",
            "Epoch 302/500\n",
            "\n",
            "Epoch 00302: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0760 - acc: 0.9833 - val_loss: 1.0685 - val_acc: 0.5000\n",
            "Epoch 303/500\n",
            "\n",
            "Epoch 00303: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1312 - acc: 0.9667 - val_loss: 1.0833 - val_acc: 0.5000\n",
            "Epoch 304/500\n",
            "\n",
            "Epoch 00304: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1052 - acc: 1.0000 - val_loss: 1.0580 - val_acc: 0.5000\n",
            "Epoch 305/500\n",
            "\n",
            "Epoch 00305: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1110 - acc: 0.9833 - val_loss: 1.0467 - val_acc: 0.5000\n",
            "Epoch 306/500\n",
            "\n",
            "Epoch 00306: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0894 - acc: 1.0000 - val_loss: 1.0707 - val_acc: 0.5000\n",
            "Epoch 307/500\n",
            "\n",
            "Epoch 00307: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0752 - acc: 1.0000 - val_loss: 1.0436 - val_acc: 0.5500\n",
            "Epoch 308/500\n",
            "\n",
            "Epoch 00308: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1343 - acc: 0.9333 - val_loss: 1.0469 - val_acc: 0.5500\n",
            "Epoch 309/500\n",
            "\n",
            "Epoch 00309: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0713 - acc: 0.9833 - val_loss: 1.0345 - val_acc: 0.5500\n",
            "Epoch 310/500\n",
            "\n",
            "Epoch 00310: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0820 - acc: 1.0000 - val_loss: 1.0633 - val_acc: 0.5500\n",
            "Epoch 311/500\n",
            "\n",
            "Epoch 00311: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1327 - acc: 0.9667 - val_loss: 1.0207 - val_acc: 0.5500\n",
            "Epoch 312/500\n",
            "\n",
            "Epoch 00312: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0836 - acc: 0.9667 - val_loss: 0.9607 - val_acc: 0.5500\n",
            "Epoch 313/500\n",
            "\n",
            "Epoch 00313: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0828 - acc: 0.9667 - val_loss: 0.9921 - val_acc: 0.5500\n",
            "Epoch 314/500\n",
            "\n",
            "Epoch 00314: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1490 - acc: 0.9500 - val_loss: 0.9716 - val_acc: 0.6000\n",
            "Epoch 315/500\n",
            "\n",
            "Epoch 00315: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0722 - acc: 1.0000 - val_loss: 0.8961 - val_acc: 0.5500\n",
            "Epoch 316/500\n",
            "\n",
            "Epoch 00316: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0987 - acc: 1.0000 - val_loss: 0.8823 - val_acc: 0.6000\n",
            "Epoch 317/500\n",
            "\n",
            "Epoch 00317: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0835 - acc: 0.9833 - val_loss: 0.9013 - val_acc: 0.6000\n",
            "Epoch 318/500\n",
            "\n",
            "Epoch 00318: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1137 - acc: 0.9833 - val_loss: 0.9986 - val_acc: 0.6000\n",
            "Epoch 319/500\n",
            "\n",
            "Epoch 00319: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1012 - acc: 0.9833 - val_loss: 1.0658 - val_acc: 0.5500\n",
            "Epoch 320/500\n",
            "\n",
            "Epoch 00320: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0825 - acc: 0.9833 - val_loss: 1.0933 - val_acc: 0.5500\n",
            "Epoch 321/500\n",
            "\n",
            "Epoch 00321: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0885 - acc: 0.9833 - val_loss: 1.0605 - val_acc: 0.5000\n",
            "Epoch 322/500\n",
            "\n",
            "Epoch 00322: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0756 - acc: 1.0000 - val_loss: 1.0334 - val_acc: 0.6500\n",
            "Epoch 323/500\n",
            "\n",
            "Epoch 00323: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1069 - acc: 0.9667 - val_loss: 1.0482 - val_acc: 0.6500\n",
            "Epoch 324/500\n",
            "\n",
            "Epoch 00324: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1157 - acc: 0.9667 - val_loss: 1.0332 - val_acc: 0.6500\n",
            "Epoch 325/500\n",
            "\n",
            "Epoch 00325: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0836 - acc: 1.0000 - val_loss: 0.8819 - val_acc: 0.6500\n",
            "Epoch 326/500\n",
            "\n",
            "Epoch 00326: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1009 - acc: 0.9667 - val_loss: 0.8707 - val_acc: 0.6500\n",
            "Epoch 327/500\n",
            "\n",
            "Epoch 00327: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0669 - acc: 1.0000 - val_loss: 0.9187 - val_acc: 0.6500\n",
            "Epoch 328/500\n",
            "\n",
            "Epoch 00328: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1061 - acc: 0.9667 - val_loss: 1.0342 - val_acc: 0.6000\n",
            "Epoch 329/500\n",
            "\n",
            "Epoch 00329: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0750 - acc: 1.0000 - val_loss: 1.1603 - val_acc: 0.5500\n",
            "Epoch 330/500\n",
            "\n",
            "Epoch 00330: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0893 - acc: 1.0000 - val_loss: 1.1873 - val_acc: 0.5500\n",
            "Epoch 331/500\n",
            "\n",
            "Epoch 00331: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0788 - acc: 1.0000 - val_loss: 1.2181 - val_acc: 0.5000\n",
            "Epoch 332/500\n",
            "\n",
            "Epoch 00332: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0697 - acc: 1.0000 - val_loss: 1.1904 - val_acc: 0.5000\n",
            "Epoch 333/500\n",
            "\n",
            "Epoch 00333: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0737 - acc: 0.9833 - val_loss: 1.1365 - val_acc: 0.5000\n",
            "Epoch 334/500\n",
            "\n",
            "Epoch 00334: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1102 - acc: 0.9833 - val_loss: 1.1080 - val_acc: 0.6000\n",
            "Epoch 335/500\n",
            "\n",
            "Epoch 00335: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0826 - acc: 1.0000 - val_loss: 1.1314 - val_acc: 0.6000\n",
            "Epoch 336/500\n",
            "\n",
            "Epoch 00336: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0574 - acc: 1.0000 - val_loss: 1.1309 - val_acc: 0.6000\n",
            "Epoch 337/500\n",
            "\n",
            "Epoch 00337: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1119 - acc: 1.0000 - val_loss: 1.1965 - val_acc: 0.5000\n",
            "Epoch 338/500\n",
            "\n",
            "Epoch 00338: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0760 - acc: 1.0000 - val_loss: 1.1658 - val_acc: 0.5500\n",
            "Epoch 339/500\n",
            "\n",
            "Epoch 00339: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0868 - acc: 0.9833 - val_loss: 1.0513 - val_acc: 0.5500\n",
            "Epoch 340/500\n",
            "\n",
            "Epoch 00340: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0844 - acc: 0.9833 - val_loss: 0.9997 - val_acc: 0.5500\n",
            "Epoch 341/500\n",
            "\n",
            "Epoch 00341: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0833 - acc: 1.0000 - val_loss: 0.9501 - val_acc: 0.6000\n",
            "Epoch 342/500\n",
            "\n",
            "Epoch 00342: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0927 - acc: 0.9833 - val_loss: 0.8938 - val_acc: 0.6500\n",
            "Epoch 343/500\n",
            "\n",
            "Epoch 00343: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1148 - acc: 0.9667 - val_loss: 0.9123 - val_acc: 0.6000\n",
            "Epoch 344/500\n",
            "\n",
            "Epoch 00344: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1326 - acc: 0.9667 - val_loss: 0.9835 - val_acc: 0.6000\n",
            "Epoch 345/500\n",
            "\n",
            "Epoch 00345: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0999 - acc: 0.9667 - val_loss: 1.1051 - val_acc: 0.5000\n",
            "Epoch 346/500\n",
            "\n",
            "Epoch 00346: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0972 - acc: 0.9833 - val_loss: 1.2536 - val_acc: 0.4000\n",
            "Epoch 347/500\n",
            "\n",
            "Epoch 00347: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1190 - acc: 0.9667 - val_loss: 1.3446 - val_acc: 0.4000\n",
            "Epoch 348/500\n",
            "\n",
            "Epoch 00348: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0970 - acc: 1.0000 - val_loss: 1.3019 - val_acc: 0.4000\n",
            "Epoch 349/500\n",
            "\n",
            "Epoch 00349: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0815 - acc: 1.0000 - val_loss: 1.2028 - val_acc: 0.4500\n",
            "Epoch 350/500\n",
            "\n",
            "Epoch 00350: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0902 - acc: 0.9833 - val_loss: 1.1222 - val_acc: 0.4500\n",
            "Epoch 351/500\n",
            "\n",
            "Epoch 00351: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0767 - acc: 1.0000 - val_loss: 1.1376 - val_acc: 0.5000\n",
            "Epoch 352/500\n",
            "\n",
            "Epoch 00352: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0757 - acc: 1.0000 - val_loss: 1.1419 - val_acc: 0.5000\n",
            "Epoch 353/500\n",
            "\n",
            "Epoch 00353: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0822 - acc: 0.9833 - val_loss: 1.1488 - val_acc: 0.5000\n",
            "Epoch 354/500\n",
            "\n",
            "Epoch 00354: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0889 - acc: 0.9833 - val_loss: 1.2490 - val_acc: 0.5000\n",
            "Epoch 355/500\n",
            "\n",
            "Epoch 00355: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0766 - acc: 1.0000 - val_loss: 1.2608 - val_acc: 0.5000\n",
            "Epoch 356/500\n",
            "\n",
            "Epoch 00356: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0736 - acc: 1.0000 - val_loss: 1.2466 - val_acc: 0.5000\n",
            "Epoch 357/500\n",
            "\n",
            "Epoch 00357: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0812 - acc: 1.0000 - val_loss: 1.3042 - val_acc: 0.5000\n",
            "Epoch 358/500\n",
            "\n",
            "Epoch 00358: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0357 - acc: 1.0000 - val_loss: 1.3422 - val_acc: 0.5000\n",
            "Epoch 359/500\n",
            "\n",
            "Epoch 00359: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0823 - acc: 1.0000 - val_loss: 1.3134 - val_acc: 0.5000\n",
            "Epoch 360/500\n",
            "\n",
            "Epoch 00360: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1366 - acc: 0.9667 - val_loss: 1.3569 - val_acc: 0.4500\n",
            "Epoch 361/500\n",
            "\n",
            "Epoch 00361: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0586 - acc: 1.0000 - val_loss: 1.4023 - val_acc: 0.4500\n",
            "Epoch 362/500\n",
            "\n",
            "Epoch 00362: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0799 - acc: 0.9833 - val_loss: 1.4008 - val_acc: 0.4500\n",
            "Epoch 363/500\n",
            "\n",
            "Epoch 00363: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0679 - acc: 0.9833 - val_loss: 1.3741 - val_acc: 0.4500\n",
            "Epoch 364/500\n",
            "\n",
            "Epoch 00364: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1010 - acc: 0.9833 - val_loss: 1.2615 - val_acc: 0.4000\n",
            "Epoch 365/500\n",
            "\n",
            "Epoch 00365: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0936 - acc: 1.0000 - val_loss: 1.0944 - val_acc: 0.5500\n",
            "Epoch 366/500\n",
            "\n",
            "Epoch 00366: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0689 - acc: 1.0000 - val_loss: 1.0124 - val_acc: 0.6000\n",
            "Epoch 367/500\n",
            "\n",
            "Epoch 00367: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0540 - acc: 1.0000 - val_loss: 1.0154 - val_acc: 0.6000\n",
            "Epoch 368/500\n",
            "\n",
            "Epoch 00368: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0704 - acc: 1.0000 - val_loss: 1.0563 - val_acc: 0.6000\n",
            "Epoch 369/500\n",
            "\n",
            "Epoch 00369: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0592 - acc: 1.0000 - val_loss: 1.1154 - val_acc: 0.6000\n",
            "Epoch 370/500\n",
            "\n",
            "Epoch 00370: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1782 - acc: 0.9167 - val_loss: 1.1137 - val_acc: 0.6000\n",
            "Epoch 371/500\n",
            "\n",
            "Epoch 00371: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0773 - acc: 0.9833 - val_loss: 1.2475 - val_acc: 0.4500\n",
            "Epoch 372/500\n",
            "\n",
            "Epoch 00372: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1026 - acc: 1.0000 - val_loss: 1.2772 - val_acc: 0.4500\n",
            "Epoch 373/500\n",
            "\n",
            "Epoch 00373: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0734 - acc: 1.0000 - val_loss: 1.2861 - val_acc: 0.4500\n",
            "Epoch 374/500\n",
            "\n",
            "Epoch 00374: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0877 - acc: 0.9833 - val_loss: 1.2849 - val_acc: 0.5000\n",
            "Epoch 375/500\n",
            "\n",
            "Epoch 00375: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0956 - acc: 1.0000 - val_loss: 1.3776 - val_acc: 0.5000\n",
            "Epoch 376/500\n",
            "\n",
            "Epoch 00376: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0682 - acc: 0.9833 - val_loss: 1.4544 - val_acc: 0.4500\n",
            "Epoch 377/500\n",
            "\n",
            "Epoch 00377: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0915 - acc: 1.0000 - val_loss: 1.5359 - val_acc: 0.4500\n",
            "Epoch 378/500\n",
            "\n",
            "Epoch 00378: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0751 - acc: 1.0000 - val_loss: 1.5876 - val_acc: 0.4500\n",
            "Epoch 379/500\n",
            "\n",
            "Epoch 00379: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0618 - acc: 1.0000 - val_loss: 1.5075 - val_acc: 0.4500\n",
            "Epoch 380/500\n",
            "\n",
            "Epoch 00380: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0841 - acc: 0.9833 - val_loss: 1.4908 - val_acc: 0.4500\n",
            "Epoch 381/500\n",
            "\n",
            "Epoch 00381: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0647 - acc: 1.0000 - val_loss: 1.3856 - val_acc: 0.4500\n",
            "Epoch 382/500\n",
            "\n",
            "Epoch 00382: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0833 - acc: 0.9833 - val_loss: 1.4305 - val_acc: 0.4500\n",
            "Epoch 383/500\n",
            "\n",
            "Epoch 00383: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1046 - acc: 0.9833 - val_loss: 1.5098 - val_acc: 0.4500\n",
            "Epoch 384/500\n",
            "\n",
            "Epoch 00384: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0643 - acc: 1.0000 - val_loss: 1.5859 - val_acc: 0.4000\n",
            "Epoch 385/500\n",
            "\n",
            "Epoch 00385: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0778 - acc: 1.0000 - val_loss: 1.7485 - val_acc: 0.4000\n",
            "Epoch 386/500\n",
            "\n",
            "Epoch 00386: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0437 - acc: 1.0000 - val_loss: 1.8207 - val_acc: 0.4000\n",
            "Epoch 387/500\n",
            "\n",
            "Epoch 00387: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0766 - acc: 0.9833 - val_loss: 1.7849 - val_acc: 0.4000\n",
            "Epoch 388/500\n",
            "\n",
            "Epoch 00388: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0889 - acc: 0.9833 - val_loss: 1.6750 - val_acc: 0.4500\n",
            "Epoch 389/500\n",
            "\n",
            "Epoch 00389: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0972 - acc: 0.9833 - val_loss: 1.5105 - val_acc: 0.4500\n",
            "Epoch 390/500\n",
            "\n",
            "Epoch 00390: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0758 - acc: 0.9833 - val_loss: 1.3832 - val_acc: 0.4500\n",
            "Epoch 391/500\n",
            "\n",
            "Epoch 00391: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0519 - acc: 1.0000 - val_loss: 1.2873 - val_acc: 0.5000\n",
            "Epoch 392/500\n",
            "\n",
            "Epoch 00392: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0787 - acc: 0.9667 - val_loss: 1.4554 - val_acc: 0.4500\n",
            "Epoch 393/500\n",
            "\n",
            "Epoch 00393: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1257 - acc: 0.9667 - val_loss: 1.5293 - val_acc: 0.4000\n",
            "Epoch 394/500\n",
            "\n",
            "Epoch 00394: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0835 - acc: 0.9833 - val_loss: 1.3738 - val_acc: 0.4500\n",
            "Epoch 395/500\n",
            "\n",
            "Epoch 00395: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0818 - acc: 1.0000 - val_loss: 1.2654 - val_acc: 0.5000\n",
            "Epoch 396/500\n",
            "\n",
            "Epoch 00396: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0629 - acc: 1.0000 - val_loss: 1.2574 - val_acc: 0.4500\n",
            "Epoch 397/500\n",
            "\n",
            "Epoch 00397: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0605 - acc: 1.0000 - val_loss: 1.3456 - val_acc: 0.4500\n",
            "Epoch 398/500\n",
            "\n",
            "Epoch 00398: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0829 - acc: 0.9833 - val_loss: 1.3456 - val_acc: 0.4500\n",
            "Epoch 399/500\n",
            "\n",
            "Epoch 00399: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0973 - acc: 0.9833 - val_loss: 1.3229 - val_acc: 0.4500\n",
            "Epoch 400/500\n",
            "\n",
            "Epoch 00400: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0667 - acc: 1.0000 - val_loss: 1.2363 - val_acc: 0.5000\n",
            "Epoch 401/500\n",
            "\n",
            "Epoch 00401: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1142 - acc: 0.9667 - val_loss: 1.1359 - val_acc: 0.4500\n",
            "Epoch 402/500\n",
            "\n",
            "Epoch 00402: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0668 - acc: 1.0000 - val_loss: 1.1374 - val_acc: 0.4500\n",
            "Epoch 403/500\n",
            "\n",
            "Epoch 00403: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0870 - acc: 1.0000 - val_loss: 1.1136 - val_acc: 0.4500\n",
            "Epoch 404/500\n",
            "\n",
            "Epoch 00404: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0889 - acc: 0.9667 - val_loss: 1.1271 - val_acc: 0.4500\n",
            "Epoch 405/500\n",
            "\n",
            "Epoch 00405: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0633 - acc: 1.0000 - val_loss: 1.1117 - val_acc: 0.4500\n",
            "Epoch 406/500\n",
            "\n",
            "Epoch 00406: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0654 - acc: 0.9833 - val_loss: 1.1557 - val_acc: 0.4500\n",
            "Epoch 407/500\n",
            "\n",
            "Epoch 00407: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0662 - acc: 1.0000 - val_loss: 1.2923 - val_acc: 0.3500\n",
            "Epoch 408/500\n",
            "\n",
            "Epoch 00408: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0690 - acc: 1.0000 - val_loss: 1.3633 - val_acc: 0.4000\n",
            "Epoch 409/500\n",
            "\n",
            "Epoch 00409: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0452 - acc: 1.0000 - val_loss: 1.3789 - val_acc: 0.4500\n",
            "Epoch 410/500\n",
            "\n",
            "Epoch 00410: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1161 - acc: 0.9500 - val_loss: 1.3570 - val_acc: 0.4500\n",
            "Epoch 411/500\n",
            "\n",
            "Epoch 00411: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0842 - acc: 0.9833 - val_loss: 1.3166 - val_acc: 0.5000\n",
            "Epoch 412/500\n",
            "\n",
            "Epoch 00412: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0616 - acc: 1.0000 - val_loss: 1.2502 - val_acc: 0.4500\n",
            "Epoch 413/500\n",
            "\n",
            "Epoch 00413: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0646 - acc: 1.0000 - val_loss: 1.1974 - val_acc: 0.4500\n",
            "Epoch 414/500\n",
            "\n",
            "Epoch 00414: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0668 - acc: 1.0000 - val_loss: 1.1523 - val_acc: 0.4500\n",
            "Epoch 415/500\n",
            "\n",
            "Epoch 00415: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0528 - acc: 1.0000 - val_loss: 1.1070 - val_acc: 0.4500\n",
            "Epoch 416/500\n",
            "\n",
            "Epoch 00416: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0686 - acc: 1.0000 - val_loss: 1.0616 - val_acc: 0.5000\n",
            "Epoch 417/500\n",
            "\n",
            "Epoch 00417: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0501 - acc: 1.0000 - val_loss: 1.0067 - val_acc: 0.6000\n",
            "Epoch 418/500\n",
            "\n",
            "Epoch 00418: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0671 - acc: 1.0000 - val_loss: 1.0322 - val_acc: 0.6000\n",
            "Epoch 419/500\n",
            "\n",
            "Epoch 00419: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0748 - acc: 1.0000 - val_loss: 1.1946 - val_acc: 0.5500\n",
            "Epoch 420/500\n",
            "\n",
            "Epoch 00420: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0466 - acc: 1.0000 - val_loss: 1.3002 - val_acc: 0.5500\n",
            "Epoch 421/500\n",
            "\n",
            "Epoch 00421: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0502 - acc: 1.0000 - val_loss: 1.3860 - val_acc: 0.4500\n",
            "Epoch 422/500\n",
            "\n",
            "Epoch 00422: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0392 - acc: 1.0000 - val_loss: 1.4154 - val_acc: 0.4500\n",
            "Epoch 423/500\n",
            "\n",
            "Epoch 00423: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0603 - acc: 1.0000 - val_loss: 1.4107 - val_acc: 0.5000\n",
            "Epoch 424/500\n",
            "\n",
            "Epoch 00424: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0617 - acc: 1.0000 - val_loss: 1.4593 - val_acc: 0.4500\n",
            "Epoch 425/500\n",
            "\n",
            "Epoch 00425: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0560 - acc: 1.0000 - val_loss: 1.4014 - val_acc: 0.4500\n",
            "Epoch 426/500\n",
            "\n",
            "Epoch 00426: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0422 - acc: 1.0000 - val_loss: 1.3719 - val_acc: 0.4500\n",
            "Epoch 427/500\n",
            "\n",
            "Epoch 00427: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0736 - acc: 0.9833 - val_loss: 1.4346 - val_acc: 0.4500\n",
            "Epoch 428/500\n",
            "\n",
            "Epoch 00428: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0785 - acc: 0.9833 - val_loss: 1.5119 - val_acc: 0.4000\n",
            "Epoch 429/500\n",
            "\n",
            "Epoch 00429: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0553 - acc: 1.0000 - val_loss: 1.5141 - val_acc: 0.4500\n",
            "Epoch 430/500\n",
            "\n",
            "Epoch 00430: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0578 - acc: 1.0000 - val_loss: 1.4798 - val_acc: 0.4500\n",
            "Epoch 431/500\n",
            "\n",
            "Epoch 00431: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0758 - acc: 0.9667 - val_loss: 1.3368 - val_acc: 0.4500\n",
            "Epoch 432/500\n",
            "\n",
            "Epoch 00432: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0671 - acc: 0.9833 - val_loss: 1.1888 - val_acc: 0.5500\n",
            "Epoch 433/500\n",
            "\n",
            "Epoch 00433: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0548 - acc: 1.0000 - val_loss: 1.0196 - val_acc: 0.5000\n",
            "Epoch 434/500\n",
            "\n",
            "Epoch 00434: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0984 - acc: 0.9667 - val_loss: 0.9551 - val_acc: 0.5500\n",
            "Epoch 435/500\n",
            "\n",
            "Epoch 00435: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0839 - acc: 0.9833 - val_loss: 1.0152 - val_acc: 0.5500\n",
            "Epoch 436/500\n",
            "\n",
            "Epoch 00436: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0956 - acc: 0.9833 - val_loss: 1.0749 - val_acc: 0.5500\n",
            "Epoch 437/500\n",
            "\n",
            "Epoch 00437: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0584 - acc: 1.0000 - val_loss: 1.1773 - val_acc: 0.5500\n",
            "Epoch 438/500\n",
            "\n",
            "Epoch 00438: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1184 - acc: 0.9667 - val_loss: 1.2585 - val_acc: 0.5000\n",
            "Epoch 439/500\n",
            "\n",
            "Epoch 00439: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0812 - acc: 0.9833 - val_loss: 1.3648 - val_acc: 0.4500\n",
            "Epoch 440/500\n",
            "\n",
            "Epoch 00440: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0670 - acc: 0.9833 - val_loss: 1.3532 - val_acc: 0.4500\n",
            "Epoch 441/500\n",
            "\n",
            "Epoch 00441: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0647 - acc: 1.0000 - val_loss: 1.3583 - val_acc: 0.4500\n",
            "Epoch 442/500\n",
            "\n",
            "Epoch 00442: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0522 - acc: 1.0000 - val_loss: 1.4288 - val_acc: 0.4500\n",
            "Epoch 443/500\n",
            "\n",
            "Epoch 00443: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0578 - acc: 1.0000 - val_loss: 1.4748 - val_acc: 0.4500\n",
            "Epoch 444/500\n",
            "\n",
            "Epoch 00444: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0469 - acc: 1.0000 - val_loss: 1.4813 - val_acc: 0.4500\n",
            "Epoch 445/500\n",
            "\n",
            "Epoch 00445: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0651 - acc: 1.0000 - val_loss: 1.4842 - val_acc: 0.5000\n",
            "Epoch 446/500\n",
            "\n",
            "Epoch 00446: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1034 - acc: 0.9667 - val_loss: 1.5684 - val_acc: 0.4500\n",
            "Epoch 447/500\n",
            "\n",
            "Epoch 00447: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0408 - acc: 1.0000 - val_loss: 1.5660 - val_acc: 0.4500\n",
            "Epoch 448/500\n",
            "\n",
            "Epoch 00448: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0896 - acc: 0.9833 - val_loss: 1.4987 - val_acc: 0.4500\n",
            "Epoch 449/500\n",
            "\n",
            "Epoch 00449: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0708 - acc: 1.0000 - val_loss: 1.3192 - val_acc: 0.5000\n",
            "Epoch 450/500\n",
            "\n",
            "Epoch 00450: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0531 - acc: 1.0000 - val_loss: 1.2125 - val_acc: 0.5500\n",
            "Epoch 451/500\n",
            "\n",
            "Epoch 00451: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0530 - acc: 1.0000 - val_loss: 1.2062 - val_acc: 0.6000\n",
            "Epoch 452/500\n",
            "\n",
            "Epoch 00452: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1165 - acc: 0.9667 - val_loss: 1.2364 - val_acc: 0.5000\n",
            "Epoch 453/500\n",
            "\n",
            "Epoch 00453: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0434 - acc: 1.0000 - val_loss: 1.2278 - val_acc: 0.5000\n",
            "Epoch 454/500\n",
            "\n",
            "Epoch 00454: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0614 - acc: 1.0000 - val_loss: 1.3572 - val_acc: 0.5000\n",
            "Epoch 455/500\n",
            "\n",
            "Epoch 00455: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0913 - acc: 0.9667 - val_loss: 1.5129 - val_acc: 0.4000\n",
            "Epoch 456/500\n",
            "\n",
            "Epoch 00456: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0670 - acc: 0.9833 - val_loss: 1.5495 - val_acc: 0.4000\n",
            "Epoch 457/500\n",
            "\n",
            "Epoch 00457: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0978 - acc: 1.0000 - val_loss: 1.5195 - val_acc: 0.4500\n",
            "Epoch 458/500\n",
            "\n",
            "Epoch 00458: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0866 - acc: 0.9833 - val_loss: 1.4315 - val_acc: 0.4500\n",
            "Epoch 459/500\n",
            "\n",
            "Epoch 00459: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0770 - acc: 0.9833 - val_loss: 1.3120 - val_acc: 0.5000\n",
            "Epoch 460/500\n",
            "\n",
            "Epoch 00460: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0705 - acc: 0.9833 - val_loss: 1.2784 - val_acc: 0.5500\n",
            "Epoch 461/500\n",
            "\n",
            "Epoch 00461: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1033 - acc: 0.9667 - val_loss: 1.3573 - val_acc: 0.5000\n",
            "Epoch 462/500\n",
            "\n",
            "Epoch 00462: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1123 - acc: 0.9667 - val_loss: 1.3428 - val_acc: 0.4500\n",
            "Epoch 463/500\n",
            "\n",
            "Epoch 00463: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0809 - acc: 1.0000 - val_loss: 1.3033 - val_acc: 0.4500\n",
            "Epoch 464/500\n",
            "\n",
            "Epoch 00464: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1065 - acc: 0.9833 - val_loss: 1.2600 - val_acc: 0.5500\n",
            "Epoch 465/500\n",
            "\n",
            "Epoch 00465: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0618 - acc: 1.0000 - val_loss: 1.3340 - val_acc: 0.5000\n",
            "Epoch 466/500\n",
            "\n",
            "Epoch 00466: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0517 - acc: 1.0000 - val_loss: 1.3918 - val_acc: 0.5000\n",
            "Epoch 467/500\n",
            "\n",
            "Epoch 00467: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0765 - acc: 1.0000 - val_loss: 1.3377 - val_acc: 0.5000\n",
            "Epoch 468/500\n",
            "\n",
            "Epoch 00468: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0957 - acc: 0.9667 - val_loss: 1.1700 - val_acc: 0.5000\n",
            "Epoch 469/500\n",
            "\n",
            "Epoch 00469: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0529 - acc: 1.0000 - val_loss: 1.1478 - val_acc: 0.5000\n",
            "Epoch 470/500\n",
            "\n",
            "Epoch 00470: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0729 - acc: 0.9833 - val_loss: 1.1524 - val_acc: 0.5000\n",
            "Epoch 471/500\n",
            "\n",
            "Epoch 00471: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0614 - acc: 1.0000 - val_loss: 1.0569 - val_acc: 0.5000\n",
            "Epoch 472/500\n",
            "\n",
            "Epoch 00472: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0597 - acc: 1.0000 - val_loss: 0.9598 - val_acc: 0.5500\n",
            "Epoch 473/500\n",
            "\n",
            "Epoch 00473: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0549 - acc: 1.0000 - val_loss: 0.9387 - val_acc: 0.6000\n",
            "Epoch 474/500\n",
            "\n",
            "Epoch 00474: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0848 - acc: 0.9833 - val_loss: 0.9532 - val_acc: 0.6000\n",
            "Epoch 475/500\n",
            "\n",
            "Epoch 00475: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0700 - acc: 1.0000 - val_loss: 1.0121 - val_acc: 0.5500\n",
            "Epoch 476/500\n",
            "\n",
            "Epoch 00476: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0640 - acc: 1.0000 - val_loss: 1.0547 - val_acc: 0.5500\n",
            "Epoch 477/500\n",
            "\n",
            "Epoch 00477: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0611 - acc: 0.9833 - val_loss: 1.0654 - val_acc: 0.5000\n",
            "Epoch 478/500\n",
            "\n",
            "Epoch 00478: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0505 - acc: 1.0000 - val_loss: 1.0960 - val_acc: 0.5000\n",
            "Epoch 479/500\n",
            "\n",
            "Epoch 00479: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0621 - acc: 0.9833 - val_loss: 1.0904 - val_acc: 0.4500\n",
            "Epoch 480/500\n",
            "\n",
            "Epoch 00480: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0702 - acc: 1.0000 - val_loss: 1.0911 - val_acc: 0.5000\n",
            "Epoch 481/500\n",
            "\n",
            "Epoch 00481: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0494 - acc: 1.0000 - val_loss: 1.0664 - val_acc: 0.5500\n",
            "Epoch 482/500\n",
            "\n",
            "Epoch 00482: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0642 - acc: 1.0000 - val_loss: 1.0937 - val_acc: 0.5500\n",
            "Epoch 483/500\n",
            "\n",
            "Epoch 00483: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0743 - acc: 0.9833 - val_loss: 1.1520 - val_acc: 0.5000\n",
            "Epoch 484/500\n",
            "\n",
            "Epoch 00484: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0722 - acc: 0.9833 - val_loss: 1.2888 - val_acc: 0.4000\n",
            "Epoch 485/500\n",
            "\n",
            "Epoch 00485: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0341 - acc: 1.0000 - val_loss: 1.3813 - val_acc: 0.4000\n",
            "Epoch 486/500\n",
            "\n",
            "Epoch 00486: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0809 - acc: 0.9667 - val_loss: 1.4326 - val_acc: 0.4000\n",
            "Epoch 487/500\n",
            "\n",
            "Epoch 00487: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0974 - acc: 0.9833 - val_loss: 1.4073 - val_acc: 0.4000\n",
            "Epoch 488/500\n",
            "\n",
            "Epoch 00488: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0658 - acc: 1.0000 - val_loss: 1.1634 - val_acc: 0.4500\n",
            "Epoch 489/500\n",
            "\n",
            "Epoch 00489: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0628 - acc: 1.0000 - val_loss: 1.0888 - val_acc: 0.5000\n",
            "Epoch 490/500\n",
            "\n",
            "Epoch 00490: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0716 - acc: 0.9833 - val_loss: 1.1008 - val_acc: 0.4500\n",
            "Epoch 491/500\n",
            "\n",
            "Epoch 00491: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.1204 - acc: 0.9667 - val_loss: 1.0657 - val_acc: 0.5000\n",
            "Epoch 492/500\n",
            "\n",
            "Epoch 00492: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0655 - acc: 1.0000 - val_loss: 0.9456 - val_acc: 0.5500\n",
            "Epoch 493/500\n",
            "\n",
            "Epoch 00493: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0633 - acc: 1.0000 - val_loss: 0.7855 - val_acc: 0.6000\n",
            "Epoch 494/500\n",
            "\n",
            "Epoch 00494: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0759 - acc: 1.0000 - val_loss: 0.7465 - val_acc: 0.6000\n",
            "Epoch 495/500\n",
            "\n",
            "Epoch 00495: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0673 - acc: 0.9833 - val_loss: 0.7841 - val_acc: 0.6000\n",
            "Epoch 496/500\n",
            "\n",
            "Epoch 00496: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0517 - acc: 1.0000 - val_loss: 0.8678 - val_acc: 0.6000\n",
            "Epoch 497/500\n",
            "\n",
            "Epoch 00497: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0526 - acc: 1.0000 - val_loss: 0.9309 - val_acc: 0.4500\n",
            "Epoch 498/500\n",
            "\n",
            "Epoch 00498: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0567 - acc: 1.0000 - val_loss: 1.0388 - val_acc: 0.5000\n",
            "Epoch 499/500\n",
            "\n",
            "Epoch 00499: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0562 - acc: 1.0000 - val_loss: 1.0761 - val_acc: 0.5000\n",
            "Epoch 500/500\n",
            "\n",
            "Epoch 00500: val_loss did not improve from 0.67063\n",
            "60/60 - 0s - loss: 0.0737 - acc: 0.9833 - val_loss: 1.0838 - val_acc: 0.6000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "# Evaluate on test data\n",
            "40/40 [==============================] - 0s 2ms/sample - loss: 0.8488 - acc: 0.6000\n",
            "test loss, test acc: [0.8488195621874184, 0.6]\n",
            "EEG_Deep/Data2A/parsed_P010T.mat\n",
            "Filtering of Training Data Finished\n",
            "EEG_Deep/Data2A/parsed_P010E.mat\n",
            "Filtering of Testing Data Finished\n",
            "(1, 2)\n",
            "Finding labels in training data\n",
            "Finding labels in testing data\n",
            "[2 2 2 1 2 1 2 2 2 2 2 1 1 2 1 2 1 2 2 2]\n",
            "(60, 12, 1536)\n",
            "(60,)\n",
            "(20, 12, 1536)\n",
            "(20,)\n",
            "[[0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]]\n",
            "X_train shape: (60, 1, 12, 1536)\n",
            "60 train samples\n",
            "20 val samples\n",
            "(40, 12, 1536)\n",
            "(40, 1)\n",
            "X_train shape: (40, 1, 12, 1536)\n",
            "40 train samples\n",
            "Train on 60 samples, validate on 20 samples\n",
            "Epoch 1/500\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.69632, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 1s - loss: 0.7368 - acc: 0.4667 - val_loss: 0.6963 - val_acc: 0.3000\n",
            "Epoch 2/500\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.69632 to 0.69557, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6722 - acc: 0.5667 - val_loss: 0.6956 - val_acc: 0.3500\n",
            "Epoch 3/500\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.69557 to 0.69302, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6588 - acc: 0.6000 - val_loss: 0.6930 - val_acc: 0.4000\n",
            "Epoch 4/500\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.69302\n",
            "60/60 - 0s - loss: 0.6157 - acc: 0.8500 - val_loss: 0.6933 - val_acc: 0.4500\n",
            "Epoch 5/500\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.69302 to 0.69265, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6047 - acc: 0.8167 - val_loss: 0.6927 - val_acc: 0.5500\n",
            "Epoch 6/500\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.69265 to 0.69196, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5956 - acc: 0.8167 - val_loss: 0.6920 - val_acc: 0.6000\n",
            "Epoch 7/500\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.69196 to 0.68954, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5808 - acc: 0.8000 - val_loss: 0.6895 - val_acc: 0.6000\n",
            "Epoch 8/500\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.68954 to 0.68692, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5499 - acc: 0.8667 - val_loss: 0.6869 - val_acc: 0.6500\n",
            "Epoch 9/500\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.68692 to 0.68372, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5186 - acc: 0.9500 - val_loss: 0.6837 - val_acc: 0.6000\n",
            "Epoch 10/500\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.68372 to 0.68202, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5156 - acc: 0.9333 - val_loss: 0.6820 - val_acc: 0.6000\n",
            "Epoch 11/500\n",
            "\n",
            "Epoch 00011: val_loss improved from 0.68202 to 0.67935, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4947 - acc: 0.9167 - val_loss: 0.6794 - val_acc: 0.6500\n",
            "Epoch 12/500\n",
            "\n",
            "Epoch 00012: val_loss improved from 0.67935 to 0.67744, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4818 - acc: 0.8833 - val_loss: 0.6774 - val_acc: 0.6000\n",
            "Epoch 13/500\n",
            "\n",
            "Epoch 00013: val_loss improved from 0.67744 to 0.67401, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4864 - acc: 0.9167 - val_loss: 0.6740 - val_acc: 0.7000\n",
            "Epoch 14/500\n",
            "\n",
            "Epoch 00014: val_loss improved from 0.67401 to 0.66737, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4675 - acc: 0.8833 - val_loss: 0.6674 - val_acc: 0.8000\n",
            "Epoch 15/500\n",
            "\n",
            "Epoch 00015: val_loss improved from 0.66737 to 0.65778, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4688 - acc: 0.9167 - val_loss: 0.6578 - val_acc: 0.8000\n",
            "Epoch 16/500\n",
            "\n",
            "Epoch 00016: val_loss improved from 0.65778 to 0.64581, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4285 - acc: 0.9167 - val_loss: 0.6458 - val_acc: 0.8000\n",
            "Epoch 17/500\n",
            "\n",
            "Epoch 00017: val_loss improved from 0.64581 to 0.63454, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4196 - acc: 0.9500 - val_loss: 0.6345 - val_acc: 0.8500\n",
            "Epoch 18/500\n",
            "\n",
            "Epoch 00018: val_loss improved from 0.63454 to 0.62231, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4171 - acc: 0.9000 - val_loss: 0.6223 - val_acc: 0.8500\n",
            "Epoch 19/500\n",
            "\n",
            "Epoch 00019: val_loss improved from 0.62231 to 0.60774, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3770 - acc: 0.9500 - val_loss: 0.6077 - val_acc: 0.9000\n",
            "Epoch 20/500\n",
            "\n",
            "Epoch 00020: val_loss improved from 0.60774 to 0.59301, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3902 - acc: 0.9333 - val_loss: 0.5930 - val_acc: 0.9500\n",
            "Epoch 21/500\n",
            "\n",
            "Epoch 00021: val_loss improved from 0.59301 to 0.57682, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3907 - acc: 0.9000 - val_loss: 0.5768 - val_acc: 0.9500\n",
            "Epoch 22/500\n",
            "\n",
            "Epoch 00022: val_loss improved from 0.57682 to 0.56149, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3770 - acc: 0.9000 - val_loss: 0.5615 - val_acc: 0.9500\n",
            "Epoch 23/500\n",
            "\n",
            "Epoch 00023: val_loss improved from 0.56149 to 0.54979, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3809 - acc: 0.8833 - val_loss: 0.5498 - val_acc: 0.9000\n",
            "Epoch 24/500\n",
            "\n",
            "Epoch 00024: val_loss improved from 0.54979 to 0.53990, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3365 - acc: 0.9667 - val_loss: 0.5399 - val_acc: 0.9000\n",
            "Epoch 25/500\n",
            "\n",
            "Epoch 00025: val_loss improved from 0.53990 to 0.52963, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3390 - acc: 0.9167 - val_loss: 0.5296 - val_acc: 0.8500\n",
            "Epoch 26/500\n",
            "\n",
            "Epoch 00026: val_loss improved from 0.52963 to 0.52178, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3505 - acc: 0.9167 - val_loss: 0.5218 - val_acc: 0.8500\n",
            "Epoch 27/500\n",
            "\n",
            "Epoch 00027: val_loss improved from 0.52178 to 0.50627, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3362 - acc: 0.9167 - val_loss: 0.5063 - val_acc: 0.8500\n",
            "Epoch 28/500\n",
            "\n",
            "Epoch 00028: val_loss improved from 0.50627 to 0.48796, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2966 - acc: 0.9500 - val_loss: 0.4880 - val_acc: 0.8000\n",
            "Epoch 29/500\n",
            "\n",
            "Epoch 00029: val_loss improved from 0.48796 to 0.46685, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3368 - acc: 0.8833 - val_loss: 0.4669 - val_acc: 0.8500\n",
            "Epoch 30/500\n",
            "\n",
            "Epoch 00030: val_loss improved from 0.46685 to 0.45024, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3138 - acc: 0.9167 - val_loss: 0.4502 - val_acc: 0.8500\n",
            "Epoch 31/500\n",
            "\n",
            "Epoch 00031: val_loss improved from 0.45024 to 0.43676, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3412 - acc: 0.9000 - val_loss: 0.4368 - val_acc: 0.8500\n",
            "Epoch 32/500\n",
            "\n",
            "Epoch 00032: val_loss improved from 0.43676 to 0.43372, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2925 - acc: 0.9167 - val_loss: 0.4337 - val_acc: 0.8500\n",
            "Epoch 33/500\n",
            "\n",
            "Epoch 00033: val_loss improved from 0.43372 to 0.42826, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2591 - acc: 0.9500 - val_loss: 0.4283 - val_acc: 0.8000\n",
            "Epoch 34/500\n",
            "\n",
            "Epoch 00034: val_loss improved from 0.42826 to 0.41681, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2744 - acc: 0.9500 - val_loss: 0.4168 - val_acc: 0.8000\n",
            "Epoch 35/500\n",
            "\n",
            "Epoch 00035: val_loss improved from 0.41681 to 0.40577, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3066 - acc: 0.9333 - val_loss: 0.4058 - val_acc: 0.8000\n",
            "Epoch 36/500\n",
            "\n",
            "Epoch 00036: val_loss improved from 0.40577 to 0.39853, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2625 - acc: 0.9667 - val_loss: 0.3985 - val_acc: 0.9000\n",
            "Epoch 37/500\n",
            "\n",
            "Epoch 00037: val_loss improved from 0.39853 to 0.39327, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2632 - acc: 0.9333 - val_loss: 0.3933 - val_acc: 0.8500\n",
            "Epoch 38/500\n",
            "\n",
            "Epoch 00038: val_loss improved from 0.39327 to 0.38767, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2572 - acc: 0.9667 - val_loss: 0.3877 - val_acc: 0.8500\n",
            "Epoch 39/500\n",
            "\n",
            "Epoch 00039: val_loss improved from 0.38767 to 0.38300, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2322 - acc: 0.9833 - val_loss: 0.3830 - val_acc: 0.8500\n",
            "Epoch 40/500\n",
            "\n",
            "Epoch 00040: val_loss improved from 0.38300 to 0.37397, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2376 - acc: 0.9500 - val_loss: 0.3740 - val_acc: 0.8500\n",
            "Epoch 41/500\n",
            "\n",
            "Epoch 00041: val_loss improved from 0.37397 to 0.36200, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2467 - acc: 0.9333 - val_loss: 0.3620 - val_acc: 0.9000\n",
            "Epoch 42/500\n",
            "\n",
            "Epoch 00042: val_loss improved from 0.36200 to 0.35543, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2171 - acc: 0.9667 - val_loss: 0.3554 - val_acc: 0.9000\n",
            "Epoch 43/500\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.35543\n",
            "60/60 - 0s - loss: 0.2396 - acc: 0.9500 - val_loss: 0.3580 - val_acc: 0.9000\n",
            "Epoch 44/500\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.35543\n",
            "60/60 - 0s - loss: 0.2512 - acc: 0.9167 - val_loss: 0.3641 - val_acc: 0.8000\n",
            "Epoch 45/500\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.35543\n",
            "60/60 - 0s - loss: 0.2392 - acc: 0.9500 - val_loss: 0.3680 - val_acc: 0.8000\n",
            "Epoch 46/500\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.35543\n",
            "60/60 - 0s - loss: 0.1970 - acc: 0.9500 - val_loss: 0.3645 - val_acc: 0.8000\n",
            "Epoch 47/500\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.35543\n",
            "60/60 - 0s - loss: 0.2577 - acc: 0.9333 - val_loss: 0.3585 - val_acc: 0.8000\n",
            "Epoch 48/500\n",
            "\n",
            "Epoch 00048: val_loss improved from 0.35543 to 0.35367, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2100 - acc: 0.9500 - val_loss: 0.3537 - val_acc: 0.8000\n",
            "Epoch 49/500\n",
            "\n",
            "Epoch 00049: val_loss improved from 0.35367 to 0.35021, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1861 - acc: 0.9500 - val_loss: 0.3502 - val_acc: 0.8000\n",
            "Epoch 50/500\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.35021\n",
            "60/60 - 0s - loss: 0.2175 - acc: 0.9667 - val_loss: 0.3551 - val_acc: 0.8000\n",
            "Epoch 51/500\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.35021\n",
            "60/60 - 0s - loss: 0.2099 - acc: 0.9500 - val_loss: 0.3551 - val_acc: 0.8000\n",
            "Epoch 52/500\n",
            "\n",
            "Epoch 00052: val_loss improved from 0.35021 to 0.34414, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2367 - acc: 0.9333 - val_loss: 0.3441 - val_acc: 0.8000\n",
            "Epoch 53/500\n",
            "\n",
            "Epoch 00053: val_loss improved from 0.34414 to 0.33011, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2021 - acc: 0.9667 - val_loss: 0.3301 - val_acc: 0.8500\n",
            "Epoch 54/500\n",
            "\n",
            "Epoch 00054: val_loss improved from 0.33011 to 0.32747, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2054 - acc: 0.9500 - val_loss: 0.3275 - val_acc: 0.8500\n",
            "Epoch 55/500\n",
            "\n",
            "Epoch 00055: val_loss improved from 0.32747 to 0.32457, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1878 - acc: 0.9667 - val_loss: 0.3246 - val_acc: 0.8500\n",
            "Epoch 56/500\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.2005 - acc: 0.9833 - val_loss: 0.3267 - val_acc: 0.8000\n",
            "Epoch 57/500\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.2010 - acc: 0.9667 - val_loss: 0.3302 - val_acc: 0.8000\n",
            "Epoch 58/500\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.1746 - acc: 0.9500 - val_loss: 0.3379 - val_acc: 0.8000\n",
            "Epoch 59/500\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.1834 - acc: 0.9667 - val_loss: 0.3546 - val_acc: 0.8000\n",
            "Epoch 60/500\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.1910 - acc: 0.9667 - val_loss: 0.3740 - val_acc: 0.8000\n",
            "Epoch 61/500\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.1884 - acc: 0.9833 - val_loss: 0.3691 - val_acc: 0.8000\n",
            "Epoch 62/500\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.2035 - acc: 0.9500 - val_loss: 0.3521 - val_acc: 0.8000\n",
            "Epoch 63/500\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.1948 - acc: 0.9167 - val_loss: 0.3494 - val_acc: 0.8000\n",
            "Epoch 64/500\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.1991 - acc: 0.9500 - val_loss: 0.3422 - val_acc: 0.8000\n",
            "Epoch 65/500\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.2403 - acc: 0.8833 - val_loss: 0.3355 - val_acc: 0.8000\n",
            "Epoch 66/500\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.2135 - acc: 0.9500 - val_loss: 0.3314 - val_acc: 0.8500\n",
            "Epoch 67/500\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.1987 - acc: 0.9500 - val_loss: 0.3377 - val_acc: 0.8500\n",
            "Epoch 68/500\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.1591 - acc: 0.9833 - val_loss: 0.3499 - val_acc: 0.8000\n",
            "Epoch 69/500\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.2144 - acc: 0.9167 - val_loss: 0.3561 - val_acc: 0.8000\n",
            "Epoch 70/500\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.1723 - acc: 0.9667 - val_loss: 0.3629 - val_acc: 0.8000\n",
            "Epoch 71/500\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.1570 - acc: 0.9833 - val_loss: 0.3640 - val_acc: 0.8000\n",
            "Epoch 72/500\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.1600 - acc: 0.9667 - val_loss: 0.3597 - val_acc: 0.8000\n",
            "Epoch 73/500\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.1482 - acc: 0.9833 - val_loss: 0.3564 - val_acc: 0.8000\n",
            "Epoch 74/500\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.2073 - acc: 0.9667 - val_loss: 0.3642 - val_acc: 0.8000\n",
            "Epoch 75/500\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.1805 - acc: 0.9833 - val_loss: 0.3795 - val_acc: 0.8000\n",
            "Epoch 76/500\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.1587 - acc: 0.9667 - val_loss: 0.4007 - val_acc: 0.8000\n",
            "Epoch 77/500\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.1904 - acc: 0.9167 - val_loss: 0.4024 - val_acc: 0.8000\n",
            "Epoch 78/500\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.1418 - acc: 0.9833 - val_loss: 0.4017 - val_acc: 0.8000\n",
            "Epoch 79/500\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.1417 - acc: 1.0000 - val_loss: 0.3955 - val_acc: 0.8000\n",
            "Epoch 80/500\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.1237 - acc: 1.0000 - val_loss: 0.3853 - val_acc: 0.8000\n",
            "Epoch 81/500\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.1715 - acc: 0.9333 - val_loss: 0.3834 - val_acc: 0.8000\n",
            "Epoch 82/500\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.2077 - acc: 0.9333 - val_loss: 0.3980 - val_acc: 0.8000\n",
            "Epoch 83/500\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.1531 - acc: 0.9833 - val_loss: 0.4071 - val_acc: 0.8000\n",
            "Epoch 84/500\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.1765 - acc: 0.9500 - val_loss: 0.4252 - val_acc: 0.8000\n",
            "Epoch 85/500\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.1834 - acc: 0.9500 - val_loss: 0.4366 - val_acc: 0.7500\n",
            "Epoch 86/500\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.1721 - acc: 0.9500 - val_loss: 0.4284 - val_acc: 0.7500\n",
            "Epoch 87/500\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.2830 - acc: 0.8167 - val_loss: 0.4039 - val_acc: 0.8000\n",
            "Epoch 88/500\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.1679 - acc: 0.9667 - val_loss: 0.4269 - val_acc: 0.7500\n",
            "Epoch 89/500\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.1215 - acc: 0.9833 - val_loss: 0.4264 - val_acc: 0.7500\n",
            "Epoch 90/500\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.1630 - acc: 1.0000 - val_loss: 0.4213 - val_acc: 0.7500\n",
            "Epoch 91/500\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.1343 - acc: 0.9667 - val_loss: 0.4160 - val_acc: 0.7500\n",
            "Epoch 92/500\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.1568 - acc: 0.9833 - val_loss: 0.4106 - val_acc: 0.8000\n",
            "Epoch 93/500\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.2180 - acc: 0.9000 - val_loss: 0.4166 - val_acc: 0.8000\n",
            "Epoch 94/500\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.1661 - acc: 0.9500 - val_loss: 0.4417 - val_acc: 0.8000\n",
            "Epoch 95/500\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.1336 - acc: 0.9833 - val_loss: 0.4456 - val_acc: 0.8000\n",
            "Epoch 96/500\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.1598 - acc: 0.9667 - val_loss: 0.4608 - val_acc: 0.7500\n",
            "Epoch 97/500\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.1526 - acc: 1.0000 - val_loss: 0.4736 - val_acc: 0.7000\n",
            "Epoch 98/500\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.1757 - acc: 0.9500 - val_loss: 0.4850 - val_acc: 0.6500\n",
            "Epoch 99/500\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.1622 - acc: 0.9500 - val_loss: 0.4863 - val_acc: 0.6500\n",
            "Epoch 100/500\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.1697 - acc: 0.9333 - val_loss: 0.4797 - val_acc: 0.7000\n",
            "Epoch 101/500\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.1505 - acc: 0.9667 - val_loss: 0.4745 - val_acc: 0.7000\n",
            "Epoch 102/500\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.1186 - acc: 1.0000 - val_loss: 0.4666 - val_acc: 0.7000\n",
            "Epoch 103/500\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.1004 - acc: 1.0000 - val_loss: 0.4603 - val_acc: 0.7500\n",
            "Epoch 104/500\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.1713 - acc: 0.9500 - val_loss: 0.4425 - val_acc: 0.7500\n",
            "Epoch 105/500\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.1208 - acc: 1.0000 - val_loss: 0.4444 - val_acc: 0.7500\n",
            "Epoch 106/500\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.1676 - acc: 0.9500 - val_loss: 0.4551 - val_acc: 0.7500\n",
            "Epoch 107/500\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.1627 - acc: 0.9667 - val_loss: 0.4649 - val_acc: 0.7500\n",
            "Epoch 108/500\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.1270 - acc: 0.9833 - val_loss: 0.5020 - val_acc: 0.7500\n",
            "Epoch 109/500\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.1377 - acc: 0.9833 - val_loss: 0.5446 - val_acc: 0.6000\n",
            "Epoch 110/500\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.1176 - acc: 1.0000 - val_loss: 0.5661 - val_acc: 0.6000\n",
            "Epoch 111/500\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.1328 - acc: 0.9667 - val_loss: 0.5549 - val_acc: 0.6500\n",
            "Epoch 112/500\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.1786 - acc: 0.9667 - val_loss: 0.5353 - val_acc: 0.7000\n",
            "Epoch 113/500\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.1572 - acc: 0.9667 - val_loss: 0.5041 - val_acc: 0.7500\n",
            "Epoch 114/500\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.1255 - acc: 1.0000 - val_loss: 0.4990 - val_acc: 0.7500\n",
            "Epoch 115/500\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.1506 - acc: 0.9833 - val_loss: 0.4772 - val_acc: 0.7500\n",
            "Epoch 116/500\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.1818 - acc: 0.9500 - val_loss: 0.4766 - val_acc: 0.7500\n",
            "Epoch 117/500\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.1278 - acc: 1.0000 - val_loss: 0.4990 - val_acc: 0.7500\n",
            "Epoch 118/500\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.1012 - acc: 0.9833 - val_loss: 0.5243 - val_acc: 0.7000\n",
            "Epoch 119/500\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.1676 - acc: 0.9500 - val_loss: 0.5450 - val_acc: 0.6500\n",
            "Epoch 120/500\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.1389 - acc: 0.9833 - val_loss: 0.5303 - val_acc: 0.7500\n",
            "Epoch 121/500\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.1260 - acc: 0.9833 - val_loss: 0.5170 - val_acc: 0.7500\n",
            "Epoch 122/500\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.1635 - acc: 0.9500 - val_loss: 0.5032 - val_acc: 0.7500\n",
            "Epoch 123/500\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.1153 - acc: 1.0000 - val_loss: 0.4601 - val_acc: 0.7500\n",
            "Epoch 124/500\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.1401 - acc: 0.9833 - val_loss: 0.4442 - val_acc: 0.7500\n",
            "Epoch 125/500\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.1677 - acc: 0.9333 - val_loss: 0.4602 - val_acc: 0.7500\n",
            "Epoch 126/500\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.1238 - acc: 1.0000 - val_loss: 0.5035 - val_acc: 0.7500\n",
            "Epoch 127/500\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.1128 - acc: 0.9833 - val_loss: 0.5333 - val_acc: 0.7000\n",
            "Epoch 128/500\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.1180 - acc: 0.9833 - val_loss: 0.5411 - val_acc: 0.6500\n",
            "Epoch 129/500\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.1660 - acc: 0.9333 - val_loss: 0.5518 - val_acc: 0.6500\n",
            "Epoch 130/500\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.1087 - acc: 0.9833 - val_loss: 0.5835 - val_acc: 0.6000\n",
            "Epoch 131/500\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.1094 - acc: 1.0000 - val_loss: 0.6159 - val_acc: 0.6500\n",
            "Epoch 132/500\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.1268 - acc: 0.9833 - val_loss: 0.6065 - val_acc: 0.6000\n",
            "Epoch 133/500\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.1237 - acc: 1.0000 - val_loss: 0.5943 - val_acc: 0.6000\n",
            "Epoch 134/500\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.1091 - acc: 1.0000 - val_loss: 0.5707 - val_acc: 0.6500\n",
            "Epoch 135/500\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.1530 - acc: 0.9667 - val_loss: 0.5528 - val_acc: 0.6500\n",
            "Epoch 136/500\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0934 - acc: 1.0000 - val_loss: 0.5342 - val_acc: 0.7500\n",
            "Epoch 137/500\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.1393 - acc: 0.9500 - val_loss: 0.5287 - val_acc: 0.7500\n",
            "Epoch 138/500\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.1081 - acc: 0.9833 - val_loss: 0.5293 - val_acc: 0.7500\n",
            "Epoch 139/500\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.1124 - acc: 1.0000 - val_loss: 0.5615 - val_acc: 0.6500\n",
            "Epoch 140/500\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.1098 - acc: 0.9833 - val_loss: 0.5898 - val_acc: 0.6500\n",
            "Epoch 141/500\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0988 - acc: 1.0000 - val_loss: 0.6172 - val_acc: 0.6000\n",
            "Epoch 142/500\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0878 - acc: 1.0000 - val_loss: 0.6273 - val_acc: 0.6000\n",
            "Epoch 143/500\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.1685 - acc: 0.9333 - val_loss: 0.5928 - val_acc: 0.6500\n",
            "Epoch 144/500\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.1052 - acc: 1.0000 - val_loss: 0.5643 - val_acc: 0.7000\n",
            "Epoch 145/500\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.1167 - acc: 0.9667 - val_loss: 0.5591 - val_acc: 0.7000\n",
            "Epoch 146/500\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.1698 - acc: 0.9333 - val_loss: 0.5460 - val_acc: 0.7500\n",
            "Epoch 147/500\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.1051 - acc: 0.9833 - val_loss: 0.5283 - val_acc: 0.7500\n",
            "Epoch 148/500\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.1097 - acc: 1.0000 - val_loss: 0.5196 - val_acc: 0.7500\n",
            "Epoch 149/500\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.1710 - acc: 0.9667 - val_loss: 0.5484 - val_acc: 0.7000\n",
            "Epoch 150/500\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0813 - acc: 1.0000 - val_loss: 0.5430 - val_acc: 0.7000\n",
            "Epoch 151/500\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.1230 - acc: 1.0000 - val_loss: 0.5398 - val_acc: 0.7000\n",
            "Epoch 152/500\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.1301 - acc: 0.9667 - val_loss: 0.5595 - val_acc: 0.6500\n",
            "Epoch 153/500\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.1027 - acc: 1.0000 - val_loss: 0.5832 - val_acc: 0.6500\n",
            "Epoch 154/500\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.1326 - acc: 0.9833 - val_loss: 0.5903 - val_acc: 0.6500\n",
            "Epoch 155/500\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.1066 - acc: 1.0000 - val_loss: 0.5733 - val_acc: 0.6500\n",
            "Epoch 156/500\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0826 - acc: 1.0000 - val_loss: 0.5318 - val_acc: 0.7500\n",
            "Epoch 157/500\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.1005 - acc: 0.9833 - val_loss: 0.5289 - val_acc: 0.7500\n",
            "Epoch 158/500\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.1298 - acc: 0.9833 - val_loss: 0.5531 - val_acc: 0.7000\n",
            "Epoch 159/500\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.1105 - acc: 0.9833 - val_loss: 0.5940 - val_acc: 0.6500\n",
            "Epoch 160/500\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.1007 - acc: 0.9833 - val_loss: 0.6333 - val_acc: 0.6000\n",
            "Epoch 161/500\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.1105 - acc: 0.9833 - val_loss: 0.6585 - val_acc: 0.6000\n",
            "Epoch 162/500\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0879 - acc: 0.9833 - val_loss: 0.6667 - val_acc: 0.6000\n",
            "Epoch 163/500\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0935 - acc: 1.0000 - val_loss: 0.6639 - val_acc: 0.6000\n",
            "Epoch 164/500\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0675 - acc: 1.0000 - val_loss: 0.6205 - val_acc: 0.6000\n",
            "Epoch 165/500\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.1137 - acc: 0.9833 - val_loss: 0.5920 - val_acc: 0.6500\n",
            "Epoch 166/500\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0885 - acc: 1.0000 - val_loss: 0.5538 - val_acc: 0.7500\n",
            "Epoch 167/500\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0937 - acc: 0.9833 - val_loss: 0.5272 - val_acc: 0.7500\n",
            "Epoch 168/500\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0979 - acc: 0.9833 - val_loss: 0.5343 - val_acc: 0.7500\n",
            "Epoch 169/500\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0852 - acc: 1.0000 - val_loss: 0.5552 - val_acc: 0.7500\n",
            "Epoch 170/500\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.1575 - acc: 0.9333 - val_loss: 0.5831 - val_acc: 0.6500\n",
            "Epoch 171/500\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.1352 - acc: 0.9500 - val_loss: 0.6032 - val_acc: 0.6500\n",
            "Epoch 172/500\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.1146 - acc: 1.0000 - val_loss: 0.6523 - val_acc: 0.6500\n",
            "Epoch 173/500\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0810 - acc: 1.0000 - val_loss: 0.6767 - val_acc: 0.6000\n",
            "Epoch 174/500\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0978 - acc: 0.9833 - val_loss: 0.6355 - val_acc: 0.6500\n",
            "Epoch 175/500\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.1050 - acc: 0.9667 - val_loss: 0.6375 - val_acc: 0.6000\n",
            "Epoch 176/500\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0648 - acc: 1.0000 - val_loss: 0.6298 - val_acc: 0.6000\n",
            "Epoch 177/500\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.1221 - acc: 0.9833 - val_loss: 0.6450 - val_acc: 0.6000\n",
            "Epoch 178/500\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.1211 - acc: 0.9667 - val_loss: 0.6400 - val_acc: 0.6500\n",
            "Epoch 179/500\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0961 - acc: 0.9833 - val_loss: 0.6056 - val_acc: 0.6500\n",
            "Epoch 180/500\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.1063 - acc: 0.9833 - val_loss: 0.5687 - val_acc: 0.6500\n",
            "Epoch 181/500\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.1190 - acc: 0.9833 - val_loss: 0.5456 - val_acc: 0.7000\n",
            "Epoch 182/500\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.1557 - acc: 0.9333 - val_loss: 0.5638 - val_acc: 0.6500\n",
            "Epoch 183/500\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0917 - acc: 0.9833 - val_loss: 0.5878 - val_acc: 0.6500\n",
            "Epoch 184/500\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0876 - acc: 0.9833 - val_loss: 0.6219 - val_acc: 0.6500\n",
            "Epoch 185/500\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.1662 - acc: 0.9333 - val_loss: 0.6282 - val_acc: 0.6500\n",
            "Epoch 186/500\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0759 - acc: 1.0000 - val_loss: 0.5964 - val_acc: 0.6500\n",
            "Epoch 187/500\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0833 - acc: 0.9833 - val_loss: 0.5956 - val_acc: 0.6500\n",
            "Epoch 188/500\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0735 - acc: 1.0000 - val_loss: 0.5982 - val_acc: 0.6500\n",
            "Epoch 189/500\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0899 - acc: 0.9833 - val_loss: 0.6023 - val_acc: 0.6500\n",
            "Epoch 190/500\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.1324 - acc: 0.9333 - val_loss: 0.6179 - val_acc: 0.6500\n",
            "Epoch 191/500\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0772 - acc: 0.9833 - val_loss: 0.6421 - val_acc: 0.6500\n",
            "Epoch 192/500\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0766 - acc: 1.0000 - val_loss: 0.6579 - val_acc: 0.6500\n",
            "Epoch 193/500\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0552 - acc: 1.0000 - val_loss: 0.6380 - val_acc: 0.6500\n",
            "Epoch 194/500\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0881 - acc: 0.9833 - val_loss: 0.6109 - val_acc: 0.6500\n",
            "Epoch 195/500\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.1074 - acc: 0.9667 - val_loss: 0.5776 - val_acc: 0.7000\n",
            "Epoch 196/500\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0601 - acc: 1.0000 - val_loss: 0.5607 - val_acc: 0.7000\n",
            "Epoch 197/500\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.1331 - acc: 0.9500 - val_loss: 0.5337 - val_acc: 0.7500\n",
            "Epoch 198/500\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.1372 - acc: 0.9500 - val_loss: 0.5334 - val_acc: 0.7000\n",
            "Epoch 199/500\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.1257 - acc: 0.9833 - val_loss: 0.5099 - val_acc: 0.7000\n",
            "Epoch 200/500\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0990 - acc: 1.0000 - val_loss: 0.4547 - val_acc: 0.7500\n",
            "Epoch 201/500\n",
            "\n",
            "Epoch 00201: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0728 - acc: 1.0000 - val_loss: 0.4341 - val_acc: 0.8000\n",
            "Epoch 202/500\n",
            "\n",
            "Epoch 00202: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.1136 - acc: 0.9833 - val_loss: 0.4144 - val_acc: 0.8500\n",
            "Epoch 203/500\n",
            "\n",
            "Epoch 00203: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0919 - acc: 1.0000 - val_loss: 0.4068 - val_acc: 0.8500\n",
            "Epoch 204/500\n",
            "\n",
            "Epoch 00204: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0732 - acc: 0.9833 - val_loss: 0.4079 - val_acc: 0.8500\n",
            "Epoch 205/500\n",
            "\n",
            "Epoch 00205: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0665 - acc: 1.0000 - val_loss: 0.4105 - val_acc: 0.8500\n",
            "Epoch 206/500\n",
            "\n",
            "Epoch 00206: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0880 - acc: 0.9833 - val_loss: 0.4057 - val_acc: 0.8500\n",
            "Epoch 207/500\n",
            "\n",
            "Epoch 00207: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0819 - acc: 0.9833 - val_loss: 0.4221 - val_acc: 0.8000\n",
            "Epoch 208/500\n",
            "\n",
            "Epoch 00208: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.1150 - acc: 0.9667 - val_loss: 0.4547 - val_acc: 0.8000\n",
            "Epoch 209/500\n",
            "\n",
            "Epoch 00209: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.1024 - acc: 0.9667 - val_loss: 0.5179 - val_acc: 0.7500\n",
            "Epoch 210/500\n",
            "\n",
            "Epoch 00210: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0618 - acc: 1.0000 - val_loss: 0.5958 - val_acc: 0.7000\n",
            "Epoch 211/500\n",
            "\n",
            "Epoch 00211: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0694 - acc: 1.0000 - val_loss: 0.6573 - val_acc: 0.6500\n",
            "Epoch 212/500\n",
            "\n",
            "Epoch 00212: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0875 - acc: 1.0000 - val_loss: 0.6595 - val_acc: 0.6500\n",
            "Epoch 213/500\n",
            "\n",
            "Epoch 00213: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0700 - acc: 0.9833 - val_loss: 0.6092 - val_acc: 0.7000\n",
            "Epoch 214/500\n",
            "\n",
            "Epoch 00214: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.1142 - acc: 1.0000 - val_loss: 0.5655 - val_acc: 0.7500\n",
            "Epoch 215/500\n",
            "\n",
            "Epoch 00215: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0632 - acc: 1.0000 - val_loss: 0.5506 - val_acc: 0.7500\n",
            "Epoch 216/500\n",
            "\n",
            "Epoch 00216: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0829 - acc: 0.9833 - val_loss: 0.5346 - val_acc: 0.7500\n",
            "Epoch 217/500\n",
            "\n",
            "Epoch 00217: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0699 - acc: 1.0000 - val_loss: 0.5418 - val_acc: 0.7500\n",
            "Epoch 218/500\n",
            "\n",
            "Epoch 00218: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.1467 - acc: 0.9500 - val_loss: 0.6207 - val_acc: 0.7000\n",
            "Epoch 219/500\n",
            "\n",
            "Epoch 00219: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0991 - acc: 0.9500 - val_loss: 0.6281 - val_acc: 0.6500\n",
            "Epoch 220/500\n",
            "\n",
            "Epoch 00220: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0704 - acc: 0.9833 - val_loss: 0.6207 - val_acc: 0.6000\n",
            "Epoch 221/500\n",
            "\n",
            "Epoch 00221: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.1012 - acc: 1.0000 - val_loss: 0.5928 - val_acc: 0.6500\n",
            "Epoch 222/500\n",
            "\n",
            "Epoch 00222: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0854 - acc: 1.0000 - val_loss: 0.5325 - val_acc: 0.7500\n",
            "Epoch 223/500\n",
            "\n",
            "Epoch 00223: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0848 - acc: 0.9833 - val_loss: 0.4868 - val_acc: 0.7500\n",
            "Epoch 224/500\n",
            "\n",
            "Epoch 00224: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0506 - acc: 1.0000 - val_loss: 0.4764 - val_acc: 0.7500\n",
            "Epoch 225/500\n",
            "\n",
            "Epoch 00225: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0668 - acc: 1.0000 - val_loss: 0.4793 - val_acc: 0.7500\n",
            "Epoch 226/500\n",
            "\n",
            "Epoch 00226: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0756 - acc: 1.0000 - val_loss: 0.5175 - val_acc: 0.7500\n",
            "Epoch 227/500\n",
            "\n",
            "Epoch 00227: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0924 - acc: 0.9667 - val_loss: 0.5569 - val_acc: 0.7500\n",
            "Epoch 228/500\n",
            "\n",
            "Epoch 00228: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.1816 - acc: 0.9000 - val_loss: 0.5940 - val_acc: 0.7000\n",
            "Epoch 229/500\n",
            "\n",
            "Epoch 00229: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.1217 - acc: 0.9833 - val_loss: 0.6434 - val_acc: 0.6000\n",
            "Epoch 230/500\n",
            "\n",
            "Epoch 00230: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0923 - acc: 0.9833 - val_loss: 0.6180 - val_acc: 0.6000\n",
            "Epoch 231/500\n",
            "\n",
            "Epoch 00231: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0756 - acc: 1.0000 - val_loss: 0.5569 - val_acc: 0.7000\n",
            "Epoch 232/500\n",
            "\n",
            "Epoch 00232: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.1162 - acc: 0.9667 - val_loss: 0.5657 - val_acc: 0.6500\n",
            "Epoch 233/500\n",
            "\n",
            "Epoch 00233: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0710 - acc: 0.9833 - val_loss: 0.5819 - val_acc: 0.6000\n",
            "Epoch 234/500\n",
            "\n",
            "Epoch 00234: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0776 - acc: 1.0000 - val_loss: 0.5769 - val_acc: 0.6000\n",
            "Epoch 235/500\n",
            "\n",
            "Epoch 00235: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0814 - acc: 1.0000 - val_loss: 0.5895 - val_acc: 0.6000\n",
            "Epoch 236/500\n",
            "\n",
            "Epoch 00236: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0569 - acc: 1.0000 - val_loss: 0.5970 - val_acc: 0.6000\n",
            "Epoch 237/500\n",
            "\n",
            "Epoch 00237: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0813 - acc: 0.9833 - val_loss: 0.6143 - val_acc: 0.6000\n",
            "Epoch 238/500\n",
            "\n",
            "Epoch 00238: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0503 - acc: 1.0000 - val_loss: 0.6708 - val_acc: 0.6000\n",
            "Epoch 239/500\n",
            "\n",
            "Epoch 00239: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.1329 - acc: 0.9167 - val_loss: 0.7256 - val_acc: 0.6000\n",
            "Epoch 240/500\n",
            "\n",
            "Epoch 00240: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0625 - acc: 1.0000 - val_loss: 0.6730 - val_acc: 0.6000\n",
            "Epoch 241/500\n",
            "\n",
            "Epoch 00241: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0651 - acc: 1.0000 - val_loss: 0.6684 - val_acc: 0.6000\n",
            "Epoch 242/500\n",
            "\n",
            "Epoch 00242: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.1113 - acc: 0.9667 - val_loss: 0.6046 - val_acc: 0.6500\n",
            "Epoch 243/500\n",
            "\n",
            "Epoch 00243: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0677 - acc: 0.9833 - val_loss: 0.5657 - val_acc: 0.6500\n",
            "Epoch 244/500\n",
            "\n",
            "Epoch 00244: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0881 - acc: 1.0000 - val_loss: 0.5214 - val_acc: 0.7500\n",
            "Epoch 245/500\n",
            "\n",
            "Epoch 00245: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0493 - acc: 1.0000 - val_loss: 0.5019 - val_acc: 0.7500\n",
            "Epoch 246/500\n",
            "\n",
            "Epoch 00246: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0597 - acc: 1.0000 - val_loss: 0.5091 - val_acc: 0.7500\n",
            "Epoch 247/500\n",
            "\n",
            "Epoch 00247: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0478 - acc: 1.0000 - val_loss: 0.5395 - val_acc: 0.7500\n",
            "Epoch 248/500\n",
            "\n",
            "Epoch 00248: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0612 - acc: 1.0000 - val_loss: 0.5668 - val_acc: 0.7000\n",
            "Epoch 249/500\n",
            "\n",
            "Epoch 00249: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0992 - acc: 0.9833 - val_loss: 0.6328 - val_acc: 0.6500\n",
            "Epoch 250/500\n",
            "\n",
            "Epoch 00250: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0872 - acc: 0.9833 - val_loss: 0.6729 - val_acc: 0.6000\n",
            "Epoch 251/500\n",
            "\n",
            "Epoch 00251: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0887 - acc: 0.9667 - val_loss: 0.6798 - val_acc: 0.6000\n",
            "Epoch 252/500\n",
            "\n",
            "Epoch 00252: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0435 - acc: 1.0000 - val_loss: 0.7240 - val_acc: 0.6000\n",
            "Epoch 253/500\n",
            "\n",
            "Epoch 00253: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0650 - acc: 0.9833 - val_loss: 0.6857 - val_acc: 0.6500\n",
            "Epoch 254/500\n",
            "\n",
            "Epoch 00254: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0617 - acc: 1.0000 - val_loss: 0.6262 - val_acc: 0.7000\n",
            "Epoch 255/500\n",
            "\n",
            "Epoch 00255: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.1035 - acc: 0.9500 - val_loss: 0.6214 - val_acc: 0.7500\n",
            "Epoch 256/500\n",
            "\n",
            "Epoch 00256: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0567 - acc: 1.0000 - val_loss: 0.6840 - val_acc: 0.6500\n",
            "Epoch 257/500\n",
            "\n",
            "Epoch 00257: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0786 - acc: 0.9833 - val_loss: 0.7733 - val_acc: 0.6000\n",
            "Epoch 258/500\n",
            "\n",
            "Epoch 00258: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0560 - acc: 1.0000 - val_loss: 0.7802 - val_acc: 0.6000\n",
            "Epoch 259/500\n",
            "\n",
            "Epoch 00259: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0583 - acc: 1.0000 - val_loss: 0.7005 - val_acc: 0.6000\n",
            "Epoch 260/500\n",
            "\n",
            "Epoch 00260: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0646 - acc: 1.0000 - val_loss: 0.6017 - val_acc: 0.6500\n",
            "Epoch 261/500\n",
            "\n",
            "Epoch 00261: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0659 - acc: 0.9833 - val_loss: 0.5162 - val_acc: 0.7500\n",
            "Epoch 262/500\n",
            "\n",
            "Epoch 00262: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0586 - acc: 0.9833 - val_loss: 0.4735 - val_acc: 0.7500\n",
            "Epoch 263/500\n",
            "\n",
            "Epoch 00263: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0753 - acc: 0.9833 - val_loss: 0.4875 - val_acc: 0.7500\n",
            "Epoch 264/500\n",
            "\n",
            "Epoch 00264: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0566 - acc: 1.0000 - val_loss: 0.5259 - val_acc: 0.7500\n",
            "Epoch 265/500\n",
            "\n",
            "Epoch 00265: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0822 - acc: 0.9667 - val_loss: 0.5393 - val_acc: 0.7500\n",
            "Epoch 266/500\n",
            "\n",
            "Epoch 00266: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0879 - acc: 0.9833 - val_loss: 0.5925 - val_acc: 0.7000\n",
            "Epoch 267/500\n",
            "\n",
            "Epoch 00267: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0931 - acc: 0.9667 - val_loss: 0.6397 - val_acc: 0.6000\n",
            "Epoch 268/500\n",
            "\n",
            "Epoch 00268: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0542 - acc: 1.0000 - val_loss: 0.6030 - val_acc: 0.7000\n",
            "Epoch 269/500\n",
            "\n",
            "Epoch 00269: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0578 - acc: 1.0000 - val_loss: 0.5773 - val_acc: 0.7500\n",
            "Epoch 270/500\n",
            "\n",
            "Epoch 00270: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0749 - acc: 0.9667 - val_loss: 0.5914 - val_acc: 0.7500\n",
            "Epoch 271/500\n",
            "\n",
            "Epoch 00271: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0469 - acc: 1.0000 - val_loss: 0.6267 - val_acc: 0.6500\n",
            "Epoch 272/500\n",
            "\n",
            "Epoch 00272: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0665 - acc: 1.0000 - val_loss: 0.6396 - val_acc: 0.6500\n",
            "Epoch 273/500\n",
            "\n",
            "Epoch 00273: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0730 - acc: 0.9667 - val_loss: 0.6376 - val_acc: 0.7000\n",
            "Epoch 274/500\n",
            "\n",
            "Epoch 00274: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0363 - acc: 1.0000 - val_loss: 0.6029 - val_acc: 0.7000\n",
            "Epoch 275/500\n",
            "\n",
            "Epoch 00275: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0696 - acc: 1.0000 - val_loss: 0.5434 - val_acc: 0.7500\n",
            "Epoch 276/500\n",
            "\n",
            "Epoch 00276: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0593 - acc: 1.0000 - val_loss: 0.5070 - val_acc: 0.7500\n",
            "Epoch 277/500\n",
            "\n",
            "Epoch 00277: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0681 - acc: 0.9833 - val_loss: 0.4473 - val_acc: 0.7500\n",
            "Epoch 278/500\n",
            "\n",
            "Epoch 00278: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0528 - acc: 1.0000 - val_loss: 0.4034 - val_acc: 0.8500\n",
            "Epoch 279/500\n",
            "\n",
            "Epoch 00279: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0677 - acc: 1.0000 - val_loss: 0.4070 - val_acc: 0.8500\n",
            "Epoch 280/500\n",
            "\n",
            "Epoch 00280: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0981 - acc: 0.9667 - val_loss: 0.4324 - val_acc: 0.8000\n",
            "Epoch 281/500\n",
            "\n",
            "Epoch 00281: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0425 - acc: 1.0000 - val_loss: 0.4455 - val_acc: 0.8000\n",
            "Epoch 282/500\n",
            "\n",
            "Epoch 00282: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.1458 - acc: 0.9167 - val_loss: 0.4620 - val_acc: 0.8000\n",
            "Epoch 283/500\n",
            "\n",
            "Epoch 00283: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0515 - acc: 1.0000 - val_loss: 0.4714 - val_acc: 0.7500\n",
            "Epoch 284/500\n",
            "\n",
            "Epoch 00284: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0551 - acc: 1.0000 - val_loss: 0.4666 - val_acc: 0.7500\n",
            "Epoch 285/500\n",
            "\n",
            "Epoch 00285: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0717 - acc: 0.9833 - val_loss: 0.4533 - val_acc: 0.7500\n",
            "Epoch 286/500\n",
            "\n",
            "Epoch 00286: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0467 - acc: 1.0000 - val_loss: 0.4310 - val_acc: 0.7500\n",
            "Epoch 287/500\n",
            "\n",
            "Epoch 00287: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0616 - acc: 1.0000 - val_loss: 0.4281 - val_acc: 0.7500\n",
            "Epoch 288/500\n",
            "\n",
            "Epoch 00288: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0637 - acc: 1.0000 - val_loss: 0.4783 - val_acc: 0.7500\n",
            "Epoch 289/500\n",
            "\n",
            "Epoch 00289: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0455 - acc: 1.0000 - val_loss: 0.5068 - val_acc: 0.7500\n",
            "Epoch 290/500\n",
            "\n",
            "Epoch 00290: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0568 - acc: 0.9833 - val_loss: 0.5498 - val_acc: 0.7000\n",
            "Epoch 291/500\n",
            "\n",
            "Epoch 00291: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0519 - acc: 1.0000 - val_loss: 0.6131 - val_acc: 0.6500\n",
            "Epoch 292/500\n",
            "\n",
            "Epoch 00292: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0578 - acc: 0.9667 - val_loss: 0.6227 - val_acc: 0.6500\n",
            "Epoch 293/500\n",
            "\n",
            "Epoch 00293: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0476 - acc: 1.0000 - val_loss: 0.5944 - val_acc: 0.6500\n",
            "Epoch 294/500\n",
            "\n",
            "Epoch 00294: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0536 - acc: 1.0000 - val_loss: 0.5579 - val_acc: 0.7000\n",
            "Epoch 295/500\n",
            "\n",
            "Epoch 00295: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0360 - acc: 1.0000 - val_loss: 0.5519 - val_acc: 0.7000\n",
            "Epoch 296/500\n",
            "\n",
            "Epoch 00296: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0496 - acc: 1.0000 - val_loss: 0.4874 - val_acc: 0.7500\n",
            "Epoch 297/500\n",
            "\n",
            "Epoch 00297: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0693 - acc: 0.9833 - val_loss: 0.4734 - val_acc: 0.7500\n",
            "Epoch 298/500\n",
            "\n",
            "Epoch 00298: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0382 - acc: 1.0000 - val_loss: 0.4637 - val_acc: 0.7500\n",
            "Epoch 299/500\n",
            "\n",
            "Epoch 00299: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0327 - acc: 1.0000 - val_loss: 0.4465 - val_acc: 0.7500\n",
            "Epoch 300/500\n",
            "\n",
            "Epoch 00300: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0448 - acc: 1.0000 - val_loss: 0.4494 - val_acc: 0.7500\n",
            "Epoch 301/500\n",
            "\n",
            "Epoch 00301: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0443 - acc: 1.0000 - val_loss: 0.4731 - val_acc: 0.7500\n",
            "Epoch 302/500\n",
            "\n",
            "Epoch 00302: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0530 - acc: 0.9833 - val_loss: 0.5211 - val_acc: 0.7500\n",
            "Epoch 303/500\n",
            "\n",
            "Epoch 00303: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0634 - acc: 0.9833 - val_loss: 0.5894 - val_acc: 0.7000\n",
            "Epoch 304/500\n",
            "\n",
            "Epoch 00304: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0491 - acc: 1.0000 - val_loss: 0.6286 - val_acc: 0.7000\n",
            "Epoch 305/500\n",
            "\n",
            "Epoch 00305: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0500 - acc: 1.0000 - val_loss: 0.6299 - val_acc: 0.7000\n",
            "Epoch 306/500\n",
            "\n",
            "Epoch 00306: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0656 - acc: 0.9833 - val_loss: 0.5552 - val_acc: 0.7000\n",
            "Epoch 307/500\n",
            "\n",
            "Epoch 00307: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0323 - acc: 1.0000 - val_loss: 0.4803 - val_acc: 0.7500\n",
            "Epoch 308/500\n",
            "\n",
            "Epoch 00308: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0580 - acc: 1.0000 - val_loss: 0.4456 - val_acc: 0.8000\n",
            "Epoch 309/500\n",
            "\n",
            "Epoch 00309: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0268 - acc: 1.0000 - val_loss: 0.4435 - val_acc: 0.8500\n",
            "Epoch 310/500\n",
            "\n",
            "Epoch 00310: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0567 - acc: 1.0000 - val_loss: 0.4384 - val_acc: 0.8500\n",
            "Epoch 311/500\n",
            "\n",
            "Epoch 00311: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0745 - acc: 0.9833 - val_loss: 0.4257 - val_acc: 0.8500\n",
            "Epoch 312/500\n",
            "\n",
            "Epoch 00312: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0601 - acc: 1.0000 - val_loss: 0.4270 - val_acc: 0.8000\n",
            "Epoch 313/500\n",
            "\n",
            "Epoch 00313: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0905 - acc: 0.9667 - val_loss: 0.4170 - val_acc: 0.8000\n",
            "Epoch 314/500\n",
            "\n",
            "Epoch 00314: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0484 - acc: 1.0000 - val_loss: 0.4059 - val_acc: 0.8000\n",
            "Epoch 315/500\n",
            "\n",
            "Epoch 00315: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0629 - acc: 1.0000 - val_loss: 0.4098 - val_acc: 0.8000\n",
            "Epoch 316/500\n",
            "\n",
            "Epoch 00316: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0371 - acc: 1.0000 - val_loss: 0.4232 - val_acc: 0.7500\n",
            "Epoch 317/500\n",
            "\n",
            "Epoch 00317: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0536 - acc: 1.0000 - val_loss: 0.4482 - val_acc: 0.7500\n",
            "Epoch 318/500\n",
            "\n",
            "Epoch 00318: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0566 - acc: 1.0000 - val_loss: 0.5274 - val_acc: 0.7000\n",
            "Epoch 319/500\n",
            "\n",
            "Epoch 00319: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0639 - acc: 0.9833 - val_loss: 0.5546 - val_acc: 0.7000\n",
            "Epoch 320/500\n",
            "\n",
            "Epoch 00320: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0541 - acc: 1.0000 - val_loss: 0.5442 - val_acc: 0.7000\n",
            "Epoch 321/500\n",
            "\n",
            "Epoch 00321: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0771 - acc: 0.9833 - val_loss: 0.4835 - val_acc: 0.7000\n",
            "Epoch 322/500\n",
            "\n",
            "Epoch 00322: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0567 - acc: 1.0000 - val_loss: 0.4311 - val_acc: 0.8000\n",
            "Epoch 323/500\n",
            "\n",
            "Epoch 00323: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0740 - acc: 0.9833 - val_loss: 0.4194 - val_acc: 0.8500\n",
            "Epoch 324/500\n",
            "\n",
            "Epoch 00324: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0550 - acc: 1.0000 - val_loss: 0.4306 - val_acc: 0.8500\n",
            "Epoch 325/500\n",
            "\n",
            "Epoch 00325: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0387 - acc: 1.0000 - val_loss: 0.4454 - val_acc: 0.7500\n",
            "Epoch 326/500\n",
            "\n",
            "Epoch 00326: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0897 - acc: 0.9833 - val_loss: 0.4473 - val_acc: 0.8000\n",
            "Epoch 327/500\n",
            "\n",
            "Epoch 00327: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0405 - acc: 1.0000 - val_loss: 0.4203 - val_acc: 0.8500\n",
            "Epoch 328/500\n",
            "\n",
            "Epoch 00328: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0596 - acc: 1.0000 - val_loss: 0.4197 - val_acc: 0.8500\n",
            "Epoch 329/500\n",
            "\n",
            "Epoch 00329: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0435 - acc: 1.0000 - val_loss: 0.4430 - val_acc: 0.8000\n",
            "Epoch 330/500\n",
            "\n",
            "Epoch 00330: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0740 - acc: 0.9667 - val_loss: 0.4739 - val_acc: 0.7500\n",
            "Epoch 331/500\n",
            "\n",
            "Epoch 00331: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0819 - acc: 1.0000 - val_loss: 0.4660 - val_acc: 0.7500\n",
            "Epoch 332/500\n",
            "\n",
            "Epoch 00332: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.1163 - acc: 0.9333 - val_loss: 0.4642 - val_acc: 0.7500\n",
            "Epoch 333/500\n",
            "\n",
            "Epoch 00333: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0655 - acc: 1.0000 - val_loss: 0.4766 - val_acc: 0.7500\n",
            "Epoch 334/500\n",
            "\n",
            "Epoch 00334: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0453 - acc: 1.0000 - val_loss: 0.4896 - val_acc: 0.7000\n",
            "Epoch 335/500\n",
            "\n",
            "Epoch 00335: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0687 - acc: 0.9833 - val_loss: 0.4474 - val_acc: 0.8000\n",
            "Epoch 336/500\n",
            "\n",
            "Epoch 00336: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0346 - acc: 1.0000 - val_loss: 0.4225 - val_acc: 0.8000\n",
            "Epoch 337/500\n",
            "\n",
            "Epoch 00337: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0502 - acc: 0.9833 - val_loss: 0.4198 - val_acc: 0.8000\n",
            "Epoch 338/500\n",
            "\n",
            "Epoch 00338: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.1012 - acc: 0.9667 - val_loss: 0.4510 - val_acc: 0.8000\n",
            "Epoch 339/500\n",
            "\n",
            "Epoch 00339: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0552 - acc: 1.0000 - val_loss: 0.4954 - val_acc: 0.7500\n",
            "Epoch 340/500\n",
            "\n",
            "Epoch 00340: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0459 - acc: 1.0000 - val_loss: 0.5567 - val_acc: 0.7000\n",
            "Epoch 341/500\n",
            "\n",
            "Epoch 00341: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0381 - acc: 1.0000 - val_loss: 0.6212 - val_acc: 0.7000\n",
            "Epoch 342/500\n",
            "\n",
            "Epoch 00342: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0775 - acc: 0.9667 - val_loss: 0.6751 - val_acc: 0.6500\n",
            "Epoch 343/500\n",
            "\n",
            "Epoch 00343: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0422 - acc: 1.0000 - val_loss: 0.6065 - val_acc: 0.7000\n",
            "Epoch 344/500\n",
            "\n",
            "Epoch 00344: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0693 - acc: 0.9833 - val_loss: 0.5743 - val_acc: 0.7500\n",
            "Epoch 345/500\n",
            "\n",
            "Epoch 00345: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0718 - acc: 0.9833 - val_loss: 0.5761 - val_acc: 0.7500\n",
            "Epoch 346/500\n",
            "\n",
            "Epoch 00346: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0256 - acc: 1.0000 - val_loss: 0.6041 - val_acc: 0.7500\n",
            "Epoch 347/500\n",
            "\n",
            "Epoch 00347: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0364 - acc: 1.0000 - val_loss: 0.5888 - val_acc: 0.7500\n",
            "Epoch 348/500\n",
            "\n",
            "Epoch 00348: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0258 - acc: 1.0000 - val_loss: 0.5406 - val_acc: 0.7500\n",
            "Epoch 349/500\n",
            "\n",
            "Epoch 00349: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0515 - acc: 1.0000 - val_loss: 0.4916 - val_acc: 0.8000\n",
            "Epoch 350/500\n",
            "\n",
            "Epoch 00350: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0381 - acc: 1.0000 - val_loss: 0.4620 - val_acc: 0.8000\n",
            "Epoch 351/500\n",
            "\n",
            "Epoch 00351: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0578 - acc: 0.9667 - val_loss: 0.4749 - val_acc: 0.8000\n",
            "Epoch 352/500\n",
            "\n",
            "Epoch 00352: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0425 - acc: 1.0000 - val_loss: 0.4906 - val_acc: 0.7500\n",
            "Epoch 353/500\n",
            "\n",
            "Epoch 00353: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0515 - acc: 0.9833 - val_loss: 0.5170 - val_acc: 0.7500\n",
            "Epoch 354/500\n",
            "\n",
            "Epoch 00354: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0388 - acc: 0.9833 - val_loss: 0.5258 - val_acc: 0.7500\n",
            "Epoch 355/500\n",
            "\n",
            "Epoch 00355: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0274 - acc: 1.0000 - val_loss: 0.4691 - val_acc: 0.8000\n",
            "Epoch 356/500\n",
            "\n",
            "Epoch 00356: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0535 - acc: 0.9833 - val_loss: 0.4574 - val_acc: 0.8000\n",
            "Epoch 357/500\n",
            "\n",
            "Epoch 00357: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0630 - acc: 0.9833 - val_loss: 0.4839 - val_acc: 0.8000\n",
            "Epoch 358/500\n",
            "\n",
            "Epoch 00358: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0400 - acc: 1.0000 - val_loss: 0.5227 - val_acc: 0.7000\n",
            "Epoch 359/500\n",
            "\n",
            "Epoch 00359: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0498 - acc: 1.0000 - val_loss: 0.5803 - val_acc: 0.7000\n",
            "Epoch 360/500\n",
            "\n",
            "Epoch 00360: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0465 - acc: 1.0000 - val_loss: 0.5947 - val_acc: 0.7000\n",
            "Epoch 361/500\n",
            "\n",
            "Epoch 00361: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0658 - acc: 1.0000 - val_loss: 0.5696 - val_acc: 0.7000\n",
            "Epoch 362/500\n",
            "\n",
            "Epoch 00362: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0454 - acc: 1.0000 - val_loss: 0.5316 - val_acc: 0.7000\n",
            "Epoch 363/500\n",
            "\n",
            "Epoch 00363: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0692 - acc: 1.0000 - val_loss: 0.5563 - val_acc: 0.7000\n",
            "Epoch 364/500\n",
            "\n",
            "Epoch 00364: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0374 - acc: 1.0000 - val_loss: 0.6576 - val_acc: 0.6500\n",
            "Epoch 365/500\n",
            "\n",
            "Epoch 00365: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0533 - acc: 1.0000 - val_loss: 0.7260 - val_acc: 0.6500\n",
            "Epoch 366/500\n",
            "\n",
            "Epoch 00366: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0337 - acc: 1.0000 - val_loss: 0.7276 - val_acc: 0.6500\n",
            "Epoch 367/500\n",
            "\n",
            "Epoch 00367: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0464 - acc: 1.0000 - val_loss: 0.7496 - val_acc: 0.6500\n",
            "Epoch 368/500\n",
            "\n",
            "Epoch 00368: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0484 - acc: 1.0000 - val_loss: 0.8048 - val_acc: 0.6500\n",
            "Epoch 369/500\n",
            "\n",
            "Epoch 00369: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0434 - acc: 1.0000 - val_loss: 0.8320 - val_acc: 0.6500\n",
            "Epoch 370/500\n",
            "\n",
            "Epoch 00370: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0420 - acc: 1.0000 - val_loss: 0.7921 - val_acc: 0.6500\n",
            "Epoch 371/500\n",
            "\n",
            "Epoch 00371: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0325 - acc: 1.0000 - val_loss: 0.7434 - val_acc: 0.6500\n",
            "Epoch 372/500\n",
            "\n",
            "Epoch 00372: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0382 - acc: 1.0000 - val_loss: 0.7318 - val_acc: 0.6500\n",
            "Epoch 373/500\n",
            "\n",
            "Epoch 00373: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0418 - acc: 1.0000 - val_loss: 0.7695 - val_acc: 0.6500\n",
            "Epoch 374/500\n",
            "\n",
            "Epoch 00374: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0287 - acc: 1.0000 - val_loss: 0.8553 - val_acc: 0.7000\n",
            "Epoch 375/500\n",
            "\n",
            "Epoch 00375: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0386 - acc: 1.0000 - val_loss: 0.8614 - val_acc: 0.7000\n",
            "Epoch 376/500\n",
            "\n",
            "Epoch 00376: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0491 - acc: 1.0000 - val_loss: 0.7816 - val_acc: 0.6500\n",
            "Epoch 377/500\n",
            "\n",
            "Epoch 00377: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0295 - acc: 1.0000 - val_loss: 0.6536 - val_acc: 0.7000\n",
            "Epoch 378/500\n",
            "\n",
            "Epoch 00378: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0384 - acc: 1.0000 - val_loss: 0.6015 - val_acc: 0.7000\n",
            "Epoch 379/500\n",
            "\n",
            "Epoch 00379: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0487 - acc: 1.0000 - val_loss: 0.6033 - val_acc: 0.7000\n",
            "Epoch 380/500\n",
            "\n",
            "Epoch 00380: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0526 - acc: 1.0000 - val_loss: 0.6611 - val_acc: 0.7000\n",
            "Epoch 381/500\n",
            "\n",
            "Epoch 00381: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0263 - acc: 1.0000 - val_loss: 0.6935 - val_acc: 0.7000\n",
            "Epoch 382/500\n",
            "\n",
            "Epoch 00382: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0566 - acc: 1.0000 - val_loss: 0.7273 - val_acc: 0.7000\n",
            "Epoch 383/500\n",
            "\n",
            "Epoch 00383: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0303 - acc: 1.0000 - val_loss: 0.7134 - val_acc: 0.7000\n",
            "Epoch 384/500\n",
            "\n",
            "Epoch 00384: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0768 - acc: 0.9833 - val_loss: 0.7158 - val_acc: 0.7000\n",
            "Epoch 385/500\n",
            "\n",
            "Epoch 00385: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0330 - acc: 1.0000 - val_loss: 0.7149 - val_acc: 0.7000\n",
            "Epoch 386/500\n",
            "\n",
            "Epoch 00386: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0263 - acc: 1.0000 - val_loss: 0.6922 - val_acc: 0.7000\n",
            "Epoch 387/500\n",
            "\n",
            "Epoch 00387: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0533 - acc: 1.0000 - val_loss: 0.6477 - val_acc: 0.7500\n",
            "Epoch 388/500\n",
            "\n",
            "Epoch 00388: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0254 - acc: 1.0000 - val_loss: 0.6376 - val_acc: 0.7500\n",
            "Epoch 389/500\n",
            "\n",
            "Epoch 00389: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0522 - acc: 1.0000 - val_loss: 0.6628 - val_acc: 0.7000\n",
            "Epoch 390/500\n",
            "\n",
            "Epoch 00390: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0296 - acc: 1.0000 - val_loss: 0.6410 - val_acc: 0.7000\n",
            "Epoch 391/500\n",
            "\n",
            "Epoch 00391: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.1220 - acc: 0.9500 - val_loss: 0.5986 - val_acc: 0.7000\n",
            "Epoch 392/500\n",
            "\n",
            "Epoch 00392: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0501 - acc: 0.9833 - val_loss: 0.5565 - val_acc: 0.7000\n",
            "Epoch 393/500\n",
            "\n",
            "Epoch 00393: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0552 - acc: 0.9833 - val_loss: 0.5281 - val_acc: 0.7000\n",
            "Epoch 394/500\n",
            "\n",
            "Epoch 00394: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0359 - acc: 1.0000 - val_loss: 0.4885 - val_acc: 0.7500\n",
            "Epoch 395/500\n",
            "\n",
            "Epoch 00395: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0544 - acc: 1.0000 - val_loss: 0.4594 - val_acc: 0.7500\n",
            "Epoch 396/500\n",
            "\n",
            "Epoch 00396: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0346 - acc: 1.0000 - val_loss: 0.4369 - val_acc: 0.8000\n",
            "Epoch 397/500\n",
            "\n",
            "Epoch 00397: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0381 - acc: 1.0000 - val_loss: 0.4207 - val_acc: 0.8500\n",
            "Epoch 398/500\n",
            "\n",
            "Epoch 00398: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0263 - acc: 1.0000 - val_loss: 0.4169 - val_acc: 0.8500\n",
            "Epoch 399/500\n",
            "\n",
            "Epoch 00399: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0491 - acc: 0.9833 - val_loss: 0.4340 - val_acc: 0.8000\n",
            "Epoch 400/500\n",
            "\n",
            "Epoch 00400: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0339 - acc: 1.0000 - val_loss: 0.4519 - val_acc: 0.8000\n",
            "Epoch 401/500\n",
            "\n",
            "Epoch 00401: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0537 - acc: 0.9833 - val_loss: 0.4699 - val_acc: 0.7500\n",
            "Epoch 402/500\n",
            "\n",
            "Epoch 00402: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0511 - acc: 0.9833 - val_loss: 0.4762 - val_acc: 0.7000\n",
            "Epoch 403/500\n",
            "\n",
            "Epoch 00403: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0332 - acc: 1.0000 - val_loss: 0.4727 - val_acc: 0.7500\n",
            "Epoch 404/500\n",
            "\n",
            "Epoch 00404: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0317 - acc: 1.0000 - val_loss: 0.4485 - val_acc: 0.7500\n",
            "Epoch 405/500\n",
            "\n",
            "Epoch 00405: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0281 - acc: 1.0000 - val_loss: 0.4317 - val_acc: 0.7500\n",
            "Epoch 406/500\n",
            "\n",
            "Epoch 00406: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0412 - acc: 1.0000 - val_loss: 0.4352 - val_acc: 0.7500\n",
            "Epoch 407/500\n",
            "\n",
            "Epoch 00407: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0290 - acc: 1.0000 - val_loss: 0.4349 - val_acc: 0.7500\n",
            "Epoch 408/500\n",
            "\n",
            "Epoch 00408: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0666 - acc: 1.0000 - val_loss: 0.4266 - val_acc: 0.7500\n",
            "Epoch 409/500\n",
            "\n",
            "Epoch 00409: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0491 - acc: 1.0000 - val_loss: 0.4348 - val_acc: 0.7500\n",
            "Epoch 410/500\n",
            "\n",
            "Epoch 00410: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0652 - acc: 0.9667 - val_loss: 0.4413 - val_acc: 0.7500\n",
            "Epoch 411/500\n",
            "\n",
            "Epoch 00411: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0777 - acc: 0.9833 - val_loss: 0.4781 - val_acc: 0.7500\n",
            "Epoch 412/500\n",
            "\n",
            "Epoch 00412: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.1365 - acc: 0.9333 - val_loss: 0.5570 - val_acc: 0.7000\n",
            "Epoch 413/500\n",
            "\n",
            "Epoch 00413: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0341 - acc: 1.0000 - val_loss: 0.5380 - val_acc: 0.7000\n",
            "Epoch 414/500\n",
            "\n",
            "Epoch 00414: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0590 - acc: 1.0000 - val_loss: 0.5387 - val_acc: 0.7500\n",
            "Epoch 415/500\n",
            "\n",
            "Epoch 00415: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0430 - acc: 1.0000 - val_loss: 0.4971 - val_acc: 0.8000\n",
            "Epoch 416/500\n",
            "\n",
            "Epoch 00416: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0608 - acc: 1.0000 - val_loss: 0.4711 - val_acc: 0.8000\n",
            "Epoch 417/500\n",
            "\n",
            "Epoch 00417: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0373 - acc: 1.0000 - val_loss: 0.4423 - val_acc: 0.8500\n",
            "Epoch 418/500\n",
            "\n",
            "Epoch 00418: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0365 - acc: 1.0000 - val_loss: 0.4202 - val_acc: 0.8500\n",
            "Epoch 419/500\n",
            "\n",
            "Epoch 00419: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0559 - acc: 0.9833 - val_loss: 0.4179 - val_acc: 0.8500\n",
            "Epoch 420/500\n",
            "\n",
            "Epoch 00420: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0270 - acc: 1.0000 - val_loss: 0.4106 - val_acc: 0.8500\n",
            "Epoch 421/500\n",
            "\n",
            "Epoch 00421: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0519 - acc: 1.0000 - val_loss: 0.4295 - val_acc: 0.8000\n",
            "Epoch 422/500\n",
            "\n",
            "Epoch 00422: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0490 - acc: 1.0000 - val_loss: 0.4394 - val_acc: 0.8000\n",
            "Epoch 423/500\n",
            "\n",
            "Epoch 00423: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0308 - acc: 1.0000 - val_loss: 0.4568 - val_acc: 0.8000\n",
            "Epoch 424/500\n",
            "\n",
            "Epoch 00424: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0649 - acc: 0.9833 - val_loss: 0.4922 - val_acc: 0.7500\n",
            "Epoch 425/500\n",
            "\n",
            "Epoch 00425: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0339 - acc: 1.0000 - val_loss: 0.5345 - val_acc: 0.7000\n",
            "Epoch 426/500\n",
            "\n",
            "Epoch 00426: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0391 - acc: 1.0000 - val_loss: 0.5779 - val_acc: 0.7000\n",
            "Epoch 427/500\n",
            "\n",
            "Epoch 00427: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0416 - acc: 0.9833 - val_loss: 0.6267 - val_acc: 0.7000\n",
            "Epoch 428/500\n",
            "\n",
            "Epoch 00428: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0571 - acc: 0.9833 - val_loss: 0.7402 - val_acc: 0.6500\n",
            "Epoch 429/500\n",
            "\n",
            "Epoch 00429: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0304 - acc: 1.0000 - val_loss: 0.7988 - val_acc: 0.6500\n",
            "Epoch 430/500\n",
            "\n",
            "Epoch 00430: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0312 - acc: 1.0000 - val_loss: 0.8421 - val_acc: 0.6500\n",
            "Epoch 431/500\n",
            "\n",
            "Epoch 00431: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0625 - acc: 1.0000 - val_loss: 0.8619 - val_acc: 0.6500\n",
            "Epoch 432/500\n",
            "\n",
            "Epoch 00432: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0412 - acc: 1.0000 - val_loss: 0.7782 - val_acc: 0.6500\n",
            "Epoch 433/500\n",
            "\n",
            "Epoch 00433: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0271 - acc: 1.0000 - val_loss: 0.7203 - val_acc: 0.6500\n",
            "Epoch 434/500\n",
            "\n",
            "Epoch 00434: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0280 - acc: 1.0000 - val_loss: 0.6816 - val_acc: 0.6500\n",
            "Epoch 435/500\n",
            "\n",
            "Epoch 00435: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0394 - acc: 1.0000 - val_loss: 0.6108 - val_acc: 0.7500\n",
            "Epoch 436/500\n",
            "\n",
            "Epoch 00436: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0229 - acc: 1.0000 - val_loss: 0.5271 - val_acc: 0.8000\n",
            "Epoch 437/500\n",
            "\n",
            "Epoch 00437: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0436 - acc: 1.0000 - val_loss: 0.5053 - val_acc: 0.8000\n",
            "Epoch 438/500\n",
            "\n",
            "Epoch 00438: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0169 - acc: 1.0000 - val_loss: 0.4972 - val_acc: 0.8000\n",
            "Epoch 439/500\n",
            "\n",
            "Epoch 00439: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0427 - acc: 1.0000 - val_loss: 0.4765 - val_acc: 0.8500\n",
            "Epoch 440/500\n",
            "\n",
            "Epoch 00440: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0332 - acc: 0.9833 - val_loss: 0.4867 - val_acc: 0.8000\n",
            "Epoch 441/500\n",
            "\n",
            "Epoch 00441: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0617 - acc: 1.0000 - val_loss: 0.4961 - val_acc: 0.8000\n",
            "Epoch 442/500\n",
            "\n",
            "Epoch 00442: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0783 - acc: 0.9833 - val_loss: 0.5289 - val_acc: 0.7500\n",
            "Epoch 443/500\n",
            "\n",
            "Epoch 00443: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0370 - acc: 1.0000 - val_loss: 0.5800 - val_acc: 0.7000\n",
            "Epoch 444/500\n",
            "\n",
            "Epoch 00444: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0570 - acc: 1.0000 - val_loss: 0.6583 - val_acc: 0.7000\n",
            "Epoch 445/500\n",
            "\n",
            "Epoch 00445: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0550 - acc: 0.9667 - val_loss: 0.5789 - val_acc: 0.7000\n",
            "Epoch 446/500\n",
            "\n",
            "Epoch 00446: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0302 - acc: 1.0000 - val_loss: 0.4867 - val_acc: 0.8000\n",
            "Epoch 447/500\n",
            "\n",
            "Epoch 00447: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0632 - acc: 0.9833 - val_loss: 0.4543 - val_acc: 0.8000\n",
            "Epoch 448/500\n",
            "\n",
            "Epoch 00448: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.1263 - acc: 0.9500 - val_loss: 0.5197 - val_acc: 0.7000\n",
            "Epoch 449/500\n",
            "\n",
            "Epoch 00449: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0732 - acc: 0.9500 - val_loss: 0.5993 - val_acc: 0.7000\n",
            "Epoch 450/500\n",
            "\n",
            "Epoch 00450: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0430 - acc: 1.0000 - val_loss: 0.6275 - val_acc: 0.6500\n",
            "Epoch 451/500\n",
            "\n",
            "Epoch 00451: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0443 - acc: 1.0000 - val_loss: 0.5839 - val_acc: 0.6500\n",
            "Epoch 452/500\n",
            "\n",
            "Epoch 00452: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0591 - acc: 1.0000 - val_loss: 0.4909 - val_acc: 0.8000\n",
            "Epoch 453/500\n",
            "\n",
            "Epoch 00453: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0322 - acc: 1.0000 - val_loss: 0.4416 - val_acc: 0.8500\n",
            "Epoch 454/500\n",
            "\n",
            "Epoch 00454: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0684 - acc: 0.9833 - val_loss: 0.4078 - val_acc: 0.8500\n",
            "Epoch 455/500\n",
            "\n",
            "Epoch 00455: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0627 - acc: 1.0000 - val_loss: 0.3980 - val_acc: 0.8000\n",
            "Epoch 456/500\n",
            "\n",
            "Epoch 00456: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0289 - acc: 1.0000 - val_loss: 0.4008 - val_acc: 0.8000\n",
            "Epoch 457/500\n",
            "\n",
            "Epoch 00457: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0227 - acc: 1.0000 - val_loss: 0.4095 - val_acc: 0.8000\n",
            "Epoch 458/500\n",
            "\n",
            "Epoch 00458: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0764 - acc: 0.9667 - val_loss: 0.4063 - val_acc: 0.8000\n",
            "Epoch 459/500\n",
            "\n",
            "Epoch 00459: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0405 - acc: 1.0000 - val_loss: 0.4041 - val_acc: 0.8000\n",
            "Epoch 460/500\n",
            "\n",
            "Epoch 00460: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0331 - acc: 0.9833 - val_loss: 0.4021 - val_acc: 0.8000\n",
            "Epoch 461/500\n",
            "\n",
            "Epoch 00461: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0476 - acc: 1.0000 - val_loss: 0.3948 - val_acc: 0.8000\n",
            "Epoch 462/500\n",
            "\n",
            "Epoch 00462: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0388 - acc: 1.0000 - val_loss: 0.3889 - val_acc: 0.8000\n",
            "Epoch 463/500\n",
            "\n",
            "Epoch 00463: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0424 - acc: 1.0000 - val_loss: 0.3935 - val_acc: 0.8000\n",
            "Epoch 464/500\n",
            "\n",
            "Epoch 00464: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0293 - acc: 1.0000 - val_loss: 0.3976 - val_acc: 0.8000\n",
            "Epoch 465/500\n",
            "\n",
            "Epoch 00465: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0672 - acc: 0.9833 - val_loss: 0.3947 - val_acc: 0.8000\n",
            "Epoch 466/500\n",
            "\n",
            "Epoch 00466: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0480 - acc: 1.0000 - val_loss: 0.4035 - val_acc: 0.8000\n",
            "Epoch 467/500\n",
            "\n",
            "Epoch 00467: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0451 - acc: 1.0000 - val_loss: 0.4232 - val_acc: 0.8500\n",
            "Epoch 468/500\n",
            "\n",
            "Epoch 00468: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0364 - acc: 1.0000 - val_loss: 0.4367 - val_acc: 0.8500\n",
            "Epoch 469/500\n",
            "\n",
            "Epoch 00469: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0322 - acc: 1.0000 - val_loss: 0.4590 - val_acc: 0.8000\n",
            "Epoch 470/500\n",
            "\n",
            "Epoch 00470: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0382 - acc: 1.0000 - val_loss: 0.4866 - val_acc: 0.7500\n",
            "Epoch 471/500\n",
            "\n",
            "Epoch 00471: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0247 - acc: 1.0000 - val_loss: 0.4931 - val_acc: 0.7500\n",
            "Epoch 472/500\n",
            "\n",
            "Epoch 00472: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0245 - acc: 1.0000 - val_loss: 0.4807 - val_acc: 0.7500\n",
            "Epoch 473/500\n",
            "\n",
            "Epoch 00473: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0349 - acc: 1.0000 - val_loss: 0.4622 - val_acc: 0.8000\n",
            "Epoch 474/500\n",
            "\n",
            "Epoch 00474: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0292 - acc: 1.0000 - val_loss: 0.4721 - val_acc: 0.7500\n",
            "Epoch 475/500\n",
            "\n",
            "Epoch 00475: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0449 - acc: 1.0000 - val_loss: 0.4756 - val_acc: 0.7500\n",
            "Epoch 476/500\n",
            "\n",
            "Epoch 00476: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0271 - acc: 1.0000 - val_loss: 0.4889 - val_acc: 0.7500\n",
            "Epoch 477/500\n",
            "\n",
            "Epoch 00477: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0371 - acc: 1.0000 - val_loss: 0.5183 - val_acc: 0.7000\n",
            "Epoch 478/500\n",
            "\n",
            "Epoch 00478: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0485 - acc: 1.0000 - val_loss: 0.5496 - val_acc: 0.7000\n",
            "Epoch 479/500\n",
            "\n",
            "Epoch 00479: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0312 - acc: 1.0000 - val_loss: 0.5428 - val_acc: 0.7000\n",
            "Epoch 480/500\n",
            "\n",
            "Epoch 00480: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0156 - acc: 1.0000 - val_loss: 0.5312 - val_acc: 0.7500\n",
            "Epoch 481/500\n",
            "\n",
            "Epoch 00481: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0248 - acc: 1.0000 - val_loss: 0.5366 - val_acc: 0.7500\n",
            "Epoch 482/500\n",
            "\n",
            "Epoch 00482: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0270 - acc: 1.0000 - val_loss: 0.5468 - val_acc: 0.7500\n",
            "Epoch 483/500\n",
            "\n",
            "Epoch 00483: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0323 - acc: 1.0000 - val_loss: 0.5702 - val_acc: 0.7500\n",
            "Epoch 484/500\n",
            "\n",
            "Epoch 00484: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0500 - acc: 0.9833 - val_loss: 0.6007 - val_acc: 0.7500\n",
            "Epoch 485/500\n",
            "\n",
            "Epoch 00485: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0478 - acc: 1.0000 - val_loss: 0.6233 - val_acc: 0.7500\n",
            "Epoch 486/500\n",
            "\n",
            "Epoch 00486: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0265 - acc: 1.0000 - val_loss: 0.6834 - val_acc: 0.7500\n",
            "Epoch 487/500\n",
            "\n",
            "Epoch 00487: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0942 - acc: 0.9667 - val_loss: 0.7849 - val_acc: 0.6000\n",
            "Epoch 488/500\n",
            "\n",
            "Epoch 00488: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0360 - acc: 0.9833 - val_loss: 0.7756 - val_acc: 0.6000\n",
            "Epoch 489/500\n",
            "\n",
            "Epoch 00489: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0471 - acc: 1.0000 - val_loss: 0.7269 - val_acc: 0.7000\n",
            "Epoch 490/500\n",
            "\n",
            "Epoch 00490: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0460 - acc: 1.0000 - val_loss: 0.6420 - val_acc: 0.7000\n",
            "Epoch 491/500\n",
            "\n",
            "Epoch 00491: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0252 - acc: 1.0000 - val_loss: 0.5446 - val_acc: 0.7000\n",
            "Epoch 492/500\n",
            "\n",
            "Epoch 00492: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0281 - acc: 1.0000 - val_loss: 0.4948 - val_acc: 0.7500\n",
            "Epoch 493/500\n",
            "\n",
            "Epoch 00493: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0423 - acc: 1.0000 - val_loss: 0.4565 - val_acc: 0.7500\n",
            "Epoch 494/500\n",
            "\n",
            "Epoch 00494: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0422 - acc: 1.0000 - val_loss: 0.4419 - val_acc: 0.8500\n",
            "Epoch 495/500\n",
            "\n",
            "Epoch 00495: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0396 - acc: 0.9833 - val_loss: 0.4474 - val_acc: 0.8500\n",
            "Epoch 496/500\n",
            "\n",
            "Epoch 00496: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0511 - acc: 0.9833 - val_loss: 0.4668 - val_acc: 0.8000\n",
            "Epoch 497/500\n",
            "\n",
            "Epoch 00497: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0772 - acc: 1.0000 - val_loss: 0.5223 - val_acc: 0.7000\n",
            "Epoch 498/500\n",
            "\n",
            "Epoch 00498: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0445 - acc: 1.0000 - val_loss: 0.6852 - val_acc: 0.7000\n",
            "Epoch 499/500\n",
            "\n",
            "Epoch 00499: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0359 - acc: 1.0000 - val_loss: 0.8024 - val_acc: 0.6000\n",
            "Epoch 500/500\n",
            "\n",
            "Epoch 00500: val_loss did not improve from 0.32457\n",
            "60/60 - 0s - loss: 0.0332 - acc: 1.0000 - val_loss: 0.8079 - val_acc: 0.6000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "# Evaluate on test data\n",
            "40/40 [==============================] - 0s 2ms/sample - loss: 0.4302 - acc: 0.8250\n",
            "test loss, test acc: [0.4302380901020115, 0.825]\n",
            "[[0.7097223 ]\n",
            " [0.53007654]\n",
            " [0.97659595]\n",
            " [0.65047685]\n",
            " [0.98661545]\n",
            " [1.26549443]\n",
            " [0.45802247]\n",
            " [0.94635249]\n",
            " [0.84881956]\n",
            " [0.43023809]]\n",
            "[[0.67500001]\n",
            " [0.82499999]\n",
            " [0.47499999]\n",
            " [0.67500001]\n",
            " [0.64999998]\n",
            " [0.67500001]\n",
            " [0.77499998]\n",
            " [0.625     ]\n",
            " [0.60000002]\n",
            " [0.82499999]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kd6FTGs7vBI3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "df_accl_all = pd.DataFrame({'Class1vs2': acc_all[:, 0]})\n",
        "df_accl_all.to_csv (r'EEG_Deep/df_accl_allPatient_8_24_2048:3584.csv', index = None, header=True) #Don't forget to add '.csv' at the end of the path\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}