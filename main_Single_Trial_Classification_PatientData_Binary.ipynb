{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "main_Single_Trial_Classification_PatientData_Binary",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sagihaider/EEG_Deep/blob/master/main_Single_Trial_Classification_PatientData_Binary.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X110PE9MjrlE",
        "colab_type": "code",
        "outputId": "3b212630-c45b-43e8-f5b8-dedf54c61d07",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        }
      },
      "source": [
        "!git clone https://github.com/sagihaider/EEG_Deep.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'EEG_Deep'...\n",
            "remote: Enumerating objects: 39, done.\u001b[K\n",
            "remote: Counting objects: 100% (39/39), done.\u001b[K\n",
            "remote: Compressing objects: 100% (35/35), done.\u001b[K\n",
            "remote: Total 192 (delta 20), reused 10 (delta 4), pack-reused 153\u001b[K\n",
            "Receiving objects: 100% (192/192), 857.72 MiB | 41.23 MiB/s, done.\n",
            "Resolving deltas: 100% (73/73), done.\n",
            "Checking out files: 100% (59/59), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ioLzMoiS60Qf",
        "colab_type": "code",
        "outputId": "d4917255-6904-404e-ec9f-3c3143796c34",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 65
        }
      },
      "source": [
        "import scipy.io as spio\n",
        "import numpy as np\n",
        "from importlib.machinery import SourceFileLoader\n",
        "\n",
        "# EEGNet-specific imports\n",
        "from EEG_Deep.EEGModels import EEGNet, ShallowConvNet, DeepConvNet\n",
        "from tensorflow.keras import utils as np_utils\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras import backend as K\n",
        "K.set_image_data_format('channels_first')\n",
        "\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# tools for plotting confusion matrices\n",
        "from matplotlib import pyplot as plt\n",
        "from scipy.signal import butter, lfilter"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mo80Jg_Pn5lP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Band-pass Filter\n",
        "def butter_bandpass_filter(data, lowcut, highcut, fs, order=4):\n",
        "    nyq = 0.5 * fs\n",
        "    low = lowcut / nyq\n",
        "    high = highcut / nyq\n",
        "    b, a = butter(order, [low, high], btype='band')\n",
        "    y = lfilter(b, a, data)\n",
        "    return y\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZjtnE_6RSpum",
        "colab_type": "code",
        "outputId": "d8d1a6a5-0836-4fa8-f3eb-e50a39fa5fba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from numpy import zeros\n",
        "K.clear_session()\n",
        "cols = 1\n",
        "rows = 10\n",
        "acc_all = zeros([rows, cols])\n",
        "loss_all = zeros([rows, cols])\n",
        "X_tr_c12 = np.empty([80, 12, 4096])\n",
        "X_ts_c12 = np.empty([80, 12, 4096])\n",
        "result=[]\n",
        "\n",
        "from itertools import combinations \n",
        "comb = combinations([1, 2], 2) \n",
        "  # Print the obtained combinations \n",
        "bincomb=[]\n",
        "for i in list(comb): \n",
        "    bincomb.append(i)\n",
        "\n",
        "for x in range(1,11):\n",
        "  fName = 'EEG_Deep/Data2A/parsed_P0' + str(x) + 'T.mat'  # Load Data\n",
        "  print(fName)\n",
        "  mat = spio.loadmat(fName)\n",
        "  r_X_tr = mat['RawEEGData']\n",
        "  r_y_tr = mat['Labels']\n",
        "\n",
        "  ### Filter Data ###\n",
        "  for t in range(r_X_tr.shape[0]):\n",
        "    tril = r_X_tr[t,:,:]\n",
        "    # tril = tril.transpose()\n",
        "    tril_filtered = butter_bandpass_filter(tril, \n",
        "                                              lowcut=8, \n",
        "                                              highcut=24, \n",
        "                                              fs=512,\n",
        "                                              order=4)\n",
        "    # tril_filtered = tril_filtered.transpose()\n",
        "    X_tr_c12[t,:,:] = tril_filtered\n",
        "\n",
        "  print(\"Filtering of Training Data Finished\")\n",
        "  ## Test Data Load \n",
        "\n",
        "  fName = 'EEG_Deep/Data2A/parsed_P0' + str(x) + 'E.mat'  # Load Data\n",
        "  print(fName)\n",
        "  mat = spio.loadmat(fName)\n",
        "  r_X_ts = mat['RawEEGData']\n",
        "  r_y_ts = mat['Labels']\n",
        "  for t in range(r_X_ts.shape[0]):\n",
        "    tril = r_X_ts[t,:,:]\n",
        "    # tril = tril.transpose()\n",
        "    tril_filtered = butter_bandpass_filter(tril, \n",
        "                                              lowcut=8, \n",
        "                                              highcut=24, \n",
        "                                              fs=512,\n",
        "                                              order=4)\n",
        "    # tril_filtered = tril_filtered.transpose()\n",
        "    X_ts_c12[t,:,:] = tril_filtered\n",
        "  \n",
        "  print(\"Filtering of Testing Data Finished\")    \n",
        "\n",
        "  for k, com in enumerate(bincomb):\n",
        "      print(com)\n",
        "      # # Find labels to specific class\n",
        "      print(\"Finding labels in training data\")\n",
        "      class1indx = list(np.where(r_y_tr == com[0]))\n",
        "      class2indx = list(np.where(r_y_tr == com[1]))\n",
        "      c1=list(class1indx[0])\n",
        "      c2=list(class2indx[0])\n",
        "      y_tr_c12 = c1 + c2\n",
        "      y_tr_c12.sort()\n",
        "      # print(y_tr_c12)\n",
        "      x_tr_12 = X_tr_c12[y_tr_c12,:,:]\n",
        "      y_tr_12 = r_y_tr[y_tr_c12]\n",
        "      # print(np.shape(x_tr_12))\n",
        "      # print(np.shape(y_tr_12))\n",
        "      # # Find labels to specific class\n",
        "      print(\"Finding labels in testing data\")\n",
        "      class1indx = list(np.where(r_y_ts == com[0]))\n",
        "      class2indx = list(np.where(r_y_ts == com[1]))\n",
        "      c1=list(class1indx[0])\n",
        "      c2=list(class2indx[0])\n",
        "      y_ts_c12 = c1 + c2\n",
        "      y_ts_c12.sort()\n",
        "      # print(y_ts_c12)\n",
        "      x_ts_12 = X_ts_c12[y_ts_c12,:,:]\n",
        "      y_ts_12 = r_y_ts[y_ts_c12]\n",
        "      # print(np.shape(x_ts_12))\n",
        "      # print(np.shape(y_ts_12))\n",
        "      del class1indx, class2indx, c1, c2\n",
        "\n",
        "      # shuffle the training data\n",
        "      indices = np.arange(x_tr_12.shape[0])\n",
        "      np.random.shuffle(indices)\n",
        "      x_tr_12 = x_tr_12[indices]\n",
        "      y_tr_12 = y_tr_12[indices]\n",
        "\n",
        "      # split data of each subject in training and validation\n",
        "      X_train = x_tr_12[0:60,:,2560:4096]\n",
        "      Y_train = y_tr_12[0:60].ravel()\n",
        "      X_val   = x_tr_12[60:,:,2560:4096]\n",
        "      Y_val   = y_tr_12[60:].ravel()\n",
        "      print(Y_val)\n",
        "      print(np.shape(X_train))\n",
        "      print(np.shape(Y_train))\n",
        "      print(np.shape(X_val))\n",
        "      print(np.shape(Y_val))\n",
        "  \n",
        "      # convert labels to one-hot encodings.\n",
        "      Y_train      = np_utils.to_categorical(Y_train-1, num_classes=2)\n",
        "      Y_val       = np_utils.to_categorical(Y_val-1, num_classes=2)\n",
        "      print(Y_val)\n",
        "\n",
        "      kernels, chans, samples = 1, 12, 1536\n",
        "      # convert data to NCHW (trials, kernels, channels, samples) format. Data \n",
        "      # contains 22 channels and 500 time-points. Set the number of kernels to 1.\n",
        "      X_train = X_train.reshape(X_train.shape[0], kernels, chans, samples)\n",
        "      X_val   = X_val.reshape(X_val.shape[0], kernels, chans, samples)\n",
        "   \n",
        "      print('X_train shape:', X_train.shape)\n",
        "      print(X_train.shape[0], 'train samples')\n",
        "      print(X_val.shape[0], 'val samples')\n",
        "\n",
        "      X_test      = x_ts_12[:,:,2560:4096]\n",
        "      Y_test      = y_ts_12[:]\n",
        "      print(np.shape(X_test))\n",
        "      print(np.shape(Y_test))\n",
        "\n",
        "      #convert labels to one-hot encodings.\n",
        "      Y_test      = np_utils.to_categorical(Y_test-1, num_classes=2)\n",
        "\n",
        "      # convert data to NCHW (trials, kernels, channels, samples) format. Data \n",
        "      # contains 22 channels and 500 time-points. Set the number of kernels to 1.\n",
        "      X_test      = X_test.reshape(X_test.shape[0], kernels, chans, samples)\n",
        "\n",
        "      print('X_train shape:', X_test.shape)\n",
        "      print(X_test.shape[0], 'train samples')\n",
        "\n",
        "      # configure the EEGNet-8,2,16 model with kernel length of 32 samples (other \n",
        "      # model configurations may do better, but this is a good starting point)\n",
        "      model = EEGNet(nb_classes = 2, Chans = 12, Samples = 1536,\n",
        "                     dropoutRate = 0.5, kernLength = 25, F1 = 8, \n",
        "                     D = 2, F2 = 16, norm_rate = 0.25, dropoutType = 'Dropout')\n",
        "      \n",
        "      # compile the model and set the optimizers\n",
        "      model.compile(loss='categorical_crossentropy', optimizer='adam', \n",
        "                    metrics = ['accuracy'])\n",
        "\n",
        "      # count number of parameters in the model\n",
        "      numParams    = model.count_params() \n",
        "\n",
        "      # set a valid path for your system to record model checkpoints\n",
        "      checkpointer = ModelCheckpoint(filepath='/tmp/checkpoint.h5', verbose=1,\n",
        "                                     save_best_only=True)\n",
        "  \n",
        "      # the syntax is {class_1:weight_1, class_2:weight_2,...}. Here just setting\n",
        "      # the weights all to be 1\n",
        "      class_weights = {0:1, 1:1}\n",
        "\n",
        "      history = model.fit(X_train, Y_train, batch_size = 16, epochs = 300, \n",
        "                      verbose = 2, validation_data=(X_val, Y_val),\n",
        "                      callbacks=[checkpointer], class_weight = class_weights)\n",
        "  \n",
        "      # Plot training & validation accuracy values\n",
        "      plt.plot(history.history['acc'])\n",
        "      plt.plot(history.history['val_acc'])\n",
        "      plt.title('Model accuracy')\n",
        "      plt.ylabel('Accuracy')\n",
        "      plt.xlabel('Epoch')\n",
        "      plt.legend(['Train', 'Test'], loc='upper left')\n",
        "      plt.show()\n",
        "      figName = 'Accuracy_A0' + str(x) + '.png'  \n",
        "      plt.savefig(figName)\n",
        "\n",
        "      print('\\n# Evaluate on test data')\n",
        "      results = model.evaluate(X_test, Y_test, batch_size=1)\n",
        "      print('test loss, test acc:', results)\n",
        "\n",
        "      loss_all[x - 1, k-1] = results[0]\n",
        "      acc_all[x - 1, k-1] = results[1]\n",
        "\n",
        "      from keras import backend as K \n",
        "      # Do some code, e.g. train and save model\n",
        "      K.clear_session()\n",
        "\n",
        "print(loss_all)\n",
        "print(acc_all)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "EEG_Deep/Data2A/parsed_P01T.mat\n",
            "Filtering of Training Data Finished\n",
            "EEG_Deep/Data2A/parsed_P01E.mat\n",
            "Filtering of Testing Data Finished\n",
            "(1, 2)\n",
            "Finding labels in training data\n",
            "Finding labels in testing data\n",
            "[2 1 2 1 2 1 1 1 1 2 2 2 1 2 1 2 1 1 1 1]\n",
            "(60, 12, 1536)\n",
            "(60,)\n",
            "(20, 12, 1536)\n",
            "(20,)\n",
            "[[0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]]\n",
            "X_train shape: (60, 1, 12, 1536)\n",
            "60 train samples\n",
            "20 val samples\n",
            "(40, 12, 1536)\n",
            "(40, 1)\n",
            "X_train shape: (40, 1, 12, 1536)\n",
            "40 train samples\n",
            "Train on 60 samples, validate on 20 samples\n",
            "Epoch 1/300\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.69452, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 1s - loss: 0.6891 - acc: 0.4333 - val_loss: 0.6945 - val_acc: 0.5000\n",
            "Epoch 2/300\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.69452\n",
            "60/60 - 0s - loss: 0.6749 - acc: 0.6333 - val_loss: 0.6948 - val_acc: 0.5000\n",
            "Epoch 3/300\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.69452\n",
            "60/60 - 0s - loss: 0.6743 - acc: 0.6333 - val_loss: 0.6949 - val_acc: 0.5000\n",
            "Epoch 4/300\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.69452\n",
            "60/60 - 0s - loss: 0.6703 - acc: 0.6500 - val_loss: 0.6947 - val_acc: 0.4500\n",
            "Epoch 5/300\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.69452\n",
            "60/60 - 0s - loss: 0.6476 - acc: 0.6500 - val_loss: 0.6946 - val_acc: 0.4500\n",
            "Epoch 6/300\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.69452\n",
            "60/60 - 0s - loss: 0.6304 - acc: 0.7833 - val_loss: 0.6952 - val_acc: 0.4500\n",
            "Epoch 7/300\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.69452\n",
            "60/60 - 0s - loss: 0.6109 - acc: 0.8333 - val_loss: 0.6960 - val_acc: 0.4000\n",
            "Epoch 8/300\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.69452\n",
            "60/60 - 0s - loss: 0.6178 - acc: 0.8667 - val_loss: 0.6966 - val_acc: 0.4000\n",
            "Epoch 9/300\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.69452\n",
            "60/60 - 0s - loss: 0.5891 - acc: 0.8833 - val_loss: 0.6969 - val_acc: 0.4000\n",
            "Epoch 10/300\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.69452\n",
            "60/60 - 0s - loss: 0.5973 - acc: 0.8500 - val_loss: 0.6972 - val_acc: 0.3500\n",
            "Epoch 11/300\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.69452\n",
            "60/60 - 0s - loss: 0.5787 - acc: 0.8333 - val_loss: 0.6979 - val_acc: 0.3500\n",
            "Epoch 12/300\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.69452\n",
            "60/60 - 0s - loss: 0.5733 - acc: 0.8667 - val_loss: 0.6981 - val_acc: 0.3500\n",
            "Epoch 13/300\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.69452\n",
            "60/60 - 0s - loss: 0.5620 - acc: 0.8333 - val_loss: 0.6981 - val_acc: 0.4000\n",
            "Epoch 14/300\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.69452\n",
            "60/60 - 0s - loss: 0.5326 - acc: 0.8833 - val_loss: 0.6983 - val_acc: 0.4000\n",
            "Epoch 15/300\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.69452\n",
            "60/60 - 0s - loss: 0.5188 - acc: 0.9333 - val_loss: 0.6983 - val_acc: 0.4000\n",
            "Epoch 16/300\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.69452\n",
            "60/60 - 0s - loss: 0.5363 - acc: 0.8667 - val_loss: 0.6978 - val_acc: 0.4000\n",
            "Epoch 17/300\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.69452\n",
            "60/60 - 0s - loss: 0.4975 - acc: 0.9333 - val_loss: 0.6985 - val_acc: 0.4000\n",
            "Epoch 18/300\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.69452\n",
            "60/60 - 0s - loss: 0.5200 - acc: 0.8833 - val_loss: 0.6998 - val_acc: 0.4000\n",
            "Epoch 19/300\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.69452\n",
            "60/60 - 0s - loss: 0.4706 - acc: 0.9667 - val_loss: 0.7022 - val_acc: 0.4000\n",
            "Epoch 20/300\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.69452\n",
            "60/60 - 0s - loss: 0.4590 - acc: 0.9667 - val_loss: 0.7037 - val_acc: 0.4000\n",
            "Epoch 21/300\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.69452\n",
            "60/60 - 0s - loss: 0.4813 - acc: 0.9000 - val_loss: 0.7035 - val_acc: 0.4000\n",
            "Epoch 22/300\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.69452\n",
            "60/60 - 0s - loss: 0.4702 - acc: 0.9500 - val_loss: 0.7020 - val_acc: 0.4000\n",
            "Epoch 23/300\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.69452\n",
            "60/60 - 0s - loss: 0.4673 - acc: 0.9333 - val_loss: 0.6993 - val_acc: 0.4000\n",
            "Epoch 24/300\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.69452\n",
            "60/60 - 0s - loss: 0.4858 - acc: 0.8000 - val_loss: 0.6962 - val_acc: 0.4000\n",
            "Epoch 25/300\n",
            "\n",
            "Epoch 00025: val_loss improved from 0.69452 to 0.69434, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4403 - acc: 0.9333 - val_loss: 0.6943 - val_acc: 0.4000\n",
            "Epoch 26/300\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.69434\n",
            "60/60 - 0s - loss: 0.4211 - acc: 0.9667 - val_loss: 0.6948 - val_acc: 0.4500\n",
            "Epoch 27/300\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.69434\n",
            "60/60 - 0s - loss: 0.4417 - acc: 0.9167 - val_loss: 0.6950 - val_acc: 0.4500\n",
            "Epoch 28/300\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.69434\n",
            "60/60 - 0s - loss: 0.4108 - acc: 0.9667 - val_loss: 0.6945 - val_acc: 0.4500\n",
            "Epoch 29/300\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.69434\n",
            "60/60 - 0s - loss: 0.4162 - acc: 0.9500 - val_loss: 0.6964 - val_acc: 0.4500\n",
            "Epoch 30/300\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.69434\n",
            "60/60 - 0s - loss: 0.3777 - acc: 0.9500 - val_loss: 0.7005 - val_acc: 0.4500\n",
            "Epoch 31/300\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.69434\n",
            "60/60 - 0s - loss: 0.3978 - acc: 0.9167 - val_loss: 0.6997 - val_acc: 0.4500\n",
            "Epoch 32/300\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.69434\n",
            "60/60 - 0s - loss: 0.3962 - acc: 0.9000 - val_loss: 0.7026 - val_acc: 0.4500\n",
            "Epoch 33/300\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.69434\n",
            "60/60 - 0s - loss: 0.3609 - acc: 0.9167 - val_loss: 0.7056 - val_acc: 0.4500\n",
            "Epoch 34/300\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.69434\n",
            "60/60 - 0s - loss: 0.3520 - acc: 0.9333 - val_loss: 0.7125 - val_acc: 0.4500\n",
            "Epoch 35/300\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.69434\n",
            "60/60 - 0s - loss: 0.3354 - acc: 0.9500 - val_loss: 0.7218 - val_acc: 0.4500\n",
            "Epoch 36/300\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.69434\n",
            "60/60 - 0s - loss: 0.3485 - acc: 0.9167 - val_loss: 0.7283 - val_acc: 0.4500\n",
            "Epoch 37/300\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.69434\n",
            "60/60 - 0s - loss: 0.3718 - acc: 0.9500 - val_loss: 0.7327 - val_acc: 0.4500\n",
            "Epoch 38/300\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.69434\n",
            "60/60 - 0s - loss: 0.3218 - acc: 0.9167 - val_loss: 0.7192 - val_acc: 0.4500\n",
            "Epoch 39/300\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.69434\n",
            "60/60 - 0s - loss: 0.3243 - acc: 0.9667 - val_loss: 0.7044 - val_acc: 0.5000\n",
            "Epoch 40/300\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.69434\n",
            "60/60 - 0s - loss: 0.3518 - acc: 0.9000 - val_loss: 0.6999 - val_acc: 0.5000\n",
            "Epoch 41/300\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.69434\n",
            "60/60 - 0s - loss: 0.3096 - acc: 0.9500 - val_loss: 0.7074 - val_acc: 0.5000\n",
            "Epoch 42/300\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.69434\n",
            "60/60 - 0s - loss: 0.3130 - acc: 0.9000 - val_loss: 0.7252 - val_acc: 0.5000\n",
            "Epoch 43/300\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.69434\n",
            "60/60 - 0s - loss: 0.3569 - acc: 0.8667 - val_loss: 0.7232 - val_acc: 0.5000\n",
            "Epoch 44/300\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.69434\n",
            "60/60 - 0s - loss: 0.3336 - acc: 0.9000 - val_loss: 0.7236 - val_acc: 0.5000\n",
            "Epoch 45/300\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.69434\n",
            "60/60 - 0s - loss: 0.3380 - acc: 0.9333 - val_loss: 0.7361 - val_acc: 0.5000\n",
            "Epoch 46/300\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.69434\n",
            "60/60 - 0s - loss: 0.3012 - acc: 0.9500 - val_loss: 0.7448 - val_acc: 0.5000\n",
            "Epoch 47/300\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.69434\n",
            "60/60 - 0s - loss: 0.3450 - acc: 0.8833 - val_loss: 0.7304 - val_acc: 0.5000\n",
            "Epoch 48/300\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.69434\n",
            "60/60 - 0s - loss: 0.2893 - acc: 0.9500 - val_loss: 0.7158 - val_acc: 0.5500\n",
            "Epoch 49/300\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.69434\n",
            "60/60 - 0s - loss: 0.2931 - acc: 0.9167 - val_loss: 0.7192 - val_acc: 0.5500\n",
            "Epoch 50/300\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.69434\n",
            "60/60 - 0s - loss: 0.3311 - acc: 0.8833 - val_loss: 0.7225 - val_acc: 0.5500\n",
            "Epoch 51/300\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.69434\n",
            "60/60 - 0s - loss: 0.2973 - acc: 0.9333 - val_loss: 0.7212 - val_acc: 0.5500\n",
            "Epoch 52/300\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.69434\n",
            "60/60 - 0s - loss: 0.2801 - acc: 0.9500 - val_loss: 0.7167 - val_acc: 0.5500\n",
            "Epoch 53/300\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.69434\n",
            "60/60 - 0s - loss: 0.2775 - acc: 0.9667 - val_loss: 0.7047 - val_acc: 0.5500\n",
            "Epoch 54/300\n",
            "\n",
            "Epoch 00054: val_loss improved from 0.69434 to 0.68877, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2855 - acc: 0.9167 - val_loss: 0.6888 - val_acc: 0.6000\n",
            "Epoch 55/300\n",
            "\n",
            "Epoch 00055: val_loss improved from 0.68877 to 0.67142, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2810 - acc: 0.9500 - val_loss: 0.6714 - val_acc: 0.6000\n",
            "Epoch 56/300\n",
            "\n",
            "Epoch 00056: val_loss improved from 0.67142 to 0.64120, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2882 - acc: 0.9167 - val_loss: 0.6412 - val_acc: 0.6000\n",
            "Epoch 57/300\n",
            "\n",
            "Epoch 00057: val_loss improved from 0.64120 to 0.63770, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2881 - acc: 0.8833 - val_loss: 0.6377 - val_acc: 0.7000\n",
            "Epoch 58/300\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.63770\n",
            "60/60 - 0s - loss: 0.2486 - acc: 0.9667 - val_loss: 0.6467 - val_acc: 0.6000\n",
            "Epoch 59/300\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.63770\n",
            "60/60 - 0s - loss: 0.2584 - acc: 0.9667 - val_loss: 0.6549 - val_acc: 0.6000\n",
            "Epoch 60/300\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.63770\n",
            "60/60 - 0s - loss: 0.2636 - acc: 0.9500 - val_loss: 0.6896 - val_acc: 0.5500\n",
            "Epoch 61/300\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.63770\n",
            "60/60 - 0s - loss: 0.2898 - acc: 0.9000 - val_loss: 0.7168 - val_acc: 0.5500\n",
            "Epoch 62/300\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.63770\n",
            "60/60 - 0s - loss: 0.2724 - acc: 0.9833 - val_loss: 0.7161 - val_acc: 0.5500\n",
            "Epoch 63/300\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.63770\n",
            "60/60 - 0s - loss: 0.2798 - acc: 0.9167 - val_loss: 0.7020 - val_acc: 0.5500\n",
            "Epoch 64/300\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.63770\n",
            "60/60 - 0s - loss: 0.2447 - acc: 0.9667 - val_loss: 0.6812 - val_acc: 0.6000\n",
            "Epoch 65/300\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.63770\n",
            "60/60 - 0s - loss: 0.2659 - acc: 0.9500 - val_loss: 0.6534 - val_acc: 0.6500\n",
            "Epoch 66/300\n",
            "\n",
            "Epoch 00066: val_loss improved from 0.63770 to 0.62988, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2376 - acc: 0.9167 - val_loss: 0.6299 - val_acc: 0.7500\n",
            "Epoch 67/300\n",
            "\n",
            "Epoch 00067: val_loss improved from 0.62988 to 0.60955, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2103 - acc: 0.9833 - val_loss: 0.6095 - val_acc: 0.7000\n",
            "Epoch 68/300\n",
            "\n",
            "Epoch 00068: val_loss improved from 0.60955 to 0.60110, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2894 - acc: 0.9000 - val_loss: 0.6011 - val_acc: 0.7000\n",
            "Epoch 69/300\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.60110\n",
            "60/60 - 0s - loss: 0.2170 - acc: 0.9667 - val_loss: 0.6022 - val_acc: 0.7500\n",
            "Epoch 70/300\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.60110\n",
            "60/60 - 0s - loss: 0.2597 - acc: 0.9667 - val_loss: 0.6263 - val_acc: 0.7500\n",
            "Epoch 71/300\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.60110\n",
            "60/60 - 0s - loss: 0.2580 - acc: 0.9333 - val_loss: 0.6891 - val_acc: 0.5500\n",
            "Epoch 72/300\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.60110\n",
            "60/60 - 0s - loss: 0.2235 - acc: 0.9500 - val_loss: 0.7396 - val_acc: 0.5500\n",
            "Epoch 73/300\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.60110\n",
            "60/60 - 0s - loss: 0.2947 - acc: 0.9000 - val_loss: 0.7708 - val_acc: 0.5500\n",
            "Epoch 74/300\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.60110\n",
            "60/60 - 0s - loss: 0.2651 - acc: 0.9167 - val_loss: 0.7389 - val_acc: 0.5500\n",
            "Epoch 75/300\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.60110\n",
            "60/60 - 0s - loss: 0.2424 - acc: 0.9167 - val_loss: 0.7009 - val_acc: 0.6000\n",
            "Epoch 76/300\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.60110\n",
            "60/60 - 0s - loss: 0.2383 - acc: 0.9500 - val_loss: 0.6952 - val_acc: 0.6500\n",
            "Epoch 77/300\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.60110\n",
            "60/60 - 0s - loss: 0.2333 - acc: 0.9167 - val_loss: 0.6835 - val_acc: 0.6500\n",
            "Epoch 78/300\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.60110\n",
            "60/60 - 0s - loss: 0.2497 - acc: 0.9333 - val_loss: 0.6318 - val_acc: 0.7000\n",
            "Epoch 79/300\n",
            "\n",
            "Epoch 00079: val_loss improved from 0.60110 to 0.58856, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2087 - acc: 0.9667 - val_loss: 0.5886 - val_acc: 0.8000\n",
            "Epoch 80/300\n",
            "\n",
            "Epoch 00080: val_loss improved from 0.58856 to 0.56211, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2311 - acc: 0.9500 - val_loss: 0.5621 - val_acc: 0.8000\n",
            "Epoch 81/300\n",
            "\n",
            "Epoch 00081: val_loss improved from 0.56211 to 0.55607, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2704 - acc: 0.9000 - val_loss: 0.5561 - val_acc: 0.8000\n",
            "Epoch 82/300\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.55607\n",
            "60/60 - 0s - loss: 0.2177 - acc: 0.9333 - val_loss: 0.5695 - val_acc: 0.8000\n",
            "Epoch 83/300\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.55607\n",
            "60/60 - 0s - loss: 0.1954 - acc: 0.9833 - val_loss: 0.5853 - val_acc: 0.7500\n",
            "Epoch 84/300\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.55607\n",
            "60/60 - 0s - loss: 0.2151 - acc: 0.9500 - val_loss: 0.5788 - val_acc: 0.7500\n",
            "Epoch 85/300\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.55607\n",
            "60/60 - 0s - loss: 0.2150 - acc: 1.0000 - val_loss: 0.5590 - val_acc: 0.8000\n",
            "Epoch 86/300\n",
            "\n",
            "Epoch 00086: val_loss improved from 0.55607 to 0.54465, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2175 - acc: 0.9667 - val_loss: 0.5446 - val_acc: 0.9000\n",
            "Epoch 87/300\n",
            "\n",
            "Epoch 00087: val_loss improved from 0.54465 to 0.54436, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1963 - acc: 0.9833 - val_loss: 0.5444 - val_acc: 0.8500\n",
            "Epoch 88/300\n",
            "\n",
            "Epoch 00088: val_loss improved from 0.54436 to 0.53983, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1887 - acc: 0.9500 - val_loss: 0.5398 - val_acc: 0.8500\n",
            "Epoch 89/300\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.53983\n",
            "60/60 - 0s - loss: 0.1992 - acc: 0.9833 - val_loss: 0.5414 - val_acc: 0.8500\n",
            "Epoch 90/300\n",
            "\n",
            "Epoch 00090: val_loss improved from 0.53983 to 0.53694, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2015 - acc: 0.9333 - val_loss: 0.5369 - val_acc: 0.8000\n",
            "Epoch 91/300\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.53694\n",
            "60/60 - 0s - loss: 0.1848 - acc: 0.9833 - val_loss: 0.5392 - val_acc: 0.8000\n",
            "Epoch 92/300\n",
            "\n",
            "Epoch 00092: val_loss improved from 0.53694 to 0.53149, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1601 - acc: 0.9833 - val_loss: 0.5315 - val_acc: 0.8000\n",
            "Epoch 93/300\n",
            "\n",
            "Epoch 00093: val_loss improved from 0.53149 to 0.52616, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2024 - acc: 0.9500 - val_loss: 0.5262 - val_acc: 0.8000\n",
            "Epoch 94/300\n",
            "\n",
            "Epoch 00094: val_loss improved from 0.52616 to 0.52502, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2166 - acc: 0.9833 - val_loss: 0.5250 - val_acc: 0.8000\n",
            "Epoch 95/300\n",
            "\n",
            "Epoch 00095: val_loss improved from 0.52502 to 0.51816, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2057 - acc: 0.9167 - val_loss: 0.5182 - val_acc: 0.8500\n",
            "Epoch 96/300\n",
            "\n",
            "Epoch 00096: val_loss improved from 0.51816 to 0.50945, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1849 - acc: 0.9833 - val_loss: 0.5095 - val_acc: 0.8500\n",
            "Epoch 97/300\n",
            "\n",
            "Epoch 00097: val_loss improved from 0.50945 to 0.50134, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2014 - acc: 0.9333 - val_loss: 0.5013 - val_acc: 0.8500\n",
            "Epoch 98/300\n",
            "\n",
            "Epoch 00098: val_loss improved from 0.50134 to 0.49590, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1836 - acc: 0.9500 - val_loss: 0.4959 - val_acc: 0.8500\n",
            "Epoch 99/300\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.49590\n",
            "60/60 - 0s - loss: 0.1721 - acc: 1.0000 - val_loss: 0.5187 - val_acc: 0.8000\n",
            "Epoch 100/300\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.49590\n",
            "60/60 - 0s - loss: 0.2333 - acc: 0.9000 - val_loss: 0.5507 - val_acc: 0.8000\n",
            "Epoch 101/300\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.49590\n",
            "60/60 - 0s - loss: 0.2138 - acc: 0.9333 - val_loss: 0.5417 - val_acc: 0.8000\n",
            "Epoch 102/300\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.49590\n",
            "60/60 - 0s - loss: 0.2584 - acc: 0.9000 - val_loss: 0.5248 - val_acc: 0.8500\n",
            "Epoch 103/300\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.49590\n",
            "60/60 - 0s - loss: 0.2337 - acc: 0.9000 - val_loss: 0.5087 - val_acc: 0.8500\n",
            "Epoch 104/300\n",
            "\n",
            "Epoch 00104: val_loss improved from 0.49590 to 0.49568, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1939 - acc: 0.9833 - val_loss: 0.4957 - val_acc: 0.9000\n",
            "Epoch 105/300\n",
            "\n",
            "Epoch 00105: val_loss improved from 0.49568 to 0.48754, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1998 - acc: 0.9667 - val_loss: 0.4875 - val_acc: 0.8500\n",
            "Epoch 106/300\n",
            "\n",
            "Epoch 00106: val_loss improved from 0.48754 to 0.48038, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2033 - acc: 0.9667 - val_loss: 0.4804 - val_acc: 0.9000\n",
            "Epoch 107/300\n",
            "\n",
            "Epoch 00107: val_loss improved from 0.48038 to 0.47582, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1840 - acc: 0.9500 - val_loss: 0.4758 - val_acc: 0.8500\n",
            "Epoch 108/300\n",
            "\n",
            "Epoch 00108: val_loss improved from 0.47582 to 0.46936, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2079 - acc: 0.9667 - val_loss: 0.4694 - val_acc: 0.8500\n",
            "Epoch 109/300\n",
            "\n",
            "Epoch 00109: val_loss improved from 0.46936 to 0.46702, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1970 - acc: 0.9667 - val_loss: 0.4670 - val_acc: 0.8500\n",
            "Epoch 110/300\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.46702\n",
            "60/60 - 0s - loss: 0.1946 - acc: 0.9667 - val_loss: 0.4730 - val_acc: 0.8500\n",
            "Epoch 111/300\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 0.46702\n",
            "60/60 - 0s - loss: 0.1779 - acc: 0.9833 - val_loss: 0.4764 - val_acc: 0.8500\n",
            "Epoch 112/300\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 0.46702\n",
            "60/60 - 0s - loss: 0.1424 - acc: 1.0000 - val_loss: 0.4755 - val_acc: 0.8500\n",
            "Epoch 113/300\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.46702\n",
            "60/60 - 0s - loss: 0.1706 - acc: 1.0000 - val_loss: 0.4710 - val_acc: 0.9000\n",
            "Epoch 114/300\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.46702\n",
            "60/60 - 0s - loss: 0.1862 - acc: 0.9500 - val_loss: 0.4679 - val_acc: 0.8500\n",
            "Epoch 115/300\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.46702\n",
            "60/60 - 0s - loss: 0.1728 - acc: 0.9833 - val_loss: 0.4673 - val_acc: 0.8000\n",
            "Epoch 116/300\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.46702\n",
            "60/60 - 0s - loss: 0.1791 - acc: 0.9667 - val_loss: 0.4693 - val_acc: 0.8000\n",
            "Epoch 117/300\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.46702\n",
            "60/60 - 0s - loss: 0.1752 - acc: 0.9667 - val_loss: 0.4790 - val_acc: 0.8500\n",
            "Epoch 118/300\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.46702\n",
            "60/60 - 0s - loss: 0.1633 - acc: 1.0000 - val_loss: 0.4950 - val_acc: 0.8500\n",
            "Epoch 119/300\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 0.46702\n",
            "60/60 - 0s - loss: 0.2310 - acc: 0.9500 - val_loss: 0.5070 - val_acc: 0.8000\n",
            "Epoch 120/300\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.46702\n",
            "60/60 - 0s - loss: 0.2042 - acc: 0.9333 - val_loss: 0.4989 - val_acc: 0.8000\n",
            "Epoch 121/300\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.46702\n",
            "60/60 - 0s - loss: 0.1705 - acc: 1.0000 - val_loss: 0.4819 - val_acc: 0.8500\n",
            "Epoch 122/300\n",
            "\n",
            "Epoch 00122: val_loss improved from 0.46702 to 0.46221, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1622 - acc: 1.0000 - val_loss: 0.4622 - val_acc: 0.8500\n",
            "Epoch 123/300\n",
            "\n",
            "Epoch 00123: val_loss improved from 0.46221 to 0.45758, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1432 - acc: 1.0000 - val_loss: 0.4576 - val_acc: 0.8500\n",
            "Epoch 124/300\n",
            "\n",
            "Epoch 00124: val_loss improved from 0.45758 to 0.45136, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1476 - acc: 0.9833 - val_loss: 0.4514 - val_acc: 0.8000\n",
            "Epoch 125/300\n",
            "\n",
            "Epoch 00125: val_loss improved from 0.45136 to 0.44475, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1454 - acc: 0.9833 - val_loss: 0.4447 - val_acc: 0.8000\n",
            "Epoch 126/300\n",
            "\n",
            "Epoch 00126: val_loss improved from 0.44475 to 0.44342, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2298 - acc: 0.9500 - val_loss: 0.4434 - val_acc: 0.8000\n",
            "Epoch 127/300\n",
            "\n",
            "Epoch 00127: val_loss improved from 0.44342 to 0.43827, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1482 - acc: 1.0000 - val_loss: 0.4383 - val_acc: 0.8500\n",
            "Epoch 128/300\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 0.43827\n",
            "60/60 - 0s - loss: 0.2233 - acc: 0.9500 - val_loss: 0.4405 - val_acc: 0.8500\n",
            "Epoch 129/300\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.43827\n",
            "60/60 - 0s - loss: 0.1761 - acc: 0.9833 - val_loss: 0.4544 - val_acc: 0.8500\n",
            "Epoch 130/300\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 0.43827\n",
            "60/60 - 0s - loss: 0.1339 - acc: 1.0000 - val_loss: 0.4670 - val_acc: 0.8000\n",
            "Epoch 131/300\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 0.43827\n",
            "60/60 - 0s - loss: 0.1497 - acc: 0.9833 - val_loss: 0.4688 - val_acc: 0.8000\n",
            "Epoch 132/300\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 0.43827\n",
            "60/60 - 0s - loss: 0.1867 - acc: 0.9833 - val_loss: 0.4636 - val_acc: 0.8000\n",
            "Epoch 133/300\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.43827\n",
            "60/60 - 0s - loss: 0.1763 - acc: 0.9667 - val_loss: 0.4547 - val_acc: 0.8000\n",
            "Epoch 134/300\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.43827\n",
            "60/60 - 0s - loss: 0.1389 - acc: 0.9833 - val_loss: 0.4431 - val_acc: 0.8500\n",
            "Epoch 135/300\n",
            "\n",
            "Epoch 00135: val_loss improved from 0.43827 to 0.42747, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1789 - acc: 0.9833 - val_loss: 0.4275 - val_acc: 0.8000\n",
            "Epoch 136/300\n",
            "\n",
            "Epoch 00136: val_loss improved from 0.42747 to 0.41930, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1639 - acc: 0.9667 - val_loss: 0.4193 - val_acc: 0.8000\n",
            "Epoch 137/300\n",
            "\n",
            "Epoch 00137: val_loss improved from 0.41930 to 0.41686, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1638 - acc: 0.9833 - val_loss: 0.4169 - val_acc: 0.7500\n",
            "Epoch 138/300\n",
            "\n",
            "Epoch 00138: val_loss improved from 0.41686 to 0.41544, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1474 - acc: 0.9833 - val_loss: 0.4154 - val_acc: 0.7500\n",
            "Epoch 139/300\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.41544\n",
            "60/60 - 0s - loss: 0.1650 - acc: 0.9667 - val_loss: 0.4192 - val_acc: 0.7500\n",
            "Epoch 140/300\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 0.41544\n",
            "60/60 - 0s - loss: 0.1882 - acc: 0.9500 - val_loss: 0.4249 - val_acc: 0.8000\n",
            "Epoch 141/300\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.41544\n",
            "60/60 - 0s - loss: 0.1555 - acc: 1.0000 - val_loss: 0.4279 - val_acc: 0.8000\n",
            "Epoch 142/300\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.41544\n",
            "60/60 - 0s - loss: 0.1228 - acc: 1.0000 - val_loss: 0.4278 - val_acc: 0.8000\n",
            "Epoch 143/300\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 0.41544\n",
            "60/60 - 0s - loss: 0.1227 - acc: 0.9833 - val_loss: 0.4247 - val_acc: 0.8000\n",
            "Epoch 144/300\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 0.41544\n",
            "60/60 - 0s - loss: 0.1979 - acc: 0.9333 - val_loss: 0.4265 - val_acc: 0.8000\n",
            "Epoch 145/300\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.41544\n",
            "60/60 - 0s - loss: 0.1439 - acc: 0.9667 - val_loss: 0.4233 - val_acc: 0.8500\n",
            "Epoch 146/300\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.41544\n",
            "60/60 - 0s - loss: 0.1320 - acc: 1.0000 - val_loss: 0.4258 - val_acc: 0.8000\n",
            "Epoch 147/300\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.41544\n",
            "60/60 - 0s - loss: 0.1257 - acc: 0.9833 - val_loss: 0.4259 - val_acc: 0.8000\n",
            "Epoch 148/300\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 0.41544\n",
            "60/60 - 0s - loss: 0.2033 - acc: 0.9667 - val_loss: 0.4254 - val_acc: 0.8000\n",
            "Epoch 149/300\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.41544\n",
            "60/60 - 0s - loss: 0.1549 - acc: 0.9833 - val_loss: 0.4390 - val_acc: 0.8000\n",
            "Epoch 150/300\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.41544\n",
            "60/60 - 0s - loss: 0.1693 - acc: 0.9667 - val_loss: 0.4466 - val_acc: 0.7500\n",
            "Epoch 151/300\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.41544\n",
            "60/60 - 0s - loss: 0.1806 - acc: 0.9500 - val_loss: 0.4572 - val_acc: 0.7500\n",
            "Epoch 152/300\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.41544\n",
            "60/60 - 0s - loss: 0.1394 - acc: 0.9500 - val_loss: 0.4693 - val_acc: 0.7500\n",
            "Epoch 153/300\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 0.41544\n",
            "60/60 - 0s - loss: 0.1337 - acc: 0.9833 - val_loss: 0.4804 - val_acc: 0.7500\n",
            "Epoch 154/300\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.41544\n",
            "60/60 - 0s - loss: 0.1356 - acc: 0.9500 - val_loss: 0.4842 - val_acc: 0.7500\n",
            "Epoch 155/300\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.41544\n",
            "60/60 - 0s - loss: 0.1073 - acc: 1.0000 - val_loss: 0.4783 - val_acc: 0.7500\n",
            "Epoch 156/300\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.41544\n",
            "60/60 - 0s - loss: 0.1295 - acc: 0.9833 - val_loss: 0.4753 - val_acc: 0.7500\n",
            "Epoch 157/300\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.41544\n",
            "60/60 - 0s - loss: 0.1217 - acc: 0.9833 - val_loss: 0.4689 - val_acc: 0.7500\n",
            "Epoch 158/300\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.41544\n",
            "60/60 - 0s - loss: 0.1473 - acc: 0.9667 - val_loss: 0.4550 - val_acc: 0.7500\n",
            "Epoch 159/300\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.41544\n",
            "60/60 - 0s - loss: 0.1380 - acc: 0.9667 - val_loss: 0.4513 - val_acc: 0.7500\n",
            "Epoch 160/300\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.41544\n",
            "60/60 - 0s - loss: 0.1476 - acc: 0.9667 - val_loss: 0.4732 - val_acc: 0.8000\n",
            "Epoch 161/300\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.41544\n",
            "60/60 - 0s - loss: 0.1194 - acc: 1.0000 - val_loss: 0.4975 - val_acc: 0.7500\n",
            "Epoch 162/300\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.41544\n",
            "60/60 - 0s - loss: 0.1307 - acc: 0.9833 - val_loss: 0.4869 - val_acc: 0.7500\n",
            "Epoch 163/300\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.41544\n",
            "60/60 - 0s - loss: 0.1440 - acc: 0.9667 - val_loss: 0.4686 - val_acc: 0.8000\n",
            "Epoch 164/300\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.41544\n",
            "60/60 - 0s - loss: 0.1239 - acc: 0.9833 - val_loss: 0.4387 - val_acc: 0.8000\n",
            "Epoch 165/300\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.41544\n",
            "60/60 - 0s - loss: 0.1294 - acc: 0.9833 - val_loss: 0.4236 - val_acc: 0.8000\n",
            "Epoch 166/300\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 0.41544\n",
            "60/60 - 0s - loss: 0.2033 - acc: 0.9500 - val_loss: 0.4177 - val_acc: 0.8000\n",
            "Epoch 167/300\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.41544\n",
            "60/60 - 0s - loss: 0.1406 - acc: 1.0000 - val_loss: 0.4271 - val_acc: 0.8000\n",
            "Epoch 168/300\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.41544\n",
            "60/60 - 0s - loss: 0.1439 - acc: 0.9667 - val_loss: 0.4392 - val_acc: 0.8000\n",
            "Epoch 169/300\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 0.41544\n",
            "60/60 - 0s - loss: 0.1601 - acc: 0.9833 - val_loss: 0.4576 - val_acc: 0.8000\n",
            "Epoch 170/300\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.41544\n",
            "60/60 - 0s - loss: 0.1793 - acc: 0.9167 - val_loss: 0.4736 - val_acc: 0.7500\n",
            "Epoch 171/300\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.41544\n",
            "60/60 - 0s - loss: 0.1493 - acc: 1.0000 - val_loss: 0.4766 - val_acc: 0.7000\n",
            "Epoch 172/300\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.41544\n",
            "60/60 - 0s - loss: 0.1976 - acc: 0.9500 - val_loss: 0.4726 - val_acc: 0.7000\n",
            "Epoch 173/300\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.41544\n",
            "60/60 - 0s - loss: 0.1581 - acc: 0.9667 - val_loss: 0.4715 - val_acc: 0.7000\n",
            "Epoch 174/300\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.41544\n",
            "60/60 - 0s - loss: 0.1656 - acc: 0.9667 - val_loss: 0.4659 - val_acc: 0.7000\n",
            "Epoch 175/300\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.41544\n",
            "60/60 - 0s - loss: 0.1328 - acc: 1.0000 - val_loss: 0.4602 - val_acc: 0.7000\n",
            "Epoch 176/300\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 0.41544\n",
            "60/60 - 0s - loss: 0.1174 - acc: 1.0000 - val_loss: 0.4545 - val_acc: 0.7000\n",
            "Epoch 177/300\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.41544\n",
            "60/60 - 0s - loss: 0.1542 - acc: 0.9667 - val_loss: 0.4536 - val_acc: 0.7500\n",
            "Epoch 178/300\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.41544\n",
            "60/60 - 0s - loss: 0.1285 - acc: 1.0000 - val_loss: 0.4689 - val_acc: 0.7500\n",
            "Epoch 179/300\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.41544\n",
            "60/60 - 0s - loss: 0.1840 - acc: 0.9167 - val_loss: 0.4937 - val_acc: 0.7500\n",
            "Epoch 180/300\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.41544\n",
            "60/60 - 0s - loss: 0.1588 - acc: 0.9833 - val_loss: 0.5025 - val_acc: 0.8000\n",
            "Epoch 181/300\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.41544\n",
            "60/60 - 0s - loss: 0.1075 - acc: 1.0000 - val_loss: 0.4974 - val_acc: 0.7500\n",
            "Epoch 182/300\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.41544\n",
            "60/60 - 0s - loss: 0.1317 - acc: 0.9833 - val_loss: 0.4925 - val_acc: 0.7500\n",
            "Epoch 183/300\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.41544\n",
            "60/60 - 0s - loss: 0.1446 - acc: 1.0000 - val_loss: 0.5022 - val_acc: 0.7500\n",
            "Epoch 184/300\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 0.41544\n",
            "60/60 - 0s - loss: 0.1722 - acc: 0.9667 - val_loss: 0.5100 - val_acc: 0.8000\n",
            "Epoch 185/300\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.41544\n",
            "60/60 - 0s - loss: 0.1205 - acc: 0.9833 - val_loss: 0.5097 - val_acc: 0.8000\n",
            "Epoch 186/300\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.41544\n",
            "60/60 - 0s - loss: 0.1138 - acc: 1.0000 - val_loss: 0.5152 - val_acc: 0.8000\n",
            "Epoch 187/300\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 0.41544\n",
            "60/60 - 0s - loss: 0.1128 - acc: 1.0000 - val_loss: 0.5263 - val_acc: 0.8000\n",
            "Epoch 188/300\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.41544\n",
            "60/60 - 0s - loss: 0.1353 - acc: 0.9667 - val_loss: 0.5382 - val_acc: 0.8000\n",
            "Epoch 189/300\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.41544\n",
            "60/60 - 0s - loss: 0.1296 - acc: 0.9833 - val_loss: 0.5306 - val_acc: 0.8000\n",
            "Epoch 190/300\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.41544\n",
            "60/60 - 0s - loss: 0.1076 - acc: 1.0000 - val_loss: 0.4969 - val_acc: 0.8000\n",
            "Epoch 191/300\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.41544\n",
            "60/60 - 0s - loss: 0.1273 - acc: 0.9833 - val_loss: 0.4686 - val_acc: 0.7500\n",
            "Epoch 192/300\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 0.41544\n",
            "60/60 - 0s - loss: 0.1003 - acc: 1.0000 - val_loss: 0.4455 - val_acc: 0.7500\n",
            "Epoch 193/300\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.41544\n",
            "60/60 - 0s - loss: 0.1654 - acc: 0.9500 - val_loss: 0.4413 - val_acc: 0.7500\n",
            "Epoch 194/300\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.41544\n",
            "60/60 - 0s - loss: 0.1501 - acc: 0.9667 - val_loss: 0.4414 - val_acc: 0.7500\n",
            "Epoch 195/300\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.41544\n",
            "60/60 - 0s - loss: 0.1033 - acc: 1.0000 - val_loss: 0.4431 - val_acc: 0.8000\n",
            "Epoch 196/300\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.41544\n",
            "60/60 - 0s - loss: 0.1613 - acc: 0.9667 - val_loss: 0.4454 - val_acc: 0.7500\n",
            "Epoch 197/300\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.41544\n",
            "60/60 - 0s - loss: 0.1286 - acc: 0.9833 - val_loss: 0.4767 - val_acc: 0.8000\n",
            "Epoch 198/300\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.41544\n",
            "60/60 - 0s - loss: 0.1038 - acc: 1.0000 - val_loss: 0.4903 - val_acc: 0.8000\n",
            "Epoch 199/300\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.41544\n",
            "60/60 - 0s - loss: 0.1378 - acc: 0.9667 - val_loss: 0.4848 - val_acc: 0.8000\n",
            "Epoch 200/300\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.41544\n",
            "60/60 - 0s - loss: 0.0912 - acc: 1.0000 - val_loss: 0.4728 - val_acc: 0.8000\n",
            "Epoch 201/300\n",
            "\n",
            "Epoch 00201: val_loss did not improve from 0.41544\n",
            "60/60 - 0s - loss: 0.0939 - acc: 1.0000 - val_loss: 0.4684 - val_acc: 0.8000\n",
            "Epoch 202/300\n",
            "\n",
            "Epoch 00202: val_loss did not improve from 0.41544\n",
            "60/60 - 0s - loss: 0.1076 - acc: 1.0000 - val_loss: 0.4636 - val_acc: 0.7500\n",
            "Epoch 203/300\n",
            "\n",
            "Epoch 00203: val_loss did not improve from 0.41544\n",
            "60/60 - 0s - loss: 0.1209 - acc: 0.9667 - val_loss: 0.4528 - val_acc: 0.7500\n",
            "Epoch 204/300\n",
            "\n",
            "Epoch 00204: val_loss did not improve from 0.41544\n",
            "60/60 - 0s - loss: 0.1398 - acc: 0.9833 - val_loss: 0.4581 - val_acc: 0.7500\n",
            "Epoch 205/300\n",
            "\n",
            "Epoch 00205: val_loss did not improve from 0.41544\n",
            "60/60 - 0s - loss: 0.1014 - acc: 1.0000 - val_loss: 0.4485 - val_acc: 0.7000\n",
            "Epoch 206/300\n",
            "\n",
            "Epoch 00206: val_loss did not improve from 0.41544\n",
            "60/60 - 0s - loss: 0.1376 - acc: 0.9667 - val_loss: 0.4435 - val_acc: 0.7000\n",
            "Epoch 207/300\n",
            "\n",
            "Epoch 00207: val_loss did not improve from 0.41544\n",
            "60/60 - 0s - loss: 0.1279 - acc: 1.0000 - val_loss: 0.4405 - val_acc: 0.7500\n",
            "Epoch 208/300\n",
            "\n",
            "Epoch 00208: val_loss did not improve from 0.41544\n",
            "60/60 - 0s - loss: 0.1189 - acc: 0.9833 - val_loss: 0.4341 - val_acc: 0.7500\n",
            "Epoch 209/300\n",
            "\n",
            "Epoch 00209: val_loss did not improve from 0.41544\n",
            "60/60 - 0s - loss: 0.1049 - acc: 1.0000 - val_loss: 0.4350 - val_acc: 0.7500\n",
            "Epoch 210/300\n",
            "\n",
            "Epoch 00210: val_loss did not improve from 0.41544\n",
            "60/60 - 0s - loss: 0.1311 - acc: 0.9833 - val_loss: 0.4261 - val_acc: 0.8000\n",
            "Epoch 211/300\n",
            "\n",
            "Epoch 00211: val_loss did not improve from 0.41544\n",
            "60/60 - 0s - loss: 0.1482 - acc: 0.9667 - val_loss: 0.4316 - val_acc: 0.7500\n",
            "Epoch 212/300\n",
            "\n",
            "Epoch 00212: val_loss did not improve from 0.41544\n",
            "60/60 - 0s - loss: 0.1483 - acc: 0.9667 - val_loss: 0.4491 - val_acc: 0.8000\n",
            "Epoch 213/300\n",
            "\n",
            "Epoch 00213: val_loss did not improve from 0.41544\n",
            "60/60 - 0s - loss: 0.1109 - acc: 1.0000 - val_loss: 0.4571 - val_acc: 0.8000\n",
            "Epoch 214/300\n",
            "\n",
            "Epoch 00214: val_loss did not improve from 0.41544\n",
            "60/60 - 0s - loss: 0.1009 - acc: 1.0000 - val_loss: 0.4570 - val_acc: 0.8000\n",
            "Epoch 215/300\n",
            "\n",
            "Epoch 00215: val_loss did not improve from 0.41544\n",
            "60/60 - 0s - loss: 0.1299 - acc: 0.9667 - val_loss: 0.4527 - val_acc: 0.8000\n",
            "Epoch 216/300\n",
            "\n",
            "Epoch 00216: val_loss did not improve from 0.41544\n",
            "60/60 - 0s - loss: 0.0968 - acc: 1.0000 - val_loss: 0.4529 - val_acc: 0.8000\n",
            "Epoch 217/300\n",
            "\n",
            "Epoch 00217: val_loss did not improve from 0.41544\n",
            "60/60 - 0s - loss: 0.1620 - acc: 0.9500 - val_loss: 0.4595 - val_acc: 0.7500\n",
            "Epoch 218/300\n",
            "\n",
            "Epoch 00218: val_loss did not improve from 0.41544\n",
            "60/60 - 0s - loss: 0.1118 - acc: 0.9833 - val_loss: 0.4538 - val_acc: 0.7500\n",
            "Epoch 219/300\n",
            "\n",
            "Epoch 00219: val_loss did not improve from 0.41544\n",
            "60/60 - 0s - loss: 0.1101 - acc: 1.0000 - val_loss: 0.4490 - val_acc: 0.7500\n",
            "Epoch 220/300\n",
            "\n",
            "Epoch 00220: val_loss did not improve from 0.41544\n",
            "60/60 - 0s - loss: 0.0942 - acc: 1.0000 - val_loss: 0.4421 - val_acc: 0.7500\n",
            "Epoch 221/300\n",
            "\n",
            "Epoch 00221: val_loss did not improve from 0.41544\n",
            "60/60 - 0s - loss: 0.1163 - acc: 1.0000 - val_loss: 0.4331 - val_acc: 0.7500\n",
            "Epoch 222/300\n",
            "\n",
            "Epoch 00222: val_loss did not improve from 0.41544\n",
            "60/60 - 0s - loss: 0.1070 - acc: 1.0000 - val_loss: 0.4276 - val_acc: 0.7500\n",
            "Epoch 223/300\n",
            "\n",
            "Epoch 00223: val_loss did not improve from 0.41544\n",
            "60/60 - 0s - loss: 0.1273 - acc: 0.9667 - val_loss: 0.4303 - val_acc: 0.7500\n",
            "Epoch 224/300\n",
            "\n",
            "Epoch 00224: val_loss did not improve from 0.41544\n",
            "60/60 - 0s - loss: 0.0937 - acc: 1.0000 - val_loss: 0.4369 - val_acc: 0.7500\n",
            "Epoch 225/300\n",
            "\n",
            "Epoch 00225: val_loss did not improve from 0.41544\n",
            "60/60 - 0s - loss: 0.1425 - acc: 1.0000 - val_loss: 0.4325 - val_acc: 0.7500\n",
            "Epoch 226/300\n",
            "\n",
            "Epoch 00226: val_loss did not improve from 0.41544\n",
            "60/60 - 0s - loss: 0.1296 - acc: 0.9667 - val_loss: 0.4308 - val_acc: 0.7500\n",
            "Epoch 227/300\n",
            "\n",
            "Epoch 00227: val_loss did not improve from 0.41544\n",
            "60/60 - 0s - loss: 0.0951 - acc: 1.0000 - val_loss: 0.4347 - val_acc: 0.7500\n",
            "Epoch 228/300\n",
            "\n",
            "Epoch 00228: val_loss did not improve from 0.41544\n",
            "60/60 - 0s - loss: 0.1178 - acc: 0.9667 - val_loss: 0.4328 - val_acc: 0.7500\n",
            "Epoch 229/300\n",
            "\n",
            "Epoch 00229: val_loss did not improve from 0.41544\n",
            "60/60 - 0s - loss: 0.1226 - acc: 0.9833 - val_loss: 0.4396 - val_acc: 0.7500\n",
            "Epoch 230/300\n",
            "\n",
            "Epoch 00230: val_loss did not improve from 0.41544\n",
            "60/60 - 0s - loss: 0.1402 - acc: 0.9500 - val_loss: 0.4423 - val_acc: 0.7500\n",
            "Epoch 231/300\n",
            "\n",
            "Epoch 00231: val_loss did not improve from 0.41544\n",
            "60/60 - 0s - loss: 0.0980 - acc: 1.0000 - val_loss: 0.4438 - val_acc: 0.7000\n",
            "Epoch 232/300\n",
            "\n",
            "Epoch 00232: val_loss did not improve from 0.41544\n",
            "60/60 - 0s - loss: 0.1102 - acc: 0.9667 - val_loss: 0.4417 - val_acc: 0.7000\n",
            "Epoch 233/300\n",
            "\n",
            "Epoch 00233: val_loss did not improve from 0.41544\n",
            "60/60 - 0s - loss: 0.1079 - acc: 0.9833 - val_loss: 0.4308 - val_acc: 0.7000\n",
            "Epoch 234/300\n",
            "\n",
            "Epoch 00234: val_loss did not improve from 0.41544\n",
            "60/60 - 0s - loss: 0.1422 - acc: 0.9833 - val_loss: 0.4274 - val_acc: 0.7500\n",
            "Epoch 235/300\n",
            "\n",
            "Epoch 00235: val_loss did not improve from 0.41544\n",
            "60/60 - 0s - loss: 0.1055 - acc: 0.9833 - val_loss: 0.4278 - val_acc: 0.7500\n",
            "Epoch 236/300\n",
            "\n",
            "Epoch 00236: val_loss did not improve from 0.41544\n",
            "60/60 - 0s - loss: 0.0867 - acc: 1.0000 - val_loss: 0.4302 - val_acc: 0.7500\n",
            "Epoch 237/300\n",
            "\n",
            "Epoch 00237: val_loss did not improve from 0.41544\n",
            "60/60 - 0s - loss: 0.1415 - acc: 0.9667 - val_loss: 0.4348 - val_acc: 0.7500\n",
            "Epoch 238/300\n",
            "\n",
            "Epoch 00238: val_loss did not improve from 0.41544\n",
            "60/60 - 0s - loss: 0.0994 - acc: 0.9833 - val_loss: 0.4420 - val_acc: 0.7500\n",
            "Epoch 239/300\n",
            "\n",
            "Epoch 00239: val_loss did not improve from 0.41544\n",
            "60/60 - 0s - loss: 0.1277 - acc: 0.9833 - val_loss: 0.4499 - val_acc: 0.7500\n",
            "Epoch 240/300\n",
            "\n",
            "Epoch 00240: val_loss did not improve from 0.41544\n",
            "60/60 - 0s - loss: 0.0982 - acc: 1.0000 - val_loss: 0.4591 - val_acc: 0.7500\n",
            "Epoch 241/300\n",
            "\n",
            "Epoch 00241: val_loss did not improve from 0.41544\n",
            "60/60 - 0s - loss: 0.1132 - acc: 0.9833 - val_loss: 0.4590 - val_acc: 0.7500\n",
            "Epoch 242/300\n",
            "\n",
            "Epoch 00242: val_loss did not improve from 0.41544\n",
            "60/60 - 0s - loss: 0.1060 - acc: 0.9833 - val_loss: 0.4601 - val_acc: 0.7500\n",
            "Epoch 243/300\n",
            "\n",
            "Epoch 00243: val_loss did not improve from 0.41544\n",
            "60/60 - 0s - loss: 0.0980 - acc: 0.9667 - val_loss: 0.4711 - val_acc: 0.7500\n",
            "Epoch 244/300\n",
            "\n",
            "Epoch 00244: val_loss did not improve from 0.41544\n",
            "60/60 - 0s - loss: 0.1033 - acc: 1.0000 - val_loss: 0.4771 - val_acc: 0.7500\n",
            "Epoch 245/300\n",
            "\n",
            "Epoch 00245: val_loss did not improve from 0.41544\n",
            "60/60 - 0s - loss: 0.0900 - acc: 1.0000 - val_loss: 0.4753 - val_acc: 0.7500\n",
            "Epoch 246/300\n",
            "\n",
            "Epoch 00246: val_loss did not improve from 0.41544\n",
            "60/60 - 0s - loss: 0.1095 - acc: 0.9833 - val_loss: 0.4758 - val_acc: 0.7000\n",
            "Epoch 247/300\n",
            "\n",
            "Epoch 00247: val_loss did not improve from 0.41544\n",
            "60/60 - 0s - loss: 0.0918 - acc: 0.9833 - val_loss: 0.4932 - val_acc: 0.8000\n",
            "Epoch 248/300\n",
            "\n",
            "Epoch 00248: val_loss did not improve from 0.41544\n",
            "60/60 - 0s - loss: 0.0899 - acc: 0.9833 - val_loss: 0.5177 - val_acc: 0.8000\n",
            "Epoch 249/300\n",
            "\n",
            "Epoch 00249: val_loss did not improve from 0.41544\n",
            "60/60 - 0s - loss: 0.1311 - acc: 0.9667 - val_loss: 0.5168 - val_acc: 0.8000\n",
            "Epoch 250/300\n",
            "\n",
            "Epoch 00250: val_loss did not improve from 0.41544\n",
            "60/60 - 0s - loss: 0.1034 - acc: 1.0000 - val_loss: 0.4879 - val_acc: 0.8000\n",
            "Epoch 251/300\n",
            "\n",
            "Epoch 00251: val_loss did not improve from 0.41544\n",
            "60/60 - 0s - loss: 0.1181 - acc: 0.9667 - val_loss: 0.4650 - val_acc: 0.7500\n",
            "Epoch 252/300\n",
            "\n",
            "Epoch 00252: val_loss did not improve from 0.41544\n",
            "60/60 - 0s - loss: 0.1110 - acc: 0.9833 - val_loss: 0.4522 - val_acc: 0.7000\n",
            "Epoch 253/300\n",
            "\n",
            "Epoch 00253: val_loss did not improve from 0.41544\n",
            "60/60 - 0s - loss: 0.0783 - acc: 1.0000 - val_loss: 0.4534 - val_acc: 0.7500\n",
            "Epoch 254/300\n",
            "\n",
            "Epoch 00254: val_loss did not improve from 0.41544\n",
            "60/60 - 0s - loss: 0.1075 - acc: 1.0000 - val_loss: 0.4510 - val_acc: 0.7500\n",
            "Epoch 255/300\n",
            "\n",
            "Epoch 00255: val_loss did not improve from 0.41544\n",
            "60/60 - 0s - loss: 0.1567 - acc: 0.9333 - val_loss: 0.4531 - val_acc: 0.7500\n",
            "Epoch 256/300\n",
            "\n",
            "Epoch 00256: val_loss did not improve from 0.41544\n",
            "60/60 - 0s - loss: 0.0923 - acc: 1.0000 - val_loss: 0.4585 - val_acc: 0.7500\n",
            "Epoch 257/300\n",
            "\n",
            "Epoch 00257: val_loss did not improve from 0.41544\n",
            "60/60 - 0s - loss: 0.1178 - acc: 0.9667 - val_loss: 0.4605 - val_acc: 0.7500\n",
            "Epoch 258/300\n",
            "\n",
            "Epoch 00258: val_loss did not improve from 0.41544\n",
            "60/60 - 0s - loss: 0.1008 - acc: 1.0000 - val_loss: 0.4662 - val_acc: 0.7000\n",
            "Epoch 259/300\n",
            "\n",
            "Epoch 00259: val_loss did not improve from 0.41544\n",
            "60/60 - 0s - loss: 0.0821 - acc: 0.9833 - val_loss: 0.4700 - val_acc: 0.7000\n",
            "Epoch 260/300\n",
            "\n",
            "Epoch 00260: val_loss did not improve from 0.41544\n",
            "60/60 - 0s - loss: 0.1393 - acc: 0.9667 - val_loss: 0.4607 - val_acc: 0.7000\n",
            "Epoch 261/300\n",
            "\n",
            "Epoch 00261: val_loss did not improve from 0.41544\n",
            "60/60 - 0s - loss: 0.0961 - acc: 1.0000 - val_loss: 0.4497 - val_acc: 0.7500\n",
            "Epoch 262/300\n",
            "\n",
            "Epoch 00262: val_loss did not improve from 0.41544\n",
            "60/60 - 0s - loss: 0.1188 - acc: 0.9833 - val_loss: 0.4459 - val_acc: 0.7500\n",
            "Epoch 263/300\n",
            "\n",
            "Epoch 00263: val_loss did not improve from 0.41544\n",
            "60/60 - 0s - loss: 0.0899 - acc: 1.0000 - val_loss: 0.4397 - val_acc: 0.7500\n",
            "Epoch 264/300\n",
            "\n",
            "Epoch 00264: val_loss did not improve from 0.41544\n",
            "60/60 - 0s - loss: 0.0866 - acc: 1.0000 - val_loss: 0.4365 - val_acc: 0.7000\n",
            "Epoch 265/300\n",
            "\n",
            "Epoch 00265: val_loss did not improve from 0.41544\n",
            "60/60 - 0s - loss: 0.0964 - acc: 1.0000 - val_loss: 0.4373 - val_acc: 0.7500\n",
            "Epoch 266/300\n",
            "\n",
            "Epoch 00266: val_loss did not improve from 0.41544\n",
            "60/60 - 0s - loss: 0.1011 - acc: 1.0000 - val_loss: 0.4228 - val_acc: 0.7500\n",
            "Epoch 267/300\n",
            "\n",
            "Epoch 00267: val_loss improved from 0.41544 to 0.40584, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1038 - acc: 0.9833 - val_loss: 0.4058 - val_acc: 0.7000\n",
            "Epoch 268/300\n",
            "\n",
            "Epoch 00268: val_loss improved from 0.40584 to 0.39796, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.0995 - acc: 0.9833 - val_loss: 0.3980 - val_acc: 0.7500\n",
            "Epoch 269/300\n",
            "\n",
            "Epoch 00269: val_loss did not improve from 0.39796\n",
            "60/60 - 0s - loss: 0.1150 - acc: 1.0000 - val_loss: 0.3986 - val_acc: 0.7500\n",
            "Epoch 270/300\n",
            "\n",
            "Epoch 00270: val_loss did not improve from 0.39796\n",
            "60/60 - 0s - loss: 0.0688 - acc: 1.0000 - val_loss: 0.3996 - val_acc: 0.7500\n",
            "Epoch 271/300\n",
            "\n",
            "Epoch 00271: val_loss did not improve from 0.39796\n",
            "60/60 - 0s - loss: 0.1038 - acc: 0.9667 - val_loss: 0.4104 - val_acc: 0.7000\n",
            "Epoch 272/300\n",
            "\n",
            "Epoch 00272: val_loss did not improve from 0.39796\n",
            "60/60 - 0s - loss: 0.1130 - acc: 1.0000 - val_loss: 0.4239 - val_acc: 0.7500\n",
            "Epoch 273/300\n",
            "\n",
            "Epoch 00273: val_loss did not improve from 0.39796\n",
            "60/60 - 0s - loss: 0.0784 - acc: 1.0000 - val_loss: 0.4368 - val_acc: 0.7500\n",
            "Epoch 274/300\n",
            "\n",
            "Epoch 00274: val_loss did not improve from 0.39796\n",
            "60/60 - 0s - loss: 0.0852 - acc: 0.9833 - val_loss: 0.4522 - val_acc: 0.7500\n",
            "Epoch 275/300\n",
            "\n",
            "Epoch 00275: val_loss did not improve from 0.39796\n",
            "60/60 - 0s - loss: 0.0818 - acc: 1.0000 - val_loss: 0.4579 - val_acc: 0.7500\n",
            "Epoch 276/300\n",
            "\n",
            "Epoch 00276: val_loss did not improve from 0.39796\n",
            "60/60 - 0s - loss: 0.1108 - acc: 1.0000 - val_loss: 0.4666 - val_acc: 0.7000\n",
            "Epoch 277/300\n",
            "\n",
            "Epoch 00277: val_loss did not improve from 0.39796\n",
            "60/60 - 0s - loss: 0.1066 - acc: 1.0000 - val_loss: 0.4782 - val_acc: 0.7000\n",
            "Epoch 278/300\n",
            "\n",
            "Epoch 00278: val_loss did not improve from 0.39796\n",
            "60/60 - 0s - loss: 0.0964 - acc: 0.9833 - val_loss: 0.4837 - val_acc: 0.7000\n",
            "Epoch 279/300\n",
            "\n",
            "Epoch 00279: val_loss did not improve from 0.39796\n",
            "60/60 - 0s - loss: 0.0888 - acc: 0.9833 - val_loss: 0.4873 - val_acc: 0.7000\n",
            "Epoch 280/300\n",
            "\n",
            "Epoch 00280: val_loss did not improve from 0.39796\n",
            "60/60 - 0s - loss: 0.1691 - acc: 0.9333 - val_loss: 0.4800 - val_acc: 0.7000\n",
            "Epoch 281/300\n",
            "\n",
            "Epoch 00281: val_loss did not improve from 0.39796\n",
            "60/60 - 0s - loss: 0.0635 - acc: 1.0000 - val_loss: 0.4833 - val_acc: 0.6500\n",
            "Epoch 282/300\n",
            "\n",
            "Epoch 00282: val_loss did not improve from 0.39796\n",
            "60/60 - 0s - loss: 0.1206 - acc: 0.9833 - val_loss: 0.4851 - val_acc: 0.6500\n",
            "Epoch 283/300\n",
            "\n",
            "Epoch 00283: val_loss did not improve from 0.39796\n",
            "60/60 - 0s - loss: 0.1272 - acc: 0.9667 - val_loss: 0.4596 - val_acc: 0.6500\n",
            "Epoch 284/300\n",
            "\n",
            "Epoch 00284: val_loss did not improve from 0.39796\n",
            "60/60 - 0s - loss: 0.0963 - acc: 0.9833 - val_loss: 0.4267 - val_acc: 0.7500\n",
            "Epoch 285/300\n",
            "\n",
            "Epoch 00285: val_loss did not improve from 0.39796\n",
            "60/60 - 0s - loss: 0.1270 - acc: 0.9833 - val_loss: 0.4121 - val_acc: 0.7500\n",
            "Epoch 286/300\n",
            "\n",
            "Epoch 00286: val_loss did not improve from 0.39796\n",
            "60/60 - 0s - loss: 0.1076 - acc: 0.9667 - val_loss: 0.4120 - val_acc: 0.8000\n",
            "Epoch 287/300\n",
            "\n",
            "Epoch 00287: val_loss did not improve from 0.39796\n",
            "60/60 - 0s - loss: 0.0738 - acc: 0.9833 - val_loss: 0.4245 - val_acc: 0.8000\n",
            "Epoch 288/300\n",
            "\n",
            "Epoch 00288: val_loss did not improve from 0.39796\n",
            "60/60 - 0s - loss: 0.0637 - acc: 1.0000 - val_loss: 0.4393 - val_acc: 0.8000\n",
            "Epoch 289/300\n",
            "\n",
            "Epoch 00289: val_loss did not improve from 0.39796\n",
            "60/60 - 0s - loss: 0.0741 - acc: 1.0000 - val_loss: 0.4544 - val_acc: 0.8000\n",
            "Epoch 290/300\n",
            "\n",
            "Epoch 00290: val_loss did not improve from 0.39796\n",
            "60/60 - 0s - loss: 0.1074 - acc: 0.9500 - val_loss: 0.4604 - val_acc: 0.8000\n",
            "Epoch 291/300\n",
            "\n",
            "Epoch 00291: val_loss did not improve from 0.39796\n",
            "60/60 - 0s - loss: 0.0682 - acc: 0.9833 - val_loss: 0.4745 - val_acc: 0.8000\n",
            "Epoch 292/300\n",
            "\n",
            "Epoch 00292: val_loss did not improve from 0.39796\n",
            "60/60 - 0s - loss: 0.1192 - acc: 0.9500 - val_loss: 0.4714 - val_acc: 0.8000\n",
            "Epoch 293/300\n",
            "\n",
            "Epoch 00293: val_loss did not improve from 0.39796\n",
            "60/60 - 0s - loss: 0.1031 - acc: 0.9833 - val_loss: 0.4642 - val_acc: 0.8000\n",
            "Epoch 294/300\n",
            "\n",
            "Epoch 00294: val_loss did not improve from 0.39796\n",
            "60/60 - 0s - loss: 0.0978 - acc: 0.9833 - val_loss: 0.4537 - val_acc: 0.7500\n",
            "Epoch 295/300\n",
            "\n",
            "Epoch 00295: val_loss did not improve from 0.39796\n",
            "60/60 - 0s - loss: 0.1430 - acc: 0.9667 - val_loss: 0.4505 - val_acc: 0.7500\n",
            "Epoch 296/300\n",
            "\n",
            "Epoch 00296: val_loss did not improve from 0.39796\n",
            "60/60 - 0s - loss: 0.0785 - acc: 1.0000 - val_loss: 0.4527 - val_acc: 0.7500\n",
            "Epoch 297/300\n",
            "\n",
            "Epoch 00297: val_loss did not improve from 0.39796\n",
            "60/60 - 0s - loss: 0.0843 - acc: 1.0000 - val_loss: 0.4453 - val_acc: 0.7500\n",
            "Epoch 298/300\n",
            "\n",
            "Epoch 00298: val_loss did not improve from 0.39796\n",
            "60/60 - 0s - loss: 0.0809 - acc: 1.0000 - val_loss: 0.4431 - val_acc: 0.7500\n",
            "Epoch 299/300\n",
            "\n",
            "Epoch 00299: val_loss did not improve from 0.39796\n",
            "60/60 - 0s - loss: 0.1045 - acc: 1.0000 - val_loss: 0.4349 - val_acc: 0.7500\n",
            "Epoch 300/300\n",
            "\n",
            "Epoch 00300: val_loss did not improve from 0.39796\n",
            "60/60 - 0s - loss: 0.1345 - acc: 0.9500 - val_loss: 0.4345 - val_acc: 0.7500\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "# Evaluate on test data\n",
            "40/40 [==============================] - 0s 2ms/sample - loss: 0.6330 - acc: 0.6500\n",
            "test loss, test acc: [0.6329623449128121, 0.65]\n",
            "EEG_Deep/Data2A/parsed_P02T.mat\n",
            "Filtering of Training Data Finished\n",
            "EEG_Deep/Data2A/parsed_P02E.mat\n",
            "Filtering of Testing Data Finished\n",
            "(1, 2)\n",
            "Finding labels in training data\n",
            "Finding labels in testing data\n",
            "[2 1 1 1 1 1 1 1 1 2 1 2 2 1 1 1 2 2 1 1]\n",
            "(60, 12, 1536)\n",
            "(60,)\n",
            "(20, 12, 1536)\n",
            "(20,)\n",
            "[[0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]]\n",
            "X_train shape: (60, 1, 12, 1536)\n",
            "60 train samples\n",
            "20 val samples\n",
            "(40, 12, 1536)\n",
            "(40, 1)\n",
            "X_train shape: (40, 1, 12, 1536)\n",
            "40 train samples\n",
            "Train on 60 samples, validate on 20 samples\n",
            "Epoch 1/300\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.69361, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 1s - loss: 0.7301 - acc: 0.5167 - val_loss: 0.6936 - val_acc: 0.5000\n",
            "Epoch 2/300\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.69361\n",
            "60/60 - 0s - loss: 0.6601 - acc: 0.6500 - val_loss: 0.6974 - val_acc: 0.3000\n",
            "Epoch 3/300\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.69361\n",
            "60/60 - 0s - loss: 0.6538 - acc: 0.5667 - val_loss: 0.7009 - val_acc: 0.3000\n",
            "Epoch 4/300\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.69361\n",
            "60/60 - 0s - loss: 0.6397 - acc: 0.5667 - val_loss: 0.7010 - val_acc: 0.3000\n",
            "Epoch 5/300\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.69361\n",
            "60/60 - 0s - loss: 0.6319 - acc: 0.5667 - val_loss: 0.6998 - val_acc: 0.3000\n",
            "Epoch 6/300\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.69361\n",
            "60/60 - 0s - loss: 0.6151 - acc: 0.6000 - val_loss: 0.6981 - val_acc: 0.3000\n",
            "Epoch 7/300\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.69361\n",
            "60/60 - 0s - loss: 0.5973 - acc: 0.6167 - val_loss: 0.6969 - val_acc: 0.3000\n",
            "Epoch 8/300\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.69361\n",
            "60/60 - 0s - loss: 0.5882 - acc: 0.7167 - val_loss: 0.6953 - val_acc: 0.3000\n",
            "Epoch 9/300\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.69361 to 0.69253, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5859 - acc: 0.7833 - val_loss: 0.6925 - val_acc: 0.4000\n",
            "Epoch 10/300\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.69253 to 0.69046, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5760 - acc: 0.7833 - val_loss: 0.6905 - val_acc: 0.4000\n",
            "Epoch 11/300\n",
            "\n",
            "Epoch 00011: val_loss improved from 0.69046 to 0.68858, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5705 - acc: 0.8500 - val_loss: 0.6886 - val_acc: 0.4500\n",
            "Epoch 12/300\n",
            "\n",
            "Epoch 00012: val_loss improved from 0.68858 to 0.68803, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5417 - acc: 0.8500 - val_loss: 0.6880 - val_acc: 0.4500\n",
            "Epoch 13/300\n",
            "\n",
            "Epoch 00013: val_loss improved from 0.68803 to 0.68634, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5324 - acc: 0.8167 - val_loss: 0.6863 - val_acc: 0.4500\n",
            "Epoch 14/300\n",
            "\n",
            "Epoch 00014: val_loss improved from 0.68634 to 0.68313, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5437 - acc: 0.7833 - val_loss: 0.6831 - val_acc: 0.5000\n",
            "Epoch 15/300\n",
            "\n",
            "Epoch 00015: val_loss improved from 0.68313 to 0.68189, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5194 - acc: 0.8500 - val_loss: 0.6819 - val_acc: 0.5000\n",
            "Epoch 16/300\n",
            "\n",
            "Epoch 00016: val_loss improved from 0.68189 to 0.68041, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4960 - acc: 0.8833 - val_loss: 0.6804 - val_acc: 0.5000\n",
            "Epoch 17/300\n",
            "\n",
            "Epoch 00017: val_loss improved from 0.68041 to 0.67650, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4938 - acc: 0.8333 - val_loss: 0.6765 - val_acc: 0.5000\n",
            "Epoch 18/300\n",
            "\n",
            "Epoch 00018: val_loss improved from 0.67650 to 0.67244, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4896 - acc: 0.8667 - val_loss: 0.6724 - val_acc: 0.6000\n",
            "Epoch 19/300\n",
            "\n",
            "Epoch 00019: val_loss improved from 0.67244 to 0.66834, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4578 - acc: 0.8833 - val_loss: 0.6683 - val_acc: 0.6000\n",
            "Epoch 20/300\n",
            "\n",
            "Epoch 00020: val_loss improved from 0.66834 to 0.66420, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4447 - acc: 0.8833 - val_loss: 0.6642 - val_acc: 0.6000\n",
            "Epoch 21/300\n",
            "\n",
            "Epoch 00021: val_loss improved from 0.66420 to 0.66032, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4516 - acc: 0.9000 - val_loss: 0.6603 - val_acc: 0.6000\n",
            "Epoch 22/300\n",
            "\n",
            "Epoch 00022: val_loss improved from 0.66032 to 0.65849, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4187 - acc: 0.8500 - val_loss: 0.6585 - val_acc: 0.6000\n",
            "Epoch 23/300\n",
            "\n",
            "Epoch 00023: val_loss improved from 0.65849 to 0.65246, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4048 - acc: 0.9167 - val_loss: 0.6525 - val_acc: 0.6000\n",
            "Epoch 24/300\n",
            "\n",
            "Epoch 00024: val_loss improved from 0.65246 to 0.64860, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3728 - acc: 0.9167 - val_loss: 0.6486 - val_acc: 0.6000\n",
            "Epoch 25/300\n",
            "\n",
            "Epoch 00025: val_loss improved from 0.64860 to 0.64409, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3680 - acc: 0.9500 - val_loss: 0.6441 - val_acc: 0.6000\n",
            "Epoch 26/300\n",
            "\n",
            "Epoch 00026: val_loss improved from 0.64409 to 0.63582, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3816 - acc: 0.8833 - val_loss: 0.6358 - val_acc: 0.6000\n",
            "Epoch 27/300\n",
            "\n",
            "Epoch 00027: val_loss improved from 0.63582 to 0.62325, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3944 - acc: 0.9000 - val_loss: 0.6232 - val_acc: 0.6000\n",
            "Epoch 28/300\n",
            "\n",
            "Epoch 00028: val_loss improved from 0.62325 to 0.60660, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3847 - acc: 0.9167 - val_loss: 0.6066 - val_acc: 0.6000\n",
            "Epoch 29/300\n",
            "\n",
            "Epoch 00029: val_loss improved from 0.60660 to 0.59206, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3821 - acc: 0.9167 - val_loss: 0.5921 - val_acc: 0.7500\n",
            "Epoch 30/300\n",
            "\n",
            "Epoch 00030: val_loss improved from 0.59206 to 0.57730, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3800 - acc: 0.9000 - val_loss: 0.5773 - val_acc: 0.7500\n",
            "Epoch 31/300\n",
            "\n",
            "Epoch 00031: val_loss improved from 0.57730 to 0.56270, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3802 - acc: 0.8833 - val_loss: 0.5627 - val_acc: 0.8000\n",
            "Epoch 32/300\n",
            "\n",
            "Epoch 00032: val_loss improved from 0.56270 to 0.54700, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3228 - acc: 0.9500 - val_loss: 0.5470 - val_acc: 0.8000\n",
            "Epoch 33/300\n",
            "\n",
            "Epoch 00033: val_loss improved from 0.54700 to 0.52981, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3249 - acc: 0.9167 - val_loss: 0.5298 - val_acc: 0.9000\n",
            "Epoch 34/300\n",
            "\n",
            "Epoch 00034: val_loss improved from 0.52981 to 0.52233, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3460 - acc: 0.8500 - val_loss: 0.5223 - val_acc: 0.8500\n",
            "Epoch 35/300\n",
            "\n",
            "Epoch 00035: val_loss improved from 0.52233 to 0.52144, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3301 - acc: 0.9333 - val_loss: 0.5214 - val_acc: 0.9000\n",
            "Epoch 36/300\n",
            "\n",
            "Epoch 00036: val_loss improved from 0.52144 to 0.51946, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3282 - acc: 0.9000 - val_loss: 0.5195 - val_acc: 0.9000\n",
            "Epoch 37/300\n",
            "\n",
            "Epoch 00037: val_loss improved from 0.51946 to 0.51664, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2874 - acc: 0.9667 - val_loss: 0.5166 - val_acc: 0.9000\n",
            "Epoch 38/300\n",
            "\n",
            "Epoch 00038: val_loss improved from 0.51664 to 0.50402, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3083 - acc: 0.9167 - val_loss: 0.5040 - val_acc: 0.9000\n",
            "Epoch 39/300\n",
            "\n",
            "Epoch 00039: val_loss improved from 0.50402 to 0.48789, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2946 - acc: 0.9333 - val_loss: 0.4879 - val_acc: 0.9000\n",
            "Epoch 40/300\n",
            "\n",
            "Epoch 00040: val_loss improved from 0.48789 to 0.47155, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3118 - acc: 0.9333 - val_loss: 0.4715 - val_acc: 0.8500\n",
            "Epoch 41/300\n",
            "\n",
            "Epoch 00041: val_loss improved from 0.47155 to 0.45937, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2913 - acc: 0.9000 - val_loss: 0.4594 - val_acc: 0.8500\n",
            "Epoch 42/300\n",
            "\n",
            "Epoch 00042: val_loss improved from 0.45937 to 0.44679, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2748 - acc: 0.9667 - val_loss: 0.4468 - val_acc: 0.8500\n",
            "Epoch 43/300\n",
            "\n",
            "Epoch 00043: val_loss improved from 0.44679 to 0.43727, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2850 - acc: 0.9333 - val_loss: 0.4373 - val_acc: 0.8500\n",
            "Epoch 44/300\n",
            "\n",
            "Epoch 00044: val_loss improved from 0.43727 to 0.43395, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2748 - acc: 0.9167 - val_loss: 0.4339 - val_acc: 0.8000\n",
            "Epoch 45/300\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.43395\n",
            "60/60 - 0s - loss: 0.2890 - acc: 0.9333 - val_loss: 0.4342 - val_acc: 0.8500\n",
            "Epoch 46/300\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.43395\n",
            "60/60 - 0s - loss: 0.2705 - acc: 0.9333 - val_loss: 0.4412 - val_acc: 0.9000\n",
            "Epoch 47/300\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.43395\n",
            "60/60 - 0s - loss: 0.2685 - acc: 0.9667 - val_loss: 0.4455 - val_acc: 0.8500\n",
            "Epoch 48/300\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.43395\n",
            "60/60 - 0s - loss: 0.2658 - acc: 0.9000 - val_loss: 0.4431 - val_acc: 0.8500\n",
            "Epoch 49/300\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.43395\n",
            "60/60 - 0s - loss: 0.2555 - acc: 0.9167 - val_loss: 0.4439 - val_acc: 0.8500\n",
            "Epoch 50/300\n",
            "\n",
            "Epoch 00050: val_loss improved from 0.43395 to 0.42885, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2413 - acc: 0.9667 - val_loss: 0.4289 - val_acc: 0.9000\n",
            "Epoch 51/300\n",
            "\n",
            "Epoch 00051: val_loss improved from 0.42885 to 0.41647, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2533 - acc: 0.9333 - val_loss: 0.4165 - val_acc: 0.8500\n",
            "Epoch 52/300\n",
            "\n",
            "Epoch 00052: val_loss improved from 0.41647 to 0.40618, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3031 - acc: 0.9000 - val_loss: 0.4062 - val_acc: 0.8000\n",
            "Epoch 53/300\n",
            "\n",
            "Epoch 00053: val_loss improved from 0.40618 to 0.39416, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2336 - acc: 0.9667 - val_loss: 0.3942 - val_acc: 0.8500\n",
            "Epoch 54/300\n",
            "\n",
            "Epoch 00054: val_loss improved from 0.39416 to 0.38342, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2538 - acc: 0.9333 - val_loss: 0.3834 - val_acc: 0.8500\n",
            "Epoch 55/300\n",
            "\n",
            "Epoch 00055: val_loss improved from 0.38342 to 0.37664, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2584 - acc: 0.9500 - val_loss: 0.3766 - val_acc: 0.8500\n",
            "Epoch 56/300\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.37664\n",
            "60/60 - 0s - loss: 0.2294 - acc: 0.9500 - val_loss: 0.3771 - val_acc: 0.8000\n",
            "Epoch 57/300\n",
            "\n",
            "Epoch 00057: val_loss improved from 0.37664 to 0.37336, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2294 - acc: 0.9333 - val_loss: 0.3734 - val_acc: 0.8500\n",
            "Epoch 58/300\n",
            "\n",
            "Epoch 00058: val_loss improved from 0.37336 to 0.36981, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2060 - acc: 0.9500 - val_loss: 0.3698 - val_acc: 0.8500\n",
            "Epoch 59/300\n",
            "\n",
            "Epoch 00059: val_loss improved from 0.36981 to 0.36496, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1992 - acc: 0.9833 - val_loss: 0.3650 - val_acc: 0.8500\n",
            "Epoch 60/300\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.36496\n",
            "60/60 - 0s - loss: 0.2349 - acc: 0.9167 - val_loss: 0.3675 - val_acc: 0.9000\n",
            "Epoch 61/300\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.36496\n",
            "60/60 - 0s - loss: 0.1967 - acc: 0.9667 - val_loss: 0.3718 - val_acc: 0.8500\n",
            "Epoch 62/300\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.36496\n",
            "60/60 - 0s - loss: 0.2189 - acc: 0.9500 - val_loss: 0.3689 - val_acc: 0.8500\n",
            "Epoch 63/300\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.36496\n",
            "60/60 - 0s - loss: 0.1856 - acc: 0.9667 - val_loss: 0.3685 - val_acc: 0.8500\n",
            "Epoch 64/300\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.36496\n",
            "60/60 - 0s - loss: 0.2042 - acc: 0.9333 - val_loss: 0.3853 - val_acc: 0.8500\n",
            "Epoch 65/300\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.36496\n",
            "60/60 - 0s - loss: 0.2006 - acc: 0.9500 - val_loss: 0.3916 - val_acc: 0.8000\n",
            "Epoch 66/300\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.36496\n",
            "60/60 - 0s - loss: 0.2228 - acc: 0.9167 - val_loss: 0.3881 - val_acc: 0.8500\n",
            "Epoch 67/300\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.36496\n",
            "60/60 - 0s - loss: 0.2085 - acc: 0.9500 - val_loss: 0.3805 - val_acc: 0.8500\n",
            "Epoch 68/300\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.36496\n",
            "60/60 - 0s - loss: 0.1884 - acc: 0.9667 - val_loss: 0.3720 - val_acc: 0.8500\n",
            "Epoch 69/300\n",
            "\n",
            "Epoch 00069: val_loss improved from 0.36496 to 0.36157, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2354 - acc: 0.9500 - val_loss: 0.3616 - val_acc: 0.8500\n",
            "Epoch 70/300\n",
            "\n",
            "Epoch 00070: val_loss improved from 0.36157 to 0.34769, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2235 - acc: 0.9500 - val_loss: 0.3477 - val_acc: 0.9000\n",
            "Epoch 71/300\n",
            "\n",
            "Epoch 00071: val_loss improved from 0.34769 to 0.33823, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2001 - acc: 0.9667 - val_loss: 0.3382 - val_acc: 0.9000\n",
            "Epoch 72/300\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.33823\n",
            "60/60 - 0s - loss: 0.2237 - acc: 0.9500 - val_loss: 0.3455 - val_acc: 0.9000\n",
            "Epoch 73/300\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.33823\n",
            "60/60 - 0s - loss: 0.2021 - acc: 0.9333 - val_loss: 0.3656 - val_acc: 0.8000\n",
            "Epoch 74/300\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.33823\n",
            "60/60 - 0s - loss: 0.1922 - acc: 0.9833 - val_loss: 0.3820 - val_acc: 0.7500\n",
            "Epoch 75/300\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.33823\n",
            "60/60 - 0s - loss: 0.1828 - acc: 0.9500 - val_loss: 0.3924 - val_acc: 0.7500\n",
            "Epoch 76/300\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.33823\n",
            "60/60 - 0s - loss: 0.1671 - acc: 0.9833 - val_loss: 0.3942 - val_acc: 0.7500\n",
            "Epoch 77/300\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.33823\n",
            "60/60 - 0s - loss: 0.1756 - acc: 0.9500 - val_loss: 0.3806 - val_acc: 0.7500\n",
            "Epoch 78/300\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.33823\n",
            "60/60 - 0s - loss: 0.2408 - acc: 0.9000 - val_loss: 0.3663 - val_acc: 0.7500\n",
            "Epoch 79/300\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.33823\n",
            "60/60 - 0s - loss: 0.1732 - acc: 0.9500 - val_loss: 0.3586 - val_acc: 0.7500\n",
            "Epoch 80/300\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.33823\n",
            "60/60 - 0s - loss: 0.1863 - acc: 0.9833 - val_loss: 0.3495 - val_acc: 0.9000\n",
            "Epoch 81/300\n",
            "\n",
            "Epoch 00081: val_loss improved from 0.33823 to 0.33801, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1644 - acc: 1.0000 - val_loss: 0.3380 - val_acc: 0.8500\n",
            "Epoch 82/300\n",
            "\n",
            "Epoch 00082: val_loss improved from 0.33801 to 0.33171, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1985 - acc: 0.9333 - val_loss: 0.3317 - val_acc: 0.8500\n",
            "Epoch 83/300\n",
            "\n",
            "Epoch 00083: val_loss improved from 0.33171 to 0.32798, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1974 - acc: 0.9333 - val_loss: 0.3280 - val_acc: 0.8500\n",
            "Epoch 84/300\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.32798\n",
            "60/60 - 0s - loss: 0.2164 - acc: 0.9000 - val_loss: 0.3293 - val_acc: 0.8500\n",
            "Epoch 85/300\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.32798\n",
            "60/60 - 0s - loss: 0.1945 - acc: 0.9500 - val_loss: 0.3410 - val_acc: 0.8000\n",
            "Epoch 86/300\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.32798\n",
            "60/60 - 0s - loss: 0.2026 - acc: 0.9167 - val_loss: 0.3596 - val_acc: 0.8000\n",
            "Epoch 87/300\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.32798\n",
            "60/60 - 0s - loss: 0.2151 - acc: 0.9500 - val_loss: 0.3689 - val_acc: 0.8000\n",
            "Epoch 88/300\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.32798\n",
            "60/60 - 0s - loss: 0.1454 - acc: 0.9833 - val_loss: 0.3697 - val_acc: 0.8000\n",
            "Epoch 89/300\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.32798\n",
            "60/60 - 0s - loss: 0.1582 - acc: 0.9667 - val_loss: 0.3643 - val_acc: 0.8000\n",
            "Epoch 90/300\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.32798\n",
            "60/60 - 0s - loss: 0.1844 - acc: 0.9500 - val_loss: 0.3585 - val_acc: 0.8000\n",
            "Epoch 91/300\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.32798\n",
            "60/60 - 0s - loss: 0.1798 - acc: 0.9833 - val_loss: 0.3569 - val_acc: 0.8000\n",
            "Epoch 92/300\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.32798\n",
            "60/60 - 0s - loss: 0.1408 - acc: 0.9833 - val_loss: 0.3557 - val_acc: 0.8000\n",
            "Epoch 93/300\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.32798\n",
            "60/60 - 0s - loss: 0.1485 - acc: 0.9667 - val_loss: 0.3546 - val_acc: 0.8000\n",
            "Epoch 94/300\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.32798\n",
            "60/60 - 0s - loss: 0.1978 - acc: 0.9333 - val_loss: 0.3637 - val_acc: 0.8000\n",
            "Epoch 95/300\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.32798\n",
            "60/60 - 0s - loss: 0.1612 - acc: 0.9833 - val_loss: 0.3715 - val_acc: 0.8000\n",
            "Epoch 96/300\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.32798\n",
            "60/60 - 0s - loss: 0.1649 - acc: 0.9667 - val_loss: 0.3830 - val_acc: 0.8000\n",
            "Epoch 97/300\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.32798\n",
            "60/60 - 0s - loss: 0.1954 - acc: 0.9500 - val_loss: 0.3941 - val_acc: 0.8000\n",
            "Epoch 98/300\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.32798\n",
            "60/60 - 0s - loss: 0.1466 - acc: 0.9833 - val_loss: 0.3996 - val_acc: 0.7500\n",
            "Epoch 99/300\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.32798\n",
            "60/60 - 0s - loss: 0.1607 - acc: 0.9667 - val_loss: 0.4010 - val_acc: 0.7500\n",
            "Epoch 100/300\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.32798\n",
            "60/60 - 0s - loss: 0.1694 - acc: 0.9667 - val_loss: 0.4186 - val_acc: 0.7500\n",
            "Epoch 101/300\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.32798\n",
            "60/60 - 0s - loss: 0.1260 - acc: 0.9833 - val_loss: 0.4368 - val_acc: 0.7500\n",
            "Epoch 102/300\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.32798\n",
            "60/60 - 0s - loss: 0.1780 - acc: 0.9333 - val_loss: 0.4375 - val_acc: 0.7500\n",
            "Epoch 103/300\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.32798\n",
            "60/60 - 0s - loss: 0.1633 - acc: 0.9833 - val_loss: 0.3939 - val_acc: 0.7500\n",
            "Epoch 104/300\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.32798\n",
            "60/60 - 0s - loss: 0.1962 - acc: 0.9333 - val_loss: 0.3796 - val_acc: 0.7500\n",
            "Epoch 105/300\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 0.32798\n",
            "60/60 - 0s - loss: 0.1609 - acc: 0.9667 - val_loss: 0.3641 - val_acc: 0.7500\n",
            "Epoch 106/300\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 0.32798\n",
            "60/60 - 0s - loss: 0.1852 - acc: 0.9500 - val_loss: 0.3514 - val_acc: 0.8500\n",
            "Epoch 107/300\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.32798\n",
            "60/60 - 0s - loss: 0.1604 - acc: 0.9667 - val_loss: 0.3480 - val_acc: 0.8500\n",
            "Epoch 108/300\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 0.32798\n",
            "60/60 - 0s - loss: 0.1745 - acc: 0.9500 - val_loss: 0.3645 - val_acc: 0.7500\n",
            "Epoch 109/300\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.32798\n",
            "60/60 - 0s - loss: 0.1082 - acc: 0.9833 - val_loss: 0.3930 - val_acc: 0.7500\n",
            "Epoch 110/300\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.32798\n",
            "60/60 - 0s - loss: 0.1397 - acc: 0.9667 - val_loss: 0.4047 - val_acc: 0.7500\n",
            "Epoch 111/300\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 0.32798\n",
            "60/60 - 0s - loss: 0.1826 - acc: 0.9667 - val_loss: 0.4123 - val_acc: 0.7500\n",
            "Epoch 112/300\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 0.32798\n",
            "60/60 - 0s - loss: 0.1547 - acc: 0.9500 - val_loss: 0.4060 - val_acc: 0.7500\n",
            "Epoch 113/300\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.32798\n",
            "60/60 - 0s - loss: 0.1339 - acc: 0.9833 - val_loss: 0.3999 - val_acc: 0.7500\n",
            "Epoch 114/300\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.32798\n",
            "60/60 - 0s - loss: 0.1271 - acc: 1.0000 - val_loss: 0.3926 - val_acc: 0.7500\n",
            "Epoch 115/300\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.32798\n",
            "60/60 - 0s - loss: 0.1415 - acc: 0.9667 - val_loss: 0.3943 - val_acc: 0.7500\n",
            "Epoch 116/300\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.32798\n",
            "60/60 - 0s - loss: 0.2246 - acc: 0.9000 - val_loss: 0.4008 - val_acc: 0.8000\n",
            "Epoch 117/300\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.32798\n",
            "60/60 - 0s - loss: 0.1689 - acc: 0.9667 - val_loss: 0.3999 - val_acc: 0.8000\n",
            "Epoch 118/300\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.32798\n",
            "60/60 - 0s - loss: 0.1452 - acc: 0.9667 - val_loss: 0.3935 - val_acc: 0.8000\n",
            "Epoch 119/300\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 0.32798\n",
            "60/60 - 0s - loss: 0.1220 - acc: 0.9833 - val_loss: 0.3988 - val_acc: 0.8000\n",
            "Epoch 120/300\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.32798\n",
            "60/60 - 0s - loss: 0.1262 - acc: 1.0000 - val_loss: 0.3900 - val_acc: 0.7500\n",
            "Epoch 121/300\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.32798\n",
            "60/60 - 0s - loss: 0.1425 - acc: 0.9667 - val_loss: 0.3799 - val_acc: 0.7500\n",
            "Epoch 122/300\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.32798\n",
            "60/60 - 0s - loss: 0.1480 - acc: 0.9667 - val_loss: 0.3734 - val_acc: 0.7500\n",
            "Epoch 123/300\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 0.32798\n",
            "60/60 - 0s - loss: 0.1331 - acc: 0.9833 - val_loss: 0.3728 - val_acc: 0.7500\n",
            "Epoch 124/300\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 0.32798\n",
            "60/60 - 0s - loss: 0.1292 - acc: 0.9833 - val_loss: 0.3743 - val_acc: 0.7500\n",
            "Epoch 125/300\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 0.32798\n",
            "60/60 - 0s - loss: 0.1258 - acc: 0.9667 - val_loss: 0.3778 - val_acc: 0.7500\n",
            "Epoch 126/300\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 0.32798\n",
            "60/60 - 0s - loss: 0.1690 - acc: 0.9667 - val_loss: 0.3948 - val_acc: 0.7500\n",
            "Epoch 127/300\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 0.32798\n",
            "60/60 - 0s - loss: 0.1125 - acc: 1.0000 - val_loss: 0.4123 - val_acc: 0.8000\n",
            "Epoch 128/300\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 0.32798\n",
            "60/60 - 0s - loss: 0.1448 - acc: 0.9667 - val_loss: 0.4242 - val_acc: 0.8000\n",
            "Epoch 129/300\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.32798\n",
            "60/60 - 0s - loss: 0.0962 - acc: 1.0000 - val_loss: 0.4371 - val_acc: 0.7500\n",
            "Epoch 130/300\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 0.32798\n",
            "60/60 - 0s - loss: 0.1111 - acc: 1.0000 - val_loss: 0.4305 - val_acc: 0.7500\n",
            "Epoch 131/300\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 0.32798\n",
            "60/60 - 0s - loss: 0.1312 - acc: 0.9833 - val_loss: 0.4082 - val_acc: 0.7000\n",
            "Epoch 132/300\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 0.32798\n",
            "60/60 - 0s - loss: 0.1367 - acc: 0.9667 - val_loss: 0.4108 - val_acc: 0.7000\n",
            "Epoch 133/300\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.32798\n",
            "60/60 - 0s - loss: 0.1602 - acc: 0.9500 - val_loss: 0.4193 - val_acc: 0.7000\n",
            "Epoch 134/300\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.32798\n",
            "60/60 - 0s - loss: 0.1140 - acc: 1.0000 - val_loss: 0.4202 - val_acc: 0.7000\n",
            "Epoch 135/300\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 0.32798\n",
            "60/60 - 0s - loss: 0.1061 - acc: 1.0000 - val_loss: 0.4142 - val_acc: 0.7000\n",
            "Epoch 136/300\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.32798\n",
            "60/60 - 0s - loss: 0.1406 - acc: 0.9667 - val_loss: 0.4058 - val_acc: 0.7000\n",
            "Epoch 137/300\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 0.32798\n",
            "60/60 - 0s - loss: 0.0923 - acc: 1.0000 - val_loss: 0.4012 - val_acc: 0.7000\n",
            "Epoch 138/300\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.32798\n",
            "60/60 - 0s - loss: 0.1579 - acc: 0.9667 - val_loss: 0.3779 - val_acc: 0.7000\n",
            "Epoch 139/300\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.32798\n",
            "60/60 - 0s - loss: 0.1183 - acc: 1.0000 - val_loss: 0.3546 - val_acc: 0.7000\n",
            "Epoch 140/300\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 0.32798\n",
            "60/60 - 0s - loss: 0.1388 - acc: 0.9500 - val_loss: 0.3531 - val_acc: 0.7500\n",
            "Epoch 141/300\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.32798\n",
            "60/60 - 0s - loss: 0.1285 - acc: 0.9833 - val_loss: 0.3553 - val_acc: 0.7000\n",
            "Epoch 142/300\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.32798\n",
            "60/60 - 0s - loss: 0.1536 - acc: 0.9667 - val_loss: 0.3534 - val_acc: 0.8000\n",
            "Epoch 143/300\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 0.32798\n",
            "60/60 - 0s - loss: 0.1116 - acc: 0.9833 - val_loss: 0.3622 - val_acc: 0.7500\n",
            "Epoch 144/300\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 0.32798\n",
            "60/60 - 0s - loss: 0.1141 - acc: 0.9833 - val_loss: 0.3911 - val_acc: 0.7000\n",
            "Epoch 145/300\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.32798\n",
            "60/60 - 0s - loss: 0.1125 - acc: 1.0000 - val_loss: 0.3953 - val_acc: 0.7000\n",
            "Epoch 146/300\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.32798\n",
            "60/60 - 0s - loss: 0.1391 - acc: 0.9667 - val_loss: 0.3839 - val_acc: 0.7000\n",
            "Epoch 147/300\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.32798\n",
            "60/60 - 0s - loss: 0.1485 - acc: 1.0000 - val_loss: 0.3907 - val_acc: 0.7000\n",
            "Epoch 148/300\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 0.32798\n",
            "60/60 - 0s - loss: 0.1320 - acc: 0.9667 - val_loss: 0.4060 - val_acc: 0.7000\n",
            "Epoch 149/300\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.32798\n",
            "60/60 - 0s - loss: 0.1646 - acc: 0.9167 - val_loss: 0.4134 - val_acc: 0.7000\n",
            "Epoch 150/300\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.32798\n",
            "60/60 - 0s - loss: 0.1184 - acc: 0.9833 - val_loss: 0.4081 - val_acc: 0.7000\n",
            "Epoch 151/300\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.32798\n",
            "60/60 - 0s - loss: 0.1556 - acc: 0.9500 - val_loss: 0.3903 - val_acc: 0.7000\n",
            "Epoch 152/300\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.32798\n",
            "60/60 - 0s - loss: 0.1466 - acc: 0.9500 - val_loss: 0.3865 - val_acc: 0.7000\n",
            "Epoch 153/300\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 0.32798\n",
            "60/60 - 0s - loss: 0.1350 - acc: 0.9833 - val_loss: 0.4089 - val_acc: 0.7000\n",
            "Epoch 154/300\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.32798\n",
            "60/60 - 0s - loss: 0.1227 - acc: 0.9833 - val_loss: 0.4249 - val_acc: 0.7500\n",
            "Epoch 155/300\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.32798\n",
            "60/60 - 0s - loss: 0.1273 - acc: 0.9500 - val_loss: 0.4338 - val_acc: 0.7500\n",
            "Epoch 156/300\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.32798\n",
            "60/60 - 0s - loss: 0.1252 - acc: 0.9500 - val_loss: 0.4655 - val_acc: 0.7500\n",
            "Epoch 157/300\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.32798\n",
            "60/60 - 0s - loss: 0.1256 - acc: 0.9833 - val_loss: 0.4672 - val_acc: 0.7500\n",
            "Epoch 158/300\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.32798\n",
            "60/60 - 0s - loss: 0.1443 - acc: 0.9667 - val_loss: 0.4677 - val_acc: 0.7500\n",
            "Epoch 159/300\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.32798\n",
            "60/60 - 0s - loss: 0.1143 - acc: 1.0000 - val_loss: 0.4526 - val_acc: 0.7000\n",
            "Epoch 160/300\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.32798\n",
            "60/60 - 0s - loss: 0.1738 - acc: 0.9500 - val_loss: 0.4416 - val_acc: 0.7500\n",
            "Epoch 161/300\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.32798\n",
            "60/60 - 0s - loss: 0.1025 - acc: 1.0000 - val_loss: 0.4415 - val_acc: 0.7500\n",
            "Epoch 162/300\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.32798\n",
            "60/60 - 0s - loss: 0.1300 - acc: 0.9667 - val_loss: 0.4439 - val_acc: 0.7500\n",
            "Epoch 163/300\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.32798\n",
            "60/60 - 0s - loss: 0.0900 - acc: 0.9833 - val_loss: 0.4465 - val_acc: 0.7500\n",
            "Epoch 164/300\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.32798\n",
            "60/60 - 0s - loss: 0.1088 - acc: 0.9833 - val_loss: 0.4226 - val_acc: 0.7000\n",
            "Epoch 165/300\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.32798\n",
            "60/60 - 0s - loss: 0.0957 - acc: 1.0000 - val_loss: 0.3939 - val_acc: 0.7500\n",
            "Epoch 166/300\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 0.32798\n",
            "60/60 - 0s - loss: 0.1108 - acc: 0.9833 - val_loss: 0.3807 - val_acc: 0.7500\n",
            "Epoch 167/300\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.32798\n",
            "60/60 - 0s - loss: 0.1006 - acc: 0.9833 - val_loss: 0.3879 - val_acc: 0.7000\n",
            "Epoch 168/300\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.32798\n",
            "60/60 - 0s - loss: 0.1292 - acc: 0.9833 - val_loss: 0.4164 - val_acc: 0.7500\n",
            "Epoch 169/300\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 0.32798\n",
            "60/60 - 0s - loss: 0.1041 - acc: 1.0000 - val_loss: 0.4246 - val_acc: 0.7000\n",
            "Epoch 170/300\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.32798\n",
            "60/60 - 0s - loss: 0.1066 - acc: 1.0000 - val_loss: 0.4183 - val_acc: 0.7000\n",
            "Epoch 171/300\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.32798\n",
            "60/60 - 0s - loss: 0.0965 - acc: 1.0000 - val_loss: 0.4182 - val_acc: 0.7000\n",
            "Epoch 172/300\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.32798\n",
            "60/60 - 0s - loss: 0.1157 - acc: 0.9500 - val_loss: 0.4583 - val_acc: 0.7000\n",
            "Epoch 173/300\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.32798\n",
            "60/60 - 0s - loss: 0.1373 - acc: 0.9667 - val_loss: 0.4915 - val_acc: 0.7000\n",
            "Epoch 174/300\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.32798\n",
            "60/60 - 0s - loss: 0.1299 - acc: 1.0000 - val_loss: 0.4818 - val_acc: 0.7000\n",
            "Epoch 175/300\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.32798\n",
            "60/60 - 0s - loss: 0.0976 - acc: 1.0000 - val_loss: 0.4969 - val_acc: 0.7000\n",
            "Epoch 176/300\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 0.32798\n",
            "60/60 - 0s - loss: 0.1276 - acc: 0.9833 - val_loss: 0.4936 - val_acc: 0.7000\n",
            "Epoch 177/300\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.32798\n",
            "60/60 - 0s - loss: 0.0895 - acc: 1.0000 - val_loss: 0.4768 - val_acc: 0.7000\n",
            "Epoch 178/300\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.32798\n",
            "60/60 - 0s - loss: 0.1142 - acc: 1.0000 - val_loss: 0.4475 - val_acc: 0.7000\n",
            "Epoch 179/300\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.32798\n",
            "60/60 - 0s - loss: 0.1044 - acc: 0.9833 - val_loss: 0.4038 - val_acc: 0.7000\n",
            "Epoch 180/300\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.32798\n",
            "60/60 - 0s - loss: 0.1186 - acc: 0.9667 - val_loss: 0.3779 - val_acc: 0.7500\n",
            "Epoch 181/300\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.32798\n",
            "60/60 - 0s - loss: 0.1034 - acc: 1.0000 - val_loss: 0.3841 - val_acc: 0.7500\n",
            "Epoch 182/300\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.32798\n",
            "60/60 - 0s - loss: 0.1173 - acc: 0.9833 - val_loss: 0.3872 - val_acc: 0.7500\n",
            "Epoch 183/300\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.32798\n",
            "60/60 - 0s - loss: 0.1150 - acc: 0.9667 - val_loss: 0.3862 - val_acc: 0.7500\n",
            "Epoch 184/300\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 0.32798\n",
            "60/60 - 0s - loss: 0.0952 - acc: 0.9833 - val_loss: 0.3805 - val_acc: 0.7000\n",
            "Epoch 185/300\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.32798\n",
            "60/60 - 0s - loss: 0.0894 - acc: 0.9833 - val_loss: 0.3803 - val_acc: 0.7000\n",
            "Epoch 186/300\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.32798\n",
            "60/60 - 0s - loss: 0.1205 - acc: 0.9833 - val_loss: 0.3870 - val_acc: 0.7000\n",
            "Epoch 187/300\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 0.32798\n",
            "60/60 - 0s - loss: 0.1194 - acc: 1.0000 - val_loss: 0.4099 - val_acc: 0.7000\n",
            "Epoch 188/300\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.32798\n",
            "60/60 - 0s - loss: 0.1571 - acc: 0.9333 - val_loss: 0.4036 - val_acc: 0.7000\n",
            "Epoch 189/300\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.32798\n",
            "60/60 - 0s - loss: 0.1347 - acc: 0.9833 - val_loss: 0.3892 - val_acc: 0.7500\n",
            "Epoch 190/300\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.32798\n",
            "60/60 - 0s - loss: 0.0997 - acc: 1.0000 - val_loss: 0.3896 - val_acc: 0.7500\n",
            "Epoch 191/300\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.32798\n",
            "60/60 - 0s - loss: 0.0862 - acc: 1.0000 - val_loss: 0.3839 - val_acc: 0.7000\n",
            "Epoch 192/300\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 0.32798\n",
            "60/60 - 0s - loss: 0.0980 - acc: 1.0000 - val_loss: 0.3782 - val_acc: 0.7000\n",
            "Epoch 193/300\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.32798\n",
            "60/60 - 0s - loss: 0.1272 - acc: 1.0000 - val_loss: 0.3714 - val_acc: 0.7000\n",
            "Epoch 194/300\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.32798\n",
            "60/60 - 0s - loss: 0.1185 - acc: 0.9833 - val_loss: 0.3914 - val_acc: 0.7000\n",
            "Epoch 195/300\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.32798\n",
            "60/60 - 0s - loss: 0.0988 - acc: 0.9833 - val_loss: 0.4151 - val_acc: 0.6500\n",
            "Epoch 196/300\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.32798\n",
            "60/60 - 0s - loss: 0.0930 - acc: 0.9833 - val_loss: 0.4238 - val_acc: 0.7000\n",
            "Epoch 197/300\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.32798\n",
            "60/60 - 0s - loss: 0.1000 - acc: 0.9667 - val_loss: 0.4217 - val_acc: 0.7000\n",
            "Epoch 198/300\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.32798\n",
            "60/60 - 0s - loss: 0.0915 - acc: 1.0000 - val_loss: 0.4305 - val_acc: 0.7000\n",
            "Epoch 199/300\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.32798\n",
            "60/60 - 0s - loss: 0.1015 - acc: 0.9833 - val_loss: 0.4446 - val_acc: 0.7000\n",
            "Epoch 200/300\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.32798\n",
            "60/60 - 0s - loss: 0.0965 - acc: 1.0000 - val_loss: 0.4578 - val_acc: 0.7500\n",
            "Epoch 201/300\n",
            "\n",
            "Epoch 00201: val_loss did not improve from 0.32798\n",
            "60/60 - 0s - loss: 0.1319 - acc: 0.9667 - val_loss: 0.4632 - val_acc: 0.7500\n",
            "Epoch 202/300\n",
            "\n",
            "Epoch 00202: val_loss did not improve from 0.32798\n",
            "60/60 - 0s - loss: 0.0913 - acc: 0.9833 - val_loss: 0.4269 - val_acc: 0.7500\n",
            "Epoch 203/300\n",
            "\n",
            "Epoch 00203: val_loss did not improve from 0.32798\n",
            "60/60 - 0s - loss: 0.1023 - acc: 0.9667 - val_loss: 0.3990 - val_acc: 0.7500\n",
            "Epoch 204/300\n",
            "\n",
            "Epoch 00204: val_loss did not improve from 0.32798\n",
            "60/60 - 0s - loss: 0.0958 - acc: 1.0000 - val_loss: 0.3973 - val_acc: 0.7500\n",
            "Epoch 205/300\n",
            "\n",
            "Epoch 00205: val_loss did not improve from 0.32798\n",
            "60/60 - 0s - loss: 0.0908 - acc: 1.0000 - val_loss: 0.4018 - val_acc: 0.7500\n",
            "Epoch 206/300\n",
            "\n",
            "Epoch 00206: val_loss did not improve from 0.32798\n",
            "60/60 - 0s - loss: 0.0850 - acc: 1.0000 - val_loss: 0.4085 - val_acc: 0.7500\n",
            "Epoch 207/300\n",
            "\n",
            "Epoch 00207: val_loss did not improve from 0.32798\n",
            "60/60 - 0s - loss: 0.0941 - acc: 0.9833 - val_loss: 0.3869 - val_acc: 0.7500\n",
            "Epoch 208/300\n",
            "\n",
            "Epoch 00208: val_loss did not improve from 0.32798\n",
            "60/60 - 0s - loss: 0.1185 - acc: 0.9833 - val_loss: 0.3460 - val_acc: 0.7500\n",
            "Epoch 209/300\n",
            "\n",
            "Epoch 00209: val_loss improved from 0.32798 to 0.32189, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.0817 - acc: 0.9833 - val_loss: 0.3219 - val_acc: 0.7500\n",
            "Epoch 210/300\n",
            "\n",
            "Epoch 00210: val_loss improved from 0.32189 to 0.31644, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1087 - acc: 0.9667 - val_loss: 0.3164 - val_acc: 0.8500\n",
            "Epoch 211/300\n",
            "\n",
            "Epoch 00211: val_loss improved from 0.31644 to 0.30552, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.0916 - acc: 1.0000 - val_loss: 0.3055 - val_acc: 0.8500\n",
            "Epoch 212/300\n",
            "\n",
            "Epoch 00212: val_loss did not improve from 0.30552\n",
            "60/60 - 0s - loss: 0.0740 - acc: 0.9833 - val_loss: 0.3079 - val_acc: 0.8500\n",
            "Epoch 213/300\n",
            "\n",
            "Epoch 00213: val_loss did not improve from 0.30552\n",
            "60/60 - 0s - loss: 0.1079 - acc: 0.9500 - val_loss: 0.3192 - val_acc: 0.8000\n",
            "Epoch 214/300\n",
            "\n",
            "Epoch 00214: val_loss did not improve from 0.30552\n",
            "60/60 - 0s - loss: 0.1022 - acc: 0.9833 - val_loss: 0.3236 - val_acc: 0.8000\n",
            "Epoch 215/300\n",
            "\n",
            "Epoch 00215: val_loss did not improve from 0.30552\n",
            "60/60 - 0s - loss: 0.0817 - acc: 1.0000 - val_loss: 0.3261 - val_acc: 0.8500\n",
            "Epoch 216/300\n",
            "\n",
            "Epoch 00216: val_loss did not improve from 0.30552\n",
            "60/60 - 0s - loss: 0.0704 - acc: 1.0000 - val_loss: 0.3311 - val_acc: 0.8000\n",
            "Epoch 217/300\n",
            "\n",
            "Epoch 00217: val_loss did not improve from 0.30552\n",
            "60/60 - 0s - loss: 0.0931 - acc: 0.9833 - val_loss: 0.3345 - val_acc: 0.8000\n",
            "Epoch 218/300\n",
            "\n",
            "Epoch 00218: val_loss did not improve from 0.30552\n",
            "60/60 - 0s - loss: 0.1042 - acc: 0.9833 - val_loss: 0.3324 - val_acc: 0.8000\n",
            "Epoch 219/300\n",
            "\n",
            "Epoch 00219: val_loss did not improve from 0.30552\n",
            "60/60 - 0s - loss: 0.0801 - acc: 1.0000 - val_loss: 0.3390 - val_acc: 0.8000\n",
            "Epoch 220/300\n",
            "\n",
            "Epoch 00220: val_loss did not improve from 0.30552\n",
            "60/60 - 0s - loss: 0.1026 - acc: 0.9667 - val_loss: 0.3587 - val_acc: 0.7500\n",
            "Epoch 221/300\n",
            "\n",
            "Epoch 00221: val_loss did not improve from 0.30552\n",
            "60/60 - 0s - loss: 0.1144 - acc: 0.9833 - val_loss: 0.3811 - val_acc: 0.6500\n",
            "Epoch 222/300\n",
            "\n",
            "Epoch 00222: val_loss did not improve from 0.30552\n",
            "60/60 - 0s - loss: 0.1020 - acc: 0.9667 - val_loss: 0.4015 - val_acc: 0.6500\n",
            "Epoch 223/300\n",
            "\n",
            "Epoch 00223: val_loss did not improve from 0.30552\n",
            "60/60 - 0s - loss: 0.1334 - acc: 0.9667 - val_loss: 0.4225 - val_acc: 0.6500\n",
            "Epoch 224/300\n",
            "\n",
            "Epoch 00224: val_loss did not improve from 0.30552\n",
            "60/60 - 0s - loss: 0.0943 - acc: 1.0000 - val_loss: 0.4096 - val_acc: 0.7000\n",
            "Epoch 225/300\n",
            "\n",
            "Epoch 00225: val_loss did not improve from 0.30552\n",
            "60/60 - 0s - loss: 0.0809 - acc: 1.0000 - val_loss: 0.4068 - val_acc: 0.7000\n",
            "Epoch 226/300\n",
            "\n",
            "Epoch 00226: val_loss did not improve from 0.30552\n",
            "60/60 - 0s - loss: 0.0964 - acc: 0.9833 - val_loss: 0.4008 - val_acc: 0.7000\n",
            "Epoch 227/300\n",
            "\n",
            "Epoch 00227: val_loss did not improve from 0.30552\n",
            "60/60 - 0s - loss: 0.1288 - acc: 0.9667 - val_loss: 0.3863 - val_acc: 0.7000\n",
            "Epoch 228/300\n",
            "\n",
            "Epoch 00228: val_loss did not improve from 0.30552\n",
            "60/60 - 0s - loss: 0.0877 - acc: 1.0000 - val_loss: 0.3913 - val_acc: 0.7000\n",
            "Epoch 229/300\n",
            "\n",
            "Epoch 00229: val_loss did not improve from 0.30552\n",
            "60/60 - 0s - loss: 0.1159 - acc: 0.9833 - val_loss: 0.4172 - val_acc: 0.7000\n",
            "Epoch 230/300\n",
            "\n",
            "Epoch 00230: val_loss did not improve from 0.30552\n",
            "60/60 - 0s - loss: 0.0715 - acc: 1.0000 - val_loss: 0.4095 - val_acc: 0.7000\n",
            "Epoch 231/300\n",
            "\n",
            "Epoch 00231: val_loss did not improve from 0.30552\n",
            "60/60 - 0s - loss: 0.0736 - acc: 1.0000 - val_loss: 0.3968 - val_acc: 0.7000\n",
            "Epoch 232/300\n",
            "\n",
            "Epoch 00232: val_loss did not improve from 0.30552\n",
            "60/60 - 0s - loss: 0.0772 - acc: 1.0000 - val_loss: 0.4016 - val_acc: 0.7000\n",
            "Epoch 233/300\n",
            "\n",
            "Epoch 00233: val_loss did not improve from 0.30552\n",
            "60/60 - 0s - loss: 0.1037 - acc: 0.9833 - val_loss: 0.3996 - val_acc: 0.7000\n",
            "Epoch 234/300\n",
            "\n",
            "Epoch 00234: val_loss did not improve from 0.30552\n",
            "60/60 - 0s - loss: 0.0636 - acc: 1.0000 - val_loss: 0.3939 - val_acc: 0.7000\n",
            "Epoch 235/300\n",
            "\n",
            "Epoch 00235: val_loss did not improve from 0.30552\n",
            "60/60 - 0s - loss: 0.0985 - acc: 0.9833 - val_loss: 0.3827 - val_acc: 0.7000\n",
            "Epoch 236/300\n",
            "\n",
            "Epoch 00236: val_loss did not improve from 0.30552\n",
            "60/60 - 0s - loss: 0.0643 - acc: 1.0000 - val_loss: 0.3847 - val_acc: 0.7000\n",
            "Epoch 237/300\n",
            "\n",
            "Epoch 00237: val_loss did not improve from 0.30552\n",
            "60/60 - 0s - loss: 0.1087 - acc: 0.9833 - val_loss: 0.3923 - val_acc: 0.7000\n",
            "Epoch 238/300\n",
            "\n",
            "Epoch 00238: val_loss did not improve from 0.30552\n",
            "60/60 - 0s - loss: 0.0811 - acc: 0.9667 - val_loss: 0.4020 - val_acc: 0.7000\n",
            "Epoch 239/300\n",
            "\n",
            "Epoch 00239: val_loss did not improve from 0.30552\n",
            "60/60 - 0s - loss: 0.0944 - acc: 1.0000 - val_loss: 0.4055 - val_acc: 0.7000\n",
            "Epoch 240/300\n",
            "\n",
            "Epoch 00240: val_loss did not improve from 0.30552\n",
            "60/60 - 0s - loss: 0.0831 - acc: 1.0000 - val_loss: 0.4202 - val_acc: 0.7000\n",
            "Epoch 241/300\n",
            "\n",
            "Epoch 00241: val_loss did not improve from 0.30552\n",
            "60/60 - 0s - loss: 0.0872 - acc: 0.9667 - val_loss: 0.4288 - val_acc: 0.7000\n",
            "Epoch 242/300\n",
            "\n",
            "Epoch 00242: val_loss did not improve from 0.30552\n",
            "60/60 - 0s - loss: 0.0706 - acc: 1.0000 - val_loss: 0.4399 - val_acc: 0.7000\n",
            "Epoch 243/300\n",
            "\n",
            "Epoch 00243: val_loss did not improve from 0.30552\n",
            "60/60 - 0s - loss: 0.0790 - acc: 1.0000 - val_loss: 0.4529 - val_acc: 0.7000\n",
            "Epoch 244/300\n",
            "\n",
            "Epoch 00244: val_loss did not improve from 0.30552\n",
            "60/60 - 0s - loss: 0.0827 - acc: 1.0000 - val_loss: 0.4749 - val_acc: 0.7000\n",
            "Epoch 245/300\n",
            "\n",
            "Epoch 00245: val_loss did not improve from 0.30552\n",
            "60/60 - 0s - loss: 0.0772 - acc: 1.0000 - val_loss: 0.4961 - val_acc: 0.7000\n",
            "Epoch 246/300\n",
            "\n",
            "Epoch 00246: val_loss did not improve from 0.30552\n",
            "60/60 - 0s - loss: 0.0738 - acc: 0.9667 - val_loss: 0.5199 - val_acc: 0.7000\n",
            "Epoch 247/300\n",
            "\n",
            "Epoch 00247: val_loss did not improve from 0.30552\n",
            "60/60 - 0s - loss: 0.0863 - acc: 0.9833 - val_loss: 0.5174 - val_acc: 0.7000\n",
            "Epoch 248/300\n",
            "\n",
            "Epoch 00248: val_loss did not improve from 0.30552\n",
            "60/60 - 0s - loss: 0.0789 - acc: 1.0000 - val_loss: 0.5105 - val_acc: 0.7000\n",
            "Epoch 249/300\n",
            "\n",
            "Epoch 00249: val_loss did not improve from 0.30552\n",
            "60/60 - 0s - loss: 0.1095 - acc: 1.0000 - val_loss: 0.5156 - val_acc: 0.7000\n",
            "Epoch 250/300\n",
            "\n",
            "Epoch 00250: val_loss did not improve from 0.30552\n",
            "60/60 - 0s - loss: 0.0680 - acc: 1.0000 - val_loss: 0.5230 - val_acc: 0.7000\n",
            "Epoch 251/300\n",
            "\n",
            "Epoch 00251: val_loss did not improve from 0.30552\n",
            "60/60 - 0s - loss: 0.0786 - acc: 0.9833 - val_loss: 0.5165 - val_acc: 0.7000\n",
            "Epoch 252/300\n",
            "\n",
            "Epoch 00252: val_loss did not improve from 0.30552\n",
            "60/60 - 0s - loss: 0.1240 - acc: 0.9667 - val_loss: 0.4808 - val_acc: 0.7000\n",
            "Epoch 253/300\n",
            "\n",
            "Epoch 00253: val_loss did not improve from 0.30552\n",
            "60/60 - 0s - loss: 0.0747 - acc: 1.0000 - val_loss: 0.4466 - val_acc: 0.7000\n",
            "Epoch 254/300\n",
            "\n",
            "Epoch 00254: val_loss did not improve from 0.30552\n",
            "60/60 - 0s - loss: 0.0861 - acc: 0.9833 - val_loss: 0.4365 - val_acc: 0.7000\n",
            "Epoch 255/300\n",
            "\n",
            "Epoch 00255: val_loss did not improve from 0.30552\n",
            "60/60 - 0s - loss: 0.0734 - acc: 1.0000 - val_loss: 0.4330 - val_acc: 0.7000\n",
            "Epoch 256/300\n",
            "\n",
            "Epoch 00256: val_loss did not improve from 0.30552\n",
            "60/60 - 0s - loss: 0.1115 - acc: 0.9833 - val_loss: 0.4201 - val_acc: 0.7000\n",
            "Epoch 257/300\n",
            "\n",
            "Epoch 00257: val_loss did not improve from 0.30552\n",
            "60/60 - 0s - loss: 0.0746 - acc: 1.0000 - val_loss: 0.4272 - val_acc: 0.7000\n",
            "Epoch 258/300\n",
            "\n",
            "Epoch 00258: val_loss did not improve from 0.30552\n",
            "60/60 - 0s - loss: 0.0629 - acc: 1.0000 - val_loss: 0.4208 - val_acc: 0.7000\n",
            "Epoch 259/300\n",
            "\n",
            "Epoch 00259: val_loss did not improve from 0.30552\n",
            "60/60 - 0s - loss: 0.0657 - acc: 1.0000 - val_loss: 0.4209 - val_acc: 0.7000\n",
            "Epoch 260/300\n",
            "\n",
            "Epoch 00260: val_loss did not improve from 0.30552\n",
            "60/60 - 0s - loss: 0.0753 - acc: 0.9667 - val_loss: 0.4237 - val_acc: 0.7000\n",
            "Epoch 261/300\n",
            "\n",
            "Epoch 00261: val_loss did not improve from 0.30552\n",
            "60/60 - 0s - loss: 0.0963 - acc: 0.9833 - val_loss: 0.4414 - val_acc: 0.7000\n",
            "Epoch 262/300\n",
            "\n",
            "Epoch 00262: val_loss did not improve from 0.30552\n",
            "60/60 - 0s - loss: 0.0623 - acc: 1.0000 - val_loss: 0.4614 - val_acc: 0.7000\n",
            "Epoch 263/300\n",
            "\n",
            "Epoch 00263: val_loss did not improve from 0.30552\n",
            "60/60 - 0s - loss: 0.0708 - acc: 1.0000 - val_loss: 0.4681 - val_acc: 0.7000\n",
            "Epoch 264/300\n",
            "\n",
            "Epoch 00264: val_loss did not improve from 0.30552\n",
            "60/60 - 0s - loss: 0.0715 - acc: 0.9833 - val_loss: 0.4889 - val_acc: 0.7000\n",
            "Epoch 265/300\n",
            "\n",
            "Epoch 00265: val_loss did not improve from 0.30552\n",
            "60/60 - 0s - loss: 0.0867 - acc: 1.0000 - val_loss: 0.4990 - val_acc: 0.7000\n",
            "Epoch 266/300\n",
            "\n",
            "Epoch 00266: val_loss did not improve from 0.30552\n",
            "60/60 - 0s - loss: 0.0823 - acc: 0.9833 - val_loss: 0.5015 - val_acc: 0.7500\n",
            "Epoch 267/300\n",
            "\n",
            "Epoch 00267: val_loss did not improve from 0.30552\n",
            "60/60 - 0s - loss: 0.0926 - acc: 0.9667 - val_loss: 0.5131 - val_acc: 0.7000\n",
            "Epoch 268/300\n",
            "\n",
            "Epoch 00268: val_loss did not improve from 0.30552\n",
            "60/60 - 0s - loss: 0.0758 - acc: 1.0000 - val_loss: 0.5271 - val_acc: 0.7000\n",
            "Epoch 269/300\n",
            "\n",
            "Epoch 00269: val_loss did not improve from 0.30552\n",
            "60/60 - 0s - loss: 0.0730 - acc: 1.0000 - val_loss: 0.5257 - val_acc: 0.7000\n",
            "Epoch 270/300\n",
            "\n",
            "Epoch 00270: val_loss did not improve from 0.30552\n",
            "60/60 - 0s - loss: 0.0797 - acc: 1.0000 - val_loss: 0.5028 - val_acc: 0.7000\n",
            "Epoch 271/300\n",
            "\n",
            "Epoch 00271: val_loss did not improve from 0.30552\n",
            "60/60 - 0s - loss: 0.0746 - acc: 1.0000 - val_loss: 0.4889 - val_acc: 0.7000\n",
            "Epoch 272/300\n",
            "\n",
            "Epoch 00272: val_loss did not improve from 0.30552\n",
            "60/60 - 0s - loss: 0.0793 - acc: 0.9833 - val_loss: 0.4881 - val_acc: 0.7000\n",
            "Epoch 273/300\n",
            "\n",
            "Epoch 00273: val_loss did not improve from 0.30552\n",
            "60/60 - 0s - loss: 0.0810 - acc: 0.9667 - val_loss: 0.4928 - val_acc: 0.7000\n",
            "Epoch 274/300\n",
            "\n",
            "Epoch 00274: val_loss did not improve from 0.30552\n",
            "60/60 - 0s - loss: 0.0589 - acc: 1.0000 - val_loss: 0.4932 - val_acc: 0.7000\n",
            "Epoch 275/300\n",
            "\n",
            "Epoch 00275: val_loss did not improve from 0.30552\n",
            "60/60 - 0s - loss: 0.0917 - acc: 1.0000 - val_loss: 0.4905 - val_acc: 0.7500\n",
            "Epoch 276/300\n",
            "\n",
            "Epoch 00276: val_loss did not improve from 0.30552\n",
            "60/60 - 0s - loss: 0.0689 - acc: 1.0000 - val_loss: 0.4885 - val_acc: 0.7500\n",
            "Epoch 277/300\n",
            "\n",
            "Epoch 00277: val_loss did not improve from 0.30552\n",
            "60/60 - 0s - loss: 0.0726 - acc: 1.0000 - val_loss: 0.4890 - val_acc: 0.8000\n",
            "Epoch 278/300\n",
            "\n",
            "Epoch 00278: val_loss did not improve from 0.30552\n",
            "60/60 - 0s - loss: 0.0533 - acc: 1.0000 - val_loss: 0.4869 - val_acc: 0.8000\n",
            "Epoch 279/300\n",
            "\n",
            "Epoch 00279: val_loss did not improve from 0.30552\n",
            "60/60 - 0s - loss: 0.0488 - acc: 1.0000 - val_loss: 0.4808 - val_acc: 0.8000\n",
            "Epoch 280/300\n",
            "\n",
            "Epoch 00280: val_loss did not improve from 0.30552\n",
            "60/60 - 0s - loss: 0.0694 - acc: 1.0000 - val_loss: 0.4844 - val_acc: 0.8000\n",
            "Epoch 281/300\n",
            "\n",
            "Epoch 00281: val_loss did not improve from 0.30552\n",
            "60/60 - 0s - loss: 0.0812 - acc: 0.9833 - val_loss: 0.4844 - val_acc: 0.8000\n",
            "Epoch 282/300\n",
            "\n",
            "Epoch 00282: val_loss did not improve from 0.30552\n",
            "60/60 - 0s - loss: 0.1015 - acc: 0.9667 - val_loss: 0.4521 - val_acc: 0.7500\n",
            "Epoch 283/300\n",
            "\n",
            "Epoch 00283: val_loss did not improve from 0.30552\n",
            "60/60 - 0s - loss: 0.0480 - acc: 1.0000 - val_loss: 0.4511 - val_acc: 0.7000\n",
            "Epoch 284/300\n",
            "\n",
            "Epoch 00284: val_loss did not improve from 0.30552\n",
            "60/60 - 0s - loss: 0.1292 - acc: 0.9833 - val_loss: 0.4428 - val_acc: 0.7000\n",
            "Epoch 285/300\n",
            "\n",
            "Epoch 00285: val_loss did not improve from 0.30552\n",
            "60/60 - 0s - loss: 0.0802 - acc: 1.0000 - val_loss: 0.4277 - val_acc: 0.7500\n",
            "Epoch 286/300\n",
            "\n",
            "Epoch 00286: val_loss did not improve from 0.30552\n",
            "60/60 - 0s - loss: 0.0638 - acc: 1.0000 - val_loss: 0.4272 - val_acc: 0.7000\n",
            "Epoch 287/300\n",
            "\n",
            "Epoch 00287: val_loss did not improve from 0.30552\n",
            "60/60 - 0s - loss: 0.0840 - acc: 1.0000 - val_loss: 0.4718 - val_acc: 0.7000\n",
            "Epoch 288/300\n",
            "\n",
            "Epoch 00288: val_loss did not improve from 0.30552\n",
            "60/60 - 0s - loss: 0.0751 - acc: 0.9833 - val_loss: 0.5388 - val_acc: 0.7000\n",
            "Epoch 289/300\n",
            "\n",
            "Epoch 00289: val_loss did not improve from 0.30552\n",
            "60/60 - 0s - loss: 0.0743 - acc: 1.0000 - val_loss: 0.6395 - val_acc: 0.6500\n",
            "Epoch 290/300\n",
            "\n",
            "Epoch 00290: val_loss did not improve from 0.30552\n",
            "60/60 - 0s - loss: 0.0651 - acc: 1.0000 - val_loss: 0.6524 - val_acc: 0.6500\n",
            "Epoch 291/300\n",
            "\n",
            "Epoch 00291: val_loss did not improve from 0.30552\n",
            "60/60 - 0s - loss: 0.0576 - acc: 1.0000 - val_loss: 0.6374 - val_acc: 0.7000\n",
            "Epoch 292/300\n",
            "\n",
            "Epoch 00292: val_loss did not improve from 0.30552\n",
            "60/60 - 0s - loss: 0.0513 - acc: 1.0000 - val_loss: 0.6144 - val_acc: 0.7000\n",
            "Epoch 293/300\n",
            "\n",
            "Epoch 00293: val_loss did not improve from 0.30552\n",
            "60/60 - 0s - loss: 0.0590 - acc: 0.9833 - val_loss: 0.6290 - val_acc: 0.7000\n",
            "Epoch 294/300\n",
            "\n",
            "Epoch 00294: val_loss did not improve from 0.30552\n",
            "60/60 - 0s - loss: 0.0675 - acc: 1.0000 - val_loss: 0.6502 - val_acc: 0.7000\n",
            "Epoch 295/300\n",
            "\n",
            "Epoch 00295: val_loss did not improve from 0.30552\n",
            "60/60 - 0s - loss: 0.0782 - acc: 1.0000 - val_loss: 0.6362 - val_acc: 0.7000\n",
            "Epoch 296/300\n",
            "\n",
            "Epoch 00296: val_loss did not improve from 0.30552\n",
            "60/60 - 0s - loss: 0.0674 - acc: 0.9833 - val_loss: 0.6465 - val_acc: 0.6500\n",
            "Epoch 297/300\n",
            "\n",
            "Epoch 00297: val_loss did not improve from 0.30552\n",
            "60/60 - 0s - loss: 0.0629 - acc: 0.9833 - val_loss: 0.6570 - val_acc: 0.6500\n",
            "Epoch 298/300\n",
            "\n",
            "Epoch 00298: val_loss did not improve from 0.30552\n",
            "60/60 - 0s - loss: 0.0524 - acc: 1.0000 - val_loss: 0.6765 - val_acc: 0.6500\n",
            "Epoch 299/300\n",
            "\n",
            "Epoch 00299: val_loss did not improve from 0.30552\n",
            "60/60 - 0s - loss: 0.0648 - acc: 1.0000 - val_loss: 0.6613 - val_acc: 0.7000\n",
            "Epoch 300/300\n",
            "\n",
            "Epoch 00300: val_loss did not improve from 0.30552\n",
            "60/60 - 0s - loss: 0.0658 - acc: 1.0000 - val_loss: 0.6099 - val_acc: 0.7000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "# Evaluate on test data\n",
            "40/40 [==============================] - 0s 2ms/sample - loss: 0.7457 - acc: 0.6750\n",
            "test loss, test acc: [0.7457377318758518, 0.675]\n",
            "EEG_Deep/Data2A/parsed_P03T.mat\n",
            "Filtering of Training Data Finished\n",
            "EEG_Deep/Data2A/parsed_P03E.mat\n",
            "Filtering of Testing Data Finished\n",
            "(1, 2)\n",
            "Finding labels in training data\n",
            "Finding labels in testing data\n",
            "[1 2 2 2 1 2 1 2 1 2 1 2 2 1 2 2 2 2 2 2]\n",
            "(60, 12, 1536)\n",
            "(60,)\n",
            "(20, 12, 1536)\n",
            "(20,)\n",
            "[[1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]]\n",
            "X_train shape: (60, 1, 12, 1536)\n",
            "60 train samples\n",
            "20 val samples\n",
            "(40, 12, 1536)\n",
            "(40, 1)\n",
            "X_train shape: (40, 1, 12, 1536)\n",
            "40 train samples\n",
            "Train on 60 samples, validate on 20 samples\n",
            "Epoch 1/300\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.68664, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 1s - loss: 0.7228 - acc: 0.4833 - val_loss: 0.6866 - val_acc: 0.6000\n",
            "Epoch 2/300\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.68664\n",
            "60/60 - 0s - loss: 0.6901 - acc: 0.4833 - val_loss: 0.6870 - val_acc: 0.3500\n",
            "Epoch 3/300\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.68664\n",
            "60/60 - 0s - loss: 0.6649 - acc: 0.5500 - val_loss: 0.6891 - val_acc: 0.4500\n",
            "Epoch 4/300\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.68664\n",
            "60/60 - 0s - loss: 0.6517 - acc: 0.6000 - val_loss: 0.6923 - val_acc: 0.4500\n",
            "Epoch 5/300\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.68664\n",
            "60/60 - 0s - loss: 0.6421 - acc: 0.5667 - val_loss: 0.6944 - val_acc: 0.3500\n",
            "Epoch 6/300\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.68664\n",
            "60/60 - 0s - loss: 0.6261 - acc: 0.7167 - val_loss: 0.6964 - val_acc: 0.3000\n",
            "Epoch 7/300\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.68664\n",
            "60/60 - 0s - loss: 0.6153 - acc: 0.6667 - val_loss: 0.6982 - val_acc: 0.3000\n",
            "Epoch 8/300\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.68664\n",
            "60/60 - 0s - loss: 0.6061 - acc: 0.6833 - val_loss: 0.7002 - val_acc: 0.3000\n",
            "Epoch 9/300\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.68664\n",
            "60/60 - 0s - loss: 0.5908 - acc: 0.7667 - val_loss: 0.7007 - val_acc: 0.3500\n",
            "Epoch 10/300\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.68664\n",
            "60/60 - 0s - loss: 0.6078 - acc: 0.7000 - val_loss: 0.7015 - val_acc: 0.3500\n",
            "Epoch 11/300\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.68664\n",
            "60/60 - 0s - loss: 0.6014 - acc: 0.6833 - val_loss: 0.7020 - val_acc: 0.3500\n",
            "Epoch 12/300\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.68664\n",
            "60/60 - 0s - loss: 0.5808 - acc: 0.7500 - val_loss: 0.7026 - val_acc: 0.3500\n",
            "Epoch 13/300\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.68664\n",
            "60/60 - 0s - loss: 0.5899 - acc: 0.7500 - val_loss: 0.7039 - val_acc: 0.3500\n",
            "Epoch 14/300\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.68664\n",
            "60/60 - 0s - loss: 0.5807 - acc: 0.7500 - val_loss: 0.7035 - val_acc: 0.3500\n",
            "Epoch 15/300\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.68664\n",
            "60/60 - 0s - loss: 0.5859 - acc: 0.7167 - val_loss: 0.7040 - val_acc: 0.3500\n",
            "Epoch 16/300\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.68664\n",
            "60/60 - 0s - loss: 0.5553 - acc: 0.8167 - val_loss: 0.7052 - val_acc: 0.3000\n",
            "Epoch 17/300\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.68664\n",
            "60/60 - 0s - loss: 0.5734 - acc: 0.8000 - val_loss: 0.7059 - val_acc: 0.3000\n",
            "Epoch 18/300\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.68664\n",
            "60/60 - 0s - loss: 0.5791 - acc: 0.7333 - val_loss: 0.7046 - val_acc: 0.3500\n",
            "Epoch 19/300\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.68664\n",
            "60/60 - 0s - loss: 0.5591 - acc: 0.8167 - val_loss: 0.7051 - val_acc: 0.3500\n",
            "Epoch 20/300\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.68664\n",
            "60/60 - 0s - loss: 0.5433 - acc: 0.8500 - val_loss: 0.7070 - val_acc: 0.3500\n",
            "Epoch 21/300\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.68664\n",
            "60/60 - 0s - loss: 0.5238 - acc: 0.9167 - val_loss: 0.7093 - val_acc: 0.3500\n",
            "Epoch 22/300\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.68664\n",
            "60/60 - 0s - loss: 0.5522 - acc: 0.8667 - val_loss: 0.7075 - val_acc: 0.3500\n",
            "Epoch 23/300\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.68664\n",
            "60/60 - 0s - loss: 0.5325 - acc: 0.8833 - val_loss: 0.7045 - val_acc: 0.3500\n",
            "Epoch 24/300\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.68664\n",
            "60/60 - 0s - loss: 0.5357 - acc: 0.8667 - val_loss: 0.7013 - val_acc: 0.3500\n",
            "Epoch 25/300\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.68664\n",
            "60/60 - 0s - loss: 0.5416 - acc: 0.8667 - val_loss: 0.6997 - val_acc: 0.4000\n",
            "Epoch 26/300\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.68664\n",
            "60/60 - 0s - loss: 0.5166 - acc: 0.8667 - val_loss: 0.6983 - val_acc: 0.4500\n",
            "Epoch 27/300\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.68664\n",
            "60/60 - 0s - loss: 0.5124 - acc: 0.9000 - val_loss: 0.6981 - val_acc: 0.4500\n",
            "Epoch 28/300\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.68664\n",
            "60/60 - 0s - loss: 0.4831 - acc: 0.9333 - val_loss: 0.6964 - val_acc: 0.5000\n",
            "Epoch 29/300\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.68664\n",
            "60/60 - 0s - loss: 0.5107 - acc: 0.9333 - val_loss: 0.6958 - val_acc: 0.5500\n",
            "Epoch 30/300\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.68664\n",
            "60/60 - 0s - loss: 0.5137 - acc: 0.9000 - val_loss: 0.6942 - val_acc: 0.5500\n",
            "Epoch 31/300\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.68664\n",
            "60/60 - 0s - loss: 0.4796 - acc: 0.9333 - val_loss: 0.6932 - val_acc: 0.5500\n",
            "Epoch 32/300\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.68664\n",
            "60/60 - 0s - loss: 0.4907 - acc: 0.9333 - val_loss: 0.6931 - val_acc: 0.5500\n",
            "Epoch 33/300\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.68664\n",
            "60/60 - 0s - loss: 0.4896 - acc: 0.9000 - val_loss: 0.6918 - val_acc: 0.6000\n",
            "Epoch 34/300\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.68664\n",
            "60/60 - 0s - loss: 0.4754 - acc: 0.9333 - val_loss: 0.6917 - val_acc: 0.6000\n",
            "Epoch 35/300\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.68664\n",
            "60/60 - 0s - loss: 0.4614 - acc: 0.9333 - val_loss: 0.6909 - val_acc: 0.6000\n",
            "Epoch 36/300\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.68664\n",
            "60/60 - 0s - loss: 0.4811 - acc: 0.9167 - val_loss: 0.6896 - val_acc: 0.5500\n",
            "Epoch 37/300\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.68664\n",
            "60/60 - 0s - loss: 0.4525 - acc: 0.9167 - val_loss: 0.6895 - val_acc: 0.5500\n",
            "Epoch 38/300\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.68664\n",
            "60/60 - 0s - loss: 0.4699 - acc: 0.8833 - val_loss: 0.6880 - val_acc: 0.5500\n",
            "Epoch 39/300\n",
            "\n",
            "Epoch 00039: val_loss improved from 0.68664 to 0.68559, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4797 - acc: 0.9167 - val_loss: 0.6856 - val_acc: 0.5500\n",
            "Epoch 40/300\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.68559\n",
            "60/60 - 0s - loss: 0.4518 - acc: 0.9667 - val_loss: 0.6867 - val_acc: 0.5500\n",
            "Epoch 41/300\n",
            "\n",
            "Epoch 00041: val_loss improved from 0.68559 to 0.68232, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4565 - acc: 0.9333 - val_loss: 0.6823 - val_acc: 0.5000\n",
            "Epoch 42/300\n",
            "\n",
            "Epoch 00042: val_loss improved from 0.68232 to 0.67504, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4474 - acc: 0.9667 - val_loss: 0.6750 - val_acc: 0.5000\n",
            "Epoch 43/300\n",
            "\n",
            "Epoch 00043: val_loss improved from 0.67504 to 0.66713, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4707 - acc: 0.8833 - val_loss: 0.6671 - val_acc: 0.7500\n",
            "Epoch 44/300\n",
            "\n",
            "Epoch 00044: val_loss improved from 0.66713 to 0.66365, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4544 - acc: 0.9167 - val_loss: 0.6636 - val_acc: 0.7500\n",
            "Epoch 45/300\n",
            "\n",
            "Epoch 00045: val_loss improved from 0.66365 to 0.65955, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4542 - acc: 0.9000 - val_loss: 0.6595 - val_acc: 0.7500\n",
            "Epoch 46/300\n",
            "\n",
            "Epoch 00046: val_loss improved from 0.65955 to 0.65570, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4404 - acc: 0.9167 - val_loss: 0.6557 - val_acc: 0.6500\n",
            "Epoch 47/300\n",
            "\n",
            "Epoch 00047: val_loss improved from 0.65570 to 0.64846, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4105 - acc: 0.9500 - val_loss: 0.6485 - val_acc: 0.7000\n",
            "Epoch 48/300\n",
            "\n",
            "Epoch 00048: val_loss improved from 0.64846 to 0.64121, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4202 - acc: 0.9000 - val_loss: 0.6412 - val_acc: 0.7000\n",
            "Epoch 49/300\n",
            "\n",
            "Epoch 00049: val_loss improved from 0.64121 to 0.63489, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4458 - acc: 0.9333 - val_loss: 0.6349 - val_acc: 0.7000\n",
            "Epoch 50/300\n",
            "\n",
            "Epoch 00050: val_loss improved from 0.63489 to 0.63300, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3985 - acc: 0.9833 - val_loss: 0.6330 - val_acc: 0.7000\n",
            "Epoch 51/300\n",
            "\n",
            "Epoch 00051: val_loss improved from 0.63300 to 0.62981, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4179 - acc: 0.9333 - val_loss: 0.6298 - val_acc: 0.7000\n",
            "Epoch 52/300\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.62981\n",
            "60/60 - 0s - loss: 0.4162 - acc: 0.9333 - val_loss: 0.6319 - val_acc: 0.7000\n",
            "Epoch 53/300\n",
            "\n",
            "Epoch 00053: val_loss improved from 0.62981 to 0.62834, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3924 - acc: 0.9167 - val_loss: 0.6283 - val_acc: 0.7000\n",
            "Epoch 54/300\n",
            "\n",
            "Epoch 00054: val_loss improved from 0.62834 to 0.61735, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4212 - acc: 0.9167 - val_loss: 0.6174 - val_acc: 0.7000\n",
            "Epoch 55/300\n",
            "\n",
            "Epoch 00055: val_loss improved from 0.61735 to 0.60734, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4092 - acc: 0.9333 - val_loss: 0.6073 - val_acc: 0.7000\n",
            "Epoch 56/300\n",
            "\n",
            "Epoch 00056: val_loss improved from 0.60734 to 0.60316, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4091 - acc: 0.9333 - val_loss: 0.6032 - val_acc: 0.7000\n",
            "Epoch 57/300\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.60316\n",
            "60/60 - 0s - loss: 0.3672 - acc: 0.9500 - val_loss: 0.6052 - val_acc: 0.7000\n",
            "Epoch 58/300\n",
            "\n",
            "Epoch 00058: val_loss improved from 0.60316 to 0.60164, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4111 - acc: 0.9167 - val_loss: 0.6016 - val_acc: 0.7000\n",
            "Epoch 59/300\n",
            "\n",
            "Epoch 00059: val_loss improved from 0.60164 to 0.59431, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3984 - acc: 0.9333 - val_loss: 0.5943 - val_acc: 0.7000\n",
            "Epoch 60/300\n",
            "\n",
            "Epoch 00060: val_loss improved from 0.59431 to 0.58673, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3554 - acc: 1.0000 - val_loss: 0.5867 - val_acc: 0.7000\n",
            "Epoch 61/300\n",
            "\n",
            "Epoch 00061: val_loss improved from 0.58673 to 0.57846, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3854 - acc: 0.9167 - val_loss: 0.5785 - val_acc: 0.7000\n",
            "Epoch 62/300\n",
            "\n",
            "Epoch 00062: val_loss improved from 0.57846 to 0.57676, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3953 - acc: 0.8833 - val_loss: 0.5768 - val_acc: 0.7000\n",
            "Epoch 63/300\n",
            "\n",
            "Epoch 00063: val_loss improved from 0.57676 to 0.56773, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3826 - acc: 0.9000 - val_loss: 0.5677 - val_acc: 0.7000\n",
            "Epoch 64/300\n",
            "\n",
            "Epoch 00064: val_loss improved from 0.56773 to 0.56360, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3920 - acc: 0.9500 - val_loss: 0.5636 - val_acc: 0.7000\n",
            "Epoch 65/300\n",
            "\n",
            "Epoch 00065: val_loss improved from 0.56360 to 0.55918, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3681 - acc: 0.9500 - val_loss: 0.5592 - val_acc: 0.7000\n",
            "Epoch 66/300\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.55918\n",
            "60/60 - 0s - loss: 0.3454 - acc: 0.9833 - val_loss: 0.5638 - val_acc: 0.7000\n",
            "Epoch 67/300\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.55918\n",
            "60/60 - 0s - loss: 0.3899 - acc: 0.9500 - val_loss: 0.5597 - val_acc: 0.7000\n",
            "Epoch 68/300\n",
            "\n",
            "Epoch 00068: val_loss improved from 0.55918 to 0.54943, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3613 - acc: 0.9500 - val_loss: 0.5494 - val_acc: 0.7000\n",
            "Epoch 69/300\n",
            "\n",
            "Epoch 00069: val_loss improved from 0.54943 to 0.53600, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3549 - acc: 0.9833 - val_loss: 0.5360 - val_acc: 0.7500\n",
            "Epoch 70/300\n",
            "\n",
            "Epoch 00070: val_loss improved from 0.53600 to 0.52814, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3159 - acc: 1.0000 - val_loss: 0.5281 - val_acc: 0.7500\n",
            "Epoch 71/300\n",
            "\n",
            "Epoch 00071: val_loss improved from 0.52814 to 0.52000, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3507 - acc: 0.9500 - val_loss: 0.5200 - val_acc: 0.8000\n",
            "Epoch 72/300\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.52000\n",
            "60/60 - 0s - loss: 0.3585 - acc: 0.9333 - val_loss: 0.5337 - val_acc: 0.7000\n",
            "Epoch 73/300\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.52000\n",
            "60/60 - 0s - loss: 0.3571 - acc: 0.9167 - val_loss: 0.5332 - val_acc: 0.7000\n",
            "Epoch 74/300\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.52000\n",
            "60/60 - 0s - loss: 0.3459 - acc: 0.9333 - val_loss: 0.5397 - val_acc: 0.7000\n",
            "Epoch 75/300\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.52000\n",
            "60/60 - 0s - loss: 0.3544 - acc: 0.9333 - val_loss: 0.5377 - val_acc: 0.7000\n",
            "Epoch 76/300\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.52000\n",
            "60/60 - 0s - loss: 0.3610 - acc: 0.8500 - val_loss: 0.5371 - val_acc: 0.7000\n",
            "Epoch 77/300\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.52000\n",
            "60/60 - 0s - loss: 0.3731 - acc: 0.9167 - val_loss: 0.5446 - val_acc: 0.7000\n",
            "Epoch 78/300\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.52000\n",
            "60/60 - 0s - loss: 0.3414 - acc: 0.9667 - val_loss: 0.5536 - val_acc: 0.7000\n",
            "Epoch 79/300\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.52000\n",
            "60/60 - 0s - loss: 0.3028 - acc: 0.9500 - val_loss: 0.5394 - val_acc: 0.7000\n",
            "Epoch 80/300\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.52000\n",
            "60/60 - 0s - loss: 0.3116 - acc: 0.9667 - val_loss: 0.5212 - val_acc: 0.7000\n",
            "Epoch 81/300\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.52000\n",
            "60/60 - 0s - loss: 0.3184 - acc: 0.9500 - val_loss: 0.5252 - val_acc: 0.7000\n",
            "Epoch 82/300\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.52000\n",
            "60/60 - 0s - loss: 0.2966 - acc: 0.9833 - val_loss: 0.5430 - val_acc: 0.7000\n",
            "Epoch 83/300\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.52000\n",
            "60/60 - 0s - loss: 0.3014 - acc: 0.9667 - val_loss: 0.5502 - val_acc: 0.7000\n",
            "Epoch 84/300\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.52000\n",
            "60/60 - 0s - loss: 0.3445 - acc: 0.9333 - val_loss: 0.5375 - val_acc: 0.7000\n",
            "Epoch 85/300\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.52000\n",
            "60/60 - 0s - loss: 0.2872 - acc: 0.9500 - val_loss: 0.5247 - val_acc: 0.7000\n",
            "Epoch 86/300\n",
            "\n",
            "Epoch 00086: val_loss improved from 0.52000 to 0.49535, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3182 - acc: 0.9333 - val_loss: 0.4954 - val_acc: 0.7500\n",
            "Epoch 87/300\n",
            "\n",
            "Epoch 00087: val_loss improved from 0.49535 to 0.46479, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2715 - acc: 0.9833 - val_loss: 0.4648 - val_acc: 0.8000\n",
            "Epoch 88/300\n",
            "\n",
            "Epoch 00088: val_loss improved from 0.46479 to 0.45434, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3038 - acc: 0.9500 - val_loss: 0.4543 - val_acc: 0.8000\n",
            "Epoch 89/300\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.45434\n",
            "60/60 - 0s - loss: 0.2833 - acc: 0.9833 - val_loss: 0.4759 - val_acc: 0.7500\n",
            "Epoch 90/300\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.45434\n",
            "60/60 - 0s - loss: 0.3485 - acc: 0.9500 - val_loss: 0.4928 - val_acc: 0.7500\n",
            "Epoch 91/300\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.45434\n",
            "60/60 - 0s - loss: 0.2766 - acc: 0.9833 - val_loss: 0.5050 - val_acc: 0.7500\n",
            "Epoch 92/300\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.45434\n",
            "60/60 - 0s - loss: 0.2988 - acc: 0.9500 - val_loss: 0.5189 - val_acc: 0.7000\n",
            "Epoch 93/300\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.45434\n",
            "60/60 - 0s - loss: 0.3191 - acc: 0.8833 - val_loss: 0.5151 - val_acc: 0.7000\n",
            "Epoch 94/300\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.45434\n",
            "60/60 - 0s - loss: 0.3326 - acc: 0.9000 - val_loss: 0.5179 - val_acc: 0.7000\n",
            "Epoch 95/300\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.45434\n",
            "60/60 - 0s - loss: 0.2936 - acc: 0.9667 - val_loss: 0.5181 - val_acc: 0.7000\n",
            "Epoch 96/300\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.45434\n",
            "60/60 - 0s - loss: 0.2706 - acc: 0.9667 - val_loss: 0.5110 - val_acc: 0.7000\n",
            "Epoch 97/300\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.45434\n",
            "60/60 - 0s - loss: 0.2646 - acc: 0.9667 - val_loss: 0.4881 - val_acc: 0.7500\n",
            "Epoch 98/300\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.45434\n",
            "60/60 - 0s - loss: 0.2800 - acc: 0.9500 - val_loss: 0.4719 - val_acc: 0.7500\n",
            "Epoch 99/300\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.45434\n",
            "60/60 - 0s - loss: 0.2912 - acc: 0.9500 - val_loss: 0.4585 - val_acc: 0.8000\n",
            "Epoch 100/300\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.45434\n",
            "60/60 - 0s - loss: 0.2562 - acc: 0.9833 - val_loss: 0.4575 - val_acc: 0.8000\n",
            "Epoch 101/300\n",
            "\n",
            "Epoch 00101: val_loss improved from 0.45434 to 0.45122, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3219 - acc: 0.9167 - val_loss: 0.4512 - val_acc: 0.8000\n",
            "Epoch 102/300\n",
            "\n",
            "Epoch 00102: val_loss improved from 0.45122 to 0.44966, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2907 - acc: 0.9333 - val_loss: 0.4497 - val_acc: 0.8000\n",
            "Epoch 103/300\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.44966\n",
            "60/60 - 0s - loss: 0.2764 - acc: 0.9667 - val_loss: 0.4701 - val_acc: 0.8000\n",
            "Epoch 104/300\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.44966\n",
            "60/60 - 0s - loss: 0.2782 - acc: 0.9167 - val_loss: 0.4822 - val_acc: 0.7500\n",
            "Epoch 105/300\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 0.44966\n",
            "60/60 - 0s - loss: 0.2315 - acc: 0.9833 - val_loss: 0.4919 - val_acc: 0.7500\n",
            "Epoch 106/300\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 0.44966\n",
            "60/60 - 0s - loss: 0.2923 - acc: 0.9667 - val_loss: 0.4824 - val_acc: 0.7500\n",
            "Epoch 107/300\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.44966\n",
            "60/60 - 0s - loss: 0.2686 - acc: 0.9333 - val_loss: 0.4916 - val_acc: 0.7500\n",
            "Epoch 108/300\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 0.44966\n",
            "60/60 - 0s - loss: 0.2500 - acc: 0.9500 - val_loss: 0.4929 - val_acc: 0.7500\n",
            "Epoch 109/300\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.44966\n",
            "60/60 - 0s - loss: 0.2399 - acc: 0.9333 - val_loss: 0.4903 - val_acc: 0.7500\n",
            "Epoch 110/300\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.44966\n",
            "60/60 - 0s - loss: 0.2416 - acc: 0.9333 - val_loss: 0.4767 - val_acc: 0.7500\n",
            "Epoch 111/300\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 0.44966\n",
            "60/60 - 0s - loss: 0.2712 - acc: 0.9333 - val_loss: 0.4609 - val_acc: 0.8000\n",
            "Epoch 112/300\n",
            "\n",
            "Epoch 00112: val_loss improved from 0.44966 to 0.44707, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2508 - acc: 0.9833 - val_loss: 0.4471 - val_acc: 0.8000\n",
            "Epoch 113/300\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.44707\n",
            "60/60 - 0s - loss: 0.2636 - acc: 0.9500 - val_loss: 0.4538 - val_acc: 0.8000\n",
            "Epoch 114/300\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.44707\n",
            "60/60 - 0s - loss: 0.2776 - acc: 0.9500 - val_loss: 0.4564 - val_acc: 0.7500\n",
            "Epoch 115/300\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.44707\n",
            "60/60 - 0s - loss: 0.2178 - acc: 0.9833 - val_loss: 0.4475 - val_acc: 0.7500\n",
            "Epoch 116/300\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.44707\n",
            "60/60 - 0s - loss: 0.3045 - acc: 0.9167 - val_loss: 0.4501 - val_acc: 0.8000\n",
            "Epoch 117/300\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.44707\n",
            "60/60 - 0s - loss: 0.2181 - acc: 0.9667 - val_loss: 0.4531 - val_acc: 0.8000\n",
            "Epoch 118/300\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.44707\n",
            "60/60 - 0s - loss: 0.2710 - acc: 0.9000 - val_loss: 0.4662 - val_acc: 0.7500\n",
            "Epoch 119/300\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 0.44707\n",
            "60/60 - 0s - loss: 0.2964 - acc: 0.9000 - val_loss: 0.4714 - val_acc: 0.7500\n",
            "Epoch 120/300\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.44707\n",
            "60/60 - 0s - loss: 0.3033 - acc: 0.8833 - val_loss: 0.4752 - val_acc: 0.7500\n",
            "Epoch 121/300\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.44707\n",
            "60/60 - 0s - loss: 0.2749 - acc: 0.9167 - val_loss: 0.4849 - val_acc: 0.7500\n",
            "Epoch 122/300\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.44707\n",
            "60/60 - 0s - loss: 0.2447 - acc: 0.9500 - val_loss: 0.4812 - val_acc: 0.7500\n",
            "Epoch 123/300\n",
            "\n",
            "Epoch 00123: val_loss improved from 0.44707 to 0.44339, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2697 - acc: 0.9000 - val_loss: 0.4434 - val_acc: 0.8000\n",
            "Epoch 124/300\n",
            "\n",
            "Epoch 00124: val_loss improved from 0.44339 to 0.42455, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2379 - acc: 0.9500 - val_loss: 0.4246 - val_acc: 0.8000\n",
            "Epoch 125/300\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 0.42455\n",
            "60/60 - 0s - loss: 0.2215 - acc: 0.9500 - val_loss: 0.4309 - val_acc: 0.8000\n",
            "Epoch 126/300\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 0.42455\n",
            "60/60 - 0s - loss: 0.2802 - acc: 0.9000 - val_loss: 0.4287 - val_acc: 0.8000\n",
            "Epoch 127/300\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 0.42455\n",
            "60/60 - 0s - loss: 0.2689 - acc: 0.9333 - val_loss: 0.4376 - val_acc: 0.8000\n",
            "Epoch 128/300\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 0.42455\n",
            "60/60 - 0s - loss: 0.2176 - acc: 0.9333 - val_loss: 0.4421 - val_acc: 0.7500\n",
            "Epoch 129/300\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.42455\n",
            "60/60 - 0s - loss: 0.2311 - acc: 0.9500 - val_loss: 0.4374 - val_acc: 0.7500\n",
            "Epoch 130/300\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 0.42455\n",
            "60/60 - 0s - loss: 0.2524 - acc: 0.9667 - val_loss: 0.4356 - val_acc: 0.7500\n",
            "Epoch 131/300\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 0.42455\n",
            "60/60 - 0s - loss: 0.2283 - acc: 0.9667 - val_loss: 0.4426 - val_acc: 0.7500\n",
            "Epoch 132/300\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 0.42455\n",
            "60/60 - 0s - loss: 0.2387 - acc: 0.9333 - val_loss: 0.4346 - val_acc: 0.7500\n",
            "Epoch 133/300\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.42455\n",
            "60/60 - 0s - loss: 0.2210 - acc: 0.9500 - val_loss: 0.4262 - val_acc: 0.8000\n",
            "Epoch 134/300\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.42455\n",
            "60/60 - 0s - loss: 0.2330 - acc: 0.9667 - val_loss: 0.4249 - val_acc: 0.8000\n",
            "Epoch 135/300\n",
            "\n",
            "Epoch 00135: val_loss improved from 0.42455 to 0.42249, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1853 - acc: 0.9667 - val_loss: 0.4225 - val_acc: 0.8000\n",
            "Epoch 136/300\n",
            "\n",
            "Epoch 00136: val_loss improved from 0.42249 to 0.41286, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1969 - acc: 1.0000 - val_loss: 0.4129 - val_acc: 0.8000\n",
            "Epoch 137/300\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 0.41286\n",
            "60/60 - 0s - loss: 0.2430 - acc: 0.9167 - val_loss: 0.4131 - val_acc: 0.8000\n",
            "Epoch 138/300\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.41286\n",
            "60/60 - 0s - loss: 0.2040 - acc: 0.9833 - val_loss: 0.4280 - val_acc: 0.8000\n",
            "Epoch 139/300\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.41286\n",
            "60/60 - 0s - loss: 0.2166 - acc: 0.9333 - val_loss: 0.4268 - val_acc: 0.8000\n",
            "Epoch 140/300\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 0.41286\n",
            "60/60 - 0s - loss: 0.2529 - acc: 0.9333 - val_loss: 0.4266 - val_acc: 0.8000\n",
            "Epoch 141/300\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.41286\n",
            "60/60 - 0s - loss: 0.2142 - acc: 0.9667 - val_loss: 0.4352 - val_acc: 0.8000\n",
            "Epoch 142/300\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.41286\n",
            "60/60 - 0s - loss: 0.1731 - acc: 0.9500 - val_loss: 0.4316 - val_acc: 0.8000\n",
            "Epoch 143/300\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 0.41286\n",
            "60/60 - 0s - loss: 0.1826 - acc: 0.9333 - val_loss: 0.4169 - val_acc: 0.8000\n",
            "Epoch 144/300\n",
            "\n",
            "Epoch 00144: val_loss improved from 0.41286 to 0.40347, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1965 - acc: 0.9500 - val_loss: 0.4035 - val_acc: 0.8000\n",
            "Epoch 145/300\n",
            "\n",
            "Epoch 00145: val_loss improved from 0.40347 to 0.39247, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2114 - acc: 0.9500 - val_loss: 0.3925 - val_acc: 0.8000\n",
            "Epoch 146/300\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.39247\n",
            "60/60 - 0s - loss: 0.2297 - acc: 0.9167 - val_loss: 0.3970 - val_acc: 0.8000\n",
            "Epoch 147/300\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.39247\n",
            "60/60 - 0s - loss: 0.1837 - acc: 0.9667 - val_loss: 0.3991 - val_acc: 0.8000\n",
            "Epoch 148/300\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 0.39247\n",
            "60/60 - 0s - loss: 0.1709 - acc: 0.9833 - val_loss: 0.3977 - val_acc: 0.8000\n",
            "Epoch 149/300\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.39247\n",
            "60/60 - 0s - loss: 0.1743 - acc: 0.9667 - val_loss: 0.4004 - val_acc: 0.8000\n",
            "Epoch 150/300\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.39247\n",
            "60/60 - 0s - loss: 0.1824 - acc: 0.9833 - val_loss: 0.4015 - val_acc: 0.8000\n",
            "Epoch 151/300\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.39247\n",
            "60/60 - 0s - loss: 0.1715 - acc: 0.9500 - val_loss: 0.4016 - val_acc: 0.8000\n",
            "Epoch 152/300\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.39247\n",
            "60/60 - 0s - loss: 0.2408 - acc: 0.9333 - val_loss: 0.4022 - val_acc: 0.8000\n",
            "Epoch 153/300\n",
            "\n",
            "Epoch 00153: val_loss improved from 0.39247 to 0.38155, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2290 - acc: 0.9167 - val_loss: 0.3815 - val_acc: 0.8000\n",
            "Epoch 154/300\n",
            "\n",
            "Epoch 00154: val_loss improved from 0.38155 to 0.35723, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1819 - acc: 0.9667 - val_loss: 0.3572 - val_acc: 0.8000\n",
            "Epoch 155/300\n",
            "\n",
            "Epoch 00155: val_loss improved from 0.35723 to 0.35208, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2464 - acc: 0.9500 - val_loss: 0.3521 - val_acc: 0.8000\n",
            "Epoch 156/300\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.35208\n",
            "60/60 - 0s - loss: 0.2064 - acc: 0.9667 - val_loss: 0.3685 - val_acc: 0.8000\n",
            "Epoch 157/300\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.35208\n",
            "60/60 - 0s - loss: 0.1731 - acc: 0.9667 - val_loss: 0.3759 - val_acc: 0.8000\n",
            "Epoch 158/300\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.35208\n",
            "60/60 - 0s - loss: 0.2389 - acc: 0.9000 - val_loss: 0.3755 - val_acc: 0.8000\n",
            "Epoch 159/300\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.35208\n",
            "60/60 - 0s - loss: 0.2357 - acc: 0.9333 - val_loss: 0.3835 - val_acc: 0.8000\n",
            "Epoch 160/300\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.35208\n",
            "60/60 - 0s - loss: 0.2146 - acc: 0.9667 - val_loss: 0.3975 - val_acc: 0.8000\n",
            "Epoch 161/300\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.35208\n",
            "60/60 - 0s - loss: 0.1821 - acc: 0.9833 - val_loss: 0.4055 - val_acc: 0.8000\n",
            "Epoch 162/300\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.35208\n",
            "60/60 - 0s - loss: 0.2370 - acc: 0.9333 - val_loss: 0.3840 - val_acc: 0.8000\n",
            "Epoch 163/300\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.35208\n",
            "60/60 - 0s - loss: 0.2114 - acc: 0.9333 - val_loss: 0.3732 - val_acc: 0.8000\n",
            "Epoch 164/300\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.35208\n",
            "60/60 - 0s - loss: 0.1971 - acc: 0.9833 - val_loss: 0.3662 - val_acc: 0.8000\n",
            "Epoch 165/300\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.35208\n",
            "60/60 - 0s - loss: 0.1930 - acc: 1.0000 - val_loss: 0.3617 - val_acc: 0.8000\n",
            "Epoch 166/300\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 0.35208\n",
            "60/60 - 0s - loss: 0.1722 - acc: 0.9667 - val_loss: 0.3624 - val_acc: 0.8000\n",
            "Epoch 167/300\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.35208\n",
            "60/60 - 0s - loss: 0.1820 - acc: 0.9500 - val_loss: 0.3755 - val_acc: 0.8000\n",
            "Epoch 168/300\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.35208\n",
            "60/60 - 0s - loss: 0.1566 - acc: 0.9833 - val_loss: 0.3883 - val_acc: 0.8000\n",
            "Epoch 169/300\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 0.35208\n",
            "60/60 - 0s - loss: 0.1593 - acc: 0.9667 - val_loss: 0.3866 - val_acc: 0.8000\n",
            "Epoch 170/300\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.35208\n",
            "60/60 - 0s - loss: 0.1195 - acc: 1.0000 - val_loss: 0.3866 - val_acc: 0.8000\n",
            "Epoch 171/300\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.35208\n",
            "60/60 - 0s - loss: 0.1798 - acc: 0.9500 - val_loss: 0.3847 - val_acc: 0.8000\n",
            "Epoch 172/300\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.35208\n",
            "60/60 - 0s - loss: 0.1925 - acc: 0.9500 - val_loss: 0.3919 - val_acc: 0.8000\n",
            "Epoch 173/300\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.35208\n",
            "60/60 - 0s - loss: 0.1637 - acc: 0.9667 - val_loss: 0.3927 - val_acc: 0.8000\n",
            "Epoch 174/300\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.35208\n",
            "60/60 - 0s - loss: 0.2238 - acc: 0.9167 - val_loss: 0.3819 - val_acc: 0.8000\n",
            "Epoch 175/300\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.35208\n",
            "60/60 - 0s - loss: 0.1991 - acc: 0.9333 - val_loss: 0.3610 - val_acc: 0.8000\n",
            "Epoch 176/300\n",
            "\n",
            "Epoch 00176: val_loss improved from 0.35208 to 0.34320, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1841 - acc: 1.0000 - val_loss: 0.3432 - val_acc: 0.8000\n",
            "Epoch 177/300\n",
            "\n",
            "Epoch 00177: val_loss improved from 0.34320 to 0.33868, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1461 - acc: 1.0000 - val_loss: 0.3387 - val_acc: 0.8000\n",
            "Epoch 178/300\n",
            "\n",
            "Epoch 00178: val_loss improved from 0.33868 to 0.33688, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1979 - acc: 0.9500 - val_loss: 0.3369 - val_acc: 0.8000\n",
            "Epoch 179/300\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.33688\n",
            "60/60 - 0s - loss: 0.1643 - acc: 0.9833 - val_loss: 0.3414 - val_acc: 0.8000\n",
            "Epoch 180/300\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.33688\n",
            "60/60 - 0s - loss: 0.1607 - acc: 1.0000 - val_loss: 0.3400 - val_acc: 0.8000\n",
            "Epoch 181/300\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.33688\n",
            "60/60 - 0s - loss: 0.1560 - acc: 0.9833 - val_loss: 0.3485 - val_acc: 0.8000\n",
            "Epoch 182/300\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.33688\n",
            "60/60 - 0s - loss: 0.1551 - acc: 0.9667 - val_loss: 0.3549 - val_acc: 0.8000\n",
            "Epoch 183/300\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.33688\n",
            "60/60 - 0s - loss: 0.1378 - acc: 1.0000 - val_loss: 0.3519 - val_acc: 0.8000\n",
            "Epoch 184/300\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 0.33688\n",
            "60/60 - 0s - loss: 0.1786 - acc: 0.9667 - val_loss: 0.3563 - val_acc: 0.8000\n",
            "Epoch 185/300\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.33688\n",
            "60/60 - 0s - loss: 0.1816 - acc: 0.9833 - val_loss: 0.3619 - val_acc: 0.8000\n",
            "Epoch 186/300\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.33688\n",
            "60/60 - 0s - loss: 0.1803 - acc: 0.9500 - val_loss: 0.3772 - val_acc: 0.8000\n",
            "Epoch 187/300\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 0.33688\n",
            "60/60 - 0s - loss: 0.1873 - acc: 0.9667 - val_loss: 0.3799 - val_acc: 0.8000\n",
            "Epoch 188/300\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.33688\n",
            "60/60 - 0s - loss: 0.2043 - acc: 0.9333 - val_loss: 0.3826 - val_acc: 0.8000\n",
            "Epoch 189/300\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.33688\n",
            "60/60 - 0s - loss: 0.1809 - acc: 0.9500 - val_loss: 0.3760 - val_acc: 0.8000\n",
            "Epoch 190/300\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.33688\n",
            "60/60 - 0s - loss: 0.1310 - acc: 1.0000 - val_loss: 0.3701 - val_acc: 0.8000\n",
            "Epoch 191/300\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.33688\n",
            "60/60 - 0s - loss: 0.1478 - acc: 0.9833 - val_loss: 0.3611 - val_acc: 0.8000\n",
            "Epoch 192/300\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 0.33688\n",
            "60/60 - 0s - loss: 0.1259 - acc: 1.0000 - val_loss: 0.3636 - val_acc: 0.8000\n",
            "Epoch 193/300\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.33688\n",
            "60/60 - 0s - loss: 0.1556 - acc: 0.9833 - val_loss: 0.3591 - val_acc: 0.8000\n",
            "Epoch 194/300\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.33688\n",
            "60/60 - 0s - loss: 0.1708 - acc: 0.9667 - val_loss: 0.3670 - val_acc: 0.8000\n",
            "Epoch 195/300\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.33688\n",
            "60/60 - 0s - loss: 0.2127 - acc: 0.9167 - val_loss: 0.3634 - val_acc: 0.8000\n",
            "Epoch 196/300\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.33688\n",
            "60/60 - 0s - loss: 0.2107 - acc: 0.9500 - val_loss: 0.3400 - val_acc: 0.8000\n",
            "Epoch 197/300\n",
            "\n",
            "Epoch 00197: val_loss improved from 0.33688 to 0.32401, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1614 - acc: 1.0000 - val_loss: 0.3240 - val_acc: 0.8000\n",
            "Epoch 198/300\n",
            "\n",
            "Epoch 00198: val_loss improved from 0.32401 to 0.31796, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1630 - acc: 0.9500 - val_loss: 0.3180 - val_acc: 0.8000\n",
            "Epoch 199/300\n",
            "\n",
            "Epoch 00199: val_loss improved from 0.31796 to 0.31409, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1852 - acc: 0.9667 - val_loss: 0.3141 - val_acc: 0.8000\n",
            "Epoch 200/300\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.31409\n",
            "60/60 - 0s - loss: 0.1480 - acc: 0.9833 - val_loss: 0.3159 - val_acc: 0.8000\n",
            "Epoch 201/300\n",
            "\n",
            "Epoch 00201: val_loss did not improve from 0.31409\n",
            "60/60 - 0s - loss: 0.1739 - acc: 0.9500 - val_loss: 0.3202 - val_acc: 0.8000\n",
            "Epoch 202/300\n",
            "\n",
            "Epoch 00202: val_loss did not improve from 0.31409\n",
            "60/60 - 0s - loss: 0.2063 - acc: 0.9167 - val_loss: 0.3209 - val_acc: 0.8000\n",
            "Epoch 203/300\n",
            "\n",
            "Epoch 00203: val_loss did not improve from 0.31409\n",
            "60/60 - 0s - loss: 0.1486 - acc: 0.9667 - val_loss: 0.3272 - val_acc: 0.8000\n",
            "Epoch 204/300\n",
            "\n",
            "Epoch 00204: val_loss did not improve from 0.31409\n",
            "60/60 - 0s - loss: 0.1884 - acc: 0.9500 - val_loss: 0.3318 - val_acc: 0.8000\n",
            "Epoch 205/300\n",
            "\n",
            "Epoch 00205: val_loss did not improve from 0.31409\n",
            "60/60 - 0s - loss: 0.1437 - acc: 0.9833 - val_loss: 0.3324 - val_acc: 0.8000\n",
            "Epoch 206/300\n",
            "\n",
            "Epoch 00206: val_loss did not improve from 0.31409\n",
            "60/60 - 0s - loss: 0.1795 - acc: 1.0000 - val_loss: 0.3379 - val_acc: 0.8000\n",
            "Epoch 207/300\n",
            "\n",
            "Epoch 00207: val_loss did not improve from 0.31409\n",
            "60/60 - 0s - loss: 0.2044 - acc: 0.9500 - val_loss: 0.3521 - val_acc: 0.8000\n",
            "Epoch 208/300\n",
            "\n",
            "Epoch 00208: val_loss did not improve from 0.31409\n",
            "60/60 - 0s - loss: 0.1725 - acc: 0.9833 - val_loss: 0.3591 - val_acc: 0.8000\n",
            "Epoch 209/300\n",
            "\n",
            "Epoch 00209: val_loss did not improve from 0.31409\n",
            "60/60 - 0s - loss: 0.1549 - acc: 0.9833 - val_loss: 0.3599 - val_acc: 0.8000\n",
            "Epoch 210/300\n",
            "\n",
            "Epoch 00210: val_loss did not improve from 0.31409\n",
            "60/60 - 0s - loss: 0.1374 - acc: 0.9833 - val_loss: 0.3708 - val_acc: 0.8000\n",
            "Epoch 211/300\n",
            "\n",
            "Epoch 00211: val_loss did not improve from 0.31409\n",
            "60/60 - 0s - loss: 0.1592 - acc: 0.9500 - val_loss: 0.3793 - val_acc: 0.8000\n",
            "Epoch 212/300\n",
            "\n",
            "Epoch 00212: val_loss did not improve from 0.31409\n",
            "60/60 - 0s - loss: 0.1644 - acc: 0.9667 - val_loss: 0.3818 - val_acc: 0.8000\n",
            "Epoch 213/300\n",
            "\n",
            "Epoch 00213: val_loss did not improve from 0.31409\n",
            "60/60 - 0s - loss: 0.1742 - acc: 0.9833 - val_loss: 0.3747 - val_acc: 0.8000\n",
            "Epoch 214/300\n",
            "\n",
            "Epoch 00214: val_loss did not improve from 0.31409\n",
            "60/60 - 0s - loss: 0.1536 - acc: 0.9667 - val_loss: 0.3823 - val_acc: 0.8000\n",
            "Epoch 215/300\n",
            "\n",
            "Epoch 00215: val_loss did not improve from 0.31409\n",
            "60/60 - 0s - loss: 0.1734 - acc: 0.9500 - val_loss: 0.4120 - val_acc: 0.8000\n",
            "Epoch 216/300\n",
            "\n",
            "Epoch 00216: val_loss did not improve from 0.31409\n",
            "60/60 - 0s - loss: 0.1598 - acc: 0.9333 - val_loss: 0.4237 - val_acc: 0.8000\n",
            "Epoch 217/300\n",
            "\n",
            "Epoch 00217: val_loss did not improve from 0.31409\n",
            "60/60 - 0s - loss: 0.1472 - acc: 0.9500 - val_loss: 0.4153 - val_acc: 0.7500\n",
            "Epoch 218/300\n",
            "\n",
            "Epoch 00218: val_loss did not improve from 0.31409\n",
            "60/60 - 0s - loss: 0.1448 - acc: 0.9667 - val_loss: 0.3922 - val_acc: 0.7500\n",
            "Epoch 219/300\n",
            "\n",
            "Epoch 00219: val_loss did not improve from 0.31409\n",
            "60/60 - 0s - loss: 0.1642 - acc: 0.9667 - val_loss: 0.3687 - val_acc: 0.8000\n",
            "Epoch 220/300\n",
            "\n",
            "Epoch 00220: val_loss did not improve from 0.31409\n",
            "60/60 - 0s - loss: 0.1978 - acc: 0.9500 - val_loss: 0.3590 - val_acc: 0.8000\n",
            "Epoch 221/300\n",
            "\n",
            "Epoch 00221: val_loss did not improve from 0.31409\n",
            "60/60 - 0s - loss: 0.2014 - acc: 0.9333 - val_loss: 0.3396 - val_acc: 0.8000\n",
            "Epoch 222/300\n",
            "\n",
            "Epoch 00222: val_loss did not improve from 0.31409\n",
            "60/60 - 0s - loss: 0.1525 - acc: 1.0000 - val_loss: 0.3387 - val_acc: 0.8000\n",
            "Epoch 223/300\n",
            "\n",
            "Epoch 00223: val_loss did not improve from 0.31409\n",
            "60/60 - 0s - loss: 0.1216 - acc: 1.0000 - val_loss: 0.3332 - val_acc: 0.8000\n",
            "Epoch 224/300\n",
            "\n",
            "Epoch 00224: val_loss did not improve from 0.31409\n",
            "60/60 - 0s - loss: 0.1495 - acc: 0.9833 - val_loss: 0.3453 - val_acc: 0.8000\n",
            "Epoch 225/300\n",
            "\n",
            "Epoch 00225: val_loss did not improve from 0.31409\n",
            "60/60 - 0s - loss: 0.1740 - acc: 0.9833 - val_loss: 0.3349 - val_acc: 0.8000\n",
            "Epoch 226/300\n",
            "\n",
            "Epoch 00226: val_loss did not improve from 0.31409\n",
            "60/60 - 0s - loss: 0.1610 - acc: 0.9500 - val_loss: 0.3206 - val_acc: 0.8000\n",
            "Epoch 227/300\n",
            "\n",
            "Epoch 00227: val_loss improved from 0.31409 to 0.31198, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1800 - acc: 0.9500 - val_loss: 0.3120 - val_acc: 0.8000\n",
            "Epoch 228/300\n",
            "\n",
            "Epoch 00228: val_loss improved from 0.31198 to 0.30740, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1175 - acc: 0.9667 - val_loss: 0.3074 - val_acc: 0.8000\n",
            "Epoch 229/300\n",
            "\n",
            "Epoch 00229: val_loss improved from 0.30740 to 0.30715, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1802 - acc: 0.9500 - val_loss: 0.3071 - val_acc: 0.8000\n",
            "Epoch 230/300\n",
            "\n",
            "Epoch 00230: val_loss did not improve from 0.30715\n",
            "60/60 - 0s - loss: 0.1461 - acc: 0.9667 - val_loss: 0.3154 - val_acc: 0.8000\n",
            "Epoch 231/300\n",
            "\n",
            "Epoch 00231: val_loss did not improve from 0.30715\n",
            "60/60 - 0s - loss: 0.1687 - acc: 0.9667 - val_loss: 0.3238 - val_acc: 0.8000\n",
            "Epoch 232/300\n",
            "\n",
            "Epoch 00232: val_loss did not improve from 0.30715\n",
            "60/60 - 0s - loss: 0.1417 - acc: 1.0000 - val_loss: 0.3275 - val_acc: 0.8000\n",
            "Epoch 233/300\n",
            "\n",
            "Epoch 00233: val_loss did not improve from 0.30715\n",
            "60/60 - 0s - loss: 0.1386 - acc: 0.9833 - val_loss: 0.3142 - val_acc: 0.8000\n",
            "Epoch 234/300\n",
            "\n",
            "Epoch 00234: val_loss improved from 0.30715 to 0.30094, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1806 - acc: 0.9667 - val_loss: 0.3009 - val_acc: 0.8000\n",
            "Epoch 235/300\n",
            "\n",
            "Epoch 00235: val_loss improved from 0.30094 to 0.29842, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1672 - acc: 0.9500 - val_loss: 0.2984 - val_acc: 0.8000\n",
            "Epoch 236/300\n",
            "\n",
            "Epoch 00236: val_loss improved from 0.29842 to 0.28447, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1163 - acc: 1.0000 - val_loss: 0.2845 - val_acc: 0.8000\n",
            "Epoch 237/300\n",
            "\n",
            "Epoch 00237: val_loss improved from 0.28447 to 0.28053, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1667 - acc: 0.9667 - val_loss: 0.2805 - val_acc: 0.8000\n",
            "Epoch 238/300\n",
            "\n",
            "Epoch 00238: val_loss did not improve from 0.28053\n",
            "60/60 - 0s - loss: 0.1944 - acc: 0.9333 - val_loss: 0.2825 - val_acc: 0.8000\n",
            "Epoch 239/300\n",
            "\n",
            "Epoch 00239: val_loss did not improve from 0.28053\n",
            "60/60 - 0s - loss: 0.1221 - acc: 0.9833 - val_loss: 0.2849 - val_acc: 0.8000\n",
            "Epoch 240/300\n",
            "\n",
            "Epoch 00240: val_loss did not improve from 0.28053\n",
            "60/60 - 0s - loss: 0.1238 - acc: 1.0000 - val_loss: 0.2973 - val_acc: 0.8000\n",
            "Epoch 241/300\n",
            "\n",
            "Epoch 00241: val_loss did not improve from 0.28053\n",
            "60/60 - 0s - loss: 0.1301 - acc: 0.9833 - val_loss: 0.3055 - val_acc: 0.8000\n",
            "Epoch 242/300\n",
            "\n",
            "Epoch 00242: val_loss did not improve from 0.28053\n",
            "60/60 - 0s - loss: 0.1450 - acc: 0.9667 - val_loss: 0.3171 - val_acc: 0.8000\n",
            "Epoch 243/300\n",
            "\n",
            "Epoch 00243: val_loss did not improve from 0.28053\n",
            "60/60 - 0s - loss: 0.1229 - acc: 0.9833 - val_loss: 0.3299 - val_acc: 0.8000\n",
            "Epoch 244/300\n",
            "\n",
            "Epoch 00244: val_loss did not improve from 0.28053\n",
            "60/60 - 0s - loss: 0.1331 - acc: 1.0000 - val_loss: 0.3469 - val_acc: 0.8000\n",
            "Epoch 245/300\n",
            "\n",
            "Epoch 00245: val_loss did not improve from 0.28053\n",
            "60/60 - 0s - loss: 0.1706 - acc: 0.9167 - val_loss: 0.3388 - val_acc: 0.8000\n",
            "Epoch 246/300\n",
            "\n",
            "Epoch 00246: val_loss did not improve from 0.28053\n",
            "60/60 - 0s - loss: 0.1092 - acc: 1.0000 - val_loss: 0.3383 - val_acc: 0.8000\n",
            "Epoch 247/300\n",
            "\n",
            "Epoch 00247: val_loss did not improve from 0.28053\n",
            "60/60 - 0s - loss: 0.1730 - acc: 0.9667 - val_loss: 0.3334 - val_acc: 0.8000\n",
            "Epoch 248/300\n",
            "\n",
            "Epoch 00248: val_loss did not improve from 0.28053\n",
            "60/60 - 0s - loss: 0.1246 - acc: 1.0000 - val_loss: 0.3417 - val_acc: 0.8000\n",
            "Epoch 249/300\n",
            "\n",
            "Epoch 00249: val_loss did not improve from 0.28053\n",
            "60/60 - 0s - loss: 0.1372 - acc: 0.9833 - val_loss: 0.3449 - val_acc: 0.8000\n",
            "Epoch 250/300\n",
            "\n",
            "Epoch 00250: val_loss did not improve from 0.28053\n",
            "60/60 - 0s - loss: 0.1273 - acc: 0.9833 - val_loss: 0.3497 - val_acc: 0.8000\n",
            "Epoch 251/300\n",
            "\n",
            "Epoch 00251: val_loss did not improve from 0.28053\n",
            "60/60 - 0s - loss: 0.1010 - acc: 1.0000 - val_loss: 0.3588 - val_acc: 0.8000\n",
            "Epoch 252/300\n",
            "\n",
            "Epoch 00252: val_loss did not improve from 0.28053\n",
            "60/60 - 0s - loss: 0.1546 - acc: 1.0000 - val_loss: 0.3559 - val_acc: 0.8000\n",
            "Epoch 253/300\n",
            "\n",
            "Epoch 00253: val_loss did not improve from 0.28053\n",
            "60/60 - 0s - loss: 0.1625 - acc: 0.9667 - val_loss: 0.3465 - val_acc: 0.8000\n",
            "Epoch 254/300\n",
            "\n",
            "Epoch 00254: val_loss did not improve from 0.28053\n",
            "60/60 - 0s - loss: 0.1472 - acc: 0.9667 - val_loss: 0.3380 - val_acc: 0.8000\n",
            "Epoch 255/300\n",
            "\n",
            "Epoch 00255: val_loss did not improve from 0.28053\n",
            "60/60 - 0s - loss: 0.0999 - acc: 0.9833 - val_loss: 0.3308 - val_acc: 0.8000\n",
            "Epoch 256/300\n",
            "\n",
            "Epoch 00256: val_loss did not improve from 0.28053\n",
            "60/60 - 0s - loss: 0.1360 - acc: 0.9833 - val_loss: 0.3329 - val_acc: 0.8000\n",
            "Epoch 257/300\n",
            "\n",
            "Epoch 00257: val_loss did not improve from 0.28053\n",
            "60/60 - 0s - loss: 0.1458 - acc: 0.9833 - val_loss: 0.3244 - val_acc: 0.8000\n",
            "Epoch 258/300\n",
            "\n",
            "Epoch 00258: val_loss did not improve from 0.28053\n",
            "60/60 - 0s - loss: 0.1206 - acc: 1.0000 - val_loss: 0.3054 - val_acc: 0.8000\n",
            "Epoch 259/300\n",
            "\n",
            "Epoch 00259: val_loss did not improve from 0.28053\n",
            "60/60 - 0s - loss: 0.1490 - acc: 0.9667 - val_loss: 0.3007 - val_acc: 0.8000\n",
            "Epoch 260/300\n",
            "\n",
            "Epoch 00260: val_loss did not improve from 0.28053\n",
            "60/60 - 0s - loss: 0.1489 - acc: 0.9667 - val_loss: 0.3026 - val_acc: 0.8000\n",
            "Epoch 261/300\n",
            "\n",
            "Epoch 00261: val_loss did not improve from 0.28053\n",
            "60/60 - 0s - loss: 0.1595 - acc: 0.9500 - val_loss: 0.3080 - val_acc: 0.8000\n",
            "Epoch 262/300\n",
            "\n",
            "Epoch 00262: val_loss did not improve from 0.28053\n",
            "60/60 - 0s - loss: 0.1177 - acc: 0.9833 - val_loss: 0.3246 - val_acc: 0.8000\n",
            "Epoch 263/300\n",
            "\n",
            "Epoch 00263: val_loss did not improve from 0.28053\n",
            "60/60 - 0s - loss: 0.1167 - acc: 0.9833 - val_loss: 0.3383 - val_acc: 0.8000\n",
            "Epoch 264/300\n",
            "\n",
            "Epoch 00264: val_loss did not improve from 0.28053\n",
            "60/60 - 0s - loss: 0.1315 - acc: 0.9500 - val_loss: 0.3378 - val_acc: 0.8000\n",
            "Epoch 265/300\n",
            "\n",
            "Epoch 00265: val_loss did not improve from 0.28053\n",
            "60/60 - 0s - loss: 0.1279 - acc: 0.9500 - val_loss: 0.3191 - val_acc: 0.8000\n",
            "Epoch 266/300\n",
            "\n",
            "Epoch 00266: val_loss did not improve from 0.28053\n",
            "60/60 - 0s - loss: 0.1518 - acc: 0.9667 - val_loss: 0.2970 - val_acc: 0.8000\n",
            "Epoch 267/300\n",
            "\n",
            "Epoch 00267: val_loss did not improve from 0.28053\n",
            "60/60 - 0s - loss: 0.1117 - acc: 0.9833 - val_loss: 0.2837 - val_acc: 0.8000\n",
            "Epoch 268/300\n",
            "\n",
            "Epoch 00268: val_loss improved from 0.28053 to 0.27526, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1464 - acc: 0.9500 - val_loss: 0.2753 - val_acc: 0.8000\n",
            "Epoch 269/300\n",
            "\n",
            "Epoch 00269: val_loss improved from 0.27526 to 0.26789, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.0984 - acc: 1.0000 - val_loss: 0.2679 - val_acc: 0.8000\n",
            "Epoch 270/300\n",
            "\n",
            "Epoch 00270: val_loss improved from 0.26789 to 0.26430, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1329 - acc: 1.0000 - val_loss: 0.2643 - val_acc: 0.8000\n",
            "Epoch 271/300\n",
            "\n",
            "Epoch 00271: val_loss improved from 0.26430 to 0.26304, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1461 - acc: 0.9667 - val_loss: 0.2630 - val_acc: 0.8000\n",
            "Epoch 272/300\n",
            "\n",
            "Epoch 00272: val_loss did not improve from 0.26304\n",
            "60/60 - 0s - loss: 0.1224 - acc: 0.9667 - val_loss: 0.2788 - val_acc: 0.8000\n",
            "Epoch 273/300\n",
            "\n",
            "Epoch 00273: val_loss did not improve from 0.26304\n",
            "60/60 - 0s - loss: 0.1527 - acc: 0.9667 - val_loss: 0.2952 - val_acc: 0.8000\n",
            "Epoch 274/300\n",
            "\n",
            "Epoch 00274: val_loss did not improve from 0.26304\n",
            "60/60 - 0s - loss: 0.1706 - acc: 0.9667 - val_loss: 0.3005 - val_acc: 0.8000\n",
            "Epoch 275/300\n",
            "\n",
            "Epoch 00275: val_loss did not improve from 0.26304\n",
            "60/60 - 0s - loss: 0.1149 - acc: 0.9833 - val_loss: 0.2975 - val_acc: 0.8000\n",
            "Epoch 276/300\n",
            "\n",
            "Epoch 00276: val_loss did not improve from 0.26304\n",
            "60/60 - 0s - loss: 0.1945 - acc: 0.9333 - val_loss: 0.2970 - val_acc: 0.8000\n",
            "Epoch 277/300\n",
            "\n",
            "Epoch 00277: val_loss did not improve from 0.26304\n",
            "60/60 - 0s - loss: 0.1043 - acc: 1.0000 - val_loss: 0.2936 - val_acc: 0.8000\n",
            "Epoch 278/300\n",
            "\n",
            "Epoch 00278: val_loss did not improve from 0.26304\n",
            "60/60 - 0s - loss: 0.1127 - acc: 0.9833 - val_loss: 0.2994 - val_acc: 0.8000\n",
            "Epoch 279/300\n",
            "\n",
            "Epoch 00279: val_loss did not improve from 0.26304\n",
            "60/60 - 0s - loss: 0.1674 - acc: 0.9833 - val_loss: 0.3063 - val_acc: 0.8000\n",
            "Epoch 280/300\n",
            "\n",
            "Epoch 00280: val_loss did not improve from 0.26304\n",
            "60/60 - 0s - loss: 0.1398 - acc: 0.9833 - val_loss: 0.3228 - val_acc: 0.8000\n",
            "Epoch 281/300\n",
            "\n",
            "Epoch 00281: val_loss did not improve from 0.26304\n",
            "60/60 - 0s - loss: 0.1327 - acc: 0.9667 - val_loss: 0.3136 - val_acc: 0.8000\n",
            "Epoch 282/300\n",
            "\n",
            "Epoch 00282: val_loss did not improve from 0.26304\n",
            "60/60 - 0s - loss: 0.1380 - acc: 0.9833 - val_loss: 0.2961 - val_acc: 0.8000\n",
            "Epoch 283/300\n",
            "\n",
            "Epoch 00283: val_loss did not improve from 0.26304\n",
            "60/60 - 0s - loss: 0.1359 - acc: 0.9833 - val_loss: 0.2815 - val_acc: 0.8000\n",
            "Epoch 284/300\n",
            "\n",
            "Epoch 00284: val_loss did not improve from 0.26304\n",
            "60/60 - 0s - loss: 0.1460 - acc: 0.9833 - val_loss: 0.2767 - val_acc: 0.8000\n",
            "Epoch 285/300\n",
            "\n",
            "Epoch 00285: val_loss did not improve from 0.26304\n",
            "60/60 - 0s - loss: 0.1191 - acc: 1.0000 - val_loss: 0.2844 - val_acc: 0.8000\n",
            "Epoch 286/300\n",
            "\n",
            "Epoch 00286: val_loss did not improve from 0.26304\n",
            "60/60 - 0s - loss: 0.1775 - acc: 0.9333 - val_loss: 0.2874 - val_acc: 0.8000\n",
            "Epoch 287/300\n",
            "\n",
            "Epoch 00287: val_loss did not improve from 0.26304\n",
            "60/60 - 0s - loss: 0.1052 - acc: 1.0000 - val_loss: 0.2906 - val_acc: 0.8000\n",
            "Epoch 288/300\n",
            "\n",
            "Epoch 00288: val_loss did not improve from 0.26304\n",
            "60/60 - 0s - loss: 0.1359 - acc: 0.9667 - val_loss: 0.2916 - val_acc: 0.8000\n",
            "Epoch 289/300\n",
            "\n",
            "Epoch 00289: val_loss did not improve from 0.26304\n",
            "60/60 - 0s - loss: 0.1656 - acc: 0.9333 - val_loss: 0.2896 - val_acc: 0.8000\n",
            "Epoch 290/300\n",
            "\n",
            "Epoch 00290: val_loss did not improve from 0.26304\n",
            "60/60 - 0s - loss: 0.1278 - acc: 0.9500 - val_loss: 0.2815 - val_acc: 0.8000\n",
            "Epoch 291/300\n",
            "\n",
            "Epoch 00291: val_loss did not improve from 0.26304\n",
            "60/60 - 0s - loss: 0.1619 - acc: 0.9833 - val_loss: 0.2717 - val_acc: 0.8500\n",
            "Epoch 292/300\n",
            "\n",
            "Epoch 00292: val_loss improved from 0.26304 to 0.24773, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1576 - acc: 1.0000 - val_loss: 0.2477 - val_acc: 0.8500\n",
            "Epoch 293/300\n",
            "\n",
            "Epoch 00293: val_loss improved from 0.24773 to 0.24093, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1408 - acc: 0.9667 - val_loss: 0.2409 - val_acc: 0.8500\n",
            "Epoch 294/300\n",
            "\n",
            "Epoch 00294: val_loss did not improve from 0.24093\n",
            "60/60 - 0s - loss: 0.1504 - acc: 0.9667 - val_loss: 0.2455 - val_acc: 0.8500\n",
            "Epoch 295/300\n",
            "\n",
            "Epoch 00295: val_loss did not improve from 0.24093\n",
            "60/60 - 0s - loss: 0.0954 - acc: 1.0000 - val_loss: 0.2571 - val_acc: 0.8500\n",
            "Epoch 296/300\n",
            "\n",
            "Epoch 00296: val_loss did not improve from 0.24093\n",
            "60/60 - 0s - loss: 0.1921 - acc: 0.9333 - val_loss: 0.2514 - val_acc: 0.8500\n",
            "Epoch 297/300\n",
            "\n",
            "Epoch 00297: val_loss improved from 0.24093 to 0.23857, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1021 - acc: 1.0000 - val_loss: 0.2386 - val_acc: 0.8500\n",
            "Epoch 298/300\n",
            "\n",
            "Epoch 00298: val_loss improved from 0.23857 to 0.22978, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1596 - acc: 0.9667 - val_loss: 0.2298 - val_acc: 0.8500\n",
            "Epoch 299/300\n",
            "\n",
            "Epoch 00299: val_loss improved from 0.22978 to 0.22878, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2260 - acc: 0.9500 - val_loss: 0.2288 - val_acc: 0.8000\n",
            "Epoch 300/300\n",
            "\n",
            "Epoch 00300: val_loss improved from 0.22878 to 0.22335, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1585 - acc: 0.9500 - val_loss: 0.2233 - val_acc: 0.8000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "# Evaluate on test data\n",
            "40/40 [==============================] - 0s 2ms/sample - loss: 0.9346 - acc: 0.5500\n",
            "test loss, test acc: [0.9345838911831379, 0.55]\n",
            "EEG_Deep/Data2A/parsed_P04T.mat\n",
            "Filtering of Training Data Finished\n",
            "EEG_Deep/Data2A/parsed_P04E.mat\n",
            "Filtering of Testing Data Finished\n",
            "(1, 2)\n",
            "Finding labels in training data\n",
            "Finding labels in testing data\n",
            "[2 2 2 1 2 1 1 2 2 2 1 2 1 2 2 2 2 1 1 1]\n",
            "(60, 12, 1536)\n",
            "(60,)\n",
            "(20, 12, 1536)\n",
            "(20,)\n",
            "[[0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]]\n",
            "X_train shape: (60, 1, 12, 1536)\n",
            "60 train samples\n",
            "20 val samples\n",
            "(40, 12, 1536)\n",
            "(40, 1)\n",
            "X_train shape: (40, 1, 12, 1536)\n",
            "40 train samples\n",
            "Train on 60 samples, validate on 20 samples\n",
            "Epoch 1/300\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.69807, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 1s - loss: 0.7281 - acc: 0.4833 - val_loss: 0.6981 - val_acc: 0.3500\n",
            "Epoch 2/300\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.69807\n",
            "60/60 - 0s - loss: 0.6793 - acc: 0.5833 - val_loss: 0.6985 - val_acc: 0.3500\n",
            "Epoch 3/300\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.69807\n",
            "60/60 - 0s - loss: 0.6672 - acc: 0.6333 - val_loss: 0.6982 - val_acc: 0.3500\n",
            "Epoch 4/300\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.69807 to 0.69800, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6568 - acc: 0.6167 - val_loss: 0.6980 - val_acc: 0.4000\n",
            "Epoch 5/300\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.69800 to 0.69753, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6498 - acc: 0.5833 - val_loss: 0.6975 - val_acc: 0.4500\n",
            "Epoch 6/300\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.69753 to 0.69680, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6372 - acc: 0.5667 - val_loss: 0.6968 - val_acc: 0.4500\n",
            "Epoch 7/300\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.69680 to 0.69622, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6137 - acc: 0.6000 - val_loss: 0.6962 - val_acc: 0.5000\n",
            "Epoch 8/300\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.69622 to 0.69567, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5980 - acc: 0.6833 - val_loss: 0.6957 - val_acc: 0.4000\n",
            "Epoch 9/300\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.69567 to 0.69544, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6011 - acc: 0.7500 - val_loss: 0.6954 - val_acc: 0.5000\n",
            "Epoch 10/300\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.69544\n",
            "60/60 - 0s - loss: 0.5768 - acc: 0.8833 - val_loss: 0.6955 - val_acc: 0.5500\n",
            "Epoch 11/300\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.69544\n",
            "60/60 - 0s - loss: 0.5733 - acc: 0.8500 - val_loss: 0.6956 - val_acc: 0.6000\n",
            "Epoch 12/300\n",
            "\n",
            "Epoch 00012: val_loss improved from 0.69544 to 0.69536, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5613 - acc: 0.8667 - val_loss: 0.6954 - val_acc: 0.6000\n",
            "Epoch 13/300\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.69536\n",
            "60/60 - 0s - loss: 0.5648 - acc: 0.8333 - val_loss: 0.6957 - val_acc: 0.6000\n",
            "Epoch 14/300\n",
            "\n",
            "Epoch 00014: val_loss improved from 0.69536 to 0.69461, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5535 - acc: 0.8500 - val_loss: 0.6946 - val_acc: 0.6000\n",
            "Epoch 15/300\n",
            "\n",
            "Epoch 00015: val_loss improved from 0.69461 to 0.69289, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5272 - acc: 0.8833 - val_loss: 0.6929 - val_acc: 0.6000\n",
            "Epoch 16/300\n",
            "\n",
            "Epoch 00016: val_loss improved from 0.69289 to 0.69142, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5290 - acc: 0.8333 - val_loss: 0.6914 - val_acc: 0.5500\n",
            "Epoch 17/300\n",
            "\n",
            "Epoch 00017: val_loss improved from 0.69142 to 0.68956, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5412 - acc: 0.8333 - val_loss: 0.6896 - val_acc: 0.5500\n",
            "Epoch 18/300\n",
            "\n",
            "Epoch 00018: val_loss improved from 0.68956 to 0.68876, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4989 - acc: 0.8833 - val_loss: 0.6888 - val_acc: 0.5500\n",
            "Epoch 19/300\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.4912 - acc: 0.8167 - val_loss: 0.6894 - val_acc: 0.5500\n",
            "Epoch 20/300\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.4658 - acc: 0.9333 - val_loss: 0.6904 - val_acc: 0.5500\n",
            "Epoch 21/300\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.4381 - acc: 0.9333 - val_loss: 0.6946 - val_acc: 0.5500\n",
            "Epoch 22/300\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.4514 - acc: 0.9167 - val_loss: 0.6985 - val_acc: 0.5500\n",
            "Epoch 23/300\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.4638 - acc: 0.8500 - val_loss: 0.6994 - val_acc: 0.5500\n",
            "Epoch 24/300\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.4299 - acc: 0.8000 - val_loss: 0.7004 - val_acc: 0.6000\n",
            "Epoch 25/300\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.4065 - acc: 0.9000 - val_loss: 0.7005 - val_acc: 0.6500\n",
            "Epoch 26/300\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.4110 - acc: 0.8833 - val_loss: 0.7024 - val_acc: 0.6500\n",
            "Epoch 27/300\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.3784 - acc: 0.9333 - val_loss: 0.7049 - val_acc: 0.6500\n",
            "Epoch 28/300\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.3709 - acc: 0.8667 - val_loss: 0.7068 - val_acc: 0.6500\n",
            "Epoch 29/300\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.3732 - acc: 0.9167 - val_loss: 0.7094 - val_acc: 0.6500\n",
            "Epoch 30/300\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.3288 - acc: 0.9333 - val_loss: 0.7148 - val_acc: 0.6500\n",
            "Epoch 31/300\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.3949 - acc: 0.8667 - val_loss: 0.7144 - val_acc: 0.6500\n",
            "Epoch 32/300\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.3862 - acc: 0.8833 - val_loss: 0.7149 - val_acc: 0.6500\n",
            "Epoch 33/300\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.3498 - acc: 0.9000 - val_loss: 0.7081 - val_acc: 0.6500\n",
            "Epoch 34/300\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.3343 - acc: 0.8667 - val_loss: 0.7070 - val_acc: 0.6500\n",
            "Epoch 35/300\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.3390 - acc: 0.9167 - val_loss: 0.7034 - val_acc: 0.6500\n",
            "Epoch 36/300\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.2995 - acc: 0.9333 - val_loss: 0.6979 - val_acc: 0.6500\n",
            "Epoch 37/300\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.3085 - acc: 0.9167 - val_loss: 0.6955 - val_acc: 0.7000\n",
            "Epoch 38/300\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.2658 - acc: 0.9667 - val_loss: 0.6965 - val_acc: 0.7000\n",
            "Epoch 39/300\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.2711 - acc: 0.9167 - val_loss: 0.7019 - val_acc: 0.6500\n",
            "Epoch 40/300\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.2600 - acc: 0.9333 - val_loss: 0.7073 - val_acc: 0.6000\n",
            "Epoch 41/300\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.2707 - acc: 0.9000 - val_loss: 0.7188 - val_acc: 0.6000\n",
            "Epoch 42/300\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.2641 - acc: 0.9333 - val_loss: 0.7215 - val_acc: 0.6500\n",
            "Epoch 43/300\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.2647 - acc: 0.9333 - val_loss: 0.7266 - val_acc: 0.7000\n",
            "Epoch 44/300\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.2660 - acc: 0.9000 - val_loss: 0.7282 - val_acc: 0.7000\n",
            "Epoch 45/300\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.2716 - acc: 0.9000 - val_loss: 0.7222 - val_acc: 0.7000\n",
            "Epoch 46/300\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.2448 - acc: 0.9667 - val_loss: 0.7177 - val_acc: 0.7000\n",
            "Epoch 47/300\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.2486 - acc: 0.9167 - val_loss: 0.7263 - val_acc: 0.7000\n",
            "Epoch 48/300\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.2409 - acc: 0.9333 - val_loss: 0.7273 - val_acc: 0.6500\n",
            "Epoch 49/300\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.2392 - acc: 0.8667 - val_loss: 0.7255 - val_acc: 0.6500\n",
            "Epoch 50/300\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.2724 - acc: 0.9167 - val_loss: 0.7213 - val_acc: 0.7000\n",
            "Epoch 51/300\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.2422 - acc: 0.9000 - val_loss: 0.7134 - val_acc: 0.6500\n",
            "Epoch 52/300\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.2220 - acc: 0.9333 - val_loss: 0.7007 - val_acc: 0.7000\n",
            "Epoch 53/300\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.2859 - acc: 0.9333 - val_loss: 0.6989 - val_acc: 0.7000\n",
            "Epoch 54/300\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.1955 - acc: 0.9500 - val_loss: 0.6982 - val_acc: 0.7000\n",
            "Epoch 55/300\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.2189 - acc: 0.9167 - val_loss: 0.7009 - val_acc: 0.7000\n",
            "Epoch 56/300\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.2391 - acc: 0.9167 - val_loss: 0.7029 - val_acc: 0.7000\n",
            "Epoch 57/300\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.2390 - acc: 0.8833 - val_loss: 0.7135 - val_acc: 0.7000\n",
            "Epoch 58/300\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.1826 - acc: 0.9500 - val_loss: 0.7280 - val_acc: 0.7000\n",
            "Epoch 59/300\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.2097 - acc: 0.9167 - val_loss: 0.7381 - val_acc: 0.7000\n",
            "Epoch 60/300\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.2484 - acc: 0.9000 - val_loss: 0.7420 - val_acc: 0.7000\n",
            "Epoch 61/300\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.2391 - acc: 0.9333 - val_loss: 0.7468 - val_acc: 0.7000\n",
            "Epoch 62/300\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.2065 - acc: 0.9333 - val_loss: 0.7412 - val_acc: 0.7000\n",
            "Epoch 63/300\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.2031 - acc: 0.9500 - val_loss: 0.7288 - val_acc: 0.7000\n",
            "Epoch 64/300\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.2176 - acc: 0.9000 - val_loss: 0.7244 - val_acc: 0.7000\n",
            "Epoch 65/300\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.1849 - acc: 0.9500 - val_loss: 0.7343 - val_acc: 0.7000\n",
            "Epoch 66/300\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.1880 - acc: 0.9500 - val_loss: 0.7418 - val_acc: 0.7000\n",
            "Epoch 67/300\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.1738 - acc: 0.9667 - val_loss: 0.7382 - val_acc: 0.7000\n",
            "Epoch 68/300\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.2389 - acc: 0.9333 - val_loss: 0.7406 - val_acc: 0.7000\n",
            "Epoch 69/300\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.2220 - acc: 0.9500 - val_loss: 0.7473 - val_acc: 0.7000\n",
            "Epoch 70/300\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.1841 - acc: 0.9667 - val_loss: 0.7591 - val_acc: 0.7000\n",
            "Epoch 71/300\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.1836 - acc: 0.9500 - val_loss: 0.7670 - val_acc: 0.7000\n",
            "Epoch 72/300\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.2178 - acc: 0.9167 - val_loss: 0.7763 - val_acc: 0.7000\n",
            "Epoch 73/300\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.1809 - acc: 0.9500 - val_loss: 0.7859 - val_acc: 0.7000\n",
            "Epoch 74/300\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.1893 - acc: 0.9333 - val_loss: 0.7930 - val_acc: 0.7000\n",
            "Epoch 75/300\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.2473 - acc: 0.8833 - val_loss: 0.7951 - val_acc: 0.7000\n",
            "Epoch 76/300\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.1592 - acc: 0.9500 - val_loss: 0.7815 - val_acc: 0.7000\n",
            "Epoch 77/300\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.1435 - acc: 1.0000 - val_loss: 0.7595 - val_acc: 0.7000\n",
            "Epoch 78/300\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.1711 - acc: 0.9500 - val_loss: 0.7528 - val_acc: 0.7000\n",
            "Epoch 79/300\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.1433 - acc: 1.0000 - val_loss: 0.7574 - val_acc: 0.7500\n",
            "Epoch 80/300\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.2083 - acc: 0.9167 - val_loss: 0.7443 - val_acc: 0.7500\n",
            "Epoch 81/300\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.1589 - acc: 0.9667 - val_loss: 0.7444 - val_acc: 0.7500\n",
            "Epoch 82/300\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.2319 - acc: 0.9500 - val_loss: 0.7558 - val_acc: 0.7500\n",
            "Epoch 83/300\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.1205 - acc: 1.0000 - val_loss: 0.7602 - val_acc: 0.7500\n",
            "Epoch 84/300\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.1809 - acc: 0.9333 - val_loss: 0.7462 - val_acc: 0.7500\n",
            "Epoch 85/300\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.1838 - acc: 0.9833 - val_loss: 0.7286 - val_acc: 0.7500\n",
            "Epoch 86/300\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.2079 - acc: 0.9000 - val_loss: 0.7211 - val_acc: 0.7500\n",
            "Epoch 87/300\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.1804 - acc: 0.9500 - val_loss: 0.7043 - val_acc: 0.7500\n",
            "Epoch 88/300\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.1744 - acc: 0.9500 - val_loss: 0.7028 - val_acc: 0.7500\n",
            "Epoch 89/300\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.1479 - acc: 0.9667 - val_loss: 0.7266 - val_acc: 0.7000\n",
            "Epoch 90/300\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.1767 - acc: 0.9333 - val_loss: 0.7344 - val_acc: 0.7500\n",
            "Epoch 91/300\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.1554 - acc: 1.0000 - val_loss: 0.7523 - val_acc: 0.7500\n",
            "Epoch 92/300\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.1521 - acc: 0.9833 - val_loss: 0.7775 - val_acc: 0.7500\n",
            "Epoch 93/300\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.1473 - acc: 0.9667 - val_loss: 0.7971 - val_acc: 0.7500\n",
            "Epoch 94/300\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.1161 - acc: 0.9833 - val_loss: 0.7847 - val_acc: 0.7500\n",
            "Epoch 95/300\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.1307 - acc: 0.9500 - val_loss: 0.7703 - val_acc: 0.7500\n",
            "Epoch 96/300\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.1512 - acc: 0.9833 - val_loss: 0.7540 - val_acc: 0.7500\n",
            "Epoch 97/300\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.1587 - acc: 0.9500 - val_loss: 0.7422 - val_acc: 0.7500\n",
            "Epoch 98/300\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.1810 - acc: 0.9000 - val_loss: 0.7405 - val_acc: 0.7500\n",
            "Epoch 99/300\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.1313 - acc: 0.9667 - val_loss: 0.7313 - val_acc: 0.7500\n",
            "Epoch 100/300\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.1661 - acc: 0.9667 - val_loss: 0.7199 - val_acc: 0.7500\n",
            "Epoch 101/300\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.1550 - acc: 0.9500 - val_loss: 0.7223 - val_acc: 0.7500\n",
            "Epoch 102/300\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.1205 - acc: 0.9667 - val_loss: 0.7430 - val_acc: 0.7500\n",
            "Epoch 103/300\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.1248 - acc: 0.9500 - val_loss: 0.7536 - val_acc: 0.8000\n",
            "Epoch 104/300\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.1620 - acc: 0.9333 - val_loss: 0.7576 - val_acc: 0.8000\n",
            "Epoch 105/300\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.1381 - acc: 0.9833 - val_loss: 0.7788 - val_acc: 0.7500\n",
            "Epoch 106/300\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.1034 - acc: 1.0000 - val_loss: 0.7929 - val_acc: 0.8000\n",
            "Epoch 107/300\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.1617 - acc: 0.9500 - val_loss: 0.7954 - val_acc: 0.7500\n",
            "Epoch 108/300\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.1653 - acc: 0.9500 - val_loss: 0.7811 - val_acc: 0.7500\n",
            "Epoch 109/300\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.1322 - acc: 0.9833 - val_loss: 0.7992 - val_acc: 0.7500\n",
            "Epoch 110/300\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.1400 - acc: 0.9667 - val_loss: 0.8262 - val_acc: 0.8000\n",
            "Epoch 111/300\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.2075 - acc: 0.9333 - val_loss: 0.8273 - val_acc: 0.7500\n",
            "Epoch 112/300\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.1086 - acc: 1.0000 - val_loss: 0.8162 - val_acc: 0.7500\n",
            "Epoch 113/300\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.0927 - acc: 1.0000 - val_loss: 0.8083 - val_acc: 0.7500\n",
            "Epoch 114/300\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.1497 - acc: 0.9500 - val_loss: 0.8104 - val_acc: 0.8000\n",
            "Epoch 115/300\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.1861 - acc: 0.9333 - val_loss: 0.8176 - val_acc: 0.8000\n",
            "Epoch 116/300\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.1202 - acc: 0.9667 - val_loss: 0.8199 - val_acc: 0.7500\n",
            "Epoch 117/300\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.1219 - acc: 0.9833 - val_loss: 0.8162 - val_acc: 0.7500\n",
            "Epoch 118/300\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.1278 - acc: 0.9833 - val_loss: 0.8269 - val_acc: 0.7500\n",
            "Epoch 119/300\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.1503 - acc: 0.9500 - val_loss: 0.8238 - val_acc: 0.7500\n",
            "Epoch 120/300\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.1571 - acc: 0.9667 - val_loss: 0.8059 - val_acc: 0.8000\n",
            "Epoch 121/300\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.1680 - acc: 0.9333 - val_loss: 0.7994 - val_acc: 0.8000\n",
            "Epoch 122/300\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.1196 - acc: 1.0000 - val_loss: 0.8108 - val_acc: 0.8500\n",
            "Epoch 123/300\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.0880 - acc: 0.9833 - val_loss: 0.8289 - val_acc: 0.8000\n",
            "Epoch 124/300\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.1660 - acc: 0.9500 - val_loss: 0.8218 - val_acc: 0.8000\n",
            "Epoch 125/300\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.1623 - acc: 0.9667 - val_loss: 0.8115 - val_acc: 0.8000\n",
            "Epoch 126/300\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.1151 - acc: 0.9833 - val_loss: 0.8002 - val_acc: 0.8000\n",
            "Epoch 127/300\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.1120 - acc: 0.9833 - val_loss: 0.8058 - val_acc: 0.8000\n",
            "Epoch 128/300\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.1047 - acc: 0.9833 - val_loss: 0.8155 - val_acc: 0.8000\n",
            "Epoch 129/300\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.1156 - acc: 0.9833 - val_loss: 0.8234 - val_acc: 0.8000\n",
            "Epoch 130/300\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.1375 - acc: 0.9667 - val_loss: 0.8151 - val_acc: 0.8000\n",
            "Epoch 131/300\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.1258 - acc: 0.9667 - val_loss: 0.8104 - val_acc: 0.8000\n",
            "Epoch 132/300\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.1026 - acc: 0.9833 - val_loss: 0.8080 - val_acc: 0.8000\n",
            "Epoch 133/300\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.1004 - acc: 1.0000 - val_loss: 0.7990 - val_acc: 0.8000\n",
            "Epoch 134/300\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.0845 - acc: 0.9833 - val_loss: 0.8030 - val_acc: 0.8000\n",
            "Epoch 135/300\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.1670 - acc: 0.9333 - val_loss: 0.8169 - val_acc: 0.8000\n",
            "Epoch 136/300\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.1024 - acc: 0.9833 - val_loss: 0.8224 - val_acc: 0.8000\n",
            "Epoch 137/300\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.0677 - acc: 1.0000 - val_loss: 0.8288 - val_acc: 0.8000\n",
            "Epoch 138/300\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.0863 - acc: 1.0000 - val_loss: 0.8297 - val_acc: 0.8000\n",
            "Epoch 139/300\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.0779 - acc: 1.0000 - val_loss: 0.8417 - val_acc: 0.8500\n",
            "Epoch 140/300\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.1340 - acc: 0.9667 - val_loss: 0.8517 - val_acc: 0.8500\n",
            "Epoch 141/300\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.1306 - acc: 0.9667 - val_loss: 0.8309 - val_acc: 0.8500\n",
            "Epoch 142/300\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.0900 - acc: 0.9833 - val_loss: 0.7918 - val_acc: 0.8500\n",
            "Epoch 143/300\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.0866 - acc: 0.9833 - val_loss: 0.7964 - val_acc: 0.8500\n",
            "Epoch 144/300\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.1242 - acc: 0.9500 - val_loss: 0.8300 - val_acc: 0.8500\n",
            "Epoch 145/300\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.1088 - acc: 0.9833 - val_loss: 0.8857 - val_acc: 0.8000\n",
            "Epoch 146/300\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.1009 - acc: 0.9833 - val_loss: 0.9456 - val_acc: 0.8000\n",
            "Epoch 147/300\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.1147 - acc: 0.9667 - val_loss: 0.9915 - val_acc: 0.8000\n",
            "Epoch 148/300\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.1270 - acc: 0.9833 - val_loss: 0.9938 - val_acc: 0.8000\n",
            "Epoch 149/300\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.1188 - acc: 0.9833 - val_loss: 0.9528 - val_acc: 0.8000\n",
            "Epoch 150/300\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.0940 - acc: 1.0000 - val_loss: 0.9258 - val_acc: 0.8000\n",
            "Epoch 151/300\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.1176 - acc: 0.9667 - val_loss: 0.9202 - val_acc: 0.8000\n",
            "Epoch 152/300\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.0962 - acc: 1.0000 - val_loss: 0.9112 - val_acc: 0.8000\n",
            "Epoch 153/300\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.0895 - acc: 0.9833 - val_loss: 0.9202 - val_acc: 0.8000\n",
            "Epoch 154/300\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.0930 - acc: 0.9833 - val_loss: 0.9523 - val_acc: 0.8000\n",
            "Epoch 155/300\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.1136 - acc: 0.9667 - val_loss: 0.9246 - val_acc: 0.8000\n",
            "Epoch 156/300\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.0883 - acc: 1.0000 - val_loss: 0.9246 - val_acc: 0.8000\n",
            "Epoch 157/300\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.1059 - acc: 0.9500 - val_loss: 0.9191 - val_acc: 0.8000\n",
            "Epoch 158/300\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.0746 - acc: 0.9833 - val_loss: 0.9344 - val_acc: 0.8000\n",
            "Epoch 159/300\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.0932 - acc: 0.9833 - val_loss: 0.9537 - val_acc: 0.8000\n",
            "Epoch 160/300\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.0936 - acc: 0.9833 - val_loss: 0.9876 - val_acc: 0.8000\n",
            "Epoch 161/300\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.0769 - acc: 0.9833 - val_loss: 1.0059 - val_acc: 0.8000\n",
            "Epoch 162/300\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.1165 - acc: 0.9833 - val_loss: 0.9925 - val_acc: 0.8000\n",
            "Epoch 163/300\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.0968 - acc: 1.0000 - val_loss: 0.9568 - val_acc: 0.8000\n",
            "Epoch 164/300\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.0545 - acc: 1.0000 - val_loss: 0.9491 - val_acc: 0.8000\n",
            "Epoch 165/300\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.1515 - acc: 0.9333 - val_loss: 0.9355 - val_acc: 0.8000\n",
            "Epoch 166/300\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.0654 - acc: 1.0000 - val_loss: 0.9483 - val_acc: 0.8000\n",
            "Epoch 167/300\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.0614 - acc: 1.0000 - val_loss: 0.9540 - val_acc: 0.8000\n",
            "Epoch 168/300\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.0630 - acc: 1.0000 - val_loss: 0.9801 - val_acc: 0.8000\n",
            "Epoch 169/300\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.0624 - acc: 1.0000 - val_loss: 0.9745 - val_acc: 0.8000\n",
            "Epoch 170/300\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.0595 - acc: 1.0000 - val_loss: 0.9744 - val_acc: 0.8000\n",
            "Epoch 171/300\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.0751 - acc: 1.0000 - val_loss: 0.9829 - val_acc: 0.8000\n",
            "Epoch 172/300\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.0753 - acc: 1.0000 - val_loss: 0.9867 - val_acc: 0.8000\n",
            "Epoch 173/300\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.0764 - acc: 0.9833 - val_loss: 0.9922 - val_acc: 0.8000\n",
            "Epoch 174/300\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.1048 - acc: 0.9833 - val_loss: 1.0043 - val_acc: 0.8000\n",
            "Epoch 175/300\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.1165 - acc: 0.9667 - val_loss: 0.9819 - val_acc: 0.8000\n",
            "Epoch 176/300\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.0905 - acc: 0.9667 - val_loss: 1.0009 - val_acc: 0.8000\n",
            "Epoch 177/300\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.0758 - acc: 1.0000 - val_loss: 1.0003 - val_acc: 0.8000\n",
            "Epoch 178/300\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.1186 - acc: 0.9833 - val_loss: 0.9656 - val_acc: 0.8000\n",
            "Epoch 179/300\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.0740 - acc: 1.0000 - val_loss: 0.9215 - val_acc: 0.8500\n",
            "Epoch 180/300\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.0936 - acc: 0.9833 - val_loss: 0.9276 - val_acc: 0.8500\n",
            "Epoch 181/300\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.0685 - acc: 1.0000 - val_loss: 0.9257 - val_acc: 0.8500\n",
            "Epoch 182/300\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.0646 - acc: 1.0000 - val_loss: 0.9240 - val_acc: 0.8500\n",
            "Epoch 183/300\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.0578 - acc: 1.0000 - val_loss: 0.9197 - val_acc: 0.8000\n",
            "Epoch 184/300\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.0825 - acc: 0.9667 - val_loss: 0.9121 - val_acc: 0.8000\n",
            "Epoch 185/300\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.0838 - acc: 1.0000 - val_loss: 0.9137 - val_acc: 0.8000\n",
            "Epoch 186/300\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.0652 - acc: 1.0000 - val_loss: 0.9382 - val_acc: 0.8000\n",
            "Epoch 187/300\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.2027 - acc: 0.9333 - val_loss: 0.9679 - val_acc: 0.8000\n",
            "Epoch 188/300\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.1070 - acc: 0.9833 - val_loss: 0.9494 - val_acc: 0.8000\n",
            "Epoch 189/300\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.0630 - acc: 1.0000 - val_loss: 0.9216 - val_acc: 0.8000\n",
            "Epoch 190/300\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.0655 - acc: 1.0000 - val_loss: 0.9040 - val_acc: 0.8000\n",
            "Epoch 191/300\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.0683 - acc: 1.0000 - val_loss: 0.8895 - val_acc: 0.8000\n",
            "Epoch 192/300\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.0939 - acc: 0.9500 - val_loss: 0.8729 - val_acc: 0.8000\n",
            "Epoch 193/300\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.0711 - acc: 1.0000 - val_loss: 0.8560 - val_acc: 0.8000\n",
            "Epoch 194/300\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.0669 - acc: 1.0000 - val_loss: 0.8781 - val_acc: 0.8000\n",
            "Epoch 195/300\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.0990 - acc: 0.9667 - val_loss: 0.9365 - val_acc: 0.8000\n",
            "Epoch 196/300\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.1302 - acc: 0.9667 - val_loss: 1.0043 - val_acc: 0.8000\n",
            "Epoch 197/300\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.0481 - acc: 1.0000 - val_loss: 1.0138 - val_acc: 0.8000\n",
            "Epoch 198/300\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.0701 - acc: 1.0000 - val_loss: 1.0275 - val_acc: 0.8000\n",
            "Epoch 199/300\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.0636 - acc: 1.0000 - val_loss: 1.0494 - val_acc: 0.8000\n",
            "Epoch 200/300\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.0619 - acc: 1.0000 - val_loss: 1.0902 - val_acc: 0.8000\n",
            "Epoch 201/300\n",
            "\n",
            "Epoch 00201: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.0554 - acc: 1.0000 - val_loss: 1.1069 - val_acc: 0.8000\n",
            "Epoch 202/300\n",
            "\n",
            "Epoch 00202: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.1475 - acc: 0.9500 - val_loss: 1.0830 - val_acc: 0.8000\n",
            "Epoch 203/300\n",
            "\n",
            "Epoch 00203: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.0837 - acc: 0.9833 - val_loss: 1.0308 - val_acc: 0.8500\n",
            "Epoch 204/300\n",
            "\n",
            "Epoch 00204: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.0842 - acc: 0.9833 - val_loss: 0.9838 - val_acc: 0.9000\n",
            "Epoch 205/300\n",
            "\n",
            "Epoch 00205: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.1375 - acc: 0.9500 - val_loss: 0.9567 - val_acc: 0.9000\n",
            "Epoch 206/300\n",
            "\n",
            "Epoch 00206: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.0631 - acc: 0.9833 - val_loss: 1.0073 - val_acc: 0.8500\n",
            "Epoch 207/300\n",
            "\n",
            "Epoch 00207: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.0790 - acc: 1.0000 - val_loss: 1.0492 - val_acc: 0.8500\n",
            "Epoch 208/300\n",
            "\n",
            "Epoch 00208: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.0634 - acc: 1.0000 - val_loss: 1.0962 - val_acc: 0.8000\n",
            "Epoch 209/300\n",
            "\n",
            "Epoch 00209: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.0793 - acc: 0.9833 - val_loss: 1.1354 - val_acc: 0.8000\n",
            "Epoch 210/300\n",
            "\n",
            "Epoch 00210: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.0671 - acc: 0.9667 - val_loss: 1.1544 - val_acc: 0.8000\n",
            "Epoch 211/300\n",
            "\n",
            "Epoch 00211: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.0598 - acc: 0.9833 - val_loss: 1.2053 - val_acc: 0.8000\n",
            "Epoch 212/300\n",
            "\n",
            "Epoch 00212: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.0756 - acc: 0.9833 - val_loss: 1.2243 - val_acc: 0.8000\n",
            "Epoch 213/300\n",
            "\n",
            "Epoch 00213: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.0774 - acc: 1.0000 - val_loss: 1.1653 - val_acc: 0.8000\n",
            "Epoch 214/300\n",
            "\n",
            "Epoch 00214: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.0657 - acc: 1.0000 - val_loss: 1.1060 - val_acc: 0.8000\n",
            "Epoch 215/300\n",
            "\n",
            "Epoch 00215: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.0496 - acc: 1.0000 - val_loss: 1.0674 - val_acc: 0.8000\n",
            "Epoch 216/300\n",
            "\n",
            "Epoch 00216: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.0830 - acc: 0.9833 - val_loss: 1.0802 - val_acc: 0.8000\n",
            "Epoch 217/300\n",
            "\n",
            "Epoch 00217: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.0530 - acc: 0.9833 - val_loss: 1.1066 - val_acc: 0.8000\n",
            "Epoch 218/300\n",
            "\n",
            "Epoch 00218: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.0579 - acc: 0.9833 - val_loss: 1.1393 - val_acc: 0.8000\n",
            "Epoch 219/300\n",
            "\n",
            "Epoch 00219: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.0664 - acc: 1.0000 - val_loss: 1.1839 - val_acc: 0.8000\n",
            "Epoch 220/300\n",
            "\n",
            "Epoch 00220: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.0501 - acc: 1.0000 - val_loss: 1.2191 - val_acc: 0.8000\n",
            "Epoch 221/300\n",
            "\n",
            "Epoch 00221: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.0870 - acc: 0.9833 - val_loss: 1.2134 - val_acc: 0.8000\n",
            "Epoch 222/300\n",
            "\n",
            "Epoch 00222: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.1253 - acc: 0.9667 - val_loss: 1.2105 - val_acc: 0.8500\n",
            "Epoch 223/300\n",
            "\n",
            "Epoch 00223: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.0666 - acc: 1.0000 - val_loss: 1.1688 - val_acc: 0.8500\n",
            "Epoch 224/300\n",
            "\n",
            "Epoch 00224: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.0573 - acc: 1.0000 - val_loss: 1.1604 - val_acc: 0.8500\n",
            "Epoch 225/300\n",
            "\n",
            "Epoch 00225: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.0553 - acc: 1.0000 - val_loss: 1.1531 - val_acc: 0.8500\n",
            "Epoch 226/300\n",
            "\n",
            "Epoch 00226: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.0650 - acc: 1.0000 - val_loss: 1.1452 - val_acc: 0.8500\n",
            "Epoch 227/300\n",
            "\n",
            "Epoch 00227: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.0734 - acc: 1.0000 - val_loss: 1.1574 - val_acc: 0.8500\n",
            "Epoch 228/300\n",
            "\n",
            "Epoch 00228: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.0996 - acc: 0.9667 - val_loss: 1.1572 - val_acc: 0.8500\n",
            "Epoch 229/300\n",
            "\n",
            "Epoch 00229: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.0905 - acc: 0.9833 - val_loss: 1.2320 - val_acc: 0.8500\n",
            "Epoch 230/300\n",
            "\n",
            "Epoch 00230: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.0731 - acc: 1.0000 - val_loss: 1.3071 - val_acc: 0.8000\n",
            "Epoch 231/300\n",
            "\n",
            "Epoch 00231: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.0620 - acc: 1.0000 - val_loss: 1.3390 - val_acc: 0.8000\n",
            "Epoch 232/300\n",
            "\n",
            "Epoch 00232: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.0845 - acc: 1.0000 - val_loss: 1.3503 - val_acc: 0.8000\n",
            "Epoch 233/300\n",
            "\n",
            "Epoch 00233: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.0411 - acc: 1.0000 - val_loss: 1.3774 - val_acc: 0.8000\n",
            "Epoch 234/300\n",
            "\n",
            "Epoch 00234: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.0925 - acc: 0.9500 - val_loss: 1.3769 - val_acc: 0.8000\n",
            "Epoch 235/300\n",
            "\n",
            "Epoch 00235: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.0653 - acc: 0.9833 - val_loss: 1.2715 - val_acc: 0.8000\n",
            "Epoch 236/300\n",
            "\n",
            "Epoch 00236: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.0602 - acc: 1.0000 - val_loss: 1.2176 - val_acc: 0.8500\n",
            "Epoch 237/300\n",
            "\n",
            "Epoch 00237: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.1008 - acc: 0.9667 - val_loss: 1.2088 - val_acc: 0.8500\n",
            "Epoch 238/300\n",
            "\n",
            "Epoch 00238: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.0497 - acc: 1.0000 - val_loss: 1.1904 - val_acc: 0.8500\n",
            "Epoch 239/300\n",
            "\n",
            "Epoch 00239: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.0439 - acc: 1.0000 - val_loss: 1.1871 - val_acc: 0.8500\n",
            "Epoch 240/300\n",
            "\n",
            "Epoch 00240: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.0533 - acc: 0.9833 - val_loss: 1.2006 - val_acc: 0.8000\n",
            "Epoch 241/300\n",
            "\n",
            "Epoch 00241: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.0272 - acc: 1.0000 - val_loss: 1.2137 - val_acc: 0.8000\n",
            "Epoch 242/300\n",
            "\n",
            "Epoch 00242: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.0511 - acc: 1.0000 - val_loss: 1.2216 - val_acc: 0.8000\n",
            "Epoch 243/300\n",
            "\n",
            "Epoch 00243: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.1262 - acc: 0.9500 - val_loss: 1.2787 - val_acc: 0.8000\n",
            "Epoch 244/300\n",
            "\n",
            "Epoch 00244: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.0703 - acc: 0.9833 - val_loss: 1.3249 - val_acc: 0.8000\n",
            "Epoch 245/300\n",
            "\n",
            "Epoch 00245: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.0991 - acc: 0.9833 - val_loss: 1.3033 - val_acc: 0.8000\n",
            "Epoch 246/300\n",
            "\n",
            "Epoch 00246: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.0594 - acc: 0.9833 - val_loss: 1.3183 - val_acc: 0.8000\n",
            "Epoch 247/300\n",
            "\n",
            "Epoch 00247: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.0607 - acc: 1.0000 - val_loss: 1.3222 - val_acc: 0.8000\n",
            "Epoch 248/300\n",
            "\n",
            "Epoch 00248: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.0489 - acc: 1.0000 - val_loss: 1.2915 - val_acc: 0.8000\n",
            "Epoch 249/300\n",
            "\n",
            "Epoch 00249: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.0464 - acc: 1.0000 - val_loss: 1.2585 - val_acc: 0.8000\n",
            "Epoch 250/300\n",
            "\n",
            "Epoch 00250: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.0465 - acc: 1.0000 - val_loss: 1.2266 - val_acc: 0.8000\n",
            "Epoch 251/300\n",
            "\n",
            "Epoch 00251: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.0860 - acc: 0.9833 - val_loss: 1.2172 - val_acc: 0.8500\n",
            "Epoch 252/300\n",
            "\n",
            "Epoch 00252: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.0791 - acc: 0.9667 - val_loss: 1.1666 - val_acc: 0.8500\n",
            "Epoch 253/300\n",
            "\n",
            "Epoch 00253: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.0566 - acc: 1.0000 - val_loss: 1.1030 - val_acc: 0.8500\n",
            "Epoch 254/300\n",
            "\n",
            "Epoch 00254: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.0653 - acc: 1.0000 - val_loss: 1.0662 - val_acc: 0.8500\n",
            "Epoch 255/300\n",
            "\n",
            "Epoch 00255: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.0495 - acc: 1.0000 - val_loss: 1.0518 - val_acc: 0.9000\n",
            "Epoch 256/300\n",
            "\n",
            "Epoch 00256: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.0633 - acc: 1.0000 - val_loss: 1.0717 - val_acc: 0.9000\n",
            "Epoch 257/300\n",
            "\n",
            "Epoch 00257: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.0488 - acc: 1.0000 - val_loss: 1.1104 - val_acc: 0.9000\n",
            "Epoch 258/300\n",
            "\n",
            "Epoch 00258: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.0569 - acc: 1.0000 - val_loss: 1.1431 - val_acc: 0.9000\n",
            "Epoch 259/300\n",
            "\n",
            "Epoch 00259: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.0750 - acc: 0.9833 - val_loss: 1.2088 - val_acc: 0.9000\n",
            "Epoch 260/300\n",
            "\n",
            "Epoch 00260: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.0488 - acc: 0.9833 - val_loss: 1.2257 - val_acc: 0.9000\n",
            "Epoch 261/300\n",
            "\n",
            "Epoch 00261: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.0595 - acc: 1.0000 - val_loss: 1.2530 - val_acc: 0.9000\n",
            "Epoch 262/300\n",
            "\n",
            "Epoch 00262: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.1102 - acc: 0.9833 - val_loss: 1.3133 - val_acc: 0.9000\n",
            "Epoch 263/300\n",
            "\n",
            "Epoch 00263: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.0547 - acc: 1.0000 - val_loss: 1.3169 - val_acc: 0.9000\n",
            "Epoch 264/300\n",
            "\n",
            "Epoch 00264: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.0682 - acc: 1.0000 - val_loss: 1.2847 - val_acc: 0.9000\n",
            "Epoch 265/300\n",
            "\n",
            "Epoch 00265: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.0717 - acc: 0.9667 - val_loss: 1.2063 - val_acc: 0.9000\n",
            "Epoch 266/300\n",
            "\n",
            "Epoch 00266: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.0361 - acc: 1.0000 - val_loss: 1.1374 - val_acc: 0.9000\n",
            "Epoch 267/300\n",
            "\n",
            "Epoch 00267: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.0679 - acc: 0.9833 - val_loss: 1.0739 - val_acc: 0.9000\n",
            "Epoch 268/300\n",
            "\n",
            "Epoch 00268: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.0428 - acc: 1.0000 - val_loss: 1.0506 - val_acc: 0.9000\n",
            "Epoch 269/300\n",
            "\n",
            "Epoch 00269: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.0637 - acc: 1.0000 - val_loss: 1.0657 - val_acc: 0.9000\n",
            "Epoch 270/300\n",
            "\n",
            "Epoch 00270: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.0556 - acc: 1.0000 - val_loss: 1.1110 - val_acc: 0.9000\n",
            "Epoch 271/300\n",
            "\n",
            "Epoch 00271: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.0641 - acc: 0.9833 - val_loss: 1.1735 - val_acc: 0.9000\n",
            "Epoch 272/300\n",
            "\n",
            "Epoch 00272: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.0871 - acc: 0.9833 - val_loss: 1.2419 - val_acc: 0.9000\n",
            "Epoch 273/300\n",
            "\n",
            "Epoch 00273: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.0720 - acc: 1.0000 - val_loss: 1.2150 - val_acc: 0.9000\n",
            "Epoch 274/300\n",
            "\n",
            "Epoch 00274: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.0629 - acc: 0.9833 - val_loss: 1.2320 - val_acc: 0.9000\n",
            "Epoch 275/300\n",
            "\n",
            "Epoch 00275: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.0503 - acc: 1.0000 - val_loss: 1.2513 - val_acc: 0.9000\n",
            "Epoch 276/300\n",
            "\n",
            "Epoch 00276: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.0396 - acc: 1.0000 - val_loss: 1.2776 - val_acc: 0.8500\n",
            "Epoch 277/300\n",
            "\n",
            "Epoch 00277: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.0417 - acc: 1.0000 - val_loss: 1.2947 - val_acc: 0.8500\n",
            "Epoch 278/300\n",
            "\n",
            "Epoch 00278: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.0329 - acc: 1.0000 - val_loss: 1.3287 - val_acc: 0.8500\n",
            "Epoch 279/300\n",
            "\n",
            "Epoch 00279: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.0469 - acc: 1.0000 - val_loss: 1.3560 - val_acc: 0.8500\n",
            "Epoch 280/300\n",
            "\n",
            "Epoch 00280: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.0408 - acc: 1.0000 - val_loss: 1.3746 - val_acc: 0.8500\n",
            "Epoch 281/300\n",
            "\n",
            "Epoch 00281: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.0300 - acc: 1.0000 - val_loss: 1.4136 - val_acc: 0.8500\n",
            "Epoch 282/300\n",
            "\n",
            "Epoch 00282: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.0295 - acc: 1.0000 - val_loss: 1.4373 - val_acc: 0.8500\n",
            "Epoch 283/300\n",
            "\n",
            "Epoch 00283: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.0592 - acc: 0.9833 - val_loss: 1.4181 - val_acc: 0.8500\n",
            "Epoch 284/300\n",
            "\n",
            "Epoch 00284: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.0409 - acc: 1.0000 - val_loss: 1.4094 - val_acc: 0.8500\n",
            "Epoch 285/300\n",
            "\n",
            "Epoch 00285: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.0510 - acc: 1.0000 - val_loss: 1.4169 - val_acc: 0.8500\n",
            "Epoch 286/300\n",
            "\n",
            "Epoch 00286: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.0495 - acc: 1.0000 - val_loss: 1.4276 - val_acc: 0.8500\n",
            "Epoch 287/300\n",
            "\n",
            "Epoch 00287: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.0283 - acc: 1.0000 - val_loss: 1.4499 - val_acc: 0.8500\n",
            "Epoch 288/300\n",
            "\n",
            "Epoch 00288: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.0461 - acc: 1.0000 - val_loss: 1.4454 - val_acc: 0.8500\n",
            "Epoch 289/300\n",
            "\n",
            "Epoch 00289: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.0436 - acc: 1.0000 - val_loss: 1.4299 - val_acc: 0.8500\n",
            "Epoch 290/300\n",
            "\n",
            "Epoch 00290: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.0673 - acc: 0.9833 - val_loss: 1.3827 - val_acc: 0.8500\n",
            "Epoch 291/300\n",
            "\n",
            "Epoch 00291: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.0696 - acc: 0.9833 - val_loss: 1.3811 - val_acc: 0.8500\n",
            "Epoch 292/300\n",
            "\n",
            "Epoch 00292: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.0289 - acc: 1.0000 - val_loss: 1.3556 - val_acc: 0.8500\n",
            "Epoch 293/300\n",
            "\n",
            "Epoch 00293: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.0533 - acc: 0.9833 - val_loss: 1.3330 - val_acc: 0.8500\n",
            "Epoch 294/300\n",
            "\n",
            "Epoch 00294: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.0550 - acc: 1.0000 - val_loss: 1.2904 - val_acc: 0.8500\n",
            "Epoch 295/300\n",
            "\n",
            "Epoch 00295: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.0326 - acc: 1.0000 - val_loss: 1.2367 - val_acc: 0.8500\n",
            "Epoch 296/300\n",
            "\n",
            "Epoch 00296: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.0508 - acc: 1.0000 - val_loss: 1.2264 - val_acc: 0.8500\n",
            "Epoch 297/300\n",
            "\n",
            "Epoch 00297: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.0481 - acc: 1.0000 - val_loss: 1.2824 - val_acc: 0.8500\n",
            "Epoch 298/300\n",
            "\n",
            "Epoch 00298: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.0465 - acc: 1.0000 - val_loss: 1.3235 - val_acc: 0.8500\n",
            "Epoch 299/300\n",
            "\n",
            "Epoch 00299: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.0375 - acc: 1.0000 - val_loss: 1.3619 - val_acc: 0.8500\n",
            "Epoch 300/300\n",
            "\n",
            "Epoch 00300: val_loss did not improve from 0.68876\n",
            "60/60 - 0s - loss: 0.0360 - acc: 1.0000 - val_loss: 1.3667 - val_acc: 0.8500\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "# Evaluate on test data\n",
            "40/40 [==============================] - 0s 2ms/sample - loss: 0.6414 - acc: 0.6250\n",
            "test loss, test acc: [0.6413899844978005, 0.625]\n",
            "EEG_Deep/Data2A/parsed_P05T.mat\n",
            "Filtering of Training Data Finished\n",
            "EEG_Deep/Data2A/parsed_P05E.mat\n",
            "Filtering of Testing Data Finished\n",
            "(1, 2)\n",
            "Finding labels in training data\n",
            "Finding labels in testing data\n",
            "[2 1 1 2 2 1 1 2 2 2 2 1 1 1 2 2 1 2 2 2]\n",
            "(60, 12, 1536)\n",
            "(60,)\n",
            "(20, 12, 1536)\n",
            "(20,)\n",
            "[[0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]]\n",
            "X_train shape: (60, 1, 12, 1536)\n",
            "60 train samples\n",
            "20 val samples\n",
            "(40, 12, 1536)\n",
            "(40, 1)\n",
            "X_train shape: (40, 1, 12, 1536)\n",
            "40 train samples\n",
            "Train on 60 samples, validate on 20 samples\n",
            "Epoch 1/300\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.69201, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 1s - loss: 0.7718 - acc: 0.4667 - val_loss: 0.6920 - val_acc: 0.6000\n",
            "Epoch 2/300\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.69201 to 0.69043, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6806 - acc: 0.5833 - val_loss: 0.6904 - val_acc: 0.6000\n",
            "Epoch 3/300\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.69043 to 0.69011, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6645 - acc: 0.6500 - val_loss: 0.6901 - val_acc: 0.7000\n",
            "Epoch 4/300\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.69011\n",
            "60/60 - 0s - loss: 0.6564 - acc: 0.6500 - val_loss: 0.6910 - val_acc: 0.6000\n",
            "Epoch 5/300\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.69011\n",
            "60/60 - 0s - loss: 0.6456 - acc: 0.7167 - val_loss: 0.6918 - val_acc: 0.5500\n",
            "Epoch 6/300\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.69011\n",
            "60/60 - 0s - loss: 0.6433 - acc: 0.7000 - val_loss: 0.6926 - val_acc: 0.4000\n",
            "Epoch 7/300\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.69011\n",
            "60/60 - 0s - loss: 0.6042 - acc: 0.7333 - val_loss: 0.6928 - val_acc: 0.4500\n",
            "Epoch 8/300\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.69011\n",
            "60/60 - 0s - loss: 0.6151 - acc: 0.7833 - val_loss: 0.6929 - val_acc: 0.4000\n",
            "Epoch 9/300\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.69011\n",
            "60/60 - 0s - loss: 0.5702 - acc: 0.8833 - val_loss: 0.6942 - val_acc: 0.4000\n",
            "Epoch 10/300\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.69011\n",
            "60/60 - 0s - loss: 0.5900 - acc: 0.8500 - val_loss: 0.6956 - val_acc: 0.4000\n",
            "Epoch 11/300\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.69011\n",
            "60/60 - 0s - loss: 0.5949 - acc: 0.8333 - val_loss: 0.6962 - val_acc: 0.4000\n",
            "Epoch 12/300\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.69011\n",
            "60/60 - 0s - loss: 0.5626 - acc: 0.9333 - val_loss: 0.6961 - val_acc: 0.4000\n",
            "Epoch 13/300\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.69011\n",
            "60/60 - 0s - loss: 0.5467 - acc: 0.8667 - val_loss: 0.6961 - val_acc: 0.4500\n",
            "Epoch 14/300\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.69011\n",
            "60/60 - 0s - loss: 0.5426 - acc: 0.8500 - val_loss: 0.6962 - val_acc: 0.5000\n",
            "Epoch 15/300\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.69011\n",
            "60/60 - 0s - loss: 0.5016 - acc: 0.9000 - val_loss: 0.6964 - val_acc: 0.5500\n",
            "Epoch 16/300\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.69011\n",
            "60/60 - 0s - loss: 0.5233 - acc: 0.9167 - val_loss: 0.6962 - val_acc: 0.5000\n",
            "Epoch 17/300\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.69011\n",
            "60/60 - 0s - loss: 0.5283 - acc: 0.8333 - val_loss: 0.6962 - val_acc: 0.4500\n",
            "Epoch 18/300\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.69011\n",
            "60/60 - 0s - loss: 0.5012 - acc: 0.8333 - val_loss: 0.6960 - val_acc: 0.4500\n",
            "Epoch 19/300\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.69011\n",
            "60/60 - 0s - loss: 0.4729 - acc: 0.9333 - val_loss: 0.6959 - val_acc: 0.5000\n",
            "Epoch 20/300\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.69011\n",
            "60/60 - 0s - loss: 0.5107 - acc: 0.8500 - val_loss: 0.6964 - val_acc: 0.5000\n",
            "Epoch 21/300\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.69011\n",
            "60/60 - 0s - loss: 0.4687 - acc: 0.9333 - val_loss: 0.6973 - val_acc: 0.5000\n",
            "Epoch 22/300\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.69011\n",
            "60/60 - 0s - loss: 0.4788 - acc: 0.9167 - val_loss: 0.6984 - val_acc: 0.5000\n",
            "Epoch 23/300\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.69011\n",
            "60/60 - 0s - loss: 0.4730 - acc: 0.8500 - val_loss: 0.6977 - val_acc: 0.5500\n",
            "Epoch 24/300\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.69011\n",
            "60/60 - 0s - loss: 0.4847 - acc: 0.8667 - val_loss: 0.6965 - val_acc: 0.5500\n",
            "Epoch 25/300\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.69011\n",
            "60/60 - 0s - loss: 0.4424 - acc: 0.9167 - val_loss: 0.6970 - val_acc: 0.5500\n",
            "Epoch 26/300\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.69011\n",
            "60/60 - 0s - loss: 0.4745 - acc: 0.9500 - val_loss: 0.6987 - val_acc: 0.5000\n",
            "Epoch 27/300\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.69011\n",
            "60/60 - 0s - loss: 0.4439 - acc: 0.9667 - val_loss: 0.7017 - val_acc: 0.4500\n",
            "Epoch 28/300\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.69011\n",
            "60/60 - 0s - loss: 0.4594 - acc: 0.8833 - val_loss: 0.7042 - val_acc: 0.4000\n",
            "Epoch 29/300\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.69011\n",
            "60/60 - 0s - loss: 0.4570 - acc: 0.9167 - val_loss: 0.7049 - val_acc: 0.4500\n",
            "Epoch 30/300\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.69011\n",
            "60/60 - 0s - loss: 0.4288 - acc: 0.9333 - val_loss: 0.7041 - val_acc: 0.4500\n",
            "Epoch 31/300\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.69011\n",
            "60/60 - 0s - loss: 0.4356 - acc: 0.9333 - val_loss: 0.7027 - val_acc: 0.4500\n",
            "Epoch 32/300\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.69011\n",
            "60/60 - 0s - loss: 0.4352 - acc: 0.9667 - val_loss: 0.7014 - val_acc: 0.5000\n",
            "Epoch 33/300\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.69011\n",
            "60/60 - 0s - loss: 0.4224 - acc: 0.9500 - val_loss: 0.7006 - val_acc: 0.4500\n",
            "Epoch 34/300\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.69011\n",
            "60/60 - 0s - loss: 0.4129 - acc: 0.9667 - val_loss: 0.6985 - val_acc: 0.4500\n",
            "Epoch 35/300\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.69011\n",
            "60/60 - 0s - loss: 0.4123 - acc: 0.9667 - val_loss: 0.6968 - val_acc: 0.4500\n",
            "Epoch 36/300\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.69011\n",
            "60/60 - 0s - loss: 0.3923 - acc: 1.0000 - val_loss: 0.6951 - val_acc: 0.4500\n",
            "Epoch 37/300\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.69011\n",
            "60/60 - 0s - loss: 0.4096 - acc: 0.9000 - val_loss: 0.6929 - val_acc: 0.4500\n",
            "Epoch 38/300\n",
            "\n",
            "Epoch 00038: val_loss improved from 0.69011 to 0.68980, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4043 - acc: 0.9667 - val_loss: 0.6898 - val_acc: 0.5000\n",
            "Epoch 39/300\n",
            "\n",
            "Epoch 00039: val_loss improved from 0.68980 to 0.68778, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3927 - acc: 0.9667 - val_loss: 0.6878 - val_acc: 0.4500\n",
            "Epoch 40/300\n",
            "\n",
            "Epoch 00040: val_loss improved from 0.68778 to 0.68697, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3914 - acc: 0.9667 - val_loss: 0.6870 - val_acc: 0.4500\n",
            "Epoch 41/300\n",
            "\n",
            "Epoch 00041: val_loss improved from 0.68697 to 0.68644, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3867 - acc: 0.9333 - val_loss: 0.6864 - val_acc: 0.5000\n",
            "Epoch 42/300\n",
            "\n",
            "Epoch 00042: val_loss improved from 0.68644 to 0.68542, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3801 - acc: 0.9833 - val_loss: 0.6854 - val_acc: 0.4500\n",
            "Epoch 43/300\n",
            "\n",
            "Epoch 00043: val_loss improved from 0.68542 to 0.68342, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3504 - acc: 0.9667 - val_loss: 0.6834 - val_acc: 0.5000\n",
            "Epoch 44/300\n",
            "\n",
            "Epoch 00044: val_loss improved from 0.68342 to 0.68313, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3796 - acc: 0.9833 - val_loss: 0.6831 - val_acc: 0.5000\n",
            "Epoch 45/300\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.68313\n",
            "60/60 - 0s - loss: 0.3606 - acc: 0.9500 - val_loss: 0.6842 - val_acc: 0.4500\n",
            "Epoch 46/300\n",
            "\n",
            "Epoch 00046: val_loss improved from 0.68313 to 0.68236, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3551 - acc: 0.9833 - val_loss: 0.6824 - val_acc: 0.4000\n",
            "Epoch 47/300\n",
            "\n",
            "Epoch 00047: val_loss improved from 0.68236 to 0.67565, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3597 - acc: 0.9833 - val_loss: 0.6757 - val_acc: 0.5500\n",
            "Epoch 48/300\n",
            "\n",
            "Epoch 00048: val_loss improved from 0.67565 to 0.67075, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3646 - acc: 0.9500 - val_loss: 0.6708 - val_acc: 0.5500\n",
            "Epoch 49/300\n",
            "\n",
            "Epoch 00049: val_loss improved from 0.67075 to 0.66986, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3606 - acc: 0.9500 - val_loss: 0.6699 - val_acc: 0.5000\n",
            "Epoch 50/300\n",
            "\n",
            "Epoch 00050: val_loss improved from 0.66986 to 0.66742, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3592 - acc: 0.9833 - val_loss: 0.6674 - val_acc: 0.5000\n",
            "Epoch 51/300\n",
            "\n",
            "Epoch 00051: val_loss improved from 0.66742 to 0.66209, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3237 - acc: 0.9667 - val_loss: 0.6621 - val_acc: 0.5500\n",
            "Epoch 52/300\n",
            "\n",
            "Epoch 00052: val_loss improved from 0.66209 to 0.65712, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3426 - acc: 0.9833 - val_loss: 0.6571 - val_acc: 0.6000\n",
            "Epoch 53/300\n",
            "\n",
            "Epoch 00053: val_loss improved from 0.65712 to 0.65018, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3390 - acc: 0.9667 - val_loss: 0.6502 - val_acc: 0.6000\n",
            "Epoch 54/300\n",
            "\n",
            "Epoch 00054: val_loss improved from 0.65018 to 0.64321, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3161 - acc: 0.9833 - val_loss: 0.6432 - val_acc: 0.6000\n",
            "Epoch 55/300\n",
            "\n",
            "Epoch 00055: val_loss improved from 0.64321 to 0.63351, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3321 - acc: 0.9500 - val_loss: 0.6335 - val_acc: 0.6000\n",
            "Epoch 56/300\n",
            "\n",
            "Epoch 00056: val_loss improved from 0.63351 to 0.62764, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3051 - acc: 0.9833 - val_loss: 0.6276 - val_acc: 0.7000\n",
            "Epoch 57/300\n",
            "\n",
            "Epoch 00057: val_loss improved from 0.62764 to 0.62278, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3356 - acc: 0.9167 - val_loss: 0.6228 - val_acc: 0.7000\n",
            "Epoch 58/300\n",
            "\n",
            "Epoch 00058: val_loss improved from 0.62278 to 0.61604, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3386 - acc: 0.9833 - val_loss: 0.6160 - val_acc: 0.7000\n",
            "Epoch 59/300\n",
            "\n",
            "Epoch 00059: val_loss improved from 0.61604 to 0.61254, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3213 - acc: 0.9500 - val_loss: 0.6125 - val_acc: 0.7000\n",
            "Epoch 60/300\n",
            "\n",
            "Epoch 00060: val_loss improved from 0.61254 to 0.60497, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3068 - acc: 0.9667 - val_loss: 0.6050 - val_acc: 0.7500\n",
            "Epoch 61/300\n",
            "\n",
            "Epoch 00061: val_loss improved from 0.60497 to 0.59818, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2973 - acc: 0.9667 - val_loss: 0.5982 - val_acc: 0.7500\n",
            "Epoch 62/300\n",
            "\n",
            "Epoch 00062: val_loss improved from 0.59818 to 0.58681, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2938 - acc: 1.0000 - val_loss: 0.5868 - val_acc: 0.7500\n",
            "Epoch 63/300\n",
            "\n",
            "Epoch 00063: val_loss improved from 0.58681 to 0.57149, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3110 - acc: 0.9833 - val_loss: 0.5715 - val_acc: 0.7500\n",
            "Epoch 64/300\n",
            "\n",
            "Epoch 00064: val_loss improved from 0.57149 to 0.56091, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3259 - acc: 0.9500 - val_loss: 0.5609 - val_acc: 0.7500\n",
            "Epoch 65/300\n",
            "\n",
            "Epoch 00065: val_loss improved from 0.56091 to 0.55576, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2845 - acc: 0.9667 - val_loss: 0.5558 - val_acc: 0.7500\n",
            "Epoch 66/300\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.55576\n",
            "60/60 - 0s - loss: 0.2973 - acc: 0.9667 - val_loss: 0.5565 - val_acc: 0.7500\n",
            "Epoch 67/300\n",
            "\n",
            "Epoch 00067: val_loss improved from 0.55576 to 0.55439, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2738 - acc: 0.9667 - val_loss: 0.5544 - val_acc: 0.7500\n",
            "Epoch 68/300\n",
            "\n",
            "Epoch 00068: val_loss improved from 0.55439 to 0.53873, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2630 - acc: 0.9833 - val_loss: 0.5387 - val_acc: 0.7500\n",
            "Epoch 69/300\n",
            "\n",
            "Epoch 00069: val_loss improved from 0.53873 to 0.52090, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2302 - acc: 0.9833 - val_loss: 0.5209 - val_acc: 0.8000\n",
            "Epoch 70/300\n",
            "\n",
            "Epoch 00070: val_loss improved from 0.52090 to 0.50674, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3036 - acc: 0.9833 - val_loss: 0.5067 - val_acc: 0.8000\n",
            "Epoch 71/300\n",
            "\n",
            "Epoch 00071: val_loss improved from 0.50674 to 0.49305, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2783 - acc: 0.9667 - val_loss: 0.4930 - val_acc: 0.8000\n",
            "Epoch 72/300\n",
            "\n",
            "Epoch 00072: val_loss improved from 0.49305 to 0.48906, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2282 - acc: 0.9667 - val_loss: 0.4891 - val_acc: 0.8000\n",
            "Epoch 73/300\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.48906\n",
            "60/60 - 0s - loss: 0.2444 - acc: 0.9667 - val_loss: 0.4986 - val_acc: 0.8000\n",
            "Epoch 74/300\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.48906\n",
            "60/60 - 0s - loss: 0.3100 - acc: 0.8833 - val_loss: 0.5027 - val_acc: 0.8000\n",
            "Epoch 75/300\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.48906\n",
            "60/60 - 0s - loss: 0.2403 - acc: 0.9833 - val_loss: 0.4934 - val_acc: 0.8000\n",
            "Epoch 76/300\n",
            "\n",
            "Epoch 00076: val_loss improved from 0.48906 to 0.46437, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2788 - acc: 0.9333 - val_loss: 0.4644 - val_acc: 0.8000\n",
            "Epoch 77/300\n",
            "\n",
            "Epoch 00077: val_loss improved from 0.46437 to 0.43309, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2359 - acc: 0.9500 - val_loss: 0.4331 - val_acc: 0.8000\n",
            "Epoch 78/300\n",
            "\n",
            "Epoch 00078: val_loss improved from 0.43309 to 0.42362, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2664 - acc: 0.9333 - val_loss: 0.4236 - val_acc: 0.8000\n",
            "Epoch 79/300\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.42362\n",
            "60/60 - 0s - loss: 0.2801 - acc: 0.9500 - val_loss: 0.4236 - val_acc: 0.8000\n",
            "Epoch 80/300\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.42362\n",
            "60/60 - 0s - loss: 0.2498 - acc: 0.9667 - val_loss: 0.4269 - val_acc: 0.8000\n",
            "Epoch 81/300\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.42362\n",
            "60/60 - 0s - loss: 0.2270 - acc: 0.9833 - val_loss: 0.4263 - val_acc: 0.8000\n",
            "Epoch 82/300\n",
            "\n",
            "Epoch 00082: val_loss improved from 0.42362 to 0.42065, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2167 - acc: 0.9833 - val_loss: 0.4207 - val_acc: 0.8000\n",
            "Epoch 83/300\n",
            "\n",
            "Epoch 00083: val_loss improved from 0.42065 to 0.41694, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2390 - acc: 0.9500 - val_loss: 0.4169 - val_acc: 0.8000\n",
            "Epoch 84/300\n",
            "\n",
            "Epoch 00084: val_loss improved from 0.41694 to 0.41144, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2384 - acc: 0.9833 - val_loss: 0.4114 - val_acc: 0.8000\n",
            "Epoch 85/300\n",
            "\n",
            "Epoch 00085: val_loss improved from 0.41144 to 0.40276, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2330 - acc: 0.9667 - val_loss: 0.4028 - val_acc: 0.8000\n",
            "Epoch 86/300\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.40276\n",
            "60/60 - 0s - loss: 0.2296 - acc: 0.9500 - val_loss: 0.4036 - val_acc: 0.8000\n",
            "Epoch 87/300\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.40276\n",
            "60/60 - 0s - loss: 0.2291 - acc: 0.9333 - val_loss: 0.4055 - val_acc: 0.8000\n",
            "Epoch 88/300\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.40276\n",
            "60/60 - 0s - loss: 0.2161 - acc: 0.9500 - val_loss: 0.4096 - val_acc: 0.8000\n",
            "Epoch 89/300\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.40276\n",
            "60/60 - 0s - loss: 0.2055 - acc: 0.9667 - val_loss: 0.4108 - val_acc: 0.8000\n",
            "Epoch 90/300\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.40276\n",
            "60/60 - 0s - loss: 0.1966 - acc: 0.9833 - val_loss: 0.4158 - val_acc: 0.8000\n",
            "Epoch 91/300\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.40276\n",
            "60/60 - 0s - loss: 0.1829 - acc: 0.9833 - val_loss: 0.4111 - val_acc: 0.8000\n",
            "Epoch 92/300\n",
            "\n",
            "Epoch 00092: val_loss improved from 0.40276 to 0.39117, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2043 - acc: 0.9833 - val_loss: 0.3912 - val_acc: 0.8000\n",
            "Epoch 93/300\n",
            "\n",
            "Epoch 00093: val_loss improved from 0.39117 to 0.37676, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2128 - acc: 0.9667 - val_loss: 0.3768 - val_acc: 0.8500\n",
            "Epoch 94/300\n",
            "\n",
            "Epoch 00094: val_loss improved from 0.37676 to 0.37065, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1670 - acc: 0.9833 - val_loss: 0.3706 - val_acc: 0.8500\n",
            "Epoch 95/300\n",
            "\n",
            "Epoch 00095: val_loss improved from 0.37065 to 0.36194, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2248 - acc: 0.9667 - val_loss: 0.3619 - val_acc: 0.8500\n",
            "Epoch 96/300\n",
            "\n",
            "Epoch 00096: val_loss improved from 0.36194 to 0.36094, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2144 - acc: 0.9500 - val_loss: 0.3609 - val_acc: 0.9000\n",
            "Epoch 97/300\n",
            "\n",
            "Epoch 00097: val_loss improved from 0.36094 to 0.35633, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1935 - acc: 0.9833 - val_loss: 0.3563 - val_acc: 0.9000\n",
            "Epoch 98/300\n",
            "\n",
            "Epoch 00098: val_loss improved from 0.35633 to 0.34731, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1829 - acc: 0.9667 - val_loss: 0.3473 - val_acc: 0.9000\n",
            "Epoch 99/300\n",
            "\n",
            "Epoch 00099: val_loss improved from 0.34731 to 0.33171, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1872 - acc: 0.9833 - val_loss: 0.3317 - val_acc: 0.9500\n",
            "Epoch 100/300\n",
            "\n",
            "Epoch 00100: val_loss improved from 0.33171 to 0.31902, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2179 - acc: 0.9667 - val_loss: 0.3190 - val_acc: 0.9500\n",
            "Epoch 101/300\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.31902\n",
            "60/60 - 0s - loss: 0.1915 - acc: 0.9667 - val_loss: 0.3192 - val_acc: 0.9500\n",
            "Epoch 102/300\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.31902\n",
            "60/60 - 0s - loss: 0.1955 - acc: 0.9667 - val_loss: 0.3313 - val_acc: 0.9500\n",
            "Epoch 103/300\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.31902\n",
            "60/60 - 0s - loss: 0.1963 - acc: 0.9833 - val_loss: 0.3491 - val_acc: 0.9000\n",
            "Epoch 104/300\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.31902\n",
            "60/60 - 0s - loss: 0.1948 - acc: 1.0000 - val_loss: 0.3601 - val_acc: 0.9000\n",
            "Epoch 105/300\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 0.31902\n",
            "60/60 - 0s - loss: 0.2203 - acc: 0.9500 - val_loss: 0.3552 - val_acc: 0.9000\n",
            "Epoch 106/300\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 0.31902\n",
            "60/60 - 0s - loss: 0.2141 - acc: 0.9833 - val_loss: 0.3467 - val_acc: 0.9000\n",
            "Epoch 107/300\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.31902\n",
            "60/60 - 0s - loss: 0.2635 - acc: 0.9167 - val_loss: 0.3568 - val_acc: 0.9000\n",
            "Epoch 108/300\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 0.31902\n",
            "60/60 - 0s - loss: 0.1943 - acc: 0.9667 - val_loss: 0.3573 - val_acc: 0.9000\n",
            "Epoch 109/300\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.31902\n",
            "60/60 - 0s - loss: 0.1465 - acc: 0.9833 - val_loss: 0.3503 - val_acc: 0.9500\n",
            "Epoch 110/300\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.31902\n",
            "60/60 - 0s - loss: 0.1767 - acc: 0.9833 - val_loss: 0.3409 - val_acc: 0.9500\n",
            "Epoch 111/300\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 0.31902\n",
            "60/60 - 0s - loss: 0.2024 - acc: 0.9667 - val_loss: 0.3323 - val_acc: 0.9500\n",
            "Epoch 112/300\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 0.31902\n",
            "60/60 - 0s - loss: 0.1799 - acc: 0.9833 - val_loss: 0.3227 - val_acc: 0.9500\n",
            "Epoch 113/300\n",
            "\n",
            "Epoch 00113: val_loss improved from 0.31902 to 0.31635, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1366 - acc: 0.9833 - val_loss: 0.3164 - val_acc: 0.9500\n",
            "Epoch 114/300\n",
            "\n",
            "Epoch 00114: val_loss improved from 0.31635 to 0.31199, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1801 - acc: 0.9667 - val_loss: 0.3120 - val_acc: 0.9500\n",
            "Epoch 115/300\n",
            "\n",
            "Epoch 00115: val_loss improved from 0.31199 to 0.31190, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1724 - acc: 0.9500 - val_loss: 0.3119 - val_acc: 0.9500\n",
            "Epoch 116/300\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.31190\n",
            "60/60 - 0s - loss: 0.2275 - acc: 0.9500 - val_loss: 0.3194 - val_acc: 0.9500\n",
            "Epoch 117/300\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.31190\n",
            "60/60 - 0s - loss: 0.1638 - acc: 0.9667 - val_loss: 0.3204 - val_acc: 0.9500\n",
            "Epoch 118/300\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.31190\n",
            "60/60 - 0s - loss: 0.1654 - acc: 1.0000 - val_loss: 0.3125 - val_acc: 0.9500\n",
            "Epoch 119/300\n",
            "\n",
            "Epoch 00119: val_loss improved from 0.31190 to 0.30693, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1824 - acc: 0.9500 - val_loss: 0.3069 - val_acc: 0.9500\n",
            "Epoch 120/300\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.30693\n",
            "60/60 - 0s - loss: 0.1453 - acc: 1.0000 - val_loss: 0.3091 - val_acc: 0.9500\n",
            "Epoch 121/300\n",
            "\n",
            "Epoch 00121: val_loss improved from 0.30693 to 0.30261, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1634 - acc: 0.9833 - val_loss: 0.3026 - val_acc: 0.9500\n",
            "Epoch 122/300\n",
            "\n",
            "Epoch 00122: val_loss improved from 0.30261 to 0.29121, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1580 - acc: 0.9667 - val_loss: 0.2912 - val_acc: 0.9500\n",
            "Epoch 123/300\n",
            "\n",
            "Epoch 00123: val_loss improved from 0.29121 to 0.27894, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1651 - acc: 0.9667 - val_loss: 0.2789 - val_acc: 0.9500\n",
            "Epoch 124/300\n",
            "\n",
            "Epoch 00124: val_loss improved from 0.27894 to 0.27033, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1735 - acc: 0.9833 - val_loss: 0.2703 - val_acc: 0.9500\n",
            "Epoch 125/300\n",
            "\n",
            "Epoch 00125: val_loss improved from 0.27033 to 0.26599, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1578 - acc: 0.9833 - val_loss: 0.2660 - val_acc: 0.9500\n",
            "Epoch 126/300\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 0.26599\n",
            "60/60 - 0s - loss: 0.1708 - acc: 0.9500 - val_loss: 0.2717 - val_acc: 0.9500\n",
            "Epoch 127/300\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 0.26599\n",
            "60/60 - 0s - loss: 0.1176 - acc: 1.0000 - val_loss: 0.2715 - val_acc: 0.9500\n",
            "Epoch 128/300\n",
            "\n",
            "Epoch 00128: val_loss improved from 0.26599 to 0.26327, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1439 - acc: 1.0000 - val_loss: 0.2633 - val_acc: 0.9500\n",
            "Epoch 129/300\n",
            "\n",
            "Epoch 00129: val_loss improved from 0.26327 to 0.25913, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1491 - acc: 0.9833 - val_loss: 0.2591 - val_acc: 0.9500\n",
            "Epoch 130/300\n",
            "\n",
            "Epoch 00130: val_loss improved from 0.25913 to 0.25381, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1456 - acc: 0.9833 - val_loss: 0.2538 - val_acc: 0.9500\n",
            "Epoch 131/300\n",
            "\n",
            "Epoch 00131: val_loss improved from 0.25381 to 0.25192, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1591 - acc: 0.9667 - val_loss: 0.2519 - val_acc: 0.9500\n",
            "Epoch 132/300\n",
            "\n",
            "Epoch 00132: val_loss improved from 0.25192 to 0.25124, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1592 - acc: 0.9667 - val_loss: 0.2512 - val_acc: 0.9500\n",
            "Epoch 133/300\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.25124\n",
            "60/60 - 0s - loss: 0.1213 - acc: 1.0000 - val_loss: 0.2563 - val_acc: 0.9500\n",
            "Epoch 134/300\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.25124\n",
            "60/60 - 0s - loss: 0.1681 - acc: 0.9667 - val_loss: 0.2574 - val_acc: 0.9500\n",
            "Epoch 135/300\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 0.25124\n",
            "60/60 - 0s - loss: 0.1461 - acc: 1.0000 - val_loss: 0.2671 - val_acc: 0.9500\n",
            "Epoch 136/300\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.25124\n",
            "60/60 - 0s - loss: 0.1380 - acc: 1.0000 - val_loss: 0.2696 - val_acc: 0.9500\n",
            "Epoch 137/300\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 0.25124\n",
            "60/60 - 0s - loss: 0.1959 - acc: 0.9333 - val_loss: 0.2926 - val_acc: 0.9500\n",
            "Epoch 138/300\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.25124\n",
            "60/60 - 0s - loss: 0.1569 - acc: 0.9833 - val_loss: 0.3022 - val_acc: 0.9500\n",
            "Epoch 139/300\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.25124\n",
            "60/60 - 0s - loss: 0.1224 - acc: 1.0000 - val_loss: 0.3003 - val_acc: 0.9500\n",
            "Epoch 140/300\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 0.25124\n",
            "60/60 - 0s - loss: 0.1431 - acc: 1.0000 - val_loss: 0.2904 - val_acc: 0.9500\n",
            "Epoch 141/300\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.25124\n",
            "60/60 - 0s - loss: 0.1416 - acc: 1.0000 - val_loss: 0.2806 - val_acc: 0.9500\n",
            "Epoch 142/300\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.25124\n",
            "60/60 - 0s - loss: 0.1753 - acc: 0.9500 - val_loss: 0.2778 - val_acc: 0.9500\n",
            "Epoch 143/300\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 0.25124\n",
            "60/60 - 0s - loss: 0.1218 - acc: 1.0000 - val_loss: 0.2752 - val_acc: 0.9500\n",
            "Epoch 144/300\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 0.25124\n",
            "60/60 - 0s - loss: 0.1465 - acc: 0.9833 - val_loss: 0.2743 - val_acc: 0.9500\n",
            "Epoch 145/300\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.25124\n",
            "60/60 - 0s - loss: 0.1428 - acc: 0.9833 - val_loss: 0.2718 - val_acc: 0.9500\n",
            "Epoch 146/300\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.25124\n",
            "60/60 - 0s - loss: 0.1017 - acc: 1.0000 - val_loss: 0.2710 - val_acc: 0.9500\n",
            "Epoch 147/300\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.25124\n",
            "60/60 - 0s - loss: 0.1384 - acc: 0.9833 - val_loss: 0.2764 - val_acc: 0.9500\n",
            "Epoch 148/300\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 0.25124\n",
            "60/60 - 0s - loss: 0.0966 - acc: 1.0000 - val_loss: 0.2822 - val_acc: 0.9500\n",
            "Epoch 149/300\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.25124\n",
            "60/60 - 0s - loss: 0.1523 - acc: 0.9667 - val_loss: 0.2810 - val_acc: 0.9500\n",
            "Epoch 150/300\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.25124\n",
            "60/60 - 0s - loss: 0.1270 - acc: 1.0000 - val_loss: 0.2801 - val_acc: 0.9500\n",
            "Epoch 151/300\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.25124\n",
            "60/60 - 0s - loss: 0.1721 - acc: 0.9500 - val_loss: 0.2850 - val_acc: 0.9500\n",
            "Epoch 152/300\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.25124\n",
            "60/60 - 0s - loss: 0.1458 - acc: 0.9667 - val_loss: 0.3058 - val_acc: 0.9000\n",
            "Epoch 153/300\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 0.25124\n",
            "60/60 - 0s - loss: 0.1331 - acc: 0.9667 - val_loss: 0.3103 - val_acc: 0.9000\n",
            "Epoch 154/300\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.25124\n",
            "60/60 - 0s - loss: 0.1713 - acc: 0.9500 - val_loss: 0.2917 - val_acc: 0.9000\n",
            "Epoch 155/300\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.25124\n",
            "60/60 - 0s - loss: 0.1873 - acc: 0.9667 - val_loss: 0.2764 - val_acc: 0.9500\n",
            "Epoch 156/300\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.25124\n",
            "60/60 - 0s - loss: 0.1536 - acc: 0.9667 - val_loss: 0.2783 - val_acc: 0.9500\n",
            "Epoch 157/300\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.25124\n",
            "60/60 - 0s - loss: 0.1210 - acc: 0.9833 - val_loss: 0.2715 - val_acc: 0.9500\n",
            "Epoch 158/300\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.25124\n",
            "60/60 - 0s - loss: 0.1425 - acc: 0.9667 - val_loss: 0.2718 - val_acc: 0.9500\n",
            "Epoch 159/300\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.25124\n",
            "60/60 - 0s - loss: 0.1070 - acc: 1.0000 - val_loss: 0.2897 - val_acc: 0.9000\n",
            "Epoch 160/300\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.25124\n",
            "60/60 - 0s - loss: 0.1470 - acc: 1.0000 - val_loss: 0.2927 - val_acc: 0.9000\n",
            "Epoch 161/300\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.25124\n",
            "60/60 - 0s - loss: 0.1658 - acc: 0.9833 - val_loss: 0.2945 - val_acc: 0.9000\n",
            "Epoch 162/300\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.25124\n",
            "60/60 - 0s - loss: 0.1412 - acc: 0.9833 - val_loss: 0.2991 - val_acc: 0.9000\n",
            "Epoch 163/300\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.25124\n",
            "60/60 - 0s - loss: 0.0998 - acc: 1.0000 - val_loss: 0.2896 - val_acc: 0.9000\n",
            "Epoch 164/300\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.25124\n",
            "60/60 - 0s - loss: 0.1484 - acc: 0.9667 - val_loss: 0.2724 - val_acc: 0.9500\n",
            "Epoch 165/300\n",
            "\n",
            "Epoch 00165: val_loss improved from 0.25124 to 0.24745, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1216 - acc: 0.9833 - val_loss: 0.2475 - val_acc: 0.9500\n",
            "Epoch 166/300\n",
            "\n",
            "Epoch 00166: val_loss improved from 0.24745 to 0.22557, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1132 - acc: 0.9833 - val_loss: 0.2256 - val_acc: 0.9500\n",
            "Epoch 167/300\n",
            "\n",
            "Epoch 00167: val_loss improved from 0.22557 to 0.21493, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1193 - acc: 0.9833 - val_loss: 0.2149 - val_acc: 0.9500\n",
            "Epoch 168/300\n",
            "\n",
            "Epoch 00168: val_loss improved from 0.21493 to 0.21160, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1157 - acc: 0.9833 - val_loss: 0.2116 - val_acc: 0.9500\n",
            "Epoch 169/300\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 0.21160\n",
            "60/60 - 0s - loss: 0.1344 - acc: 0.9667 - val_loss: 0.2188 - val_acc: 0.9500\n",
            "Epoch 170/300\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.21160\n",
            "60/60 - 0s - loss: 0.1701 - acc: 0.9667 - val_loss: 0.2262 - val_acc: 0.9500\n",
            "Epoch 171/300\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.21160\n",
            "60/60 - 0s - loss: 0.1416 - acc: 1.0000 - val_loss: 0.2324 - val_acc: 0.9500\n",
            "Epoch 172/300\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.21160\n",
            "60/60 - 0s - loss: 0.1179 - acc: 1.0000 - val_loss: 0.2285 - val_acc: 0.9500\n",
            "Epoch 173/300\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.21160\n",
            "60/60 - 0s - loss: 0.2105 - acc: 0.9333 - val_loss: 0.2238 - val_acc: 0.9500\n",
            "Epoch 174/300\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.21160\n",
            "60/60 - 0s - loss: 0.1193 - acc: 1.0000 - val_loss: 0.2311 - val_acc: 0.9500\n",
            "Epoch 175/300\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.21160\n",
            "60/60 - 0s - loss: 0.1048 - acc: 1.0000 - val_loss: 0.2372 - val_acc: 0.9500\n",
            "Epoch 176/300\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 0.21160\n",
            "60/60 - 0s - loss: 0.1484 - acc: 0.9833 - val_loss: 0.2296 - val_acc: 0.9500\n",
            "Epoch 177/300\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.21160\n",
            "60/60 - 0s - loss: 0.1212 - acc: 0.9833 - val_loss: 0.2226 - val_acc: 0.9500\n",
            "Epoch 178/300\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.21160\n",
            "60/60 - 0s - loss: 0.1356 - acc: 1.0000 - val_loss: 0.2252 - val_acc: 0.9500\n",
            "Epoch 179/300\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.21160\n",
            "60/60 - 0s - loss: 0.1151 - acc: 0.9833 - val_loss: 0.2299 - val_acc: 0.9500\n",
            "Epoch 180/300\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.21160\n",
            "60/60 - 0s - loss: 0.1258 - acc: 0.9667 - val_loss: 0.2281 - val_acc: 0.9500\n",
            "Epoch 181/300\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.21160\n",
            "60/60 - 0s - loss: 0.1034 - acc: 0.9833 - val_loss: 0.2313 - val_acc: 0.9500\n",
            "Epoch 182/300\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.21160\n",
            "60/60 - 0s - loss: 0.1506 - acc: 0.9667 - val_loss: 0.2370 - val_acc: 0.9500\n",
            "Epoch 183/300\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.21160\n",
            "60/60 - 0s - loss: 0.1173 - acc: 0.9833 - val_loss: 0.2363 - val_acc: 0.9500\n",
            "Epoch 184/300\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 0.21160\n",
            "60/60 - 0s - loss: 0.1050 - acc: 0.9833 - val_loss: 0.2381 - val_acc: 0.9500\n",
            "Epoch 185/300\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.21160\n",
            "60/60 - 0s - loss: 0.1360 - acc: 0.9667 - val_loss: 0.2436 - val_acc: 0.9500\n",
            "Epoch 186/300\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.21160\n",
            "60/60 - 0s - loss: 0.0982 - acc: 1.0000 - val_loss: 0.2458 - val_acc: 0.9500\n",
            "Epoch 187/300\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 0.21160\n",
            "60/60 - 0s - loss: 0.1097 - acc: 1.0000 - val_loss: 0.2438 - val_acc: 0.9500\n",
            "Epoch 188/300\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.21160\n",
            "60/60 - 0s - loss: 0.1131 - acc: 0.9833 - val_loss: 0.2325 - val_acc: 0.9500\n",
            "Epoch 189/300\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.21160\n",
            "60/60 - 0s - loss: 0.1081 - acc: 0.9833 - val_loss: 0.2294 - val_acc: 0.9500\n",
            "Epoch 190/300\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.21160\n",
            "60/60 - 0s - loss: 0.0780 - acc: 1.0000 - val_loss: 0.2198 - val_acc: 0.9500\n",
            "Epoch 191/300\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.21160\n",
            "60/60 - 0s - loss: 0.0765 - acc: 1.0000 - val_loss: 0.2137 - val_acc: 0.9500\n",
            "Epoch 192/300\n",
            "\n",
            "Epoch 00192: val_loss improved from 0.21160 to 0.21034, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.0915 - acc: 1.0000 - val_loss: 0.2103 - val_acc: 0.9500\n",
            "Epoch 193/300\n",
            "\n",
            "Epoch 00193: val_loss improved from 0.21034 to 0.20599, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.0870 - acc: 1.0000 - val_loss: 0.2060 - val_acc: 0.9500\n",
            "Epoch 194/300\n",
            "\n",
            "Epoch 00194: val_loss improved from 0.20599 to 0.20538, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.0804 - acc: 1.0000 - val_loss: 0.2054 - val_acc: 0.9500\n",
            "Epoch 195/300\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.20538\n",
            "60/60 - 0s - loss: 0.0982 - acc: 0.9833 - val_loss: 0.2123 - val_acc: 0.9500\n",
            "Epoch 196/300\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.20538\n",
            "60/60 - 0s - loss: 0.0676 - acc: 1.0000 - val_loss: 0.2174 - val_acc: 0.9500\n",
            "Epoch 197/300\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.20538\n",
            "60/60 - 0s - loss: 0.0798 - acc: 1.0000 - val_loss: 0.2162 - val_acc: 0.9500\n",
            "Epoch 198/300\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.20538\n",
            "60/60 - 0s - loss: 0.0927 - acc: 1.0000 - val_loss: 0.2196 - val_acc: 0.9500\n",
            "Epoch 199/300\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.20538\n",
            "60/60 - 0s - loss: 0.0735 - acc: 1.0000 - val_loss: 0.2171 - val_acc: 0.9500\n",
            "Epoch 200/300\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.20538\n",
            "60/60 - 0s - loss: 0.1092 - acc: 1.0000 - val_loss: 0.2059 - val_acc: 0.9500\n",
            "Epoch 201/300\n",
            "\n",
            "Epoch 00201: val_loss improved from 0.20538 to 0.20355, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.0796 - acc: 1.0000 - val_loss: 0.2035 - val_acc: 0.9500\n",
            "Epoch 202/300\n",
            "\n",
            "Epoch 00202: val_loss did not improve from 0.20355\n",
            "60/60 - 0s - loss: 0.1274 - acc: 0.9500 - val_loss: 0.2151 - val_acc: 0.9500\n",
            "Epoch 203/300\n",
            "\n",
            "Epoch 00203: val_loss did not improve from 0.20355\n",
            "60/60 - 0s - loss: 0.1033 - acc: 0.9833 - val_loss: 0.2385 - val_acc: 0.9500\n",
            "Epoch 204/300\n",
            "\n",
            "Epoch 00204: val_loss did not improve from 0.20355\n",
            "60/60 - 0s - loss: 0.1457 - acc: 0.9667 - val_loss: 0.2625 - val_acc: 0.9500\n",
            "Epoch 205/300\n",
            "\n",
            "Epoch 00205: val_loss did not improve from 0.20355\n",
            "60/60 - 0s - loss: 0.0875 - acc: 1.0000 - val_loss: 0.2741 - val_acc: 0.9500\n",
            "Epoch 206/300\n",
            "\n",
            "Epoch 00206: val_loss did not improve from 0.20355\n",
            "60/60 - 0s - loss: 0.0927 - acc: 1.0000 - val_loss: 0.2777 - val_acc: 0.9500\n",
            "Epoch 207/300\n",
            "\n",
            "Epoch 00207: val_loss did not improve from 0.20355\n",
            "60/60 - 0s - loss: 0.1047 - acc: 0.9833 - val_loss: 0.2670 - val_acc: 0.9500\n",
            "Epoch 208/300\n",
            "\n",
            "Epoch 00208: val_loss did not improve from 0.20355\n",
            "60/60 - 0s - loss: 0.1230 - acc: 0.9667 - val_loss: 0.2534 - val_acc: 0.9500\n",
            "Epoch 209/300\n",
            "\n",
            "Epoch 00209: val_loss did not improve from 0.20355\n",
            "60/60 - 0s - loss: 0.1119 - acc: 1.0000 - val_loss: 0.2477 - val_acc: 0.9500\n",
            "Epoch 210/300\n",
            "\n",
            "Epoch 00210: val_loss did not improve from 0.20355\n",
            "60/60 - 0s - loss: 0.1242 - acc: 0.9667 - val_loss: 0.2460 - val_acc: 0.9500\n",
            "Epoch 211/300\n",
            "\n",
            "Epoch 00211: val_loss did not improve from 0.20355\n",
            "60/60 - 0s - loss: 0.0877 - acc: 1.0000 - val_loss: 0.2443 - val_acc: 0.9500\n",
            "Epoch 212/300\n",
            "\n",
            "Epoch 00212: val_loss did not improve from 0.20355\n",
            "60/60 - 0s - loss: 0.0988 - acc: 0.9833 - val_loss: 0.2406 - val_acc: 0.9500\n",
            "Epoch 213/300\n",
            "\n",
            "Epoch 00213: val_loss did not improve from 0.20355\n",
            "60/60 - 0s - loss: 0.0942 - acc: 0.9833 - val_loss: 0.2240 - val_acc: 0.9500\n",
            "Epoch 214/300\n",
            "\n",
            "Epoch 00214: val_loss did not improve from 0.20355\n",
            "60/60 - 0s - loss: 0.0843 - acc: 1.0000 - val_loss: 0.2159 - val_acc: 0.9500\n",
            "Epoch 215/300\n",
            "\n",
            "Epoch 00215: val_loss did not improve from 0.20355\n",
            "60/60 - 0s - loss: 0.0976 - acc: 1.0000 - val_loss: 0.2168 - val_acc: 0.9500\n",
            "Epoch 216/300\n",
            "\n",
            "Epoch 00216: val_loss did not improve from 0.20355\n",
            "60/60 - 0s - loss: 0.0801 - acc: 1.0000 - val_loss: 0.2217 - val_acc: 0.9500\n",
            "Epoch 217/300\n",
            "\n",
            "Epoch 00217: val_loss did not improve from 0.20355\n",
            "60/60 - 0s - loss: 0.0982 - acc: 1.0000 - val_loss: 0.2201 - val_acc: 0.9500\n",
            "Epoch 218/300\n",
            "\n",
            "Epoch 00218: val_loss did not improve from 0.20355\n",
            "60/60 - 0s - loss: 0.0746 - acc: 1.0000 - val_loss: 0.2137 - val_acc: 0.9500\n",
            "Epoch 219/300\n",
            "\n",
            "Epoch 00219: val_loss did not improve from 0.20355\n",
            "60/60 - 0s - loss: 0.1412 - acc: 0.9500 - val_loss: 0.2132 - val_acc: 0.9500\n",
            "Epoch 220/300\n",
            "\n",
            "Epoch 00220: val_loss did not improve from 0.20355\n",
            "60/60 - 0s - loss: 0.0983 - acc: 1.0000 - val_loss: 0.2214 - val_acc: 0.9500\n",
            "Epoch 221/300\n",
            "\n",
            "Epoch 00221: val_loss did not improve from 0.20355\n",
            "60/60 - 0s - loss: 0.1417 - acc: 0.9667 - val_loss: 0.2245 - val_acc: 0.9500\n",
            "Epoch 222/300\n",
            "\n",
            "Epoch 00222: val_loss did not improve from 0.20355\n",
            "60/60 - 0s - loss: 0.0698 - acc: 1.0000 - val_loss: 0.2251 - val_acc: 0.9500\n",
            "Epoch 223/300\n",
            "\n",
            "Epoch 00223: val_loss did not improve from 0.20355\n",
            "60/60 - 0s - loss: 0.0814 - acc: 1.0000 - val_loss: 0.2238 - val_acc: 0.9500\n",
            "Epoch 224/300\n",
            "\n",
            "Epoch 00224: val_loss did not improve from 0.20355\n",
            "60/60 - 0s - loss: 0.0929 - acc: 1.0000 - val_loss: 0.2180 - val_acc: 0.9500\n",
            "Epoch 225/300\n",
            "\n",
            "Epoch 00225: val_loss did not improve from 0.20355\n",
            "60/60 - 0s - loss: 0.0796 - acc: 1.0000 - val_loss: 0.2170 - val_acc: 0.9500\n",
            "Epoch 226/300\n",
            "\n",
            "Epoch 00226: val_loss did not improve from 0.20355\n",
            "60/60 - 0s - loss: 0.0851 - acc: 1.0000 - val_loss: 0.2204 - val_acc: 0.9500\n",
            "Epoch 227/300\n",
            "\n",
            "Epoch 00227: val_loss did not improve from 0.20355\n",
            "60/60 - 0s - loss: 0.1020 - acc: 0.9833 - val_loss: 0.2154 - val_acc: 0.9500\n",
            "Epoch 228/300\n",
            "\n",
            "Epoch 00228: val_loss did not improve from 0.20355\n",
            "60/60 - 0s - loss: 0.0882 - acc: 1.0000 - val_loss: 0.2116 - val_acc: 0.9500\n",
            "Epoch 229/300\n",
            "\n",
            "Epoch 00229: val_loss did not improve from 0.20355\n",
            "60/60 - 0s - loss: 0.1140 - acc: 1.0000 - val_loss: 0.2134 - val_acc: 0.9500\n",
            "Epoch 230/300\n",
            "\n",
            "Epoch 00230: val_loss did not improve from 0.20355\n",
            "60/60 - 0s - loss: 0.1041 - acc: 0.9833 - val_loss: 0.2217 - val_acc: 0.9500\n",
            "Epoch 231/300\n",
            "\n",
            "Epoch 00231: val_loss did not improve from 0.20355\n",
            "60/60 - 0s - loss: 0.0876 - acc: 1.0000 - val_loss: 0.2290 - val_acc: 0.9500\n",
            "Epoch 232/300\n",
            "\n",
            "Epoch 00232: val_loss did not improve from 0.20355\n",
            "60/60 - 0s - loss: 0.0705 - acc: 1.0000 - val_loss: 0.2332 - val_acc: 0.9500\n",
            "Epoch 233/300\n",
            "\n",
            "Epoch 00233: val_loss did not improve from 0.20355\n",
            "60/60 - 0s - loss: 0.1094 - acc: 0.9833 - val_loss: 0.2346 - val_acc: 0.9500\n",
            "Epoch 234/300\n",
            "\n",
            "Epoch 00234: val_loss did not improve from 0.20355\n",
            "60/60 - 0s - loss: 0.0875 - acc: 0.9833 - val_loss: 0.2247 - val_acc: 0.9500\n",
            "Epoch 235/300\n",
            "\n",
            "Epoch 00235: val_loss did not improve from 0.20355\n",
            "60/60 - 0s - loss: 0.0819 - acc: 1.0000 - val_loss: 0.2228 - val_acc: 0.9500\n",
            "Epoch 236/300\n",
            "\n",
            "Epoch 00236: val_loss did not improve from 0.20355\n",
            "60/60 - 0s - loss: 0.0944 - acc: 0.9833 - val_loss: 0.2328 - val_acc: 0.9500\n",
            "Epoch 237/300\n",
            "\n",
            "Epoch 00237: val_loss did not improve from 0.20355\n",
            "60/60 - 0s - loss: 0.0978 - acc: 1.0000 - val_loss: 0.2374 - val_acc: 0.9500\n",
            "Epoch 238/300\n",
            "\n",
            "Epoch 00238: val_loss did not improve from 0.20355\n",
            "60/60 - 0s - loss: 0.1032 - acc: 0.9667 - val_loss: 0.2381 - val_acc: 0.9500\n",
            "Epoch 239/300\n",
            "\n",
            "Epoch 00239: val_loss did not improve from 0.20355\n",
            "60/60 - 0s - loss: 0.1094 - acc: 0.9667 - val_loss: 0.2452 - val_acc: 0.9500\n",
            "Epoch 240/300\n",
            "\n",
            "Epoch 00240: val_loss did not improve from 0.20355\n",
            "60/60 - 0s - loss: 0.0899 - acc: 0.9833 - val_loss: 0.2430 - val_acc: 0.9500\n",
            "Epoch 241/300\n",
            "\n",
            "Epoch 00241: val_loss did not improve from 0.20355\n",
            "60/60 - 0s - loss: 0.0917 - acc: 0.9667 - val_loss: 0.2394 - val_acc: 0.9500\n",
            "Epoch 242/300\n",
            "\n",
            "Epoch 00242: val_loss did not improve from 0.20355\n",
            "60/60 - 0s - loss: 0.1099 - acc: 0.9667 - val_loss: 0.2362 - val_acc: 0.9500\n",
            "Epoch 243/300\n",
            "\n",
            "Epoch 00243: val_loss did not improve from 0.20355\n",
            "60/60 - 0s - loss: 0.0874 - acc: 1.0000 - val_loss: 0.2408 - val_acc: 0.9500\n",
            "Epoch 244/300\n",
            "\n",
            "Epoch 00244: val_loss did not improve from 0.20355\n",
            "60/60 - 0s - loss: 0.0580 - acc: 1.0000 - val_loss: 0.2395 - val_acc: 0.9500\n",
            "Epoch 245/300\n",
            "\n",
            "Epoch 00245: val_loss did not improve from 0.20355\n",
            "60/60 - 0s - loss: 0.1125 - acc: 1.0000 - val_loss: 0.2302 - val_acc: 0.9500\n",
            "Epoch 246/300\n",
            "\n",
            "Epoch 00246: val_loss did not improve from 0.20355\n",
            "60/60 - 0s - loss: 0.0726 - acc: 1.0000 - val_loss: 0.2216 - val_acc: 0.9500\n",
            "Epoch 247/300\n",
            "\n",
            "Epoch 00247: val_loss did not improve from 0.20355\n",
            "60/60 - 0s - loss: 0.0725 - acc: 1.0000 - val_loss: 0.2215 - val_acc: 0.9500\n",
            "Epoch 248/300\n",
            "\n",
            "Epoch 00248: val_loss did not improve from 0.20355\n",
            "60/60 - 0s - loss: 0.0957 - acc: 0.9833 - val_loss: 0.2262 - val_acc: 0.9500\n",
            "Epoch 249/300\n",
            "\n",
            "Epoch 00249: val_loss did not improve from 0.20355\n",
            "60/60 - 0s - loss: 0.1183 - acc: 0.9833 - val_loss: 0.2251 - val_acc: 0.9500\n",
            "Epoch 250/300\n",
            "\n",
            "Epoch 00250: val_loss did not improve from 0.20355\n",
            "60/60 - 0s - loss: 0.1124 - acc: 1.0000 - val_loss: 0.2223 - val_acc: 0.9500\n",
            "Epoch 251/300\n",
            "\n",
            "Epoch 00251: val_loss did not improve from 0.20355\n",
            "60/60 - 0s - loss: 0.0544 - acc: 1.0000 - val_loss: 0.2234 - val_acc: 0.9500\n",
            "Epoch 252/300\n",
            "\n",
            "Epoch 00252: val_loss did not improve from 0.20355\n",
            "60/60 - 0s - loss: 0.1009 - acc: 0.9833 - val_loss: 0.2244 - val_acc: 0.9500\n",
            "Epoch 253/300\n",
            "\n",
            "Epoch 00253: val_loss did not improve from 0.20355\n",
            "60/60 - 0s - loss: 0.0995 - acc: 0.9833 - val_loss: 0.2294 - val_acc: 0.9500\n",
            "Epoch 254/300\n",
            "\n",
            "Epoch 00254: val_loss did not improve from 0.20355\n",
            "60/60 - 0s - loss: 0.0895 - acc: 0.9833 - val_loss: 0.2295 - val_acc: 0.9500\n",
            "Epoch 255/300\n",
            "\n",
            "Epoch 00255: val_loss did not improve from 0.20355\n",
            "60/60 - 0s - loss: 0.0649 - acc: 0.9833 - val_loss: 0.2245 - val_acc: 0.9500\n",
            "Epoch 256/300\n",
            "\n",
            "Epoch 00256: val_loss did not improve from 0.20355\n",
            "60/60 - 0s - loss: 0.0751 - acc: 1.0000 - val_loss: 0.2163 - val_acc: 0.9500\n",
            "Epoch 257/300\n",
            "\n",
            "Epoch 00257: val_loss did not improve from 0.20355\n",
            "60/60 - 0s - loss: 0.0713 - acc: 1.0000 - val_loss: 0.2120 - val_acc: 0.9500\n",
            "Epoch 258/300\n",
            "\n",
            "Epoch 00258: val_loss did not improve from 0.20355\n",
            "60/60 - 0s - loss: 0.0890 - acc: 0.9833 - val_loss: 0.2140 - val_acc: 0.9500\n",
            "Epoch 259/300\n",
            "\n",
            "Epoch 00259: val_loss did not improve from 0.20355\n",
            "60/60 - 0s - loss: 0.0694 - acc: 1.0000 - val_loss: 0.2074 - val_acc: 0.9500\n",
            "Epoch 260/300\n",
            "\n",
            "Epoch 00260: val_loss did not improve from 0.20355\n",
            "60/60 - 0s - loss: 0.0824 - acc: 0.9833 - val_loss: 0.2099 - val_acc: 0.9500\n",
            "Epoch 261/300\n",
            "\n",
            "Epoch 00261: val_loss did not improve from 0.20355\n",
            "60/60 - 0s - loss: 0.0782 - acc: 0.9833 - val_loss: 0.2165 - val_acc: 0.9500\n",
            "Epoch 262/300\n",
            "\n",
            "Epoch 00262: val_loss did not improve from 0.20355\n",
            "60/60 - 0s - loss: 0.0853 - acc: 1.0000 - val_loss: 0.2191 - val_acc: 0.9500\n",
            "Epoch 263/300\n",
            "\n",
            "Epoch 00263: val_loss did not improve from 0.20355\n",
            "60/60 - 0s - loss: 0.0915 - acc: 0.9667 - val_loss: 0.2239 - val_acc: 0.9500\n",
            "Epoch 264/300\n",
            "\n",
            "Epoch 00264: val_loss did not improve from 0.20355\n",
            "60/60 - 0s - loss: 0.0919 - acc: 1.0000 - val_loss: 0.2311 - val_acc: 0.9500\n",
            "Epoch 265/300\n",
            "\n",
            "Epoch 00265: val_loss did not improve from 0.20355\n",
            "60/60 - 0s - loss: 0.0717 - acc: 1.0000 - val_loss: 0.2308 - val_acc: 0.9500\n",
            "Epoch 266/300\n",
            "\n",
            "Epoch 00266: val_loss did not improve from 0.20355\n",
            "60/60 - 0s - loss: 0.0787 - acc: 1.0000 - val_loss: 0.2266 - val_acc: 0.9500\n",
            "Epoch 267/300\n",
            "\n",
            "Epoch 00267: val_loss did not improve from 0.20355\n",
            "60/60 - 0s - loss: 0.0877 - acc: 0.9833 - val_loss: 0.2201 - val_acc: 0.9500\n",
            "Epoch 268/300\n",
            "\n",
            "Epoch 00268: val_loss did not improve from 0.20355\n",
            "60/60 - 0s - loss: 0.0733 - acc: 1.0000 - val_loss: 0.2158 - val_acc: 0.9500\n",
            "Epoch 269/300\n",
            "\n",
            "Epoch 00269: val_loss did not improve from 0.20355\n",
            "60/60 - 0s - loss: 0.1110 - acc: 0.9833 - val_loss: 0.2242 - val_acc: 0.9500\n",
            "Epoch 270/300\n",
            "\n",
            "Epoch 00270: val_loss did not improve from 0.20355\n",
            "60/60 - 0s - loss: 0.1039 - acc: 0.9833 - val_loss: 0.2413 - val_acc: 0.9500\n",
            "Epoch 271/300\n",
            "\n",
            "Epoch 00271: val_loss did not improve from 0.20355\n",
            "60/60 - 0s - loss: 0.0945 - acc: 0.9833 - val_loss: 0.2442 - val_acc: 0.9500\n",
            "Epoch 272/300\n",
            "\n",
            "Epoch 00272: val_loss did not improve from 0.20355\n",
            "60/60 - 0s - loss: 0.0855 - acc: 1.0000 - val_loss: 0.2421 - val_acc: 0.9500\n",
            "Epoch 273/300\n",
            "\n",
            "Epoch 00273: val_loss did not improve from 0.20355\n",
            "60/60 - 0s - loss: 0.1025 - acc: 0.9833 - val_loss: 0.2317 - val_acc: 0.9500\n",
            "Epoch 274/300\n",
            "\n",
            "Epoch 00274: val_loss did not improve from 0.20355\n",
            "60/60 - 0s - loss: 0.0661 - acc: 1.0000 - val_loss: 0.2280 - val_acc: 0.9500\n",
            "Epoch 275/300\n",
            "\n",
            "Epoch 00275: val_loss did not improve from 0.20355\n",
            "60/60 - 0s - loss: 0.0601 - acc: 1.0000 - val_loss: 0.2268 - val_acc: 0.9500\n",
            "Epoch 276/300\n",
            "\n",
            "Epoch 00276: val_loss did not improve from 0.20355\n",
            "60/60 - 0s - loss: 0.0819 - acc: 1.0000 - val_loss: 0.2258 - val_acc: 0.9500\n",
            "Epoch 277/300\n",
            "\n",
            "Epoch 00277: val_loss did not improve from 0.20355\n",
            "60/60 - 0s - loss: 0.0855 - acc: 0.9833 - val_loss: 0.2234 - val_acc: 0.9500\n",
            "Epoch 278/300\n",
            "\n",
            "Epoch 00278: val_loss did not improve from 0.20355\n",
            "60/60 - 0s - loss: 0.0992 - acc: 0.9833 - val_loss: 0.2227 - val_acc: 0.9500\n",
            "Epoch 279/300\n",
            "\n",
            "Epoch 00279: val_loss did not improve from 0.20355\n",
            "60/60 - 0s - loss: 0.0916 - acc: 0.9833 - val_loss: 0.2243 - val_acc: 0.9500\n",
            "Epoch 280/300\n",
            "\n",
            "Epoch 00280: val_loss did not improve from 0.20355\n",
            "60/60 - 0s - loss: 0.0800 - acc: 1.0000 - val_loss: 0.2263 - val_acc: 0.9500\n",
            "Epoch 281/300\n",
            "\n",
            "Epoch 00281: val_loss did not improve from 0.20355\n",
            "60/60 - 0s - loss: 0.0742 - acc: 0.9833 - val_loss: 0.2194 - val_acc: 0.9500\n",
            "Epoch 282/300\n",
            "\n",
            "Epoch 00282: val_loss did not improve from 0.20355\n",
            "60/60 - 0s - loss: 0.0780 - acc: 1.0000 - val_loss: 0.2116 - val_acc: 0.9500\n",
            "Epoch 283/300\n",
            "\n",
            "Epoch 00283: val_loss improved from 0.20355 to 0.20253, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.0648 - acc: 1.0000 - val_loss: 0.2025 - val_acc: 0.9500\n",
            "Epoch 284/300\n",
            "\n",
            "Epoch 00284: val_loss improved from 0.20253 to 0.20037, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.0997 - acc: 0.9833 - val_loss: 0.2004 - val_acc: 0.9500\n",
            "Epoch 285/300\n",
            "\n",
            "Epoch 00285: val_loss did not improve from 0.20037\n",
            "60/60 - 0s - loss: 0.0816 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.9500\n",
            "Epoch 286/300\n",
            "\n",
            "Epoch 00286: val_loss did not improve from 0.20037\n",
            "60/60 - 0s - loss: 0.0761 - acc: 1.0000 - val_loss: 0.2073 - val_acc: 0.9500\n",
            "Epoch 287/300\n",
            "\n",
            "Epoch 00287: val_loss did not improve from 0.20037\n",
            "60/60 - 0s - loss: 0.0928 - acc: 1.0000 - val_loss: 0.2110 - val_acc: 0.9500\n",
            "Epoch 288/300\n",
            "\n",
            "Epoch 00288: val_loss did not improve from 0.20037\n",
            "60/60 - 0s - loss: 0.0759 - acc: 1.0000 - val_loss: 0.2071 - val_acc: 0.9500\n",
            "Epoch 289/300\n",
            "\n",
            "Epoch 00289: val_loss did not improve from 0.20037\n",
            "60/60 - 0s - loss: 0.0794 - acc: 0.9833 - val_loss: 0.2063 - val_acc: 0.9500\n",
            "Epoch 290/300\n",
            "\n",
            "Epoch 00290: val_loss did not improve from 0.20037\n",
            "60/60 - 0s - loss: 0.0724 - acc: 1.0000 - val_loss: 0.2020 - val_acc: 0.9500\n",
            "Epoch 291/300\n",
            "\n",
            "Epoch 00291: val_loss did not improve from 0.20037\n",
            "60/60 - 0s - loss: 0.0760 - acc: 1.0000 - val_loss: 0.2042 - val_acc: 0.9500\n",
            "Epoch 292/300\n",
            "\n",
            "Epoch 00292: val_loss did not improve from 0.20037\n",
            "60/60 - 0s - loss: 0.0736 - acc: 1.0000 - val_loss: 0.2058 - val_acc: 0.9500\n",
            "Epoch 293/300\n",
            "\n",
            "Epoch 00293: val_loss did not improve from 0.20037\n",
            "60/60 - 0s - loss: 0.0659 - acc: 1.0000 - val_loss: 0.2151 - val_acc: 0.9500\n",
            "Epoch 294/300\n",
            "\n",
            "Epoch 00294: val_loss did not improve from 0.20037\n",
            "60/60 - 0s - loss: 0.0533 - acc: 1.0000 - val_loss: 0.2208 - val_acc: 0.9500\n",
            "Epoch 295/300\n",
            "\n",
            "Epoch 00295: val_loss did not improve from 0.20037\n",
            "60/60 - 0s - loss: 0.0901 - acc: 0.9833 - val_loss: 0.2197 - val_acc: 0.9500\n",
            "Epoch 296/300\n",
            "\n",
            "Epoch 00296: val_loss did not improve from 0.20037\n",
            "60/60 - 0s - loss: 0.0673 - acc: 1.0000 - val_loss: 0.2160 - val_acc: 0.9500\n",
            "Epoch 297/300\n",
            "\n",
            "Epoch 00297: val_loss did not improve from 0.20037\n",
            "60/60 - 0s - loss: 0.1160 - acc: 0.9500 - val_loss: 0.2227 - val_acc: 0.9500\n",
            "Epoch 298/300\n",
            "\n",
            "Epoch 00298: val_loss did not improve from 0.20037\n",
            "60/60 - 0s - loss: 0.0805 - acc: 1.0000 - val_loss: 0.2290 - val_acc: 0.9500\n",
            "Epoch 299/300\n",
            "\n",
            "Epoch 00299: val_loss did not improve from 0.20037\n",
            "60/60 - 0s - loss: 0.0537 - acc: 1.0000 - val_loss: 0.2313 - val_acc: 0.9500\n",
            "Epoch 300/300\n",
            "\n",
            "Epoch 00300: val_loss did not improve from 0.20037\n",
            "60/60 - 0s - loss: 0.1149 - acc: 1.0000 - val_loss: 0.2187 - val_acc: 0.9500\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAgAElEQVR4nOy9eZxcVZn//35q76WqO+kl6aSzkb2T\nEJZAANkEBIIIisuAKwqiXxW3QQYdRxHHEUdlRNTxhwJCFBF1QFQURUBEloQlBNJZydrp7qTTne7q\nrfbz++Mudau6uruSVPV63q9Xvbruvefee25V1/mc53nOeY4opdBoNBrN5MU12hXQaDQazeiihUCj\n0WgmOVoINBqNZpKjhUCj0WgmOVoINBqNZpKjhUCj0WgmOVoINJMCEZkrIkpEPHmUvVpEnhmJemk0\nYwEtBJoxh4jsFpGYiFRn7X/FbMznjk7NNJqJiRYCzVhlF3CVtSEiK4DS0avO2CAfi0ajOVK0EGjG\nKmuBDzq2PwTc5ywgIhUicp+ItInIHhH5soi4zGNuEfmOiBwSkZ3AW3Oce5eItIjIfhH5TxFx51Mx\nEfm1iLSKSJeIPC0iyxzHSkTku2Z9ukTkGREpMY+dKSLPikiniOwTkavN/U+JyLWOa2S4pkwr6JMi\nsh3Ybu673bxGWEReEpGzHOXdIvIlEXlDRLrN47NE5Ici8t2sZ3lERD6Xz3NrJi5aCDRjleeBkIgs\nNRvoK4GfZ5W5A6gAjgPOwRCOD5vHPgpcCpwIrALelXXuz4AEsMAscyFwLfnxJ2AhUAu8DPzCcew7\nwMnAGcBU4EYgJSJzzPPuAGqAE4ANed4P4O3AaqDB3F5vXmMqcD/waxEJmMc+j2FNXQKEgI8AfcC9\nwFUOsawGLjDP10xmlFL6pV9j6gXsxmigvgx8E7gY+CvgARQwF3ADMaDBcd7HgKfM908AH3ccu9A8\n1wNMA6JAieP4VcCT5vurgWfyrGuled0KjI5VP7AyR7kvAg8Nco2ngGsd2xn3N69/3jD1OGzdF9gK\nXD5Iuc3AW8z3nwIeHe3vW79G/6X9jZqxzFrgaWAeWW4hoBrwAnsc+/YAM833M4B9Wccs5pjntoiI\ntc+VVT4npnXyDeDdGD37lKM+fiAAvJHj1FmD7M+XjLqJyA3ANRjPqTB6/lZwfah73Qu8H0NY3w/c\nfgx10kwQtGtIM2ZRSu3BCBpfAvxf1uFDQByjUbeYDew337dgNIjOYxb7MCyCaqVUpfkKKaWWMTzv\nBS7HsFgqMKwTADHrFAHm5zhv3yD7AXrJDIRPz1HGThNsxgNuBN4DTFFKVQJdZh2Gu9fPgctFZCWw\nFHh4kHKaSYQWAs1Y5xoMt0ivc6dSKgk8CHxDRIKmD/7zpOMIDwKfFpF6EZkC3OQ4twX4C/BdEQmJ\niEtE5ovIOXnUJ4ghIu0Yjfd/Oa6bAu4GbhORGWbQ9nQR8WPEES4QkfeIiEdEqkTkBPPUDcAVIlIq\nIgvMZx6uDgmgDfCIyFcwLAKLnwJfF5GFYnC8iFSZdWzCiC+sBX6rlOrP45k1ExwtBJoxjVLqDaXU\ni4Mcvh6jN70TeAYj6Hm3eewnwGPAqxgB3WyL4oOAD2jE8K//BqjLo0r3YbiZ9pvnPp91/AbgNYzG\ntgP4FuBSSu3FsGz+1dy/AVhpnvM/GPGOAxium18wNI8Bfwa2mXWJkOk6ug1DCP8ChIG7gBLH8XuB\nFRhioNEgSumFaTSayYSInI1hOc1RugHQoC0CjWZSISJe4DPAT7UIaCy0EGg0kwQRWQp0YrjAvjfK\n1dGMIbRrSKPRaCY52iLQaDSaSc64m1BWXV2t5s6dO9rV0Gg0mnHFSy+9dEgpVZPr2LgTgrlz5/Li\ni4ONJtRoNBpNLkRkz2DHtGtIo9FoJjlaCDQajWaSo4VAo9FoJjnjLkaQi3g8TlNTE5FIZLSrMmIE\nAgHq6+vxer2jXRWNRjPOmRBC0NTURDAYZO7cuTjSCk9YlFK0t7fT1NTEvHnzRrs6Go1mnFM015CI\n3C0iB0Xk9UGOi4h8X0R2iMhGETnpaO8ViUSoqqqaFCIAICJUVVVNKgtIo9EUj2LGCH6GsbLUYKzB\nWO5vIXAd8L/HcrPJIgIWk+15NRpN8Siaa0gp9bSIzB2iyOXAfWbiq+dFpFJE6sxc8ROe3mgCt0sI\nePNaL/2o6Ykm+PPrrbzzpJljXjzeaOuhubOfsxbmnPMyKH2xBI++1sqlx9fxyKvNvOukelyuzGdN\nJFPc88/dxJIpPnTGXMr96X/9323Yz7mLavnzphZau6JceeospoUCPPzKfna29YAIV5w4kz0dfcye\nWsq86jKe3XGI6qCfw70xQiVe+mJJ/B4Xy2dWDFrPra3d/HFjM8tnVnDhsuns6+hj24Fuzl86LWf5\n9p4ov3hhL1NKvVx2wkzWPrebgNfNR940j3gqxUMv7+fdq2bhdg38XpVS/OzZ3fREErz/tDlMKfNx\n/wt7ae1KLz8gIrx7VT0v7+1kx4HuAde45Pg6lkw3ljl4dschnt/ZzjmLazl5zhRea+oinkoR8Ljp\niSY4dd5UGpvD/Pn1zJ/vkroQK2dVsrk5zNIZIX794j5SqXRam2kVAS5ZXsfPn99DecDDO0+uZ+1z\ne/C5XXzkzHmklOK3LzXxjpNm8tDL+7nipHp8nnT/VSnF2uf3cKg7au9zuYSrTp3NtFCAh15pYveh\nPt55Uj2zq9Jr//y18QCvNXWyZkUdXf1xKkq8LK0L8dKew/jcLqKJJE9vawNg9XFVTC3z0dkX5/T5\nVQDsbOth3+F+Zk0p4eENzaAU82vLufwEY4G8Qz1RnnujndPnV3G/+R2+beUM1j63h3jSWNQuGPDy\n3tWzufe53URiSbtufq+bq8+Yyx9fa+HS4+t4cP0+OnpjAJy/dBorZ1Xm/H85FkYzRjCTzBzqTea+\nAUIgItdhWA3Mnj07+/Co097ezvnnnw9Aa2srbrebmhqjMVu3bh0+n2/AOW+09QBwfL3xpX74wx/m\npptuYvHixQWt26MbW7jxtxtZMj04ZCM1FvjOY1t5ac9h1v37BUd03h82tnDjbzZyIBzh249tZV51\nGafMnZpRZlNzmG88uhmAmqCf96wyFi9r6ernMw9s4NPnL+T7f9sOgNcjfPD0uXzuwQ1YqbjC/XF+\n9uxuAHZ98xI+9ctXOGFWJVtbu1kyPUhrOMLUMh9rr1k9aD1/+OQOHnm1mTKfm9duvog7ntjO7zY0\ns+XrF+cU6Udebea2v24z7h9J8J2/GO9PnD2F/Z393PR/r1FXWcI5iwYK546DPXzt940AlPo9XNgw\njS899BoA1q2UgoPdUR58cR/JlMJZBaVgb0cf37vyRABu+UMjW1q7+ceOQzz0iTfxjUcb6YslmVrm\n4422Hv5x43l89y9b+duWgxnXL/d7eN/q2dz1zC6uO/s4fvTUGxnHAba0dLP2eWOu0862Xvv9ivoK\nOvti3PR/r9HYEua+5/ZQE/RnCOee9j6+8rtNA54rkVRcd85xfO5XrwLQ3hvlP9++AoBUSvH5X22g\nO5qgsaWb1/d3saC2nJ9fu5p/++1Gqst9JFOK9bsPAzD31WZmTS1l+4Eenv+S8Tu/7a/beGLLQS5e\nPp3/e3m/ff8Llk6jzO/h3md3c8cTO/jAaXPs59l2oMd+b7G3o8/eJ5L+TDY2dfLYpgM8s/0Qj7za\nbB+vDQUmnBDkjVLqTuBOgFWrVo25LHlVVVVs2LABgJtvvpny8nJuuOGGjDLWItEuV25v3D333FOU\nuh0IG3GExubwmBeCxpYwnf3xIz7voPmMuw4Zi5g1NocHCIHzuo3NYfv9gbDRk3x9f5e9ryeSYEtL\nGKXgrg+t4iu/20SX4/wD4SgdvTFe3N1BOJJgapmPA+Fozp55Rj27jXr2xpLs7eijsSVMNJEi3J+g\nonTg6K+Djl5ua1c6HtTYEmb/4X77WXIJgfPcxuYwMyuNdWke+sQZnDh7CgDv+NE/efS1FpIpxY/f\nfxIXL0+vy/O2O57JeGbreltaukmmFAfDUfpiSeJJxb6Ofrr64zS2hLn8hBncborH7Y9v538e38ah\nnhiJlGJLazfTQn5e+JIh9BubOrnsB/+0Gzog431jc5jOPqMn/LsNzQOey7m99ppTbUvywv/5O40t\nYba0dGdcy6LpcD/d0QQA63d30NUfJ5ZM0R9LsrOth5QqI5lSXLZyBgtqy7ntr9to743RHUnQ0Rtj\napmPxpYwfbEkf9t8kLMX1fD+1bO5bu1LbGnt5uQ5U+z7ZT/b1DIfL335Atp6opz6jb/xyKvNeFzC\nplsuwu9xk0imaPjqYzy26QBgCBjA3Vev4rwluS3HQjCa8wj2k7mmbD3p9WYnBDt27KChoYH3ve99\nLFu2jJaWFq677jpWrVrFO84/nR9/77/tsmeeeSYbNmwgkUhQWVnJTTfdxMqVKzn99NM5ePDgUdeh\nrcf4R2psCQ9TcnTpiSbY095HLJEiEk8Of4KDNrMxaDrcB8DmHM8aNhu1YMCT8VlY5zobir5Y0i7T\nMCNEqMRLi8Ol8speo6cYjhiNSWs4Qkdv1L7HUPWsn2I0yBv3d7HtgGEVtvXkDvq3ORo9673P7WJz\nS9h+xlzP6ixfP6XELu8SbFcPQENdyG7sG+oyOwmhEo/9fPFkio7eGPVTSuiPJ9nT3ktbd5RDPVHa\nTHF7fmc7LV0RGupCGdcA2N9pfC+NzWFqgn77+KJpQdwuoas/ztmLaijxuunqj7NqzhSqy31sbgnb\n34NVz7YsIbC2nddtqAsZ5zYb4r5m+XS2tHbbLinrmmuWT7ev29Eb4+ntbaSUcc227ig1Qb/9PN3m\nZ7G5JUxfLGF3Orr64zTUhWiYEcr4Ppz1Pn9JLV632GVFhNpggOpyP139cRbUluP3GC5ij9vF4mlB\n+1kSSaPOoUBxh4mPpkXwCPApEXkAWA10FSI+8LXfb8r4UReChhkhvvq2fNY1H8iWLVu47777WLVq\nFQC33norZaEKNjUd5tr3vI3GxkYaGhoyzunq6uKcc87h1ltv5fOf/zx33303N910U67LD8uhcSIE\nWxz1C0fiRxQ7scSuyeol5xKCiPGDXz2vihd2taOUQkTsz6c1nG6Me6MJNreEqSz1Mj0UIBTw2NcG\neHhDZn/FaoyshnMwDvXEuHjZdH7zchN/3NhMLGH4ig92R1lQGxxQPkMIeqKIwImzK2lsDrO/c/Bn\nNe5lnHv2ohp+/eI+poX8zKsuo8SX/lyXmo1cud9jC5RFKODlYNgQqvaemH2t+1/Yy8t7O+0e9SHz\n2G9fagKwG0TrGpD+XlrDEZbWpZ8z4HVzXHUZ2w/2sHxGiO5InFf2drJsRogSn9uwEPsyxdV6ruzt\nmnKHEMwI8fCGZp59o52pZT7OXVzDn15vZU9HH/Oqy2g0RfHtJ87kT6+32udZz2A1+jVBf8bzgCFm\npT43zuz9S+uCzKwsIWR2Mg73xmhxWHDH11fS0hWhsSWccb2GGSGe3taWIZ7W9V4zLVTrew6VFFcI\nijl89JfAc8BiEWkSkWtE5OMi8nGzyKMYa83uwFhf9hPFqstoMn/+fFsEAH75y1+y+pRVXLnmHHbt\n2EZjY+OAc0pKSlizZg0AJ598Mrt27eJo142wGpPNLeEB10imFIlkimQq/2unzHOGQimjzHDlnDh7\ntuH+zAY1kUzZvblkSmUEGwEOdRuNkfXj29rabd87mVIkU8q+5mnHTaU7kmBfh/EDy+5hTgv56Ysn\naWwOs3S60XszLIL0D/uxTQeozOHKCffHB/2eookkXf1x6qeUcFx1mW36g9GYWp+X81kP9USZFvLb\n9Sz1ummYEWLDvk7auqNUlnrZ2daTYUGlzOdt647i87hYPW8q8aTiya1tdsNvYW0vmR4cEFwPBby2\neFqN7Rnzq/C4hH9sbxvwfH9pPJBxTUg3Xs7PrtrRYDvLL60LZbxvqAuxyRQ852edyyJwu4Qppb4B\n1/xL4wGW1gVta+f1/V0kkikam8PMqy7jxNmGr73CrKf1DBY15X7qKgL28YoSL40tYV43O5pWvaxe\n/tK6EI3NYTZlHV9aF3Q8W1oIrfeDfS/Oz27cWgRKqauGOa6ATxb6vkfbcy8WZWVl9vvt27dz++23\n89e/P0NX0scXP33dgLkAe9p78Xh9tHT1E08quqNJ2sL97DrUy3E15RzqidLVF6ey1MvB7gibXmri\nrmd28enzFvC9x7dz48WL+cajm3n002cR8Lpp6zZ6kt2RBPs7+6mfYoyc2H6gm0vveIZoIoXP4+Kh\nT5zBshnGD6Y7EmfN7f/g9itP4OQ5aV97PJni3G8/RXNXP99990rufXY3V546m6tOzQzgv/+uF/jn\njnYArj1zHu29MSpKvNx82TJufmQTfbEEwYCXtu4o37/K8Cc7e7Y/enIHreEI93/0NH7xwh7+/aHX\nmVNVymcvWMjnH3yV2qCfv3/hzbbVYFkElqBFEynbdL/0jmcAozfrdQsnzTH84+d99ynWXrM6o2EJ\neF3UBgP0RhNsPdDNe0+dAxg/wmyxXD1vKq/u66KtJ2ofS6QU/fEkpb7Mn9V3HtvK02bjafUytx/s\nwe0SkinFD57Yzqd/+Ypdvq4iwFNfOJe27ihzq8s4EDbcMGV+T0bv8bKVM7jvuT38tfEAt/yhkW+/\n63g++YuXiSVTzJpaSk25n2VZPVAnS6YHccnA/WC4dbojCT75i5fZ02F8ljMrS1hQW84/th/KKGs9\nR23Qn9HQhwKejO/Fen4ny2aEeOTVZhpmhGzhaTAtAou3nzCTnz27G7dLaOuOcjAc4W0/eIb/7wOr\naOuOUlXmyxAy52fUUBdi4bRyPC7hesdn/LaVM6gNBqgJ+lk+I8SOth72dfTbz2LVVURYNiPEvsN9\nLKgp56FX9vPQK/sJBjy8eXEtf3q9hXnVZXa97/nnbt5/1wsAXL5yBvc+t4eGGSGaDvfz25czXXDW\n7y3787f2Oz87y81WLMZFsHiiEA6HCQaDlJYF2bF9D8/9/QmuuuKyjDKGz9LowaaUImq6D/rNXl9f\nNElvLIECYgnF/S/uYXNLmCe3HmTrgW7W7e5gZ1svB8IR5lSVcagnxrzqMnufJQTP72wnmkjx0bPm\n8ZN/7OL5nR32P+CBcJSmw/1sag5nCMGOgz22qfr8znZebeqiNnQwQwgi8STPvdHOWQurORiO8tS2\nNg50RagsM4Rg3a4OWrr6qSjxcrA7SiqlcLmExpZuSn1u+mJJnth60O7lWkP49rT3sfa5PShl1G9r\na7c9esLZmFvXaGwJE44k7M9v/e4OggEvJ9RXcsvly/ja7xt59o1DGa6GmqCfUp+bQz1RIvEU0yuM\nRsv5I/zcBYtwCVy8fDqd/XG2tnbz5YfTcya7I4kBQvD45gNsaTUCl9Xlfj5z/kIW1pazcFqQ6+9/\nhW0HeqgJ+vngaXPYeaiXh17Zz9bWbtp7Y5yzqIZ1uzroiyWpDfq5ZEUdh/tieN0uzllUw33P7eGe\nf+4yRPVv2+k1hyHubOtl5axK5teUc+sVKzjcF+dfTpmVUa8yv4effmjVgB4pGEMb+2JJntlxyPaj\nV5cbPvMtrZmusa++rYGuvrgdhLbI5c7IFoKrVs9mRmUJ82vKmVFRQqnPzYqZFSysDfLvlyzF7RLe\nd9psls+s4IktB2hsDvPinsMcCEf5x7Y2DvVEB1yzqtzP9/7lBPZ39nPFSTMJeN384L0nst2MyYjA\nmhVGYPyH7z2JqWVe2rqN4P/Uch///tDr9vMCfOVtDfRGE5T5PZxkPuPymRXMrynnipNm4nEbjpWP\nnnUc1eV+UinFrKmlnLOohtXHVVE/pZT3nDKLqnIfi6aV2/W8eNl0vvvulZx+XFVG/U+ZO4Xvvnsl\nP31mF5tbwrhdQkmRh5lrIRhBTjrpJBoaGjjtpOOpnVHPiaecNmjZaCKJiJBSmS6ReMpo2Ppihqvj\n5b2dQLpHbTWKVrCrJ5rg3Loadrb1ZjSYjS3dVJR4+dIlS3noleYM10w0YTQm2cFPZ5nN5oiM7GDl\ntgPdpBS899TZNLaEueOJHQB0RxOEI3HaeqIc7otz2PT97u3oY9bUUra2GiN9/rH9kO0XTqYUm1u6\nmVtVyu72Pl7e28m86jJ2Heplc0uYlbMq6Y8l6YmmXUnLZ1awYW8njS1h29cL0NkXZ25VKS6XMTR0\n7XOGgDp90NXlfsr8Hntor2WOO83yy0+YwdzqtJU3tSxzaHC4P860UMDejiVS9vXAaAiPqynnU+ct\nNO/po7krwilzp3D9+QvZ2dbDQ6/s55872kmmVMa9Sn0eyvwerjt7PmC4gcp8bvt/4OW9nfjcLvua\nNeVGj/bKUwcfcj3YSBSrN+8cOVQT9Bui8UqmEJy9sCajnvY1cghBtmsoFDDG1wOU+Ny848R6+/1H\nzz7OLveuk+tpbA7z961t6UB5a5i2nuiAa4Lh/3dy8fI6Ll4+8DlPnWd0dBbUwunzqzIGBVgC4wyw\nO98DGXMTZlSW8Mk3L8g4fokpOOV+jz3HwMLncfHOk+sH1ElEeOfJ9fzu1WY2txjfRbHnAGkhKDA3\n33yz/X7BggX2sFIwvuC1a9fSdLiPjt4YbpfYvfBnnjFcGBubOnlmkzGuWCnFBZdewQWXXmH43VMp\nexRBNttazREoDiGwfOcNM0L8YWNLlhCEHb7NYJYQGGKTHfxsbA7bk6asYFbT4X57Qo5Vxrpndk03\n7Q/TnhXs29wSJpFSROIpTjuuKsPtcLA7wt6OPj59/kJ+9OQOEinFhQ3T+MULe23hyw4eVpf7WFBb\nzuaWbrojcU6dN5WX9xwmkVIZDVPDjBDrd3Xg9bjwe1xEEylqyv34PC47AGqVDwbSP5Psxi27N2q5\nNyy2H+wmnhzcNVId9NPcFWGp2cDMqSqjxOu2LaHZU9MNTZk/s1focglLzElQFgtqy5k9tZTmrtYB\n9zoSsp8zGPAQ8LozrAef20UsmRr0Ps7PzeJY6lQT9NMbS/KiOb6/sdkYfrto2sBA+9FSVWbUzyUD\nRX6kscS42IFi0GmoRwWrMc8n/quUIuA1vqZ40hCDXMTM4KgtBD1Re1ji4mlBRNLHkinF1taw/aNu\nmBFi+4Eee8ajNZplgEXQGmbx9CDTKwJ2Gcgc8bO5JUyZz82sKaUDRkM898YhsuPSjY6hkKcdlzn2\n/+U9Rk/3hFkVLKgtt+u6ZHpauLLHlYcCXhpmhNi0v4stLd0smxGye4zOnv3SuhDNXRH2tPexZLrR\nkNQE/ZQ53Dq2ReD4IWY3bkG/B79jpmt2oHtzS+aM3aryzMbFGu1i+YndLmFJXZDndhoxlukVAfv7\nz3Y5AQM+44YZ6aDrMQlBILfgWQHOqWU+akOGK63Mn7s/We7zkN2RPVYhAOzPZnd7Hy1dkWO6ZjY+\nj4sppV6qyv3DzgspNsEcFmmx0EJQBLr6YhkNJRiNaiRujByx/P1KKbojcVq7+mnt6qcnknscuuUf\njGeN8HE2QBZWD3lLazdrnzMsi+kVAarKfOxu7+P2x7dzy+83EYmn7B91Q12IWDLF136/idsf325f\nw9m7Vcpw0zTUhTKG6sFAl9HSuhAulzB7aillPjfza8qYUurl747efkWJl4W15fz59VbWPr8Hj0tY\nPrMCrzv943txTweQOaLEGrO9ucUYF26Jm9V7C5mpAtp7Y/THkyytC9kNhdPX7+zZOhvOUkev22r0\nrR9iideN1535mYsINUG/ff+/NB7gv/+8hWd3HOLlvYd5YN1eAl4Xy2eGqCz12uPFLdIN7MD6gCEU\nlgCU+gb6ia2yp8yd4vis0sJ2tAywfMzvvKrcz7SQn5pyIzA81D1cLiFoioT1+RRCCCD9vM66FYqa\noL/g1zwarP/XYgeKQbuGCk4ylWJPRx8lXjcLHSbrvsN9lPs99EQTpFLGGHalFC1dETsw2mX2Jl1i\nBId6zTiANTrGKlfq8+ASiPvdnDxnChubOm33Q7uZk+SX6/aiFEwPBZg9tZTqcj+/39iMUuBxCZWl\nXlbPM4JUp8ydytQyHw+s20fCnFEJZPjYrdm0S+tCtk9eBCrNIXVgiUXY9s+6XMKaFXVMDwV4Zd9h\nnn3D6MmtmjOFhdOCTCn18pN/7GR3ey/nLq7B73ETDHjtvCrWmO3poQAXLJ3GG209zKsuM+uwh6bD\n/fYksoW15bywq4Og38ObFlQRDHhwiXDavCr+bI4VD/rTjdsJ9ZVMC/npjiS4ePl0NjZ1sWrOVJ43\ne5uQbgyH+0FaM1p/uW4vv1y3FzBy2Uwp9fHinsO8dUUdy2aGBlgHYPiodx7qpa4iHVc4d1EN//dy\nEzVBP9MrApT63HT05rYI3rSgitlTS/ncWxZx029f4+yF1Uwt83FcTRknHkMqAqfls3reVM6YX21v\nv3WF8f9R6nNz2Jz5OxihEi/hSIILltays63XFoajYbH5PxNPKj755gV84Tcb6YsmWFFf2BnzZy6o\nweMeXWsA0h0Q5/9tsdBCUGD644Yl4HSBpJQxtrsnmiCZUsysLCFlikAiqag0x0B3mYHLmVNKjBmw\npr/dEgLr2rVBP6ESL5sPefjt/zuDM775N5rN8caWu0kpY7jfP286DzB6OVtauwl4XWz62sUZZu+M\nyhJe/o+30HS4jzO/9aT943a6hqxef8OMELvajOGEVWV+001jNHDW1H1nj/Y7714JwH/+odEeUvqd\nd6+0g4s3Xrwk4/MLBTy2ELSGI1SWeBER3np8HW893gi8We6QRnPmaU3QmCz1wq4OQiVelkwP8drN\nF9nXrC63rIX0v3tFqddOdQBw7uJaADbu78yoi/F3aBP9m1esIBJP2iIARi6pEq+bD5w2h6+/PUeU\n0uSKk+q54qTMgOGFy6az5etr7G3LXZUdIwAjpvD0jW8GsP8CPPGv5w56z3xwWgT3fPiUDBH6ytsa\ncp2S+zoBL9DPVafOHjCq6EiZXhHgla9caG+vP8KcVPlyJM9XTLI7IsVEu4YKjNVrd7ptrJiA5dYJ\neN32KIBkSuESo5euzPCqWwS3iF3G6xY8Lpd97ezeSvUg5na2iwGMUQ+D+T4tF5Q1UsQZLLZ6/Uum\nB20TvbrcR8OMEFsPGBO4No3YtYsAACAASURBVDkCxdk49w3lHnA2QC1dkZyBssXm+PfGljCNzUbQ\ne6gfje0aysPX6owRWD7aipKBsYJsnDOhL1kxnZQycgrl+iyOFMtdlcsiKBaWCJb53Md037Q1pVfS\nO1KyOyLFRAtBgbHSyTob2+wAb8DrxjqsULhEMhp3t8sQAa9ZyONy4XGLHcz1ZCWuG8yf2eCYxZjL\nF51NIFsInInaWsLMnlpKMOC1g6/GcMIgsUSKneaQTpeQkSvFwrpviXfw4CJk/tPHEqmcP4KA181x\nNeVsbOrkjbYeltaFhvzRWJ9PPo2RNZGpxOu20x2nLYL8GsR3Onr4Q33e+WJbBDliBMWizHQ/Hmsg\ndjhrSjM4oTw6IIVCu4YKgDMN9f7mFsTlprqmGp/bxbp160gk04283+OyG3oLlwhel4uHHvg5Z533\nFju24HG7SKYUbpfgcQhLtkUw2I81Vy98qB5qthB0RxJ2Tp7NzeEBQUgjKZfhn21sNtw02flsLObX\nlONzu9J1VQoSjlnVbj+4XANG5YRKPBCPQNZg1OOn+fnja/txAcun+ejsjeEnRsiThHh/RtlppeAn\nRoU3MeBYNkF3Aj8xagI+u2y5W+EnxlRfasjz/cSI4uXsRTWU+dz0xxMsnupJn2M+4wByPJ+TCo9R\np6Bn+PoXChdQ5U8xs5xjuucUX8qouzsO8fxTjmjS33ul8//W5QV34ZttLQQFwEpDrZTiE//6RUpL\ny7j+s5+3/eDdZipZIb0QjbM5cLmMxv3hB3/O0hUrcYsx2cjrFpIpl/ne+OtxCa6sMXnTQgHbwkgp\n47x4UmVMZ7cmOS0bQgjcLsHndtmWQCyZ4qSv/5VvvGMFu9p77QkxVeU+PC5hWijAcTVlGRkxTxgk\nQOnzuFg0vTw9Q/LPN8ELP04XmH8efOAhppT57PoDvKX/z/CNgb7g24DbrPjqI8af9weAXwy89xpg\nTQD4o/kaArtsHPiG+bkAWwPA9vS+XGwNwP2JN+N1X0rDjBBXdfyIkm+/L13guHPhg7/LPGn9XfDH\nzw9Zpx8CBIAnzdcIsR7gAEM+83B8C/hWAPjv4UpqsjkJ8//uCfMF8Nbb4JRrCn4vLQQFJOGIEFsz\ngu+9916+9/076I9EOevMN/H9O+4gkUjwsWs/zEsvv4JSimuu/Sj1M+rYuul1bvzER/ivYBnr1q1j\nWihgJyCrDfrxe105p5p/4PQ5nDi7ks/+agOdfXGuPGU2Zy6szpj1+JaGadxx1YnDjiQJeF2EI+me\n2+G+OHc+vROl0mPIvW4Xd199CkvqgnjdRgP//K4Omg73897Vg89i/dY7j0cwFav1NZgyF06+Gjb/\nAVqNaf3/75z5XLxsOh+8ex0AcxO7wFMC5/5bxrX640lea+oi4DVSEiRSih0Hu1kyPTRg7LpS5hwI\nM+3xUOzr6OcXL+xhZmUJHzh9jr1/T3sfU0q9Q5rp0fVreYffmOz0n29fwczftUHEfMYtf4QDmwae\ndGAT+Mrh7BsGHjP586ZWNuzt5G0rZwwp5IVmf2c/JV73MU2ssnL4z3H8L2ryZ0trN/NrytPDqutX\nDX3CUTLxhOBPNxmNTCGZvgLW3DpsMees35SC119/nYceeoiH//wkPfEUt9/8BR76za+ZP38+7e3t\n/PbxZwEISYyZ06u57fbb+dLXv827LjxzwJRyv9dN7SD5RqrL/Zy7uJZSr5tO4kyvCHDRsukZZQJe\ntz2VfygCXveAGcUb9hkjaZxupbMdi6EsnR7i12YK36F84s5kWkTCUNsAZ34O+jrggCEEs6aWUj+l\nxE7+VS59UF5jlHNQApzq2PYCSwe5rwD5jgPp2t/Fj599hnOra/jAmek7zBniHAt/y6u2oC2eHgTV\nCzVLjbr3d0LLxoEnRcNQXjvg+Zy81NXIT3btYtXyVSxrKN7iJNnMHL7IsATNl+boWDJ8kYIw8YRg\nFLGCwi6XkSPo8ccfZ/369aw5700oBal4lFmzZnHRRRexY9s2bv3Kv3HWeRdy1RWX4nEZfWWXHP3C\n9KX+Yw8qDrYOQCjgsVe5ysbZ+GfPdB2USBcEzGGVgQojXhCPgDeAiFDqc9MdSVCueo3jI4QVyD6q\n4Gagwngui0gYaivSx5JR+xnTZbqGfT57QlmO4aMaTSGYeEKQR8+9WFgWgdslKGVMsPrIRz7C1df/\nGyJwXE068+ALL73MfQ8+zK/u/SnrnvwT99z1U0AG5IU/EiwBOJbhfoNlOVxq5iXKhWUpVJX5qM13\nlImzAbT+RsN2I2kJQUmqBwKFX6N1MKzP8KjGbg8QAuczhtL7soXAP7R4WvMHykZw+KhmcqGHj+Yg\npRSHeqI5FxlRStHeEyWZStHeG81YfMWyCNxm1tALLriAB371K1oOHsTjctHe3s7evXtpazMSil14\n6dv5xA1f4tUNRp708vJy+nt7j7reheg5WnltshnK5WMlTBtKLDJIpYxG32oArcbS0YhajV4g0TNs\nQ1lILKsqeDQWgT+U7vVbz2gJgCVmTqGwtvO0CHJNKNNoCoHuYuSgL5qgubOfgNdNedaY91gyxf7O\nfhIpxYFwhE5fnPlmQrR4UhnuHdM1tGzZcq65/kau/ZfLcQsE/D5+/OMf43a7+chHriEST4AI3/7W\ntwB43wc/xH/ccD3//dVS1q1bh893ZEG6QvQcLddQbdBPIqX4xtuX84Mnd3DB0sF90xWlXs5fUsu5\nS2rzu0k0DKiBFoGjkbTEzJfoHlHXUKnXzanzpnLy0cyCdT6HtwRUashntLeHeb7j6ytYWheiriK3\na06jOVa0EOTAGvyTvSSi85g1S9jKBwTGqKFP3/glKkq8tPfEiCaSXPKOd/Hxaz5op5GwWPfiS2xp\nNWbiLjazX37s6vfzsavff9T1LhkiOVm+WEIwtczHnz97NpBexGMo7rr6lPxvEjWT1A0lBOazeOM9\nIyoELpfw4MdOP7qTrV5/NAxJMwfPAPdXthCE8xCCSv70mbOOrk4aTR5oIciBNfQzNYhrCMi5zm8i\nmcLjcuEyXUNWbqBcfndnKCB7XsDRYvm3h5q5OxxWXXNlNi0YVoM/hBCU+dy4SOGOj6xFcEw4n8MT\nzdznd8QILJJxiI9sMFyjyYUWghxY7X9uITD+ZswZMJdbTKQUfo/LHsfeF0vgErFTFTjJnllcCIZK\nV5wvVowgO11yQclDCEr9Hsrpyzw+1rGfo9OY++Dcl8s1FMmyjDSaUWLCBItzBXaPlhRWrz/HsRwW\nQcRc2tGwCNIzf/tiyYwEc04kwyI48jrmel47RnAsFoEpIv5BgsYFIU+LICTjVQi68nJ/EenMPKbR\njBITQggCgQDt7e0FE4NsiyAST7KxqZOeaMJhEaRVIhJPklKKRErhcbvshj0STw46Csdq+10iRzxv\nQClFe3s7gUAgY39FiReXHJsQWJbAiLqGvKXg8mQ0khUlXur8We6VsY6zsR/wjCVGnpjs4aXOMhrN\nKDEhXEP19fU0NTXZwzKPle5InK7+BH0BDx0lXsL9ccKRBH0HPfjcLtp7Y7gkHTiOtHko83k40BUh\nWmo0xh29Rr6eSImH7tbcQxHbOvuNhG7hQM7jQxEIBKivz8xj/+5Vs1g0LThgpNORYFsERXUNZfWW\nRQwfejS90tm1Zx1Hx9Td8BjpIZhjHXuuQNjw/wP4Hc8YqEg/Owy0GjSaUWJCCIHX62XevHkFu973\n/7ad2/66jQ+ePodbLl/KJ+9/mT9ubOG/3rGCAC4+/8irGeU/c/5C3nHiTD669ilue89KSn0ePv7I\nSwD897uO5z1LZ+W8z5Vf+wuVpV7+/oU35zx+pFSUeDNSPxwNAVMAcsU1CobVE3bOD8iajDUtFGBa\nZSp9bDzgtGzsUUPOZwzltghGcJ6ERpOLorqGRORiEdkqIjtE5KYcx+eIyN9EZKOIPCUi9bmuM9JY\n6w33mWsLWCtyJVKpAWsRG+US9vq+oYA3Iw3zUGuf+j2uEV1sJB9KfFawuMhC4CvPTKebPSvXKmcd\nGw/YvX7TNeQtA7fDGsw189jar9GMIkX7tYuIGyOD7hqMnF9XiUh27q/vAPcppY4HbgG+Waz6HAlR\nM/jbZ84R2HXIEIJIPEk0Swj8Hhe9sSRhc73hUImXgKMRHWphD7/XNaKLjeRDYKSGj2Y3fhNBCMAh\nBJ3DP+N4fD7NhKSY3dFTgR1KqZ0AIvIAcDnQ6CjTAFjJ2J8EHi5iffLGaux7o0kOdkfoN5eIjORY\nWKOqzEdfNEG3ZRGUeIg6yg0pBB63ndJgrGC5hvyD5Bw6Jt54Arb9BfY+l7uR3P2akT3WYr/hXhtX\nrpNABTStN1xEuZ6xZWP6GZvWg7gM60ijGUWK2QrNBPY5tpuA1VllXgWuAG4H3gEERaRKKdXuLCQi\n1wHXAcyePXi++0JhNeR9sQT7Ovrs/f3xJM7xPW6XECrxGhaBKQTBgBeXpGcbD5XL/cwF1faCMWOF\ngK+IFsFTt0LTi0bDd/x7Mo/NOQN2/h023J+5f+5Z4BpbVtOQzHkTvLzWeH/8uzOPzT4D3ngq8xnn\nnpV71TKNZgQZ7e7oDcAPRORq4GlgP5DMLqSUuhO4E2DVqlWFmzAwCJZrqDeadvmA4RpyO4Z6Wuvv\n9sUSaddQwEPSsS6BtbJYLm6+bFmhq37MWG6toghBfycsvRTec9/AY6f9P+M13rnoG8YrF6d93Hhp\nNGOMYgrBfsA5XKbe3GejlGrGsAgQkXLgnUqpziLWKS+iibRFYPX0wRAC59rBAa/LTpccjsSNMfw+\nj+1KGo8UdfhoHgnWNBrNyFNMm3Q9sFBE5omID7gSe3VZAxGpFhGrDl8E7i5iffLGjhHEkvb6vaU+\nN5F4KiNYHPC6KfNZFkHccAu5ZNDFXcYDdrC4GDOLtRBoNGOSogmBUioBfApjStBm4EGl1CYRuUVE\nLjOLnQtsFZFtwDSOaZnswmG5hvpjSXvZxtqgn/5YMiMQHPC6KfW76TPLWYuZDLa4y3igaEnnEjFI\n9Gsh0GjGIEWNESilHgUezdr3Fcf73wC/KWYd8uUf29v4wF3rePzzZ9uNfa/Z0/d7XFSUeIkkkgQS\n6QayxLYIknRH4vbyhlZcYDwu2G09Q7n/KBZmGQp7Fu3IrTam0WjyY7SDxWOGv2w6AMAz2w/Z7h+l\n4GB3lFCJF7/XTX8smdHbt2IEvVEjWOxc5/bBj53O/JqykX2IAjC7qpR7rj6FMxZUFfbCesy8RjNm\n0ePWTCy3TjiSsF1DAC1d/YQCHkq8biKJgTGCUp+HaCJFR1+MYCCtq6fOm0rVELOKxzJvXlJb+GCx\nlWlzPM0J0GgmCVoITKw1arsj8YzGvrUrYswW9rqIxJIZIhHwuu3UzwfMcppB0BaBRjNm0UJgYvn1\nw/0JYomU3btv6YoQDHhNiyCZkWuoxLQIALqjma4hTRZaCDSaMYsWAhOrge/qNywCa0ZwNJEiFPAQ\nMGMEma4hl20RQNq9pMmBFgKNZsyihcDEcvmEI3Gi8SR1FenUD4ZryG0knYuncJuTypwWAQydV2jS\no4VAoxmzaCEwsXr6Hb0xookUy2dU2A1+KGAJQYpoIkmFGQswJpSlLYKldToQOiiRMIgbfONvJJVG\nM9HRQmBizR1o6YqQSClCJV7qpxgLkIdKjFFDsWSKvlimEDizhy6ZHhz5io8XrFnFR7gsp0ajKT5a\nCEws11CXmVLC73HZQhAMeO21h7v645lC4LAIxtoiM2OKSNf4WXJSo5lkaCEwybXgTH2lMTM4mUzZ\nydiiiZQtBCXmhDLNECQT0HMQett0fECjGaNoITDJXoLS73Vz8pwpAJT6PfaCLQDV5kSxUImXoJmK\n4ayF1SNU03HGA1fBdxbCziehtMCzlTUaTUHQvgyTaCKJ1y3EzbUEfG4XV5w0k+qgj3MX1fKH11rs\nsnOqSvn5NatZNXcKAa+b+69dzcpZOodOTg5tg5mr4ISrjEVYNBrNmENbBCbRRIrFjmCv3+tCRDhv\nyTQjtbQjG6fP4+LMhdV2yuYzFlRTNsaWnBwzRMIw40Q45VqoWTzatdFoNDnQQmASjaco9XrsIaPZ\nuXZKHLGAoi7sPpFQSgeJNZpxgG7RTKKJJH6vi+OqjXHuvqzG3rnYTFFW75qIxHpBJXWQWKMZ42gh\nMIkmUvg9LubXlANwuDeWcbzEqy2CI0bPJtZoxgXasW0STaTweVx87oJFbD3QzZsWZI4CmjWllPk1\nZUTiKZbU6YljeaGFQKMZF2ghMIkmkvg9bhZOC/LkDecOOF5R6uVv/zpwv2YItBBoNOMC7eMwicZT\n2uVTaCwh8Gsh0GjGMrrlM7FiBJoCYq9TrIVAoxnL6JbPxBg1pEcDFRTtGtJoxgVaCACllLYIioG1\nTrGeR6DRjGl0ywfEkwql9LDQghPpAk8JePSCPRrNWEa3fKRTUOuJYgVGzyrWaMYFRRUCEblYRLaK\nyA4RuSnH8dki8qSIvCIiG0XkkmLWZzCszKN+r9bFgmItRqPRaMY0RWv5RMQN/BBYAzQAV4lIQ1ax\nLwMPKqVOBK4EflSs+gyFtRaBdg0VGC0EGs24oJgTyk4FdiildgKIyAPA5UCjo4wCLN9BBdBcxPoM\nSloIJplrKNYLf7sFoj3FuX7r6zDjhOJcW6PRFIxiCsFMYJ9juwlYnVXmZuAvInI9UAZckOtCInId\ncB3A7NmzC17RdIxgklkETevhhR9DWQ24ixDQ9QRg/nmFv65Goykoo51i4irgZ0qp74rI6cBaEVmu\nlMpYLkwpdSdwJ8CqVatUoSthLVw/6WIE1jj/DzwM05ePbl00Gs2oUcyWbz8wy7Fdb+5zcg3wIIBS\n6jkgAIz4mo+Wa8jnnmSuIT3hS6PRUFwhWA8sFJF5IuLDCAY/klVmL3A+gIgsxRCCtiLWKSe2a2iy\nWgRaCDSaSU3RWj6lVAL4FPAYsBljdNAmEblFRC4zi/0r8FEReRX4JXC1Uqrgrp/hsF1Dky1GEOkC\ncYGvfLRrotFoRpGixgiUUo8Cj2bt+4rjfSPwpmLWIR8ik3VCWSQM/hC4JpkAajSaDHQLAPREEgAE\nA6MdOx9h9MxfjUaDFgIAwpE4AKES7yjXZITRE740Gg1aCAAI9ydwCZT5JptrqAsClaNdC41GM8po\nIQC6I3GCAS8iMtpVGVm0RaDRaNBCAEA4kiBUMsniA6CFQKPRAFoIAAj3xwkFJll8ALQQaDQaQAsB\nYASLJ50QpJIQ69ZCoNFohhcCEbleRKaMRGVGi3B/YvINHbUWlvfr4aMazWQnH4tgGrBeRB40F5qZ\ncBHV7kh8cg4dBW0RaDSa4WcWK6W+LCL/AVwIfBj4gYg8CNyllHqj2BUcCcKRxMRzDSkFXfsgGc99\nvG2r8VcLgUYz6cnLH6KUUiLSCrQCCWAK8BsR+atS6sZiVrDYJJIpeqITcNRQ4+/g1x8avlxZTfHr\notFoxjTDtn4i8hngg8Ah4KfAF5RScRFxAduBcS0EPVEjvcSEswg69xh/L/sBeAZZdMZXDvWnjFyd\nNBrNmCSfbvBU4Aql1B7nTqVUSkQuLU61Ro5wvykEEy1GEOkCccOJ74eJF9bRaDQFJJ9g8Z+ADmtD\nREIishpAKbW5WBUbKaw8QxNu1JCVUE6LgEajGYZ8hOB/Aefq5j3mvgmBnXBuormG9GQxjUaTJ/kI\ngTgXizHXE54w3ee0a2jCPJKBFgKNRpMn+QjBThH5tIh4zddngJ3FrthIcbgvBkBlqW+Ua1JgImEt\nBBqNJi/yEYKPA2dgLDzfBKwGritmpUaSQ91RAKrLJ5oQaItAo9HkRz4Tyg5iLDw/IWnriVJR4p14\ny1RqIdBoNHmSzzyCAHANsAwIWPuVUh8pYr1GjEM9UWqCg4yzH8/oRWc0Gk2e5OMaWgtMBy4C/g7U\nA93FrNRIoJSiOxKnrTtKTfkEE4JkHOK9OqGcRqPJi3yEYIFS6j+AXqXUvcBbMeIE45r71+1lxc1/\nYWNTF9UTzSKImjqtXUMajSYP8hECK2tZp4gsByqA2uJVaWR49o12AKKJ1MSzCCKdxl8tBBqNJg/y\nGTx/p7kewZeBR4By4D+KWqsRYPbUUvv9hIsR6BTTGo3mCBhSCMzEcmGl1GHgaeC4I7m4iFwM3A64\ngZ8qpW7NOv4/wJvNzVKgVik1IhFOZ+KFCTl0FLQQaDSavBhSCMzEcjcCDx7phUXEDfwQeAvG/IP1\nIvKIUqrRcf3POcpfD5x4pPc5WvpiSfu9tgg0Gs1kJh/X0OMicgPwK6DX2qmU6hj8FABOBXYopXYC\niMgDwOVA4yDlrwK+mkd9jo3De2DD/fRF19i7JoQQHGiEp78NqYSxIA1oIdBoNHmRjxD8i/n3k459\niuHdRDOBfY5ta1byAERkDjAPeGKQ49dhzmaePXv28DUeii1/hL/fivs4Iw//BUtrmV9TfmzXHAts\n+SNs+j+oWQIIzD8PgtNHu1YajWYckM/M4nkjUI8rgd8opZK5Diql7gTuBFi1apXKVSZvEhEA4tF+\njq+fwk8/NEEWZklEjPUHPvnCaNdEo9GMM/KZWfzBXPuVUvcNc+p+YJZju97cl4srybQ4ikfCyC0U\nj/VT6qsekVuOCIkIeALDl9NoNJos8nENObvMAeB84GVgOCFYDywUkXkYAnAl8N7sQiKyBGMN5Ofy\nqfAxY1oEqVg/pWUTKPV0Ijr4kpQajUYzBPm4hq53botIJfBAHuclRORTwGMYw0fvVkptEpFbgBeV\nUo+YRa8EHnCueVBUTIsgGYtQ6ptAiea0RaDRaI6So+kS92IEdodFKfUo8GjWvq9kbd98FHU4epKG\nEKTiEYK+iWYRTLD5EBqNZkTIJ0bwe4xRQmCkpGjgKOYVjBlMi0DFI5T6tUWg0Wg0+XSJv+N4nwD2\nKKWailSf4mPGCEhEKZtIFkEypmMEGo3mqMinJdwLtCilIgAiUiIic5VSu4tas2JhWgQeFdcWgUaj\n0ZBf9tFfAynHdtLcNz4xLQI/sYllEehRQxqN5ijJRwg8SqmYtWG+H79RSdMi8EucEj1qSKPRaPIS\ngjYRuczaEJHLgUPFq1KRsS2CuLYINBqNhvxiBB8HfiEiPzC3m4Ccs43HBQ4h0DECjUajyW9C2RvA\naSJSbm73FL1WxcRyDWmLQKPRaIA8XEMi8l8iUqmU6lFK9YjIFBH5z5GoXFGwLAKJ6ZnFGo1GQ34x\ngjVKqU5rw1yt7JLiVam4qIQR9/YTp8w/0SwCLQQajebIyUcI3CJi+xxEpAQYtz6IeLQPMISgosQ7\nyrUpIIkIuMfvYC6NRjN65NMl/gXwNxG5B2Op36uBe4tZqWIiZq6hixZXMrVsgjScqaSxMpm2CDQa\nzVGQT7D4WyLyKnABRs6hx4A5xa5YUVAKT8pwDdWUjHJdCokZANfBYo1GczTk4xoCOIAhAu8GzgM2\nF61GxSQZR8z8eWLlHJoIWM+iLQKNRnMUDGoRiMgijAXlr8KYQPYrQJRSbx6huhUeZ+Nv9aInAtoi\n0Gg0x8BQrqEtwD+AS5VSOwBE5HMjUqti4Wz8kxNJCLRFoNFojp6hXENXAC3AkyLyExE5HyNYPH7R\nFoFGo9EMYFAhUEo9rJS6ElgCPAl8FqgVkf8VkQtHqoIFxdn46xiBRqPRAHkEi5VSvUqp+5VSbwPq\ngVeAfyt6zYqBtUwlLm0RaDQajUm+o4YAY1axUupOpdT5xapQUTF7zhFXqbYINBqNxuSIhGDcY/ac\nI+7yCWoRaCHQaDRHziQTAqPnHPUEJ6hFoF1DGo3myJlkQmD0nGPe4AS1CLQQaDSaI2dSCUE82g9A\nwhsyetFKjXKNCoS2CDQazTFQVCEQkYtFZKuI7BCRmwYp8x4RaRSRTSJyfzHrEzMzj6Z8QVApI1Hb\nRCCpYwQajeboKVpCfhFxAz8E3oKxvOV6EXlEKdXoKLMQ+CLwJqXUYRGpLVZ9AGKRfsoA5a8wdiQi\n4J4Aqai1a0ij0RwDxVyZ5VRgh1JqJ4CIPABcDjQ6ynwU+KG52A1KqYNFq82mh5jytxsAUAFTCOIR\n8AeLdsuisvuf8MxthmVzeI+xT1sEGo3mKCima2gmsM+x3WTuc7IIWCQi/xSR50Xk4lwXEpHrRORF\nEXmxra3t6GqTjNNbcyK/T55GKjjD2BfvPbprjQUaH4adT0G0G0qnwsr36oVpNBrNUTHaazV6gIXA\nuRizlp8WkRXOpTEBlFJ3AncCrFq16ugivMe/h1fLzuP6n7zAY2Vdxr5I+KgrPupEuiA0E659fLRr\notFoxjnFtAj2A7Mc2/XmPidNwCNKqbhSahewDUMYikJfNAmAp7TS2BHpKtatik8kDJaLS6PRaI6B\nYgrBemChiMwTER9wJfBIVpmHMawBRKQaw1W0s1gV6o0Zo4S8ZVONHeNaCLq0EGg0moJQNCFQSiWA\nT2EsbbkZeFAptUlEbhGRy8xijwHtItKIkeH0C0qp9mLVqS9mWASB4ESwCLQQaDSawlDUGIFS6lHg\n0ax9X3G8V8DnzVfRsYTAX15l7NBCoNFoNJNrZnE0YQpBmdmARsd5sFgLgUajKQCTSghiiRQAXq8X\nfMHxaxEkExDr1kKg0WgKwqQSgngyhdsluF1iNKLjVQgsS0YLgUajKQCTSghiiRQ+t/nIE0EI/KHR\nrYdGo5kQTD4h8EwAIbDqrS0CjUZTACaXECS1EGg0Gk02k0sIEmpiuIa0EGg0mgIyuYQgwyIIaSHQ\naDQaJpsQJJKZFkE0PD5XKbOFQAeLNRrNsTPJhCArRqBSEOsZ/sSOXUMLRts22PUP2PMsJGKFqayT\naLdx/a4mYzuiRw1pNJrCMbmEIJnC6xZjw2pEh0tF3bYNvn8C7H0u9/FoN/zvGXDvpXDPGnjxrsJV\n2OKvXzWuf9/lxnaky6i/y134e2k0mknHpBKCeEKlLQJrEZfkMD34sNkLt3rj2fS1QyoOb/qscc3u\nlsJU1kn/YbMOZhZvhCAA+wAAEI1JREFUnV5Co9EUkEklBNFkCp/H7EVbaxUn40OfZPnjBwssW/vr\nV0GgsjgB6JRZx0S/kV5CC4FGoykgk0oIMmYWW0KQylcIOoc+Hqgo3pBUp1hFw1oINBpNQZlkQpDE\nb7mGXAW2CEZKCCJd6RiBRqPRFIDJJQTOYLFtESSGPskKJg8WVI44EsAFKoqzDnIqhxBoi0Cj0RSI\nSSUEmcFiyyIYJlh8RBZBkSapJRMgrvT9oloINBpN4ZhUQpAxs7jQriF/qIiuoRiUVhvv+w/rhes1\nGk1BmVxCkEjhc2eNGso7WDyEEPiCxpj+YglBKg5lphCE9wNKC4FGoykYk08IbIvAXK75WC2CqKN3\nHqiAZBTikWOvrJNkAkrNdZY796XvpdFoNAVg0giBUspwDdnBYmtCWZ5CMNj6xs7ArfW30GshJ2NQ\nOhUQ6Nxr3kuPGtJoNIVh0ghBPGnkChoQLB7ONWQ16pGu3PmGnELgr0jvKySpOLj9RhzCFgJtEWg0\nmsIwaYQgljQWrh/oGhpu+KjZqCdjkMjh8ol0DrQICi0EyYQhXIEK6NJCoNFoCsvkEYKEKQTZM4uH\nGj6aTBjZSYN1xnauBj6Xa2iwWchHSypuCJczGK2FQKPRFIiiCoGIXCwiW0Vkh4jclOP41SLSJiIb\nzNe1xaqLLQR2riEzRjCUa8hyC1XONv7mFILwCFgEMaO+zsY/UFnYe2g0mklL0YRARNzAD4E1QANw\nlYg05Cj6K6XUCebrp8WqT9x0Ddkzi+15BEO4hqyefcUsczurgU+lzFFDZuC26K4hR4DYHyzsPTQa\nzaTFU8RrnwrsUErtBBCRB4DLgcYi3nNQoomsGIHbihEM4RqyGvRKSwjCxtDQX18NfYeMhW1UymER\nDLHGwZPfhJknwaKLMvc//2PjvBPeCy/da6S8cHnglbWw6GI4+4ZM1xAYgWPLtaXRaDTHSDGFYCaw\nz7HdBKzOUe6dInI2sA34nFJqX3YBEbkOuA5g9uzZR1UZyzU0IOncUK6hSLZrqBM6dsK2P8G0FVBe\nAwsvggVvMY57S40GO5dF8Pz/wuI1A4XgpXugrMYQglfWGkLgK4em9dDXYQhBMm64hla8C3oOwsyT\nj+oz0Gg0mlwUUwjy4ffAL5VSURH5GHAvcF52IaXUncCdAKtWrTqqRYYHjBqy5xEM5RqyLAJHjMDa\nd+EtMD+rqiK5ZxdbLqTBYgwef/p9Kg6ppLEdDRvnqqRhASy4wHhpNBpNASlmsHg/MMuxXW/us1FK\ntSuloubmT4GidXUti8BrjRqylnkc0iKwhGBOetsKIPsHGbWTSwhi3YDKPdHMKS6RLkMMoo6Mp1b9\nXKOt2RqNZqJSTCFYDywUkXki4gOuBB5xFhCROsfmZcDmYlXGChbbw0dFDPdQPjGC8lrDgnA22oMN\n38wlBIOlqUjGId6bJQRd0G8GqZNRiPYY73VMQKPRFImidTOVUgkR+RTwGOAG7lZKbRKRW4AXlVKP\nAJ8WkcuABNABXF2s+sSyg8VgNK5DpZiIdAFiJJULVGS6d4YSguye/2BCEHHMWk5EjaUowYhFlEwx\nMo32HTLr6hv6ATUajeYoKaq/QSn1KPBo1r6vON5/EfhiMetgMWDUEBhCMNTCNJEuY0SPy2Wkd4h0\npYeUDpbrxx+C7taB13H+tfeb11IpCDdnHquYZQhBb5uxrV1DGo2mSEyemcXJrFFDYLqGhplQ5pws\nZrluPCXpAG82Q7mGouF0INjatujKGixlBah7LYtAu4Y0Gk1xmDxCkB0sBtM1NEyMIJcQDJX5cygh\ngMzG37nfSiZnYQWo+9qNvy4tBBqNpjhMGiGIZw8fhTxdQ2YqB1sIhlkdLFAJ8T5IOATG2eAP9n6A\nEJgWgSUEOkag0WiKxKQRggFJ52B411Cky/D5Q5ZFMJQQ5FiTIDLY+6GEwBx5a8UI3DpGoNFoisPk\nE4IBo4aOxDUUzkMIQulzndcZ7n3nMDEC7RrSaDRFYtIIQf2UEt68uAa/lX0UjMZ1SNdQODOPUKLf\n6KHnYxHkJQTOYHGWRVA+3ahfnw4WazSa4jJphGDNijru+fCp+c8jsDOLWkJgxgq69qXdRbnIKQSd\nRh6iAfu70vs794K40rGAQMi4Vq8VI9BCoNFoisOkEYKcuL2Dp5iIhgE1cK2B7PfZDGYR5FrTINIF\nZdXGcFTr3EBlenhqoMIxj0ALgUajKQ6TWwiGChbbM4iz1hrIfp/NYEJQUZ97f6AiU2wytkOOUUNa\nCDQaTXGY3ENR3B4jl0/r6zB9eeax7FQS+QqB5TaKhuFAIxzebaSOrl1qpKpo3QhbzMnWnXugtMpI\nL9HTalzX5TVcRNZ9lDkBTVsEGo2mSExyIfDB/hfhx2fCZzem3TcwUAhCM9LHrN59LnzlRkMe6YJ7\n1qTTSIRmQsX/397dx9hRlXEc//5Yu12g20JprbUvbisVUxWwKa9BYjAI9A+qQkKJUTQoBgHxD4k1\nJARfYgIJxKBELAGDhgiIEpuICgK+JEoBtS0tpLDWGmgKpRKqVVIKPP5xzu1Ot/du996903sv8/sk\nN3fmzOzMc3q2+8yceTlzYNP96VNz7Ip0tL/jGZg6F/oPG7mIXEw4PiMws5JUOxHsPcqOdNReTAS7\nR/0xPmI+fGltOnqfecwY28zvJdq1PSWBEz4PSz4Fb18Mp16x//MCM96T7lx6eTMcdXQ+G4h99w1O\nBGZWmmonguJDWrUj973zdd4yOn3B+LY7MG3k3UEzj4HZx6Xpw6anTz3vPL7+dmrcNWRmJfHF4ppG\n7wca63pAIwPTRo78W/n54nZq/GSxmZWk2omg+P6eRolgrGcGGhmYNvKk8EQSQXEUNL9ryMxKUvFE\nUOwaqjOYTP/gyJCWzRiYlkYXg9YSSXE7Ne4aMrOSVDsRHKhrqNWj+fHeatrMdtw1ZGYlqXYi6Oul\nROCuITMrhxNBTbcnAncNmVlJqp0IDtg11GL/fu26gPqg//DWtgH77t/PEZhZSaqdCMo+IxiYClJr\n2yhuR30T246Z2RiqnQiKA8mXkggm0C0EI6+r8PUBMytRtW9F2fO/keni0JIR+45F0Kx2JQIpbaOY\nsMzM2qzaZwSv/Td990/Z94zgtV0Qb3Y+EdS2cUi187WZlavURCDpbEmbJA1LWjnGeudJCklLy4xn\nP7UzgsF3pOnX8/jFE3mqGNqbCCZP9YViMytVaYlAUh9wM3AOsBi4UNLiOusNAlcCa8qKpaE9r6bv\nwdnpu9Y9NJH3DMHI3T6T23RG4GsEZlaiMvscTgSGI2IzgKS7gOXAU6PW+yZwHXBVibHUV7sTZ+qc\n9H3bmemPbu1MYaK3j7b680XuGjKzkpX5F2YO8Fxh/nngpOIKkpYA8yLil5IaJgJJlwCXAMyfP7/R\nas1bdkMaD+CEz6X5118dWTZ0OsxpsafqkD4469sw9KGJx3jSF2Dn1olvx8ysgY4dako6BLgR+MyB\n1o2IVcAqgKVLl0bbghicBR+5Jk1/4gdt2ywAp1zWnu0sOL092zEza6DMi8VbgXmF+bm5rGYQeD/w\nO0lbgJOB1Qf9grGZWcWVmQgeBxZJWiCpH1gBrK4tjIidETEjIoYiYgh4FDg3Ip4oMSYzMxultEQQ\nEa8DlwO/AZ4G7omIjZK+IencsvZrZmbNKfUaQUTcD9w/quyaBut+uMxYzMysvmo/WWxmZk4EZmZV\n50RgZlZxTgRmZhWniPY9n3UwSHoJ+GeLPz4D2NHGcDrJdelOrkt3cl3gXRExs96CnksEEyHpiYh4\nSzyw5rp0J9elO7kuY3PXkJlZxTkRmJlVXNUSwapOB9BGrkt3cl26k+syhkpdIzAzs/1V7YzAzMxG\ncSIwM6u4yiQCSWdL2iRpWNLKTsfTLElbJD0paa2kJ3LZdEkPSno2fx/Z6TjrkXS7pO2SNhTK6sau\n5KbcTuvzKHZdo0FdrpW0NbfNWknLCsu+luuySdJZnYl6f5LmSXpE0lOSNkq6Mpf3XLuMUZdebJcB\nSY9JWpfr8vVcvkDSmhzz3fnV/kianOeH8/KhlnYcEW/5D9AH/B1YCPQD64DFnY6ryTpsAWaMKrse\nWJmnVwLXdTrOBrGfDiwBNhwodmAZ8CtApMGK1nQ6/nHU5VrgK3XWXZx/1yYDC/LvYF+n65Bjmw0s\nydODwDM53p5rlzHq0ovtImBKnp4ErMn/3vcAK3L5LcClefqLwC15egVwdyv7rcoZwYnAcERsjojX\ngLuA5R2OqR2WA3fk6TuAj3UwloYi4g/Ay6OKG8W+HPhRJI8CR0iafXAiPbAGdWlkOXBXROyOiH8A\nw6TfxY6LiG0R8dc8/R/SmCFz6MF2GaMujXRzu0RE7Mqzk/IngDOAe3P56Haptde9wEckqdn9ViUR\nzAGeK8w/z9i/KN0ogAck/UXSJblsVkRsy9MvALM6E1pLGsXeq211ee4yub3QRdcTdcndCR8kHX32\ndLuMqgv0YLtI6pO0FtgOPEg6Y3kl0mBfsG+8e+uSl+8Ejmp2n1VJBG8Fp0XEEuAc4DJJ+4xqH+nc\nsCfvBe7l2LPvA+8Gjge2ATd0NpzxkzQF+Bnw5Yj4d3FZr7VLnbr0ZLtExBsRcTxpnPcTgfeWvc+q\nJIKtwLzC/Nxc1jMiYmv+3g7cR/oFebF2ep6/t3cuwqY1ir3n2ioiXsz/ed8EbmWkm6Gr6yJpEukP\n550R8fNc3JPtUq8uvdouNRHxCvAIcAqpK642omQx3r11ycunAf9qdl9VSQSPA4vylfd+0kWV1R2O\nadwkHS5psDYNfBTYQKrDRXm1i4BfdCbCljSKfTXw6XyXysnAzkJXRVca1Vf+cVLbQKrLinxnxwJg\nEfDYwY6vntyPfBvwdETcWFjUc+3SqC492i4zJR2Rpw8FziRd83gEOD+vNrpdau11PvBwPpNrTqev\nkh+sD+muh2dI/W1XdzqeJmNfSLrLYR2wsRY/qS/wIeBZ4LfA9E7H2iD+n5BOzfeQ+jcvbhQ76a6J\nm3M7PQks7XT846jLj3Os6/N/zNmF9a/OddkEnNPp+AtxnUbq9lkPrM2fZb3YLmPUpRfb5Vjgbznm\nDcA1uXwhKVkNAz8FJufygTw/nJcvbGW/fsWEmVnFVaVryMzMGnAiMDOrOCcCM7OKcyIwM6s4JwIz\ns4pzIjAbRdIbhTdWrlUb31Yraaj45lKzbvC2A69iVjmvRnrE36wSfEZgNk5KY0JcrzQuxGOSjs7l\nQ5Iezi83e0jS/Fw+S9J9+d3y6ySdmjfVJ+nW/L75B/ITpGYd40Rgtr9DR3UNXVBYtjMiPgB8D/hO\nLvsucEdEHAvcCdyUy28Cfh8Rx5HGMNiYyxcBN0fE+4BXgPNKro/ZmPxksdkoknZFxJQ65VuAMyJi\nc37J2QsRcZSkHaTXF+zJ5dsiYoakl4C5EbG7sI0h4MGIWJTnvwpMiohvlV8zs/p8RmDWnGgw3Yzd\nhek38LU66zAnArPmXFD4/nOe/hPpjbYAnwT+mKcfAi6FvYONTDtYQZo1w0ciZvs7NI8QVfPriKjd\nQnqkpPWko/oLc9kVwA8lXQW8BHw2l18JrJJ0MenI/1LSm0vNuoqvEZiNU75GsDQidnQ6FrN2cteQ\nmVnF+YzAzKzifEZgZlZxTgRmZhXnRGBmVnFOBGZmFedEYGZWcf8HELGDf6/oeTMAAAAASUVORK5C\nYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "# Evaluate on test data\n",
            "40/40 [==============================] - 0s 2ms/sample - loss: 0.4058 - acc: 0.8250\n",
            "test loss, test acc: [0.40575066056626385, 0.825]\n",
            "EEG_Deep/Data2A/parsed_P06T.mat\n",
            "Filtering of Training Data Finished\n",
            "EEG_Deep/Data2A/parsed_P06E.mat\n",
            "Filtering of Testing Data Finished\n",
            "(1, 2)\n",
            "Finding labels in training data\n",
            "Finding labels in testing data\n",
            "[1 2 1 2 2 2 1 2 1 1 2 1 2 2 1 1 2 1 2 1]\n",
            "(60, 12, 1536)\n",
            "(60,)\n",
            "(20, 12, 1536)\n",
            "(20,)\n",
            "[[1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]]\n",
            "X_train shape: (60, 1, 12, 1536)\n",
            "60 train samples\n",
            "20 val samples\n",
            "(40, 12, 1536)\n",
            "(40, 1)\n",
            "X_train shape: (40, 1, 12, 1536)\n",
            "40 train samples\n",
            "Train on 60 samples, validate on 20 samples\n",
            "Epoch 1/300\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.69352, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 1s - loss: 0.7031 - acc: 0.5333 - val_loss: 0.6935 - val_acc: 0.5000\n",
            "Epoch 2/300\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.69352\n",
            "60/60 - 0s - loss: 0.6683 - acc: 0.5833 - val_loss: 0.6939 - val_acc: 0.4500\n",
            "Epoch 3/300\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.69352\n",
            "60/60 - 0s - loss: 0.6604 - acc: 0.6333 - val_loss: 0.6945 - val_acc: 0.6000\n",
            "Epoch 4/300\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.69352\n",
            "60/60 - 0s - loss: 0.6465 - acc: 0.6333 - val_loss: 0.6951 - val_acc: 0.5500\n",
            "Epoch 5/300\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.69352\n",
            "60/60 - 0s - loss: 0.6362 - acc: 0.6833 - val_loss: 0.6958 - val_acc: 0.4500\n",
            "Epoch 6/300\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.69352\n",
            "60/60 - 0s - loss: 0.6215 - acc: 0.7333 - val_loss: 0.6966 - val_acc: 0.4500\n",
            "Epoch 7/300\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.69352\n",
            "60/60 - 0s - loss: 0.6055 - acc: 0.8000 - val_loss: 0.6976 - val_acc: 0.4500\n",
            "Epoch 8/300\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.69352\n",
            "60/60 - 0s - loss: 0.5842 - acc: 0.8500 - val_loss: 0.6986 - val_acc: 0.4500\n",
            "Epoch 9/300\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.69352\n",
            "60/60 - 0s - loss: 0.5598 - acc: 0.8667 - val_loss: 0.6994 - val_acc: 0.4500\n",
            "Epoch 10/300\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.69352\n",
            "60/60 - 0s - loss: 0.5650 - acc: 0.8667 - val_loss: 0.7003 - val_acc: 0.5500\n",
            "Epoch 11/300\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.69352\n",
            "60/60 - 0s - loss: 0.5496 - acc: 0.8167 - val_loss: 0.7018 - val_acc: 0.4500\n",
            "Epoch 12/300\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.69352\n",
            "60/60 - 0s - loss: 0.5502 - acc: 0.8500 - val_loss: 0.7024 - val_acc: 0.5000\n",
            "Epoch 13/300\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.69352\n",
            "60/60 - 0s - loss: 0.5056 - acc: 0.9500 - val_loss: 0.7025 - val_acc: 0.5000\n",
            "Epoch 14/300\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.69352\n",
            "60/60 - 0s - loss: 0.5035 - acc: 0.8667 - val_loss: 0.7031 - val_acc: 0.4000\n",
            "Epoch 15/300\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.69352\n",
            "60/60 - 0s - loss: 0.4878 - acc: 0.9000 - val_loss: 0.7031 - val_acc: 0.4000\n",
            "Epoch 16/300\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.69352\n",
            "60/60 - 0s - loss: 0.4720 - acc: 0.8500 - val_loss: 0.7033 - val_acc: 0.4000\n",
            "Epoch 17/300\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.69352\n",
            "60/60 - 0s - loss: 0.4289 - acc: 0.9167 - val_loss: 0.7034 - val_acc: 0.4500\n",
            "Epoch 18/300\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.69352\n",
            "60/60 - 0s - loss: 0.4504 - acc: 0.8833 - val_loss: 0.7040 - val_acc: 0.5000\n",
            "Epoch 19/300\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.69352\n",
            "60/60 - 0s - loss: 0.4137 - acc: 0.9000 - val_loss: 0.7056 - val_acc: 0.5000\n",
            "Epoch 20/300\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.69352\n",
            "60/60 - 0s - loss: 0.4075 - acc: 0.9333 - val_loss: 0.7084 - val_acc: 0.4500\n",
            "Epoch 21/300\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.69352\n",
            "60/60 - 0s - loss: 0.3757 - acc: 0.9000 - val_loss: 0.7113 - val_acc: 0.4500\n",
            "Epoch 22/300\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.69352\n",
            "60/60 - 0s - loss: 0.3712 - acc: 0.9333 - val_loss: 0.7149 - val_acc: 0.4500\n",
            "Epoch 23/300\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.69352\n",
            "60/60 - 0s - loss: 0.3832 - acc: 0.8667 - val_loss: 0.7194 - val_acc: 0.4500\n",
            "Epoch 24/300\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.69352\n",
            "60/60 - 0s - loss: 0.3789 - acc: 0.8500 - val_loss: 0.7232 - val_acc: 0.4500\n",
            "Epoch 25/300\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.69352\n",
            "60/60 - 0s - loss: 0.3388 - acc: 0.8833 - val_loss: 0.7244 - val_acc: 0.4500\n",
            "Epoch 26/300\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.69352\n",
            "60/60 - 0s - loss: 0.3210 - acc: 0.9333 - val_loss: 0.7258 - val_acc: 0.4500\n",
            "Epoch 27/300\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.69352\n",
            "60/60 - 0s - loss: 0.3104 - acc: 0.9333 - val_loss: 0.7244 - val_acc: 0.4500\n",
            "Epoch 28/300\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.69352\n",
            "60/60 - 0s - loss: 0.3179 - acc: 0.8833 - val_loss: 0.7235 - val_acc: 0.4500\n",
            "Epoch 29/300\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.69352\n",
            "60/60 - 0s - loss: 0.3116 - acc: 0.9167 - val_loss: 0.7194 - val_acc: 0.5000\n",
            "Epoch 30/300\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.69352\n",
            "60/60 - 0s - loss: 0.2979 - acc: 0.8667 - val_loss: 0.7202 - val_acc: 0.5000\n",
            "Epoch 31/300\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.69352\n",
            "60/60 - 0s - loss: 0.2968 - acc: 0.9167 - val_loss: 0.7232 - val_acc: 0.5000\n",
            "Epoch 32/300\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.69352\n",
            "60/60 - 0s - loss: 0.2882 - acc: 0.8833 - val_loss: 0.7338 - val_acc: 0.5000\n",
            "Epoch 33/300\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.69352\n",
            "60/60 - 0s - loss: 0.2222 - acc: 0.9833 - val_loss: 0.7487 - val_acc: 0.5000\n",
            "Epoch 34/300\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.69352\n",
            "60/60 - 0s - loss: 0.2618 - acc: 0.9500 - val_loss: 0.7588 - val_acc: 0.5000\n",
            "Epoch 35/300\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.69352\n",
            "60/60 - 0s - loss: 0.2723 - acc: 0.9167 - val_loss: 0.7633 - val_acc: 0.5000\n",
            "Epoch 36/300\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.69352\n",
            "60/60 - 0s - loss: 0.2866 - acc: 0.9333 - val_loss: 0.7703 - val_acc: 0.5000\n",
            "Epoch 37/300\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.69352\n",
            "60/60 - 0s - loss: 0.2506 - acc: 0.9500 - val_loss: 0.7668 - val_acc: 0.5000\n",
            "Epoch 38/300\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.69352\n",
            "60/60 - 0s - loss: 0.2301 - acc: 0.9500 - val_loss: 0.7546 - val_acc: 0.5000\n",
            "Epoch 39/300\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.69352\n",
            "60/60 - 0s - loss: 0.2406 - acc: 0.9667 - val_loss: 0.7477 - val_acc: 0.5000\n",
            "Epoch 40/300\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.69352\n",
            "60/60 - 0s - loss: 0.2347 - acc: 0.9500 - val_loss: 0.7321 - val_acc: 0.5000\n",
            "Epoch 41/300\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.69352\n",
            "60/60 - 0s - loss: 0.2589 - acc: 0.9167 - val_loss: 0.7123 - val_acc: 0.5000\n",
            "Epoch 42/300\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.69352\n",
            "60/60 - 0s - loss: 0.2004 - acc: 0.9667 - val_loss: 0.7027 - val_acc: 0.5500\n",
            "Epoch 43/300\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.69352\n",
            "60/60 - 0s - loss: 0.2165 - acc: 0.9333 - val_loss: 0.7030 - val_acc: 0.5500\n",
            "Epoch 44/300\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.69352\n",
            "60/60 - 0s - loss: 0.2285 - acc: 0.9000 - val_loss: 0.7061 - val_acc: 0.5500\n",
            "Epoch 45/300\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.69352\n",
            "60/60 - 0s - loss: 0.1855 - acc: 0.9667 - val_loss: 0.7182 - val_acc: 0.5500\n",
            "Epoch 46/300\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.69352\n",
            "60/60 - 0s - loss: 0.2194 - acc: 0.9333 - val_loss: 0.7365 - val_acc: 0.5500\n",
            "Epoch 47/300\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.69352\n",
            "60/60 - 0s - loss: 0.2755 - acc: 0.9333 - val_loss: 0.7514 - val_acc: 0.5500\n",
            "Epoch 48/300\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.69352\n",
            "60/60 - 0s - loss: 0.2251 - acc: 0.9333 - val_loss: 0.7742 - val_acc: 0.5000\n",
            "Epoch 49/300\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.69352\n",
            "60/60 - 0s - loss: 0.1463 - acc: 0.9667 - val_loss: 0.7907 - val_acc: 0.5000\n",
            "Epoch 50/300\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.69352\n",
            "60/60 - 0s - loss: 0.2005 - acc: 0.9500 - val_loss: 0.8052 - val_acc: 0.5000\n",
            "Epoch 51/300\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.69352\n",
            "60/60 - 0s - loss: 0.2360 - acc: 0.9000 - val_loss: 0.8010 - val_acc: 0.5000\n",
            "Epoch 52/300\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.69352\n",
            "60/60 - 0s - loss: 0.1864 - acc: 0.9333 - val_loss: 0.7750 - val_acc: 0.5000\n",
            "Epoch 53/300\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.69352\n",
            "60/60 - 0s - loss: 0.2000 - acc: 0.9500 - val_loss: 0.7216 - val_acc: 0.6000\n",
            "Epoch 54/300\n",
            "\n",
            "Epoch 00054: val_loss improved from 0.69352 to 0.69194, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1631 - acc: 0.9333 - val_loss: 0.6919 - val_acc: 0.6000\n",
            "Epoch 55/300\n",
            "\n",
            "Epoch 00055: val_loss improved from 0.69194 to 0.67758, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1874 - acc: 0.9500 - val_loss: 0.6776 - val_acc: 0.6500\n",
            "Epoch 56/300\n",
            "\n",
            "Epoch 00056: val_loss improved from 0.67758 to 0.66365, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2466 - acc: 0.9167 - val_loss: 0.6636 - val_acc: 0.6500\n",
            "Epoch 57/300\n",
            "\n",
            "Epoch 00057: val_loss improved from 0.66365 to 0.65864, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1683 - acc: 0.9500 - val_loss: 0.6586 - val_acc: 0.6000\n",
            "Epoch 58/300\n",
            "\n",
            "Epoch 00058: val_loss improved from 0.65864 to 0.63854, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2192 - acc: 0.9167 - val_loss: 0.6385 - val_acc: 0.6000\n",
            "Epoch 59/300\n",
            "\n",
            "Epoch 00059: val_loss improved from 0.63854 to 0.63543, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1747 - acc: 0.9333 - val_loss: 0.6354 - val_acc: 0.6000\n",
            "Epoch 60/300\n",
            "\n",
            "Epoch 00060: val_loss improved from 0.63543 to 0.62670, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1829 - acc: 0.9833 - val_loss: 0.6267 - val_acc: 0.6000\n",
            "Epoch 61/300\n",
            "\n",
            "Epoch 00061: val_loss improved from 0.62670 to 0.61099, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1946 - acc: 0.9333 - val_loss: 0.6110 - val_acc: 0.6000\n",
            "Epoch 62/300\n",
            "\n",
            "Epoch 00062: val_loss improved from 0.61099 to 0.60388, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1757 - acc: 0.9333 - val_loss: 0.6039 - val_acc: 0.6000\n",
            "Epoch 63/300\n",
            "\n",
            "Epoch 00063: val_loss improved from 0.60388 to 0.59327, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2094 - acc: 0.9667 - val_loss: 0.5933 - val_acc: 0.6500\n",
            "Epoch 64/300\n",
            "\n",
            "Epoch 00064: val_loss improved from 0.59327 to 0.58291, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2108 - acc: 0.9000 - val_loss: 0.5829 - val_acc: 0.7000\n",
            "Epoch 65/300\n",
            "\n",
            "Epoch 00065: val_loss improved from 0.58291 to 0.56401, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2017 - acc: 0.9000 - val_loss: 0.5640 - val_acc: 0.7000\n",
            "Epoch 66/300\n",
            "\n",
            "Epoch 00066: val_loss improved from 0.56401 to 0.54894, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2025 - acc: 0.9333 - val_loss: 0.5489 - val_acc: 0.7500\n",
            "Epoch 67/300\n",
            "\n",
            "Epoch 00067: val_loss improved from 0.54894 to 0.53938, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1534 - acc: 0.9667 - val_loss: 0.5394 - val_acc: 0.7500\n",
            "Epoch 68/300\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.53938\n",
            "60/60 - 0s - loss: 0.1636 - acc: 0.9667 - val_loss: 0.5435 - val_acc: 0.7500\n",
            "Epoch 69/300\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.53938\n",
            "60/60 - 0s - loss: 0.1472 - acc: 0.9833 - val_loss: 0.5423 - val_acc: 0.7500\n",
            "Epoch 70/300\n",
            "\n",
            "Epoch 00070: val_loss improved from 0.53938 to 0.53459, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1405 - acc: 0.9667 - val_loss: 0.5346 - val_acc: 0.7500\n",
            "Epoch 71/300\n",
            "\n",
            "Epoch 00071: val_loss improved from 0.53459 to 0.53265, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1185 - acc: 0.9833 - val_loss: 0.5326 - val_acc: 0.7500\n",
            "Epoch 72/300\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.53265\n",
            "60/60 - 0s - loss: 0.1116 - acc: 0.9833 - val_loss: 0.5333 - val_acc: 0.7500\n",
            "Epoch 73/300\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.53265\n",
            "60/60 - 0s - loss: 0.1389 - acc: 0.9833 - val_loss: 0.5386 - val_acc: 0.7500\n",
            "Epoch 74/300\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.53265\n",
            "60/60 - 0s - loss: 0.1219 - acc: 0.9667 - val_loss: 0.5471 - val_acc: 0.7000\n",
            "Epoch 75/300\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.53265\n",
            "60/60 - 0s - loss: 0.1417 - acc: 0.9500 - val_loss: 0.5590 - val_acc: 0.7500\n",
            "Epoch 76/300\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.53265\n",
            "60/60 - 0s - loss: 0.1514 - acc: 0.9667 - val_loss: 0.5620 - val_acc: 0.7500\n",
            "Epoch 77/300\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.53265\n",
            "60/60 - 0s - loss: 0.1155 - acc: 0.9500 - val_loss: 0.5517 - val_acc: 0.7500\n",
            "Epoch 78/300\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.53265\n",
            "60/60 - 0s - loss: 0.1377 - acc: 0.9333 - val_loss: 0.5402 - val_acc: 0.7500\n",
            "Epoch 79/300\n",
            "\n",
            "Epoch 00079: val_loss improved from 0.53265 to 0.52622, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1265 - acc: 0.9667 - val_loss: 0.5262 - val_acc: 0.7500\n",
            "Epoch 80/300\n",
            "\n",
            "Epoch 00080: val_loss improved from 0.52622 to 0.50751, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1463 - acc: 0.9500 - val_loss: 0.5075 - val_acc: 0.7500\n",
            "Epoch 81/300\n",
            "\n",
            "Epoch 00081: val_loss improved from 0.50751 to 0.48860, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1437 - acc: 0.9667 - val_loss: 0.4886 - val_acc: 0.8000\n",
            "Epoch 82/300\n",
            "\n",
            "Epoch 00082: val_loss improved from 0.48860 to 0.47255, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1193 - acc: 0.9833 - val_loss: 0.4726 - val_acc: 0.8000\n",
            "Epoch 83/300\n",
            "\n",
            "Epoch 00083: val_loss improved from 0.47255 to 0.46376, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1213 - acc: 0.9667 - val_loss: 0.4638 - val_acc: 0.8000\n",
            "Epoch 84/300\n",
            "\n",
            "Epoch 00084: val_loss improved from 0.46376 to 0.45741, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1200 - acc: 0.9833 - val_loss: 0.4574 - val_acc: 0.8000\n",
            "Epoch 85/300\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.1397 - acc: 0.9500 - val_loss: 0.4623 - val_acc: 0.7500\n",
            "Epoch 86/300\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.1455 - acc: 0.9333 - val_loss: 0.4785 - val_acc: 0.7500\n",
            "Epoch 87/300\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0948 - acc: 1.0000 - val_loss: 0.4944 - val_acc: 0.7500\n",
            "Epoch 88/300\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.1362 - acc: 0.9667 - val_loss: 0.5052 - val_acc: 0.7500\n",
            "Epoch 89/300\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0947 - acc: 1.0000 - val_loss: 0.5150 - val_acc: 0.7500\n",
            "Epoch 90/300\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.1335 - acc: 0.9667 - val_loss: 0.5208 - val_acc: 0.7500\n",
            "Epoch 91/300\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0934 - acc: 0.9833 - val_loss: 0.5218 - val_acc: 0.7500\n",
            "Epoch 92/300\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.1455 - acc: 0.9333 - val_loss: 0.5166 - val_acc: 0.7500\n",
            "Epoch 93/300\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.1349 - acc: 0.9667 - val_loss: 0.5145 - val_acc: 0.7500\n",
            "Epoch 94/300\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0809 - acc: 0.9833 - val_loss: 0.5094 - val_acc: 0.7500\n",
            "Epoch 95/300\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.1596 - acc: 0.9833 - val_loss: 0.5164 - val_acc: 0.8000\n",
            "Epoch 96/300\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.1322 - acc: 0.9667 - val_loss: 0.5226 - val_acc: 0.8000\n",
            "Epoch 97/300\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.1161 - acc: 0.9667 - val_loss: 0.5322 - val_acc: 0.8000\n",
            "Epoch 98/300\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.1041 - acc: 0.9833 - val_loss: 0.5383 - val_acc: 0.8000\n",
            "Epoch 99/300\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.1028 - acc: 0.9667 - val_loss: 0.5390 - val_acc: 0.8000\n",
            "Epoch 100/300\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0966 - acc: 0.9833 - val_loss: 0.5410 - val_acc: 0.8000\n",
            "Epoch 101/300\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.1157 - acc: 0.9667 - val_loss: 0.5372 - val_acc: 0.8000\n",
            "Epoch 102/300\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.1188 - acc: 0.9667 - val_loss: 0.5376 - val_acc: 0.8000\n",
            "Epoch 103/300\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0980 - acc: 0.9833 - val_loss: 0.5388 - val_acc: 0.8500\n",
            "Epoch 104/300\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.1178 - acc: 0.9667 - val_loss: 0.5466 - val_acc: 0.8500\n",
            "Epoch 105/300\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.1105 - acc: 0.9667 - val_loss: 0.5557 - val_acc: 0.8500\n",
            "Epoch 106/300\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0927 - acc: 0.9833 - val_loss: 0.5498 - val_acc: 0.8500\n",
            "Epoch 107/300\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.1320 - acc: 0.9333 - val_loss: 0.5364 - val_acc: 0.8500\n",
            "Epoch 108/300\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.1086 - acc: 0.9833 - val_loss: 0.5328 - val_acc: 0.8500\n",
            "Epoch 109/300\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.1192 - acc: 0.9667 - val_loss: 0.5303 - val_acc: 0.8500\n",
            "Epoch 110/300\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.1202 - acc: 0.9667 - val_loss: 0.5294 - val_acc: 0.8500\n",
            "Epoch 111/300\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.1095 - acc: 0.9667 - val_loss: 0.5374 - val_acc: 0.8500\n",
            "Epoch 112/300\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0902 - acc: 1.0000 - val_loss: 0.5397 - val_acc: 0.8500\n",
            "Epoch 113/300\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.1345 - acc: 0.9333 - val_loss: 0.5400 - val_acc: 0.8500\n",
            "Epoch 114/300\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0773 - acc: 0.9833 - val_loss: 0.5435 - val_acc: 0.8500\n",
            "Epoch 115/300\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0803 - acc: 0.9833 - val_loss: 0.5506 - val_acc: 0.8500\n",
            "Epoch 116/300\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.1066 - acc: 0.9667 - val_loss: 0.5583 - val_acc: 0.8500\n",
            "Epoch 117/300\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0873 - acc: 1.0000 - val_loss: 0.5585 - val_acc: 0.8500\n",
            "Epoch 118/300\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0804 - acc: 1.0000 - val_loss: 0.5600 - val_acc: 0.8500\n",
            "Epoch 119/300\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0702 - acc: 1.0000 - val_loss: 0.5600 - val_acc: 0.8500\n",
            "Epoch 120/300\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.1416 - acc: 0.9667 - val_loss: 0.5562 - val_acc: 0.8500\n",
            "Epoch 121/300\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0674 - acc: 1.0000 - val_loss: 0.5488 - val_acc: 0.8500\n",
            "Epoch 122/300\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0947 - acc: 0.9833 - val_loss: 0.5480 - val_acc: 0.8500\n",
            "Epoch 123/300\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0703 - acc: 1.0000 - val_loss: 0.5483 - val_acc: 0.8500\n",
            "Epoch 124/300\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0777 - acc: 0.9833 - val_loss: 0.5508 - val_acc: 0.8500\n",
            "Epoch 125/300\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0995 - acc: 0.9833 - val_loss: 0.5628 - val_acc: 0.8500\n",
            "Epoch 126/300\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0925 - acc: 0.9833 - val_loss: 0.5665 - val_acc: 0.8500\n",
            "Epoch 127/300\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.1230 - acc: 0.9500 - val_loss: 0.5704 - val_acc: 0.8500\n",
            "Epoch 128/300\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.1491 - acc: 0.9333 - val_loss: 0.5745 - val_acc: 0.8500\n",
            "Epoch 129/300\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0689 - acc: 1.0000 - val_loss: 0.5842 - val_acc: 0.8500\n",
            "Epoch 130/300\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0987 - acc: 0.9667 - val_loss: 0.5862 - val_acc: 0.8500\n",
            "Epoch 131/300\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0803 - acc: 0.9833 - val_loss: 0.6034 - val_acc: 0.8500\n",
            "Epoch 132/300\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0881 - acc: 0.9833 - val_loss: 0.6224 - val_acc: 0.8500\n",
            "Epoch 133/300\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0516 - acc: 1.0000 - val_loss: 0.6397 - val_acc: 0.8500\n",
            "Epoch 134/300\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.1189 - acc: 0.9500 - val_loss: 0.6384 - val_acc: 0.8500\n",
            "Epoch 135/300\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.1027 - acc: 0.9833 - val_loss: 0.6392 - val_acc: 0.8500\n",
            "Epoch 136/300\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0916 - acc: 0.9667 - val_loss: 0.6410 - val_acc: 0.8500\n",
            "Epoch 137/300\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.1274 - acc: 0.9667 - val_loss: 0.6346 - val_acc: 0.8500\n",
            "Epoch 138/300\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0762 - acc: 0.9833 - val_loss: 0.6264 - val_acc: 0.8500\n",
            "Epoch 139/300\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.1296 - acc: 0.9667 - val_loss: 0.6223 - val_acc: 0.8500\n",
            "Epoch 140/300\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0628 - acc: 1.0000 - val_loss: 0.6234 - val_acc: 0.8500\n",
            "Epoch 141/300\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0724 - acc: 1.0000 - val_loss: 0.6245 - val_acc: 0.8500\n",
            "Epoch 142/300\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0714 - acc: 1.0000 - val_loss: 0.6258 - val_acc: 0.8500\n",
            "Epoch 143/300\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0852 - acc: 0.9667 - val_loss: 0.6363 - val_acc: 0.8500\n",
            "Epoch 144/300\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.1039 - acc: 0.9833 - val_loss: 0.6506 - val_acc: 0.8500\n",
            "Epoch 145/300\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0796 - acc: 0.9833 - val_loss: 0.6629 - val_acc: 0.8500\n",
            "Epoch 146/300\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0790 - acc: 1.0000 - val_loss: 0.6704 - val_acc: 0.8500\n",
            "Epoch 147/300\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0728 - acc: 0.9833 - val_loss: 0.6701 - val_acc: 0.8500\n",
            "Epoch 148/300\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.1029 - acc: 0.9833 - val_loss: 0.6506 - val_acc: 0.8500\n",
            "Epoch 149/300\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0590 - acc: 1.0000 - val_loss: 0.6411 - val_acc: 0.8500\n",
            "Epoch 150/300\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0889 - acc: 0.9833 - val_loss: 0.6388 - val_acc: 0.8500\n",
            "Epoch 151/300\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0854 - acc: 0.9667 - val_loss: 0.6352 - val_acc: 0.8500\n",
            "Epoch 152/300\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0994 - acc: 0.9833 - val_loss: 0.6241 - val_acc: 0.8500\n",
            "Epoch 153/300\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.1317 - acc: 0.9500 - val_loss: 0.6020 - val_acc: 0.8500\n",
            "Epoch 154/300\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0671 - acc: 0.9833 - val_loss: 0.5946 - val_acc: 0.8500\n",
            "Epoch 155/300\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0906 - acc: 0.9667 - val_loss: 0.5987 - val_acc: 0.8500\n",
            "Epoch 156/300\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0847 - acc: 1.0000 - val_loss: 0.6160 - val_acc: 0.8500\n",
            "Epoch 157/300\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0647 - acc: 1.0000 - val_loss: 0.6302 - val_acc: 0.8500\n",
            "Epoch 158/300\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0443 - acc: 1.0000 - val_loss: 0.6413 - val_acc: 0.8500\n",
            "Epoch 159/300\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0991 - acc: 0.9833 - val_loss: 0.6508 - val_acc: 0.8500\n",
            "Epoch 160/300\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.1494 - acc: 0.9333 - val_loss: 0.6632 - val_acc: 0.8500\n",
            "Epoch 161/300\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0545 - acc: 1.0000 - val_loss: 0.6838 - val_acc: 0.8500\n",
            "Epoch 162/300\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.1101 - acc: 0.9500 - val_loss: 0.6898 - val_acc: 0.8500\n",
            "Epoch 163/300\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0634 - acc: 1.0000 - val_loss: 0.6732 - val_acc: 0.8500\n",
            "Epoch 164/300\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0800 - acc: 0.9833 - val_loss: 0.6674 - val_acc: 0.8500\n",
            "Epoch 165/300\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0740 - acc: 1.0000 - val_loss: 0.6728 - val_acc: 0.8500\n",
            "Epoch 166/300\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.1311 - acc: 0.9833 - val_loss: 0.6663 - val_acc: 0.8500\n",
            "Epoch 167/300\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0656 - acc: 0.9833 - val_loss: 0.6535 - val_acc: 0.8500\n",
            "Epoch 168/300\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0969 - acc: 0.9667 - val_loss: 0.6412 - val_acc: 0.8500\n",
            "Epoch 169/300\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.1268 - acc: 0.9500 - val_loss: 0.6315 - val_acc: 0.8500\n",
            "Epoch 170/300\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0736 - acc: 0.9833 - val_loss: 0.6012 - val_acc: 0.8500\n",
            "Epoch 171/300\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0911 - acc: 0.9833 - val_loss: 0.5892 - val_acc: 0.8500\n",
            "Epoch 172/300\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0871 - acc: 0.9833 - val_loss: 0.5798 - val_acc: 0.8500\n",
            "Epoch 173/300\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0749 - acc: 1.0000 - val_loss: 0.5641 - val_acc: 0.8500\n",
            "Epoch 174/300\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0608 - acc: 1.0000 - val_loss: 0.5390 - val_acc: 0.8500\n",
            "Epoch 175/300\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0921 - acc: 0.9833 - val_loss: 0.5351 - val_acc: 0.8500\n",
            "Epoch 176/300\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0713 - acc: 0.9833 - val_loss: 0.5267 - val_acc: 0.8500\n",
            "Epoch 177/300\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0844 - acc: 0.9667 - val_loss: 0.5250 - val_acc: 0.8500\n",
            "Epoch 178/300\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0544 - acc: 0.9833 - val_loss: 0.5240 - val_acc: 0.8500\n",
            "Epoch 179/300\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0613 - acc: 1.0000 - val_loss: 0.5198 - val_acc: 0.8500\n",
            "Epoch 180/300\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0711 - acc: 0.9833 - val_loss: 0.5256 - val_acc: 0.8500\n",
            "Epoch 181/300\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0538 - acc: 0.9833 - val_loss: 0.5289 - val_acc: 0.8500\n",
            "Epoch 182/300\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.1126 - acc: 1.0000 - val_loss: 0.5515 - val_acc: 0.8500\n",
            "Epoch 183/300\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.1012 - acc: 0.9667 - val_loss: 0.5787 - val_acc: 0.8500\n",
            "Epoch 184/300\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0590 - acc: 1.0000 - val_loss: 0.6171 - val_acc: 0.8500\n",
            "Epoch 185/300\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.1040 - acc: 0.9833 - val_loss: 0.5747 - val_acc: 0.8500\n",
            "Epoch 186/300\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0713 - acc: 0.9833 - val_loss: 0.5410 - val_acc: 0.8500\n",
            "Epoch 187/300\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0790 - acc: 0.9667 - val_loss: 0.5428 - val_acc: 0.8500\n",
            "Epoch 188/300\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0740 - acc: 1.0000 - val_loss: 0.5528 - val_acc: 0.8500\n",
            "Epoch 189/300\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.1842 - acc: 0.9167 - val_loss: 0.5729 - val_acc: 0.8500\n",
            "Epoch 190/300\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0871 - acc: 0.9833 - val_loss: 0.5823 - val_acc: 0.8500\n",
            "Epoch 191/300\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0665 - acc: 0.9833 - val_loss: 0.6079 - val_acc: 0.8500\n",
            "Epoch 192/300\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0650 - acc: 0.9833 - val_loss: 0.6390 - val_acc: 0.8500\n",
            "Epoch 193/300\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0545 - acc: 1.0000 - val_loss: 0.6667 - val_acc: 0.8500\n",
            "Epoch 194/300\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0569 - acc: 1.0000 - val_loss: 0.6916 - val_acc: 0.8500\n",
            "Epoch 195/300\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.2432 - acc: 0.8833 - val_loss: 0.6800 - val_acc: 0.8500\n",
            "Epoch 196/300\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.1049 - acc: 0.9833 - val_loss: 0.6394 - val_acc: 0.8500\n",
            "Epoch 197/300\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0932 - acc: 0.9667 - val_loss: 0.6172 - val_acc: 0.8500\n",
            "Epoch 198/300\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.1088 - acc: 0.9333 - val_loss: 0.5868 - val_acc: 0.8500\n",
            "Epoch 199/300\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0560 - acc: 0.9833 - val_loss: 0.5796 - val_acc: 0.8500\n",
            "Epoch 200/300\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0660 - acc: 0.9833 - val_loss: 0.5879 - val_acc: 0.8500\n",
            "Epoch 201/300\n",
            "\n",
            "Epoch 00201: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.1542 - acc: 0.9500 - val_loss: 0.5992 - val_acc: 0.8500\n",
            "Epoch 202/300\n",
            "\n",
            "Epoch 00202: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0523 - acc: 1.0000 - val_loss: 0.6038 - val_acc: 0.8500\n",
            "Epoch 203/300\n",
            "\n",
            "Epoch 00203: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0517 - acc: 1.0000 - val_loss: 0.6207 - val_acc: 0.8500\n",
            "Epoch 204/300\n",
            "\n",
            "Epoch 00204: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0671 - acc: 1.0000 - val_loss: 0.6290 - val_acc: 0.8500\n",
            "Epoch 205/300\n",
            "\n",
            "Epoch 00205: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0663 - acc: 0.9833 - val_loss: 0.6311 - val_acc: 0.8500\n",
            "Epoch 206/300\n",
            "\n",
            "Epoch 00206: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0497 - acc: 1.0000 - val_loss: 0.6381 - val_acc: 0.8500\n",
            "Epoch 207/300\n",
            "\n",
            "Epoch 00207: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0984 - acc: 0.9833 - val_loss: 0.5967 - val_acc: 0.8500\n",
            "Epoch 208/300\n",
            "\n",
            "Epoch 00208: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0624 - acc: 1.0000 - val_loss: 0.5728 - val_acc: 0.8500\n",
            "Epoch 209/300\n",
            "\n",
            "Epoch 00209: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0350 - acc: 1.0000 - val_loss: 0.5604 - val_acc: 0.8500\n",
            "Epoch 210/300\n",
            "\n",
            "Epoch 00210: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0733 - acc: 0.9667 - val_loss: 0.5607 - val_acc: 0.8500\n",
            "Epoch 211/300\n",
            "\n",
            "Epoch 00211: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0821 - acc: 0.9667 - val_loss: 0.5476 - val_acc: 0.8500\n",
            "Epoch 212/300\n",
            "\n",
            "Epoch 00212: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0514 - acc: 1.0000 - val_loss: 0.5543 - val_acc: 0.8500\n",
            "Epoch 213/300\n",
            "\n",
            "Epoch 00213: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.1126 - acc: 0.9500 - val_loss: 0.5794 - val_acc: 0.8500\n",
            "Epoch 214/300\n",
            "\n",
            "Epoch 00214: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0439 - acc: 1.0000 - val_loss: 0.6033 - val_acc: 0.8500\n",
            "Epoch 215/300\n",
            "\n",
            "Epoch 00215: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.1068 - acc: 0.9833 - val_loss: 0.6267 - val_acc: 0.8500\n",
            "Epoch 216/300\n",
            "\n",
            "Epoch 00216: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0666 - acc: 0.9667 - val_loss: 0.6535 - val_acc: 0.8500\n",
            "Epoch 217/300\n",
            "\n",
            "Epoch 00217: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0505 - acc: 1.0000 - val_loss: 0.6794 - val_acc: 0.8500\n",
            "Epoch 218/300\n",
            "\n",
            "Epoch 00218: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0512 - acc: 1.0000 - val_loss: 0.7051 - val_acc: 0.8500\n",
            "Epoch 219/300\n",
            "\n",
            "Epoch 00219: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0401 - acc: 1.0000 - val_loss: 0.7225 - val_acc: 0.8500\n",
            "Epoch 220/300\n",
            "\n",
            "Epoch 00220: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0459 - acc: 1.0000 - val_loss: 0.7250 - val_acc: 0.8500\n",
            "Epoch 221/300\n",
            "\n",
            "Epoch 00221: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0372 - acc: 1.0000 - val_loss: 0.7427 - val_acc: 0.8500\n",
            "Epoch 222/300\n",
            "\n",
            "Epoch 00222: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0389 - acc: 1.0000 - val_loss: 0.7514 - val_acc: 0.8500\n",
            "Epoch 223/300\n",
            "\n",
            "Epoch 00223: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0627 - acc: 0.9833 - val_loss: 0.7559 - val_acc: 0.8500\n",
            "Epoch 224/300\n",
            "\n",
            "Epoch 00224: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0998 - acc: 0.9667 - val_loss: 0.7531 - val_acc: 0.8500\n",
            "Epoch 225/300\n",
            "\n",
            "Epoch 00225: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0864 - acc: 0.9667 - val_loss: 0.7341 - val_acc: 0.8500\n",
            "Epoch 226/300\n",
            "\n",
            "Epoch 00226: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0696 - acc: 0.9833 - val_loss: 0.7150 - val_acc: 0.8500\n",
            "Epoch 227/300\n",
            "\n",
            "Epoch 00227: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0581 - acc: 1.0000 - val_loss: 0.6967 - val_acc: 0.8500\n",
            "Epoch 228/300\n",
            "\n",
            "Epoch 00228: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0568 - acc: 1.0000 - val_loss: 0.6939 - val_acc: 0.8500\n",
            "Epoch 229/300\n",
            "\n",
            "Epoch 00229: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0694 - acc: 0.9833 - val_loss: 0.6904 - val_acc: 0.8500\n",
            "Epoch 230/300\n",
            "\n",
            "Epoch 00230: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0581 - acc: 1.0000 - val_loss: 0.6842 - val_acc: 0.8500\n",
            "Epoch 231/300\n",
            "\n",
            "Epoch 00231: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0735 - acc: 1.0000 - val_loss: 0.6907 - val_acc: 0.8500\n",
            "Epoch 232/300\n",
            "\n",
            "Epoch 00232: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0949 - acc: 0.9667 - val_loss: 0.6699 - val_acc: 0.8500\n",
            "Epoch 233/300\n",
            "\n",
            "Epoch 00233: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0623 - acc: 0.9833 - val_loss: 0.6537 - val_acc: 0.8500\n",
            "Epoch 234/300\n",
            "\n",
            "Epoch 00234: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0383 - acc: 1.0000 - val_loss: 0.6445 - val_acc: 0.8500\n",
            "Epoch 235/300\n",
            "\n",
            "Epoch 00235: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0975 - acc: 0.9667 - val_loss: 0.6459 - val_acc: 0.8500\n",
            "Epoch 236/300\n",
            "\n",
            "Epoch 00236: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0661 - acc: 1.0000 - val_loss: 0.6397 - val_acc: 0.8500\n",
            "Epoch 237/300\n",
            "\n",
            "Epoch 00237: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0795 - acc: 0.9833 - val_loss: 0.6513 - val_acc: 0.8500\n",
            "Epoch 238/300\n",
            "\n",
            "Epoch 00238: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0358 - acc: 1.0000 - val_loss: 0.6562 - val_acc: 0.8500\n",
            "Epoch 239/300\n",
            "\n",
            "Epoch 00239: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0976 - acc: 0.9833 - val_loss: 0.6486 - val_acc: 0.8500\n",
            "Epoch 240/300\n",
            "\n",
            "Epoch 00240: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0626 - acc: 0.9667 - val_loss: 0.6283 - val_acc: 0.8500\n",
            "Epoch 241/300\n",
            "\n",
            "Epoch 00241: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0512 - acc: 0.9833 - val_loss: 0.6279 - val_acc: 0.8500\n",
            "Epoch 242/300\n",
            "\n",
            "Epoch 00242: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0662 - acc: 0.9833 - val_loss: 0.6254 - val_acc: 0.8500\n",
            "Epoch 243/300\n",
            "\n",
            "Epoch 00243: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0803 - acc: 1.0000 - val_loss: 0.6282 - val_acc: 0.8500\n",
            "Epoch 244/300\n",
            "\n",
            "Epoch 00244: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0455 - acc: 1.0000 - val_loss: 0.6348 - val_acc: 0.8500\n",
            "Epoch 245/300\n",
            "\n",
            "Epoch 00245: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0793 - acc: 0.9500 - val_loss: 0.6316 - val_acc: 0.8500\n",
            "Epoch 246/300\n",
            "\n",
            "Epoch 00246: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0336 - acc: 1.0000 - val_loss: 0.6342 - val_acc: 0.8500\n",
            "Epoch 247/300\n",
            "\n",
            "Epoch 00247: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0442 - acc: 1.0000 - val_loss: 0.6362 - val_acc: 0.8500\n",
            "Epoch 248/300\n",
            "\n",
            "Epoch 00248: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0471 - acc: 1.0000 - val_loss: 0.6380 - val_acc: 0.8500\n",
            "Epoch 249/300\n",
            "\n",
            "Epoch 00249: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0527 - acc: 1.0000 - val_loss: 0.6390 - val_acc: 0.8500\n",
            "Epoch 250/300\n",
            "\n",
            "Epoch 00250: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0622 - acc: 0.9833 - val_loss: 0.6431 - val_acc: 0.8500\n",
            "Epoch 251/300\n",
            "\n",
            "Epoch 00251: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0372 - acc: 0.9833 - val_loss: 0.6440 - val_acc: 0.8500\n",
            "Epoch 252/300\n",
            "\n",
            "Epoch 00252: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0405 - acc: 1.0000 - val_loss: 0.6420 - val_acc: 0.8500\n",
            "Epoch 253/300\n",
            "\n",
            "Epoch 00253: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0588 - acc: 0.9667 - val_loss: 0.6461 - val_acc: 0.8500\n",
            "Epoch 254/300\n",
            "\n",
            "Epoch 00254: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0489 - acc: 1.0000 - val_loss: 0.6668 - val_acc: 0.8500\n",
            "Epoch 255/300\n",
            "\n",
            "Epoch 00255: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0295 - acc: 1.0000 - val_loss: 0.6796 - val_acc: 0.8500\n",
            "Epoch 256/300\n",
            "\n",
            "Epoch 00256: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0482 - acc: 1.0000 - val_loss: 0.6961 - val_acc: 0.8500\n",
            "Epoch 257/300\n",
            "\n",
            "Epoch 00257: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0353 - acc: 0.9833 - val_loss: 0.7141 - val_acc: 0.8500\n",
            "Epoch 258/300\n",
            "\n",
            "Epoch 00258: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0409 - acc: 1.0000 - val_loss: 0.7271 - val_acc: 0.8500\n",
            "Epoch 259/300\n",
            "\n",
            "Epoch 00259: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0561 - acc: 0.9833 - val_loss: 0.7259 - val_acc: 0.8500\n",
            "Epoch 260/300\n",
            "\n",
            "Epoch 00260: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0675 - acc: 0.9833 - val_loss: 0.7102 - val_acc: 0.8500\n",
            "Epoch 261/300\n",
            "\n",
            "Epoch 00261: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0484 - acc: 1.0000 - val_loss: 0.6951 - val_acc: 0.8500\n",
            "Epoch 262/300\n",
            "\n",
            "Epoch 00262: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0456 - acc: 1.0000 - val_loss: 0.6823 - val_acc: 0.8500\n",
            "Epoch 263/300\n",
            "\n",
            "Epoch 00263: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0416 - acc: 1.0000 - val_loss: 0.6734 - val_acc: 0.8500\n",
            "Epoch 264/300\n",
            "\n",
            "Epoch 00264: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0371 - acc: 1.0000 - val_loss: 0.6751 - val_acc: 0.8500\n",
            "Epoch 265/300\n",
            "\n",
            "Epoch 00265: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0528 - acc: 1.0000 - val_loss: 0.6922 - val_acc: 0.8500\n",
            "Epoch 266/300\n",
            "\n",
            "Epoch 00266: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0685 - acc: 0.9833 - val_loss: 0.6990 - val_acc: 0.8500\n",
            "Epoch 267/300\n",
            "\n",
            "Epoch 00267: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0651 - acc: 0.9833 - val_loss: 0.6986 - val_acc: 0.8500\n",
            "Epoch 268/300\n",
            "\n",
            "Epoch 00268: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0485 - acc: 1.0000 - val_loss: 0.6950 - val_acc: 0.8500\n",
            "Epoch 269/300\n",
            "\n",
            "Epoch 00269: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0370 - acc: 1.0000 - val_loss: 0.6906 - val_acc: 0.8500\n",
            "Epoch 270/300\n",
            "\n",
            "Epoch 00270: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0546 - acc: 0.9833 - val_loss: 0.6939 - val_acc: 0.8500\n",
            "Epoch 271/300\n",
            "\n",
            "Epoch 00271: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0595 - acc: 0.9667 - val_loss: 0.6918 - val_acc: 0.8500\n",
            "Epoch 272/300\n",
            "\n",
            "Epoch 00272: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0430 - acc: 0.9667 - val_loss: 0.6812 - val_acc: 0.8500\n",
            "Epoch 273/300\n",
            "\n",
            "Epoch 00273: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0533 - acc: 1.0000 - val_loss: 0.6535 - val_acc: 0.8500\n",
            "Epoch 274/300\n",
            "\n",
            "Epoch 00274: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0505 - acc: 1.0000 - val_loss: 0.6326 - val_acc: 0.8500\n",
            "Epoch 275/300\n",
            "\n",
            "Epoch 00275: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0503 - acc: 1.0000 - val_loss: 0.6391 - val_acc: 0.8500\n",
            "Epoch 276/300\n",
            "\n",
            "Epoch 00276: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0717 - acc: 0.9833 - val_loss: 0.6460 - val_acc: 0.8500\n",
            "Epoch 277/300\n",
            "\n",
            "Epoch 00277: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0545 - acc: 0.9833 - val_loss: 0.6333 - val_acc: 0.8500\n",
            "Epoch 278/300\n",
            "\n",
            "Epoch 00278: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0662 - acc: 0.9833 - val_loss: 0.6235 - val_acc: 0.8500\n",
            "Epoch 279/300\n",
            "\n",
            "Epoch 00279: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0450 - acc: 0.9833 - val_loss: 0.6322 - val_acc: 0.8500\n",
            "Epoch 280/300\n",
            "\n",
            "Epoch 00280: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0634 - acc: 0.9833 - val_loss: 0.6647 - val_acc: 0.8500\n",
            "Epoch 281/300\n",
            "\n",
            "Epoch 00281: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0674 - acc: 1.0000 - val_loss: 0.6917 - val_acc: 0.8500\n",
            "Epoch 282/300\n",
            "\n",
            "Epoch 00282: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0439 - acc: 1.0000 - val_loss: 0.7171 - val_acc: 0.8500\n",
            "Epoch 283/300\n",
            "\n",
            "Epoch 00283: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0409 - acc: 0.9833 - val_loss: 0.7233 - val_acc: 0.8500\n",
            "Epoch 284/300\n",
            "\n",
            "Epoch 00284: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0945 - acc: 0.9833 - val_loss: 0.7199 - val_acc: 0.8500\n",
            "Epoch 285/300\n",
            "\n",
            "Epoch 00285: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0977 - acc: 0.9667 - val_loss: 0.7177 - val_acc: 0.8500\n",
            "Epoch 286/300\n",
            "\n",
            "Epoch 00286: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0769 - acc: 0.9667 - val_loss: 0.6958 - val_acc: 0.8500\n",
            "Epoch 287/300\n",
            "\n",
            "Epoch 00287: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0668 - acc: 0.9833 - val_loss: 0.5964 - val_acc: 0.8500\n",
            "Epoch 288/300\n",
            "\n",
            "Epoch 00288: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0648 - acc: 1.0000 - val_loss: 0.5451 - val_acc: 0.8500\n",
            "Epoch 289/300\n",
            "\n",
            "Epoch 00289: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0725 - acc: 0.9833 - val_loss: 0.5139 - val_acc: 0.8500\n",
            "Epoch 290/300\n",
            "\n",
            "Epoch 00290: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0770 - acc: 1.0000 - val_loss: 0.4938 - val_acc: 0.8500\n",
            "Epoch 291/300\n",
            "\n",
            "Epoch 00291: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0568 - acc: 1.0000 - val_loss: 0.4959 - val_acc: 0.8500\n",
            "Epoch 292/300\n",
            "\n",
            "Epoch 00292: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0475 - acc: 1.0000 - val_loss: 0.5124 - val_acc: 0.8500\n",
            "Epoch 293/300\n",
            "\n",
            "Epoch 00293: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0471 - acc: 1.0000 - val_loss: 0.5232 - val_acc: 0.8500\n",
            "Epoch 294/300\n",
            "\n",
            "Epoch 00294: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0432 - acc: 1.0000 - val_loss: 0.5464 - val_acc: 0.8500\n",
            "Epoch 295/300\n",
            "\n",
            "Epoch 00295: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0856 - acc: 0.9833 - val_loss: 0.5598 - val_acc: 0.8500\n",
            "Epoch 296/300\n",
            "\n",
            "Epoch 00296: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0460 - acc: 1.0000 - val_loss: 0.5785 - val_acc: 0.8500\n",
            "Epoch 297/300\n",
            "\n",
            "Epoch 00297: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0440 - acc: 0.9833 - val_loss: 0.5943 - val_acc: 0.8500\n",
            "Epoch 298/300\n",
            "\n",
            "Epoch 00298: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0520 - acc: 0.9833 - val_loss: 0.6124 - val_acc: 0.8500\n",
            "Epoch 299/300\n",
            "\n",
            "Epoch 00299: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0656 - acc: 1.0000 - val_loss: 0.6234 - val_acc: 0.8500\n",
            "Epoch 300/300\n",
            "\n",
            "Epoch 00300: val_loss did not improve from 0.45741\n",
            "60/60 - 0s - loss: 0.0972 - acc: 0.9833 - val_loss: 0.6180 - val_acc: 0.8500\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "# Evaluate on test data\n",
            "40/40 [==============================] - 0s 2ms/sample - loss: 1.2756 - acc: 0.5750\n",
            "test loss, test acc: [1.2755977675900794, 0.575]\n",
            "EEG_Deep/Data2A/parsed_P07T.mat\n",
            "Filtering of Training Data Finished\n",
            "EEG_Deep/Data2A/parsed_P07E.mat\n",
            "Filtering of Testing Data Finished\n",
            "(1, 2)\n",
            "Finding labels in training data\n",
            "Finding labels in testing data\n",
            "[2 1 2 1 1 2 2 2 2 1 1 1 1 2 2 2 2 1 2 2]\n",
            "(60, 12, 1536)\n",
            "(60,)\n",
            "(20, 12, 1536)\n",
            "(20,)\n",
            "[[0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]]\n",
            "X_train shape: (60, 1, 12, 1536)\n",
            "60 train samples\n",
            "20 val samples\n",
            "(40, 12, 1536)\n",
            "(40, 1)\n",
            "X_train shape: (40, 1, 12, 1536)\n",
            "40 train samples\n",
            "Train on 60 samples, validate on 20 samples\n",
            "Epoch 1/300\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.69062, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 1s - loss: 0.6897 - acc: 0.4500 - val_loss: 0.6906 - val_acc: 0.5500\n",
            "Epoch 2/300\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.7020 - acc: 0.5000 - val_loss: 0.6906 - val_acc: 0.5000\n",
            "Epoch 3/300\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.6684 - acc: 0.5500 - val_loss: 0.6910 - val_acc: 0.6000\n",
            "Epoch 4/300\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.6405 - acc: 0.7333 - val_loss: 0.6920 - val_acc: 0.5000\n",
            "Epoch 5/300\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.6376 - acc: 0.6667 - val_loss: 0.6931 - val_acc: 0.4500\n",
            "Epoch 6/300\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.6185 - acc: 0.6833 - val_loss: 0.6943 - val_acc: 0.4500\n",
            "Epoch 7/300\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.5864 - acc: 0.8000 - val_loss: 0.6955 - val_acc: 0.4500\n",
            "Epoch 8/300\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.5822 - acc: 0.8000 - val_loss: 0.6970 - val_acc: 0.4500\n",
            "Epoch 9/300\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.5770 - acc: 0.7667 - val_loss: 0.6989 - val_acc: 0.4500\n",
            "Epoch 10/300\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.5443 - acc: 0.8333 - val_loss: 0.7006 - val_acc: 0.4500\n",
            "Epoch 11/300\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.5395 - acc: 0.8500 - val_loss: 0.7025 - val_acc: 0.4500\n",
            "Epoch 12/300\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.5167 - acc: 0.8833 - val_loss: 0.7032 - val_acc: 0.4500\n",
            "Epoch 13/300\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.5109 - acc: 0.8667 - val_loss: 0.7048 - val_acc: 0.4000\n",
            "Epoch 14/300\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.4969 - acc: 0.8667 - val_loss: 0.7076 - val_acc: 0.4000\n",
            "Epoch 15/300\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.5097 - acc: 0.8667 - val_loss: 0.7102 - val_acc: 0.4000\n",
            "Epoch 16/300\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.5093 - acc: 0.8167 - val_loss: 0.7129 - val_acc: 0.4000\n",
            "Epoch 17/300\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.5047 - acc: 0.8000 - val_loss: 0.7137 - val_acc: 0.4000\n",
            "Epoch 18/300\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.4664 - acc: 0.9000 - val_loss: 0.7136 - val_acc: 0.4000\n",
            "Epoch 19/300\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.4814 - acc: 0.8667 - val_loss: 0.7149 - val_acc: 0.4000\n",
            "Epoch 20/300\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.4681 - acc: 0.8667 - val_loss: 0.7164 - val_acc: 0.4000\n",
            "Epoch 21/300\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.4630 - acc: 0.8833 - val_loss: 0.7191 - val_acc: 0.4000\n",
            "Epoch 22/300\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.4600 - acc: 0.9333 - val_loss: 0.7230 - val_acc: 0.4000\n",
            "Epoch 23/300\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.4631 - acc: 0.8833 - val_loss: 0.7250 - val_acc: 0.4500\n",
            "Epoch 24/300\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.4424 - acc: 0.8833 - val_loss: 0.7263 - val_acc: 0.4500\n",
            "Epoch 25/300\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.4450 - acc: 0.8833 - val_loss: 0.7282 - val_acc: 0.4000\n",
            "Epoch 26/300\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.4168 - acc: 0.9500 - val_loss: 0.7288 - val_acc: 0.4000\n",
            "Epoch 27/300\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.4240 - acc: 0.9167 - val_loss: 0.7301 - val_acc: 0.4000\n",
            "Epoch 28/300\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.4332 - acc: 0.9000 - val_loss: 0.7318 - val_acc: 0.4000\n",
            "Epoch 29/300\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.4137 - acc: 0.9500 - val_loss: 0.7348 - val_acc: 0.4000\n",
            "Epoch 30/300\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.4332 - acc: 0.9333 - val_loss: 0.7394 - val_acc: 0.4000\n",
            "Epoch 31/300\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.4269 - acc: 0.9000 - val_loss: 0.7455 - val_acc: 0.4000\n",
            "Epoch 32/300\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.4197 - acc: 0.9167 - val_loss: 0.7483 - val_acc: 0.4000\n",
            "Epoch 33/300\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.4299 - acc: 0.9167 - val_loss: 0.7484 - val_acc: 0.4000\n",
            "Epoch 34/300\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.4197 - acc: 0.9000 - val_loss: 0.7475 - val_acc: 0.3500\n",
            "Epoch 35/300\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.4361 - acc: 0.9000 - val_loss: 0.7455 - val_acc: 0.4000\n",
            "Epoch 36/300\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.4277 - acc: 0.8667 - val_loss: 0.7449 - val_acc: 0.4000\n",
            "Epoch 37/300\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.4107 - acc: 0.9333 - val_loss: 0.7442 - val_acc: 0.4000\n",
            "Epoch 38/300\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.3914 - acc: 0.9500 - val_loss: 0.7442 - val_acc: 0.4000\n",
            "Epoch 39/300\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.4110 - acc: 0.9667 - val_loss: 0.7479 - val_acc: 0.4000\n",
            "Epoch 40/300\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.3904 - acc: 0.9333 - val_loss: 0.7514 - val_acc: 0.4000\n",
            "Epoch 41/300\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.3970 - acc: 0.9167 - val_loss: 0.7546 - val_acc: 0.4000\n",
            "Epoch 42/300\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.4052 - acc: 0.9500 - val_loss: 0.7566 - val_acc: 0.4000\n",
            "Epoch 43/300\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.3904 - acc: 0.9333 - val_loss: 0.7563 - val_acc: 0.4000\n",
            "Epoch 44/300\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.3890 - acc: 0.9333 - val_loss: 0.7589 - val_acc: 0.4000\n",
            "Epoch 45/300\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.3734 - acc: 0.8833 - val_loss: 0.7606 - val_acc: 0.4000\n",
            "Epoch 46/300\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.3788 - acc: 0.9500 - val_loss: 0.7594 - val_acc: 0.4000\n",
            "Epoch 47/300\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.3711 - acc: 0.9500 - val_loss: 0.7585 - val_acc: 0.4000\n",
            "Epoch 48/300\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.3692 - acc: 0.9667 - val_loss: 0.7584 - val_acc: 0.4000\n",
            "Epoch 49/300\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.3922 - acc: 0.9333 - val_loss: 0.7592 - val_acc: 0.5000\n",
            "Epoch 50/300\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.3815 - acc: 0.9667 - val_loss: 0.7605 - val_acc: 0.5000\n",
            "Epoch 51/300\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.3770 - acc: 0.9500 - val_loss: 0.7623 - val_acc: 0.5000\n",
            "Epoch 52/300\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.3513 - acc: 0.9667 - val_loss: 0.7649 - val_acc: 0.4500\n",
            "Epoch 53/300\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.3547 - acc: 0.9500 - val_loss: 0.7665 - val_acc: 0.4500\n",
            "Epoch 54/300\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.3481 - acc: 1.0000 - val_loss: 0.7671 - val_acc: 0.5000\n",
            "Epoch 55/300\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.3662 - acc: 0.9833 - val_loss: 0.7714 - val_acc: 0.4500\n",
            "Epoch 56/300\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.3436 - acc: 0.9667 - val_loss: 0.7781 - val_acc: 0.4500\n",
            "Epoch 57/300\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.3503 - acc: 0.9667 - val_loss: 0.7850 - val_acc: 0.4500\n",
            "Epoch 58/300\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.3791 - acc: 0.9500 - val_loss: 0.7900 - val_acc: 0.4500\n",
            "Epoch 59/300\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.3736 - acc: 0.9500 - val_loss: 0.7933 - val_acc: 0.4500\n",
            "Epoch 60/300\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.3489 - acc: 0.9833 - val_loss: 0.7891 - val_acc: 0.4000\n",
            "Epoch 61/300\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.3647 - acc: 0.9500 - val_loss: 0.7859 - val_acc: 0.4500\n",
            "Epoch 62/300\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.3589 - acc: 0.9167 - val_loss: 0.7837 - val_acc: 0.4500\n",
            "Epoch 63/300\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.3302 - acc: 0.9833 - val_loss: 0.7873 - val_acc: 0.4500\n",
            "Epoch 64/300\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.3743 - acc: 0.9333 - val_loss: 0.7867 - val_acc: 0.4500\n",
            "Epoch 65/300\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.3266 - acc: 0.9833 - val_loss: 0.7853 - val_acc: 0.3500\n",
            "Epoch 66/300\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.3479 - acc: 0.9833 - val_loss: 0.7830 - val_acc: 0.3500\n",
            "Epoch 67/300\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.3446 - acc: 0.9667 - val_loss: 0.7758 - val_acc: 0.3500\n",
            "Epoch 68/300\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.3259 - acc: 0.9833 - val_loss: 0.7756 - val_acc: 0.3500\n",
            "Epoch 69/300\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.3365 - acc: 0.9667 - val_loss: 0.7829 - val_acc: 0.3500\n",
            "Epoch 70/300\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.3238 - acc: 0.9833 - val_loss: 0.7887 - val_acc: 0.3000\n",
            "Epoch 71/300\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.3332 - acc: 0.9833 - val_loss: 0.7942 - val_acc: 0.3500\n",
            "Epoch 72/300\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.3306 - acc: 0.9667 - val_loss: 0.7954 - val_acc: 0.3500\n",
            "Epoch 73/300\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.3276 - acc: 0.9500 - val_loss: 0.7886 - val_acc: 0.3000\n",
            "Epoch 74/300\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.3367 - acc: 0.9833 - val_loss: 0.7791 - val_acc: 0.4000\n",
            "Epoch 75/300\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.3235 - acc: 0.9500 - val_loss: 0.7774 - val_acc: 0.4000\n",
            "Epoch 76/300\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.3533 - acc: 0.9500 - val_loss: 0.7784 - val_acc: 0.3000\n",
            "Epoch 77/300\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.3038 - acc: 0.9667 - val_loss: 0.7799 - val_acc: 0.4000\n",
            "Epoch 78/300\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.3055 - acc: 1.0000 - val_loss: 0.7820 - val_acc: 0.4000\n",
            "Epoch 79/300\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.3120 - acc: 0.9500 - val_loss: 0.7885 - val_acc: 0.3500\n",
            "Epoch 80/300\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.3071 - acc: 0.9833 - val_loss: 0.7910 - val_acc: 0.4500\n",
            "Epoch 81/300\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.3384 - acc: 0.9333 - val_loss: 0.7933 - val_acc: 0.3500\n",
            "Epoch 82/300\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.3232 - acc: 0.9667 - val_loss: 0.7908 - val_acc: 0.4500\n",
            "Epoch 83/300\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.3062 - acc: 0.9833 - val_loss: 0.7862 - val_acc: 0.4500\n",
            "Epoch 84/300\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.3102 - acc: 0.9667 - val_loss: 0.7792 - val_acc: 0.4500\n",
            "Epoch 85/300\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.2888 - acc: 0.9667 - val_loss: 0.7766 - val_acc: 0.5000\n",
            "Epoch 86/300\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.2998 - acc: 0.9833 - val_loss: 0.7741 - val_acc: 0.5000\n",
            "Epoch 87/300\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.2992 - acc: 0.9833 - val_loss: 0.7726 - val_acc: 0.5000\n",
            "Epoch 88/300\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.2923 - acc: 1.0000 - val_loss: 0.7710 - val_acc: 0.5000\n",
            "Epoch 89/300\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.2918 - acc: 1.0000 - val_loss: 0.7716 - val_acc: 0.6000\n",
            "Epoch 90/300\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.3219 - acc: 0.9333 - val_loss: 0.7768 - val_acc: 0.6000\n",
            "Epoch 91/300\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.2986 - acc: 0.9667 - val_loss: 0.7877 - val_acc: 0.5500\n",
            "Epoch 92/300\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.2981 - acc: 0.9667 - val_loss: 0.7935 - val_acc: 0.5000\n",
            "Epoch 93/300\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.2989 - acc: 0.9667 - val_loss: 0.7895 - val_acc: 0.5000\n",
            "Epoch 94/300\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.2849 - acc: 0.9667 - val_loss: 0.7866 - val_acc: 0.5000\n",
            "Epoch 95/300\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.2813 - acc: 0.9833 - val_loss: 0.7871 - val_acc: 0.5500\n",
            "Epoch 96/300\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.2784 - acc: 0.9833 - val_loss: 0.7868 - val_acc: 0.5000\n",
            "Epoch 97/300\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.2712 - acc: 0.9667 - val_loss: 0.7878 - val_acc: 0.4000\n",
            "Epoch 98/300\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.2754 - acc: 0.9667 - val_loss: 0.7896 - val_acc: 0.4000\n",
            "Epoch 99/300\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.3004 - acc: 0.9667 - val_loss: 0.7855 - val_acc: 0.4000\n",
            "Epoch 100/300\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.2833 - acc: 0.9833 - val_loss: 0.7815 - val_acc: 0.5000\n",
            "Epoch 101/300\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.2662 - acc: 0.9833 - val_loss: 0.7751 - val_acc: 0.5000\n",
            "Epoch 102/300\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.2797 - acc: 0.9833 - val_loss: 0.7697 - val_acc: 0.5500\n",
            "Epoch 103/300\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.2660 - acc: 0.9833 - val_loss: 0.7693 - val_acc: 0.5500\n",
            "Epoch 104/300\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.2566 - acc: 0.9833 - val_loss: 0.7690 - val_acc: 0.5500\n",
            "Epoch 105/300\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.2633 - acc: 0.9833 - val_loss: 0.7750 - val_acc: 0.5500\n",
            "Epoch 106/300\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.2419 - acc: 0.9833 - val_loss: 0.7762 - val_acc: 0.5500\n",
            "Epoch 107/300\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.2550 - acc: 0.9833 - val_loss: 0.7784 - val_acc: 0.5500\n",
            "Epoch 108/300\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.2709 - acc: 0.9667 - val_loss: 0.7791 - val_acc: 0.5000\n",
            "Epoch 109/300\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.2648 - acc: 0.9667 - val_loss: 0.7786 - val_acc: 0.5500\n",
            "Epoch 110/300\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.2767 - acc: 0.9667 - val_loss: 0.7742 - val_acc: 0.5500\n",
            "Epoch 111/300\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.2460 - acc: 1.0000 - val_loss: 0.7609 - val_acc: 0.5500\n",
            "Epoch 112/300\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.2419 - acc: 0.9833 - val_loss: 0.7567 - val_acc: 0.5500\n",
            "Epoch 113/300\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.2482 - acc: 0.9667 - val_loss: 0.7592 - val_acc: 0.5000\n",
            "Epoch 114/300\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.2409 - acc: 1.0000 - val_loss: 0.7658 - val_acc: 0.5500\n",
            "Epoch 115/300\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.2476 - acc: 0.9833 - val_loss: 0.7687 - val_acc: 0.5500\n",
            "Epoch 116/300\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.2370 - acc: 0.9833 - val_loss: 0.7606 - val_acc: 0.6500\n",
            "Epoch 117/300\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.2571 - acc: 0.9833 - val_loss: 0.7611 - val_acc: 0.6500\n",
            "Epoch 118/300\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.2482 - acc: 0.9333 - val_loss: 0.7599 - val_acc: 0.6500\n",
            "Epoch 119/300\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.2620 - acc: 0.9833 - val_loss: 0.7551 - val_acc: 0.5500\n",
            "Epoch 120/300\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.2483 - acc: 1.0000 - val_loss: 0.7489 - val_acc: 0.6000\n",
            "Epoch 121/300\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.2381 - acc: 0.9833 - val_loss: 0.7442 - val_acc: 0.6000\n",
            "Epoch 122/300\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.2279 - acc: 0.9833 - val_loss: 0.7487 - val_acc: 0.6000\n",
            "Epoch 123/300\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.2356 - acc: 0.9667 - val_loss: 0.7485 - val_acc: 0.5500\n",
            "Epoch 124/300\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.2455 - acc: 0.9833 - val_loss: 0.7424 - val_acc: 0.5500\n",
            "Epoch 125/300\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.2143 - acc: 0.9833 - val_loss: 0.7437 - val_acc: 0.6000\n",
            "Epoch 126/300\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.2369 - acc: 0.9667 - val_loss: 0.7319 - val_acc: 0.6000\n",
            "Epoch 127/300\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.2405 - acc: 0.9667 - val_loss: 0.7276 - val_acc: 0.6000\n",
            "Epoch 128/300\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.2268 - acc: 0.9833 - val_loss: 0.7292 - val_acc: 0.6000\n",
            "Epoch 129/300\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.2377 - acc: 0.9667 - val_loss: 0.7377 - val_acc: 0.6500\n",
            "Epoch 130/300\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.1973 - acc: 1.0000 - val_loss: 0.7375 - val_acc: 0.6500\n",
            "Epoch 131/300\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.2141 - acc: 0.9833 - val_loss: 0.7396 - val_acc: 0.6500\n",
            "Epoch 132/300\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.2665 - acc: 0.9667 - val_loss: 0.7410 - val_acc: 0.6500\n",
            "Epoch 133/300\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.2453 - acc: 0.9833 - val_loss: 0.7434 - val_acc: 0.6500\n",
            "Epoch 134/300\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.1969 - acc: 1.0000 - val_loss: 0.7461 - val_acc: 0.6500\n",
            "Epoch 135/300\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.2209 - acc: 1.0000 - val_loss: 0.7498 - val_acc: 0.6000\n",
            "Epoch 136/300\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.2378 - acc: 0.9667 - val_loss: 0.7568 - val_acc: 0.5500\n",
            "Epoch 137/300\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.2177 - acc: 0.9833 - val_loss: 0.7564 - val_acc: 0.6500\n",
            "Epoch 138/300\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.2226 - acc: 0.9333 - val_loss: 0.7565 - val_acc: 0.6500\n",
            "Epoch 139/300\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.2030 - acc: 0.9667 - val_loss: 0.7399 - val_acc: 0.6000\n",
            "Epoch 140/300\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.2467 - acc: 0.9333 - val_loss: 0.7379 - val_acc: 0.6000\n",
            "Epoch 141/300\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.2363 - acc: 0.9667 - val_loss: 0.7259 - val_acc: 0.6000\n",
            "Epoch 142/300\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.2560 - acc: 0.9667 - val_loss: 0.7178 - val_acc: 0.6500\n",
            "Epoch 143/300\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.2154 - acc: 0.9667 - val_loss: 0.7140 - val_acc: 0.6500\n",
            "Epoch 144/300\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.2504 - acc: 0.9667 - val_loss: 0.7112 - val_acc: 0.6500\n",
            "Epoch 145/300\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.1915 - acc: 0.9833 - val_loss: 0.7127 - val_acc: 0.5500\n",
            "Epoch 146/300\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.2423 - acc: 0.9333 - val_loss: 0.7097 - val_acc: 0.6500\n",
            "Epoch 147/300\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.2159 - acc: 0.9833 - val_loss: 0.7125 - val_acc: 0.6500\n",
            "Epoch 148/300\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.2252 - acc: 1.0000 - val_loss: 0.7190 - val_acc: 0.6500\n",
            "Epoch 149/300\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.2061 - acc: 0.9833 - val_loss: 0.7182 - val_acc: 0.6000\n",
            "Epoch 150/300\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.2080 - acc: 0.9667 - val_loss: 0.7287 - val_acc: 0.6500\n",
            "Epoch 151/300\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.2377 - acc: 0.9500 - val_loss: 0.7217 - val_acc: 0.6000\n",
            "Epoch 152/300\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.2018 - acc: 0.9667 - val_loss: 0.7210 - val_acc: 0.6000\n",
            "Epoch 153/300\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.1869 - acc: 1.0000 - val_loss: 0.7175 - val_acc: 0.6000\n",
            "Epoch 154/300\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.2145 - acc: 0.9667 - val_loss: 0.7234 - val_acc: 0.6000\n",
            "Epoch 155/300\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.2365 - acc: 0.9333 - val_loss: 0.7336 - val_acc: 0.5500\n",
            "Epoch 156/300\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.1964 - acc: 0.9833 - val_loss: 0.7310 - val_acc: 0.6500\n",
            "Epoch 157/300\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.2082 - acc: 0.9667 - val_loss: 0.7367 - val_acc: 0.6500\n",
            "Epoch 158/300\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.1717 - acc: 1.0000 - val_loss: 0.7443 - val_acc: 0.6500\n",
            "Epoch 159/300\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.2184 - acc: 0.9500 - val_loss: 0.7475 - val_acc: 0.6500\n",
            "Epoch 160/300\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.1996 - acc: 1.0000 - val_loss: 0.7395 - val_acc: 0.6500\n",
            "Epoch 161/300\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.2171 - acc: 0.9833 - val_loss: 0.7304 - val_acc: 0.6500\n",
            "Epoch 162/300\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.1961 - acc: 0.9833 - val_loss: 0.7235 - val_acc: 0.6500\n",
            "Epoch 163/300\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.2188 - acc: 0.9833 - val_loss: 0.7085 - val_acc: 0.6500\n",
            "Epoch 164/300\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.2210 - acc: 0.9667 - val_loss: 0.7046 - val_acc: 0.6500\n",
            "Epoch 165/300\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.1981 - acc: 0.9667 - val_loss: 0.7059 - val_acc: 0.6000\n",
            "Epoch 166/300\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.1842 - acc: 0.9667 - val_loss: 0.7015 - val_acc: 0.5500\n",
            "Epoch 167/300\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.1927 - acc: 1.0000 - val_loss: 0.7031 - val_acc: 0.5500\n",
            "Epoch 168/300\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.1597 - acc: 1.0000 - val_loss: 0.6989 - val_acc: 0.6000\n",
            "Epoch 169/300\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.1902 - acc: 0.9833 - val_loss: 0.6985 - val_acc: 0.6500\n",
            "Epoch 170/300\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.1846 - acc: 0.9833 - val_loss: 0.7180 - val_acc: 0.6500\n",
            "Epoch 171/300\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.1951 - acc: 0.9667 - val_loss: 0.7443 - val_acc: 0.7000\n",
            "Epoch 172/300\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.1815 - acc: 1.0000 - val_loss: 0.7499 - val_acc: 0.6500\n",
            "Epoch 173/300\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.2039 - acc: 0.9667 - val_loss: 0.7536 - val_acc: 0.6500\n",
            "Epoch 174/300\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.2072 - acc: 0.9500 - val_loss: 0.7692 - val_acc: 0.6000\n",
            "Epoch 175/300\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.2004 - acc: 1.0000 - val_loss: 0.7711 - val_acc: 0.5500\n",
            "Epoch 176/300\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.1763 - acc: 0.9833 - val_loss: 0.7929 - val_acc: 0.5500\n",
            "Epoch 177/300\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.1781 - acc: 1.0000 - val_loss: 0.7955 - val_acc: 0.5500\n",
            "Epoch 178/300\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.1702 - acc: 1.0000 - val_loss: 0.8026 - val_acc: 0.4500\n",
            "Epoch 179/300\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.2295 - acc: 0.9500 - val_loss: 0.8149 - val_acc: 0.5000\n",
            "Epoch 180/300\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.1740 - acc: 0.9833 - val_loss: 0.8117 - val_acc: 0.5000\n",
            "Epoch 181/300\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.1773 - acc: 1.0000 - val_loss: 0.7979 - val_acc: 0.5500\n",
            "Epoch 182/300\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.1977 - acc: 0.9833 - val_loss: 0.8050 - val_acc: 0.5000\n",
            "Epoch 183/300\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.1611 - acc: 0.9833 - val_loss: 0.8338 - val_acc: 0.4500\n",
            "Epoch 184/300\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.1784 - acc: 1.0000 - val_loss: 0.8336 - val_acc: 0.4500\n",
            "Epoch 185/300\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.2000 - acc: 0.9667 - val_loss: 0.8207 - val_acc: 0.4500\n",
            "Epoch 186/300\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.1543 - acc: 0.9833 - val_loss: 0.7952 - val_acc: 0.6000\n",
            "Epoch 187/300\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.1651 - acc: 1.0000 - val_loss: 0.7830 - val_acc: 0.6000\n",
            "Epoch 188/300\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.1682 - acc: 1.0000 - val_loss: 0.7819 - val_acc: 0.6000\n",
            "Epoch 189/300\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.1947 - acc: 0.9667 - val_loss: 0.7994 - val_acc: 0.5500\n",
            "Epoch 190/300\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.1735 - acc: 0.9833 - val_loss: 0.8068 - val_acc: 0.5500\n",
            "Epoch 191/300\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.1749 - acc: 0.9667 - val_loss: 0.7965 - val_acc: 0.6000\n",
            "Epoch 192/300\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.1709 - acc: 0.9833 - val_loss: 0.7917 - val_acc: 0.6500\n",
            "Epoch 193/300\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.1742 - acc: 1.0000 - val_loss: 0.7823 - val_acc: 0.6500\n",
            "Epoch 194/300\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.2129 - acc: 0.9500 - val_loss: 0.7780 - val_acc: 0.6000\n",
            "Epoch 195/300\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.1691 - acc: 1.0000 - val_loss: 0.7806 - val_acc: 0.6000\n",
            "Epoch 196/300\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.1752 - acc: 1.0000 - val_loss: 0.7784 - val_acc: 0.6000\n",
            "Epoch 197/300\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.1488 - acc: 1.0000 - val_loss: 0.7825 - val_acc: 0.6000\n",
            "Epoch 198/300\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.1861 - acc: 0.9833 - val_loss: 0.7696 - val_acc: 0.6500\n",
            "Epoch 199/300\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.1523 - acc: 1.0000 - val_loss: 0.7525 - val_acc: 0.6500\n",
            "Epoch 200/300\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.1609 - acc: 1.0000 - val_loss: 0.7428 - val_acc: 0.6500\n",
            "Epoch 201/300\n",
            "\n",
            "Epoch 00201: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.1846 - acc: 0.9833 - val_loss: 0.7358 - val_acc: 0.6500\n",
            "Epoch 202/300\n",
            "\n",
            "Epoch 00202: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.1691 - acc: 1.0000 - val_loss: 0.7451 - val_acc: 0.6500\n",
            "Epoch 203/300\n",
            "\n",
            "Epoch 00203: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.1762 - acc: 0.9500 - val_loss: 0.7577 - val_acc: 0.6500\n",
            "Epoch 204/300\n",
            "\n",
            "Epoch 00204: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.1859 - acc: 0.9500 - val_loss: 0.7587 - val_acc: 0.6000\n",
            "Epoch 205/300\n",
            "\n",
            "Epoch 00205: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.1655 - acc: 0.9833 - val_loss: 0.7592 - val_acc: 0.6000\n",
            "Epoch 206/300\n",
            "\n",
            "Epoch 00206: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.1693 - acc: 0.9833 - val_loss: 0.7754 - val_acc: 0.6500\n",
            "Epoch 207/300\n",
            "\n",
            "Epoch 00207: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.1678 - acc: 0.9833 - val_loss: 0.7808 - val_acc: 0.6000\n",
            "Epoch 208/300\n",
            "\n",
            "Epoch 00208: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.1745 - acc: 0.9833 - val_loss: 0.7790 - val_acc: 0.6000\n",
            "Epoch 209/300\n",
            "\n",
            "Epoch 00209: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.1771 - acc: 0.9833 - val_loss: 0.7783 - val_acc: 0.6000\n",
            "Epoch 210/300\n",
            "\n",
            "Epoch 00210: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.1768 - acc: 0.9667 - val_loss: 0.7640 - val_acc: 0.6500\n",
            "Epoch 211/300\n",
            "\n",
            "Epoch 00211: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.1338 - acc: 1.0000 - val_loss: 0.7498 - val_acc: 0.7000\n",
            "Epoch 212/300\n",
            "\n",
            "Epoch 00212: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.1686 - acc: 0.9833 - val_loss: 0.7495 - val_acc: 0.7000\n",
            "Epoch 213/300\n",
            "\n",
            "Epoch 00213: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.1522 - acc: 1.0000 - val_loss: 0.7502 - val_acc: 0.7000\n",
            "Epoch 214/300\n",
            "\n",
            "Epoch 00214: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.1859 - acc: 0.9667 - val_loss: 0.7794 - val_acc: 0.6500\n",
            "Epoch 215/300\n",
            "\n",
            "Epoch 00215: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.1773 - acc: 1.0000 - val_loss: 0.8047 - val_acc: 0.6000\n",
            "Epoch 216/300\n",
            "\n",
            "Epoch 00216: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.1478 - acc: 0.9833 - val_loss: 0.8039 - val_acc: 0.6000\n",
            "Epoch 217/300\n",
            "\n",
            "Epoch 00217: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.1276 - acc: 1.0000 - val_loss: 0.8014 - val_acc: 0.5500\n",
            "Epoch 218/300\n",
            "\n",
            "Epoch 00218: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.1982 - acc: 0.9667 - val_loss: 0.7830 - val_acc: 0.6000\n",
            "Epoch 219/300\n",
            "\n",
            "Epoch 00219: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.1430 - acc: 1.0000 - val_loss: 0.7725 - val_acc: 0.6000\n",
            "Epoch 220/300\n",
            "\n",
            "Epoch 00220: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.1593 - acc: 0.9833 - val_loss: 0.7512 - val_acc: 0.6500\n",
            "Epoch 221/300\n",
            "\n",
            "Epoch 00221: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.1937 - acc: 0.9833 - val_loss: 0.7363 - val_acc: 0.6500\n",
            "Epoch 222/300\n",
            "\n",
            "Epoch 00222: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.1361 - acc: 0.9833 - val_loss: 0.7409 - val_acc: 0.6000\n",
            "Epoch 223/300\n",
            "\n",
            "Epoch 00223: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.1663 - acc: 0.9833 - val_loss: 0.7499 - val_acc: 0.6000\n",
            "Epoch 224/300\n",
            "\n",
            "Epoch 00224: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.1691 - acc: 1.0000 - val_loss: 0.7506 - val_acc: 0.7000\n",
            "Epoch 225/300\n",
            "\n",
            "Epoch 00225: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.1467 - acc: 1.0000 - val_loss: 0.7626 - val_acc: 0.7000\n",
            "Epoch 226/300\n",
            "\n",
            "Epoch 00226: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.1942 - acc: 0.9833 - val_loss: 0.7802 - val_acc: 0.6000\n",
            "Epoch 227/300\n",
            "\n",
            "Epoch 00227: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.1435 - acc: 1.0000 - val_loss: 0.7883 - val_acc: 0.6000\n",
            "Epoch 228/300\n",
            "\n",
            "Epoch 00228: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.1439 - acc: 1.0000 - val_loss: 0.7855 - val_acc: 0.6000\n",
            "Epoch 229/300\n",
            "\n",
            "Epoch 00229: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.1419 - acc: 1.0000 - val_loss: 0.7779 - val_acc: 0.6500\n",
            "Epoch 230/300\n",
            "\n",
            "Epoch 00230: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.1937 - acc: 0.9500 - val_loss: 0.7725 - val_acc: 0.6500\n",
            "Epoch 231/300\n",
            "\n",
            "Epoch 00231: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.1275 - acc: 1.0000 - val_loss: 0.7478 - val_acc: 0.6500\n",
            "Epoch 232/300\n",
            "\n",
            "Epoch 00232: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.2107 - acc: 0.9167 - val_loss: 0.7385 - val_acc: 0.6500\n",
            "Epoch 233/300\n",
            "\n",
            "Epoch 00233: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.1249 - acc: 1.0000 - val_loss: 0.7593 - val_acc: 0.6500\n",
            "Epoch 234/300\n",
            "\n",
            "Epoch 00234: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.1520 - acc: 0.9667 - val_loss: 0.8040 - val_acc: 0.6500\n",
            "Epoch 235/300\n",
            "\n",
            "Epoch 00235: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.1628 - acc: 0.9833 - val_loss: 0.8441 - val_acc: 0.5000\n",
            "Epoch 236/300\n",
            "\n",
            "Epoch 00236: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.1685 - acc: 1.0000 - val_loss: 0.8358 - val_acc: 0.6500\n",
            "Epoch 237/300\n",
            "\n",
            "Epoch 00237: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.1592 - acc: 0.9667 - val_loss: 0.8017 - val_acc: 0.6500\n",
            "Epoch 238/300\n",
            "\n",
            "Epoch 00238: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.1568 - acc: 1.0000 - val_loss: 0.7574 - val_acc: 0.7000\n",
            "Epoch 239/300\n",
            "\n",
            "Epoch 00239: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.1504 - acc: 1.0000 - val_loss: 0.7268 - val_acc: 0.7000\n",
            "Epoch 240/300\n",
            "\n",
            "Epoch 00240: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.1584 - acc: 0.9667 - val_loss: 0.7103 - val_acc: 0.7000\n",
            "Epoch 241/300\n",
            "\n",
            "Epoch 00241: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.1121 - acc: 1.0000 - val_loss: 0.7055 - val_acc: 0.7000\n",
            "Epoch 242/300\n",
            "\n",
            "Epoch 00242: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.1635 - acc: 0.9833 - val_loss: 0.6998 - val_acc: 0.7000\n",
            "Epoch 243/300\n",
            "\n",
            "Epoch 00243: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.1541 - acc: 1.0000 - val_loss: 0.7245 - val_acc: 0.6500\n",
            "Epoch 244/300\n",
            "\n",
            "Epoch 00244: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.1484 - acc: 1.0000 - val_loss: 0.7354 - val_acc: 0.6500\n",
            "Epoch 245/300\n",
            "\n",
            "Epoch 00245: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.1714 - acc: 0.9833 - val_loss: 0.7332 - val_acc: 0.6500\n",
            "Epoch 246/300\n",
            "\n",
            "Epoch 00246: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.1538 - acc: 0.9833 - val_loss: 0.7510 - val_acc: 0.6500\n",
            "Epoch 247/300\n",
            "\n",
            "Epoch 00247: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.1850 - acc: 0.9500 - val_loss: 0.7520 - val_acc: 0.6500\n",
            "Epoch 248/300\n",
            "\n",
            "Epoch 00248: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.1667 - acc: 0.9833 - val_loss: 0.7441 - val_acc: 0.7000\n",
            "Epoch 249/300\n",
            "\n",
            "Epoch 00249: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.1220 - acc: 0.9833 - val_loss: 0.7759 - val_acc: 0.6500\n",
            "Epoch 250/300\n",
            "\n",
            "Epoch 00250: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.1485 - acc: 0.9833 - val_loss: 0.7905 - val_acc: 0.6500\n",
            "Epoch 251/300\n",
            "\n",
            "Epoch 00251: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.1566 - acc: 0.9833 - val_loss: 0.7773 - val_acc: 0.6500\n",
            "Epoch 252/300\n",
            "\n",
            "Epoch 00252: val_loss did not improve from 0.69062\n",
            "60/60 - 0s - loss: 0.1737 - acc: 0.9667 - val_loss: 0.7257 - val_acc: 0.8000\n",
            "Epoch 253/300\n",
            "\n",
            "Epoch 00253: val_loss improved from 0.69062 to 0.68768, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1539 - acc: 0.9833 - val_loss: 0.6877 - val_acc: 0.7000\n",
            "Epoch 254/300\n",
            "\n",
            "Epoch 00254: val_loss improved from 0.68768 to 0.66995, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1622 - acc: 0.9833 - val_loss: 0.6699 - val_acc: 0.7000\n",
            "Epoch 255/300\n",
            "\n",
            "Epoch 00255: val_loss did not improve from 0.66995\n",
            "60/60 - 0s - loss: 0.1446 - acc: 1.0000 - val_loss: 0.6792 - val_acc: 0.7000\n",
            "Epoch 256/300\n",
            "\n",
            "Epoch 00256: val_loss did not improve from 0.66995\n",
            "60/60 - 0s - loss: 0.1818 - acc: 0.9500 - val_loss: 0.6988 - val_acc: 0.7500\n",
            "Epoch 257/300\n",
            "\n",
            "Epoch 00257: val_loss did not improve from 0.66995\n",
            "60/60 - 0s - loss: 0.1625 - acc: 0.9833 - val_loss: 0.7264 - val_acc: 0.7000\n",
            "Epoch 258/300\n",
            "\n",
            "Epoch 00258: val_loss did not improve from 0.66995\n",
            "60/60 - 0s - loss: 0.1231 - acc: 1.0000 - val_loss: 0.7526 - val_acc: 0.7500\n",
            "Epoch 259/300\n",
            "\n",
            "Epoch 00259: val_loss did not improve from 0.66995\n",
            "60/60 - 0s - loss: 0.1273 - acc: 0.9833 - val_loss: 0.7542 - val_acc: 0.7500\n",
            "Epoch 260/300\n",
            "\n",
            "Epoch 00260: val_loss did not improve from 0.66995\n",
            "60/60 - 0s - loss: 0.1474 - acc: 0.9667 - val_loss: 0.7274 - val_acc: 0.7500\n",
            "Epoch 261/300\n",
            "\n",
            "Epoch 00261: val_loss did not improve from 0.66995\n",
            "60/60 - 0s - loss: 0.1378 - acc: 0.9833 - val_loss: 0.6957 - val_acc: 0.7500\n",
            "Epoch 262/300\n",
            "\n",
            "Epoch 00262: val_loss did not improve from 0.66995\n",
            "60/60 - 0s - loss: 0.1591 - acc: 0.9833 - val_loss: 0.6823 - val_acc: 0.6500\n",
            "Epoch 263/300\n",
            "\n",
            "Epoch 00263: val_loss did not improve from 0.66995\n",
            "60/60 - 0s - loss: 0.1441 - acc: 0.9667 - val_loss: 0.6951 - val_acc: 0.6500\n",
            "Epoch 264/300\n",
            "\n",
            "Epoch 00264: val_loss did not improve from 0.66995\n",
            "60/60 - 0s - loss: 0.1442 - acc: 0.9833 - val_loss: 0.7359 - val_acc: 0.7500\n",
            "Epoch 265/300\n",
            "\n",
            "Epoch 00265: val_loss did not improve from 0.66995\n",
            "60/60 - 0s - loss: 0.1731 - acc: 0.9667 - val_loss: 0.7631 - val_acc: 0.7500\n",
            "Epoch 266/300\n",
            "\n",
            "Epoch 00266: val_loss did not improve from 0.66995\n",
            "60/60 - 0s - loss: 0.1481 - acc: 0.9833 - val_loss: 0.7521 - val_acc: 0.7000\n",
            "Epoch 267/300\n",
            "\n",
            "Epoch 00267: val_loss did not improve from 0.66995\n",
            "60/60 - 0s - loss: 0.1361 - acc: 1.0000 - val_loss: 0.7229 - val_acc: 0.7000\n",
            "Epoch 268/300\n",
            "\n",
            "Epoch 00268: val_loss did not improve from 0.66995\n",
            "60/60 - 0s - loss: 0.1546 - acc: 1.0000 - val_loss: 0.7153 - val_acc: 0.7000\n",
            "Epoch 269/300\n",
            "\n",
            "Epoch 00269: val_loss did not improve from 0.66995\n",
            "60/60 - 0s - loss: 0.1407 - acc: 1.0000 - val_loss: 0.7291 - val_acc: 0.7500\n",
            "Epoch 270/300\n",
            "\n",
            "Epoch 00270: val_loss did not improve from 0.66995\n",
            "60/60 - 0s - loss: 0.1192 - acc: 0.9833 - val_loss: 0.7670 - val_acc: 0.7500\n",
            "Epoch 271/300\n",
            "\n",
            "Epoch 00271: val_loss did not improve from 0.66995\n",
            "60/60 - 0s - loss: 0.1223 - acc: 1.0000 - val_loss: 0.7789 - val_acc: 0.7500\n",
            "Epoch 272/300\n",
            "\n",
            "Epoch 00272: val_loss did not improve from 0.66995\n",
            "60/60 - 0s - loss: 0.1342 - acc: 1.0000 - val_loss: 0.7865 - val_acc: 0.6500\n",
            "Epoch 273/300\n",
            "\n",
            "Epoch 00273: val_loss did not improve from 0.66995\n",
            "60/60 - 0s - loss: 0.1143 - acc: 1.0000 - val_loss: 0.8112 - val_acc: 0.7000\n",
            "Epoch 274/300\n",
            "\n",
            "Epoch 00274: val_loss did not improve from 0.66995\n",
            "60/60 - 0s - loss: 0.1576 - acc: 0.9833 - val_loss: 0.8248 - val_acc: 0.7000\n",
            "Epoch 275/300\n",
            "\n",
            "Epoch 00275: val_loss did not improve from 0.66995\n",
            "60/60 - 0s - loss: 0.1553 - acc: 0.9667 - val_loss: 0.8385 - val_acc: 0.6500\n",
            "Epoch 276/300\n",
            "\n",
            "Epoch 00276: val_loss did not improve from 0.66995\n",
            "60/60 - 0s - loss: 0.1125 - acc: 1.0000 - val_loss: 0.8478 - val_acc: 0.6500\n",
            "Epoch 277/300\n",
            "\n",
            "Epoch 00277: val_loss did not improve from 0.66995\n",
            "60/60 - 0s - loss: 0.1341 - acc: 0.9833 - val_loss: 0.8360 - val_acc: 0.6000\n",
            "Epoch 278/300\n",
            "\n",
            "Epoch 00278: val_loss did not improve from 0.66995\n",
            "60/60 - 0s - loss: 0.1172 - acc: 1.0000 - val_loss: 0.7983 - val_acc: 0.7000\n",
            "Epoch 279/300\n",
            "\n",
            "Epoch 00279: val_loss did not improve from 0.66995\n",
            "60/60 - 0s - loss: 0.1547 - acc: 0.9833 - val_loss: 0.7783 - val_acc: 0.7000\n",
            "Epoch 280/300\n",
            "\n",
            "Epoch 00280: val_loss did not improve from 0.66995\n",
            "60/60 - 0s - loss: 0.1487 - acc: 0.9833 - val_loss: 0.7710 - val_acc: 0.7500\n",
            "Epoch 281/300\n",
            "\n",
            "Epoch 00281: val_loss did not improve from 0.66995\n",
            "60/60 - 0s - loss: 0.1311 - acc: 0.9667 - val_loss: 0.7902 - val_acc: 0.7500\n",
            "Epoch 282/300\n",
            "\n",
            "Epoch 00282: val_loss did not improve from 0.66995\n",
            "60/60 - 0s - loss: 0.1228 - acc: 1.0000 - val_loss: 0.8186 - val_acc: 0.7500\n",
            "Epoch 283/300\n",
            "\n",
            "Epoch 00283: val_loss did not improve from 0.66995\n",
            "60/60 - 0s - loss: 0.1089 - acc: 0.9833 - val_loss: 0.8260 - val_acc: 0.7500\n",
            "Epoch 284/300\n",
            "\n",
            "Epoch 00284: val_loss did not improve from 0.66995\n",
            "60/60 - 0s - loss: 0.1151 - acc: 1.0000 - val_loss: 0.7880 - val_acc: 0.7500\n",
            "Epoch 285/300\n",
            "\n",
            "Epoch 00285: val_loss did not improve from 0.66995\n",
            "60/60 - 0s - loss: 0.1197 - acc: 1.0000 - val_loss: 0.7429 - val_acc: 0.7500\n",
            "Epoch 286/300\n",
            "\n",
            "Epoch 00286: val_loss did not improve from 0.66995\n",
            "60/60 - 0s - loss: 0.1109 - acc: 0.9833 - val_loss: 0.7159 - val_acc: 0.8000\n",
            "Epoch 287/300\n",
            "\n",
            "Epoch 00287: val_loss did not improve from 0.66995\n",
            "60/60 - 0s - loss: 0.1123 - acc: 0.9833 - val_loss: 0.7146 - val_acc: 0.7000\n",
            "Epoch 288/300\n",
            "\n",
            "Epoch 00288: val_loss did not improve from 0.66995\n",
            "60/60 - 0s - loss: 0.1300 - acc: 1.0000 - val_loss: 0.7269 - val_acc: 0.7000\n",
            "Epoch 289/300\n",
            "\n",
            "Epoch 00289: val_loss did not improve from 0.66995\n",
            "60/60 - 0s - loss: 0.1326 - acc: 0.9833 - val_loss: 0.7553 - val_acc: 0.7000\n",
            "Epoch 290/300\n",
            "\n",
            "Epoch 00290: val_loss did not improve from 0.66995\n",
            "60/60 - 0s - loss: 0.1069 - acc: 1.0000 - val_loss: 0.7948 - val_acc: 0.6000\n",
            "Epoch 291/300\n",
            "\n",
            "Epoch 00291: val_loss did not improve from 0.66995\n",
            "60/60 - 0s - loss: 0.1308 - acc: 0.9833 - val_loss: 0.8215 - val_acc: 0.6500\n",
            "Epoch 292/300\n",
            "\n",
            "Epoch 00292: val_loss did not improve from 0.66995\n",
            "60/60 - 0s - loss: 0.1740 - acc: 0.9667 - val_loss: 0.7638 - val_acc: 0.7000\n",
            "Epoch 293/300\n",
            "\n",
            "Epoch 00293: val_loss did not improve from 0.66995\n",
            "60/60 - 0s - loss: 0.1319 - acc: 0.9833 - val_loss: 0.7486 - val_acc: 0.7500\n",
            "Epoch 294/300\n",
            "\n",
            "Epoch 00294: val_loss did not improve from 0.66995\n",
            "60/60 - 0s - loss: 0.1305 - acc: 0.9833 - val_loss: 0.7410 - val_acc: 0.8000\n",
            "Epoch 295/300\n",
            "\n",
            "Epoch 00295: val_loss did not improve from 0.66995\n",
            "60/60 - 0s - loss: 0.1073 - acc: 1.0000 - val_loss: 0.7276 - val_acc: 0.7500\n",
            "Epoch 296/300\n",
            "\n",
            "Epoch 00296: val_loss did not improve from 0.66995\n",
            "60/60 - 0s - loss: 0.1256 - acc: 1.0000 - val_loss: 0.7500 - val_acc: 0.7500\n",
            "Epoch 297/300\n",
            "\n",
            "Epoch 00297: val_loss did not improve from 0.66995\n",
            "60/60 - 0s - loss: 0.1063 - acc: 1.0000 - val_loss: 0.8013 - val_acc: 0.7500\n",
            "Epoch 298/300\n",
            "\n",
            "Epoch 00298: val_loss did not improve from 0.66995\n",
            "60/60 - 0s - loss: 0.1361 - acc: 1.0000 - val_loss: 0.8334 - val_acc: 0.7500\n",
            "Epoch 299/300\n",
            "\n",
            "Epoch 00299: val_loss did not improve from 0.66995\n",
            "60/60 - 0s - loss: 0.1279 - acc: 1.0000 - val_loss: 0.8174 - val_acc: 0.7500\n",
            "Epoch 300/300\n",
            "\n",
            "Epoch 00300: val_loss did not improve from 0.66995\n",
            "60/60 - 0s - loss: 0.1285 - acc: 0.9833 - val_loss: 0.7889 - val_acc: 0.7500\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "# Evaluate on test data\n",
            "40/40 [==============================] - 0s 2ms/sample - loss: 0.5777 - acc: 0.7500\n",
            "test loss, test acc: [0.577663104236126, 0.75]\n",
            "EEG_Deep/Data2A/parsed_P08T.mat\n",
            "Filtering of Training Data Finished\n",
            "EEG_Deep/Data2A/parsed_P08E.mat\n",
            "Filtering of Testing Data Finished\n",
            "(1, 2)\n",
            "Finding labels in training data\n",
            "Finding labels in testing data\n",
            "[2 2 1 1 1 2 2 2 2 1 2 1 1 2 1 1 1 1 1 1]\n",
            "(60, 12, 1536)\n",
            "(60,)\n",
            "(20, 12, 1536)\n",
            "(20,)\n",
            "[[0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]]\n",
            "X_train shape: (60, 1, 12, 1536)\n",
            "60 train samples\n",
            "20 val samples\n",
            "(40, 12, 1536)\n",
            "(40, 1)\n",
            "X_train shape: (40, 1, 12, 1536)\n",
            "40 train samples\n",
            "Train on 60 samples, validate on 20 samples\n",
            "Epoch 1/300\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.70309, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 1s - loss: 0.7031 - acc: 0.4667 - val_loss: 0.7031 - val_acc: 0.4000\n",
            "Epoch 2/300\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.70309\n",
            "60/60 - 0s - loss: 0.6678 - acc: 0.5667 - val_loss: 0.7117 - val_acc: 0.4000\n",
            "Epoch 3/300\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.70309\n",
            "60/60 - 0s - loss: 0.6372 - acc: 0.6000 - val_loss: 0.7170 - val_acc: 0.4000\n",
            "Epoch 4/300\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.70309\n",
            "60/60 - 0s - loss: 0.6341 - acc: 0.5833 - val_loss: 0.7201 - val_acc: 0.4000\n",
            "Epoch 5/300\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.70309\n",
            "60/60 - 0s - loss: 0.6260 - acc: 0.6667 - val_loss: 0.7232 - val_acc: 0.4000\n",
            "Epoch 6/300\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.70309\n",
            "60/60 - 0s - loss: 0.5921 - acc: 0.7333 - val_loss: 0.7257 - val_acc: 0.4000\n",
            "Epoch 7/300\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.70309\n",
            "60/60 - 0s - loss: 0.5582 - acc: 0.7833 - val_loss: 0.7269 - val_acc: 0.4000\n",
            "Epoch 8/300\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.70309\n",
            "60/60 - 0s - loss: 0.5504 - acc: 0.8000 - val_loss: 0.7272 - val_acc: 0.4000\n",
            "Epoch 9/300\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.70309\n",
            "60/60 - 0s - loss: 0.5246 - acc: 0.8667 - val_loss: 0.7295 - val_acc: 0.4000\n",
            "Epoch 10/300\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.70309\n",
            "60/60 - 0s - loss: 0.5325 - acc: 0.8500 - val_loss: 0.7307 - val_acc: 0.4500\n",
            "Epoch 11/300\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.70309\n",
            "60/60 - 0s - loss: 0.5093 - acc: 0.8500 - val_loss: 0.7310 - val_acc: 0.4500\n",
            "Epoch 12/300\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.70309\n",
            "60/60 - 0s - loss: 0.5011 - acc: 0.8833 - val_loss: 0.7293 - val_acc: 0.4500\n",
            "Epoch 13/300\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.70309\n",
            "60/60 - 0s - loss: 0.4872 - acc: 0.8833 - val_loss: 0.7309 - val_acc: 0.4500\n",
            "Epoch 14/300\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.70309\n",
            "60/60 - 0s - loss: 0.4899 - acc: 0.8333 - val_loss: 0.7314 - val_acc: 0.4500\n",
            "Epoch 15/300\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.70309\n",
            "60/60 - 0s - loss: 0.4547 - acc: 0.9167 - val_loss: 0.7307 - val_acc: 0.4000\n",
            "Epoch 16/300\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.70309\n",
            "60/60 - 0s - loss: 0.4448 - acc: 0.9167 - val_loss: 0.7301 - val_acc: 0.4000\n",
            "Epoch 17/300\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.70309\n",
            "60/60 - 0s - loss: 0.4495 - acc: 0.9333 - val_loss: 0.7289 - val_acc: 0.4000\n",
            "Epoch 18/300\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.70309\n",
            "60/60 - 0s - loss: 0.4233 - acc: 0.9000 - val_loss: 0.7257 - val_acc: 0.4000\n",
            "Epoch 19/300\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.70309\n",
            "60/60 - 0s - loss: 0.4121 - acc: 0.9167 - val_loss: 0.7247 - val_acc: 0.4500\n",
            "Epoch 20/300\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.70309\n",
            "60/60 - 0s - loss: 0.3892 - acc: 0.9667 - val_loss: 0.7252 - val_acc: 0.4500\n",
            "Epoch 21/300\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.70309\n",
            "60/60 - 0s - loss: 0.3882 - acc: 0.9500 - val_loss: 0.7250 - val_acc: 0.5000\n",
            "Epoch 22/300\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.70309\n",
            "60/60 - 0s - loss: 0.3997 - acc: 0.9333 - val_loss: 0.7222 - val_acc: 0.5000\n",
            "Epoch 23/300\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.70309\n",
            "60/60 - 0s - loss: 0.3604 - acc: 0.9667 - val_loss: 0.7215 - val_acc: 0.5000\n",
            "Epoch 24/300\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.70309\n",
            "60/60 - 0s - loss: 0.4095 - acc: 0.8833 - val_loss: 0.7221 - val_acc: 0.5000\n",
            "Epoch 25/300\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.70309\n",
            "60/60 - 0s - loss: 0.3624 - acc: 0.9167 - val_loss: 0.7198 - val_acc: 0.4500\n",
            "Epoch 26/300\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.70309\n",
            "60/60 - 0s - loss: 0.3575 - acc: 0.9833 - val_loss: 0.7226 - val_acc: 0.4500\n",
            "Epoch 27/300\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.70309\n",
            "60/60 - 0s - loss: 0.3475 - acc: 0.9833 - val_loss: 0.7244 - val_acc: 0.4500\n",
            "Epoch 28/300\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.70309\n",
            "60/60 - 0s - loss: 0.3336 - acc: 0.9500 - val_loss: 0.7266 - val_acc: 0.4500\n",
            "Epoch 29/300\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.70309\n",
            "60/60 - 0s - loss: 0.3515 - acc: 0.9167 - val_loss: 0.7255 - val_acc: 0.5000\n",
            "Epoch 30/300\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.70309\n",
            "60/60 - 0s - loss: 0.3799 - acc: 0.9000 - val_loss: 0.7198 - val_acc: 0.4500\n",
            "Epoch 31/300\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.70309\n",
            "60/60 - 0s - loss: 0.3433 - acc: 0.9667 - val_loss: 0.7173 - val_acc: 0.5000\n",
            "Epoch 32/300\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.70309\n",
            "60/60 - 0s - loss: 0.3211 - acc: 0.9833 - val_loss: 0.7114 - val_acc: 0.5000\n",
            "Epoch 33/300\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.70309\n",
            "60/60 - 0s - loss: 0.3244 - acc: 0.9833 - val_loss: 0.7064 - val_acc: 0.5000\n",
            "Epoch 34/300\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.70309\n",
            "60/60 - 0s - loss: 0.3383 - acc: 0.9333 - val_loss: 0.7041 - val_acc: 0.5000\n",
            "Epoch 35/300\n",
            "\n",
            "Epoch 00035: val_loss improved from 0.70309 to 0.70155, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3159 - acc: 0.9833 - val_loss: 0.7015 - val_acc: 0.5500\n",
            "Epoch 36/300\n",
            "\n",
            "Epoch 00036: val_loss improved from 0.70155 to 0.69691, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3185 - acc: 1.0000 - val_loss: 0.6969 - val_acc: 0.6000\n",
            "Epoch 37/300\n",
            "\n",
            "Epoch 00037: val_loss improved from 0.69691 to 0.69044, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3446 - acc: 0.9500 - val_loss: 0.6904 - val_acc: 0.6000\n",
            "Epoch 38/300\n",
            "\n",
            "Epoch 00038: val_loss improved from 0.69044 to 0.68538, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2992 - acc: 0.9667 - val_loss: 0.6854 - val_acc: 0.6000\n",
            "Epoch 39/300\n",
            "\n",
            "Epoch 00039: val_loss improved from 0.68538 to 0.68227, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3267 - acc: 0.9333 - val_loss: 0.6823 - val_acc: 0.6000\n",
            "Epoch 40/300\n",
            "\n",
            "Epoch 00040: val_loss improved from 0.68227 to 0.67683, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3066 - acc: 0.9833 - val_loss: 0.6768 - val_acc: 0.6000\n",
            "Epoch 41/300\n",
            "\n",
            "Epoch 00041: val_loss improved from 0.67683 to 0.67209, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2914 - acc: 0.9833 - val_loss: 0.6721 - val_acc: 0.6000\n",
            "Epoch 42/300\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.67209\n",
            "60/60 - 0s - loss: 0.2948 - acc: 0.9500 - val_loss: 0.6726 - val_acc: 0.6500\n",
            "Epoch 43/300\n",
            "\n",
            "Epoch 00043: val_loss improved from 0.67209 to 0.67040, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3033 - acc: 0.9500 - val_loss: 0.6704 - val_acc: 0.6500\n",
            "Epoch 44/300\n",
            "\n",
            "Epoch 00044: val_loss improved from 0.67040 to 0.66978, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3167 - acc: 0.9500 - val_loss: 0.6698 - val_acc: 0.7000\n",
            "Epoch 45/300\n",
            "\n",
            "Epoch 00045: val_loss improved from 0.66978 to 0.66695, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2967 - acc: 0.9667 - val_loss: 0.6670 - val_acc: 0.7000\n",
            "Epoch 46/300\n",
            "\n",
            "Epoch 00046: val_loss improved from 0.66695 to 0.66096, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2900 - acc: 1.0000 - val_loss: 0.6610 - val_acc: 0.7000\n",
            "Epoch 47/300\n",
            "\n",
            "Epoch 00047: val_loss improved from 0.66096 to 0.65668, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2797 - acc: 0.9667 - val_loss: 0.6567 - val_acc: 0.7000\n",
            "Epoch 48/300\n",
            "\n",
            "Epoch 00048: val_loss improved from 0.65668 to 0.65597, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2747 - acc: 0.9833 - val_loss: 0.6560 - val_acc: 0.7000\n",
            "Epoch 49/300\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.2913 - acc: 0.9333 - val_loss: 0.6602 - val_acc: 0.7000\n",
            "Epoch 50/300\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.2610 - acc: 0.9667 - val_loss: 0.6616 - val_acc: 0.7000\n",
            "Epoch 51/300\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.2640 - acc: 0.9833 - val_loss: 0.6633 - val_acc: 0.6500\n",
            "Epoch 52/300\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.2874 - acc: 0.9333 - val_loss: 0.6660 - val_acc: 0.6000\n",
            "Epoch 53/300\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.2569 - acc: 1.0000 - val_loss: 0.6718 - val_acc: 0.6000\n",
            "Epoch 54/300\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.2433 - acc: 1.0000 - val_loss: 0.6782 - val_acc: 0.6000\n",
            "Epoch 55/300\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.2624 - acc: 0.9833 - val_loss: 0.6821 - val_acc: 0.6000\n",
            "Epoch 56/300\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.2553 - acc: 0.9833 - val_loss: 0.6811 - val_acc: 0.6000\n",
            "Epoch 57/300\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.2554 - acc: 0.9667 - val_loss: 0.6840 - val_acc: 0.6000\n",
            "Epoch 58/300\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.2667 - acc: 0.9667 - val_loss: 0.6839 - val_acc: 0.6000\n",
            "Epoch 59/300\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.2486 - acc: 1.0000 - val_loss: 0.6800 - val_acc: 0.6000\n",
            "Epoch 60/300\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.2197 - acc: 0.9833 - val_loss: 0.6736 - val_acc: 0.6000\n",
            "Epoch 61/300\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.2232 - acc: 1.0000 - val_loss: 0.6703 - val_acc: 0.6000\n",
            "Epoch 62/300\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.2686 - acc: 0.9667 - val_loss: 0.6703 - val_acc: 0.6500\n",
            "Epoch 63/300\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.2700 - acc: 1.0000 - val_loss: 0.6758 - val_acc: 0.6500\n",
            "Epoch 64/300\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.2528 - acc: 0.9500 - val_loss: 0.6742 - val_acc: 0.6500\n",
            "Epoch 65/300\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.2354 - acc: 0.9667 - val_loss: 0.6723 - val_acc: 0.6500\n",
            "Epoch 66/300\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.2264 - acc: 1.0000 - val_loss: 0.6702 - val_acc: 0.6500\n",
            "Epoch 67/300\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.2512 - acc: 1.0000 - val_loss: 0.6711 - val_acc: 0.6000\n",
            "Epoch 68/300\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.2318 - acc: 0.9667 - val_loss: 0.6694 - val_acc: 0.6000\n",
            "Epoch 69/300\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.2455 - acc: 0.9667 - val_loss: 0.6707 - val_acc: 0.6000\n",
            "Epoch 70/300\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.2196 - acc: 1.0000 - val_loss: 0.6693 - val_acc: 0.6000\n",
            "Epoch 71/300\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.2483 - acc: 0.9667 - val_loss: 0.6693 - val_acc: 0.6000\n",
            "Epoch 72/300\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.2170 - acc: 1.0000 - val_loss: 0.6661 - val_acc: 0.6000\n",
            "Epoch 73/300\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.2029 - acc: 1.0000 - val_loss: 0.6669 - val_acc: 0.6000\n",
            "Epoch 74/300\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.2099 - acc: 1.0000 - val_loss: 0.6679 - val_acc: 0.6000\n",
            "Epoch 75/300\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.2402 - acc: 0.9167 - val_loss: 0.6654 - val_acc: 0.6000\n",
            "Epoch 76/300\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.2242 - acc: 0.9833 - val_loss: 0.6623 - val_acc: 0.6000\n",
            "Epoch 77/300\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.2187 - acc: 0.9833 - val_loss: 0.6589 - val_acc: 0.6000\n",
            "Epoch 78/300\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.2147 - acc: 0.9667 - val_loss: 0.6582 - val_acc: 0.6000\n",
            "Epoch 79/300\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.2224 - acc: 0.9833 - val_loss: 0.6637 - val_acc: 0.6000\n",
            "Epoch 80/300\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.1926 - acc: 0.9667 - val_loss: 0.6703 - val_acc: 0.6000\n",
            "Epoch 81/300\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.2042 - acc: 0.9667 - val_loss: 0.6800 - val_acc: 0.6000\n",
            "Epoch 82/300\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.1937 - acc: 0.9833 - val_loss: 0.6914 - val_acc: 0.6000\n",
            "Epoch 83/300\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.1997 - acc: 1.0000 - val_loss: 0.6943 - val_acc: 0.6000\n",
            "Epoch 84/300\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.2169 - acc: 0.9833 - val_loss: 0.6943 - val_acc: 0.6000\n",
            "Epoch 85/300\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.2158 - acc: 0.9833 - val_loss: 0.6898 - val_acc: 0.6000\n",
            "Epoch 86/300\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.1824 - acc: 1.0000 - val_loss: 0.6872 - val_acc: 0.6500\n",
            "Epoch 87/300\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.1605 - acc: 1.0000 - val_loss: 0.6896 - val_acc: 0.6500\n",
            "Epoch 88/300\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.2203 - acc: 0.9667 - val_loss: 0.6916 - val_acc: 0.6500\n",
            "Epoch 89/300\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.2074 - acc: 0.9500 - val_loss: 0.7052 - val_acc: 0.6500\n",
            "Epoch 90/300\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.2084 - acc: 0.9667 - val_loss: 0.7080 - val_acc: 0.6500\n",
            "Epoch 91/300\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.1596 - acc: 1.0000 - val_loss: 0.7034 - val_acc: 0.6500\n",
            "Epoch 92/300\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.1752 - acc: 1.0000 - val_loss: 0.6982 - val_acc: 0.6500\n",
            "Epoch 93/300\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.1811 - acc: 1.0000 - val_loss: 0.6972 - val_acc: 0.6500\n",
            "Epoch 94/300\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.1824 - acc: 0.9833 - val_loss: 0.6985 - val_acc: 0.6000\n",
            "Epoch 95/300\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.1810 - acc: 1.0000 - val_loss: 0.6874 - val_acc: 0.6000\n",
            "Epoch 96/300\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.2127 - acc: 1.0000 - val_loss: 0.6773 - val_acc: 0.6000\n",
            "Epoch 97/300\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.1904 - acc: 1.0000 - val_loss: 0.6941 - val_acc: 0.6000\n",
            "Epoch 98/300\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.1512 - acc: 1.0000 - val_loss: 0.7008 - val_acc: 0.6000\n",
            "Epoch 99/300\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.1588 - acc: 0.9833 - val_loss: 0.7062 - val_acc: 0.6000\n",
            "Epoch 100/300\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.1867 - acc: 1.0000 - val_loss: 0.7086 - val_acc: 0.6500\n",
            "Epoch 101/300\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.2189 - acc: 0.9667 - val_loss: 0.7154 - val_acc: 0.6000\n",
            "Epoch 102/300\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.1758 - acc: 1.0000 - val_loss: 0.7230 - val_acc: 0.6000\n",
            "Epoch 103/300\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.1729 - acc: 0.9833 - val_loss: 0.7242 - val_acc: 0.6000\n",
            "Epoch 104/300\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.1565 - acc: 1.0000 - val_loss: 0.7199 - val_acc: 0.6000\n",
            "Epoch 105/300\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.1721 - acc: 1.0000 - val_loss: 0.7176 - val_acc: 0.6000\n",
            "Epoch 106/300\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.1539 - acc: 1.0000 - val_loss: 0.7202 - val_acc: 0.5500\n",
            "Epoch 107/300\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.1632 - acc: 1.0000 - val_loss: 0.7283 - val_acc: 0.6000\n",
            "Epoch 108/300\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.1628 - acc: 0.9833 - val_loss: 0.7340 - val_acc: 0.6000\n",
            "Epoch 109/300\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.1714 - acc: 1.0000 - val_loss: 0.7378 - val_acc: 0.5500\n",
            "Epoch 110/300\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.1737 - acc: 1.0000 - val_loss: 0.7413 - val_acc: 0.6000\n",
            "Epoch 111/300\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.1769 - acc: 0.9667 - val_loss: 0.7425 - val_acc: 0.6500\n",
            "Epoch 112/300\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.1518 - acc: 1.0000 - val_loss: 0.7437 - val_acc: 0.6000\n",
            "Epoch 113/300\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.1782 - acc: 0.9667 - val_loss: 0.7389 - val_acc: 0.6500\n",
            "Epoch 114/300\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.1882 - acc: 0.9833 - val_loss: 0.7315 - val_acc: 0.5500\n",
            "Epoch 115/300\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.1508 - acc: 0.9833 - val_loss: 0.7213 - val_acc: 0.5500\n",
            "Epoch 116/300\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.1747 - acc: 1.0000 - val_loss: 0.7139 - val_acc: 0.5500\n",
            "Epoch 117/300\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.1489 - acc: 1.0000 - val_loss: 0.7043 - val_acc: 0.5500\n",
            "Epoch 118/300\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.1710 - acc: 1.0000 - val_loss: 0.7025 - val_acc: 0.5500\n",
            "Epoch 119/300\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.1746 - acc: 0.9833 - val_loss: 0.7048 - val_acc: 0.5500\n",
            "Epoch 120/300\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.1527 - acc: 1.0000 - val_loss: 0.7068 - val_acc: 0.6000\n",
            "Epoch 121/300\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.1521 - acc: 1.0000 - val_loss: 0.7078 - val_acc: 0.6000\n",
            "Epoch 122/300\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.1796 - acc: 0.9833 - val_loss: 0.7107 - val_acc: 0.6000\n",
            "Epoch 123/300\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.1622 - acc: 0.9833 - val_loss: 0.7129 - val_acc: 0.5500\n",
            "Epoch 124/300\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.1504 - acc: 1.0000 - val_loss: 0.7103 - val_acc: 0.5500\n",
            "Epoch 125/300\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.1485 - acc: 0.9833 - val_loss: 0.7116 - val_acc: 0.5500\n",
            "Epoch 126/300\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.1579 - acc: 0.9833 - val_loss: 0.7105 - val_acc: 0.5500\n",
            "Epoch 127/300\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.1913 - acc: 0.9500 - val_loss: 0.7092 - val_acc: 0.5500\n",
            "Epoch 128/300\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.1341 - acc: 1.0000 - val_loss: 0.7175 - val_acc: 0.5500\n",
            "Epoch 129/300\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.1346 - acc: 1.0000 - val_loss: 0.7226 - val_acc: 0.5500\n",
            "Epoch 130/300\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.1367 - acc: 1.0000 - val_loss: 0.7154 - val_acc: 0.5500\n",
            "Epoch 131/300\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.1947 - acc: 0.9500 - val_loss: 0.7135 - val_acc: 0.5500\n",
            "Epoch 132/300\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.1639 - acc: 0.9500 - val_loss: 0.7247 - val_acc: 0.5500\n",
            "Epoch 133/300\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.1395 - acc: 0.9833 - val_loss: 0.7339 - val_acc: 0.5500\n",
            "Epoch 134/300\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.1577 - acc: 0.9833 - val_loss: 0.7431 - val_acc: 0.5500\n",
            "Epoch 135/300\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.1672 - acc: 0.9667 - val_loss: 0.7477 - val_acc: 0.5500\n",
            "Epoch 136/300\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.1399 - acc: 1.0000 - val_loss: 0.7485 - val_acc: 0.5500\n",
            "Epoch 137/300\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.1692 - acc: 0.9667 - val_loss: 0.7300 - val_acc: 0.5500\n",
            "Epoch 138/300\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.1602 - acc: 1.0000 - val_loss: 0.7101 - val_acc: 0.6000\n",
            "Epoch 139/300\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.1727 - acc: 0.9667 - val_loss: 0.7043 - val_acc: 0.6000\n",
            "Epoch 140/300\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.1620 - acc: 0.9667 - val_loss: 0.6943 - val_acc: 0.6000\n",
            "Epoch 141/300\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.1495 - acc: 0.9667 - val_loss: 0.6938 - val_acc: 0.6000\n",
            "Epoch 142/300\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.1513 - acc: 0.9833 - val_loss: 0.6968 - val_acc: 0.6000\n",
            "Epoch 143/300\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.1443 - acc: 1.0000 - val_loss: 0.7017 - val_acc: 0.5500\n",
            "Epoch 144/300\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.1256 - acc: 0.9833 - val_loss: 0.7027 - val_acc: 0.5500\n",
            "Epoch 145/300\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.1483 - acc: 1.0000 - val_loss: 0.7080 - val_acc: 0.5500\n",
            "Epoch 146/300\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.1537 - acc: 1.0000 - val_loss: 0.7129 - val_acc: 0.5500\n",
            "Epoch 147/300\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.1324 - acc: 1.0000 - val_loss: 0.7233 - val_acc: 0.6500\n",
            "Epoch 148/300\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.1371 - acc: 1.0000 - val_loss: 0.7153 - val_acc: 0.6500\n",
            "Epoch 149/300\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.1867 - acc: 0.9500 - val_loss: 0.7034 - val_acc: 0.6000\n",
            "Epoch 150/300\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.1510 - acc: 0.9667 - val_loss: 0.7028 - val_acc: 0.6000\n",
            "Epoch 151/300\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.1621 - acc: 0.9833 - val_loss: 0.7094 - val_acc: 0.6000\n",
            "Epoch 152/300\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.1284 - acc: 1.0000 - val_loss: 0.7129 - val_acc: 0.6000\n",
            "Epoch 153/300\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.1601 - acc: 0.9667 - val_loss: 0.7243 - val_acc: 0.6000\n",
            "Epoch 154/300\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.1113 - acc: 1.0000 - val_loss: 0.7435 - val_acc: 0.6500\n",
            "Epoch 155/300\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.1145 - acc: 1.0000 - val_loss: 0.7711 - val_acc: 0.6500\n",
            "Epoch 156/300\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.1189 - acc: 1.0000 - val_loss: 0.7988 - val_acc: 0.6000\n",
            "Epoch 157/300\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.1498 - acc: 0.9667 - val_loss: 0.7905 - val_acc: 0.6000\n",
            "Epoch 158/300\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.1185 - acc: 1.0000 - val_loss: 0.7625 - val_acc: 0.6000\n",
            "Epoch 159/300\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.1471 - acc: 0.9667 - val_loss: 0.7459 - val_acc: 0.5500\n",
            "Epoch 160/300\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.1225 - acc: 1.0000 - val_loss: 0.7454 - val_acc: 0.5500\n",
            "Epoch 161/300\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.1258 - acc: 1.0000 - val_loss: 0.7383 - val_acc: 0.6000\n",
            "Epoch 162/300\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.1417 - acc: 0.9833 - val_loss: 0.7378 - val_acc: 0.6000\n",
            "Epoch 163/300\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.1147 - acc: 1.0000 - val_loss: 0.7483 - val_acc: 0.5500\n",
            "Epoch 164/300\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.1354 - acc: 1.0000 - val_loss: 0.7610 - val_acc: 0.6000\n",
            "Epoch 165/300\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.1262 - acc: 1.0000 - val_loss: 0.7617 - val_acc: 0.6000\n",
            "Epoch 166/300\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.1538 - acc: 0.9667 - val_loss: 0.7454 - val_acc: 0.6000\n",
            "Epoch 167/300\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.1365 - acc: 0.9833 - val_loss: 0.7246 - val_acc: 0.5500\n",
            "Epoch 168/300\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.1356 - acc: 0.9833 - val_loss: 0.7176 - val_acc: 0.6000\n",
            "Epoch 169/300\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.1270 - acc: 0.9833 - val_loss: 0.7288 - val_acc: 0.6000\n",
            "Epoch 170/300\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.1293 - acc: 1.0000 - val_loss: 0.7365 - val_acc: 0.5500\n",
            "Epoch 171/300\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.1261 - acc: 1.0000 - val_loss: 0.7306 - val_acc: 0.6000\n",
            "Epoch 172/300\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.1366 - acc: 0.9833 - val_loss: 0.7293 - val_acc: 0.6000\n",
            "Epoch 173/300\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.1062 - acc: 1.0000 - val_loss: 0.7285 - val_acc: 0.6000\n",
            "Epoch 174/300\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.1474 - acc: 0.9667 - val_loss: 0.7303 - val_acc: 0.6000\n",
            "Epoch 175/300\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.1341 - acc: 0.9833 - val_loss: 0.7364 - val_acc: 0.6000\n",
            "Epoch 176/300\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.1322 - acc: 0.9833 - val_loss: 0.7422 - val_acc: 0.6000\n",
            "Epoch 177/300\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.1005 - acc: 1.0000 - val_loss: 0.7463 - val_acc: 0.6000\n",
            "Epoch 178/300\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.1244 - acc: 0.9833 - val_loss: 0.7430 - val_acc: 0.6000\n",
            "Epoch 179/300\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.1219 - acc: 1.0000 - val_loss: 0.7490 - val_acc: 0.6000\n",
            "Epoch 180/300\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.1464 - acc: 1.0000 - val_loss: 0.7525 - val_acc: 0.6000\n",
            "Epoch 181/300\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.1040 - acc: 1.0000 - val_loss: 0.7519 - val_acc: 0.5500\n",
            "Epoch 182/300\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.1166 - acc: 1.0000 - val_loss: 0.7512 - val_acc: 0.5500\n",
            "Epoch 183/300\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.0993 - acc: 1.0000 - val_loss: 0.7588 - val_acc: 0.5500\n",
            "Epoch 184/300\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.1093 - acc: 1.0000 - val_loss: 0.7529 - val_acc: 0.5500\n",
            "Epoch 185/300\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.1208 - acc: 1.0000 - val_loss: 0.7450 - val_acc: 0.5500\n",
            "Epoch 186/300\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.0932 - acc: 1.0000 - val_loss: 0.7350 - val_acc: 0.5500\n",
            "Epoch 187/300\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.0994 - acc: 0.9833 - val_loss: 0.7264 - val_acc: 0.5500\n",
            "Epoch 188/300\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.1227 - acc: 0.9833 - val_loss: 0.7143 - val_acc: 0.5500\n",
            "Epoch 189/300\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.1121 - acc: 1.0000 - val_loss: 0.7063 - val_acc: 0.5500\n",
            "Epoch 190/300\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.1335 - acc: 1.0000 - val_loss: 0.7048 - val_acc: 0.6000\n",
            "Epoch 191/300\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.1308 - acc: 1.0000 - val_loss: 0.7217 - val_acc: 0.5500\n",
            "Epoch 192/300\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.1394 - acc: 0.9667 - val_loss: 0.7361 - val_acc: 0.5500\n",
            "Epoch 193/300\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.1128 - acc: 1.0000 - val_loss: 0.7471 - val_acc: 0.6000\n",
            "Epoch 194/300\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.1144 - acc: 1.0000 - val_loss: 0.7567 - val_acc: 0.6000\n",
            "Epoch 195/300\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.1185 - acc: 1.0000 - val_loss: 0.7788 - val_acc: 0.6500\n",
            "Epoch 196/300\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.1109 - acc: 1.0000 - val_loss: 0.7794 - val_acc: 0.6000\n",
            "Epoch 197/300\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.1014 - acc: 1.0000 - val_loss: 0.7737 - val_acc: 0.5500\n",
            "Epoch 198/300\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.1134 - acc: 1.0000 - val_loss: 0.7814 - val_acc: 0.5500\n",
            "Epoch 199/300\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.1158 - acc: 1.0000 - val_loss: 0.7851 - val_acc: 0.6000\n",
            "Epoch 200/300\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.1036 - acc: 1.0000 - val_loss: 0.7818 - val_acc: 0.6000\n",
            "Epoch 201/300\n",
            "\n",
            "Epoch 00201: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.1026 - acc: 1.0000 - val_loss: 0.7631 - val_acc: 0.5500\n",
            "Epoch 202/300\n",
            "\n",
            "Epoch 00202: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.1038 - acc: 1.0000 - val_loss: 0.7632 - val_acc: 0.5500\n",
            "Epoch 203/300\n",
            "\n",
            "Epoch 00203: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.1168 - acc: 1.0000 - val_loss: 0.7555 - val_acc: 0.5500\n",
            "Epoch 204/300\n",
            "\n",
            "Epoch 00204: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.1157 - acc: 1.0000 - val_loss: 0.7653 - val_acc: 0.5500\n",
            "Epoch 205/300\n",
            "\n",
            "Epoch 00205: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.1097 - acc: 1.0000 - val_loss: 0.7807 - val_acc: 0.5500\n",
            "Epoch 206/300\n",
            "\n",
            "Epoch 00206: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.0923 - acc: 1.0000 - val_loss: 0.8039 - val_acc: 0.6000\n",
            "Epoch 207/300\n",
            "\n",
            "Epoch 00207: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.0957 - acc: 1.0000 - val_loss: 0.8096 - val_acc: 0.6000\n",
            "Epoch 208/300\n",
            "\n",
            "Epoch 00208: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.1052 - acc: 0.9833 - val_loss: 0.8165 - val_acc: 0.6000\n",
            "Epoch 209/300\n",
            "\n",
            "Epoch 00209: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.1039 - acc: 1.0000 - val_loss: 0.8298 - val_acc: 0.6000\n",
            "Epoch 210/300\n",
            "\n",
            "Epoch 00210: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.1342 - acc: 0.9833 - val_loss: 0.8252 - val_acc: 0.6000\n",
            "Epoch 211/300\n",
            "\n",
            "Epoch 00211: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.1221 - acc: 1.0000 - val_loss: 0.8180 - val_acc: 0.6000\n",
            "Epoch 212/300\n",
            "\n",
            "Epoch 00212: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.1088 - acc: 1.0000 - val_loss: 0.8176 - val_acc: 0.6500\n",
            "Epoch 213/300\n",
            "\n",
            "Epoch 00213: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.0910 - acc: 1.0000 - val_loss: 0.8154 - val_acc: 0.6500\n",
            "Epoch 214/300\n",
            "\n",
            "Epoch 00214: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.1184 - acc: 1.0000 - val_loss: 0.8207 - val_acc: 0.6500\n",
            "Epoch 215/300\n",
            "\n",
            "Epoch 00215: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.1275 - acc: 1.0000 - val_loss: 0.8391 - val_acc: 0.6500\n",
            "Epoch 216/300\n",
            "\n",
            "Epoch 00216: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.1075 - acc: 1.0000 - val_loss: 0.8266 - val_acc: 0.6500\n",
            "Epoch 217/300\n",
            "\n",
            "Epoch 00217: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.0954 - acc: 0.9833 - val_loss: 0.8259 - val_acc: 0.6500\n",
            "Epoch 218/300\n",
            "\n",
            "Epoch 00218: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.1107 - acc: 1.0000 - val_loss: 0.8228 - val_acc: 0.6500\n",
            "Epoch 219/300\n",
            "\n",
            "Epoch 00219: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.0916 - acc: 1.0000 - val_loss: 0.8111 - val_acc: 0.5500\n",
            "Epoch 220/300\n",
            "\n",
            "Epoch 00220: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.0966 - acc: 1.0000 - val_loss: 0.8136 - val_acc: 0.5000\n",
            "Epoch 221/300\n",
            "\n",
            "Epoch 00221: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.0990 - acc: 0.9833 - val_loss: 0.8137 - val_acc: 0.5000\n",
            "Epoch 222/300\n",
            "\n",
            "Epoch 00222: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.0845 - acc: 1.0000 - val_loss: 0.8159 - val_acc: 0.5000\n",
            "Epoch 223/300\n",
            "\n",
            "Epoch 00223: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.1136 - acc: 0.9833 - val_loss: 0.8183 - val_acc: 0.5000\n",
            "Epoch 224/300\n",
            "\n",
            "Epoch 00224: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.1160 - acc: 0.9667 - val_loss: 0.8153 - val_acc: 0.5500\n",
            "Epoch 225/300\n",
            "\n",
            "Epoch 00225: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.1363 - acc: 0.9833 - val_loss: 0.8149 - val_acc: 0.5500\n",
            "Epoch 226/300\n",
            "\n",
            "Epoch 00226: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.0939 - acc: 1.0000 - val_loss: 0.8313 - val_acc: 0.5500\n",
            "Epoch 227/300\n",
            "\n",
            "Epoch 00227: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.0954 - acc: 1.0000 - val_loss: 0.8482 - val_acc: 0.6000\n",
            "Epoch 228/300\n",
            "\n",
            "Epoch 00228: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.1029 - acc: 1.0000 - val_loss: 0.8590 - val_acc: 0.6000\n",
            "Epoch 229/300\n",
            "\n",
            "Epoch 00229: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.1120 - acc: 1.0000 - val_loss: 0.8569 - val_acc: 0.6500\n",
            "Epoch 230/300\n",
            "\n",
            "Epoch 00230: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.1133 - acc: 0.9833 - val_loss: 0.8136 - val_acc: 0.5500\n",
            "Epoch 231/300\n",
            "\n",
            "Epoch 00231: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.0838 - acc: 1.0000 - val_loss: 0.7873 - val_acc: 0.5500\n",
            "Epoch 232/300\n",
            "\n",
            "Epoch 00232: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.0906 - acc: 0.9833 - val_loss: 0.7703 - val_acc: 0.6500\n",
            "Epoch 233/300\n",
            "\n",
            "Epoch 00233: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.0896 - acc: 1.0000 - val_loss: 0.7605 - val_acc: 0.6500\n",
            "Epoch 234/300\n",
            "\n",
            "Epoch 00234: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.0973 - acc: 1.0000 - val_loss: 0.7626 - val_acc: 0.6000\n",
            "Epoch 235/300\n",
            "\n",
            "Epoch 00235: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.0906 - acc: 1.0000 - val_loss: 0.7716 - val_acc: 0.6500\n",
            "Epoch 236/300\n",
            "\n",
            "Epoch 00236: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.0835 - acc: 1.0000 - val_loss: 0.7661 - val_acc: 0.6500\n",
            "Epoch 237/300\n",
            "\n",
            "Epoch 00237: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.0797 - acc: 1.0000 - val_loss: 0.7511 - val_acc: 0.6500\n",
            "Epoch 238/300\n",
            "\n",
            "Epoch 00238: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.1039 - acc: 0.9833 - val_loss: 0.7417 - val_acc: 0.6500\n",
            "Epoch 239/300\n",
            "\n",
            "Epoch 00239: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.0989 - acc: 0.9833 - val_loss: 0.7541 - val_acc: 0.6000\n",
            "Epoch 240/300\n",
            "\n",
            "Epoch 00240: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.0923 - acc: 1.0000 - val_loss: 0.7647 - val_acc: 0.6000\n",
            "Epoch 241/300\n",
            "\n",
            "Epoch 00241: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.0786 - acc: 1.0000 - val_loss: 0.7629 - val_acc: 0.6000\n",
            "Epoch 242/300\n",
            "\n",
            "Epoch 00242: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.0890 - acc: 1.0000 - val_loss: 0.7662 - val_acc: 0.6000\n",
            "Epoch 243/300\n",
            "\n",
            "Epoch 00243: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.0856 - acc: 1.0000 - val_loss: 0.7678 - val_acc: 0.5500\n",
            "Epoch 244/300\n",
            "\n",
            "Epoch 00244: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.0895 - acc: 0.9833 - val_loss: 0.7640 - val_acc: 0.5500\n",
            "Epoch 245/300\n",
            "\n",
            "Epoch 00245: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.0989 - acc: 1.0000 - val_loss: 0.7710 - val_acc: 0.5500\n",
            "Epoch 246/300\n",
            "\n",
            "Epoch 00246: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.0704 - acc: 1.0000 - val_loss: 0.7754 - val_acc: 0.6000\n",
            "Epoch 247/300\n",
            "\n",
            "Epoch 00247: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.1016 - acc: 1.0000 - val_loss: 0.7807 - val_acc: 0.6000\n",
            "Epoch 248/300\n",
            "\n",
            "Epoch 00248: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.1084 - acc: 0.9667 - val_loss: 0.7965 - val_acc: 0.6000\n",
            "Epoch 249/300\n",
            "\n",
            "Epoch 00249: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.0749 - acc: 1.0000 - val_loss: 0.8134 - val_acc: 0.6000\n",
            "Epoch 250/300\n",
            "\n",
            "Epoch 00250: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.1259 - acc: 0.9833 - val_loss: 0.8127 - val_acc: 0.6000\n",
            "Epoch 251/300\n",
            "\n",
            "Epoch 00251: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.0963 - acc: 1.0000 - val_loss: 0.8096 - val_acc: 0.5500\n",
            "Epoch 252/300\n",
            "\n",
            "Epoch 00252: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.1262 - acc: 0.9500 - val_loss: 0.8224 - val_acc: 0.6000\n",
            "Epoch 253/300\n",
            "\n",
            "Epoch 00253: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.0735 - acc: 1.0000 - val_loss: 0.8384 - val_acc: 0.6000\n",
            "Epoch 254/300\n",
            "\n",
            "Epoch 00254: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.1092 - acc: 0.9833 - val_loss: 0.8385 - val_acc: 0.6000\n",
            "Epoch 255/300\n",
            "\n",
            "Epoch 00255: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.0985 - acc: 1.0000 - val_loss: 0.8296 - val_acc: 0.6000\n",
            "Epoch 256/300\n",
            "\n",
            "Epoch 00256: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.0958 - acc: 1.0000 - val_loss: 0.8188 - val_acc: 0.6500\n",
            "Epoch 257/300\n",
            "\n",
            "Epoch 00257: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.0916 - acc: 0.9833 - val_loss: 0.8032 - val_acc: 0.6500\n",
            "Epoch 258/300\n",
            "\n",
            "Epoch 00258: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.0927 - acc: 1.0000 - val_loss: 0.7832 - val_acc: 0.6500\n",
            "Epoch 259/300\n",
            "\n",
            "Epoch 00259: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.0876 - acc: 0.9833 - val_loss: 0.7667 - val_acc: 0.6500\n",
            "Epoch 260/300\n",
            "\n",
            "Epoch 00260: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.0854 - acc: 1.0000 - val_loss: 0.7458 - val_acc: 0.6500\n",
            "Epoch 261/300\n",
            "\n",
            "Epoch 00261: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.0942 - acc: 1.0000 - val_loss: 0.7345 - val_acc: 0.6000\n",
            "Epoch 262/300\n",
            "\n",
            "Epoch 00262: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.1483 - acc: 0.9667 - val_loss: 0.7724 - val_acc: 0.6000\n",
            "Epoch 263/300\n",
            "\n",
            "Epoch 00263: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.0672 - acc: 1.0000 - val_loss: 0.8136 - val_acc: 0.6000\n",
            "Epoch 264/300\n",
            "\n",
            "Epoch 00264: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.0855 - acc: 1.0000 - val_loss: 0.8370 - val_acc: 0.6000\n",
            "Epoch 265/300\n",
            "\n",
            "Epoch 00265: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.0940 - acc: 0.9833 - val_loss: 0.8380 - val_acc: 0.6000\n",
            "Epoch 266/300\n",
            "\n",
            "Epoch 00266: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.0786 - acc: 1.0000 - val_loss: 0.8312 - val_acc: 0.6000\n",
            "Epoch 267/300\n",
            "\n",
            "Epoch 00267: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.0922 - acc: 1.0000 - val_loss: 0.8221 - val_acc: 0.6000\n",
            "Epoch 268/300\n",
            "\n",
            "Epoch 00268: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.0898 - acc: 1.0000 - val_loss: 0.8220 - val_acc: 0.6000\n",
            "Epoch 269/300\n",
            "\n",
            "Epoch 00269: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.1072 - acc: 0.9833 - val_loss: 0.8118 - val_acc: 0.6000\n",
            "Epoch 270/300\n",
            "\n",
            "Epoch 00270: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.0851 - acc: 1.0000 - val_loss: 0.8216 - val_acc: 0.6000\n",
            "Epoch 271/300\n",
            "\n",
            "Epoch 00271: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.0986 - acc: 1.0000 - val_loss: 0.8168 - val_acc: 0.6000\n",
            "Epoch 272/300\n",
            "\n",
            "Epoch 00272: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.1012 - acc: 1.0000 - val_loss: 0.8198 - val_acc: 0.5500\n",
            "Epoch 273/300\n",
            "\n",
            "Epoch 00273: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.1116 - acc: 0.9833 - val_loss: 0.8134 - val_acc: 0.5500\n",
            "Epoch 274/300\n",
            "\n",
            "Epoch 00274: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.0718 - acc: 1.0000 - val_loss: 0.8076 - val_acc: 0.5500\n",
            "Epoch 275/300\n",
            "\n",
            "Epoch 00275: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.1130 - acc: 0.9833 - val_loss: 0.7888 - val_acc: 0.6000\n",
            "Epoch 276/300\n",
            "\n",
            "Epoch 00276: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.1081 - acc: 0.9833 - val_loss: 0.7811 - val_acc: 0.6000\n",
            "Epoch 277/300\n",
            "\n",
            "Epoch 00277: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.1068 - acc: 0.9833 - val_loss: 0.7660 - val_acc: 0.6000\n",
            "Epoch 278/300\n",
            "\n",
            "Epoch 00278: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.1031 - acc: 1.0000 - val_loss: 0.7617 - val_acc: 0.6000\n",
            "Epoch 279/300\n",
            "\n",
            "Epoch 00279: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.1213 - acc: 0.9833 - val_loss: 0.7470 - val_acc: 0.5500\n",
            "Epoch 280/300\n",
            "\n",
            "Epoch 00280: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.1133 - acc: 1.0000 - val_loss: 0.7476 - val_acc: 0.5500\n",
            "Epoch 281/300\n",
            "\n",
            "Epoch 00281: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.0915 - acc: 1.0000 - val_loss: 0.7472 - val_acc: 0.5500\n",
            "Epoch 282/300\n",
            "\n",
            "Epoch 00282: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.0931 - acc: 1.0000 - val_loss: 0.7525 - val_acc: 0.5500\n",
            "Epoch 283/300\n",
            "\n",
            "Epoch 00283: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.0814 - acc: 1.0000 - val_loss: 0.7598 - val_acc: 0.5500\n",
            "Epoch 284/300\n",
            "\n",
            "Epoch 00284: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.1006 - acc: 1.0000 - val_loss: 0.7651 - val_acc: 0.5500\n",
            "Epoch 285/300\n",
            "\n",
            "Epoch 00285: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.0873 - acc: 1.0000 - val_loss: 0.7686 - val_acc: 0.5500\n",
            "Epoch 286/300\n",
            "\n",
            "Epoch 00286: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.1114 - acc: 0.9833 - val_loss: 0.7730 - val_acc: 0.5000\n",
            "Epoch 287/300\n",
            "\n",
            "Epoch 00287: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.0983 - acc: 0.9833 - val_loss: 0.7840 - val_acc: 0.5000\n",
            "Epoch 288/300\n",
            "\n",
            "Epoch 00288: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.0817 - acc: 1.0000 - val_loss: 0.7994 - val_acc: 0.5000\n",
            "Epoch 289/300\n",
            "\n",
            "Epoch 00289: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.0771 - acc: 1.0000 - val_loss: 0.8021 - val_acc: 0.5500\n",
            "Epoch 290/300\n",
            "\n",
            "Epoch 00290: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.0728 - acc: 1.0000 - val_loss: 0.7921 - val_acc: 0.5500\n",
            "Epoch 291/300\n",
            "\n",
            "Epoch 00291: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.0945 - acc: 1.0000 - val_loss: 0.7846 - val_acc: 0.5500\n",
            "Epoch 292/300\n",
            "\n",
            "Epoch 00292: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.0830 - acc: 1.0000 - val_loss: 0.7817 - val_acc: 0.5500\n",
            "Epoch 293/300\n",
            "\n",
            "Epoch 00293: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.0708 - acc: 1.0000 - val_loss: 0.7871 - val_acc: 0.5500\n",
            "Epoch 294/300\n",
            "\n",
            "Epoch 00294: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.0704 - acc: 1.0000 - val_loss: 0.7886 - val_acc: 0.5500\n",
            "Epoch 295/300\n",
            "\n",
            "Epoch 00295: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.0820 - acc: 1.0000 - val_loss: 0.7941 - val_acc: 0.5500\n",
            "Epoch 296/300\n",
            "\n",
            "Epoch 00296: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.0917 - acc: 1.0000 - val_loss: 0.7979 - val_acc: 0.5500\n",
            "Epoch 297/300\n",
            "\n",
            "Epoch 00297: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.0808 - acc: 1.0000 - val_loss: 0.7974 - val_acc: 0.6000\n",
            "Epoch 298/300\n",
            "\n",
            "Epoch 00298: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.0789 - acc: 1.0000 - val_loss: 0.8015 - val_acc: 0.6000\n",
            "Epoch 299/300\n",
            "\n",
            "Epoch 00299: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.0758 - acc: 1.0000 - val_loss: 0.8094 - val_acc: 0.6500\n",
            "Epoch 300/300\n",
            "\n",
            "Epoch 00300: val_loss did not improve from 0.65597\n",
            "60/60 - 0s - loss: 0.0753 - acc: 1.0000 - val_loss: 0.8041 - val_acc: 0.6500\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nxo0lAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "# Evaluate on test data\n",
            "40/40 [==============================] - 0s 2ms/sample - loss: 1.0056 - acc: 0.6250\n",
            "test loss, test acc: [1.0055989949440118, 0.625]\n",
            "EEG_Deep/Data2A/parsed_P09T.mat\n",
            "Filtering of Training Data Finished\n",
            "EEG_Deep/Data2A/parsed_P09E.mat\n",
            "Filtering of Testing Data Finished\n",
            "(1, 2)\n",
            "Finding labels in training data\n",
            "Finding labels in testing data\n",
            "[1 2 2 2 1 2 2 2 2 1 1 1 2 2 1 1 2 1 1 1]\n",
            "(60, 12, 1536)\n",
            "(60,)\n",
            "(20, 12, 1536)\n",
            "(20,)\n",
            "[[1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]]\n",
            "X_train shape: (60, 1, 12, 1536)\n",
            "60 train samples\n",
            "20 val samples\n",
            "(40, 12, 1536)\n",
            "(40, 1)\n",
            "X_train shape: (40, 1, 12, 1536)\n",
            "40 train samples\n",
            "Train on 60 samples, validate on 20 samples\n",
            "Epoch 1/300\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.69506, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 1s - loss: 0.7673 - acc: 0.4833 - val_loss: 0.6951 - val_acc: 0.3500\n",
            "Epoch 2/300\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.6882 - acc: 0.5333 - val_loss: 0.6957 - val_acc: 0.4500\n",
            "Epoch 3/300\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.6573 - acc: 0.6333 - val_loss: 0.6966 - val_acc: 0.4000\n",
            "Epoch 4/300\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.6338 - acc: 0.7167 - val_loss: 0.6973 - val_acc: 0.3000\n",
            "Epoch 5/300\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.6187 - acc: 0.8000 - val_loss: 0.6984 - val_acc: 0.2500\n",
            "Epoch 6/300\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.6092 - acc: 0.7500 - val_loss: 0.6988 - val_acc: 0.3000\n",
            "Epoch 7/300\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.6243 - acc: 0.7667 - val_loss: 0.6997 - val_acc: 0.2500\n",
            "Epoch 8/300\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.5935 - acc: 0.8333 - val_loss: 0.7002 - val_acc: 0.3000\n",
            "Epoch 9/300\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.5747 - acc: 0.8333 - val_loss: 0.7010 - val_acc: 0.4000\n",
            "Epoch 10/300\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.5845 - acc: 0.8167 - val_loss: 0.7015 - val_acc: 0.3500\n",
            "Epoch 11/300\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.5385 - acc: 0.8333 - val_loss: 0.7018 - val_acc: 0.4000\n",
            "Epoch 12/300\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.5248 - acc: 0.8833 - val_loss: 0.7022 - val_acc: 0.4500\n",
            "Epoch 13/300\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.5263 - acc: 0.8667 - val_loss: 0.7024 - val_acc: 0.4500\n",
            "Epoch 14/300\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.5195 - acc: 0.8333 - val_loss: 0.7021 - val_acc: 0.4500\n",
            "Epoch 15/300\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.5050 - acc: 0.9500 - val_loss: 0.7014 - val_acc: 0.4500\n",
            "Epoch 16/300\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.5000 - acc: 0.9167 - val_loss: 0.7011 - val_acc: 0.4500\n",
            "Epoch 17/300\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.4964 - acc: 0.8333 - val_loss: 0.7012 - val_acc: 0.4500\n",
            "Epoch 18/300\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.5009 - acc: 0.9000 - val_loss: 0.7012 - val_acc: 0.4000\n",
            "Epoch 19/300\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.4782 - acc: 0.8667 - val_loss: 0.7015 - val_acc: 0.4000\n",
            "Epoch 20/300\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.4887 - acc: 0.8500 - val_loss: 0.7017 - val_acc: 0.4000\n",
            "Epoch 21/300\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.4647 - acc: 0.8667 - val_loss: 0.7017 - val_acc: 0.4500\n",
            "Epoch 22/300\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.4689 - acc: 0.8667 - val_loss: 0.7021 - val_acc: 0.4000\n",
            "Epoch 23/300\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.4634 - acc: 0.8833 - val_loss: 0.7023 - val_acc: 0.4000\n",
            "Epoch 24/300\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.4620 - acc: 0.8833 - val_loss: 0.7031 - val_acc: 0.4000\n",
            "Epoch 25/300\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.4520 - acc: 0.8833 - val_loss: 0.7033 - val_acc: 0.4000\n",
            "Epoch 26/300\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.4270 - acc: 0.9167 - val_loss: 0.7034 - val_acc: 0.3500\n",
            "Epoch 27/300\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.4443 - acc: 0.8833 - val_loss: 0.7029 - val_acc: 0.3500\n",
            "Epoch 28/300\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.4267 - acc: 0.8500 - val_loss: 0.7023 - val_acc: 0.3500\n",
            "Epoch 29/300\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.4118 - acc: 0.8833 - val_loss: 0.7019 - val_acc: 0.3500\n",
            "Epoch 30/300\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.4118 - acc: 0.9000 - val_loss: 0.7012 - val_acc: 0.3500\n",
            "Epoch 31/300\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.3942 - acc: 0.9000 - val_loss: 0.7005 - val_acc: 0.4000\n",
            "Epoch 32/300\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.3981 - acc: 0.9333 - val_loss: 0.6998 - val_acc: 0.4000\n",
            "Epoch 33/300\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.4023 - acc: 0.9167 - val_loss: 0.6994 - val_acc: 0.3500\n",
            "Epoch 34/300\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.3983 - acc: 0.9333 - val_loss: 0.6999 - val_acc: 0.4000\n",
            "Epoch 35/300\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.4488 - acc: 0.8167 - val_loss: 0.7000 - val_acc: 0.4000\n",
            "Epoch 36/300\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.4144 - acc: 0.9167 - val_loss: 0.6999 - val_acc: 0.4000\n",
            "Epoch 37/300\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.3734 - acc: 0.9500 - val_loss: 0.6996 - val_acc: 0.4000\n",
            "Epoch 38/300\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.3844 - acc: 0.9167 - val_loss: 0.7002 - val_acc: 0.3000\n",
            "Epoch 39/300\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.3883 - acc: 0.9000 - val_loss: 0.7011 - val_acc: 0.3500\n",
            "Epoch 40/300\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.3928 - acc: 0.9000 - val_loss: 0.7011 - val_acc: 0.3500\n",
            "Epoch 41/300\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.4131 - acc: 0.9000 - val_loss: 0.7014 - val_acc: 0.4000\n",
            "Epoch 42/300\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.3781 - acc: 0.9167 - val_loss: 0.7004 - val_acc: 0.4000\n",
            "Epoch 43/300\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.3651 - acc: 0.9333 - val_loss: 0.7007 - val_acc: 0.4000\n",
            "Epoch 44/300\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.3693 - acc: 0.9333 - val_loss: 0.7000 - val_acc: 0.5000\n",
            "Epoch 45/300\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.3543 - acc: 0.9667 - val_loss: 0.7007 - val_acc: 0.4500\n",
            "Epoch 46/300\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.3638 - acc: 0.9333 - val_loss: 0.7018 - val_acc: 0.5000\n",
            "Epoch 47/300\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.3710 - acc: 0.9333 - val_loss: 0.7030 - val_acc: 0.4500\n",
            "Epoch 48/300\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.3318 - acc: 0.9500 - val_loss: 0.7029 - val_acc: 0.4500\n",
            "Epoch 49/300\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.3605 - acc: 0.9333 - val_loss: 0.7024 - val_acc: 0.4500\n",
            "Epoch 50/300\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.3591 - acc: 0.9667 - val_loss: 0.7010 - val_acc: 0.4500\n",
            "Epoch 51/300\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.3680 - acc: 0.9000 - val_loss: 0.7018 - val_acc: 0.5500\n",
            "Epoch 52/300\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.3584 - acc: 0.9833 - val_loss: 0.7014 - val_acc: 0.5500\n",
            "Epoch 53/300\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.3507 - acc: 0.9000 - val_loss: 0.7026 - val_acc: 0.5000\n",
            "Epoch 54/300\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.3491 - acc: 0.9333 - val_loss: 0.7038 - val_acc: 0.5000\n",
            "Epoch 55/300\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.3534 - acc: 0.9167 - val_loss: 0.7055 - val_acc: 0.4000\n",
            "Epoch 56/300\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.3730 - acc: 0.9500 - val_loss: 0.7070 - val_acc: 0.4000\n",
            "Epoch 57/300\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.3319 - acc: 0.9500 - val_loss: 0.7073 - val_acc: 0.4000\n",
            "Epoch 58/300\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.3453 - acc: 0.9000 - val_loss: 0.7065 - val_acc: 0.4500\n",
            "Epoch 59/300\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.2981 - acc: 0.9833 - val_loss: 0.7042 - val_acc: 0.4500\n",
            "Epoch 60/300\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.3283 - acc: 0.9167 - val_loss: 0.7026 - val_acc: 0.4500\n",
            "Epoch 61/300\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.3242 - acc: 0.9667 - val_loss: 0.7010 - val_acc: 0.5000\n",
            "Epoch 62/300\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.3084 - acc: 0.9667 - val_loss: 0.7004 - val_acc: 0.5000\n",
            "Epoch 63/300\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.3571 - acc: 0.9333 - val_loss: 0.7003 - val_acc: 0.5000\n",
            "Epoch 64/300\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.3371 - acc: 0.9667 - val_loss: 0.7008 - val_acc: 0.4500\n",
            "Epoch 65/300\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.3191 - acc: 0.9500 - val_loss: 0.6996 - val_acc: 0.5000\n",
            "Epoch 66/300\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.3058 - acc: 0.9667 - val_loss: 0.7010 - val_acc: 0.5000\n",
            "Epoch 67/300\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.3093 - acc: 0.9833 - val_loss: 0.7033 - val_acc: 0.5000\n",
            "Epoch 68/300\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.3290 - acc: 0.9333 - val_loss: 0.7052 - val_acc: 0.4500\n",
            "Epoch 69/300\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.3390 - acc: 0.9667 - val_loss: 0.7051 - val_acc: 0.5000\n",
            "Epoch 70/300\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.3292 - acc: 0.9500 - val_loss: 0.7048 - val_acc: 0.5000\n",
            "Epoch 71/300\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.3174 - acc: 0.9667 - val_loss: 0.7056 - val_acc: 0.5000\n",
            "Epoch 72/300\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.3072 - acc: 0.9667 - val_loss: 0.7066 - val_acc: 0.4500\n",
            "Epoch 73/300\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.3142 - acc: 0.9333 - val_loss: 0.7078 - val_acc: 0.4000\n",
            "Epoch 74/300\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.3334 - acc: 0.9333 - val_loss: 0.7078 - val_acc: 0.4500\n",
            "Epoch 75/300\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.2779 - acc: 0.9833 - val_loss: 0.7092 - val_acc: 0.5000\n",
            "Epoch 76/300\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.3239 - acc: 0.9500 - val_loss: 0.7102 - val_acc: 0.5000\n",
            "Epoch 77/300\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.2966 - acc: 0.9833 - val_loss: 0.7120 - val_acc: 0.5000\n",
            "Epoch 78/300\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.2943 - acc: 0.9500 - val_loss: 0.7130 - val_acc: 0.5000\n",
            "Epoch 79/300\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.2969 - acc: 0.9667 - val_loss: 0.7136 - val_acc: 0.4500\n",
            "Epoch 80/300\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.2844 - acc: 0.9833 - val_loss: 0.7146 - val_acc: 0.4500\n",
            "Epoch 81/300\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.2943 - acc: 1.0000 - val_loss: 0.7145 - val_acc: 0.5000\n",
            "Epoch 82/300\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.2960 - acc: 0.9667 - val_loss: 0.7159 - val_acc: 0.4500\n",
            "Epoch 83/300\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.2813 - acc: 0.9333 - val_loss: 0.7189 - val_acc: 0.3500\n",
            "Epoch 84/300\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.2965 - acc: 0.9500 - val_loss: 0.7205 - val_acc: 0.3500\n",
            "Epoch 85/300\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.2791 - acc: 0.9667 - val_loss: 0.7216 - val_acc: 0.3500\n",
            "Epoch 86/300\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.2942 - acc: 0.9667 - val_loss: 0.7241 - val_acc: 0.3500\n",
            "Epoch 87/300\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.2719 - acc: 1.0000 - val_loss: 0.7245 - val_acc: 0.3500\n",
            "Epoch 88/300\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.2700 - acc: 0.9667 - val_loss: 0.7249 - val_acc: 0.4000\n",
            "Epoch 89/300\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.2991 - acc: 0.9500 - val_loss: 0.7239 - val_acc: 0.4500\n",
            "Epoch 90/300\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.2951 - acc: 0.9833 - val_loss: 0.7213 - val_acc: 0.4500\n",
            "Epoch 91/300\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.3034 - acc: 0.9667 - val_loss: 0.7168 - val_acc: 0.4500\n",
            "Epoch 92/300\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.2490 - acc: 0.9833 - val_loss: 0.7158 - val_acc: 0.4500\n",
            "Epoch 93/300\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.2818 - acc: 0.9833 - val_loss: 0.7161 - val_acc: 0.4500\n",
            "Epoch 94/300\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.2957 - acc: 0.9500 - val_loss: 0.7158 - val_acc: 0.4500\n",
            "Epoch 95/300\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.2734 - acc: 0.9500 - val_loss: 0.7155 - val_acc: 0.4500\n",
            "Epoch 96/300\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.2664 - acc: 0.9667 - val_loss: 0.7145 - val_acc: 0.4500\n",
            "Epoch 97/300\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.2920 - acc: 0.9500 - val_loss: 0.7154 - val_acc: 0.4000\n",
            "Epoch 98/300\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.2749 - acc: 0.9667 - val_loss: 0.7186 - val_acc: 0.4000\n",
            "Epoch 99/300\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.2802 - acc: 0.9667 - val_loss: 0.7193 - val_acc: 0.4000\n",
            "Epoch 100/300\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.2886 - acc: 0.9500 - val_loss: 0.7193 - val_acc: 0.4500\n",
            "Epoch 101/300\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.2590 - acc: 0.9833 - val_loss: 0.7204 - val_acc: 0.4000\n",
            "Epoch 102/300\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.2679 - acc: 0.9833 - val_loss: 0.7204 - val_acc: 0.4000\n",
            "Epoch 103/300\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.2743 - acc: 0.9667 - val_loss: 0.7218 - val_acc: 0.4000\n",
            "Epoch 104/300\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.2804 - acc: 0.9500 - val_loss: 0.7227 - val_acc: 0.4000\n",
            "Epoch 105/300\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.2718 - acc: 0.9667 - val_loss: 0.7219 - val_acc: 0.4000\n",
            "Epoch 106/300\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.2935 - acc: 0.9667 - val_loss: 0.7196 - val_acc: 0.4000\n",
            "Epoch 107/300\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.2390 - acc: 1.0000 - val_loss: 0.7189 - val_acc: 0.4000\n",
            "Epoch 108/300\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.2681 - acc: 0.9833 - val_loss: 0.7171 - val_acc: 0.4000\n",
            "Epoch 109/300\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.2624 - acc: 0.9833 - val_loss: 0.7138 - val_acc: 0.4000\n",
            "Epoch 110/300\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.2516 - acc: 0.9833 - val_loss: 0.7105 - val_acc: 0.4000\n",
            "Epoch 111/300\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.2430 - acc: 0.9833 - val_loss: 0.7080 - val_acc: 0.4000\n",
            "Epoch 112/300\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.2601 - acc: 0.9500 - val_loss: 0.7081 - val_acc: 0.4000\n",
            "Epoch 113/300\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.2796 - acc: 0.9500 - val_loss: 0.7123 - val_acc: 0.4000\n",
            "Epoch 114/300\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.2508 - acc: 0.9500 - val_loss: 0.7172 - val_acc: 0.4000\n",
            "Epoch 115/300\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.2397 - acc: 1.0000 - val_loss: 0.7207 - val_acc: 0.4000\n",
            "Epoch 116/300\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.2528 - acc: 0.9667 - val_loss: 0.7197 - val_acc: 0.4000\n",
            "Epoch 117/300\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.2635 - acc: 0.9667 - val_loss: 0.7208 - val_acc: 0.4500\n",
            "Epoch 118/300\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.2580 - acc: 0.9333 - val_loss: 0.7270 - val_acc: 0.5000\n",
            "Epoch 119/300\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.2542 - acc: 0.9667 - val_loss: 0.7258 - val_acc: 0.4500\n",
            "Epoch 120/300\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.2638 - acc: 0.9500 - val_loss: 0.7239 - val_acc: 0.4000\n",
            "Epoch 121/300\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.2616 - acc: 0.9833 - val_loss: 0.7210 - val_acc: 0.4000\n",
            "Epoch 122/300\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.2573 - acc: 0.9667 - val_loss: 0.7164 - val_acc: 0.4000\n",
            "Epoch 123/300\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.2344 - acc: 0.9833 - val_loss: 0.7159 - val_acc: 0.4000\n",
            "Epoch 124/300\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.2388 - acc: 0.9667 - val_loss: 0.7203 - val_acc: 0.4500\n",
            "Epoch 125/300\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.2609 - acc: 0.9667 - val_loss: 0.7238 - val_acc: 0.4500\n",
            "Epoch 126/300\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.2449 - acc: 1.0000 - val_loss: 0.7221 - val_acc: 0.4500\n",
            "Epoch 127/300\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.2401 - acc: 1.0000 - val_loss: 0.7172 - val_acc: 0.4000\n",
            "Epoch 128/300\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.2303 - acc: 0.9833 - val_loss: 0.7147 - val_acc: 0.4000\n",
            "Epoch 129/300\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.2471 - acc: 0.9833 - val_loss: 0.7145 - val_acc: 0.4000\n",
            "Epoch 130/300\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.2528 - acc: 0.9500 - val_loss: 0.7150 - val_acc: 0.4000\n",
            "Epoch 131/300\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.2463 - acc: 0.9667 - val_loss: 0.7148 - val_acc: 0.4000\n",
            "Epoch 132/300\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.2329 - acc: 0.9667 - val_loss: 0.7149 - val_acc: 0.4000\n",
            "Epoch 133/300\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.2265 - acc: 0.9833 - val_loss: 0.7152 - val_acc: 0.4500\n",
            "Epoch 134/300\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.2698 - acc: 0.9500 - val_loss: 0.7140 - val_acc: 0.4500\n",
            "Epoch 135/300\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.2636 - acc: 0.9833 - val_loss: 0.7126 - val_acc: 0.4000\n",
            "Epoch 136/300\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.2348 - acc: 0.9833 - val_loss: 0.7164 - val_acc: 0.4000\n",
            "Epoch 137/300\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.2236 - acc: 0.9833 - val_loss: 0.7175 - val_acc: 0.4500\n",
            "Epoch 138/300\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.2531 - acc: 0.9833 - val_loss: 0.7189 - val_acc: 0.4500\n",
            "Epoch 139/300\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.2511 - acc: 0.9333 - val_loss: 0.7214 - val_acc: 0.4500\n",
            "Epoch 140/300\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.2392 - acc: 1.0000 - val_loss: 0.7195 - val_acc: 0.4000\n",
            "Epoch 141/300\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.2458 - acc: 0.9667 - val_loss: 0.7209 - val_acc: 0.4000\n",
            "Epoch 142/300\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.2229 - acc: 1.0000 - val_loss: 0.7199 - val_acc: 0.4000\n",
            "Epoch 143/300\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.2384 - acc: 0.9833 - val_loss: 0.7208 - val_acc: 0.4500\n",
            "Epoch 144/300\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.2260 - acc: 0.9833 - val_loss: 0.7192 - val_acc: 0.4500\n",
            "Epoch 145/300\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.2248 - acc: 0.9667 - val_loss: 0.7205 - val_acc: 0.5000\n",
            "Epoch 146/300\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.2333 - acc: 0.9667 - val_loss: 0.7259 - val_acc: 0.4500\n",
            "Epoch 147/300\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.2537 - acc: 0.9500 - val_loss: 0.7338 - val_acc: 0.4500\n",
            "Epoch 148/300\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.2418 - acc: 1.0000 - val_loss: 0.7367 - val_acc: 0.4000\n",
            "Epoch 149/300\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.2228 - acc: 0.9667 - val_loss: 0.7352 - val_acc: 0.4000\n",
            "Epoch 150/300\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.2557 - acc: 0.9667 - val_loss: 0.7316 - val_acc: 0.4000\n",
            "Epoch 151/300\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.2427 - acc: 0.9833 - val_loss: 0.7308 - val_acc: 0.4000\n",
            "Epoch 152/300\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.2464 - acc: 0.9500 - val_loss: 0.7318 - val_acc: 0.4000\n",
            "Epoch 153/300\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.2199 - acc: 0.9667 - val_loss: 0.7339 - val_acc: 0.4000\n",
            "Epoch 154/300\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.2203 - acc: 1.0000 - val_loss: 0.7340 - val_acc: 0.4000\n",
            "Epoch 155/300\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.2367 - acc: 0.9833 - val_loss: 0.7345 - val_acc: 0.4000\n",
            "Epoch 156/300\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.2412 - acc: 1.0000 - val_loss: 0.7351 - val_acc: 0.4000\n",
            "Epoch 157/300\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.2318 - acc: 1.0000 - val_loss: 0.7387 - val_acc: 0.4500\n",
            "Epoch 158/300\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.2128 - acc: 0.9833 - val_loss: 0.7383 - val_acc: 0.4500\n",
            "Epoch 159/300\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.2298 - acc: 0.9833 - val_loss: 0.7359 - val_acc: 0.4500\n",
            "Epoch 160/300\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.2345 - acc: 0.9833 - val_loss: 0.7333 - val_acc: 0.4000\n",
            "Epoch 161/300\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.2161 - acc: 0.9667 - val_loss: 0.7355 - val_acc: 0.4000\n",
            "Epoch 162/300\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.2411 - acc: 0.9667 - val_loss: 0.7376 - val_acc: 0.4500\n",
            "Epoch 163/300\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.2362 - acc: 0.9667 - val_loss: 0.7393 - val_acc: 0.4500\n",
            "Epoch 164/300\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.2204 - acc: 0.9667 - val_loss: 0.7395 - val_acc: 0.4500\n",
            "Epoch 165/300\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.2351 - acc: 0.9500 - val_loss: 0.7397 - val_acc: 0.4500\n",
            "Epoch 166/300\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.2304 - acc: 0.9833 - val_loss: 0.7424 - val_acc: 0.4000\n",
            "Epoch 167/300\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.2003 - acc: 0.9833 - val_loss: 0.7468 - val_acc: 0.4000\n",
            "Epoch 168/300\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.2219 - acc: 0.9667 - val_loss: 0.7452 - val_acc: 0.4000\n",
            "Epoch 169/300\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.1966 - acc: 1.0000 - val_loss: 0.7393 - val_acc: 0.4500\n",
            "Epoch 170/300\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.2217 - acc: 0.9833 - val_loss: 0.7344 - val_acc: 0.4500\n",
            "Epoch 171/300\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.2117 - acc: 0.9667 - val_loss: 0.7352 - val_acc: 0.4500\n",
            "Epoch 172/300\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.1980 - acc: 1.0000 - val_loss: 0.7389 - val_acc: 0.4000\n",
            "Epoch 173/300\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.2173 - acc: 0.9833 - val_loss: 0.7420 - val_acc: 0.4000\n",
            "Epoch 174/300\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.2108 - acc: 0.9833 - val_loss: 0.7409 - val_acc: 0.4000\n",
            "Epoch 175/300\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.2167 - acc: 0.9833 - val_loss: 0.7427 - val_acc: 0.4000\n",
            "Epoch 176/300\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.2090 - acc: 0.9833 - val_loss: 0.7451 - val_acc: 0.4000\n",
            "Epoch 177/300\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.2013 - acc: 1.0000 - val_loss: 0.7447 - val_acc: 0.4000\n",
            "Epoch 178/300\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.2179 - acc: 0.9833 - val_loss: 0.7437 - val_acc: 0.4000\n",
            "Epoch 179/300\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.2137 - acc: 0.9833 - val_loss: 0.7424 - val_acc: 0.4000\n",
            "Epoch 180/300\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.2056 - acc: 0.9833 - val_loss: 0.7431 - val_acc: 0.4000\n",
            "Epoch 181/300\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.2137 - acc: 0.9667 - val_loss: 0.7396 - val_acc: 0.4000\n",
            "Epoch 182/300\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.2219 - acc: 0.9167 - val_loss: 0.7379 - val_acc: 0.4000\n",
            "Epoch 183/300\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.2086 - acc: 0.9833 - val_loss: 0.7429 - val_acc: 0.4000\n",
            "Epoch 184/300\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.2020 - acc: 1.0000 - val_loss: 0.7420 - val_acc: 0.4000\n",
            "Epoch 185/300\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.2195 - acc: 1.0000 - val_loss: 0.7389 - val_acc: 0.4000\n",
            "Epoch 186/300\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.1966 - acc: 1.0000 - val_loss: 0.7348 - val_acc: 0.4000\n",
            "Epoch 187/300\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.1970 - acc: 0.9833 - val_loss: 0.7322 - val_acc: 0.4000\n",
            "Epoch 188/300\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.2166 - acc: 0.9667 - val_loss: 0.7320 - val_acc: 0.4000\n",
            "Epoch 189/300\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.2024 - acc: 1.0000 - val_loss: 0.7306 - val_acc: 0.4000\n",
            "Epoch 190/300\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.2002 - acc: 0.9833 - val_loss: 0.7317 - val_acc: 0.4500\n",
            "Epoch 191/300\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.2054 - acc: 0.9833 - val_loss: 0.7338 - val_acc: 0.5000\n",
            "Epoch 192/300\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.2037 - acc: 0.9500 - val_loss: 0.7352 - val_acc: 0.4500\n",
            "Epoch 193/300\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.1957 - acc: 1.0000 - val_loss: 0.7373 - val_acc: 0.4500\n",
            "Epoch 194/300\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.2048 - acc: 1.0000 - val_loss: 0.7354 - val_acc: 0.4500\n",
            "Epoch 195/300\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.1999 - acc: 0.9833 - val_loss: 0.7349 - val_acc: 0.4000\n",
            "Epoch 196/300\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.2035 - acc: 1.0000 - val_loss: 0.7350 - val_acc: 0.4500\n",
            "Epoch 197/300\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.2126 - acc: 0.9833 - val_loss: 0.7445 - val_acc: 0.4000\n",
            "Epoch 198/300\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.1828 - acc: 1.0000 - val_loss: 0.7509 - val_acc: 0.4000\n",
            "Epoch 199/300\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.1701 - acc: 1.0000 - val_loss: 0.7526 - val_acc: 0.4000\n",
            "Epoch 200/300\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.1919 - acc: 0.9833 - val_loss: 0.7467 - val_acc: 0.4000\n",
            "Epoch 201/300\n",
            "\n",
            "Epoch 00201: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.1866 - acc: 0.9833 - val_loss: 0.7431 - val_acc: 0.4000\n",
            "Epoch 202/300\n",
            "\n",
            "Epoch 00202: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.2093 - acc: 1.0000 - val_loss: 0.7416 - val_acc: 0.4000\n",
            "Epoch 203/300\n",
            "\n",
            "Epoch 00203: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.1894 - acc: 0.9500 - val_loss: 0.7440 - val_acc: 0.4000\n",
            "Epoch 204/300\n",
            "\n",
            "Epoch 00204: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.1966 - acc: 0.9833 - val_loss: 0.7445 - val_acc: 0.4000\n",
            "Epoch 205/300\n",
            "\n",
            "Epoch 00205: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.1969 - acc: 1.0000 - val_loss: 0.7386 - val_acc: 0.4000\n",
            "Epoch 206/300\n",
            "\n",
            "Epoch 00206: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.1784 - acc: 1.0000 - val_loss: 0.7325 - val_acc: 0.4000\n",
            "Epoch 207/300\n",
            "\n",
            "Epoch 00207: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.2138 - acc: 0.9667 - val_loss: 0.7355 - val_acc: 0.4000\n",
            "Epoch 208/300\n",
            "\n",
            "Epoch 00208: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.2109 - acc: 0.9667 - val_loss: 0.7351 - val_acc: 0.4000\n",
            "Epoch 209/300\n",
            "\n",
            "Epoch 00209: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.1811 - acc: 0.9833 - val_loss: 0.7336 - val_acc: 0.4000\n",
            "Epoch 210/300\n",
            "\n",
            "Epoch 00210: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.2288 - acc: 0.9667 - val_loss: 0.7314 - val_acc: 0.4000\n",
            "Epoch 211/300\n",
            "\n",
            "Epoch 00211: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.1873 - acc: 0.9833 - val_loss: 0.7271 - val_acc: 0.4500\n",
            "Epoch 212/300\n",
            "\n",
            "Epoch 00212: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.1917 - acc: 0.9833 - val_loss: 0.7247 - val_acc: 0.5000\n",
            "Epoch 213/300\n",
            "\n",
            "Epoch 00213: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.1906 - acc: 0.9833 - val_loss: 0.7296 - val_acc: 0.5000\n",
            "Epoch 214/300\n",
            "\n",
            "Epoch 00214: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.1932 - acc: 1.0000 - val_loss: 0.7414 - val_acc: 0.4500\n",
            "Epoch 215/300\n",
            "\n",
            "Epoch 00215: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.1845 - acc: 0.9833 - val_loss: 0.7489 - val_acc: 0.4000\n",
            "Epoch 216/300\n",
            "\n",
            "Epoch 00216: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.1745 - acc: 0.9833 - val_loss: 0.7477 - val_acc: 0.4500\n",
            "Epoch 217/300\n",
            "\n",
            "Epoch 00217: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.1856 - acc: 0.9833 - val_loss: 0.7508 - val_acc: 0.4500\n",
            "Epoch 218/300\n",
            "\n",
            "Epoch 00218: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.1955 - acc: 0.9667 - val_loss: 0.7524 - val_acc: 0.5000\n",
            "Epoch 219/300\n",
            "\n",
            "Epoch 00219: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.1833 - acc: 0.9500 - val_loss: 0.7560 - val_acc: 0.5000\n",
            "Epoch 220/300\n",
            "\n",
            "Epoch 00220: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.1887 - acc: 0.9667 - val_loss: 0.7638 - val_acc: 0.4500\n",
            "Epoch 221/300\n",
            "\n",
            "Epoch 00221: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.1842 - acc: 0.9833 - val_loss: 0.7654 - val_acc: 0.5000\n",
            "Epoch 222/300\n",
            "\n",
            "Epoch 00222: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.1733 - acc: 1.0000 - val_loss: 0.7618 - val_acc: 0.5000\n",
            "Epoch 223/300\n",
            "\n",
            "Epoch 00223: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.2117 - acc: 0.9833 - val_loss: 0.7529 - val_acc: 0.5000\n",
            "Epoch 224/300\n",
            "\n",
            "Epoch 00224: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.1652 - acc: 1.0000 - val_loss: 0.7441 - val_acc: 0.5000\n",
            "Epoch 225/300\n",
            "\n",
            "Epoch 00225: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.1876 - acc: 1.0000 - val_loss: 0.7396 - val_acc: 0.5000\n",
            "Epoch 226/300\n",
            "\n",
            "Epoch 00226: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.1920 - acc: 0.9833 - val_loss: 0.7439 - val_acc: 0.5000\n",
            "Epoch 227/300\n",
            "\n",
            "Epoch 00227: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.1819 - acc: 1.0000 - val_loss: 0.7483 - val_acc: 0.4500\n",
            "Epoch 228/300\n",
            "\n",
            "Epoch 00228: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.1903 - acc: 1.0000 - val_loss: 0.7461 - val_acc: 0.4500\n",
            "Epoch 229/300\n",
            "\n",
            "Epoch 00229: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.1967 - acc: 0.9500 - val_loss: 0.7466 - val_acc: 0.4500\n",
            "Epoch 230/300\n",
            "\n",
            "Epoch 00230: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.2007 - acc: 1.0000 - val_loss: 0.7390 - val_acc: 0.4500\n",
            "Epoch 231/300\n",
            "\n",
            "Epoch 00231: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.2039 - acc: 0.9667 - val_loss: 0.7334 - val_acc: 0.5000\n",
            "Epoch 232/300\n",
            "\n",
            "Epoch 00232: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.1787 - acc: 1.0000 - val_loss: 0.7391 - val_acc: 0.5000\n",
            "Epoch 233/300\n",
            "\n",
            "Epoch 00233: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.1734 - acc: 0.9667 - val_loss: 0.7434 - val_acc: 0.5000\n",
            "Epoch 234/300\n",
            "\n",
            "Epoch 00234: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.1589 - acc: 1.0000 - val_loss: 0.7378 - val_acc: 0.5000\n",
            "Epoch 235/300\n",
            "\n",
            "Epoch 00235: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.1600 - acc: 0.9667 - val_loss: 0.7372 - val_acc: 0.5000\n",
            "Epoch 236/300\n",
            "\n",
            "Epoch 00236: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.1674 - acc: 1.0000 - val_loss: 0.7356 - val_acc: 0.5000\n",
            "Epoch 237/300\n",
            "\n",
            "Epoch 00237: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.1939 - acc: 0.9833 - val_loss: 0.7350 - val_acc: 0.5000\n",
            "Epoch 238/300\n",
            "\n",
            "Epoch 00238: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.1639 - acc: 1.0000 - val_loss: 0.7325 - val_acc: 0.4500\n",
            "Epoch 239/300\n",
            "\n",
            "Epoch 00239: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.1610 - acc: 1.0000 - val_loss: 0.7357 - val_acc: 0.4500\n",
            "Epoch 240/300\n",
            "\n",
            "Epoch 00240: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.2024 - acc: 1.0000 - val_loss: 0.7397 - val_acc: 0.5000\n",
            "Epoch 241/300\n",
            "\n",
            "Epoch 00241: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.2174 - acc: 0.9667 - val_loss: 0.7432 - val_acc: 0.4500\n",
            "Epoch 242/300\n",
            "\n",
            "Epoch 00242: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.1874 - acc: 1.0000 - val_loss: 0.7355 - val_acc: 0.5000\n",
            "Epoch 243/300\n",
            "\n",
            "Epoch 00243: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.1781 - acc: 1.0000 - val_loss: 0.7338 - val_acc: 0.5000\n",
            "Epoch 244/300\n",
            "\n",
            "Epoch 00244: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.1747 - acc: 1.0000 - val_loss: 0.7401 - val_acc: 0.5000\n",
            "Epoch 245/300\n",
            "\n",
            "Epoch 00245: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.1771 - acc: 0.9667 - val_loss: 0.7438 - val_acc: 0.5000\n",
            "Epoch 246/300\n",
            "\n",
            "Epoch 00246: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.1845 - acc: 0.9667 - val_loss: 0.7376 - val_acc: 0.4500\n",
            "Epoch 247/300\n",
            "\n",
            "Epoch 00247: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.1421 - acc: 1.0000 - val_loss: 0.7342 - val_acc: 0.5000\n",
            "Epoch 248/300\n",
            "\n",
            "Epoch 00248: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.1607 - acc: 0.9833 - val_loss: 0.7303 - val_acc: 0.5000\n",
            "Epoch 249/300\n",
            "\n",
            "Epoch 00249: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.2029 - acc: 0.9667 - val_loss: 0.7227 - val_acc: 0.5000\n",
            "Epoch 250/300\n",
            "\n",
            "Epoch 00250: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.1674 - acc: 0.9667 - val_loss: 0.7221 - val_acc: 0.5000\n",
            "Epoch 251/300\n",
            "\n",
            "Epoch 00251: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.1709 - acc: 1.0000 - val_loss: 0.7236 - val_acc: 0.5000\n",
            "Epoch 252/300\n",
            "\n",
            "Epoch 00252: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.1555 - acc: 1.0000 - val_loss: 0.7265 - val_acc: 0.5000\n",
            "Epoch 253/300\n",
            "\n",
            "Epoch 00253: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.1781 - acc: 0.9667 - val_loss: 0.7265 - val_acc: 0.5000\n",
            "Epoch 254/300\n",
            "\n",
            "Epoch 00254: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.1907 - acc: 0.9833 - val_loss: 0.7228 - val_acc: 0.5000\n",
            "Epoch 255/300\n",
            "\n",
            "Epoch 00255: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.1819 - acc: 0.9833 - val_loss: 0.7219 - val_acc: 0.4500\n",
            "Epoch 256/300\n",
            "\n",
            "Epoch 00256: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.1491 - acc: 1.0000 - val_loss: 0.7313 - val_acc: 0.5000\n",
            "Epoch 257/300\n",
            "\n",
            "Epoch 00257: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.1560 - acc: 1.0000 - val_loss: 0.7351 - val_acc: 0.5000\n",
            "Epoch 258/300\n",
            "\n",
            "Epoch 00258: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.1705 - acc: 1.0000 - val_loss: 0.7383 - val_acc: 0.5000\n",
            "Epoch 259/300\n",
            "\n",
            "Epoch 00259: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.1783 - acc: 0.9833 - val_loss: 0.7315 - val_acc: 0.5500\n",
            "Epoch 260/300\n",
            "\n",
            "Epoch 00260: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.1618 - acc: 0.9833 - val_loss: 0.7273 - val_acc: 0.5000\n",
            "Epoch 261/300\n",
            "\n",
            "Epoch 00261: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.1859 - acc: 0.9833 - val_loss: 0.7269 - val_acc: 0.5000\n",
            "Epoch 262/300\n",
            "\n",
            "Epoch 00262: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.1741 - acc: 1.0000 - val_loss: 0.7201 - val_acc: 0.4500\n",
            "Epoch 263/300\n",
            "\n",
            "Epoch 00263: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.1994 - acc: 0.9500 - val_loss: 0.7212 - val_acc: 0.4500\n",
            "Epoch 264/300\n",
            "\n",
            "Epoch 00264: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.1506 - acc: 1.0000 - val_loss: 0.7285 - val_acc: 0.4500\n",
            "Epoch 265/300\n",
            "\n",
            "Epoch 00265: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.1396 - acc: 0.9833 - val_loss: 0.7330 - val_acc: 0.4500\n",
            "Epoch 266/300\n",
            "\n",
            "Epoch 00266: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.1940 - acc: 0.9833 - val_loss: 0.7363 - val_acc: 0.5000\n",
            "Epoch 267/300\n",
            "\n",
            "Epoch 00267: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.1637 - acc: 0.9833 - val_loss: 0.7398 - val_acc: 0.5000\n",
            "Epoch 268/300\n",
            "\n",
            "Epoch 00268: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.1540 - acc: 0.9667 - val_loss: 0.7440 - val_acc: 0.5000\n",
            "Epoch 269/300\n",
            "\n",
            "Epoch 00269: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.1584 - acc: 1.0000 - val_loss: 0.7424 - val_acc: 0.5000\n",
            "Epoch 270/300\n",
            "\n",
            "Epoch 00270: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.1928 - acc: 0.9500 - val_loss: 0.7400 - val_acc: 0.5000\n",
            "Epoch 271/300\n",
            "\n",
            "Epoch 00271: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.1359 - acc: 1.0000 - val_loss: 0.7387 - val_acc: 0.5000\n",
            "Epoch 272/300\n",
            "\n",
            "Epoch 00272: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.1476 - acc: 1.0000 - val_loss: 0.7349 - val_acc: 0.5000\n",
            "Epoch 273/300\n",
            "\n",
            "Epoch 00273: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.1683 - acc: 0.9833 - val_loss: 0.7384 - val_acc: 0.5500\n",
            "Epoch 274/300\n",
            "\n",
            "Epoch 00274: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.1716 - acc: 1.0000 - val_loss: 0.7393 - val_acc: 0.5500\n",
            "Epoch 275/300\n",
            "\n",
            "Epoch 00275: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.1295 - acc: 1.0000 - val_loss: 0.7466 - val_acc: 0.5000\n",
            "Epoch 276/300\n",
            "\n",
            "Epoch 00276: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.1486 - acc: 1.0000 - val_loss: 0.7488 - val_acc: 0.4500\n",
            "Epoch 277/300\n",
            "\n",
            "Epoch 00277: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.1831 - acc: 0.9833 - val_loss: 0.7400 - val_acc: 0.4500\n",
            "Epoch 278/300\n",
            "\n",
            "Epoch 00278: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.1710 - acc: 1.0000 - val_loss: 0.7330 - val_acc: 0.4500\n",
            "Epoch 279/300\n",
            "\n",
            "Epoch 00279: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.1458 - acc: 1.0000 - val_loss: 0.7337 - val_acc: 0.4500\n",
            "Epoch 280/300\n",
            "\n",
            "Epoch 00280: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.1691 - acc: 0.9833 - val_loss: 0.7453 - val_acc: 0.4500\n",
            "Epoch 281/300\n",
            "\n",
            "Epoch 00281: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.1780 - acc: 0.9833 - val_loss: 0.7577 - val_acc: 0.5000\n",
            "Epoch 282/300\n",
            "\n",
            "Epoch 00282: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.1880 - acc: 0.9500 - val_loss: 0.7548 - val_acc: 0.4500\n",
            "Epoch 283/300\n",
            "\n",
            "Epoch 00283: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.1621 - acc: 1.0000 - val_loss: 0.7517 - val_acc: 0.4500\n",
            "Epoch 284/300\n",
            "\n",
            "Epoch 00284: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.1937 - acc: 0.9833 - val_loss: 0.7616 - val_acc: 0.5000\n",
            "Epoch 285/300\n",
            "\n",
            "Epoch 00285: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.1707 - acc: 0.9667 - val_loss: 0.7590 - val_acc: 0.4500\n",
            "Epoch 286/300\n",
            "\n",
            "Epoch 00286: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.1557 - acc: 1.0000 - val_loss: 0.7588 - val_acc: 0.4500\n",
            "Epoch 287/300\n",
            "\n",
            "Epoch 00287: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.1472 - acc: 0.9833 - val_loss: 0.7569 - val_acc: 0.4500\n",
            "Epoch 288/300\n",
            "\n",
            "Epoch 00288: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.1501 - acc: 1.0000 - val_loss: 0.7563 - val_acc: 0.4500\n",
            "Epoch 289/300\n",
            "\n",
            "Epoch 00289: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.1578 - acc: 1.0000 - val_loss: 0.7545 - val_acc: 0.4500\n",
            "Epoch 290/300\n",
            "\n",
            "Epoch 00290: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.1505 - acc: 1.0000 - val_loss: 0.7505 - val_acc: 0.4500\n",
            "Epoch 291/300\n",
            "\n",
            "Epoch 00291: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.1573 - acc: 1.0000 - val_loss: 0.7486 - val_acc: 0.5000\n",
            "Epoch 292/300\n",
            "\n",
            "Epoch 00292: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.1696 - acc: 1.0000 - val_loss: 0.7411 - val_acc: 0.4500\n",
            "Epoch 293/300\n",
            "\n",
            "Epoch 00293: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.1412 - acc: 1.0000 - val_loss: 0.7316 - val_acc: 0.5000\n",
            "Epoch 294/300\n",
            "\n",
            "Epoch 00294: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.1484 - acc: 1.0000 - val_loss: 0.7299 - val_acc: 0.5000\n",
            "Epoch 295/300\n",
            "\n",
            "Epoch 00295: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.1527 - acc: 1.0000 - val_loss: 0.7368 - val_acc: 0.5000\n",
            "Epoch 296/300\n",
            "\n",
            "Epoch 00296: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.1336 - acc: 1.0000 - val_loss: 0.7449 - val_acc: 0.5000\n",
            "Epoch 297/300\n",
            "\n",
            "Epoch 00297: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.1313 - acc: 1.0000 - val_loss: 0.7538 - val_acc: 0.5000\n",
            "Epoch 298/300\n",
            "\n",
            "Epoch 00298: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.1545 - acc: 0.9833 - val_loss: 0.7591 - val_acc: 0.5000\n",
            "Epoch 299/300\n",
            "\n",
            "Epoch 00299: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.1386 - acc: 1.0000 - val_loss: 0.7567 - val_acc: 0.5000\n",
            "Epoch 300/300\n",
            "\n",
            "Epoch 00300: val_loss did not improve from 0.69506\n",
            "60/60 - 0s - loss: 0.1549 - acc: 0.9833 - val_loss: 0.7553 - val_acc: 0.5000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAgAElEQVR4nOydd5icVb34P2fazsz2lmRLdje9J4Rs\nQgtNItKbdLleUC7qT6yXq6Ao6NUr13LVq6CioGADRIVwRUGUSBXSKQnpu8mWJNvbzO608/vjvO87\n78zO7k7CTnY3ez7PM8/M28/7znnP93zL+R4hpUSj0Wg0kxfHWBdAo9FoNGOLFgQajUYzydGCQKPR\naCY5WhBoNBrNJEcLAo1Go5nkaEGg0Wg0kxwtCDSTAiFEjRBCCiFcaex7oxDipWNRLo1mPKAFgWbc\nIYSoE0KEhBAlSes3G415zdiUTKM5PtGCQDNe2QdcZy4IIZYA/rErzvggHY1GozlStCDQjFd+CXzQ\ntvyvwMP2HYQQ+UKIh4UQLUKIeiHEnUIIh7HNKYT4thCiVQixF7gwxbEPCCGahRCNQoivCSGc6RRM\nCPE7IcRBIUSXEOIFIcQi2zafEOI7Rnm6hBAvCSF8xrbVQohXhBCdQogDQogbjfXrhBA3286RYJoy\ntKCPCyF2AbuMdd83ztEthNgohDjdtr9TCPEFIcQeIUSPsX26EOJeIcR3ku5lrRDiM+nct+b4RQsC\nzXjln0CeEGKB0UBfC/wqaZ8fAPnATOBMlOC4ydj2b8BFwHKgFrgy6dhfABFgtrHPucDNpMefgTnA\nFGAT8Gvbtm8DK4BTgSLgc0BMCFFtHPcDoBQ4AdiS5vUALgNOAhYay+uNcxQBvwF+J4TwGts+i9Km\nLgDygA8BAeAh4DqbsCwB1hjHayYzUkr90Z9x9QHqUA3UncA3gPOAvwIuQAI1gBMIAQttx30EWGf8\n/jvwUdu2c41jXcBUYADw2bZfBzxv/L4ReCnNshYY581HdayCwLIU+90B/HGIc6wDbrYtJ1zfOP97\nRihHh3ldYAdw6RD7bQfea/y+FXh6rP9v/Rn7j7Y3asYzvwReAGaQZBYCSgA3UG9bVw9UGL/LgQNJ\n20yqjWObhRDmOkfS/ikxtJOvA1ehevYxW3myAC+wJ8Wh04dYny4JZRNC3AZ8GHWfEtXzN53rw13r\nIeAGlGC9Afj+uyiT5jhBm4Y04xYpZT3KaXwB8Iekza1AGNWom1QBjcbvZlSDaN9mcgClEZRIKQuM\nT56UchEjcz1wKUpjyUdpJwDCKFM/MCvFcQeGWA/QR6IjfFqKfaw0wYY/4HPA1UChlLIA6DLKMNK1\nfgVcKoRYBiwAnhhiP80kQgsCzXjnwyizSJ99pZQyCjwGfF0IkWvY4D9L3I/wGPBJIUSlEKIQuN12\nbDPwLPAdIUSeEMIhhJglhDgzjfLkooRIG6rx/i/beWPAg8D/CCHKDaftKUKILJQfYY0Q4mohhEsI\nUSyEOME4dAtwhRDCL4SYbdzzSGWIAC2ASwjxZZRGYPIz4D+FEHOEYqkQotgoYwPKv/BL4PdSymAa\n96w5ztGCQDOukVLukVJuGGLzJ1C96b3ASyin54PGtp8CzwBbUQ7dZI3ig4AH2Iayrz8OlKVRpIdR\nZqZG49h/Jm2/DXgT1di2A/8NOKSU+1Gazb8b67cAy4xjvovydxxCmW5+zfA8A/wF2GmUpZ9E09H/\noAThs0A38ADgs21/CFiCEgYaDUJKPTGNRjOZEEKcgdKcqqVuADRojUCjmVQIIdzAp4CfaSGgMdGC\nQKOZJAghFgCdKBPY98a4OJpxhDYNaTQazSRHawQajUYzyZlwA8pKSkpkTU3NWBdDo9FoJhQbN25s\nlVKWpto24QRBTU0NGzYMFU2o0Wg0mlQIIeqH2qZNQxqNRjPJ0YJAo9FoJjlaEGg0Gs0kZ8L5CFIR\nDodpaGigv79/rItyzPB6vVRWVuJ2u8e6KBqNZoJzXAiChoYGcnNzqampwZZW+LhFSklbWxsNDQ3M\nmDFjrIuj0WgmOBkzDQkhHhRCHBZCvDXEdiGE+F8hxG4hxBtCiBOP9lr9/f0UFxdPCiEAIISguLh4\nUmlAGo0mc2TSR/AL1MxSQ3E+arq/OcAtwI/ezcUmixAwmWz3q9FoMkfGTENSyheEEDXD7HIp8LCR\n+OqfQogCIUSZkSteM0750xvNrKwpZEqeN2H9P3a2UF3kp6Yke1Suc6A9wM5DPZyzYGrK7V2BMOt2\nHubSEypSbrcTjsb4+cv7CEclN55aQ3ZW+tX+b9sPMb8sj50He5hZmk11ceL9vb6vnVyviwVleQnr\n32rsoj8cxeNy8Ny2QyyvLuTseVPSumZb7wAv72njkmXldAXDPLftEFecWGEJ//5wlD9ubuTq2uk4\nHYkdgmhM8vOX99EdDON2OvjgKTXk+938cXMDZ8+bQoHfA8DvNzawvz3AlSsq2d7czYKyPN5s7OKd\ngz1cvLSMOVNzeWV3KyW5WcydmsvLu1t5bW8bZ82fwolVhSnLvfNQD4e7B1g9pyTl9oNd/Wza38GJ\nVYU8sn4/U/O8nLtwKr9+bT8FfjdXrqjk6TcPctHSMtZuaeLS5eU8sbmRy5dX8vtNDVy+vIKHX62j\ntz9CltvJB0+p5vcbG2jvC+FyOvjASVUU52Tx1NYmTplVTElOFgBPbmlkb0sfV5xYYf1/f912iDcb\nOgE4d9E0FlfkA7Cxvh2P00lMSqJSJtzrzkM9tPWGKM7x8H9bm1hYns/cqTk8saUJpGTWlBxqa4r4\n3YYDlBf4uLpWzYvUFQjz9x2HuOyECoLhKP/3RjMXLCnjoVfqGAhHh60L5n/47LaDnLtoGvm+zPgE\nx9JHUEFiDvUGY90gQSCEuAWlNVBVVZW8ecxpa2vjnHPOAeDgwYM4nU5KS9UAvtdffx2PxzPiOW66\n6SZuv/125s2bl9GyvhsOdvXz8d9s4pPvmc1nz42XU0rJx3+9iQuWTOObVy4b5gzp85MX9vDI6wd4\n4+5z8XsGV9M/bG7gK09to7amiIoCX4ozxNlQ18F/Pf0OAKU5WVy9cvqw+5tIKfnYrzdx/aoqHl1/\ngDULp/KD65Yn7POFP75JZaGPX9y0KmH91/+0nYbOAGV5Pl6va6co28PGO9ekpck9sv4A33pmB8sq\n8/nHzha+/OTbzC/LZVG5aqzWbm3ijj+8SXmBjzPnJg4UfW1vG1/703Zr2edxcv6SMj7z6FY+s2Yu\nn1ozh2Aoyr//bisA+9sDPLmlkQuXlvPMWwcJRWPsPNjDfR84ket/9hoAdfdcyOcef4PGziCv7Wvn\n0Y+ckrLcX//Tdjbv72DLl8/F4Rh8nz/+xx5+8Uod7z9RNeyg/hvzd3tfiO89t4u9Lb3ct24PdW19\n3LduD/VtAe5bt4eN9R08vrHBOl/vQIQfrYvPyJmT5eKy5RV84reb+ciZM7nj/AWEIjE+8+gWYhI6\nAyG+culiItEYn35kM30h1Qhv2t/Jr24+CYD3/+jVhDLX3XOh9fs7z+5ge3MPtTWF/GFTI1kuB2sW\nTuVPb6gmSwi4/IQK/rBZTZJ3+pwSyvJ9rN3ayJeefJv50/J4q7GLzz3+Bq/va7fuZbgqISXUtwd4\nfGMDLb0D/L+zZg+987tgQjiLpZT3A/cD1NbWjrssecXFxWzZsgWAu+++m5ycHG677baEfcxJoh2O\n1Na4n//85xkv57tlQ307AA2diZNadQXD9A5EaOwcvcmuGjqCRGKSLQc6OXXW4B5mW28IgMaO4IiC\nwF6uDfXtaQuC/nCMUCTGW41dBMNRNta1pyjHAKne48bOIAfagzR39pPrddHeF2Jvax+zSnNGvG5D\nhyrvhrqOhN+mINhglGNjXfsgQbChvgMhYMuXz+XC/32RjfUdLK0ssO4doD0QsvZ/amsTMQl/frOZ\nSEyS63Wxob6D3S291j5NnUHrGQ71H0djkk31HfQMRNh1uJd503IH7bPeKPeTWxrJzXLRMxDhyS2N\n1vZdh9Q1X9+n9nvN+DaXn9jciNMh2PDFNSz/z7+y3lj/+4+dwgd+9hqNnUEajee1sa4DgEPd/cSM\nFsMs+zsHe+gLRfn+tSewsb6D329sIBKN0REIDypze1+IomzVkWvoCNLWO2DVvYFIjGfeOsgFS6Zx\nw0nVXP+z11i7tcm6tw11HVy8zEersf+G+g5aewase0mnc3DaPX/nCUOwmPeUCcZyHEEjiXPKVhKf\nb/a4YPfu3SxcuJAPfOADLFq0iObmZm655RZqa2tZtGgRX/3qV619V69ezZYtW4hEIhQUFHD77bez\nbNkyTjnlFA4fPjyGdxFng1ERmzsTndRNxnJT5+g5r81rbBii8ncYjVlz18jCp9loAFbPLhnyfKno\n6VcNwxsNXQA0dfUnNITRmKQrGKapM4g9i28sJq1yRWKSj56ppg9O90U2j91Q305Tp/k7fqx5D+tT\nnG9DfQdzp+SS73OzsqaI9XUd1jk21XeoBq9PPbtF5XlEjFbS/P7IGTNp7R2weukel8O69ulzSjjU\n3U80NrgvtuNgDz0DEaNcgwVm70CE7c3d1rWuXTWdPK+LSEyy0DCr7TGEj/m83zS+zeVITLK4PI/C\nbA/F2R5rfXmBj/ICH81dQZqMZ/dGgzLNmffucTms+mmWb2VNEbU1RfSForxzsIeN9YPLvdH23Ju7\n+ukLRTnU3W+VORKT1FYXcUJVAU6HIBKT/Msp1fg9Tktgm3V1Q117Qr1YUV04ooZYW1No/Tcb6juI\npXj2o8FYagRrgVuFEI8AJwFdo+Ef+MpTb7OtqftdF87OwvI87ro4nXnNB/POO+/w8MMPU1tbC8A9\n99xDUVERkUiEs88+myuvvJKFCxcmHNPV1cWZZ57JPffcw2c/+1kefPBBbr/99oR9lIaR+ppSSqIx\nics5tJwPRWJ4XENvD0djuJ0OwtEYLodACGH1KJuSGl/zZTMbxGhMIoQgEovR1htiap53kC3bJBqT\nHOrupyjbg9ftHHTOV/e0ceWKSgDyfW7Lvm++XKl6qOa9DUSiZLmcNHUFKcnxcNrsEv77L++wr7WP\nGTZfRigSw+0U9A5E6OlXjZnTIazfoWjM2vf5dw5z/aoqHA5BdzBMTEJfKEp3f4RsjxMhBG29A4Sj\n8T/n6trp/OzFvby0u5XVc0oG3Suo/+xwzwDZWa5441/XQa5X3e/6fUoodPeH2dvah8/tZMuBTg60\nB8jOclGU7bF65ZeeUA7AiupC/ri5kdf2tQGqnK/XtVsN+XsXTuXtpm58bifBcJRZpdm8d+E0vv3s\nTh58aR8AeV43G+va8XucnDN/Ci/uaqW1d4Di7Li583DPAOt2qs6Kz+3klT2tvGf+FAr9HnwedZ+b\n93cQk1jXWllTxO7DvTy/o4VzF01lW3M3+1r7Ep538re6pyIAygq8tDV243QIpuR6qSjw0djZbz27\nUFRpcmZdXVlTyNtN3QRDUV7Z00Z5vpfyAh+1xnnX7ThMU9fgjsyLu1pYVJ5HdpbS6gD2tfZxwZIy\negci7G8PUFtTiN/jYlF5Hm80dHHSzGK2HOjktX3t9PSHLU1jQ11HQr1bWZPa12KntrqQJ7c04XM7\n6QqG2dPSy5ypg7Wtd0vGBIEQ4rfAWUCJEKIBuAtwA0gpfww8jZrDdTcQAG7KVFnGklmzZllCAOC3\nv/0tDzzwAJFIhKamJrZt2zZIEPh8Ps4//3wAVqxYwYsvvjjovK29IQ739LNw0BblCLvtd1t59Y5z\nUjpGtx7o5Mofv8KznzkzoWKatPeFOO2ev/Ptq5Zx++/f4NtXL2P17BK2N/fgdgqaO/uJxaRlBzZf\ntoFIjPa+ELf8ciNLK/N5o6GLjfUdXLWikm9dldp3cNfat/jVP/ezpCKfpz6xGoDu/jA9AxE8Tgev\n7m3j1Hv+DkCh383rX1yD2+mwXsqmJEGwaX8H1/7kn3zrqqXc9rutPPWJ1TR29lOW72PVDPXinf3t\ndfzqwyexek4JHX0hzvjm8/znZYv50hNvWb1agCuWJzqiPU4Hdz7xFnWtfdx50cIEE0tTZ5DvPLuT\n7CwnN52mxnZ4XA4qC3yU5mZRW1PE2q1NrN3axOwpOTz32TMTzv3wq/XctfZt8n1uQwgLdh3uJdfr\nwuNycLC733oOANefVMUDL+3j9G8+D8DaW0/D53bSOxCxHJy1RkPz9JsHcTsF4ajk+p++xhmGOens\neVO47/k9XHFiBX/Y1MiqGUXMmZJDgd9Np9F4dQZCbD7QybLKAqYX+QElfL/2p+0EBiLk+9yWTXxa\nnpcV1YX86c1mnn7zINXFftbddhZCCNbXdeAQcM3K6Tz8ah0rqgvZ09KnBMHCaXzvuV0MROINfjJZ\nLgcDkZj1H5bn+3irsZtpRiejLN/LOwdbaOoM4jJ65uvrOogZvaUVVYW8vLuNc76zjqaufktYlhf4\nqCjw8e1nd6a87sOv1vPwq/WU58eDIwYiMQr9HlbNKKK9L2QFCqyqKWJbUzfLqwpYWVPE9/+2i1O+\n8XfmTM2xnlvvQASPy0EoEmPVjOIh79fE3Oe6VVU8+PI+NtR3TCxBIKW8boTtEvj4aF/3aHvumSI7\nO97Q7tq1i+9///u8/vrrFBQUcMMNN6QcC2B3LjudTiKRyKB9QpEokahMaJBN9rX20d0foa03lFIQ\nPL/jMOGoZG9Lb0pB0NARIBiOsnZrIz0DEd5q7CIny0U0Jjlr/hT+9s5h2vpClOaqqAy7SaixM8hb\njV0caA/Q0qvsoe8c7Bny+WxvVtvebOyy7LGmWegLF8zHn+VCSsnG+g4e29DAoe5+Kgv9dPSphirZ\nTPV2YxehaIx7/vwO4ajkxZ2tNHcGmVmazYlVhXz/2hP41CNbeOdgN6vnlPB6XTs9AxH+8tZBegYi\nfOCkKpZW5vOVp7ax1YgqAdWo//bfTuLutdt4YVcLgGViAahvC/DirhYqC32WcPrhdcuZWaqe710X\nL2TNgimsr1MOz6bOIOU234ZpNukKqvs6x3jOPf0RbjljJvOn5RI2esZ5XjdrFk5laWU+Pf0R7nzi\nLV7c1cqKatVITjMarblTcsn1uugKhllQlsfnz5vHx361idf2Kg2hstDHox85mRkl2VyzcjoVBT4c\nDsHPb1zJzkM9bG/u4Rev1LHjYA9Xrqi0ytvQEWTdO4cZiMbIyXJx8swiLl9ewaLyfIqyPZwxt4RN\n9Z08uuEA+9sDVBdns7G+nfnT8vjsuXO5aGkZxTlZ3HhqDcurClhQlms13kPx3oVTubp2OqtnK3+R\nWZbyAq+13NIzQF1bgOlFfgQqAmhqnpdCv5tZU1Rj3NTVz3WrpvOpc+Za5/7Jv6zg7SZlZjrQHuSH\nz+8G4JPnzKGiwMu6HS38+a2DCeUpynbzsbNmceOpNbgNzfsT75nDBUvLyPO6+fDpM+gKhvnFK3Vs\na+pmal4Wh7oH6AqGufHUGs5ZMIUTphcMeb8m86bl8pubT2JFTSHVxX5Onjmy8DgadK6hY0h3dze5\nubnk5eXR3NzMM888c9TnikmQQH9kcPiZGQ3R3T/Y+QVxu2e7rSGzY67fWK8awqbOftbXtSMEXLi0\nzFgX74nbf7/V2M1AJMbhngGkhOpi/6Beu53mziDVxf6Ecpn7L6ks4Ora6VyzsooLl5ZbZYGhTUOm\net/cFbcHm42uEIJLlpXj9zit85jXNO3gly2v4JqVVZTkZHGgI37uigIfK6qLOHfhVHYe6qUrEE54\nfs9uO8hAJEZzV9w8cdKMYmZPUb23ykI/16ys4sZTaxKuZy/39KK4YDhv8TRchoCfVZrNFSdWcs3K\nKq5ZWcX5S8pwOx1cekIFN5xczazSbDbUtVumLNOc5HAISzhUFHg5a94Uygu8DERiCKFMbcurCinw\ne1haWUCxEW65vKqQa1ZWWSGVA5GYZYcHZUbpGYgQMjTAS5ZVWPuXF/i4ZmUVN6027rNO+SU27++k\ntqaQPK+b2hpl3vF5nJw8Uw0ENcNah6Ki0McZc0utTo9dANi/N+/voLxAaSYb65Wz3V52gJtOm2EJ\nS4DFFfnWs11eFW+cV1Sr53DtqsGRioXZHkpzs6xnBJDvd1vaWJ7XzcXLyqznd+qsEnyGObCqyM/p\nc1JOC5CSU2eXkOVy8q+n1qTsuI0GWhAcQ0488UQWLlzI/Pnz+eAHP8hpp5121OcyVd6+gcGCIGCY\nN8yGwU4kGmOT0Qh1poiSsK9vNXr0TZ1BNtZ3MG9qrhUNkiwI5hrq7wabw83pEFywpIy2vhD9KeKl\nI9EYB7v7OW/RNDxOh+VcMxt382UH1ZCBcqRKKS1BkCxkkpef33GYvlCU8nzVEAghKC+I99pNx6F5\nr2aDUZjtIWQzVZQZDccKw9yycX97wvN7amsTAIFQlG3N3fg9TvJ8g7Wx+dNyExyJ9nIvKstnlqFB\nzCzNYZHRyJTlDx8VtbKmiI31HXQb2kSuNx5rXmsIAvMc5v3led3D+pBAmeJMKgp85HldZHuc1r1a\n10hh6za1kQ317Wxv7iEQiloCIBVF2Ynx8WZ9Mr/Lk56BeR/WfRnfrb0hyvJ91NYU0hEI8/LuVsry\nfdb/l+d1MXuYyK1Cm+/DvP/lVQWDQjwLRxBc9jIClOZmWRqAvV6PFyZE+OhE4u6777Z+z5492wor\nBdUI/fKXv0x53EsvvWT97uxUPfGuYJj3XnQ51157rbWttz9CIByxHH6BUATISjiXqRG81djFz17c\nS47XxW3nzuO+dXt4/4kV1vb69j4+//gb3Pqe2Xzjz9sJRyVfvXTRIE3hQEeAzkCYy5dXWC+c2fP+\nxtPbebupm/MWT6O+LZAQlbOwLM966Zq7lFbhMN6oJ7c0Mn9aLjEJM0qyWVyRx4b6Dp7fcZgHXtpn\nOQFNzBfetLOGo5KibA/tfSE+/chm/t24v70tfdYxJ80oskIQ7S9leYGPurY+Pv3IZisyBcAhYKph\n7iryJzZM5vEnTC/A5RB87U/byTMa3OJsD222Z7ahrsPSQJJxOR2cWFXI2q1NlnN07tRcmjqDnDGn\nlAK/mz0tfVQU+FhZXcjWA50JZU/FiupCHll/gE371bM3NQLAanytnrPxHIuyR27I7I1iWb7XEqK7\nDvcyJTcLv8dJRyCcsmE1tZENdR3MNWzaplBKeS2jYTWf5YrqInYe6rW+k5+BWR8qLM0gXlfKC3zW\nfUdikooCL1PzvDiEeh6pxjiYFPntgkD9zvO6mT8tj0Pd/da7kY4gmJKr/BfRmKTQ72FlTSGv7m0b\n8f8cC7QgGMe09Q4wEIlZKjuoGPDuYNiKOkmpEYSUJvB/bzaz9YASKnleN799fT8NHQFAOT6fefsQ\nLT0DNHUFeXFXKwDnLpxq9bZNzFj22ppCCvxufG4nTZ1BWnsH+MkLe6kp9nPxsjJ2HOxhm2HrNu2/\nZmPe1Bnku3/diUMIPC4H+1r7rGuWGS/uL16u40fr9tDaM8C/nFydEGmUneUi3+emqTNo+QcuX17B\ny7tbeWJLE7nG/QGcOquYAr+bz753LnetfRtIjNAoz/fyws4W3jnYw+KKPIqzs/jHzham5nmtXrLZ\nCAoBN6+eYTlY/R4XHz59Bo+8foC9LX1kuRx8aPUMntt+iFmlOTxujNh936LUI6IBbji5mvtf2EPv\nQIT2vpD1HMoLvFYDNiU3i6tqp9PeF7JMZ0Mxf5pyVr5lRMvZBcHyqgKuWF7BOQumGNcwNB7/yCNU\n7Y2iedw1K6fzpzebuXBJGTlZLnr6I0M2rLXVhazb0cLfth+2onSGvJbxvC9bXsHhngE+euZMevrD\nfOzMWXT3hwdF2Cwqz+PSE8o5yxitXVXk5/zF02jtHeCc+VOYWZLN5csr1H+xeBpup4N/O30mJ88a\n3sZuF352YXnLGTM42DXAfet209MfSUuQOh2CaXleGjuDFGW7WbV4GnVtAUswjie0IBjHhKOScDRG\nTEqrJx2OqGXTNGQ2+nZM4bC/Ld47Nk0gL+5qpSzfS67XxU5jAM+Lu1qtnktTZ/8gQWBixj2XF3gt\ncxHAd65exorqItbtaGFbczdZLgd3XbwQIQT1Rhk21HVYdnuA2VNy2H1YXb+iwEttdSH3v7CX1/e1\nc8PJVdx9yWCnf3mBj+bOfita59RZxVy8rJzL7n05IXZ9eVUB//G++QD8+uaTU54HVCP/65tP5skt\njfxjZ4tlPoB4I5jjcfHFCxNjs+44fwFI+MkLe/F7nHz87Nl8/OzZHO7pt0aL1lYPbQY5b/E0zls8\nDVA27cvve8Uq1wnTCywTwrxpufzPNScMeR6TQsOssr+tD4/LQZYrHpqa5XImnKPM6DkfiUYgRNwB\nffPpM7n59JkjHgtxbeSl3a1csqx82H1NH8HiijwuX67ChX94vcpDee/1g/NRet1Ovn9tfJS3y+ng\nRzesSNjnu0nP7o4LFoxY5jyvC6dD4HQI/J74czTL9Mj6/fT0R6xnPhLlBUoQFPo9zCjJ5n+TRqaP\nF7SPYJwipbSiRCK2OGpznfltmnnsmMLBPlJyx6F45M6K6sJBDcHKmkJKcrISetwQbzCm5XmtEbzl\nBT6auvrZUNeOx+WwHGZxx2TcLGI2IGu3Jo4V/MgZ8cakLN9nHQtDN6IVxktlCqrCbI8V1me/v5FU\nb3O7OfCqPMl+bp4bEnvXdszy2p9xSXYWHkOjWJFGjDjAovJ8sozxHEdrMjD/o45AmNwR8iiZ/+FI\nzlmIN4pTc71WZMyRsKyywHJ4p/Ij2DF9BOmUK5MIISj0uyn0u1Oa9szyFfjSK6fd5zSe0YIgDQKh\nSMLI0dEiFIlaDXoy0Vi81x+OSiLRGP3hqDVQyfQRBI1Gv88Igdy0vyNBONQU+/G4HAmDz2qrCwfZ\nOFfWFFFR4KWpK0h7XyjeoFXHY9LNF6M8XzlbN9R3sLQi3+qBrqyJD/YxyXI5KcnJYk9LH9keJz63\nE4/LwcXLyinO9liDxIpzsphpREQM1WiU5fto6Ajyjx0qfLPI76EkJwu3UyTcX7JjMRlTeJjXKUuK\nQIG4DThnBEFgx+EQTMv3kuVysLg8P8VRg/G4HHEnYv7RORF9bqclTIYSXCam1pOORmA2imVH6dz0\neZyWwzvV87JjPu+iMRYEoGSIpgYAACAASURBVMoylA+gyO8mN8s17GBMO6YvIx2fwliiTUMj0GuM\n5Cwv8FnZDEeL+vYAHqdjUEZLIEFAhKIqTK8rGEaSKJBMM9DDr9bz3395B4dIjDIpyvYghGBfax9T\n87Jo7Q1x2uwSdhpmmdLcLNp6Bzh9Tim7D/ey81APbqeDpZX5vNnYxfsWTePl3a0JOW3MmO3OQIgP\nr56ZsH7OlBzLZm0yd2oOrb0DVqhgKBrD63Zy5tzShPDPM+aWgmDI3EFzp+bQOxDhF6/UkeVyUJqb\nhcMhKMv3sb89YN3f7CnD5/OZNSUHt1NY91RV5Cfb42RBWdx2a/ZQ7RE4dky/zaqkSJgFZbnMnpKT\ndkMBcOa8Uva09B51/RJCqPEXXf1DltekvMBHvs9tCd2RmFmSw+ypI+dHGooz55bS3BkcVCeSmVWq\nnllF4dg7UmeWZuMaIifYzNKcIcOuU7GwPA+/x8nUvNFtO0YbLQhGwBzk0jcQGXVBEArHhkwTEbKl\nKAhHYvQORCwNwY5pBtrfrmzxMZmYf8dMZ7CvtY9LT6jgY2fOojDbY/W8Tp9dwpcuWkhhtoe/vHWQ\ndTtayPW6WFZZwAP/upI8n4v3LpiaEApp9hDDUTkoEuSJj582yIzw0w/W0tARpKrIjxBY9/xfVyxJ\nuP8vXLCA26Lzhsy/8oGTqjllVjHRmLKLm4PlyvK97G8PcPnySj565swRzQtT87ys/+Iaa79cr5tX\n7jgnwaxS6B/eNASw7avvG9Rg2O3W6XLL6TO54eTqYaNZRqLQbwqC4V9pr9vJS58/O2VG11Q89KFV\nDNEmpsUn3jObm0+fMWSKEZOz5pWy/gtryE/DiZ1phvsPP3/efCKxoUdAJ3Px0jLOmlc6ooAea7Qg\nGAGzAkeiQ5uGjiYNdTQWIyol2Hr+Dz74IBdccAHTpk2zNAIhBIHQ0CYk0wzUaBthax+gWeD3UGAE\nnZTney1bpfldXuCz/fYSDEcJhqMUZnuslzL55bT32JNV/lQjmbOzXCmzUSbn2/G4HMP2pB0OYQ3Q\nSlWeigJv2jbm5P2S87zHfQRDv8CpGtPke0oHl9NB3lHY4O0UWhrMyK/0kTRKPs+R348dt9ORln9B\nCDEuhAAM/x96XA48R2BRF0JYYcbjGS0IRsBsVIcb/p5OGupkQjZbfzQmcToEDz74ICeeeKIlCIQQ\nZLkc1ghhASSXwhw81tQZZEpuFoeNNLcm9gRniTZw96B1FSm2p8I8ZlZp9rhwgiWPLh0N0tEIxhOF\n/pEFl0YzFBOjlo8hppP4SNRBOw899BD33nsvvcF+Tj75FL7wtW+R7XHymY/fwsZNm9XkJx/9CJXl\nZWzZsoVrrrkGtyeLh9f+jWyfikLpD0dxCEFOlmtQ2ojfb2q0Ug2fPLOYv7+TmLK60O+x7N32hrLI\npgWYpIqaSYXpcBwuRPJYkglBUOB3IwQTojcH8f9zogguzfji+Ks1f74dDr45aqfLjsUoy5tP8yl3\nHfGxb731Fn/84x95+eWXeedQgK/d/mkef+xRKmtm0Nrayu+fU/Hjha4w06eV8oMf/IAf/vCHZE2d\nqcL28ry4HGoAlt/jJMvlJN/v5kB7wLpGo23SkEXleYMEQVG2mwuWlBEIRa0c6gCnzirhzgsXJEz6\nsrgin0+vmUMwHGXNEFNEglKdv3PVshFDAo8VFy4tYyASZX4K89PR4nY6+N41J7B8+vi4x5GwNIIj\nmIZTozHRtWYkbLYYKeURTRr/3HPPsX79empXrmQgHKW/v5+SaeWsPnsNu3bu5J4vf57T33MuV10W\nnw4vHInhiMYozfVZdmu73d3rdtAgRMpw1nnTcq2BYX6Pk0AoSqHfQ67XbaVGNvG4HIMGBjkdgk+v\nmUs6vN+YI2A8kO8bfH+jQTrzIY8XitLwaWg0Q3H8CYLz7xnV03X2DFhROOGoxONKXxBIKfnQhz7E\n7XfeZc2+BMrJ+JcXXuMvf/kzjz70M15+7ml+9YsHADUxeRaQPYSTTgiBUwgiKQRBRYGPQr+b1t4Q\n0/K87G3tGxc2fE3mKfCn7yzWaJLRA8pGwN7zHipyx05/OGo5ltesWcNjjz3GwUPKXNPZ0U5z4wHa\n21oIRaNccvn7+eTnvsgbWzYDkJubS0tHFw4hho1cMMP5kiPylCBQDf8UI255vA9k0YwOWiPQvBt0\n92EE7MFCkREEgTmHrcenInmWLFnCXXfdxaUXnU8oHMHj8fCFr3+H/p5O7vjMrYBEILjtS18B4Kab\nbuLfP/kxfF4fWzZtSBl2CuA0zFOmmWp5VQEOISjJybIa/lNnldA7EBlxYnfN8cG8abnMmZLDovLh\nB25pNKnIqCAQQpwHfB9wAj+TUt6TtL0aeBAoBdqBG6SUDZks05FiH8k7zFACQGkMH/vs7Ql5SK6/\n/nrOuuByOvpCLCzPY397gIFIjN8/+yI5WS7cTgctPQNEY5L3X3klC049lyl53iGFABAfeGRoKx86\nbQYXG0m9zHjyM+aW8slz5hzNLWsmIFNyvfw1afpLjSZdMmYaEkI4gXuB84GFwHVCiOQpdr8NPCyl\nXAp8FfhGpspztNg1glQje+0kJ4QzUZOjOxBCTQKvJp5XU0z6s5xIJMFQhEAoioSErIepMDUCszT2\nEFDTRDCUj0Gj0WiSyaSPYBWwW0q5V0oZAh4BLk3aZyFgzsj9fIrtY440BnuBEgTRYQaWmTNa2QVB\nJBYjFI3hNkbMOlDCJSaVjd9s9Lv7I/T0RxCkHrFqx+kQanCZUZRUydL8OoxQo9GkSSYFQQVwwLbc\nYKyzsxW4wvh9OZArhDiq2ZkzkR0UIEY8zUQoEmNbczc9Q8wFbGYGDUel1evfdaiX/nDUyuYphLDm\nExBC4HI48LmdtPYO0No7gNfjHDEvi8spcAhhCZfEmbzU2IM8HT2i0WjSZKxbi9uAHwohbgReABqB\nQQn2hRC3ALcAVFUNnkja6/XS1tZGcXHxEcX5p4M0JoVxCsFAOIaUkv5wNGV0hqkJSCThqGrsw9EY\nJTlZTDGmQHQIiBlahdneVxX5CRpz+vrSyFVTku1BBnvwFOfyzKfPSBAcV9VOZ3lVoY4e0Wg0aZNJ\nQdAITLctVxrrLKSUTRgagRAiB3i/lLIz+URSyvuB+wFqa2sHdf0rKytpaGigpaVl9Epv0No7YJiE\nVK6fSEzSl+WiNUUunpaeAUKRGBKQHVmEozE6AmFEXhZdhkbQHQzTbUwqP+B303aUJhyv10tNVRVu\nd2I5vG6nNVGMRqPRpEMmBcF6YI4QYgZKAFwLXG/fQQhRArRLKWPAHagIoiPG7XYzY8bojywFuO7+\nfxKJxYy5ffsJRWK8d+FUfvrBpYP2/cg3n8fvcfLOwR5+cN1ynt9xmBd2tbH+i2ssTeWHf9/Ft5/d\nCcB/v38J15wwWMPRaDSaY0nGfARSyghwK/AMsB14TEr5thDiq0KIS4zdzgJ2CCF2AlOBr2eqPEdC\nR1+ITz+ymY6+EAORKFkuJ36Py3IGN9kmU4lEY3zqkc1c/eNXaewMWmmZmzqDbNzfYc3za2KfT/Zo\nUhZrNBrNaJNRH4GU8mng6aR1X7b9fhx4PJNlOBqe2NLIE1uaKPB76A/HKMp2kJ0Vb7Ttk7Bva+7m\nyS1NLCzL45SZxVxVO521W5uoa+ujvi3AFcsTc/J43XHZm44/QKPRaDLNWDuLxyVmSOeuwz02jSDu\nmmjvCxEMRfF5nGyo6wDggRtrrSkiy/N9bKpXro7ypPle7RrBu530Q6PRaEYDnWsoBZ0BFR66t6WP\n/nCMLHeiRgDQZCSi21jfQUWBL2Ge4PICLzsO9QCD59/NsmkE2jSk0WjGA1ojSEF7QE1O3dzVj9Mh\nyHI5cSSFpTZ39tPYEeS1fW2cNrskYVuZrfEvSxYEdo1ACwKNRjMO0IIgBR19Iet3NCbxuh14nEoQ\n5Ga56BmI8PyOwzzw0j4AVicJArsWYM7mZeLVGoFGoxlnaEGQgo5AmDyvy4r3z3I5MRWC2VNzaOwI\n8tvX9wPw6C0ns2pG4pSNpl+gJMczqLHXPgKNRjPe0D6CFHT0hZg/LZ7O1+t2WEnccr1uamsKCYSi\nFPjdrKwpGjSa2fQX2P0G9nOZaNOQRqMZD2hBkIL2QIjSvCxr1idzHAGoGaBWGJO2r6gqjKeEtlFh\nTabuHbRN+wg0Gs14QwuCFHT0hSjye6yUzl63wwopzfO6WGlM2r5iiMnbp+Z5cYjErKAm9qihLJd+\n/BqNZuzRPoIkzFnGCrM9FGd72NvSpzSCLFMjcLOkIp9vXLGECxaXpTyHx+Xgvg+cyKLywTl/TJ+B\n1+1IqU1oNBrNsUYLgiS6g2FiEgr9boqzVcZQj8vmI8hyIYTgulXD5wg6bwghYWoBOmJIo9GMF7Rt\nIglzDEFRtoeiHGUaCoQiCT6Cd4MpALR/QKPRjBe0RoDyCdy3bjcel8MaHFboV6YhgLbeECdMVw13\nzrvM829qBFoQaDSa8YIWBMALu1r46YtqcFh9WwBQET9X107nt6/v57LlFRRle1hWmc8J099drn+3\n04HTIbRpSKPRjBu0IABr4BjA0282k+9zM7MkB4dDsOHO91rbnrx19ahcL8vl0IPJNBrNuEH7CMCa\ng3jOlBxiElZUpx4fMFp43U5tGtJoNOMGLQiAnv4Ibqew/AO1Q4wPGC2yXA5tGtJoNOMGLQhQGkGu\n180ps4oBOHlmcUavV5Qdd0RrNBrNWKN9BCiNINfr4tyFU3nq1tUsqczs5O/3f7AWv9YINBrNOCGj\nGoEQ4jwhxA4hxG4hxO0ptlcJIZ4XQmwWQrwhhLggk+UZip7+CDnGQLFMCwFQuYgKtUag0WjGCRkT\nBEIIJ3AvcD6wELhOCLEwabc7UZPaLweuBe7LVHmGo9fQCDQajWYykkmNYBWwW0q5V0oZAh4BLk3a\nRwJmvud8oCmD5UlgT0svn3pkM6FIjG7DR6DRaDSTkUwKggrggG25wVhn527gBiFEA/A08IlUJxJC\n3CKE2CCE2NDS0jIqhXtlTxtPbmlif3uf5SPQaDSaychYRw1dB/xCSlkJXAD8UggxqExSyvullLVS\nytrS0tJRuXBgQA0i6wiE6ekPk6c1Ao1GM0nJpCBoBKbbliuNdXY+DDwGIKV8FfACJRwD+kJRQOUR\n6h3QGoFGo5m8ZFIQrAfmCCFmCCE8KGfw2qR99gPnAAghFqAEwejYfkbA1AgaO4PE5LvPKqrRaDQT\nlYwJAillBLgVeAbYjooOelsI8VUhxCXGbv8O/JsQYivwW+BGKaXMVJnsmBrB/rY+AHKytGlIo9FM\nTjLaDZZSPo1yAtvXfdn2extwWibLMBSBkNII9rerbKNaI9BoNJOVsXYWjxl9A0ojqNeCQKPRTHIm\nrSAwNYIDliDQpiGNRjM5mcSCQGkE4ahySeRpjUCj0UxSJrEgiE9G43E5qCr2j2FpNBqNZuyYtILA\n9BEALKvMJ8uls4FqNJrJyaQVBHaNYHlVZiei0Wg0mvHMpBUE5jgCgNmlOWNYEo1GoxlbJqWHNByN\nEYrEuHBJGc1dQd63eNpYF0mj0WjGjEkpCMyIoROrC/nw6hPHuDQajUYztkxK05DpH8j2aAexRqPR\nTEpBYEYM+bQg0Gg0mskpCIKGaSjbMyktYxqNRpPApBQEfYZpyJ+lNQKNRqOZlIIg7iPQGoFGo9FM\nSkFg+giytUag0Wg0k1MQmBqBX2sEGo1GMzkFgaURaEGg0Wg0mRUEQojzhBA7hBC7hRC3p9j+XSHE\nFuOzUwjRmcnymJgagQ4f1Wg0mgyOLBZCOIF7gfcCDcB6IcRaY3pKAKSUn7Ht/wlgeabKY6cvFMXt\nFHhck1Ih0mg0mgQy2RKuAnZLKfdKKUPAI8Clw+x/HWoC+4wTDEW1f0Cj0WgMRhQEQohPCCGOJk9z\nBXDAttxgrEt1jWpgBvD3IbbfIoTYIITY0NLSchRFSaRvIKLTS2g0Go1BOhrBVJRZ5zHD5i8yUI5r\ngcellNFUG6WU90spa6WUtaWlpe/6YoFQFH+W1gg0Go0G0hAEUso7gTnAA8CNwC4hxH8JIWaNcGgj\nMN22XGmsS8W1HCOzEKiRxVoj0Gg0GkVaPgIppQQOGp8IUAg8LoT45jCHrQfmCCFmCCE8qMZ+bfJO\nQoj5xvlePcKyHzWBAe0j0Gg0GpN0fASfEkJsBL4JvAwskVJ+DFgBvH+o46SUEeBW4BlgO/CYlPJt\nIcRXhRCX2Ha9FnjEEDbHhL5QBL/WCDQajQZIL3y0CLhCSllvXymljAkhLhruQCnl08DTSeu+nLR8\nd3pFHT20j0Cj0WjipGMa+jPQbi4IIfKEECcBSCm3Z6pgmURHDWk0Gk2cdATBj4Be23KvsW7CEtDj\nCDQajcYiHUEg7PZ7KWWMCTzXsZSSQCiiM49qNBqNQTqCYK8Q4pNCCLfx+RSwN9MFyxQDkRgxqTOP\najQajUk6guCjwKmoMQANwEnALZksVCbpGzAmpdEagUaj0QBpmHiklIdRIZ7HBQFjvmKfWwsCjUaj\ngTQEgRDCC3wYWAR4zfVSyg9lsFwZw5yvOFuHj2o0Gg2Qnmnol8A04H3AP1CpInoyWahMYk5KoweU\naTQajSIdQTBbSvkloE9K+RBwIcpPMCEJaI1Ao9FoEkhHEISN704hxGIgH5iSuSJlFq0RaDQaTSLp\ndIvvN+YjuBOVNC4H+FJGS5VBgmFDI9DhoxqNRgOMIAiEEA6gW0rZAbwAzDwmpcogWiPQaDSaRIY1\nDRmjiD93jMpyTAia4aNaEGg0Gg2Qno/gOSHEbUKI6UKIIvOT8ZJliGBYCQKvHkeg0Wg0QHo+gmuM\n74/b1kkmqJkoGI7idgrczrTm5NFoNJrjnnRGFs84FgU5VgRDUa0NaDQajY10RhZ/MNV6KeXDo1+c\nzDMQier0EhqNRmMjHdPQSttvL3AOsAmYkIIgGIpqR7FGo9HYSMc09An7shCiAHgknZMLIc4Dvg84\ngZ9JKe9Jsc/VwN0ov8NWKeX16Zz7aAmGtUag0Wg0do5mVFUfMKLfQAjhBO4F3otKX71eCLFWSrnN\nts8c4A7gNCllhxAi4yOWg+GY9hFoNBqNjXR8BE+heuugwk0XAo+lce5VwG4p5V7jPI8AlwLbbPv8\nG3CvMWDNTHmdUfpDUbxuHTGk0Wg0JuloBN+2/Y4A9VLKhjSOqwAO2JbNSW3szAUQQryMMh/dLaX8\nS/KJhBC3YEyGU1VVlcalhyYYjlKS43lX59BoNJrjiXQEwX6gWUrZDyCE8AkhaqSUdaN0/TnAWaj0\n1i8IIZZIKTvtO0kp7wfuB6itrZXJJzkSgmHtLNZoNBo76dhIfgfEbMtRY91INALTbcuVxjo7DcBa\nKWVYSrkP2IkSDBlDjyPQaDSaRNIRBC4pZchcMH6nY1tZD8wRQswQQnhQ012uTdrnCZQ2gBCiBGUq\n2pvGuY8aPY5Ao9FoEklHELQIIS4xF4QQlwKtIx0kpYwAtwLPANuBx6SUbwshvmo73zNAmxBiG/A8\n8B9SyrYjvYkjIRjSgkCj0WjspOMj+CjwayHED43lBiDlaONkpJRPA08nrfuy7bcEPmt8Mo6UUvsI\nNBqNJol0BpTtAU4WQuQYy70ZL1WGCEVjxKTOPKrRaDR2RjQNCSH+SwhRIKXslVL2CiEKhRBfOxaF\nG236Q8rnrU1DGo1GEycdH8H59nBOY/DXBZkrUuYw5yLQpiGNRqOJk44gcAohsswFIYQPyBpm/3FL\nfFIaPbJYo9FoTNJxFv8a+JsQ4ueAAG4EHspkoTJFv6kRaNOQRqPRWKTjLP5vIcRWYA0q59AzQHWm\nC5YJ9DSVGo1GM5h0bSSHUELgKuA9qHEBE47+0HGiEbzxO/jWbPjVlWNdEo1m4vOXO+CfPz7y47b8\nBr41B3573eiX6RgzpEYghJgLXGd8WoFHASGlPPsYlW3UOW6cxftfhb4W2P1XiEbAeTTZxDUaDQDb\nnoTSeXDyR4/suL3/gL7DsONpiMXAMXF9j8OV/B1U7/8iKeVqKeUPUHmGJizB48VHEA7Gfwc7xq4c\nGs3xQKBdfY6UoO2Yga7RK88YMJwguAJoBp4XQvxUCHEOylk8YQmGjhMfQTgQ/x3IaEYOjeb4JhSA\nSPDoBIH93Tua48cRQwoCKeUTUsprgfmoPECfBqYIIX4khDj3WBVwNOk/XkxDdo1ACwKN5ugx35+j\neY8CbeArOvrjxxEjGrWklH1Syt9IKS9GpZLeDHw+4yXLAL0DShBkeya4TT0cAE+O+j3BK6BGM6aY\n5p1wX2IHKx0C7VBiZM2f4O/hEXk3pJQdUsr7pZTnZKpAmaQzECLL5Tg+NIL8SvU7OLFVUo1mTDla\n804kBAPdUDwJBcFEp70vRFH2cTBNZTgIeRXq9wSvgBrNmGJv/I+kU2UGaVgawcTukE0qQdARCFHo\nPx4EQQD8ReD2T/gKqNGMKQkawRF0qsx9C6rA6ZnwHbJJJQiOK43A7QN/sRYEGs27wf7+HI0gyC4x\n3kMtCCYMnYEwBX73WBfj3RMJKm3AXzThK6BGM6YE2kAYzeCRdKrM985XpD4TvEOWUUEghDhPCLFD\nCLFbCHF7iu03CiFahBBbjM/NmSxPe+A40wh8WhBoNO+KQBvkT4//ThfTn+AvPi46ZBmLoxRCOIF7\ngfeiprdcL4RYK6XclrTro1LKWzNVDpNINEZXMDzxfQTRCERDhkZQDB11Y10ijWbiEmyHnKnQ33l0\npiF/kXoPDyc3axOLTAbUrwJ2Syn3AgghHgEuBcbkiXUFw0gJhRPFNLT9/1Q+obxy6KiHk25R6yNG\nrLPLawiCfSpp1nnfGHyOtj3wzBeU4PAVwXvuhGe+qM7hyYFzvwbP3gmhXiVYLvwO5E5ToXFPfhwC\nreo8DjesuRumLIC/fQUWXQFlS4/FU0iPrkZ48Ttw3j3gOgaC/tX7oGQuzFmT+WuNJlLCX78EJ3xA\n/ZejxdtPwKaHYObZcNon4+v3/gNe/j5MWwzzL4J//DfIWOKxueVwyf+Cwwjp7jwAf/4cRPoHX8ft\nh4u+B948+PPn4fR/h799dXA9ff1+6KwforACTr0VGjdC/SvQuAmqT1Pvx+v3Q+VKWHp1fPc3fgdb\nfzP4NG27wZMLrqyhfQQtO2DjQ+o96zoAr/xA1dHk3GD9Xeo9PPN2eOYOGOgZouzAqo/AvPOG3n6U\nZFIQVAAHbMsNwEkp9nu/EOIMYCfwGSnlgeQdhBC3ALcAVFVVHVVhOgIhAAonimlo00OqIS+dB81b\n44LAHPTi9sH8C+D1n8A/71OVzZE0PmL332DnX6BoJuz5uxIqO/6kltv3QmE1bF8bX178flh8hark\nbz4GxbPBWwCNG6D6VHX8S98F4RxfgmDXs7DhAaj9kGp0Ms2L34Ga1RNPEPS1qMYoK290BcHmX6n6\ndfidREHw1uOw52/qEw3Dnueh4kRbeVrVcWfdDgWGeabuRZXErWyZisYxCQfh0Fuw7DrILYONP4es\nnMH1tHSu2lZQDTlTBpe1+Q11/I6nwelW79fi90PXfiVUNj2cKAg2PaTev9J5iefJmQoLLlG//cUq\nnDQWTXwHtz8F/7wXTv0E7HwG1v8UVt2iymin/lV1XW+BkQBvgbq3VMTCqde/S8Z6iO1TwG+llANC\niI+gJrx5T/JOUsr7gfsBamtr5dFcqCOgHuCE8RGEg6qXYX6kBCHieYbcfph5VrxXHw4OrjyBNkDA\nxf8LD10ErbvU+kt+CL+4IL586b3w8/MHD7e/8H9gxhnw9Wnxcti3jxeOZbliMWVOGG/PIB0y9ZzM\n8yUnQLQ7UNv3qrEvNz8XX/fOn+CR69XxBUl2+n/9P9XzN+lqgO8uUttNAWHW34u+CzWnw9emxtet\nuUs18Mn8eLXKGBrsgDM/B2d/Ib6tcRO07xt8DzPOgGt/PfT9+4uUptPfpX4n3/9I7465ziz7VT8f\nXUGdBpl0FjcC023LlcY6Cyllm5RywFj8GbAiU4Vp7zM0goniIwgHld2y97BSk00BYNcI7N+phscH\n2sCbH+8Zte4EhNIAIF7ximcb+9sqLqhKLUQ8KiJ5+3jhWJarv1O99BMxSiTTgiASVEnckteDqmv+\nwsTjUuXpCbSBwwVZuUPvm9xw+ox66i9KXJcKX5HStJGD90nl9A20gS+p3Mn4iwffh305bUGwc/iy\nZ5BMCoL1wBwhxAwhhAe4Flhr30EIUWZbvIQMTnjT0TfBTENmw965X32bDY9dIwBw+RLX2wm2G1EN\nRkXtqANfgYp9NpeFQ23Pyo9HQtgjIsxve2Ueb6mvrXIdg8Y5kPSMJhKWwBzlsgfa47mv7M/Fvr6j\nLl6fTMxle30KGHVWJCU69vhVXQ+0xa9hBkrY62nyumSG28dfrM4tDaODlPF3aDhMLSD5uQZtHZTk\ndyvVfla5jiNBIKWMALeiprbcDjwmpXxbCPFVIYRhXOOTQoi3jakwP4maDzkjtBs+gqIJoxEYDbs0\npoAwG7sj1Qj8xcr2iFDn8hcr26g3Xy37CpVd094bssdIg9oWbE+s2OOJYIYauGGv1RZvMCYKmdAI\nIiEI9aROvhZoi683656dVD1ps86mwrTFm/ub74bfVk+tdcOcI/k4+7ZoSAVPgPqOhtIQBCNoBHZT\n4nAagYyqDpnz2Ae0ZNRHIKV8Gng6ad2Xbb/vAO7IZBlMPrCqmrPmTpk4CeeSG/ZBgsCf+J1KIwi0\nQV6lilLwFaiXyN576u8a3OuHeE/O7Y1vO/iG9hHYr2E2GMkmjPGM/f8dLUzBWDwHmjbHr2H6Uma9\nR62HwQ2qz+igJCd+G7IRNzor9obSjNxJPv9QveqEfZLLYzM/ZeXaTKTvUhAERhIEtv9jDLQBmEQj\ni/P9bhaW542843hhkCBINg2loxG0Jzb0w34XJVZce4VMNg2Nt97wWAiCY3W90SQTvhTzXMnJ10xf\nirkeBjeoDqcSBoM0ZrMnAAAAG6ZJREFUgmEacXs9hMH1FFTnyHwvUp1jqPIkN+jpCoKh5iQIdMTX\nD2eWS7ifEa6VISaNIJhwJPfwLY3AiK+2BIGpEQxlGjIcXWZlHerbnrfIPuEGGKYhw3ENEIuoFLzj\nhUzZvlNeK6n3OpFIyL2fIk7/aBgkCJL8SGZSNkjtdE2OwU+uewn7Gp2VQEfiOhN7XR4Kf1Hq3/bj\nkuvTSL10TzY4sxLvIxqOT1+ZIAiGMQ2lc60MoQXBeCQaHhwvHEzWCEzT0BDO4lBARRul6vkPtWyv\nrIN6TlKFAJqMl0YwGo4LpWOuEYyTZ5Au9rKPlrPbfAZFs1BmnuTIs5LhG2h7vYvFEs2XKfdN1ghS\n9PCHa0zNbS5v/B1K3jZIEIzQSxci7mg2sdeNrob4QNCUGoHdNKQ1Ao3JUL17+zbTfj+UaShZrU1H\nIJizNKUUBMRD82D8NIKBIV6+Y3K9iWYayoBZyzxPztREM489BDm57tmxJ2wb6ErtVDYx/Vq9hxLX\nJf8eLvzSXpbkyCR/konHfg8jkZwJ2C4UEt6bpOdu+lKSy3eM0YJgPJLcqOdV2ARBmhrBIEGQ3PAn\nfftsvaHkXpn1grSOvwlxzHLYn1FGr9cefwYTLYQ00Db6/5/Z+PkKE5Mg2nvTyXXPTnKQwlD72dcH\nbf9Bqno6rGloGK0hK1+NmrcLAuFU60cieQxCQr1stf1Oeu6mL8W6H20a0pjYG3WXV01LadcIhCNu\ndz1qjWCI756DytQylFNtvM3RardRj6bte7jrFc1U/8F4eQbpEugY/f8v0KZSVrg8SY26rf4NpxGY\nDaiUIztn7Y2keR+pnMXDCYLhzFQOR1LQhOG4dqTRTKbyddjLaf7u71SJI6392pPuZ2w0grFOMXFs\n6T0Mu5+DE64ffr9QQCXLMuOJTVxelTfEV6Aq7qaHVL6R0Zbi9oRbPkO1blivEsbVv6y0AVOtNTUD\n0wbZ3QSv/QRa3okfD7be0lDfRgVsM9RY+yhQu6pdPAf2rhu6IXn7CVXWoZhzLsw8U/1u3KSEzsyz\nht5/JMxymOUKtoO7XK0LB+Gl78WT6p32qcFpOKSEV+9VvbZVH4G8Mlj/gPKHCAesuBGKZ6l933wc\nDvwTFl6mxma88C2Yex5U1g5fxoaN8PYf1O/KlbDoMji8Hbb8Oh59VToPTvyg+t3VqBKgxSIqL84p\nHx9sxjhSTOel+Zw2/RKatkDtTSr/TtnS+IjzI8Ee5eMvVvbwg2/CX7+setOe7MF1zY6/CKIDKjli\nl5FmLHkEskmqepgc1DDUdUzMgWnDjTyue0m9a3UvpT/K11+k3r1nvqiWD29PLKf9919uj4e8mgEY\nqe7nGDK5BMEbj8GzX1SNkTm6NhX1L8M/7lENvzDGHciYamxL58PSq1SlfepTakCNmRButDA1gurT\nVCNUMldlctzwc7W+6uT4vk6ParBMjeCNx+Dl76mGr2hmfJL7ilqYslCdC1RSr9L5MG2JWk72A9h7\nJjlToWSeeslnvUclz+rvTF32P39ONQ7OrMHbIkElJExBsO4elT311mEEx0gEk3pUgTaVHA/i/6PT\no+L+y0+A+RcmHt/dqOoEqHuu/RD86bMqk6XpsD/3P9X3U59W31WnqIb57T+qJHzD5aEBeOl/VF4d\nh1MJk0WXKWG98efgzlZli0Vg2fVqzMcbj6r/0Cz3osvi/+PRYu95Tl0MB16Hvc+ruvPqvbDyZjj/\nniM/r33krTne5NX7AKkEvBCqHnfUxRs/OxUrlFDd+JBazp8+tEAqnQ/ZU9QzWXyFyh5auTK+PWea\nqufTU+W2tDH3fTDj9NTbalbD1kfi79qya4Y/l8n0k2Dro/HjQCWPm3ueOp+/CBZcpBLxbUnKZpoz\nVSW6q3tRvZdjwOQSBKE+9R1oG14QmL3Mj70S7w0G2uGbM+Lb+loT9x1NzEb9rNtVwitQmkgqhFCN\nvnmM2Qh/oSmxFzl1Ify/V+PLBVXw8dfiy2Yvqi2FIHB54NbX48v269kx1ftTP6mSfiXzu5tUQ2ES\naH33z8/SCGYnLkO88bvuEfjVFSOH7tkjUi76Lqz7Rvwc0bAaQXvWF+Dkj6pPX5rlD7SpBqayFl75\nYfw5lc5X/8Fr98Of/0P5ZnJK1Ta3H674KTz6AbX8rgWBUc7sUvjYy+r3t+cpQRwLH/3/EGhTkUGg\nevKBNvW/li2DfzG0oCVXqk8qZpwBtw+VMjqJvDL4D5vj9f+9krjd5YF/+9vI57n6oaG3XfQ/6nOk\nLL06MWupnS80xH9/vm7oc9jfx2PM5PIRmD3tkSp9KlulNz/RLpzJRGfJo4dHwu2z3dsQuVpGwozx\nTqURpLxeCkEw0K16tiOF/5kE2uLpe48WcxS0qQWkiuoxtaAjEQSmkzM5Lj7VQLsRy9gWt5XHwirf\nfMJgv+RolaQcUaNRx5LzR5m/zf/7XQkCm0YQ6Vea4xjZujVHx+QSBKbtPR1BIJyq8TdxOFVjmewM\ny4ggSBo9PBL2hjk59DNdzPxDbXvU8rCCYAiNYERnX7EamGY6y/5/e+caJEd1HeDvaLUr7Qq9dqWA\nQG+xGMvmEWWL2InscgA7iDjICcSGOGWckFKZgEMgoVDKCbGd/DHlUC5iypRISIGDjY1jO6qEGBOe\noWweAsRDEMEiXhJCIAkJqfTc3ZMf9/bOnZ7uee3M9jZ9vqqp6b7d033u3Jl7+pxz77kHdpfS9zZL\n5KNO6jQP7HLKe8bxzs2XqAh8Bzmpk7JUAFFHXG2WaTOKINqP+9bDe1TUqQWjkxLl7y21d9OKIGH2\n+q6XTRHkjGIpgkYsgigFc0j4x29nArZ4YrladPaU163Z4HVPnwvcQfWgVWd3KTgdUnP4Xy+gLr4w\ndKQ1E8GiTjayaOLzCkaT6vVRNiM1/Dw433k4AzRVEcRnXNewaEaGS8Nxw449/iQd3iNJcYyVRPmD\n9m5G2QwddoH4uEIbPpxZ0NNojoIpguCpuRppT9Vl46RjLqJWEp8rUIvJU2MWQZN/wujP2zW9+pKP\naa6hWhNwwo7tYIILpxmiJ9KOTjfeO24RhKk0Ul1Dfo2GMkXQm9zecYuglkVzaK87J7oeOB/6wd2V\nI7rCe3X3Jidla5boGkmjbMLjDV0zloKhO6ZkjNxQUEVQo/M+kDLNvaev5CuOWwatJJJz8tT6zg9d\nNfXkT0+jnin64IbfJWY7rZGbJZzC36oZwWFnH6XLDo+FdUpzDXXPcov3RE/qMsm5yaKZrMNDydZO\nPa6bpIlV775WHkuJ5A9z1vf0BUnZWuEaerc8o2y8Ls3MwUibqwKtH1JttJWCKYIgoFqNtKfqpJz9\n0WSYVtKoRRAFi4eHnA9+zIqgxudrWgQpn09aZSr8XDPEfdTxUUNpx8J7R26Yg7vdur6hOylyZVV7\noq4mf2gljQ7RfbEkE7ix7Z09Tt7ho+npwcdC0m863k6NPtRUVQRmEeSJgimCBlxDST7O6E+pWlIm\n4TKSrZRTOupfoCLqmA/tAXQMiqCOKfpQPVg8abKbaZp4/ViwNPxcMwwddkM60zrNg7vL65TU0YWK\nQEfccMqk0TzxNRri9Ukj7Cyn+hQGcUUQyh4fndRSRRBr1/h+o/eJK4LIlZV0bWNCUzBFUEewOBrj\nnTYdPlqQpJ3Jx44eLJ89XIsoWJz01NoI9bqGOtNcQ/57S5O71Yog7orqCRKYjbZj6DaKTe+PrtEd\n+O93vpTsu096OEjLQ192/aBN4uvqlqVQnl3+vYR+91aNGkqTPy5rI9cMrxO5ssBcQzmjrYpARM4R\nkc0iMigia6ucd76IqIjUmKs/RkI/ehqHqmRArBj1Mae032o56x0xBL5jPlTbR1+Lui2CKq6hakqo\nq8fFPcJlL9Oe1OvhYFwR9JW+g/gyg6GbJ36NcITOe9sSRvPsLrcuIqLj1eRPWv9535vlcoeyx2MR\nY/l+4nJUWATR99bk7zhtbkX4buSCtikCEekAbgRWAcuBi0RkecJ504ErgPZPq6vHNZQ08SZitGPw\noz7alYCtYUXQU0ofHcrZKHVbBCmuoWq55MN7RB1e1zEw/fjmFWmFj7o3SKWd0KFC+b1CqyFpwZKK\ncf+xuiUtSJIkY8cUd254zaTtMosgFuQeaxwqaQnIaL/ahLuq19xVucZuPABu5IJ2ppg4AxhU1S0A\nInIHsBp4Pnbe3wPfAK5uoyyOqPM6tNcF5cIf8MgwvP5IsP5pSowA4Jk73dPmnH54/Zew+S6XPGpS\nh8tjMjWWtnbbE84dsPS3XC71HZtKxxb/pkv3AG4izhuPOR9yQ4pgqnsCfunucjkbpRXB4qhTSb1H\nr0tKFiUj6+l19d34/cblja+FG70/cWtpjkJcuT17ZymXzcjR0uI9SR101JlteQD2vAFLl5XfP1qQ\nZOsT6fJve7LcXTb6u5LyWEpPn/sNxduwp8/J+NR33aS3ZtCRyoyy4T36lsHrv4BXHnLKuV7e3Jhs\nJcVjKcaEp52K4ATgjWB/K1CWDUpEVgALVPW/RCRVEYjIGmANwMKFC5uX6OhB93Q2fNgpgzDf0Mv3\nwe0XwNlfc/tJneGshS4Y+uh33P6ilU4pPP7P7gVw9ldh5ZXln7v9s86KGPgTePl+F5CMWL4aPnub\n2/7PK+GVB932srPqr9fM+c6d9eRtbg7AtLn1fzZk9hL3/cw9ufp5nT3uOxwZdsovop5Zzb3L4Pmf\nuu2ln3Cd8isPwk+/1JzMHVNK6SV6fUf9s2tKx2cv8e+LAYGHrkuQaakbPto5zVkUkaLo6nEWS5Q5\nNCkZWt8ylyzstYfTZVy0MriXl3FOf3kspW+Zu/dT/+YURNhJA6z/cvr166V3Sfl+1zSYMd8lHpy1\nCF5Y716NEP+dzv1A+cIxRi7ILOmciEwCrge+WOtcVV0HrAMYGBhozkZWdQHOGcc7P3DFQi7eZbDj\nOfeeZBFMPw7+8kX3dNXRBTNPgP5PliYU3fQx2Bf7EwwfZXRhin073J9kxRdg5VVw5xfLJyPt3wEn\nng3nftOlH66XgUtcRtWRYRd0bPZpbMY8WPt67c+HayBEaZ1HRpLdD3F+/2anLMHVcdJkl6SuWabO\nLM0qXvIxuOr/SqlEuqa5Dh5cJ371oMvzExK1I8BVm+Dw/vIEb5c96ucWCMxMeAj5/J1uDYdqTD+u\ntH3W37nUzz2xpIdnrIEPrCq1YWSZfvB34crnnQU6Fjo6KxPXicCfP+Xa4PQ/LCVSbIT47/TMv3UW\niJEr2qkItgELgv35vixiOvBh4AFxT0bHAetF5DxV3dByaYYOA+rM/fe2Vbo2IsVQK+natD73igj9\ny0mTlg4GaQ3e2+ruM3uxezrrnlUux4FdLsV0/MmtFiIl99JYqUeJJCmC0WUGa01G66qsX6P1rcaM\nKgp02pzqWWe7Z1cusD51hnul0dndmPyTJnnrJEa1NowUVTuIZpBPme5eY2VSB9BR8zRjYtHOUUOP\nA/0iskREuoALgVG7U1X3quocVV2sqouBR4D2KAEodfRRRxW3CKIOeddg9bHw1Uga4REGJ+MJ3cIc\nQdHchDyMtkhaHrPehb4Nw5hwtE0RqOoQcDlwN/AC8ENV3SQiXxeR89p131Sijj7qqNIsgiP7S2O+\nGyVp8k+037ustOLZqCLoLg9gV1u4eyKRtDymKQLDyC1tjRGo6l3AXbGya1PO/UQ7ZSkpghoWAYxh\n1E0v7NxcXjaa3fIk2B23CLorh7TmYdhdfHlMqJwIZRhGbijOzOJR11CKRRCuEzyW4ZfxMfGjiuDE\nyutPDhTB6OScHDxRJ1oEY5zDYBhGZhRIEcRdQ7FMi6GFMJaZuUf2+8C0J+og+/pLZdFTf5JFkIeO\nNLIIymIEObJoDMMooziKYCiuCKq5hsaYqye0Cg6+6zrOcORHNDKls8fJNTKSL9dKkkVwcLeb8NSK\nkSeGYYwrxVEEFTGClGAxNP9UnpSELL7a1NRZ0OFDM1GHOnQoZ4ogsghirqFqCecMw5iwFEgR+I6+\nu53B4oS0xKO5bBLSN4Qd6oHdzQ9bHW/Sho/mwa1lGEYFBVIEvqOPcqZXWATtVAR9QWrjUBEEHWqe\nnqjTgsV5sGYMw6ggsxQT487ogvDTyidyjR4P9pvO5+8/99azpRwx+3a4fDdRpsqws4w61P1vw6sP\n5yfQGlkyu7fA9qfd9r7tMO+07GQyDKNpCqQIouUfu10ahSSLYMpMlyohnpOlXrp7XWf/8PXuFTFj\nnk8hsKD82lGH+r0/cE/Uy85s7r7jTUeXU6iPrXOviJNWZSeTYRhNUxxFcPKnXeKxzp7kfPpHD8BJ\nn3IJ0I6tWDahPiZ3wZ/eA3u3BoXiUk0D/NGPy/PWRHl9IrfQed9u7r7jjQhccjfseT0shEUfzUwk\nwzCapziKoG9ZyV2TtNRitDzkvFPHdp95p6W7SGYvKt8PF6f/8PntTS7Wao47xb0Mw8g9xQkWhyQt\nrBIpgvGWIyIv8QHDMN53FFQR9JTnyYHGl4dslRwRNvTSMIyMKKgiiFkEw0fdsoXjrgiC+9nQS8Mw\nMqKgiiAWLB4dWmoWgWEYxaOYimDy1PJgcWaKILQITBEYhpENxVQEcdfQ6ByDcQ4WTw6WhTRFYBhG\nRhRUEfRMDItAxK1JABYjMAwjM4ozjyCkwiKIFME4WwSRLCLjr4QMwzA8bbUIROQcEdksIoMisjbh\n+JdE5FkR2SgiD4tIk1N6G6SzB4aPwPCQ2w/TT4w3nT3mFjIMI1PapghEpAO4EVgFLAcuSujov6eq\np6jq6cB1wPWMB6PrAHhLIGuLwNxChmFkSDstgjOAQVXdoqpHgDuA1eEJqvpesDsN0DbKUyJSBFse\ngG+dAvvfcvth8Ha8mDIdps0d//sahmF42hkjOAF4I9jfCvx6/CQRuQy4CugCEtNvisgaYA3AwoUL\nxy5ZtFTky/e7xGlvbiwvH0/O/WYp+ZxhGEYGZD5qSFVvVNVlwDXA36Scs05VB1R1YO7cFjw9Rx3+\nzhfL37Nw0cz/NTj2Q+N/X8MwDE87FcE2YEGwP9+XpXEH8Jk2ylMiCs7uGiy9d/bYyB3DMApJOxXB\n40C/iCwRkS7gQmB9eIKI9Ae7vwO81EZ5SkSKYN/20ruN3DEMo6C0LUagqkMicjlwN9AB3KKqm0Tk\n68AGVV0PXC4iZwNHgXeBi9slTxlJnb6N3DEMo6C0dUKZqt4F3BUruzbYvqKd90+lq8fN6B1qwYL1\nhmEYOSfzYHFmxC0AWxjGMIyCYopgdN8sAsMwikmBFUFf9X3DMIyCYIpgkg+TWLDYMIyCUlxFEMUE\nepe6d1MEhmEUlOIqgsgi6Osv3zcMwygYpgjmnOjebdSQYRgFpZgL0wB88NOwfwecsQYQ+JXxWQrB\nMAxjoiGq45P5uVUMDAzohg0bshbDMAwjV4jIE6o6kHSsuK4hwzAMAzBFYBiGUXhMERiGYRQcUwSG\nYRgFxxSBYRhGwTFFYBiGUXBMERiGYRQcUwSGYRgFJ3cTykTkHeC1Jj8+B9jZQnGyxOoyMbG6TEys\nLrBIVecmHcidIhgLIrIhbWZd3rC6TEysLhMTq0t1zDVkGIZRcEwRGIZhFJyiKYJ1WQvQQqwuExOr\ny8TE6lKFQsUIDMMwjEqKZhEYhmEYMUwRGIZhFJzCKAIROUdENovIoIiszVqeRhGRV0XkWRHZKCIb\nfFmviNwjIi/599lZy5mEiNwiIm+LyHNBWaLs4rjBt9MzIrIiO8krSanLV0Vkm2+bjSJybnDsr31d\nNovIb2cjdSUiskBE7heR50Vkk4hc4ctz1y5V6pLHdpkqIo+JyNO+Ll/z5UtE5FEv8w9EpMuXT/H7\ng/744qZurKrv+xfQAbwMLAW6gKeB5VnL1WAdXgXmxMquA9b67bXAN7KWM0X2jwMrgOdqyQ6cC/w3\nIMBHgEezlr+OunwV+KuEc5f739oUYIn/DXZkXQcv2zxghd+eDrzo5c1du1SpSx7bRYBj/HYn8Kj/\nvn8IXOjLbwIu9dt/Btzkty8EftDMfYtiEZwBDKrqFlU9AtwBrM5YplawGrjVb98KfCZDWVJR1YeA\n3bHiNNlXA7ep4xFglojMGx9Ja5NSlzRWA3eo6mFVfQUYxP0WM0dVt6vqk357H/ACcAI5bJcqdUlj\nIreLqup+v9vpXwqcCfzIl8fbJWqvHwFniYg0et+iKIITgDeC/a1U/6FMRBT4uYg8ISJrfNmxqrrd\nb78FHJuNaE2RJnte2+py7zK5JXDR5aIu3p3wq7inz1y3S6wukMN2EZEOEdkIvA3cg7NY9qjqkD8l\nlHe0Lv74XqCv0XsWRRG8H1ipqiuAVcBlIvLx8KA62zCXY4HzLLvnO8Ay4HRgO/CP2YpTPyJyDPDv\nwF+o6nvhsby1S0JdctkuqjqsqqcD83GWysntvmdRFME2YEGwP9+X5QZV3ebf3wZ+gvuB7IjMc//+\ndnYSNkya7LlrK1Xd4f+8I8DNlNwME7ouItKJ6zhvV9Uf++JctktSXfLaLhGquge4H/gozhU32R8K\n5R2tiz8+E9jV6L2KoggeB/p95L0LF1RZn7FMdSMi00RkerQNfAp4DleHi/1pFwP/kY2ETZEm+3rg\nC36UykeAvYGrYkIS85X/Hq5twNXlQj+yYwnQDzw23vIl4f3I/wK8oKrXB4dy1y5pdclpu8wVkVl+\nuxv4JC7mcT9wgT8t3i5Re10A3OctucbIOko+Xi/cqIcXcf62r2QtT4OyL8WNcnga2BTJj/MF3gu8\nBPwP0Ju1rCnyfx9nmh/F+TcvSZMdN2riRt9OzwIDWctfR12+62V9xv8x5wXnf8XXZTOwKmv5A7lW\n4tw+zwAb/evcPLZLlbrksV1OBZ7yMj8HXOvLl+KU1SBwJzDFl0/1+4P++NJm7mspJgzDMApOUVxD\nhmEYRgqmCAzDMAqOKQLDMIyCY4rAMAyj4JgiMAzDKDimCAwjhogMBxkrN0oLs9WKyOIwc6lhTAQm\n1z7FMArHQXVT/A2jEJhFYBh1Im5NiOvErQvxmIic6MsXi8h9PrnZvSKy0JcfKyI/8bnlnxaR3/CX\n6hCRm32++Z/7GaSGkRmmCAyjku6Ya+hzwbG9qnoK8G3gW77sn4BbVfVU4HbgBl9+A/Cgqp6GW8Ng\nky/vB25U1Q8Be4Dz21wfw6iKzSw2jBgisl9Vj0kofxU4U1W3+CRnb6lqn4jsxKUvOOrLt6vqHBF5\nB5ivqoeDaywG7lHVfr9/DdCpqv/Q/poZRjJmERhGY2jKdiMcDraHsVidkTGmCAyjMT4XvP/Sb/8C\nl9EW4PPA//rte4FLYXSxkZnjJaRhNII9iRhGJd1+haiIn6lqNIR0tog8g3uqv8iXfRn4VxG5GngH\n+GNffgWwTkQuwT35X4rLXGoYEwqLERhGnfgYwYCq7sxaFsNoJeYaMgzDKDhmERiGYRQcswgMwzAK\njikCwzCMgmOKwDAMo+CYIjAMwyg4pggMwzAKzv8DAfIXVd7Ie+8AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "# Evaluate on test data\n",
            "40/40 [==============================] - 0s 2ms/sample - loss: 0.7832 - acc: 0.4750\n",
            "test loss, test acc: [0.7831976838409901, 0.475]\n",
            "EEG_Deep/Data2A/parsed_P010T.mat\n",
            "Filtering of Training Data Finished\n",
            "EEG_Deep/Data2A/parsed_P010E.mat\n",
            "Filtering of Testing Data Finished\n",
            "(1, 2)\n",
            "Finding labels in training data\n",
            "Finding labels in testing data\n",
            "[2 2 1 2 1 2 1 1 1 1 1 1 1 1 2 1 1 2 2 1]\n",
            "(60, 12, 1536)\n",
            "(60,)\n",
            "(20, 12, 1536)\n",
            "(20,)\n",
            "[[0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]]\n",
            "X_train shape: (60, 1, 12, 1536)\n",
            "60 train samples\n",
            "20 val samples\n",
            "(40, 12, 1536)\n",
            "(40, 1)\n",
            "X_train shape: (40, 1, 12, 1536)\n",
            "40 train samples\n",
            "Train on 60 samples, validate on 20 samples\n",
            "Epoch 1/300\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.69166, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 1s - loss: 0.7278 - acc: 0.4833 - val_loss: 0.6917 - val_acc: 0.5000\n",
            "Epoch 2/300\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.69166 to 0.69127, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6588 - acc: 0.5333 - val_loss: 0.6913 - val_acc: 0.5000\n",
            "Epoch 3/300\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.69127\n",
            "60/60 - 0s - loss: 0.6175 - acc: 0.6167 - val_loss: 0.6913 - val_acc: 0.3500\n",
            "Epoch 4/300\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.69127 to 0.68933, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5760 - acc: 0.6167 - val_loss: 0.6893 - val_acc: 0.3500\n",
            "Epoch 5/300\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.68933 to 0.68696, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5680 - acc: 0.6333 - val_loss: 0.6870 - val_acc: 0.3500\n",
            "Epoch 6/300\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.68696 to 0.68364, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5447 - acc: 0.6833 - val_loss: 0.6836 - val_acc: 0.3500\n",
            "Epoch 7/300\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.68364 to 0.68134, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5026 - acc: 0.8000 - val_loss: 0.6813 - val_acc: 0.4000\n",
            "Epoch 8/300\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.68134 to 0.68000, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4747 - acc: 0.9000 - val_loss: 0.6800 - val_acc: 0.4000\n",
            "Epoch 9/300\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.68000 to 0.67922, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4709 - acc: 0.9167 - val_loss: 0.6792 - val_acc: 0.4000\n",
            "Epoch 10/300\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.67922 to 0.67889, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4216 - acc: 0.9333 - val_loss: 0.6789 - val_acc: 0.4000\n",
            "Epoch 11/300\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.67889\n",
            "60/60 - 0s - loss: 0.3952 - acc: 0.9333 - val_loss: 0.6809 - val_acc: 0.4000\n",
            "Epoch 12/300\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.67889\n",
            "60/60 - 0s - loss: 0.3862 - acc: 0.9167 - val_loss: 0.6852 - val_acc: 0.4000\n",
            "Epoch 13/300\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.67889\n",
            "60/60 - 0s - loss: 0.3918 - acc: 0.9000 - val_loss: 0.6856 - val_acc: 0.4500\n",
            "Epoch 14/300\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.67889\n",
            "60/60 - 0s - loss: 0.3498 - acc: 0.9333 - val_loss: 0.6812 - val_acc: 0.5000\n",
            "Epoch 15/300\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.67889\n",
            "60/60 - 0s - loss: 0.3162 - acc: 1.0000 - val_loss: 0.6790 - val_acc: 0.5000\n",
            "Epoch 16/300\n",
            "\n",
            "Epoch 00016: val_loss improved from 0.67889 to 0.67509, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3746 - acc: 0.9000 - val_loss: 0.6751 - val_acc: 0.5500\n",
            "Epoch 17/300\n",
            "\n",
            "Epoch 00017: val_loss improved from 0.67509 to 0.66790, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3722 - acc: 0.8667 - val_loss: 0.6679 - val_acc: 0.6500\n",
            "Epoch 18/300\n",
            "\n",
            "Epoch 00018: val_loss improved from 0.66790 to 0.66341, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3320 - acc: 0.9000 - val_loss: 0.6634 - val_acc: 0.6500\n",
            "Epoch 19/300\n",
            "\n",
            "Epoch 00019: val_loss improved from 0.66341 to 0.65821, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3337 - acc: 0.9333 - val_loss: 0.6582 - val_acc: 0.6500\n",
            "Epoch 20/300\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.65821\n",
            "60/60 - 0s - loss: 0.3589 - acc: 0.9000 - val_loss: 0.6607 - val_acc: 0.6500\n",
            "Epoch 21/300\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.65821\n",
            "60/60 - 0s - loss: 0.3193 - acc: 0.8833 - val_loss: 0.6719 - val_acc: 0.6500\n",
            "Epoch 22/300\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.65821\n",
            "60/60 - 0s - loss: 0.3185 - acc: 0.9667 - val_loss: 0.6780 - val_acc: 0.6500\n",
            "Epoch 23/300\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.65821\n",
            "60/60 - 0s - loss: 0.3278 - acc: 0.8833 - val_loss: 0.6730 - val_acc: 0.7000\n",
            "Epoch 24/300\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.65821\n",
            "60/60 - 0s - loss: 0.2962 - acc: 0.9500 - val_loss: 0.6728 - val_acc: 0.7000\n",
            "Epoch 25/300\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.65821\n",
            "60/60 - 0s - loss: 0.2904 - acc: 0.9333 - val_loss: 0.6641 - val_acc: 0.7000\n",
            "Epoch 26/300\n",
            "\n",
            "Epoch 00026: val_loss improved from 0.65821 to 0.64897, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2950 - acc: 0.9667 - val_loss: 0.6490 - val_acc: 0.7000\n",
            "Epoch 27/300\n",
            "\n",
            "Epoch 00027: val_loss improved from 0.64897 to 0.64382, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2835 - acc: 0.9667 - val_loss: 0.6438 - val_acc: 0.7000\n",
            "Epoch 28/300\n",
            "\n",
            "Epoch 00028: val_loss improved from 0.64382 to 0.62864, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3083 - acc: 0.9000 - val_loss: 0.6286 - val_acc: 0.7500\n",
            "Epoch 29/300\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.62864\n",
            "60/60 - 0s - loss: 0.3060 - acc: 0.9333 - val_loss: 0.6302 - val_acc: 0.8000\n",
            "Epoch 30/300\n",
            "\n",
            "Epoch 00030: val_loss improved from 0.62864 to 0.62143, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2857 - acc: 0.9000 - val_loss: 0.6214 - val_acc: 0.8000\n",
            "Epoch 31/300\n",
            "\n",
            "Epoch 00031: val_loss improved from 0.62143 to 0.60579, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3055 - acc: 0.9167 - val_loss: 0.6058 - val_acc: 0.8000\n",
            "Epoch 32/300\n",
            "\n",
            "Epoch 00032: val_loss improved from 0.60579 to 0.58938, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2543 - acc: 0.9500 - val_loss: 0.5894 - val_acc: 0.8500\n",
            "Epoch 33/300\n",
            "\n",
            "Epoch 00033: val_loss improved from 0.58938 to 0.57833, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2971 - acc: 0.8833 - val_loss: 0.5783 - val_acc: 0.9000\n",
            "Epoch 34/300\n",
            "\n",
            "Epoch 00034: val_loss improved from 0.57833 to 0.56041, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2995 - acc: 0.8833 - val_loss: 0.5604 - val_acc: 0.8500\n",
            "Epoch 35/300\n",
            "\n",
            "Epoch 00035: val_loss improved from 0.56041 to 0.55191, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2510 - acc: 0.9500 - val_loss: 0.5519 - val_acc: 0.8000\n",
            "Epoch 36/300\n",
            "\n",
            "Epoch 00036: val_loss improved from 0.55191 to 0.54551, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2557 - acc: 0.9667 - val_loss: 0.5455 - val_acc: 0.7500\n",
            "Epoch 37/300\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.54551\n",
            "60/60 - 0s - loss: 0.2428 - acc: 0.9500 - val_loss: 0.5459 - val_acc: 0.7500\n",
            "Epoch 38/300\n",
            "\n",
            "Epoch 00038: val_loss improved from 0.54551 to 0.54138, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2739 - acc: 0.9333 - val_loss: 0.5414 - val_acc: 0.7500\n",
            "Epoch 39/300\n",
            "\n",
            "Epoch 00039: val_loss improved from 0.54138 to 0.53935, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2296 - acc: 0.9333 - val_loss: 0.5394 - val_acc: 0.7500\n",
            "Epoch 40/300\n",
            "\n",
            "Epoch 00040: val_loss improved from 0.53935 to 0.53732, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2260 - acc: 0.9500 - val_loss: 0.5373 - val_acc: 0.7500\n",
            "Epoch 41/300\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.53732\n",
            "60/60 - 0s - loss: 0.2804 - acc: 0.9000 - val_loss: 0.5411 - val_acc: 0.7500\n",
            "Epoch 42/300\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.53732\n",
            "60/60 - 0s - loss: 0.2883 - acc: 0.9000 - val_loss: 0.5482 - val_acc: 0.7500\n",
            "Epoch 43/300\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.53732\n",
            "60/60 - 0s - loss: 0.3101 - acc: 0.8833 - val_loss: 0.5496 - val_acc: 0.7500\n",
            "Epoch 44/300\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.53732\n",
            "60/60 - 0s - loss: 0.2453 - acc: 0.9500 - val_loss: 0.5497 - val_acc: 0.7500\n",
            "Epoch 45/300\n",
            "\n",
            "Epoch 00045: val_loss improved from 0.53732 to 0.53708, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2244 - acc: 0.9833 - val_loss: 0.5371 - val_acc: 0.7500\n",
            "Epoch 46/300\n",
            "\n",
            "Epoch 00046: val_loss improved from 0.53708 to 0.52925, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2450 - acc: 0.9667 - val_loss: 0.5292 - val_acc: 0.7500\n",
            "Epoch 47/300\n",
            "\n",
            "Epoch 00047: val_loss improved from 0.52925 to 0.52594, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2045 - acc: 0.9833 - val_loss: 0.5259 - val_acc: 0.7500\n",
            "Epoch 48/300\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.52594\n",
            "60/60 - 0s - loss: 0.2524 - acc: 0.9333 - val_loss: 0.5296 - val_acc: 0.7500\n",
            "Epoch 49/300\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.52594\n",
            "60/60 - 0s - loss: 0.2620 - acc: 0.9500 - val_loss: 0.5456 - val_acc: 0.7500\n",
            "Epoch 50/300\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.52594\n",
            "60/60 - 0s - loss: 0.2277 - acc: 0.9500 - val_loss: 0.5485 - val_acc: 0.7500\n",
            "Epoch 51/300\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.52594\n",
            "60/60 - 0s - loss: 0.2529 - acc: 0.9667 - val_loss: 0.5523 - val_acc: 0.7500\n",
            "Epoch 52/300\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.52594\n",
            "60/60 - 0s - loss: 0.2211 - acc: 0.9667 - val_loss: 0.5488 - val_acc: 0.7000\n",
            "Epoch 53/300\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.52594\n",
            "60/60 - 0s - loss: 0.2456 - acc: 0.9333 - val_loss: 0.5463 - val_acc: 0.7000\n",
            "Epoch 54/300\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.52594\n",
            "60/60 - 0s - loss: 0.2450 - acc: 0.9333 - val_loss: 0.5554 - val_acc: 0.7000\n",
            "Epoch 55/300\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.52594\n",
            "60/60 - 0s - loss: 0.2142 - acc: 0.9667 - val_loss: 0.5629 - val_acc: 0.7000\n",
            "Epoch 56/300\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.52594\n",
            "60/60 - 0s - loss: 0.2299 - acc: 0.9500 - val_loss: 0.5708 - val_acc: 0.7000\n",
            "Epoch 57/300\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.52594\n",
            "60/60 - 0s - loss: 0.2230 - acc: 0.9667 - val_loss: 0.5719 - val_acc: 0.7000\n",
            "Epoch 58/300\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.52594\n",
            "60/60 - 0s - loss: 0.2151 - acc: 0.9833 - val_loss: 0.5681 - val_acc: 0.7000\n",
            "Epoch 59/300\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.52594\n",
            "60/60 - 0s - loss: 0.2198 - acc: 0.9667 - val_loss: 0.5649 - val_acc: 0.7500\n",
            "Epoch 60/300\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.52594\n",
            "60/60 - 0s - loss: 0.2235 - acc: 0.9667 - val_loss: 0.5677 - val_acc: 0.7500\n",
            "Epoch 61/300\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.52594\n",
            "60/60 - 0s - loss: 0.1946 - acc: 0.9833 - val_loss: 0.5669 - val_acc: 0.7500\n",
            "Epoch 62/300\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.52594\n",
            "60/60 - 0s - loss: 0.1836 - acc: 0.9833 - val_loss: 0.5678 - val_acc: 0.7500\n",
            "Epoch 63/300\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.52594\n",
            "60/60 - 0s - loss: 0.2980 - acc: 0.9167 - val_loss: 0.5702 - val_acc: 0.7500\n",
            "Epoch 64/300\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.52594\n",
            "60/60 - 0s - loss: 0.1901 - acc: 0.9833 - val_loss: 0.5634 - val_acc: 0.7500\n",
            "Epoch 65/300\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.52594\n",
            "60/60 - 0s - loss: 0.1925 - acc: 0.9667 - val_loss: 0.5576 - val_acc: 0.7500\n",
            "Epoch 66/300\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.52594\n",
            "60/60 - 0s - loss: 0.2073 - acc: 0.9667 - val_loss: 0.5562 - val_acc: 0.7500\n",
            "Epoch 67/300\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.52594\n",
            "60/60 - 0s - loss: 0.2037 - acc: 0.9333 - val_loss: 0.5526 - val_acc: 0.7500\n",
            "Epoch 68/300\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.52594\n",
            "60/60 - 0s - loss: 0.1885 - acc: 0.9667 - val_loss: 0.5593 - val_acc: 0.7500\n",
            "Epoch 69/300\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.52594\n",
            "60/60 - 0s - loss: 0.1992 - acc: 0.9667 - val_loss: 0.5684 - val_acc: 0.7500\n",
            "Epoch 70/300\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.52594\n",
            "60/60 - 0s - loss: 0.1628 - acc: 0.9833 - val_loss: 0.5748 - val_acc: 0.7500\n",
            "Epoch 71/300\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.52594\n",
            "60/60 - 0s - loss: 0.2101 - acc: 0.9500 - val_loss: 0.5716 - val_acc: 0.7500\n",
            "Epoch 72/300\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.52594\n",
            "60/60 - 0s - loss: 0.2543 - acc: 0.9167 - val_loss: 0.5528 - val_acc: 0.7500\n",
            "Epoch 73/300\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.52594\n",
            "60/60 - 0s - loss: 0.1898 - acc: 0.9667 - val_loss: 0.5484 - val_acc: 0.7500\n",
            "Epoch 74/300\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.52594\n",
            "60/60 - 0s - loss: 0.2230 - acc: 0.9333 - val_loss: 0.5592 - val_acc: 0.7500\n",
            "Epoch 75/300\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.52594\n",
            "60/60 - 0s - loss: 0.1765 - acc: 1.0000 - val_loss: 0.5726 - val_acc: 0.7000\n",
            "Epoch 76/300\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.52594\n",
            "60/60 - 0s - loss: 0.2243 - acc: 0.9500 - val_loss: 0.5696 - val_acc: 0.7000\n",
            "Epoch 77/300\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.52594\n",
            "60/60 - 0s - loss: 0.1739 - acc: 1.0000 - val_loss: 0.5804 - val_acc: 0.7000\n",
            "Epoch 78/300\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.52594\n",
            "60/60 - 0s - loss: 0.1866 - acc: 0.9833 - val_loss: 0.5825 - val_acc: 0.7000\n",
            "Epoch 79/300\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.52594\n",
            "60/60 - 0s - loss: 0.1812 - acc: 1.0000 - val_loss: 0.5853 - val_acc: 0.7000\n",
            "Epoch 80/300\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.52594\n",
            "60/60 - 0s - loss: 0.1853 - acc: 0.9833 - val_loss: 0.5870 - val_acc: 0.7000\n",
            "Epoch 81/300\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.52594\n",
            "60/60 - 0s - loss: 0.2570 - acc: 0.9000 - val_loss: 0.5847 - val_acc: 0.7000\n",
            "Epoch 82/300\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.52594\n",
            "60/60 - 0s - loss: 0.1626 - acc: 0.9667 - val_loss: 0.5846 - val_acc: 0.7000\n",
            "Epoch 83/300\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.52594\n",
            "60/60 - 0s - loss: 0.1916 - acc: 0.9667 - val_loss: 0.5861 - val_acc: 0.7000\n",
            "Epoch 84/300\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.52594\n",
            "60/60 - 0s - loss: 0.2114 - acc: 0.9500 - val_loss: 0.5830 - val_acc: 0.7000\n",
            "Epoch 85/300\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.52594\n",
            "60/60 - 0s - loss: 0.2129 - acc: 0.9500 - val_loss: 0.5871 - val_acc: 0.7000\n",
            "Epoch 86/300\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.52594\n",
            "60/60 - 0s - loss: 0.1662 - acc: 0.9667 - val_loss: 0.5889 - val_acc: 0.7500\n",
            "Epoch 87/300\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.52594\n",
            "60/60 - 0s - loss: 0.2292 - acc: 0.9667 - val_loss: 0.5962 - val_acc: 0.7500\n",
            "Epoch 88/300\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.52594\n",
            "60/60 - 0s - loss: 0.2055 - acc: 0.9500 - val_loss: 0.5951 - val_acc: 0.7500\n",
            "Epoch 89/300\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.52594\n",
            "60/60 - 0s - loss: 0.1622 - acc: 1.0000 - val_loss: 0.5939 - val_acc: 0.7500\n",
            "Epoch 90/300\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.52594\n",
            "60/60 - 0s - loss: 0.2232 - acc: 0.9667 - val_loss: 0.5886 - val_acc: 0.7500\n",
            "Epoch 91/300\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.52594\n",
            "60/60 - 0s - loss: 0.2400 - acc: 0.8833 - val_loss: 0.5843 - val_acc: 0.7500\n",
            "Epoch 92/300\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.52594\n",
            "60/60 - 0s - loss: 0.2356 - acc: 0.9167 - val_loss: 0.5741 - val_acc: 0.7500\n",
            "Epoch 93/300\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.52594\n",
            "60/60 - 0s - loss: 0.1852 - acc: 0.9833 - val_loss: 0.5591 - val_acc: 0.7500\n",
            "Epoch 94/300\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.52594\n",
            "60/60 - 0s - loss: 0.2133 - acc: 0.9500 - val_loss: 0.5409 - val_acc: 0.7500\n",
            "Epoch 95/300\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.52594\n",
            "60/60 - 0s - loss: 0.1743 - acc: 0.9833 - val_loss: 0.5303 - val_acc: 0.7500\n",
            "Epoch 96/300\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.52594\n",
            "60/60 - 0s - loss: 0.2544 - acc: 0.9333 - val_loss: 0.5324 - val_acc: 0.7500\n",
            "Epoch 97/300\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.52594\n",
            "60/60 - 0s - loss: 0.1385 - acc: 0.9833 - val_loss: 0.5377 - val_acc: 0.7500\n",
            "Epoch 98/300\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.52594\n",
            "60/60 - 0s - loss: 0.1624 - acc: 0.9833 - val_loss: 0.5551 - val_acc: 0.7500\n",
            "Epoch 99/300\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.52594\n",
            "60/60 - 0s - loss: 0.1595 - acc: 1.0000 - val_loss: 0.5709 - val_acc: 0.7500\n",
            "Epoch 100/300\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.52594\n",
            "60/60 - 0s - loss: 0.1731 - acc: 0.9833 - val_loss: 0.5871 - val_acc: 0.7500\n",
            "Epoch 101/300\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.52594\n",
            "60/60 - 0s - loss: 0.1498 - acc: 0.9500 - val_loss: 0.5959 - val_acc: 0.7500\n",
            "Epoch 102/300\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.52594\n",
            "60/60 - 0s - loss: 0.1752 - acc: 0.9667 - val_loss: 0.5952 - val_acc: 0.7500\n",
            "Epoch 103/300\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.52594\n",
            "60/60 - 0s - loss: 0.1627 - acc: 0.9833 - val_loss: 0.5920 - val_acc: 0.7500\n",
            "Epoch 104/300\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.52594\n",
            "60/60 - 0s - loss: 0.1900 - acc: 0.9500 - val_loss: 0.5804 - val_acc: 0.7500\n",
            "Epoch 105/300\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 0.52594\n",
            "60/60 - 0s - loss: 0.1294 - acc: 1.0000 - val_loss: 0.5675 - val_acc: 0.7500\n",
            "Epoch 106/300\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 0.52594\n",
            "60/60 - 0s - loss: 0.1703 - acc: 0.9667 - val_loss: 0.5565 - val_acc: 0.7500\n",
            "Epoch 107/300\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.52594\n",
            "60/60 - 0s - loss: 0.1805 - acc: 0.9500 - val_loss: 0.5583 - val_acc: 0.7500\n",
            "Epoch 108/300\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 0.52594\n",
            "60/60 - 0s - loss: 0.2014 - acc: 0.9333 - val_loss: 0.5573 - val_acc: 0.7500\n",
            "Epoch 109/300\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.52594\n",
            "60/60 - 0s - loss: 0.2203 - acc: 0.9000 - val_loss: 0.5633 - val_acc: 0.7500\n",
            "Epoch 110/300\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.52594\n",
            "60/60 - 0s - loss: 0.1645 - acc: 0.9833 - val_loss: 0.5715 - val_acc: 0.7500\n",
            "Epoch 111/300\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 0.52594\n",
            "60/60 - 0s - loss: 0.1547 - acc: 0.9833 - val_loss: 0.5779 - val_acc: 0.7500\n",
            "Epoch 112/300\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 0.52594\n",
            "60/60 - 0s - loss: 0.1527 - acc: 0.9833 - val_loss: 0.5822 - val_acc: 0.7500\n",
            "Epoch 113/300\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.52594\n",
            "60/60 - 0s - loss: 0.1586 - acc: 0.9667 - val_loss: 0.5751 - val_acc: 0.7500\n",
            "Epoch 114/300\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.52594\n",
            "60/60 - 0s - loss: 0.1560 - acc: 0.9833 - val_loss: 0.5612 - val_acc: 0.7500\n",
            "Epoch 115/300\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.52594\n",
            "60/60 - 0s - loss: 0.1590 - acc: 1.0000 - val_loss: 0.5405 - val_acc: 0.7500\n",
            "Epoch 116/300\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.52594\n",
            "60/60 - 0s - loss: 0.1601 - acc: 0.9500 - val_loss: 0.5373 - val_acc: 0.8000\n",
            "Epoch 117/300\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.52594\n",
            "60/60 - 0s - loss: 0.1566 - acc: 0.9500 - val_loss: 0.5353 - val_acc: 0.7500\n",
            "Epoch 118/300\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.52594\n",
            "60/60 - 0s - loss: 0.1401 - acc: 0.9833 - val_loss: 0.5486 - val_acc: 0.7500\n",
            "Epoch 119/300\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 0.52594\n",
            "60/60 - 0s - loss: 0.1384 - acc: 0.9833 - val_loss: 0.5626 - val_acc: 0.7500\n",
            "Epoch 120/300\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.52594\n",
            "60/60 - 0s - loss: 0.1346 - acc: 0.9833 - val_loss: 0.5697 - val_acc: 0.7500\n",
            "Epoch 121/300\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.52594\n",
            "60/60 - 0s - loss: 0.1548 - acc: 0.9667 - val_loss: 0.5767 - val_acc: 0.7500\n",
            "Epoch 122/300\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.52594\n",
            "60/60 - 0s - loss: 0.1590 - acc: 1.0000 - val_loss: 0.5929 - val_acc: 0.7500\n",
            "Epoch 123/300\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 0.52594\n",
            "60/60 - 0s - loss: 0.1430 - acc: 1.0000 - val_loss: 0.5969 - val_acc: 0.7000\n",
            "Epoch 124/300\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 0.52594\n",
            "60/60 - 0s - loss: 0.1429 - acc: 0.9833 - val_loss: 0.5975 - val_acc: 0.7000\n",
            "Epoch 125/300\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 0.52594\n",
            "60/60 - 0s - loss: 0.1885 - acc: 0.9667 - val_loss: 0.5932 - val_acc: 0.7000\n",
            "Epoch 126/300\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 0.52594\n",
            "60/60 - 0s - loss: 0.1477 - acc: 1.0000 - val_loss: 0.5874 - val_acc: 0.7000\n",
            "Epoch 127/300\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 0.52594\n",
            "60/60 - 0s - loss: 0.1284 - acc: 0.9667 - val_loss: 0.5721 - val_acc: 0.7000\n",
            "Epoch 128/300\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 0.52594\n",
            "60/60 - 0s - loss: 0.1866 - acc: 0.9333 - val_loss: 0.5584 - val_acc: 0.7500\n",
            "Epoch 129/300\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.52594\n",
            "60/60 - 0s - loss: 0.1510 - acc: 0.9833 - val_loss: 0.5671 - val_acc: 0.7500\n",
            "Epoch 130/300\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 0.52594\n",
            "60/60 - 0s - loss: 0.1124 - acc: 1.0000 - val_loss: 0.5864 - val_acc: 0.7500\n",
            "Epoch 131/300\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 0.52594\n",
            "60/60 - 0s - loss: 0.1730 - acc: 0.9333 - val_loss: 0.5912 - val_acc: 0.7500\n",
            "Epoch 132/300\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 0.52594\n",
            "60/60 - 0s - loss: 0.1344 - acc: 1.0000 - val_loss: 0.5990 - val_acc: 0.7000\n",
            "Epoch 133/300\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.52594\n",
            "60/60 - 0s - loss: 0.1232 - acc: 0.9833 - val_loss: 0.6120 - val_acc: 0.7500\n",
            "Epoch 134/300\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.52594\n",
            "60/60 - 0s - loss: 0.1510 - acc: 0.9833 - val_loss: 0.6189 - val_acc: 0.7500\n",
            "Epoch 135/300\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 0.52594\n",
            "60/60 - 0s - loss: 0.1482 - acc: 1.0000 - val_loss: 0.6247 - val_acc: 0.7500\n",
            "Epoch 136/300\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.52594\n",
            "60/60 - 0s - loss: 0.1227 - acc: 0.9833 - val_loss: 0.6254 - val_acc: 0.7500\n",
            "Epoch 137/300\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 0.52594\n",
            "60/60 - 0s - loss: 0.2125 - acc: 0.9333 - val_loss: 0.6289 - val_acc: 0.7500\n",
            "Epoch 138/300\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.52594\n",
            "60/60 - 0s - loss: 0.1487 - acc: 0.9833 - val_loss: 0.6370 - val_acc: 0.7500\n",
            "Epoch 139/300\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.52594\n",
            "60/60 - 0s - loss: 0.1347 - acc: 0.9833 - val_loss: 0.6469 - val_acc: 0.7500\n",
            "Epoch 140/300\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 0.52594\n",
            "60/60 - 0s - loss: 0.1780 - acc: 0.9333 - val_loss: 0.6369 - val_acc: 0.7500\n",
            "Epoch 141/300\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.52594\n",
            "60/60 - 0s - loss: 0.1689 - acc: 0.9333 - val_loss: 0.6151 - val_acc: 0.7500\n",
            "Epoch 142/300\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.52594\n",
            "60/60 - 0s - loss: 0.1524 - acc: 0.9833 - val_loss: 0.5911 - val_acc: 0.7500\n",
            "Epoch 143/300\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 0.52594\n",
            "60/60 - 0s - loss: 0.1299 - acc: 1.0000 - val_loss: 0.5709 - val_acc: 0.7500\n",
            "Epoch 144/300\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 0.52594\n",
            "60/60 - 0s - loss: 0.1504 - acc: 0.9667 - val_loss: 0.5580 - val_acc: 0.7500\n",
            "Epoch 145/300\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.52594\n",
            "60/60 - 0s - loss: 0.2069 - acc: 0.9167 - val_loss: 0.5330 - val_acc: 0.7500\n",
            "Epoch 146/300\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.52594\n",
            "60/60 - 0s - loss: 0.1082 - acc: 1.0000 - val_loss: 0.5334 - val_acc: 0.7500\n",
            "Epoch 147/300\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.52594\n",
            "60/60 - 0s - loss: 0.1090 - acc: 1.0000 - val_loss: 0.5419 - val_acc: 0.7500\n",
            "Epoch 148/300\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 0.52594\n",
            "60/60 - 0s - loss: 0.1076 - acc: 1.0000 - val_loss: 0.5565 - val_acc: 0.7500\n",
            "Epoch 149/300\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.52594\n",
            "60/60 - 0s - loss: 0.1037 - acc: 1.0000 - val_loss: 0.5669 - val_acc: 0.7500\n",
            "Epoch 150/300\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.52594\n",
            "60/60 - 0s - loss: 0.2098 - acc: 0.9000 - val_loss: 0.5653 - val_acc: 0.7500\n",
            "Epoch 151/300\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.52594\n",
            "60/60 - 0s - loss: 0.1117 - acc: 1.0000 - val_loss: 0.5608 - val_acc: 0.7500\n",
            "Epoch 152/300\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.52594\n",
            "60/60 - 0s - loss: 0.1863 - acc: 0.9833 - val_loss: 0.5605 - val_acc: 0.7500\n",
            "Epoch 153/300\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 0.52594\n",
            "60/60 - 0s - loss: 0.1877 - acc: 0.9500 - val_loss: 0.5678 - val_acc: 0.7500\n",
            "Epoch 154/300\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.52594\n",
            "60/60 - 0s - loss: 0.1394 - acc: 0.9667 - val_loss: 0.5790 - val_acc: 0.7500\n",
            "Epoch 155/300\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.52594\n",
            "60/60 - 0s - loss: 0.1246 - acc: 1.0000 - val_loss: 0.5932 - val_acc: 0.7500\n",
            "Epoch 156/300\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.52594\n",
            "60/60 - 0s - loss: 0.1361 - acc: 0.9833 - val_loss: 0.5951 - val_acc: 0.7500\n",
            "Epoch 157/300\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.52594\n",
            "60/60 - 0s - loss: 0.1039 - acc: 1.0000 - val_loss: 0.5897 - val_acc: 0.7500\n",
            "Epoch 158/300\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.52594\n",
            "60/60 - 0s - loss: 0.1670 - acc: 0.9833 - val_loss: 0.5991 - val_acc: 0.7500\n",
            "Epoch 159/300\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.52594\n",
            "60/60 - 0s - loss: 0.2068 - acc: 0.9333 - val_loss: 0.6124 - val_acc: 0.7500\n",
            "Epoch 160/300\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.52594\n",
            "60/60 - 0s - loss: 0.1031 - acc: 1.0000 - val_loss: 0.6114 - val_acc: 0.7500\n",
            "Epoch 161/300\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.52594\n",
            "60/60 - 0s - loss: 0.1652 - acc: 0.9833 - val_loss: 0.6046 - val_acc: 0.7500\n",
            "Epoch 162/300\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.52594\n",
            "60/60 - 0s - loss: 0.1611 - acc: 0.9667 - val_loss: 0.6037 - val_acc: 0.7500\n",
            "Epoch 163/300\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.52594\n",
            "60/60 - 0s - loss: 0.1369 - acc: 0.9833 - val_loss: 0.6107 - val_acc: 0.7500\n",
            "Epoch 164/300\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.52594\n",
            "60/60 - 0s - loss: 0.1786 - acc: 0.9333 - val_loss: 0.6093 - val_acc: 0.7500\n",
            "Epoch 165/300\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.52594\n",
            "60/60 - 0s - loss: 0.2030 - acc: 0.9333 - val_loss: 0.6031 - val_acc: 0.7500\n",
            "Epoch 166/300\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 0.52594\n",
            "60/60 - 0s - loss: 0.1307 - acc: 0.9833 - val_loss: 0.5891 - val_acc: 0.7500\n",
            "Epoch 167/300\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.52594\n",
            "60/60 - 0s - loss: 0.1348 - acc: 0.9833 - val_loss: 0.5732 - val_acc: 0.7500\n",
            "Epoch 168/300\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.52594\n",
            "60/60 - 0s - loss: 0.1408 - acc: 0.9667 - val_loss: 0.5510 - val_acc: 0.7500\n",
            "Epoch 169/300\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 0.52594\n",
            "60/60 - 0s - loss: 0.1396 - acc: 0.9833 - val_loss: 0.5468 - val_acc: 0.7500\n",
            "Epoch 170/300\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.52594\n",
            "60/60 - 0s - loss: 0.1156 - acc: 0.9833 - val_loss: 0.5491 - val_acc: 0.7500\n",
            "Epoch 171/300\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.52594\n",
            "60/60 - 0s - loss: 0.1335 - acc: 0.9500 - val_loss: 0.5638 - val_acc: 0.7500\n",
            "Epoch 172/300\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.52594\n",
            "60/60 - 0s - loss: 0.1380 - acc: 0.9833 - val_loss: 0.5610 - val_acc: 0.7500\n",
            "Epoch 173/300\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.52594\n",
            "60/60 - 0s - loss: 0.1136 - acc: 1.0000 - val_loss: 0.5521 - val_acc: 0.7500\n",
            "Epoch 174/300\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.52594\n",
            "60/60 - 0s - loss: 0.1122 - acc: 0.9833 - val_loss: 0.5493 - val_acc: 0.7500\n",
            "Epoch 175/300\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.52594\n",
            "60/60 - 0s - loss: 0.1022 - acc: 0.9833 - val_loss: 0.5510 - val_acc: 0.7500\n",
            "Epoch 176/300\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 0.52594\n",
            "60/60 - 0s - loss: 0.1351 - acc: 0.9667 - val_loss: 0.5554 - val_acc: 0.7500\n",
            "Epoch 177/300\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.52594\n",
            "60/60 - 0s - loss: 0.1348 - acc: 0.9667 - val_loss: 0.5596 - val_acc: 0.7500\n",
            "Epoch 178/300\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.52594\n",
            "60/60 - 0s - loss: 0.1504 - acc: 0.9833 - val_loss: 0.5715 - val_acc: 0.7500\n",
            "Epoch 179/300\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.52594\n",
            "60/60 - 0s - loss: 0.2007 - acc: 0.9000 - val_loss: 0.5840 - val_acc: 0.7500\n",
            "Epoch 180/300\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.52594\n",
            "60/60 - 0s - loss: 0.1524 - acc: 0.9667 - val_loss: 0.5640 - val_acc: 0.7500\n",
            "Epoch 181/300\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.52594\n",
            "60/60 - 0s - loss: 0.1187 - acc: 1.0000 - val_loss: 0.5488 - val_acc: 0.7500\n",
            "Epoch 182/300\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.52594\n",
            "60/60 - 0s - loss: 0.1563 - acc: 1.0000 - val_loss: 0.5413 - val_acc: 0.7500\n",
            "Epoch 183/300\n",
            "\n",
            "Epoch 00183: val_loss improved from 0.52594 to 0.52581, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1367 - acc: 0.9500 - val_loss: 0.5258 - val_acc: 0.7500\n",
            "Epoch 184/300\n",
            "\n",
            "Epoch 00184: val_loss improved from 0.52581 to 0.52148, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1103 - acc: 1.0000 - val_loss: 0.5215 - val_acc: 0.7500\n",
            "Epoch 185/300\n",
            "\n",
            "Epoch 00185: val_loss improved from 0.52148 to 0.52137, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1166 - acc: 1.0000 - val_loss: 0.5214 - val_acc: 0.8000\n",
            "Epoch 186/300\n",
            "\n",
            "Epoch 00186: val_loss improved from 0.52137 to 0.51725, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1602 - acc: 0.9667 - val_loss: 0.5173 - val_acc: 0.8000\n",
            "Epoch 187/300\n",
            "\n",
            "Epoch 00187: val_loss improved from 0.51725 to 0.50845, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1162 - acc: 0.9833 - val_loss: 0.5084 - val_acc: 0.8000\n",
            "Epoch 188/300\n",
            "\n",
            "Epoch 00188: val_loss improved from 0.50845 to 0.50396, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1037 - acc: 0.9833 - val_loss: 0.5040 - val_acc: 0.7500\n",
            "Epoch 189/300\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.50396\n",
            "60/60 - 0s - loss: 0.1136 - acc: 0.9833 - val_loss: 0.5105 - val_acc: 0.7500\n",
            "Epoch 190/300\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.50396\n",
            "60/60 - 0s - loss: 0.0901 - acc: 1.0000 - val_loss: 0.5056 - val_acc: 0.7500\n",
            "Epoch 191/300\n",
            "\n",
            "Epoch 00191: val_loss improved from 0.50396 to 0.50287, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.0989 - acc: 0.9833 - val_loss: 0.5029 - val_acc: 0.7500\n",
            "Epoch 192/300\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 0.50287\n",
            "60/60 - 0s - loss: 0.1519 - acc: 0.9667 - val_loss: 0.5131 - val_acc: 0.7500\n",
            "Epoch 193/300\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.50287\n",
            "60/60 - 0s - loss: 0.1432 - acc: 0.9500 - val_loss: 0.5432 - val_acc: 0.7500\n",
            "Epoch 194/300\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.50287\n",
            "60/60 - 0s - loss: 0.1493 - acc: 0.9667 - val_loss: 0.5600 - val_acc: 0.7500\n",
            "Epoch 195/300\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.50287\n",
            "60/60 - 0s - loss: 0.1269 - acc: 1.0000 - val_loss: 0.5687 - val_acc: 0.7500\n",
            "Epoch 196/300\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.50287\n",
            "60/60 - 0s - loss: 0.0922 - acc: 0.9833 - val_loss: 0.5887 - val_acc: 0.7500\n",
            "Epoch 197/300\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.50287\n",
            "60/60 - 0s - loss: 0.1049 - acc: 1.0000 - val_loss: 0.6073 - val_acc: 0.7500\n",
            "Epoch 198/300\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.50287\n",
            "60/60 - 0s - loss: 0.1207 - acc: 0.9667 - val_loss: 0.6096 - val_acc: 0.7500\n",
            "Epoch 199/300\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.50287\n",
            "60/60 - 0s - loss: 0.0942 - acc: 1.0000 - val_loss: 0.6038 - val_acc: 0.7500\n",
            "Epoch 200/300\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.50287\n",
            "60/60 - 0s - loss: 0.1438 - acc: 0.9667 - val_loss: 0.5863 - val_acc: 0.7500\n",
            "Epoch 201/300\n",
            "\n",
            "Epoch 00201: val_loss did not improve from 0.50287\n",
            "60/60 - 0s - loss: 0.1147 - acc: 0.9833 - val_loss: 0.5560 - val_acc: 0.7500\n",
            "Epoch 202/300\n",
            "\n",
            "Epoch 00202: val_loss did not improve from 0.50287\n",
            "60/60 - 0s - loss: 0.1337 - acc: 0.9833 - val_loss: 0.5318 - val_acc: 0.7500\n",
            "Epoch 203/300\n",
            "\n",
            "Epoch 00203: val_loss did not improve from 0.50287\n",
            "60/60 - 0s - loss: 0.1142 - acc: 0.9833 - val_loss: 0.5195 - val_acc: 0.7500\n",
            "Epoch 204/300\n",
            "\n",
            "Epoch 00204: val_loss did not improve from 0.50287\n",
            "60/60 - 0s - loss: 0.0994 - acc: 1.0000 - val_loss: 0.5163 - val_acc: 0.8000\n",
            "Epoch 205/300\n",
            "\n",
            "Epoch 00205: val_loss did not improve from 0.50287\n",
            "60/60 - 0s - loss: 0.0957 - acc: 0.9833 - val_loss: 0.5119 - val_acc: 0.8000\n",
            "Epoch 206/300\n",
            "\n",
            "Epoch 00206: val_loss did not improve from 0.50287\n",
            "60/60 - 0s - loss: 0.1182 - acc: 0.9667 - val_loss: 0.5071 - val_acc: 0.8500\n",
            "Epoch 207/300\n",
            "\n",
            "Epoch 00207: val_loss did not improve from 0.50287\n",
            "60/60 - 0s - loss: 0.1125 - acc: 0.9833 - val_loss: 0.5036 - val_acc: 0.8500\n",
            "Epoch 208/300\n",
            "\n",
            "Epoch 00208: val_loss improved from 0.50287 to 0.50152, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1108 - acc: 1.0000 - val_loss: 0.5015 - val_acc: 0.8000\n",
            "Epoch 209/300\n",
            "\n",
            "Epoch 00209: val_loss improved from 0.50152 to 0.49066, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1319 - acc: 0.9667 - val_loss: 0.4907 - val_acc: 0.8000\n",
            "Epoch 210/300\n",
            "\n",
            "Epoch 00210: val_loss improved from 0.49066 to 0.48733, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1344 - acc: 0.9667 - val_loss: 0.4873 - val_acc: 0.7500\n",
            "Epoch 211/300\n",
            "\n",
            "Epoch 00211: val_loss improved from 0.48733 to 0.48477, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.0821 - acc: 1.0000 - val_loss: 0.4848 - val_acc: 0.7500\n",
            "Epoch 212/300\n",
            "\n",
            "Epoch 00212: val_loss did not improve from 0.48477\n",
            "60/60 - 0s - loss: 0.0917 - acc: 1.0000 - val_loss: 0.4849 - val_acc: 0.7500\n",
            "Epoch 213/300\n",
            "\n",
            "Epoch 00213: val_loss improved from 0.48477 to 0.48110, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.0955 - acc: 1.0000 - val_loss: 0.4811 - val_acc: 0.7500\n",
            "Epoch 214/300\n",
            "\n",
            "Epoch 00214: val_loss did not improve from 0.48110\n",
            "60/60 - 0s - loss: 0.1557 - acc: 0.9500 - val_loss: 0.4907 - val_acc: 0.7500\n",
            "Epoch 215/300\n",
            "\n",
            "Epoch 00215: val_loss did not improve from 0.48110\n",
            "60/60 - 0s - loss: 0.0881 - acc: 1.0000 - val_loss: 0.5083 - val_acc: 0.7000\n",
            "Epoch 216/300\n",
            "\n",
            "Epoch 00216: val_loss did not improve from 0.48110\n",
            "60/60 - 0s - loss: 0.1129 - acc: 0.9833 - val_loss: 0.5203 - val_acc: 0.7000\n",
            "Epoch 217/300\n",
            "\n",
            "Epoch 00217: val_loss did not improve from 0.48110\n",
            "60/60 - 0s - loss: 0.0983 - acc: 0.9833 - val_loss: 0.5185 - val_acc: 0.7500\n",
            "Epoch 218/300\n",
            "\n",
            "Epoch 00218: val_loss did not improve from 0.48110\n",
            "60/60 - 0s - loss: 0.0968 - acc: 0.9833 - val_loss: 0.5255 - val_acc: 0.7500\n",
            "Epoch 219/300\n",
            "\n",
            "Epoch 00219: val_loss did not improve from 0.48110\n",
            "60/60 - 0s - loss: 0.1084 - acc: 0.9833 - val_loss: 0.5323 - val_acc: 0.7500\n",
            "Epoch 220/300\n",
            "\n",
            "Epoch 00220: val_loss did not improve from 0.48110\n",
            "60/60 - 0s - loss: 0.1186 - acc: 0.9667 - val_loss: 0.5616 - val_acc: 0.7500\n",
            "Epoch 221/300\n",
            "\n",
            "Epoch 00221: val_loss did not improve from 0.48110\n",
            "60/60 - 0s - loss: 0.0674 - acc: 0.9833 - val_loss: 0.5867 - val_acc: 0.7000\n",
            "Epoch 222/300\n",
            "\n",
            "Epoch 00222: val_loss did not improve from 0.48110\n",
            "60/60 - 0s - loss: 0.1033 - acc: 0.9833 - val_loss: 0.6096 - val_acc: 0.7000\n",
            "Epoch 223/300\n",
            "\n",
            "Epoch 00223: val_loss did not improve from 0.48110\n",
            "60/60 - 0s - loss: 0.0802 - acc: 0.9833 - val_loss: 0.6117 - val_acc: 0.7000\n",
            "Epoch 224/300\n",
            "\n",
            "Epoch 00224: val_loss did not improve from 0.48110\n",
            "60/60 - 0s - loss: 0.1433 - acc: 0.9667 - val_loss: 0.5874 - val_acc: 0.7000\n",
            "Epoch 225/300\n",
            "\n",
            "Epoch 00225: val_loss did not improve from 0.48110\n",
            "60/60 - 0s - loss: 0.1119 - acc: 1.0000 - val_loss: 0.5743 - val_acc: 0.7000\n",
            "Epoch 226/300\n",
            "\n",
            "Epoch 00226: val_loss did not improve from 0.48110\n",
            "60/60 - 0s - loss: 0.0753 - acc: 1.0000 - val_loss: 0.5629 - val_acc: 0.7000\n",
            "Epoch 227/300\n",
            "\n",
            "Epoch 00227: val_loss did not improve from 0.48110\n",
            "60/60 - 0s - loss: 0.0777 - acc: 1.0000 - val_loss: 0.5631 - val_acc: 0.7000\n",
            "Epoch 228/300\n",
            "\n",
            "Epoch 00228: val_loss did not improve from 0.48110\n",
            "60/60 - 0s - loss: 0.1047 - acc: 1.0000 - val_loss: 0.5705 - val_acc: 0.7000\n",
            "Epoch 229/300\n",
            "\n",
            "Epoch 00229: val_loss did not improve from 0.48110\n",
            "60/60 - 0s - loss: 0.1438 - acc: 0.9500 - val_loss: 0.5797 - val_acc: 0.7500\n",
            "Epoch 230/300\n",
            "\n",
            "Epoch 00230: val_loss did not improve from 0.48110\n",
            "60/60 - 0s - loss: 0.0831 - acc: 1.0000 - val_loss: 0.5846 - val_acc: 0.7500\n",
            "Epoch 231/300\n",
            "\n",
            "Epoch 00231: val_loss did not improve from 0.48110\n",
            "60/60 - 0s - loss: 0.1175 - acc: 0.9833 - val_loss: 0.5957 - val_acc: 0.7000\n",
            "Epoch 232/300\n",
            "\n",
            "Epoch 00232: val_loss did not improve from 0.48110\n",
            "60/60 - 0s - loss: 0.0592 - acc: 1.0000 - val_loss: 0.5977 - val_acc: 0.7000\n",
            "Epoch 233/300\n",
            "\n",
            "Epoch 00233: val_loss did not improve from 0.48110\n",
            "60/60 - 0s - loss: 0.0874 - acc: 1.0000 - val_loss: 0.5903 - val_acc: 0.7500\n",
            "Epoch 234/300\n",
            "\n",
            "Epoch 00234: val_loss did not improve from 0.48110\n",
            "60/60 - 0s - loss: 0.1200 - acc: 0.9667 - val_loss: 0.5536 - val_acc: 0.8000\n",
            "Epoch 235/300\n",
            "\n",
            "Epoch 00235: val_loss did not improve from 0.48110\n",
            "60/60 - 0s - loss: 0.0999 - acc: 0.9833 - val_loss: 0.5367 - val_acc: 0.8000\n",
            "Epoch 236/300\n",
            "\n",
            "Epoch 00236: val_loss did not improve from 0.48110\n",
            "60/60 - 0s - loss: 0.0777 - acc: 1.0000 - val_loss: 0.5321 - val_acc: 0.8000\n",
            "Epoch 237/300\n",
            "\n",
            "Epoch 00237: val_loss did not improve from 0.48110\n",
            "60/60 - 0s - loss: 0.1186 - acc: 0.9667 - val_loss: 0.5342 - val_acc: 0.8000\n",
            "Epoch 238/300\n",
            "\n",
            "Epoch 00238: val_loss did not improve from 0.48110\n",
            "60/60 - 0s - loss: 0.0945 - acc: 0.9833 - val_loss: 0.5459 - val_acc: 0.8000\n",
            "Epoch 239/300\n",
            "\n",
            "Epoch 00239: val_loss did not improve from 0.48110\n",
            "60/60 - 0s - loss: 0.1075 - acc: 0.9667 - val_loss: 0.5507 - val_acc: 0.8000\n",
            "Epoch 240/300\n",
            "\n",
            "Epoch 00240: val_loss did not improve from 0.48110\n",
            "60/60 - 0s - loss: 0.0961 - acc: 0.9833 - val_loss: 0.5457 - val_acc: 0.7500\n",
            "Epoch 241/300\n",
            "\n",
            "Epoch 00241: val_loss did not improve from 0.48110\n",
            "60/60 - 0s - loss: 0.1109 - acc: 1.0000 - val_loss: 0.5481 - val_acc: 0.7500\n",
            "Epoch 242/300\n",
            "\n",
            "Epoch 00242: val_loss did not improve from 0.48110\n",
            "60/60 - 0s - loss: 0.1407 - acc: 0.9500 - val_loss: 0.5439 - val_acc: 0.8000\n",
            "Epoch 243/300\n",
            "\n",
            "Epoch 00243: val_loss did not improve from 0.48110\n",
            "60/60 - 0s - loss: 0.0956 - acc: 0.9833 - val_loss: 0.5371 - val_acc: 0.8500\n",
            "Epoch 244/300\n",
            "\n",
            "Epoch 00244: val_loss did not improve from 0.48110\n",
            "60/60 - 0s - loss: 0.0694 - acc: 1.0000 - val_loss: 0.5278 - val_acc: 0.8500\n",
            "Epoch 245/300\n",
            "\n",
            "Epoch 00245: val_loss did not improve from 0.48110\n",
            "60/60 - 0s - loss: 0.1083 - acc: 0.9833 - val_loss: 0.5197 - val_acc: 0.8500\n",
            "Epoch 246/300\n",
            "\n",
            "Epoch 00246: val_loss did not improve from 0.48110\n",
            "60/60 - 0s - loss: 0.0825 - acc: 1.0000 - val_loss: 0.5257 - val_acc: 0.8500\n",
            "Epoch 247/300\n",
            "\n",
            "Epoch 00247: val_loss did not improve from 0.48110\n",
            "60/60 - 0s - loss: 0.1025 - acc: 0.9833 - val_loss: 0.5290 - val_acc: 0.8500\n",
            "Epoch 248/300\n",
            "\n",
            "Epoch 00248: val_loss did not improve from 0.48110\n",
            "60/60 - 0s - loss: 0.0780 - acc: 1.0000 - val_loss: 0.5279 - val_acc: 0.8500\n",
            "Epoch 249/300\n",
            "\n",
            "Epoch 00249: val_loss did not improve from 0.48110\n",
            "60/60 - 0s - loss: 0.0702 - acc: 1.0000 - val_loss: 0.5382 - val_acc: 0.8500\n",
            "Epoch 250/300\n",
            "\n",
            "Epoch 00250: val_loss did not improve from 0.48110\n",
            "60/60 - 0s - loss: 0.0997 - acc: 0.9833 - val_loss: 0.5443 - val_acc: 0.8500\n",
            "Epoch 251/300\n",
            "\n",
            "Epoch 00251: val_loss did not improve from 0.48110\n",
            "60/60 - 0s - loss: 0.1158 - acc: 0.9667 - val_loss: 0.5383 - val_acc: 0.8500\n",
            "Epoch 252/300\n",
            "\n",
            "Epoch 00252: val_loss did not improve from 0.48110\n",
            "60/60 - 0s - loss: 0.0734 - acc: 0.9833 - val_loss: 0.5346 - val_acc: 0.8000\n",
            "Epoch 253/300\n",
            "\n",
            "Epoch 00253: val_loss did not improve from 0.48110\n",
            "60/60 - 0s - loss: 0.0865 - acc: 1.0000 - val_loss: 0.5430 - val_acc: 0.8000\n",
            "Epoch 254/300\n",
            "\n",
            "Epoch 00254: val_loss did not improve from 0.48110\n",
            "60/60 - 0s - loss: 0.0956 - acc: 1.0000 - val_loss: 0.5641 - val_acc: 0.8000\n",
            "Epoch 255/300\n",
            "\n",
            "Epoch 00255: val_loss did not improve from 0.48110\n",
            "60/60 - 0s - loss: 0.0903 - acc: 1.0000 - val_loss: 0.5813 - val_acc: 0.8000\n",
            "Epoch 256/300\n",
            "\n",
            "Epoch 00256: val_loss did not improve from 0.48110\n",
            "60/60 - 0s - loss: 0.0714 - acc: 1.0000 - val_loss: 0.5923 - val_acc: 0.8000\n",
            "Epoch 257/300\n",
            "\n",
            "Epoch 00257: val_loss did not improve from 0.48110\n",
            "60/60 - 0s - loss: 0.0902 - acc: 0.9833 - val_loss: 0.6005 - val_acc: 0.8000\n",
            "Epoch 258/300\n",
            "\n",
            "Epoch 00258: val_loss did not improve from 0.48110\n",
            "60/60 - 0s - loss: 0.1156 - acc: 0.9667 - val_loss: 0.6095 - val_acc: 0.8500\n",
            "Epoch 259/300\n",
            "\n",
            "Epoch 00259: val_loss did not improve from 0.48110\n",
            "60/60 - 0s - loss: 0.0802 - acc: 0.9833 - val_loss: 0.5891 - val_acc: 0.8500\n",
            "Epoch 260/300\n",
            "\n",
            "Epoch 00260: val_loss did not improve from 0.48110\n",
            "60/60 - 0s - loss: 0.0887 - acc: 1.0000 - val_loss: 0.5598 - val_acc: 0.8000\n",
            "Epoch 261/300\n",
            "\n",
            "Epoch 00261: val_loss did not improve from 0.48110\n",
            "60/60 - 0s - loss: 0.0826 - acc: 1.0000 - val_loss: 0.5332 - val_acc: 0.8000\n",
            "Epoch 262/300\n",
            "\n",
            "Epoch 00262: val_loss did not improve from 0.48110\n",
            "60/60 - 0s - loss: 0.1806 - acc: 0.9167 - val_loss: 0.5165 - val_acc: 0.8000\n",
            "Epoch 263/300\n",
            "\n",
            "Epoch 00263: val_loss did not improve from 0.48110\n",
            "60/60 - 0s - loss: 0.1388 - acc: 0.9667 - val_loss: 0.5256 - val_acc: 0.8000\n",
            "Epoch 264/300\n",
            "\n",
            "Epoch 00264: val_loss did not improve from 0.48110\n",
            "60/60 - 0s - loss: 0.0861 - acc: 1.0000 - val_loss: 0.5406 - val_acc: 0.7500\n",
            "Epoch 265/300\n",
            "\n",
            "Epoch 00265: val_loss did not improve from 0.48110\n",
            "60/60 - 0s - loss: 0.0848 - acc: 0.9833 - val_loss: 0.5591 - val_acc: 0.7500\n",
            "Epoch 266/300\n",
            "\n",
            "Epoch 00266: val_loss did not improve from 0.48110\n",
            "60/60 - 0s - loss: 0.0749 - acc: 0.9833 - val_loss: 0.5767 - val_acc: 0.7500\n",
            "Epoch 267/300\n",
            "\n",
            "Epoch 00267: val_loss did not improve from 0.48110\n",
            "60/60 - 0s - loss: 0.0883 - acc: 0.9833 - val_loss: 0.5775 - val_acc: 0.7500\n",
            "Epoch 268/300\n",
            "\n",
            "Epoch 00268: val_loss did not improve from 0.48110\n",
            "60/60 - 0s - loss: 0.1584 - acc: 0.9833 - val_loss: 0.5499 - val_acc: 0.7500\n",
            "Epoch 269/300\n",
            "\n",
            "Epoch 00269: val_loss did not improve from 0.48110\n",
            "60/60 - 0s - loss: 0.0710 - acc: 1.0000 - val_loss: 0.5221 - val_acc: 0.8000\n",
            "Epoch 270/300\n",
            "\n",
            "Epoch 00270: val_loss did not improve from 0.48110\n",
            "60/60 - 0s - loss: 0.0748 - acc: 1.0000 - val_loss: 0.5158 - val_acc: 0.8000\n",
            "Epoch 271/300\n",
            "\n",
            "Epoch 00271: val_loss did not improve from 0.48110\n",
            "60/60 - 0s - loss: 0.0669 - acc: 1.0000 - val_loss: 0.5132 - val_acc: 0.8000\n",
            "Epoch 272/300\n",
            "\n",
            "Epoch 00272: val_loss did not improve from 0.48110\n",
            "60/60 - 0s - loss: 0.0791 - acc: 1.0000 - val_loss: 0.5195 - val_acc: 0.8000\n",
            "Epoch 273/300\n",
            "\n",
            "Epoch 00273: val_loss did not improve from 0.48110\n",
            "60/60 - 0s - loss: 0.0873 - acc: 1.0000 - val_loss: 0.5218 - val_acc: 0.8500\n",
            "Epoch 274/300\n",
            "\n",
            "Epoch 00274: val_loss did not improve from 0.48110\n",
            "60/60 - 0s - loss: 0.0817 - acc: 0.9833 - val_loss: 0.5385 - val_acc: 0.8500\n",
            "Epoch 275/300\n",
            "\n",
            "Epoch 00275: val_loss did not improve from 0.48110\n",
            "60/60 - 0s - loss: 0.1006 - acc: 1.0000 - val_loss: 0.5781 - val_acc: 0.8000\n",
            "Epoch 276/300\n",
            "\n",
            "Epoch 00276: val_loss did not improve from 0.48110\n",
            "60/60 - 0s - loss: 0.1036 - acc: 0.9833 - val_loss: 0.6078 - val_acc: 0.7500\n",
            "Epoch 277/300\n",
            "\n",
            "Epoch 00277: val_loss did not improve from 0.48110\n",
            "60/60 - 0s - loss: 0.1384 - acc: 0.9667 - val_loss: 0.5953 - val_acc: 0.8000\n",
            "Epoch 278/300\n",
            "\n",
            "Epoch 00278: val_loss did not improve from 0.48110\n",
            "60/60 - 0s - loss: 0.1574 - acc: 0.9333 - val_loss: 0.5425 - val_acc: 0.8000\n",
            "Epoch 279/300\n",
            "\n",
            "Epoch 00279: val_loss did not improve from 0.48110\n",
            "60/60 - 0s - loss: 0.0673 - acc: 1.0000 - val_loss: 0.5041 - val_acc: 0.7500\n",
            "Epoch 280/300\n",
            "\n",
            "Epoch 00280: val_loss did not improve from 0.48110\n",
            "60/60 - 0s - loss: 0.0811 - acc: 1.0000 - val_loss: 0.4926 - val_acc: 0.7500\n",
            "Epoch 281/300\n",
            "\n",
            "Epoch 00281: val_loss did not improve from 0.48110\n",
            "60/60 - 0s - loss: 0.0936 - acc: 0.9833 - val_loss: 0.5081 - val_acc: 0.8000\n",
            "Epoch 282/300\n",
            "\n",
            "Epoch 00282: val_loss did not improve from 0.48110\n",
            "60/60 - 0s - loss: 0.0924 - acc: 1.0000 - val_loss: 0.5545 - val_acc: 0.8000\n",
            "Epoch 283/300\n",
            "\n",
            "Epoch 00283: val_loss did not improve from 0.48110\n",
            "60/60 - 0s - loss: 0.1273 - acc: 0.9500 - val_loss: 0.5582 - val_acc: 0.8000\n",
            "Epoch 284/300\n",
            "\n",
            "Epoch 00284: val_loss did not improve from 0.48110\n",
            "60/60 - 0s - loss: 0.0848 - acc: 1.0000 - val_loss: 0.5343 - val_acc: 0.8000\n",
            "Epoch 285/300\n",
            "\n",
            "Epoch 00285: val_loss did not improve from 0.48110\n",
            "60/60 - 0s - loss: 0.1263 - acc: 0.9333 - val_loss: 0.5278 - val_acc: 0.8000\n",
            "Epoch 286/300\n",
            "\n",
            "Epoch 00286: val_loss did not improve from 0.48110\n",
            "60/60 - 0s - loss: 0.0624 - acc: 1.0000 - val_loss: 0.5145 - val_acc: 0.8000\n",
            "Epoch 287/300\n",
            "\n",
            "Epoch 00287: val_loss did not improve from 0.48110\n",
            "60/60 - 0s - loss: 0.0807 - acc: 1.0000 - val_loss: 0.5209 - val_acc: 0.8000\n",
            "Epoch 288/300\n",
            "\n",
            "Epoch 00288: val_loss did not improve from 0.48110\n",
            "60/60 - 0s - loss: 0.0739 - acc: 1.0000 - val_loss: 0.5231 - val_acc: 0.8000\n",
            "Epoch 289/300\n",
            "\n",
            "Epoch 00289: val_loss did not improve from 0.48110\n",
            "60/60 - 0s - loss: 0.0879 - acc: 0.9833 - val_loss: 0.5381 - val_acc: 0.8500\n",
            "Epoch 290/300\n",
            "\n",
            "Epoch 00290: val_loss did not improve from 0.48110\n",
            "60/60 - 0s - loss: 0.0652 - acc: 1.0000 - val_loss: 0.5811 - val_acc: 0.9000\n",
            "Epoch 291/300\n",
            "\n",
            "Epoch 00291: val_loss did not improve from 0.48110\n",
            "60/60 - 0s - loss: 0.0623 - acc: 1.0000 - val_loss: 0.6086 - val_acc: 0.9000\n",
            "Epoch 292/300\n",
            "\n",
            "Epoch 00292: val_loss did not improve from 0.48110\n",
            "60/60 - 0s - loss: 0.0568 - acc: 0.9833 - val_loss: 0.6264 - val_acc: 0.9000\n",
            "Epoch 293/300\n",
            "\n",
            "Epoch 00293: val_loss did not improve from 0.48110\n",
            "60/60 - 0s - loss: 0.0830 - acc: 1.0000 - val_loss: 0.6281 - val_acc: 0.9000\n",
            "Epoch 294/300\n",
            "\n",
            "Epoch 00294: val_loss did not improve from 0.48110\n",
            "60/60 - 0s - loss: 0.0844 - acc: 0.9833 - val_loss: 0.6190 - val_acc: 0.8500\n",
            "Epoch 295/300\n",
            "\n",
            "Epoch 00295: val_loss did not improve from 0.48110\n",
            "60/60 - 0s - loss: 0.0791 - acc: 0.9833 - val_loss: 0.6166 - val_acc: 0.8500\n",
            "Epoch 296/300\n",
            "\n",
            "Epoch 00296: val_loss did not improve from 0.48110\n",
            "60/60 - 0s - loss: 0.0701 - acc: 0.9833 - val_loss: 0.6101 - val_acc: 0.8500\n",
            "Epoch 297/300\n",
            "\n",
            "Epoch 00297: val_loss did not improve from 0.48110\n",
            "60/60 - 0s - loss: 0.0685 - acc: 1.0000 - val_loss: 0.6012 - val_acc: 0.8000\n",
            "Epoch 298/300\n",
            "\n",
            "Epoch 00298: val_loss did not improve from 0.48110\n",
            "60/60 - 0s - loss: 0.0554 - acc: 1.0000 - val_loss: 0.6201 - val_acc: 0.8000\n",
            "Epoch 299/300\n",
            "\n",
            "Epoch 00299: val_loss did not improve from 0.48110\n",
            "60/60 - 0s - loss: 0.0854 - acc: 1.0000 - val_loss: 0.6370 - val_acc: 0.8000\n",
            "Epoch 300/300\n",
            "\n",
            "Epoch 00300: val_loss did not improve from 0.48110\n",
            "60/60 - 0s - loss: 0.1201 - acc: 0.9667 - val_loss: 0.6348 - val_acc: 0.8000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "# Evaluate on test data\n",
            "40/40 [==============================] - 0s 2ms/sample - loss: 0.3966 - acc: 0.8250\n",
            "test loss, test acc: [0.39663084022577094, 0.825]\n",
            "[[0.63296234]\n",
            " [0.74573773]\n",
            " [0.93458389]\n",
            " [0.64138998]\n",
            " [0.40575066]\n",
            " [1.27559777]\n",
            " [0.5776631 ]\n",
            " [1.00559899]\n",
            " [0.78319768]\n",
            " [0.39663084]]\n",
            "[[0.64999998]\n",
            " [0.67500001]\n",
            " [0.55000001]\n",
            " [0.625     ]\n",
            " [0.82499999]\n",
            " [0.57499999]\n",
            " [0.75      ]\n",
            " [0.625     ]\n",
            " [0.47499999]\n",
            " [0.82499999]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kd6FTGs7vBI3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "df_accl_all = pd.DataFrame({'Class1vs2': acc_all[:, 0]})\n",
        "df_accl_all.to_csv (r'EEG_Deep/df_accl_allPatient_8_24_2560:4096.csv', index = None, header=True) #Don't forget to add '.csv' at the end of the path\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}