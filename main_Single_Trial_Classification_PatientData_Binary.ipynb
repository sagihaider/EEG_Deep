{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "main_Single_Trial_Classification_PatientData_Binary",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sagihaider/EEG_Deep/blob/master/main_Single_Trial_Classification_PatientData_Binary.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X110PE9MjrlE",
        "colab_type": "code",
        "outputId": "e29f351b-6f20-4bf3-c6e8-34b99fa87443",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        }
      },
      "source": [
        "!git clone https://github.com/sagihaider/EEG_Deep.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'EEG_Deep'...\n",
            "remote: Enumerating objects: 48, done.\u001b[K\n",
            "remote: Counting objects: 100% (48/48), done.\u001b[K\n",
            "remote: Compressing objects: 100% (44/44), done.\u001b[K\n",
            "remote: Total 201 (delta 25), reused 10 (delta 4), pack-reused 153\u001b[K\n",
            "Receiving objects: 100% (201/201), 858.46 MiB | 10.17 MiB/s, done.\n",
            "Resolving deltas: 100% (78/78), done.\n",
            "Checking out files: 100% (60/60), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ioLzMoiS60Qf",
        "colab_type": "code",
        "outputId": "02b22351-bcad-4cd9-dd75-df39d80aad03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 65
        }
      },
      "source": [
        "import scipy.io as spio\n",
        "import numpy as np\n",
        "from importlib.machinery import SourceFileLoader\n",
        "\n",
        "# EEGNet-specific imports\n",
        "from EEG_Deep.EEGModels import EEGNet, ShallowConvNet, DeepConvNet\n",
        "from tensorflow.keras import utils as np_utils\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras import backend as K\n",
        "K.set_image_data_format('channels_first')\n",
        "\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# tools for plotting confusion matrices\n",
        "from matplotlib import pyplot as plt\n",
        "from scipy.signal import butter, lfilter"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mo80Jg_Pn5lP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Band-pass Filter\n",
        "def butter_bandpass_filter(data, lowcut, highcut, fs, order=4):\n",
        "    nyq = 0.5 * fs\n",
        "    low = lowcut / nyq\n",
        "    high = highcut / nyq\n",
        "    b, a = butter(order, [low, high], btype='band')\n",
        "    y = lfilter(b, a, data)\n",
        "    return y\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZjtnE_6RSpum",
        "colab_type": "code",
        "outputId": "1df9e41c-3885-4ef5-af9b-9164515a0832",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from numpy import zeros\n",
        "K.clear_session()\n",
        "cols = 1\n",
        "rows = 10\n",
        "acc_all = zeros([rows, cols])\n",
        "loss_all = zeros([rows, cols])\n",
        "X_tr_c12 = np.empty([80, 12, 4096])\n",
        "X_ts_c12 = np.empty([80, 12, 4096])\n",
        "result=[]\n",
        "\n",
        "from itertools import combinations \n",
        "comb = combinations([1, 2], 2) \n",
        "  # Print the obtained combinations \n",
        "bincomb=[]\n",
        "for i in list(comb): \n",
        "    bincomb.append(i)\n",
        "\n",
        "for x in range(1,11):\n",
        "  fName = 'EEG_Deep/Data2A/parsed_P0' + str(x) + 'T.mat'  # Load Data\n",
        "  print(fName)\n",
        "  mat = spio.loadmat(fName)\n",
        "  r_X_tr = mat['RawEEGData']\n",
        "  r_y_tr = mat['Labels']\n",
        "\n",
        "  ### Filter Data ###\n",
        "  for t in range(r_X_tr.shape[0]):\n",
        "    tril = r_X_tr[t,:,:]\n",
        "    # tril = tril.transpose()\n",
        "    tril_filtered = butter_bandpass_filter(tril, \n",
        "                                              lowcut=8, \n",
        "                                              highcut=24, \n",
        "                                              fs=512,\n",
        "                                              order=4)\n",
        "    # tril_filtered = tril_filtered.transpose()\n",
        "    X_tr_c12[t,:,:] = tril_filtered\n",
        "\n",
        "  print(\"Filtering of Training Data Finished\")\n",
        "  ## Test Data Load \n",
        "\n",
        "  fName = 'EEG_Deep/Data2A/parsed_P0' + str(x) + 'E.mat'  # Load Data\n",
        "  print(fName)\n",
        "  mat = spio.loadmat(fName)\n",
        "  r_X_ts = mat['RawEEGData']\n",
        "  r_y_ts = mat['Labels']\n",
        "  for t in range(r_X_ts.shape[0]):\n",
        "    tril = r_X_ts[t,:,:]\n",
        "    # tril = tril.transpose()\n",
        "    tril_filtered = butter_bandpass_filter(tril, \n",
        "                                              lowcut=8, \n",
        "                                              highcut=24, \n",
        "                                              fs=512,\n",
        "                                              order=4)\n",
        "    # tril_filtered = tril_filtered.transpose()\n",
        "    X_ts_c12[t,:,:] = tril_filtered\n",
        "  \n",
        "  print(\"Filtering of Testing Data Finished\")    \n",
        "\n",
        "  for k, com in enumerate(bincomb):\n",
        "      print(com)\n",
        "      # # Find labels to specific class\n",
        "      print(\"Finding labels in training data\")\n",
        "      class1indx = list(np.where(r_y_tr == com[0]))\n",
        "      class2indx = list(np.where(r_y_tr == com[1]))\n",
        "      c1=list(class1indx[0])\n",
        "      c2=list(class2indx[0])\n",
        "      y_tr_c12 = c1 + c2\n",
        "      y_tr_c12.sort()\n",
        "      # print(y_tr_c12)\n",
        "      x_tr_12 = X_tr_c12[y_tr_c12,:,:]\n",
        "      y_tr_12 = r_y_tr[y_tr_c12]\n",
        "      # print(np.shape(x_tr_12))\n",
        "      # print(np.shape(y_tr_12))\n",
        "      # # Find labels to specific class\n",
        "      print(\"Finding labels in testing data\")\n",
        "      class1indx = list(np.where(r_y_ts == com[0]))\n",
        "      class2indx = list(np.where(r_y_ts == com[1]))\n",
        "      c1=list(class1indx[0])\n",
        "      c2=list(class2indx[0])\n",
        "      y_ts_c12 = c1 + c2\n",
        "      y_ts_c12.sort()\n",
        "      # print(y_ts_c12)\n",
        "      x_ts_12 = X_ts_c12[y_ts_c12,:,:]\n",
        "      y_ts_12 = r_y_ts[y_ts_c12]\n",
        "      # print(np.shape(x_ts_12))\n",
        "      # print(np.shape(y_ts_12))\n",
        "      del class1indx, class2indx, c1, c2\n",
        "\n",
        "      # shuffle the training data\n",
        "      indices = np.arange(x_tr_12.shape[0])\n",
        "      np.random.shuffle(indices)\n",
        "      x_tr_12 = x_tr_12[indices]\n",
        "      y_tr_12 = y_tr_12[indices]\n",
        "\n",
        "      # split data of each subject in training and validation\n",
        "      X_train = x_tr_12[0:60,:,2560:4096]\n",
        "      Y_train = y_tr_12[0:60].ravel()\n",
        "      X_val   = x_tr_12[60:,:,2560:4096]\n",
        "      Y_val   = y_tr_12[60:].ravel()\n",
        "      print(Y_val)\n",
        "      print(np.shape(X_train))\n",
        "      print(np.shape(Y_train))\n",
        "      print(np.shape(X_val))\n",
        "      print(np.shape(Y_val))\n",
        "  \n",
        "      # convert labels to one-hot encodings.\n",
        "      Y_train      = np_utils.to_categorical(Y_train-1, num_classes=2)\n",
        "      Y_val       = np_utils.to_categorical(Y_val-1, num_classes=2)\n",
        "      print(Y_val)\n",
        "\n",
        "      kernels, chans, samples = 1, 12, 1536\n",
        "      # convert data to NCHW (trials, kernels, channels, samples) format. Data \n",
        "      # contains 22 channels and 500 time-points. Set the number of kernels to 1.\n",
        "      X_train = X_train.reshape(X_train.shape[0], kernels, chans, samples)\n",
        "      X_val   = X_val.reshape(X_val.shape[0], kernels, chans, samples)\n",
        "   \n",
        "      print('X_train shape:', X_train.shape)\n",
        "      print(X_train.shape[0], 'train samples')\n",
        "      print(X_val.shape[0], 'val samples')\n",
        "\n",
        "      X_test      = x_ts_12[:,:,2560:4096]\n",
        "      Y_test      = y_ts_12[:]\n",
        "      print(np.shape(X_test))\n",
        "      print(np.shape(Y_test))\n",
        "\n",
        "      #convert labels to one-hot encodings.\n",
        "      Y_test      = np_utils.to_categorical(Y_test-1, num_classes=2)\n",
        "\n",
        "      # convert data to NCHW (trials, kernels, channels, samples) format. Data \n",
        "      # contains 22 channels and 500 time-points. Set the number of kernels to 1.\n",
        "      X_test      = X_test.reshape(X_test.shape[0], kernels, chans, samples)\n",
        "\n",
        "      print('X_train shape:', X_test.shape)\n",
        "      print(X_test.shape[0], 'train samples')\n",
        "\n",
        "      # configure the EEGNet-8,2,16 model with kernel length of 32 samples (other \n",
        "      # model configurations may do better, but this is a good starting point)\n",
        "      model = EEGNet(nb_classes = 2, Chans = 12, Samples = 1536,\n",
        "                     dropoutRate = 0.5, kernLength = 25, F1 = 8, \n",
        "                     D = 2, F2 = 16, norm_rate = 0.25, dropoutType = 'Dropout')\n",
        "      \n",
        "      # compile the model and set the optimizers\n",
        "      model.compile(loss='categorical_crossentropy', optimizer='adam', \n",
        "                    metrics = ['accuracy'])\n",
        "\n",
        "      # count number of parameters in the model\n",
        "      numParams    = model.count_params() \n",
        "\n",
        "      # set a valid path for your system to record model checkpoints\n",
        "      checkpointer = ModelCheckpoint(filepath='/tmp/checkpoint.h5', verbose=1,\n",
        "                                     save_best_only=True)\n",
        "  \n",
        "      # the syntax is {class_1:weight_1, class_2:weight_2,...}. Here just setting\n",
        "      # the weights all to be 1\n",
        "      class_weights = {0:1, 1:1}\n",
        "\n",
        "      history = model.fit(X_train, Y_train, batch_size = 16, epochs = 300, \n",
        "                      verbose = 2, validation_data=(X_val, Y_val),\n",
        "                      callbacks=[checkpointer], class_weight = class_weights)\n",
        "  \n",
        "      # Plot training & validation accuracy values\n",
        "      plt.plot(history.history['acc'])\n",
        "      plt.plot(history.history['val_acc'])\n",
        "      plt.title('Model accuracy')\n",
        "      plt.ylabel('Accuracy')\n",
        "      plt.xlabel('Epoch')\n",
        "      plt.legend(['Train', 'Test'], loc='upper left')\n",
        "      plt.show()\n",
        "      figName = 'Accuracy_A0' + str(x) + '.png'  \n",
        "      plt.savefig(figName)\n",
        "\n",
        "      print('\\n# Evaluate on test data')\n",
        "      results = model.evaluate(X_test, Y_test, batch_size=1)\n",
        "      print('test loss, test acc:', results)\n",
        "\n",
        "      loss_all[x - 1, k-1] = results[0]\n",
        "      acc_all[x - 1, k-1] = results[1]\n",
        "\n",
        "      from keras import backend as K \n",
        "      # Do some code, e.g. train and save model\n",
        "      K.clear_session()\n",
        "\n",
        "print(loss_all)\n",
        "print(acc_all)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "EEG_Deep/Data2A/parsed_P01T.mat\n",
            "Filtering of Training Data Finished\n",
            "EEG_Deep/Data2A/parsed_P01E.mat\n",
            "Filtering of Testing Data Finished\n",
            "(1, 2)\n",
            "Finding labels in training data\n",
            "Finding labels in testing data\n",
            "[1 2 2 1 2 1 2 1 2 1 1 1 2 2 1 1 2 1 2 2]\n",
            "(60, 12, 1536)\n",
            "(60,)\n",
            "(20, 12, 1536)\n",
            "(20,)\n",
            "[[1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]]\n",
            "X_train shape: (60, 1, 12, 1536)\n",
            "60 train samples\n",
            "20 val samples\n",
            "(40, 12, 1536)\n",
            "(40, 1)\n",
            "X_train shape: (40, 1, 12, 1536)\n",
            "40 train samples\n",
            "Train on 60 samples, validate on 20 samples\n",
            "Epoch 1/300\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.69069, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 1s - loss: 0.6804 - acc: 0.4667 - val_loss: 0.6907 - val_acc: 0.5000\n",
            "Epoch 2/300\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.69069\n",
            "60/60 - 0s - loss: 0.6855 - acc: 0.5833 - val_loss: 0.6909 - val_acc: 0.5500\n",
            "Epoch 3/300\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.69069\n",
            "60/60 - 0s - loss: 0.6769 - acc: 0.6500 - val_loss: 0.6916 - val_acc: 0.4500\n",
            "Epoch 4/300\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.69069\n",
            "60/60 - 0s - loss: 0.6521 - acc: 0.6833 - val_loss: 0.6915 - val_acc: 0.5500\n",
            "Epoch 5/300\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.69069\n",
            "60/60 - 0s - loss: 0.6279 - acc: 0.8000 - val_loss: 0.6917 - val_acc: 0.7000\n",
            "Epoch 6/300\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.69069\n",
            "60/60 - 0s - loss: 0.6261 - acc: 0.8000 - val_loss: 0.6909 - val_acc: 0.6500\n",
            "Epoch 7/300\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.69069 to 0.69053, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6059 - acc: 0.8333 - val_loss: 0.6905 - val_acc: 0.6500\n",
            "Epoch 8/300\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.69053\n",
            "60/60 - 0s - loss: 0.6028 - acc: 0.7833 - val_loss: 0.6913 - val_acc: 0.6500\n",
            "Epoch 9/300\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.69053\n",
            "60/60 - 0s - loss: 0.5798 - acc: 0.9000 - val_loss: 0.6919 - val_acc: 0.7000\n",
            "Epoch 10/300\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.69053\n",
            "60/60 - 0s - loss: 0.5851 - acc: 0.8167 - val_loss: 0.6923 - val_acc: 0.7000\n",
            "Epoch 11/300\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.69053\n",
            "60/60 - 0s - loss: 0.5600 - acc: 0.8833 - val_loss: 0.6937 - val_acc: 0.5500\n",
            "Epoch 12/300\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.69053\n",
            "60/60 - 0s - loss: 0.5447 - acc: 0.8667 - val_loss: 0.6953 - val_acc: 0.5500\n",
            "Epoch 13/300\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.69053\n",
            "60/60 - 0s - loss: 0.5396 - acc: 0.8500 - val_loss: 0.6980 - val_acc: 0.5500\n",
            "Epoch 14/300\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.69053\n",
            "60/60 - 0s - loss: 0.5106 - acc: 0.9167 - val_loss: 0.7002 - val_acc: 0.5500\n",
            "Epoch 15/300\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.69053\n",
            "60/60 - 0s - loss: 0.5095 - acc: 0.9000 - val_loss: 0.7009 - val_acc: 0.5500\n",
            "Epoch 16/300\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.69053\n",
            "60/60 - 0s - loss: 0.5113 - acc: 0.9167 - val_loss: 0.7027 - val_acc: 0.5500\n",
            "Epoch 17/300\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.69053\n",
            "60/60 - 0s - loss: 0.5019 - acc: 0.8833 - val_loss: 0.7044 - val_acc: 0.4500\n",
            "Epoch 18/300\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.69053\n",
            "60/60 - 0s - loss: 0.5010 - acc: 0.8833 - val_loss: 0.7061 - val_acc: 0.5500\n",
            "Epoch 19/300\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.69053\n",
            "60/60 - 0s - loss: 0.4688 - acc: 0.9500 - val_loss: 0.7071 - val_acc: 0.5500\n",
            "Epoch 20/300\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.69053\n",
            "60/60 - 0s - loss: 0.4856 - acc: 0.9167 - val_loss: 0.7077 - val_acc: 0.5000\n",
            "Epoch 21/300\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.69053\n",
            "60/60 - 0s - loss: 0.4576 - acc: 0.9000 - val_loss: 0.7073 - val_acc: 0.4500\n",
            "Epoch 22/300\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.69053\n",
            "60/60 - 0s - loss: 0.4795 - acc: 0.9000 - val_loss: 0.7064 - val_acc: 0.4000\n",
            "Epoch 23/300\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.69053\n",
            "60/60 - 0s - loss: 0.4734 - acc: 0.9000 - val_loss: 0.7064 - val_acc: 0.4500\n",
            "Epoch 24/300\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.69053\n",
            "60/60 - 0s - loss: 0.4356 - acc: 0.9500 - val_loss: 0.7098 - val_acc: 0.4000\n",
            "Epoch 25/300\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.69053\n",
            "60/60 - 0s - loss: 0.4528 - acc: 0.9000 - val_loss: 0.7134 - val_acc: 0.4500\n",
            "Epoch 26/300\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.69053\n",
            "60/60 - 0s - loss: 0.4404 - acc: 0.9167 - val_loss: 0.7155 - val_acc: 0.4500\n",
            "Epoch 27/300\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.69053\n",
            "60/60 - 0s - loss: 0.4231 - acc: 0.9333 - val_loss: 0.7174 - val_acc: 0.4500\n",
            "Epoch 28/300\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.69053\n",
            "60/60 - 0s - loss: 0.4190 - acc: 0.9167 - val_loss: 0.7219 - val_acc: 0.4500\n",
            "Epoch 29/300\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.69053\n",
            "60/60 - 0s - loss: 0.4300 - acc: 0.9333 - val_loss: 0.7265 - val_acc: 0.4500\n",
            "Epoch 30/300\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.69053\n",
            "60/60 - 0s - loss: 0.4220 - acc: 0.9333 - val_loss: 0.7307 - val_acc: 0.4500\n",
            "Epoch 31/300\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.69053\n",
            "60/60 - 0s - loss: 0.3999 - acc: 0.9500 - val_loss: 0.7334 - val_acc: 0.4500\n",
            "Epoch 32/300\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.69053\n",
            "60/60 - 0s - loss: 0.3791 - acc: 0.9333 - val_loss: 0.7374 - val_acc: 0.4500\n",
            "Epoch 33/300\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.69053\n",
            "60/60 - 0s - loss: 0.3966 - acc: 0.9500 - val_loss: 0.7410 - val_acc: 0.4500\n",
            "Epoch 34/300\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.69053\n",
            "60/60 - 0s - loss: 0.4000 - acc: 0.9500 - val_loss: 0.7423 - val_acc: 0.4500\n",
            "Epoch 35/300\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.69053\n",
            "60/60 - 0s - loss: 0.3660 - acc: 0.9667 - val_loss: 0.7441 - val_acc: 0.4500\n",
            "Epoch 36/300\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.69053\n",
            "60/60 - 0s - loss: 0.3605 - acc: 0.9500 - val_loss: 0.7453 - val_acc: 0.4500\n",
            "Epoch 37/300\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.69053\n",
            "60/60 - 0s - loss: 0.3567 - acc: 0.9667 - val_loss: 0.7504 - val_acc: 0.4500\n",
            "Epoch 38/300\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.69053\n",
            "60/60 - 0s - loss: 0.3765 - acc: 0.9000 - val_loss: 0.7568 - val_acc: 0.5000\n",
            "Epoch 39/300\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.69053\n",
            "60/60 - 0s - loss: 0.3762 - acc: 0.9167 - val_loss: 0.7621 - val_acc: 0.5000\n",
            "Epoch 40/300\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.69053\n",
            "60/60 - 0s - loss: 0.3476 - acc: 0.9833 - val_loss: 0.7673 - val_acc: 0.5000\n",
            "Epoch 41/300\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.69053\n",
            "60/60 - 0s - loss: 0.3602 - acc: 0.9667 - val_loss: 0.7731 - val_acc: 0.5000\n",
            "Epoch 42/300\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.69053\n",
            "60/60 - 0s - loss: 0.3554 - acc: 0.9667 - val_loss: 0.7740 - val_acc: 0.4500\n",
            "Epoch 43/300\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.69053\n",
            "60/60 - 0s - loss: 0.3243 - acc: 0.9500 - val_loss: 0.7812 - val_acc: 0.5500\n",
            "Epoch 44/300\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.69053\n",
            "60/60 - 0s - loss: 0.3052 - acc: 0.9833 - val_loss: 0.7885 - val_acc: 0.5500\n",
            "Epoch 45/300\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.69053\n",
            "60/60 - 0s - loss: 0.3209 - acc: 0.9500 - val_loss: 0.7925 - val_acc: 0.5500\n",
            "Epoch 46/300\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.69053\n",
            "60/60 - 0s - loss: 0.3630 - acc: 0.8833 - val_loss: 0.8016 - val_acc: 0.5500\n",
            "Epoch 47/300\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.69053\n",
            "60/60 - 0s - loss: 0.2912 - acc: 0.9667 - val_loss: 0.8074 - val_acc: 0.5500\n",
            "Epoch 48/300\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.69053\n",
            "60/60 - 0s - loss: 0.2903 - acc: 0.9333 - val_loss: 0.8067 - val_acc: 0.5500\n",
            "Epoch 49/300\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.69053\n",
            "60/60 - 0s - loss: 0.3224 - acc: 0.9333 - val_loss: 0.8155 - val_acc: 0.5500\n",
            "Epoch 50/300\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.69053\n",
            "60/60 - 0s - loss: 0.2891 - acc: 0.9667 - val_loss: 0.8282 - val_acc: 0.5500\n",
            "Epoch 51/300\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.69053\n",
            "60/60 - 0s - loss: 0.3196 - acc: 0.9500 - val_loss: 0.8328 - val_acc: 0.5500\n",
            "Epoch 52/300\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.69053\n",
            "60/60 - 0s - loss: 0.2714 - acc: 0.9333 - val_loss: 0.8400 - val_acc: 0.5500\n",
            "Epoch 53/300\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.69053\n",
            "60/60 - 0s - loss: 0.3163 - acc: 0.9500 - val_loss: 0.8421 - val_acc: 0.5500\n",
            "Epoch 54/300\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.69053\n",
            "60/60 - 0s - loss: 0.2966 - acc: 0.9333 - val_loss: 0.8417 - val_acc: 0.6000\n",
            "Epoch 55/300\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.69053\n",
            "60/60 - 0s - loss: 0.3023 - acc: 0.9167 - val_loss: 0.8350 - val_acc: 0.6500\n",
            "Epoch 56/300\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.69053\n",
            "60/60 - 0s - loss: 0.2628 - acc: 0.9667 - val_loss: 0.8330 - val_acc: 0.6000\n",
            "Epoch 57/300\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.69053\n",
            "60/60 - 0s - loss: 0.2674 - acc: 0.9500 - val_loss: 0.8214 - val_acc: 0.5500\n",
            "Epoch 58/300\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.69053\n",
            "60/60 - 0s - loss: 0.2694 - acc: 0.9333 - val_loss: 0.8007 - val_acc: 0.5500\n",
            "Epoch 59/300\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.69053\n",
            "60/60 - 0s - loss: 0.2419 - acc: 0.9500 - val_loss: 0.7894 - val_acc: 0.5500\n",
            "Epoch 60/300\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.69053\n",
            "60/60 - 0s - loss: 0.2925 - acc: 0.9167 - val_loss: 0.7770 - val_acc: 0.6000\n",
            "Epoch 61/300\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.69053\n",
            "60/60 - 0s - loss: 0.2413 - acc: 0.9833 - val_loss: 0.7751 - val_acc: 0.6500\n",
            "Epoch 62/300\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.69053\n",
            "60/60 - 0s - loss: 0.2698 - acc: 0.9333 - val_loss: 0.7737 - val_acc: 0.6500\n",
            "Epoch 63/300\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.69053\n",
            "60/60 - 0s - loss: 0.2564 - acc: 0.9500 - val_loss: 0.7825 - val_acc: 0.6000\n",
            "Epoch 64/300\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.69053\n",
            "60/60 - 0s - loss: 0.2337 - acc: 0.9667 - val_loss: 0.7982 - val_acc: 0.5500\n",
            "Epoch 65/300\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.69053\n",
            "60/60 - 0s - loss: 0.2101 - acc: 0.9667 - val_loss: 0.7983 - val_acc: 0.6000\n",
            "Epoch 66/300\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.69053\n",
            "60/60 - 0s - loss: 0.2561 - acc: 0.9333 - val_loss: 0.7960 - val_acc: 0.6000\n",
            "Epoch 67/300\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.69053\n",
            "60/60 - 0s - loss: 0.2428 - acc: 0.9167 - val_loss: 0.7698 - val_acc: 0.6000\n",
            "Epoch 68/300\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.69053\n",
            "60/60 - 0s - loss: 0.2454 - acc: 0.9833 - val_loss: 0.7469 - val_acc: 0.5500\n",
            "Epoch 69/300\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.69053\n",
            "60/60 - 0s - loss: 0.2337 - acc: 0.9667 - val_loss: 0.7364 - val_acc: 0.6000\n",
            "Epoch 70/300\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.69053\n",
            "60/60 - 0s - loss: 0.2543 - acc: 0.9667 - val_loss: 0.7371 - val_acc: 0.6000\n",
            "Epoch 71/300\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.69053\n",
            "60/60 - 0s - loss: 0.2929 - acc: 0.9167 - val_loss: 0.7285 - val_acc: 0.6500\n",
            "Epoch 72/300\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.69053\n",
            "60/60 - 0s - loss: 0.2534 - acc: 0.9000 - val_loss: 0.7182 - val_acc: 0.6000\n",
            "Epoch 73/300\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.69053\n",
            "60/60 - 0s - loss: 0.2360 - acc: 0.9500 - val_loss: 0.7299 - val_acc: 0.6500\n",
            "Epoch 74/300\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.69053\n",
            "60/60 - 0s - loss: 0.2723 - acc: 0.9333 - val_loss: 0.7256 - val_acc: 0.6500\n",
            "Epoch 75/300\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.69053\n",
            "60/60 - 0s - loss: 0.1982 - acc: 1.0000 - val_loss: 0.7181 - val_acc: 0.6500\n",
            "Epoch 76/300\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.69053\n",
            "60/60 - 0s - loss: 0.2416 - acc: 0.9667 - val_loss: 0.6982 - val_acc: 0.6500\n",
            "Epoch 77/300\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.69053\n",
            "60/60 - 0s - loss: 0.2175 - acc: 1.0000 - val_loss: 0.6991 - val_acc: 0.6500\n",
            "Epoch 78/300\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.69053\n",
            "60/60 - 0s - loss: 0.2327 - acc: 0.9333 - val_loss: 0.7018 - val_acc: 0.6500\n",
            "Epoch 79/300\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.69053\n",
            "60/60 - 0s - loss: 0.2176 - acc: 0.9833 - val_loss: 0.7010 - val_acc: 0.6500\n",
            "Epoch 80/300\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.69053\n",
            "60/60 - 0s - loss: 0.2287 - acc: 0.9667 - val_loss: 0.6923 - val_acc: 0.6500\n",
            "Epoch 81/300\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.69053\n",
            "60/60 - 0s - loss: 0.1888 - acc: 1.0000 - val_loss: 0.6924 - val_acc: 0.6500\n",
            "Epoch 82/300\n",
            "\n",
            "Epoch 00082: val_loss improved from 0.69053 to 0.68140, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2327 - acc: 0.9833 - val_loss: 0.6814 - val_acc: 0.6500\n",
            "Epoch 83/300\n",
            "\n",
            "Epoch 00083: val_loss improved from 0.68140 to 0.65957, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2187 - acc: 0.9667 - val_loss: 0.6596 - val_acc: 0.6500\n",
            "Epoch 84/300\n",
            "\n",
            "Epoch 00084: val_loss improved from 0.65957 to 0.63853, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2173 - acc: 0.9833 - val_loss: 0.6385 - val_acc: 0.6500\n",
            "Epoch 85/300\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.63853\n",
            "60/60 - 0s - loss: 0.2225 - acc: 0.9667 - val_loss: 0.6447 - val_acc: 0.6500\n",
            "Epoch 86/300\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.63853\n",
            "60/60 - 0s - loss: 0.2091 - acc: 0.9500 - val_loss: 0.6419 - val_acc: 0.7000\n",
            "Epoch 87/300\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.63853\n",
            "60/60 - 0s - loss: 0.2152 - acc: 0.9500 - val_loss: 0.6393 - val_acc: 0.7000\n",
            "Epoch 88/300\n",
            "\n",
            "Epoch 00088: val_loss improved from 0.63853 to 0.63551, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2191 - acc: 0.9667 - val_loss: 0.6355 - val_acc: 0.7000\n",
            "Epoch 89/300\n",
            "\n",
            "Epoch 00089: val_loss improved from 0.63551 to 0.63061, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1949 - acc: 0.9833 - val_loss: 0.6306 - val_acc: 0.6500\n",
            "Epoch 90/300\n",
            "\n",
            "Epoch 00090: val_loss improved from 0.63061 to 0.62447, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1904 - acc: 0.9833 - val_loss: 0.6245 - val_acc: 0.7500\n",
            "Epoch 91/300\n",
            "\n",
            "Epoch 00091: val_loss improved from 0.62447 to 0.62389, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2210 - acc: 0.9667 - val_loss: 0.6239 - val_acc: 0.7500\n",
            "Epoch 92/300\n",
            "\n",
            "Epoch 00092: val_loss improved from 0.62389 to 0.61594, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1860 - acc: 0.9667 - val_loss: 0.6159 - val_acc: 0.7500\n",
            "Epoch 93/300\n",
            "\n",
            "Epoch 00093: val_loss improved from 0.61594 to 0.59435, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1754 - acc: 1.0000 - val_loss: 0.5943 - val_acc: 0.7500\n",
            "Epoch 94/300\n",
            "\n",
            "Epoch 00094: val_loss improved from 0.59435 to 0.58656, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2017 - acc: 0.9833 - val_loss: 0.5866 - val_acc: 0.7500\n",
            "Epoch 95/300\n",
            "\n",
            "Epoch 00095: val_loss improved from 0.58656 to 0.57752, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2078 - acc: 0.9500 - val_loss: 0.5775 - val_acc: 0.6500\n",
            "Epoch 96/300\n",
            "\n",
            "Epoch 00096: val_loss improved from 0.57752 to 0.56593, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1931 - acc: 0.9833 - val_loss: 0.5659 - val_acc: 0.6500\n",
            "Epoch 97/300\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.56593\n",
            "60/60 - 0s - loss: 0.1862 - acc: 1.0000 - val_loss: 0.5672 - val_acc: 0.7000\n",
            "Epoch 98/300\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.56593\n",
            "60/60 - 0s - loss: 0.2104 - acc: 0.9500 - val_loss: 0.5671 - val_acc: 0.6500\n",
            "Epoch 99/300\n",
            "\n",
            "Epoch 00099: val_loss improved from 0.56593 to 0.54689, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1804 - acc: 0.9833 - val_loss: 0.5469 - val_acc: 0.7000\n",
            "Epoch 100/300\n",
            "\n",
            "Epoch 00100: val_loss improved from 0.54689 to 0.54595, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1712 - acc: 1.0000 - val_loss: 0.5459 - val_acc: 0.7000\n",
            "Epoch 101/300\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.54595\n",
            "60/60 - 0s - loss: 0.1772 - acc: 0.9500 - val_loss: 0.5615 - val_acc: 0.7500\n",
            "Epoch 102/300\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.54595\n",
            "60/60 - 0s - loss: 0.1875 - acc: 0.9833 - val_loss: 0.5690 - val_acc: 0.7000\n",
            "Epoch 103/300\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.54595\n",
            "60/60 - 0s - loss: 0.1731 - acc: 0.9833 - val_loss: 0.5700 - val_acc: 0.7000\n",
            "Epoch 104/300\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.54595\n",
            "60/60 - 0s - loss: 0.1689 - acc: 0.9667 - val_loss: 0.5648 - val_acc: 0.7000\n",
            "Epoch 105/300\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 0.54595\n",
            "60/60 - 0s - loss: 0.2049 - acc: 0.9667 - val_loss: 0.5513 - val_acc: 0.7000\n",
            "Epoch 106/300\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 0.54595\n",
            "60/60 - 0s - loss: 0.2021 - acc: 0.9667 - val_loss: 0.5564 - val_acc: 0.7000\n",
            "Epoch 107/300\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.54595\n",
            "60/60 - 0s - loss: 0.1923 - acc: 0.9333 - val_loss: 0.5539 - val_acc: 0.7000\n",
            "Epoch 108/300\n",
            "\n",
            "Epoch 00108: val_loss improved from 0.54595 to 0.53878, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1961 - acc: 0.9833 - val_loss: 0.5388 - val_acc: 0.7500\n",
            "Epoch 109/300\n",
            "\n",
            "Epoch 00109: val_loss improved from 0.53878 to 0.51797, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1859 - acc: 0.9667 - val_loss: 0.5180 - val_acc: 0.7500\n",
            "Epoch 110/300\n",
            "\n",
            "Epoch 00110: val_loss improved from 0.51797 to 0.50313, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1774 - acc: 0.9833 - val_loss: 0.5031 - val_acc: 0.7000\n",
            "Epoch 111/300\n",
            "\n",
            "Epoch 00111: val_loss improved from 0.50313 to 0.49317, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1474 - acc: 1.0000 - val_loss: 0.4932 - val_acc: 0.7000\n",
            "Epoch 112/300\n",
            "\n",
            "Epoch 00112: val_loss improved from 0.49317 to 0.49101, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1742 - acc: 0.9667 - val_loss: 0.4910 - val_acc: 0.7500\n",
            "Epoch 113/300\n",
            "\n",
            "Epoch 00113: val_loss improved from 0.49101 to 0.49033, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1595 - acc: 0.9667 - val_loss: 0.4903 - val_acc: 0.7500\n",
            "Epoch 114/300\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.49033\n",
            "60/60 - 0s - loss: 0.1533 - acc: 0.9833 - val_loss: 0.4985 - val_acc: 0.8000\n",
            "Epoch 115/300\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.49033\n",
            "60/60 - 0s - loss: 0.1739 - acc: 0.9833 - val_loss: 0.5173 - val_acc: 0.7500\n",
            "Epoch 116/300\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.49033\n",
            "60/60 - 0s - loss: 0.1527 - acc: 1.0000 - val_loss: 0.5304 - val_acc: 0.8000\n",
            "Epoch 117/300\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.49033\n",
            "60/60 - 0s - loss: 0.1664 - acc: 0.9833 - val_loss: 0.5351 - val_acc: 0.8000\n",
            "Epoch 118/300\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.49033\n",
            "60/60 - 0s - loss: 0.1578 - acc: 0.9667 - val_loss: 0.5195 - val_acc: 0.8000\n",
            "Epoch 119/300\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 0.49033\n",
            "60/60 - 0s - loss: 0.2003 - acc: 0.9333 - val_loss: 0.5168 - val_acc: 0.8000\n",
            "Epoch 120/300\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.49033\n",
            "60/60 - 0s - loss: 0.2256 - acc: 0.9333 - val_loss: 0.5173 - val_acc: 0.8000\n",
            "Epoch 121/300\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.49033\n",
            "60/60 - 0s - loss: 0.1389 - acc: 0.9833 - val_loss: 0.5124 - val_acc: 0.8000\n",
            "Epoch 122/300\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.49033\n",
            "60/60 - 0s - loss: 0.1414 - acc: 1.0000 - val_loss: 0.5075 - val_acc: 0.8000\n",
            "Epoch 123/300\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 0.49033\n",
            "60/60 - 0s - loss: 0.1611 - acc: 0.9667 - val_loss: 0.4978 - val_acc: 0.8000\n",
            "Epoch 124/300\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 0.49033\n",
            "60/60 - 0s - loss: 0.1925 - acc: 0.9333 - val_loss: 0.4939 - val_acc: 0.8000\n",
            "Epoch 125/300\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 0.49033\n",
            "60/60 - 0s - loss: 0.1580 - acc: 0.9667 - val_loss: 0.4918 - val_acc: 0.8000\n",
            "Epoch 126/300\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 0.49033\n",
            "60/60 - 0s - loss: 0.1531 - acc: 0.9667 - val_loss: 0.4907 - val_acc: 0.8000\n",
            "Epoch 127/300\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 0.49033\n",
            "60/60 - 0s - loss: 0.1638 - acc: 0.9667 - val_loss: 0.5038 - val_acc: 0.8000\n",
            "Epoch 128/300\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 0.49033\n",
            "60/60 - 0s - loss: 0.1370 - acc: 0.9833 - val_loss: 0.5166 - val_acc: 0.8000\n",
            "Epoch 129/300\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.49033\n",
            "60/60 - 0s - loss: 0.1310 - acc: 0.9833 - val_loss: 0.5252 - val_acc: 0.7000\n",
            "Epoch 130/300\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 0.49033\n",
            "60/60 - 0s - loss: 0.1929 - acc: 0.9500 - val_loss: 0.5408 - val_acc: 0.7000\n",
            "Epoch 131/300\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 0.49033\n",
            "60/60 - 0s - loss: 0.1782 - acc: 0.9833 - val_loss: 0.5572 - val_acc: 0.7000\n",
            "Epoch 132/300\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 0.49033\n",
            "60/60 - 0s - loss: 0.1277 - acc: 1.0000 - val_loss: 0.5953 - val_acc: 0.6500\n",
            "Epoch 133/300\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.49033\n",
            "60/60 - 0s - loss: 0.1729 - acc: 0.9667 - val_loss: 0.6130 - val_acc: 0.6500\n",
            "Epoch 134/300\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.49033\n",
            "60/60 - 0s - loss: 0.1455 - acc: 0.9833 - val_loss: 0.5943 - val_acc: 0.6500\n",
            "Epoch 135/300\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 0.49033\n",
            "60/60 - 0s - loss: 0.1812 - acc: 0.9500 - val_loss: 0.5999 - val_acc: 0.7000\n",
            "Epoch 136/300\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.49033\n",
            "60/60 - 0s - loss: 0.1374 - acc: 0.9833 - val_loss: 0.5839 - val_acc: 0.7000\n",
            "Epoch 137/300\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 0.49033\n",
            "60/60 - 0s - loss: 0.1862 - acc: 0.9667 - val_loss: 0.5627 - val_acc: 0.7000\n",
            "Epoch 138/300\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.49033\n",
            "60/60 - 0s - loss: 0.1315 - acc: 0.9833 - val_loss: 0.5643 - val_acc: 0.7500\n",
            "Epoch 139/300\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.49033\n",
            "60/60 - 0s - loss: 0.1134 - acc: 1.0000 - val_loss: 0.5615 - val_acc: 0.7500\n",
            "Epoch 140/300\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 0.49033\n",
            "60/60 - 0s - loss: 0.1420 - acc: 1.0000 - val_loss: 0.5240 - val_acc: 0.8000\n",
            "Epoch 141/300\n",
            "\n",
            "Epoch 00141: val_loss improved from 0.49033 to 0.48684, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1460 - acc: 0.9833 - val_loss: 0.4868 - val_acc: 0.8000\n",
            "Epoch 142/300\n",
            "\n",
            "Epoch 00142: val_loss improved from 0.48684 to 0.47560, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1322 - acc: 0.9667 - val_loss: 0.4756 - val_acc: 0.8000\n",
            "Epoch 143/300\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 0.47560\n",
            "60/60 - 0s - loss: 0.1496 - acc: 0.9833 - val_loss: 0.4844 - val_acc: 0.8000\n",
            "Epoch 144/300\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 0.47560\n",
            "60/60 - 0s - loss: 0.1500 - acc: 0.9833 - val_loss: 0.4907 - val_acc: 0.8000\n",
            "Epoch 145/300\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.47560\n",
            "60/60 - 0s - loss: 0.1303 - acc: 0.9667 - val_loss: 0.5184 - val_acc: 0.8000\n",
            "Epoch 146/300\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.47560\n",
            "60/60 - 0s - loss: 0.1474 - acc: 0.9667 - val_loss: 0.5638 - val_acc: 0.7000\n",
            "Epoch 147/300\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.47560\n",
            "60/60 - 0s - loss: 0.1480 - acc: 1.0000 - val_loss: 0.6100 - val_acc: 0.7000\n",
            "Epoch 148/300\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 0.47560\n",
            "60/60 - 0s - loss: 0.1834 - acc: 0.9500 - val_loss: 0.6550 - val_acc: 0.6500\n",
            "Epoch 149/300\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.47560\n",
            "60/60 - 0s - loss: 0.1224 - acc: 0.9833 - val_loss: 0.6724 - val_acc: 0.6000\n",
            "Epoch 150/300\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.47560\n",
            "60/60 - 0s - loss: 0.1430 - acc: 0.9667 - val_loss: 0.7189 - val_acc: 0.6000\n",
            "Epoch 151/300\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.47560\n",
            "60/60 - 0s - loss: 0.1453 - acc: 0.9667 - val_loss: 0.7597 - val_acc: 0.6000\n",
            "Epoch 152/300\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.47560\n",
            "60/60 - 0s - loss: 0.1671 - acc: 0.9833 - val_loss: 0.7399 - val_acc: 0.6000\n",
            "Epoch 153/300\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 0.47560\n",
            "60/60 - 0s - loss: 0.2270 - acc: 0.9167 - val_loss: 0.7681 - val_acc: 0.5500\n",
            "Epoch 154/300\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.47560\n",
            "60/60 - 0s - loss: 0.1635 - acc: 0.9667 - val_loss: 0.8022 - val_acc: 0.5000\n",
            "Epoch 155/300\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.47560\n",
            "60/60 - 0s - loss: 0.1391 - acc: 0.9833 - val_loss: 0.7887 - val_acc: 0.5000\n",
            "Epoch 156/300\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.47560\n",
            "60/60 - 0s - loss: 0.1531 - acc: 1.0000 - val_loss: 0.8278 - val_acc: 0.5000\n",
            "Epoch 157/300\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.47560\n",
            "60/60 - 0s - loss: 0.1344 - acc: 0.9833 - val_loss: 0.8538 - val_acc: 0.5000\n",
            "Epoch 158/300\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.47560\n",
            "60/60 - 0s - loss: 0.1570 - acc: 0.9833 - val_loss: 0.8192 - val_acc: 0.5000\n",
            "Epoch 159/300\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.47560\n",
            "60/60 - 0s - loss: 0.1581 - acc: 0.9667 - val_loss: 0.7237 - val_acc: 0.6000\n",
            "Epoch 160/300\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.47560\n",
            "60/60 - 0s - loss: 0.1342 - acc: 1.0000 - val_loss: 0.7268 - val_acc: 0.6000\n",
            "Epoch 161/300\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.47560\n",
            "60/60 - 0s - loss: 0.1261 - acc: 0.9833 - val_loss: 0.7054 - val_acc: 0.6000\n",
            "Epoch 162/300\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.47560\n",
            "60/60 - 0s - loss: 0.1228 - acc: 1.0000 - val_loss: 0.6759 - val_acc: 0.6000\n",
            "Epoch 163/300\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.47560\n",
            "60/60 - 0s - loss: 0.1021 - acc: 1.0000 - val_loss: 0.6290 - val_acc: 0.6500\n",
            "Epoch 164/300\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.47560\n",
            "60/60 - 0s - loss: 0.1529 - acc: 0.9500 - val_loss: 0.5915 - val_acc: 0.7000\n",
            "Epoch 165/300\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.47560\n",
            "60/60 - 0s - loss: 0.1316 - acc: 0.9667 - val_loss: 0.5456 - val_acc: 0.7000\n",
            "Epoch 166/300\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 0.47560\n",
            "60/60 - 0s - loss: 0.1288 - acc: 1.0000 - val_loss: 0.4965 - val_acc: 0.8000\n",
            "Epoch 167/300\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.47560\n",
            "60/60 - 0s - loss: 0.1065 - acc: 0.9833 - val_loss: 0.4839 - val_acc: 0.8000\n",
            "Epoch 168/300\n",
            "\n",
            "Epoch 00168: val_loss improved from 0.47560 to 0.47252, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1077 - acc: 1.0000 - val_loss: 0.4725 - val_acc: 0.8500\n",
            "Epoch 169/300\n",
            "\n",
            "Epoch 00169: val_loss improved from 0.47252 to 0.46475, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1091 - acc: 1.0000 - val_loss: 0.4647 - val_acc: 0.8500\n",
            "Epoch 170/300\n",
            "\n",
            "Epoch 00170: val_loss improved from 0.46475 to 0.46130, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1351 - acc: 1.0000 - val_loss: 0.4613 - val_acc: 0.8500\n",
            "Epoch 171/300\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.46130\n",
            "60/60 - 0s - loss: 0.1199 - acc: 1.0000 - val_loss: 0.4654 - val_acc: 0.8500\n",
            "Epoch 172/300\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.46130\n",
            "60/60 - 0s - loss: 0.1614 - acc: 0.9167 - val_loss: 0.5020 - val_acc: 0.8000\n",
            "Epoch 173/300\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.46130\n",
            "60/60 - 0s - loss: 0.1198 - acc: 1.0000 - val_loss: 0.5441 - val_acc: 0.7000\n",
            "Epoch 174/300\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.46130\n",
            "60/60 - 0s - loss: 0.1631 - acc: 0.9500 - val_loss: 0.6416 - val_acc: 0.6000\n",
            "Epoch 175/300\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.46130\n",
            "60/60 - 0s - loss: 0.0939 - acc: 0.9833 - val_loss: 0.6778 - val_acc: 0.6000\n",
            "Epoch 176/300\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 0.46130\n",
            "60/60 - 0s - loss: 0.1183 - acc: 1.0000 - val_loss: 0.6761 - val_acc: 0.6000\n",
            "Epoch 177/300\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.46130\n",
            "60/60 - 0s - loss: 0.0995 - acc: 1.0000 - val_loss: 0.6218 - val_acc: 0.7000\n",
            "Epoch 178/300\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.46130\n",
            "60/60 - 0s - loss: 0.1083 - acc: 1.0000 - val_loss: 0.5658 - val_acc: 0.7000\n",
            "Epoch 179/300\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.46130\n",
            "60/60 - 0s - loss: 0.1195 - acc: 1.0000 - val_loss: 0.5405 - val_acc: 0.7500\n",
            "Epoch 180/300\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.46130\n",
            "60/60 - 0s - loss: 0.1282 - acc: 0.9667 - val_loss: 0.5177 - val_acc: 0.7500\n",
            "Epoch 181/300\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.46130\n",
            "60/60 - 0s - loss: 0.1073 - acc: 1.0000 - val_loss: 0.5022 - val_acc: 0.8000\n",
            "Epoch 182/300\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.46130\n",
            "60/60 - 0s - loss: 0.0856 - acc: 1.0000 - val_loss: 0.4789 - val_acc: 0.8500\n",
            "Epoch 183/300\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.46130\n",
            "60/60 - 0s - loss: 0.1278 - acc: 0.9667 - val_loss: 0.4732 - val_acc: 0.8500\n",
            "Epoch 184/300\n",
            "\n",
            "Epoch 00184: val_loss improved from 0.46130 to 0.45484, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1040 - acc: 0.9833 - val_loss: 0.4548 - val_acc: 0.8500\n",
            "Epoch 185/300\n",
            "\n",
            "Epoch 00185: val_loss improved from 0.45484 to 0.44355, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1450 - acc: 0.9500 - val_loss: 0.4435 - val_acc: 0.8500\n",
            "Epoch 186/300\n",
            "\n",
            "Epoch 00186: val_loss improved from 0.44355 to 0.44014, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1306 - acc: 1.0000 - val_loss: 0.4401 - val_acc: 0.8000\n",
            "Epoch 187/300\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 0.44014\n",
            "60/60 - 0s - loss: 0.1944 - acc: 0.9667 - val_loss: 0.4518 - val_acc: 0.8500\n",
            "Epoch 188/300\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.44014\n",
            "60/60 - 0s - loss: 0.1238 - acc: 0.9667 - val_loss: 0.4434 - val_acc: 0.7500\n",
            "Epoch 189/300\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.44014\n",
            "60/60 - 0s - loss: 0.1099 - acc: 0.9833 - val_loss: 0.4604 - val_acc: 0.8000\n",
            "Epoch 190/300\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.44014\n",
            "60/60 - 0s - loss: 0.0973 - acc: 1.0000 - val_loss: 0.4697 - val_acc: 0.8500\n",
            "Epoch 191/300\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.44014\n",
            "60/60 - 0s - loss: 0.1917 - acc: 0.9667 - val_loss: 0.4884 - val_acc: 0.8500\n",
            "Epoch 192/300\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 0.44014\n",
            "60/60 - 0s - loss: 0.1599 - acc: 0.9667 - val_loss: 0.5096 - val_acc: 0.8500\n",
            "Epoch 193/300\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.44014\n",
            "60/60 - 0s - loss: 0.1492 - acc: 1.0000 - val_loss: 0.5239 - val_acc: 0.7500\n",
            "Epoch 194/300\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.44014\n",
            "60/60 - 0s - loss: 0.0924 - acc: 1.0000 - val_loss: 0.5569 - val_acc: 0.7000\n",
            "Epoch 195/300\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.44014\n",
            "60/60 - 0s - loss: 0.1008 - acc: 1.0000 - val_loss: 0.5654 - val_acc: 0.7000\n",
            "Epoch 196/300\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.44014\n",
            "60/60 - 0s - loss: 0.1032 - acc: 1.0000 - val_loss: 0.5683 - val_acc: 0.7000\n",
            "Epoch 197/300\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.44014\n",
            "60/60 - 0s - loss: 0.1345 - acc: 0.9667 - val_loss: 0.5902 - val_acc: 0.7000\n",
            "Epoch 198/300\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.44014\n",
            "60/60 - 0s - loss: 0.0908 - acc: 1.0000 - val_loss: 0.6644 - val_acc: 0.7000\n",
            "Epoch 199/300\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.44014\n",
            "60/60 - 0s - loss: 0.0723 - acc: 1.0000 - val_loss: 0.7036 - val_acc: 0.6000\n",
            "Epoch 200/300\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.44014\n",
            "60/60 - 0s - loss: 0.1269 - acc: 0.9833 - val_loss: 0.6914 - val_acc: 0.6000\n",
            "Epoch 201/300\n",
            "\n",
            "Epoch 00201: val_loss did not improve from 0.44014\n",
            "60/60 - 0s - loss: 0.0834 - acc: 1.0000 - val_loss: 0.7156 - val_acc: 0.6000\n",
            "Epoch 202/300\n",
            "\n",
            "Epoch 00202: val_loss did not improve from 0.44014\n",
            "60/60 - 0s - loss: 0.0794 - acc: 0.9833 - val_loss: 0.7005 - val_acc: 0.6000\n",
            "Epoch 203/300\n",
            "\n",
            "Epoch 00203: val_loss did not improve from 0.44014\n",
            "60/60 - 0s - loss: 0.0951 - acc: 1.0000 - val_loss: 0.6369 - val_acc: 0.7000\n",
            "Epoch 204/300\n",
            "\n",
            "Epoch 00204: val_loss did not improve from 0.44014\n",
            "60/60 - 0s - loss: 0.0887 - acc: 0.9833 - val_loss: 0.5782 - val_acc: 0.7000\n",
            "Epoch 205/300\n",
            "\n",
            "Epoch 00205: val_loss did not improve from 0.44014\n",
            "60/60 - 0s - loss: 0.0869 - acc: 1.0000 - val_loss: 0.4944 - val_acc: 0.8500\n",
            "Epoch 206/300\n",
            "\n",
            "Epoch 00206: val_loss did not improve from 0.44014\n",
            "60/60 - 0s - loss: 0.1099 - acc: 1.0000 - val_loss: 0.4409 - val_acc: 0.8500\n",
            "Epoch 207/300\n",
            "\n",
            "Epoch 00207: val_loss improved from 0.44014 to 0.41748, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1085 - acc: 0.9667 - val_loss: 0.4175 - val_acc: 0.8500\n",
            "Epoch 208/300\n",
            "\n",
            "Epoch 00208: val_loss improved from 0.41748 to 0.41044, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.0915 - acc: 0.9833 - val_loss: 0.4104 - val_acc: 0.8500\n",
            "Epoch 209/300\n",
            "\n",
            "Epoch 00209: val_loss did not improve from 0.41044\n",
            "60/60 - 0s - loss: 0.1518 - acc: 0.9500 - val_loss: 0.4348 - val_acc: 0.8500\n",
            "Epoch 210/300\n",
            "\n",
            "Epoch 00210: val_loss did not improve from 0.41044\n",
            "60/60 - 0s - loss: 0.0983 - acc: 0.9833 - val_loss: 0.4329 - val_acc: 0.8500\n",
            "Epoch 211/300\n",
            "\n",
            "Epoch 00211: val_loss did not improve from 0.41044\n",
            "60/60 - 0s - loss: 0.1097 - acc: 0.9833 - val_loss: 0.4178 - val_acc: 0.9000\n",
            "Epoch 212/300\n",
            "\n",
            "Epoch 00212: val_loss did not improve from 0.41044\n",
            "60/60 - 0s - loss: 0.0913 - acc: 1.0000 - val_loss: 0.4233 - val_acc: 0.8500\n",
            "Epoch 213/300\n",
            "\n",
            "Epoch 00213: val_loss did not improve from 0.41044\n",
            "60/60 - 0s - loss: 0.1342 - acc: 0.9500 - val_loss: 0.4245 - val_acc: 0.8500\n",
            "Epoch 214/300\n",
            "\n",
            "Epoch 00214: val_loss did not improve from 0.41044\n",
            "60/60 - 0s - loss: 0.0588 - acc: 1.0000 - val_loss: 0.4230 - val_acc: 0.8500\n",
            "Epoch 215/300\n",
            "\n",
            "Epoch 00215: val_loss did not improve from 0.41044\n",
            "60/60 - 0s - loss: 0.0982 - acc: 0.9833 - val_loss: 0.4291 - val_acc: 0.8500\n",
            "Epoch 216/300\n",
            "\n",
            "Epoch 00216: val_loss did not improve from 0.41044\n",
            "60/60 - 0s - loss: 0.1272 - acc: 0.9833 - val_loss: 0.4345 - val_acc: 0.8000\n",
            "Epoch 217/300\n",
            "\n",
            "Epoch 00217: val_loss did not improve from 0.41044\n",
            "60/60 - 0s - loss: 0.0780 - acc: 1.0000 - val_loss: 0.4437 - val_acc: 0.8000\n",
            "Epoch 218/300\n",
            "\n",
            "Epoch 00218: val_loss did not improve from 0.41044\n",
            "60/60 - 0s - loss: 0.0884 - acc: 0.9833 - val_loss: 0.4366 - val_acc: 0.8500\n",
            "Epoch 219/300\n",
            "\n",
            "Epoch 00219: val_loss did not improve from 0.41044\n",
            "60/60 - 0s - loss: 0.1067 - acc: 0.9833 - val_loss: 0.4506 - val_acc: 0.8500\n",
            "Epoch 220/300\n",
            "\n",
            "Epoch 00220: val_loss did not improve from 0.41044\n",
            "60/60 - 0s - loss: 0.0695 - acc: 1.0000 - val_loss: 0.4850 - val_acc: 0.8000\n",
            "Epoch 221/300\n",
            "\n",
            "Epoch 00221: val_loss did not improve from 0.41044\n",
            "60/60 - 0s - loss: 0.0683 - acc: 1.0000 - val_loss: 0.5121 - val_acc: 0.7500\n",
            "Epoch 222/300\n",
            "\n",
            "Epoch 00222: val_loss did not improve from 0.41044\n",
            "60/60 - 0s - loss: 0.0843 - acc: 1.0000 - val_loss: 0.5149 - val_acc: 0.7500\n",
            "Epoch 223/300\n",
            "\n",
            "Epoch 00223: val_loss did not improve from 0.41044\n",
            "60/60 - 0s - loss: 0.1092 - acc: 0.9833 - val_loss: 0.5076 - val_acc: 0.7500\n",
            "Epoch 224/300\n",
            "\n",
            "Epoch 00224: val_loss did not improve from 0.41044\n",
            "60/60 - 0s - loss: 0.0947 - acc: 1.0000 - val_loss: 0.5317 - val_acc: 0.7000\n",
            "Epoch 225/300\n",
            "\n",
            "Epoch 00225: val_loss did not improve from 0.41044\n",
            "60/60 - 0s - loss: 0.0860 - acc: 0.9833 - val_loss: 0.5586 - val_acc: 0.6500\n",
            "Epoch 226/300\n",
            "\n",
            "Epoch 00226: val_loss did not improve from 0.41044\n",
            "60/60 - 0s - loss: 0.1017 - acc: 0.9833 - val_loss: 0.5713 - val_acc: 0.6500\n",
            "Epoch 227/300\n",
            "\n",
            "Epoch 00227: val_loss did not improve from 0.41044\n",
            "60/60 - 0s - loss: 0.0663 - acc: 1.0000 - val_loss: 0.5545 - val_acc: 0.7000\n",
            "Epoch 228/300\n",
            "\n",
            "Epoch 00228: val_loss did not improve from 0.41044\n",
            "60/60 - 0s - loss: 0.0799 - acc: 1.0000 - val_loss: 0.5223 - val_acc: 0.7000\n",
            "Epoch 229/300\n",
            "\n",
            "Epoch 00229: val_loss did not improve from 0.41044\n",
            "60/60 - 0s - loss: 0.0970 - acc: 0.9833 - val_loss: 0.5101 - val_acc: 0.7500\n",
            "Epoch 230/300\n",
            "\n",
            "Epoch 00230: val_loss did not improve from 0.41044\n",
            "60/60 - 0s - loss: 0.1317 - acc: 0.9833 - val_loss: 0.5179 - val_acc: 0.7000\n",
            "Epoch 231/300\n",
            "\n",
            "Epoch 00231: val_loss did not improve from 0.41044\n",
            "60/60 - 0s - loss: 0.0971 - acc: 1.0000 - val_loss: 0.5060 - val_acc: 0.7500\n",
            "Epoch 232/300\n",
            "\n",
            "Epoch 00232: val_loss did not improve from 0.41044\n",
            "60/60 - 0s - loss: 0.0907 - acc: 0.9833 - val_loss: 0.4817 - val_acc: 0.8000\n",
            "Epoch 233/300\n",
            "\n",
            "Epoch 00233: val_loss did not improve from 0.41044\n",
            "60/60 - 0s - loss: 0.0935 - acc: 0.9667 - val_loss: 0.4764 - val_acc: 0.8000\n",
            "Epoch 234/300\n",
            "\n",
            "Epoch 00234: val_loss did not improve from 0.41044\n",
            "60/60 - 0s - loss: 0.0747 - acc: 1.0000 - val_loss: 0.5069 - val_acc: 0.8000\n",
            "Epoch 235/300\n",
            "\n",
            "Epoch 00235: val_loss did not improve from 0.41044\n",
            "60/60 - 0s - loss: 0.0902 - acc: 1.0000 - val_loss: 0.5600 - val_acc: 0.6500\n",
            "Epoch 236/300\n",
            "\n",
            "Epoch 00236: val_loss did not improve from 0.41044\n",
            "60/60 - 0s - loss: 0.1207 - acc: 0.9667 - val_loss: 0.6002 - val_acc: 0.6500\n",
            "Epoch 237/300\n",
            "\n",
            "Epoch 00237: val_loss did not improve from 0.41044\n",
            "60/60 - 0s - loss: 0.0662 - acc: 0.9833 - val_loss: 0.6247 - val_acc: 0.6500\n",
            "Epoch 238/300\n",
            "\n",
            "Epoch 00238: val_loss did not improve from 0.41044\n",
            "60/60 - 0s - loss: 0.0981 - acc: 0.9833 - val_loss: 0.6497 - val_acc: 0.6500\n",
            "Epoch 239/300\n",
            "\n",
            "Epoch 00239: val_loss did not improve from 0.41044\n",
            "60/60 - 0s - loss: 0.1160 - acc: 0.9833 - val_loss: 0.6079 - val_acc: 0.7000\n",
            "Epoch 240/300\n",
            "\n",
            "Epoch 00240: val_loss did not improve from 0.41044\n",
            "60/60 - 0s - loss: 0.0824 - acc: 1.0000 - val_loss: 0.5554 - val_acc: 0.7500\n",
            "Epoch 241/300\n",
            "\n",
            "Epoch 00241: val_loss did not improve from 0.41044\n",
            "60/60 - 0s - loss: 0.0991 - acc: 0.9667 - val_loss: 0.5438 - val_acc: 0.7500\n",
            "Epoch 242/300\n",
            "\n",
            "Epoch 00242: val_loss did not improve from 0.41044\n",
            "60/60 - 0s - loss: 0.0800 - acc: 1.0000 - val_loss: 0.5299 - val_acc: 0.7500\n",
            "Epoch 243/300\n",
            "\n",
            "Epoch 00243: val_loss did not improve from 0.41044\n",
            "60/60 - 0s - loss: 0.1289 - acc: 0.9833 - val_loss: 0.5103 - val_acc: 0.7500\n",
            "Epoch 244/300\n",
            "\n",
            "Epoch 00244: val_loss did not improve from 0.41044\n",
            "60/60 - 0s - loss: 0.1021 - acc: 0.9833 - val_loss: 0.5054 - val_acc: 0.7500\n",
            "Epoch 245/300\n",
            "\n",
            "Epoch 00245: val_loss did not improve from 0.41044\n",
            "60/60 - 0s - loss: 0.0962 - acc: 0.9833 - val_loss: 0.5115 - val_acc: 0.7500\n",
            "Epoch 246/300\n",
            "\n",
            "Epoch 00246: val_loss did not improve from 0.41044\n",
            "60/60 - 0s - loss: 0.1280 - acc: 0.9500 - val_loss: 0.5540 - val_acc: 0.7500\n",
            "Epoch 247/300\n",
            "\n",
            "Epoch 00247: val_loss did not improve from 0.41044\n",
            "60/60 - 0s - loss: 0.1742 - acc: 0.9500 - val_loss: 0.5860 - val_acc: 0.7500\n",
            "Epoch 248/300\n",
            "\n",
            "Epoch 00248: val_loss did not improve from 0.41044\n",
            "60/60 - 0s - loss: 0.0558 - acc: 1.0000 - val_loss: 0.6322 - val_acc: 0.7000\n",
            "Epoch 249/300\n",
            "\n",
            "Epoch 00249: val_loss did not improve from 0.41044\n",
            "60/60 - 0s - loss: 0.0763 - acc: 1.0000 - val_loss: 0.6121 - val_acc: 0.7000\n",
            "Epoch 250/300\n",
            "\n",
            "Epoch 00250: val_loss did not improve from 0.41044\n",
            "60/60 - 0s - loss: 0.1015 - acc: 1.0000 - val_loss: 0.6066 - val_acc: 0.7000\n",
            "Epoch 251/300\n",
            "\n",
            "Epoch 00251: val_loss did not improve from 0.41044\n",
            "60/60 - 0s - loss: 0.0928 - acc: 1.0000 - val_loss: 0.6130 - val_acc: 0.7000\n",
            "Epoch 252/300\n",
            "\n",
            "Epoch 00252: val_loss did not improve from 0.41044\n",
            "60/60 - 0s - loss: 0.0783 - acc: 1.0000 - val_loss: 0.5853 - val_acc: 0.7000\n",
            "Epoch 253/300\n",
            "\n",
            "Epoch 00253: val_loss did not improve from 0.41044\n",
            "60/60 - 0s - loss: 0.1252 - acc: 0.9667 - val_loss: 0.5484 - val_acc: 0.7000\n",
            "Epoch 254/300\n",
            "\n",
            "Epoch 00254: val_loss did not improve from 0.41044\n",
            "60/60 - 0s - loss: 0.0647 - acc: 1.0000 - val_loss: 0.5601 - val_acc: 0.7000\n",
            "Epoch 255/300\n",
            "\n",
            "Epoch 00255: val_loss did not improve from 0.41044\n",
            "60/60 - 0s - loss: 0.0911 - acc: 1.0000 - val_loss: 0.5233 - val_acc: 0.7500\n",
            "Epoch 256/300\n",
            "\n",
            "Epoch 00256: val_loss did not improve from 0.41044\n",
            "60/60 - 0s - loss: 0.0665 - acc: 1.0000 - val_loss: 0.4618 - val_acc: 0.7500\n",
            "Epoch 257/300\n",
            "\n",
            "Epoch 00257: val_loss did not improve from 0.41044\n",
            "60/60 - 0s - loss: 0.0824 - acc: 1.0000 - val_loss: 0.4336 - val_acc: 0.9000\n",
            "Epoch 258/300\n",
            "\n",
            "Epoch 00258: val_loss did not improve from 0.41044\n",
            "60/60 - 0s - loss: 0.0832 - acc: 1.0000 - val_loss: 0.4312 - val_acc: 0.8500\n",
            "Epoch 259/300\n",
            "\n",
            "Epoch 00259: val_loss did not improve from 0.41044\n",
            "60/60 - 0s - loss: 0.0900 - acc: 0.9833 - val_loss: 0.4283 - val_acc: 0.8000\n",
            "Epoch 260/300\n",
            "\n",
            "Epoch 00260: val_loss did not improve from 0.41044\n",
            "60/60 - 0s - loss: 0.0893 - acc: 0.9833 - val_loss: 0.4369 - val_acc: 0.8000\n",
            "Epoch 261/300\n",
            "\n",
            "Epoch 00261: val_loss did not improve from 0.41044\n",
            "60/60 - 0s - loss: 0.0642 - acc: 1.0000 - val_loss: 0.4344 - val_acc: 0.8000\n",
            "Epoch 262/300\n",
            "\n",
            "Epoch 00262: val_loss did not improve from 0.41044\n",
            "60/60 - 0s - loss: 0.0701 - acc: 1.0000 - val_loss: 0.4118 - val_acc: 0.8000\n",
            "Epoch 263/300\n",
            "\n",
            "Epoch 00263: val_loss did not improve from 0.41044\n",
            "60/60 - 0s - loss: 0.0896 - acc: 0.9667 - val_loss: 0.4230 - val_acc: 0.8000\n",
            "Epoch 264/300\n",
            "\n",
            "Epoch 00264: val_loss did not improve from 0.41044\n",
            "60/60 - 0s - loss: 0.1100 - acc: 1.0000 - val_loss: 0.4388 - val_acc: 0.8000\n",
            "Epoch 265/300\n",
            "\n",
            "Epoch 00265: val_loss did not improve from 0.41044\n",
            "60/60 - 0s - loss: 0.1032 - acc: 0.9667 - val_loss: 0.4433 - val_acc: 0.8000\n",
            "Epoch 266/300\n",
            "\n",
            "Epoch 00266: val_loss did not improve from 0.41044\n",
            "60/60 - 0s - loss: 0.1069 - acc: 0.9833 - val_loss: 0.4763 - val_acc: 0.8500\n",
            "Epoch 267/300\n",
            "\n",
            "Epoch 00267: val_loss did not improve from 0.41044\n",
            "60/60 - 0s - loss: 0.0964 - acc: 1.0000 - val_loss: 0.5063 - val_acc: 0.7500\n",
            "Epoch 268/300\n",
            "\n",
            "Epoch 00268: val_loss did not improve from 0.41044\n",
            "60/60 - 0s - loss: 0.0633 - acc: 1.0000 - val_loss: 0.5496 - val_acc: 0.7500\n",
            "Epoch 269/300\n",
            "\n",
            "Epoch 00269: val_loss did not improve from 0.41044\n",
            "60/60 - 0s - loss: 0.1357 - acc: 0.9333 - val_loss: 0.4783 - val_acc: 0.8000\n",
            "Epoch 270/300\n",
            "\n",
            "Epoch 00270: val_loss improved from 0.41044 to 0.40598, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.0581 - acc: 1.0000 - val_loss: 0.4060 - val_acc: 0.8500\n",
            "Epoch 271/300\n",
            "\n",
            "Epoch 00271: val_loss improved from 0.40598 to 0.39350, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.0887 - acc: 0.9833 - val_loss: 0.3935 - val_acc: 0.8500\n",
            "Epoch 272/300\n",
            "\n",
            "Epoch 00272: val_loss did not improve from 0.39350\n",
            "60/60 - 0s - loss: 0.1213 - acc: 0.9500 - val_loss: 0.4097 - val_acc: 0.8500\n",
            "Epoch 273/300\n",
            "\n",
            "Epoch 00273: val_loss did not improve from 0.39350\n",
            "60/60 - 0s - loss: 0.0886 - acc: 0.9833 - val_loss: 0.4438 - val_acc: 0.8500\n",
            "Epoch 274/300\n",
            "\n",
            "Epoch 00274: val_loss did not improve from 0.39350\n",
            "60/60 - 0s - loss: 0.1012 - acc: 0.9833 - val_loss: 0.4899 - val_acc: 0.8500\n",
            "Epoch 275/300\n",
            "\n",
            "Epoch 00275: val_loss did not improve from 0.39350\n",
            "60/60 - 0s - loss: 0.1318 - acc: 0.9833 - val_loss: 0.5039 - val_acc: 0.8000\n",
            "Epoch 276/300\n",
            "\n",
            "Epoch 00276: val_loss did not improve from 0.39350\n",
            "60/60 - 0s - loss: 0.0762 - acc: 1.0000 - val_loss: 0.6226 - val_acc: 0.6500\n",
            "Epoch 277/300\n",
            "\n",
            "Epoch 00277: val_loss did not improve from 0.39350\n",
            "60/60 - 0s - loss: 0.0789 - acc: 1.0000 - val_loss: 0.6943 - val_acc: 0.5500\n",
            "Epoch 278/300\n",
            "\n",
            "Epoch 00278: val_loss did not improve from 0.39350\n",
            "60/60 - 0s - loss: 0.0862 - acc: 1.0000 - val_loss: 0.6875 - val_acc: 0.6000\n",
            "Epoch 279/300\n",
            "\n",
            "Epoch 00279: val_loss did not improve from 0.39350\n",
            "60/60 - 0s - loss: 0.1094 - acc: 0.9833 - val_loss: 0.6413 - val_acc: 0.7000\n",
            "Epoch 280/300\n",
            "\n",
            "Epoch 00280: val_loss did not improve from 0.39350\n",
            "60/60 - 0s - loss: 0.0627 - acc: 0.9833 - val_loss: 0.5557 - val_acc: 0.7500\n",
            "Epoch 281/300\n",
            "\n",
            "Epoch 00281: val_loss did not improve from 0.39350\n",
            "60/60 - 0s - loss: 0.0610 - acc: 1.0000 - val_loss: 0.4696 - val_acc: 0.8000\n",
            "Epoch 282/300\n",
            "\n",
            "Epoch 00282: val_loss did not improve from 0.39350\n",
            "60/60 - 0s - loss: 0.0561 - acc: 1.0000 - val_loss: 0.4291 - val_acc: 0.8000\n",
            "Epoch 283/300\n",
            "\n",
            "Epoch 00283: val_loss did not improve from 0.39350\n",
            "60/60 - 0s - loss: 0.0973 - acc: 1.0000 - val_loss: 0.4233 - val_acc: 0.8500\n",
            "Epoch 284/300\n",
            "\n",
            "Epoch 00284: val_loss did not improve from 0.39350\n",
            "60/60 - 0s - loss: 0.0801 - acc: 1.0000 - val_loss: 0.4346 - val_acc: 0.8500\n",
            "Epoch 285/300\n",
            "\n",
            "Epoch 00285: val_loss did not improve from 0.39350\n",
            "60/60 - 0s - loss: 0.0730 - acc: 0.9833 - val_loss: 0.4859 - val_acc: 0.7500\n",
            "Epoch 286/300\n",
            "\n",
            "Epoch 00286: val_loss did not improve from 0.39350\n",
            "60/60 - 0s - loss: 0.0945 - acc: 0.9833 - val_loss: 0.5220 - val_acc: 0.7500\n",
            "Epoch 287/300\n",
            "\n",
            "Epoch 00287: val_loss did not improve from 0.39350\n",
            "60/60 - 0s - loss: 0.0728 - acc: 1.0000 - val_loss: 0.5620 - val_acc: 0.6500\n",
            "Epoch 288/300\n",
            "\n",
            "Epoch 00288: val_loss did not improve from 0.39350\n",
            "60/60 - 0s - loss: 0.0831 - acc: 0.9833 - val_loss: 0.5464 - val_acc: 0.7500\n",
            "Epoch 289/300\n",
            "\n",
            "Epoch 00289: val_loss did not improve from 0.39350\n",
            "60/60 - 0s - loss: 0.0744 - acc: 1.0000 - val_loss: 0.4999 - val_acc: 0.8000\n",
            "Epoch 290/300\n",
            "\n",
            "Epoch 00290: val_loss did not improve from 0.39350\n",
            "60/60 - 0s - loss: 0.0910 - acc: 1.0000 - val_loss: 0.4622 - val_acc: 0.8000\n",
            "Epoch 291/300\n",
            "\n",
            "Epoch 00291: val_loss did not improve from 0.39350\n",
            "60/60 - 0s - loss: 0.1101 - acc: 0.9667 - val_loss: 0.4641 - val_acc: 0.8000\n",
            "Epoch 292/300\n",
            "\n",
            "Epoch 00292: val_loss did not improve from 0.39350\n",
            "60/60 - 0s - loss: 0.0768 - acc: 0.9833 - val_loss: 0.4960 - val_acc: 0.8000\n",
            "Epoch 293/300\n",
            "\n",
            "Epoch 00293: val_loss did not improve from 0.39350\n",
            "60/60 - 0s - loss: 0.0716 - acc: 1.0000 - val_loss: 0.5591 - val_acc: 0.7500\n",
            "Epoch 294/300\n",
            "\n",
            "Epoch 00294: val_loss did not improve from 0.39350\n",
            "60/60 - 0s - loss: 0.0668 - acc: 0.9833 - val_loss: 0.5574 - val_acc: 0.7500\n",
            "Epoch 295/300\n",
            "\n",
            "Epoch 00295: val_loss did not improve from 0.39350\n",
            "60/60 - 0s - loss: 0.0724 - acc: 1.0000 - val_loss: 0.5573 - val_acc: 0.7500\n",
            "Epoch 296/300\n",
            "\n",
            "Epoch 00296: val_loss did not improve from 0.39350\n",
            "60/60 - 0s - loss: 0.0671 - acc: 0.9833 - val_loss: 0.5384 - val_acc: 0.7500\n",
            "Epoch 297/300\n",
            "\n",
            "Epoch 00297: val_loss did not improve from 0.39350\n",
            "60/60 - 0s - loss: 0.0923 - acc: 0.9833 - val_loss: 0.4986 - val_acc: 0.7500\n",
            "Epoch 298/300\n",
            "\n",
            "Epoch 00298: val_loss did not improve from 0.39350\n",
            "60/60 - 0s - loss: 0.0824 - acc: 0.9833 - val_loss: 0.4342 - val_acc: 0.8500\n",
            "Epoch 299/300\n",
            "\n",
            "Epoch 00299: val_loss did not improve from 0.39350\n",
            "60/60 - 0s - loss: 0.0747 - acc: 1.0000 - val_loss: 0.4226 - val_acc: 0.8500\n",
            "Epoch 300/300\n",
            "\n",
            "Epoch 00300: val_loss did not improve from 0.39350\n",
            "60/60 - 0s - loss: 0.0478 - acc: 1.0000 - val_loss: 0.4201 - val_acc: 0.8500\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "# Evaluate on test data\n",
            "40/40 [==============================] - 0s 2ms/sample - loss: 0.5284 - acc: 0.7750\n",
            "test loss, test acc: [0.528361277654767, 0.775]\n",
            "EEG_Deep/Data2A/parsed_P02T.mat\n",
            "Filtering of Training Data Finished\n",
            "EEG_Deep/Data2A/parsed_P02E.mat\n",
            "Filtering of Testing Data Finished\n",
            "(1, 2)\n",
            "Finding labels in training data\n",
            "Finding labels in testing data\n",
            "[1 1 2 2 1 2 2 1 2 2 2 1 1 2 2 1 1 2 2 1]\n",
            "(60, 12, 1536)\n",
            "(60,)\n",
            "(20, 12, 1536)\n",
            "(20,)\n",
            "[[1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]]\n",
            "X_train shape: (60, 1, 12, 1536)\n",
            "60 train samples\n",
            "20 val samples\n",
            "(40, 12, 1536)\n",
            "(40, 1)\n",
            "X_train shape: (40, 1, 12, 1536)\n",
            "40 train samples\n",
            "Train on 60 samples, validate on 20 samples\n",
            "Epoch 1/300\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.69027, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 1s - loss: 0.6903 - acc: 0.6333 - val_loss: 0.6903 - val_acc: 0.5500\n",
            "Epoch 2/300\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.69027\n",
            "60/60 - 0s - loss: 0.6651 - acc: 0.6167 - val_loss: 0.6904 - val_acc: 0.5000\n",
            "Epoch 3/300\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.69027 to 0.69016, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6690 - acc: 0.6500 - val_loss: 0.6902 - val_acc: 0.5500\n",
            "Epoch 4/300\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.69016 to 0.68922, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6357 - acc: 0.7333 - val_loss: 0.6892 - val_acc: 0.5000\n",
            "Epoch 5/300\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.68922 to 0.68859, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6279 - acc: 0.7833 - val_loss: 0.6886 - val_acc: 0.5500\n",
            "Epoch 6/300\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.68859 to 0.68698, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5862 - acc: 0.8167 - val_loss: 0.6870 - val_acc: 0.5500\n",
            "Epoch 7/300\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.68698 to 0.68567, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6165 - acc: 0.7833 - val_loss: 0.6857 - val_acc: 0.5500\n",
            "Epoch 8/300\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.68567 to 0.68404, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5871 - acc: 0.8667 - val_loss: 0.6840 - val_acc: 0.6000\n",
            "Epoch 9/300\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.68404 to 0.68204, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5753 - acc: 0.9167 - val_loss: 0.6820 - val_acc: 0.6500\n",
            "Epoch 10/300\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.68204 to 0.68073, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5534 - acc: 0.8833 - val_loss: 0.6807 - val_acc: 0.5500\n",
            "Epoch 11/300\n",
            "\n",
            "Epoch 00011: val_loss improved from 0.68073 to 0.67925, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5539 - acc: 0.8833 - val_loss: 0.6793 - val_acc: 0.6500\n",
            "Epoch 12/300\n",
            "\n",
            "Epoch 00012: val_loss improved from 0.67925 to 0.67682, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5472 - acc: 0.8500 - val_loss: 0.6768 - val_acc: 0.6000\n",
            "Epoch 13/300\n",
            "\n",
            "Epoch 00013: val_loss improved from 0.67682 to 0.67390, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5331 - acc: 0.9167 - val_loss: 0.6739 - val_acc: 0.6000\n",
            "Epoch 14/300\n",
            "\n",
            "Epoch 00014: val_loss improved from 0.67390 to 0.67125, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5231 - acc: 0.9167 - val_loss: 0.6712 - val_acc: 0.5500\n",
            "Epoch 15/300\n",
            "\n",
            "Epoch 00015: val_loss improved from 0.67125 to 0.66904, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4847 - acc: 0.9333 - val_loss: 0.6690 - val_acc: 0.6500\n",
            "Epoch 16/300\n",
            "\n",
            "Epoch 00016: val_loss improved from 0.66904 to 0.66695, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4790 - acc: 0.9500 - val_loss: 0.6669 - val_acc: 0.6500\n",
            "Epoch 17/300\n",
            "\n",
            "Epoch 00017: val_loss improved from 0.66695 to 0.66459, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4564 - acc: 0.9833 - val_loss: 0.6646 - val_acc: 0.6500\n",
            "Epoch 18/300\n",
            "\n",
            "Epoch 00018: val_loss improved from 0.66459 to 0.66169, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4943 - acc: 0.8833 - val_loss: 0.6617 - val_acc: 0.6500\n",
            "Epoch 19/300\n",
            "\n",
            "Epoch 00019: val_loss improved from 0.66169 to 0.65942, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4534 - acc: 0.9167 - val_loss: 0.6594 - val_acc: 0.7000\n",
            "Epoch 20/300\n",
            "\n",
            "Epoch 00020: val_loss improved from 0.65942 to 0.65701, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4569 - acc: 0.8833 - val_loss: 0.6570 - val_acc: 0.7000\n",
            "Epoch 21/300\n",
            "\n",
            "Epoch 00021: val_loss improved from 0.65701 to 0.65464, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4459 - acc: 0.9500 - val_loss: 0.6546 - val_acc: 0.6500\n",
            "Epoch 22/300\n",
            "\n",
            "Epoch 00022: val_loss improved from 0.65464 to 0.65148, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3945 - acc: 1.0000 - val_loss: 0.6515 - val_acc: 0.6000\n",
            "Epoch 23/300\n",
            "\n",
            "Epoch 00023: val_loss improved from 0.65148 to 0.64825, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4034 - acc: 0.9333 - val_loss: 0.6483 - val_acc: 0.5500\n",
            "Epoch 24/300\n",
            "\n",
            "Epoch 00024: val_loss improved from 0.64825 to 0.64508, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4085 - acc: 0.9167 - val_loss: 0.6451 - val_acc: 0.6000\n",
            "Epoch 25/300\n",
            "\n",
            "Epoch 00025: val_loss improved from 0.64508 to 0.64122, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3737 - acc: 0.9833 - val_loss: 0.6412 - val_acc: 0.6000\n",
            "Epoch 26/300\n",
            "\n",
            "Epoch 00026: val_loss improved from 0.64122 to 0.63721, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3854 - acc: 0.9167 - val_loss: 0.6372 - val_acc: 0.6000\n",
            "Epoch 27/300\n",
            "\n",
            "Epoch 00027: val_loss improved from 0.63721 to 0.63210, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3759 - acc: 0.9833 - val_loss: 0.6321 - val_acc: 0.5500\n",
            "Epoch 28/300\n",
            "\n",
            "Epoch 00028: val_loss improved from 0.63210 to 0.62599, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4078 - acc: 0.9167 - val_loss: 0.6260 - val_acc: 0.5500\n",
            "Epoch 29/300\n",
            "\n",
            "Epoch 00029: val_loss improved from 0.62599 to 0.62199, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3484 - acc: 0.9000 - val_loss: 0.6220 - val_acc: 0.5500\n",
            "Epoch 30/300\n",
            "\n",
            "Epoch 00030: val_loss improved from 0.62199 to 0.61627, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3466 - acc: 0.9000 - val_loss: 0.6163 - val_acc: 0.6000\n",
            "Epoch 31/300\n",
            "\n",
            "Epoch 00031: val_loss improved from 0.61627 to 0.61144, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3297 - acc: 0.9500 - val_loss: 0.6114 - val_acc: 0.5500\n",
            "Epoch 32/300\n",
            "\n",
            "Epoch 00032: val_loss improved from 0.61144 to 0.60866, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3267 - acc: 0.9167 - val_loss: 0.6087 - val_acc: 0.5500\n",
            "Epoch 33/300\n",
            "\n",
            "Epoch 00033: val_loss improved from 0.60866 to 0.60363, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3116 - acc: 0.9333 - val_loss: 0.6036 - val_acc: 0.5500\n",
            "Epoch 34/300\n",
            "\n",
            "Epoch 00034: val_loss improved from 0.60363 to 0.59706, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3074 - acc: 0.9167 - val_loss: 0.5971 - val_acc: 0.6500\n",
            "Epoch 35/300\n",
            "\n",
            "Epoch 00035: val_loss improved from 0.59706 to 0.59156, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3195 - acc: 0.9167 - val_loss: 0.5916 - val_acc: 0.6500\n",
            "Epoch 36/300\n",
            "\n",
            "Epoch 00036: val_loss improved from 0.59156 to 0.58605, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2945 - acc: 0.9500 - val_loss: 0.5860 - val_acc: 0.7000\n",
            "Epoch 37/300\n",
            "\n",
            "Epoch 00037: val_loss improved from 0.58605 to 0.57729, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2736 - acc: 0.9667 - val_loss: 0.5773 - val_acc: 0.7000\n",
            "Epoch 38/300\n",
            "\n",
            "Epoch 00038: val_loss improved from 0.57729 to 0.57013, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2507 - acc: 0.9833 - val_loss: 0.5701 - val_acc: 0.7000\n",
            "Epoch 39/300\n",
            "\n",
            "Epoch 00039: val_loss improved from 0.57013 to 0.55804, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2633 - acc: 0.9667 - val_loss: 0.5580 - val_acc: 0.7500\n",
            "Epoch 40/300\n",
            "\n",
            "Epoch 00040: val_loss improved from 0.55804 to 0.54206, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2801 - acc: 0.9167 - val_loss: 0.5421 - val_acc: 0.8000\n",
            "Epoch 41/300\n",
            "\n",
            "Epoch 00041: val_loss improved from 0.54206 to 0.52347, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2544 - acc: 0.9333 - val_loss: 0.5235 - val_acc: 0.8500\n",
            "Epoch 42/300\n",
            "\n",
            "Epoch 00042: val_loss improved from 0.52347 to 0.51050, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2563 - acc: 0.9500 - val_loss: 0.5105 - val_acc: 0.8500\n",
            "Epoch 43/300\n",
            "\n",
            "Epoch 00043: val_loss improved from 0.51050 to 0.50840, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3195 - acc: 0.9333 - val_loss: 0.5084 - val_acc: 0.8500\n",
            "Epoch 44/300\n",
            "\n",
            "Epoch 00044: val_loss improved from 0.50840 to 0.50721, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2273 - acc: 0.9667 - val_loss: 0.5072 - val_acc: 0.8500\n",
            "Epoch 45/300\n",
            "\n",
            "Epoch 00045: val_loss improved from 0.50721 to 0.50715, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2329 - acc: 0.9333 - val_loss: 0.5072 - val_acc: 0.8500\n",
            "Epoch 46/300\n",
            "\n",
            "Epoch 00046: val_loss improved from 0.50715 to 0.50679, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2557 - acc: 0.9667 - val_loss: 0.5068 - val_acc: 0.8500\n",
            "Epoch 47/300\n",
            "\n",
            "Epoch 00047: val_loss improved from 0.50679 to 0.50253, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2009 - acc: 0.9833 - val_loss: 0.5025 - val_acc: 0.8500\n",
            "Epoch 48/300\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.50253\n",
            "60/60 - 0s - loss: 0.2496 - acc: 0.9500 - val_loss: 0.5031 - val_acc: 0.8500\n",
            "Epoch 49/300\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.50253\n",
            "60/60 - 0s - loss: 0.2059 - acc: 0.9500 - val_loss: 0.5031 - val_acc: 0.8500\n",
            "Epoch 50/300\n",
            "\n",
            "Epoch 00050: val_loss improved from 0.50253 to 0.49908, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2321 - acc: 0.9500 - val_loss: 0.4991 - val_acc: 0.8500\n",
            "Epoch 51/300\n",
            "\n",
            "Epoch 00051: val_loss improved from 0.49908 to 0.49669, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2175 - acc: 0.9333 - val_loss: 0.4967 - val_acc: 0.8500\n",
            "Epoch 52/300\n",
            "\n",
            "Epoch 00052: val_loss improved from 0.49669 to 0.49032, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2259 - acc: 0.9500 - val_loss: 0.4903 - val_acc: 0.8500\n",
            "Epoch 53/300\n",
            "\n",
            "Epoch 00053: val_loss improved from 0.49032 to 0.48193, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2120 - acc: 0.9833 - val_loss: 0.4819 - val_acc: 0.8500\n",
            "Epoch 54/300\n",
            "\n",
            "Epoch 00054: val_loss improved from 0.48193 to 0.47278, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2033 - acc: 0.9833 - val_loss: 0.4728 - val_acc: 0.8500\n",
            "Epoch 55/300\n",
            "\n",
            "Epoch 00055: val_loss improved from 0.47278 to 0.47022, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2236 - acc: 0.9500 - val_loss: 0.4702 - val_acc: 0.8500\n",
            "Epoch 56/300\n",
            "\n",
            "Epoch 00056: val_loss improved from 0.47022 to 0.46799, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2511 - acc: 0.9500 - val_loss: 0.4680 - val_acc: 0.8500\n",
            "Epoch 57/300\n",
            "\n",
            "Epoch 00057: val_loss improved from 0.46799 to 0.46445, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2262 - acc: 0.9667 - val_loss: 0.4645 - val_acc: 0.8500\n",
            "Epoch 58/300\n",
            "\n",
            "Epoch 00058: val_loss improved from 0.46445 to 0.46220, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2063 - acc: 0.9833 - val_loss: 0.4622 - val_acc: 0.8500\n",
            "Epoch 59/300\n",
            "\n",
            "Epoch 00059: val_loss improved from 0.46220 to 0.46192, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1905 - acc: 0.9833 - val_loss: 0.4619 - val_acc: 0.8500\n",
            "Epoch 60/300\n",
            "\n",
            "Epoch 00060: val_loss improved from 0.46192 to 0.45433, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2042 - acc: 0.9333 - val_loss: 0.4543 - val_acc: 0.8500\n",
            "Epoch 61/300\n",
            "\n",
            "Epoch 00061: val_loss improved from 0.45433 to 0.44397, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2191 - acc: 0.9167 - val_loss: 0.4440 - val_acc: 0.8500\n",
            "Epoch 62/300\n",
            "\n",
            "Epoch 00062: val_loss improved from 0.44397 to 0.43989, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2202 - acc: 0.9500 - val_loss: 0.4399 - val_acc: 0.8500\n",
            "Epoch 63/300\n",
            "\n",
            "Epoch 00063: val_loss improved from 0.43989 to 0.43142, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2042 - acc: 0.9333 - val_loss: 0.4314 - val_acc: 0.8500\n",
            "Epoch 64/300\n",
            "\n",
            "Epoch 00064: val_loss improved from 0.43142 to 0.42226, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1878 - acc: 0.9500 - val_loss: 0.4223 - val_acc: 0.8500\n",
            "Epoch 65/300\n",
            "\n",
            "Epoch 00065: val_loss improved from 0.42226 to 0.41599, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1903 - acc: 0.9667 - val_loss: 0.4160 - val_acc: 0.8500\n",
            "Epoch 66/300\n",
            "\n",
            "Epoch 00066: val_loss improved from 0.41599 to 0.41278, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2204 - acc: 0.9833 - val_loss: 0.4128 - val_acc: 0.8500\n",
            "Epoch 67/300\n",
            "\n",
            "Epoch 00067: val_loss improved from 0.41278 to 0.40940, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1551 - acc: 1.0000 - val_loss: 0.4094 - val_acc: 0.8500\n",
            "Epoch 68/300\n",
            "\n",
            "Epoch 00068: val_loss improved from 0.40940 to 0.40403, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1687 - acc: 0.9833 - val_loss: 0.4040 - val_acc: 0.8500\n",
            "Epoch 69/300\n",
            "\n",
            "Epoch 00069: val_loss improved from 0.40403 to 0.39761, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1751 - acc: 0.9667 - val_loss: 0.3976 - val_acc: 0.8500\n",
            "Epoch 70/300\n",
            "\n",
            "Epoch 00070: val_loss improved from 0.39761 to 0.39232, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2014 - acc: 0.9833 - val_loss: 0.3923 - val_acc: 0.8500\n",
            "Epoch 71/300\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.39232\n",
            "60/60 - 0s - loss: 0.2344 - acc: 0.9333 - val_loss: 0.3949 - val_acc: 0.8500\n",
            "Epoch 72/300\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.39232\n",
            "60/60 - 0s - loss: 0.1793 - acc: 0.9667 - val_loss: 0.3966 - val_acc: 0.8500\n",
            "Epoch 73/300\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.39232\n",
            "60/60 - 0s - loss: 0.1642 - acc: 0.9500 - val_loss: 0.3930 - val_acc: 0.8500\n",
            "Epoch 74/300\n",
            "\n",
            "Epoch 00074: val_loss improved from 0.39232 to 0.39187, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1740 - acc: 0.9833 - val_loss: 0.3919 - val_acc: 0.8500\n",
            "Epoch 75/300\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.39187\n",
            "60/60 - 0s - loss: 0.1755 - acc: 0.9667 - val_loss: 0.3934 - val_acc: 0.8500\n",
            "Epoch 76/300\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.39187\n",
            "60/60 - 0s - loss: 0.1470 - acc: 0.9667 - val_loss: 0.3949 - val_acc: 0.8500\n",
            "Epoch 77/300\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.39187\n",
            "60/60 - 0s - loss: 0.1880 - acc: 0.9667 - val_loss: 0.3961 - val_acc: 0.8500\n",
            "Epoch 78/300\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.39187\n",
            "60/60 - 0s - loss: 0.1558 - acc: 0.9667 - val_loss: 0.3987 - val_acc: 0.8500\n",
            "Epoch 79/300\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.39187\n",
            "60/60 - 0s - loss: 0.2036 - acc: 0.9333 - val_loss: 0.4056 - val_acc: 0.8000\n",
            "Epoch 80/300\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.39187\n",
            "60/60 - 0s - loss: 0.1673 - acc: 0.9667 - val_loss: 0.4055 - val_acc: 0.8000\n",
            "Epoch 81/300\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.39187\n",
            "60/60 - 0s - loss: 0.1816 - acc: 0.9833 - val_loss: 0.4040 - val_acc: 0.8000\n",
            "Epoch 82/300\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.39187\n",
            "60/60 - 0s - loss: 0.1491 - acc: 0.9667 - val_loss: 0.4035 - val_acc: 0.8000\n",
            "Epoch 83/300\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.39187\n",
            "60/60 - 0s - loss: 0.1636 - acc: 1.0000 - val_loss: 0.4028 - val_acc: 0.8000\n",
            "Epoch 84/300\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.39187\n",
            "60/60 - 0s - loss: 0.1546 - acc: 0.9833 - val_loss: 0.4013 - val_acc: 0.8000\n",
            "Epoch 85/300\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.39187\n",
            "60/60 - 0s - loss: 0.1592 - acc: 0.9833 - val_loss: 0.3974 - val_acc: 0.8000\n",
            "Epoch 86/300\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.39187\n",
            "60/60 - 0s - loss: 0.1445 - acc: 0.9833 - val_loss: 0.3944 - val_acc: 0.8000\n",
            "Epoch 87/300\n",
            "\n",
            "Epoch 00087: val_loss improved from 0.39187 to 0.39053, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1968 - acc: 0.9667 - val_loss: 0.3905 - val_acc: 0.8500\n",
            "Epoch 88/300\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.39053\n",
            "60/60 - 0s - loss: 0.1607 - acc: 0.9500 - val_loss: 0.3925 - val_acc: 0.8500\n",
            "Epoch 89/300\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.39053\n",
            "60/60 - 0s - loss: 0.1728 - acc: 0.9667 - val_loss: 0.3993 - val_acc: 0.9000\n",
            "Epoch 90/300\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.39053\n",
            "60/60 - 0s - loss: 0.1366 - acc: 0.9833 - val_loss: 0.4064 - val_acc: 0.9000\n",
            "Epoch 91/300\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.39053\n",
            "60/60 - 0s - loss: 0.1700 - acc: 0.9667 - val_loss: 0.4107 - val_acc: 0.9000\n",
            "Epoch 92/300\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.39053\n",
            "60/60 - 0s - loss: 0.1391 - acc: 1.0000 - val_loss: 0.4076 - val_acc: 0.8500\n",
            "Epoch 93/300\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.39053\n",
            "60/60 - 0s - loss: 0.1693 - acc: 0.9667 - val_loss: 0.4082 - val_acc: 0.8500\n",
            "Epoch 94/300\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.39053\n",
            "60/60 - 0s - loss: 0.1650 - acc: 1.0000 - val_loss: 0.4109 - val_acc: 0.8500\n",
            "Epoch 95/300\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.39053\n",
            "60/60 - 0s - loss: 0.1446 - acc: 1.0000 - val_loss: 0.4025 - val_acc: 0.9000\n",
            "Epoch 96/300\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.39053\n",
            "60/60 - 0s - loss: 0.1673 - acc: 0.9833 - val_loss: 0.4001 - val_acc: 0.9000\n",
            "Epoch 97/300\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.39053\n",
            "60/60 - 0s - loss: 0.1712 - acc: 0.9667 - val_loss: 0.4080 - val_acc: 0.9000\n",
            "Epoch 98/300\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.39053\n",
            "60/60 - 0s - loss: 0.1245 - acc: 1.0000 - val_loss: 0.4108 - val_acc: 0.9000\n",
            "Epoch 99/300\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.39053\n",
            "60/60 - 0s - loss: 0.1233 - acc: 1.0000 - val_loss: 0.4127 - val_acc: 0.9000\n",
            "Epoch 100/300\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.39053\n",
            "60/60 - 0s - loss: 0.1542 - acc: 0.9833 - val_loss: 0.4121 - val_acc: 0.9000\n",
            "Epoch 101/300\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.39053\n",
            "60/60 - 0s - loss: 0.1647 - acc: 0.9833 - val_loss: 0.4101 - val_acc: 0.8500\n",
            "Epoch 102/300\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.39053\n",
            "60/60 - 0s - loss: 0.1977 - acc: 0.9500 - val_loss: 0.4107 - val_acc: 0.8000\n",
            "Epoch 103/300\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.39053\n",
            "60/60 - 0s - loss: 0.1655 - acc: 0.9333 - val_loss: 0.4081 - val_acc: 0.8000\n",
            "Epoch 104/300\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.39053\n",
            "60/60 - 0s - loss: 0.1172 - acc: 1.0000 - val_loss: 0.4020 - val_acc: 0.7500\n",
            "Epoch 105/300\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 0.39053\n",
            "60/60 - 0s - loss: 0.1724 - acc: 0.9500 - val_loss: 0.4044 - val_acc: 0.8000\n",
            "Epoch 106/300\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 0.39053\n",
            "60/60 - 0s - loss: 0.1384 - acc: 1.0000 - val_loss: 0.4150 - val_acc: 0.8000\n",
            "Epoch 107/300\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.39053\n",
            "60/60 - 0s - loss: 0.1394 - acc: 1.0000 - val_loss: 0.4199 - val_acc: 0.8500\n",
            "Epoch 108/300\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 0.39053\n",
            "60/60 - 0s - loss: 0.1499 - acc: 0.9833 - val_loss: 0.4268 - val_acc: 0.9000\n",
            "Epoch 109/300\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.39053\n",
            "60/60 - 0s - loss: 0.1133 - acc: 1.0000 - val_loss: 0.4310 - val_acc: 0.8000\n",
            "Epoch 110/300\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.39053\n",
            "60/60 - 0s - loss: 0.1127 - acc: 1.0000 - val_loss: 0.4210 - val_acc: 0.9000\n",
            "Epoch 111/300\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 0.39053\n",
            "60/60 - 0s - loss: 0.1316 - acc: 1.0000 - val_loss: 0.4225 - val_acc: 0.9000\n",
            "Epoch 112/300\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 0.39053\n",
            "60/60 - 0s - loss: 0.1442 - acc: 0.9667 - val_loss: 0.4241 - val_acc: 0.9000\n",
            "Epoch 113/300\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.39053\n",
            "60/60 - 0s - loss: 0.1582 - acc: 0.9833 - val_loss: 0.4086 - val_acc: 0.9000\n",
            "Epoch 114/300\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.39053\n",
            "60/60 - 0s - loss: 0.1439 - acc: 0.9833 - val_loss: 0.3973 - val_acc: 0.8500\n",
            "Epoch 115/300\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.39053\n",
            "60/60 - 0s - loss: 0.1371 - acc: 1.0000 - val_loss: 0.3939 - val_acc: 0.8500\n",
            "Epoch 116/300\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.39053\n",
            "60/60 - 0s - loss: 0.1502 - acc: 0.9333 - val_loss: 0.3975 - val_acc: 0.8500\n",
            "Epoch 117/300\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.39053\n",
            "60/60 - 0s - loss: 0.1012 - acc: 1.0000 - val_loss: 0.4075 - val_acc: 0.8500\n",
            "Epoch 118/300\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.39053\n",
            "60/60 - 0s - loss: 0.0935 - acc: 0.9833 - val_loss: 0.4119 - val_acc: 0.8500\n",
            "Epoch 119/300\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 0.39053\n",
            "60/60 - 0s - loss: 0.0929 - acc: 1.0000 - val_loss: 0.4131 - val_acc: 0.9000\n",
            "Epoch 120/300\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.39053\n",
            "60/60 - 0s - loss: 0.1402 - acc: 0.9833 - val_loss: 0.4125 - val_acc: 0.9000\n",
            "Epoch 121/300\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.39053\n",
            "60/60 - 0s - loss: 0.1559 - acc: 0.9667 - val_loss: 0.4166 - val_acc: 0.9000\n",
            "Epoch 122/300\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.39053\n",
            "60/60 - 0s - loss: 0.1308 - acc: 0.9833 - val_loss: 0.4237 - val_acc: 0.9000\n",
            "Epoch 123/300\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 0.39053\n",
            "60/60 - 0s - loss: 0.1169 - acc: 1.0000 - val_loss: 0.4307 - val_acc: 0.9000\n",
            "Epoch 124/300\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 0.39053\n",
            "60/60 - 0s - loss: 0.1135 - acc: 0.9667 - val_loss: 0.4394 - val_acc: 0.9000\n",
            "Epoch 125/300\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 0.39053\n",
            "60/60 - 0s - loss: 0.1146 - acc: 1.0000 - val_loss: 0.4441 - val_acc: 0.9000\n",
            "Epoch 126/300\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 0.39053\n",
            "60/60 - 0s - loss: 0.1062 - acc: 1.0000 - val_loss: 0.4436 - val_acc: 0.9000\n",
            "Epoch 127/300\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 0.39053\n",
            "60/60 - 0s - loss: 0.1060 - acc: 1.0000 - val_loss: 0.4443 - val_acc: 0.9000\n",
            "Epoch 128/300\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 0.39053\n",
            "60/60 - 0s - loss: 0.1063 - acc: 0.9833 - val_loss: 0.4357 - val_acc: 0.8500\n",
            "Epoch 129/300\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.39053\n",
            "60/60 - 0s - loss: 0.1063 - acc: 1.0000 - val_loss: 0.4318 - val_acc: 0.8000\n",
            "Epoch 130/300\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 0.39053\n",
            "60/60 - 0s - loss: 0.1180 - acc: 1.0000 - val_loss: 0.4287 - val_acc: 0.8000\n",
            "Epoch 131/300\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 0.39053\n",
            "60/60 - 0s - loss: 0.1232 - acc: 1.0000 - val_loss: 0.4361 - val_acc: 0.8000\n",
            "Epoch 132/300\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 0.39053\n",
            "60/60 - 0s - loss: 0.1873 - acc: 0.9000 - val_loss: 0.4665 - val_acc: 0.7500\n",
            "Epoch 133/300\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.39053\n",
            "60/60 - 0s - loss: 0.1135 - acc: 0.9833 - val_loss: 0.4740 - val_acc: 0.7500\n",
            "Epoch 134/300\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.39053\n",
            "60/60 - 0s - loss: 0.1095 - acc: 1.0000 - val_loss: 0.4565 - val_acc: 0.7500\n",
            "Epoch 135/300\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 0.39053\n",
            "60/60 - 0s - loss: 0.1286 - acc: 0.9833 - val_loss: 0.4251 - val_acc: 0.8000\n",
            "Epoch 136/300\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.39053\n",
            "60/60 - 0s - loss: 0.1064 - acc: 1.0000 - val_loss: 0.3986 - val_acc: 0.8500\n",
            "Epoch 137/300\n",
            "\n",
            "Epoch 00137: val_loss improved from 0.39053 to 0.38813, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1340 - acc: 1.0000 - val_loss: 0.3881 - val_acc: 0.9000\n",
            "Epoch 138/300\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.38813\n",
            "60/60 - 0s - loss: 0.1210 - acc: 1.0000 - val_loss: 0.4024 - val_acc: 0.8500\n",
            "Epoch 139/300\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.38813\n",
            "60/60 - 0s - loss: 0.0805 - acc: 1.0000 - val_loss: 0.4120 - val_acc: 0.8500\n",
            "Epoch 140/300\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 0.38813\n",
            "60/60 - 0s - loss: 0.0985 - acc: 1.0000 - val_loss: 0.4112 - val_acc: 0.8000\n",
            "Epoch 141/300\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.38813\n",
            "60/60 - 0s - loss: 0.0944 - acc: 1.0000 - val_loss: 0.4044 - val_acc: 0.8500\n",
            "Epoch 142/300\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.38813\n",
            "60/60 - 0s - loss: 0.1093 - acc: 1.0000 - val_loss: 0.4003 - val_acc: 0.8500\n",
            "Epoch 143/300\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 0.38813\n",
            "60/60 - 0s - loss: 0.1145 - acc: 1.0000 - val_loss: 0.4041 - val_acc: 0.8500\n",
            "Epoch 144/300\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 0.38813\n",
            "60/60 - 0s - loss: 0.0834 - acc: 1.0000 - val_loss: 0.4107 - val_acc: 0.8500\n",
            "Epoch 145/300\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.38813\n",
            "60/60 - 0s - loss: 0.0947 - acc: 1.0000 - val_loss: 0.4160 - val_acc: 0.9000\n",
            "Epoch 146/300\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.38813\n",
            "60/60 - 0s - loss: 0.1195 - acc: 0.9667 - val_loss: 0.4143 - val_acc: 0.9000\n",
            "Epoch 147/300\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.38813\n",
            "60/60 - 0s - loss: 0.0848 - acc: 1.0000 - val_loss: 0.4129 - val_acc: 0.8500\n",
            "Epoch 148/300\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 0.38813\n",
            "60/60 - 0s - loss: 0.1598 - acc: 0.9333 - val_loss: 0.4124 - val_acc: 0.8500\n",
            "Epoch 149/300\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.38813\n",
            "60/60 - 0s - loss: 0.1001 - acc: 0.9667 - val_loss: 0.4099 - val_acc: 0.8500\n",
            "Epoch 150/300\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.38813\n",
            "60/60 - 0s - loss: 0.0798 - acc: 1.0000 - val_loss: 0.4143 - val_acc: 0.8500\n",
            "Epoch 151/300\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.38813\n",
            "60/60 - 0s - loss: 0.1184 - acc: 1.0000 - val_loss: 0.4140 - val_acc: 0.8500\n",
            "Epoch 152/300\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.38813\n",
            "60/60 - 0s - loss: 0.0983 - acc: 1.0000 - val_loss: 0.4084 - val_acc: 0.8500\n",
            "Epoch 153/300\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 0.38813\n",
            "60/60 - 0s - loss: 0.1058 - acc: 1.0000 - val_loss: 0.4033 - val_acc: 0.9000\n",
            "Epoch 154/300\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.38813\n",
            "60/60 - 0s - loss: 0.0804 - acc: 1.0000 - val_loss: 0.3996 - val_acc: 0.8500\n",
            "Epoch 155/300\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.38813\n",
            "60/60 - 0s - loss: 0.1134 - acc: 0.9833 - val_loss: 0.4066 - val_acc: 0.8500\n",
            "Epoch 156/300\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.38813\n",
            "60/60 - 0s - loss: 0.0772 - acc: 1.0000 - val_loss: 0.4138 - val_acc: 0.8500\n",
            "Epoch 157/300\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.38813\n",
            "60/60 - 0s - loss: 0.0979 - acc: 1.0000 - val_loss: 0.4184 - val_acc: 0.8500\n",
            "Epoch 158/300\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.38813\n",
            "60/60 - 0s - loss: 0.0851 - acc: 1.0000 - val_loss: 0.4225 - val_acc: 0.8000\n",
            "Epoch 159/300\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.38813\n",
            "60/60 - 0s - loss: 0.0841 - acc: 1.0000 - val_loss: 0.4249 - val_acc: 0.8500\n",
            "Epoch 160/300\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.38813\n",
            "60/60 - 0s - loss: 0.0826 - acc: 1.0000 - val_loss: 0.4317 - val_acc: 0.8500\n",
            "Epoch 161/300\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.38813\n",
            "60/60 - 0s - loss: 0.0863 - acc: 1.0000 - val_loss: 0.4338 - val_acc: 0.8500\n",
            "Epoch 162/300\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.38813\n",
            "60/60 - 0s - loss: 0.0931 - acc: 1.0000 - val_loss: 0.4309 - val_acc: 0.8500\n",
            "Epoch 163/300\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.38813\n",
            "60/60 - 0s - loss: 0.1060 - acc: 0.9833 - val_loss: 0.4270 - val_acc: 0.8500\n",
            "Epoch 164/300\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.38813\n",
            "60/60 - 0s - loss: 0.0877 - acc: 1.0000 - val_loss: 0.4241 - val_acc: 0.8500\n",
            "Epoch 165/300\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.38813\n",
            "60/60 - 0s - loss: 0.0994 - acc: 0.9833 - val_loss: 0.4208 - val_acc: 0.8500\n",
            "Epoch 166/300\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 0.38813\n",
            "60/60 - 0s - loss: 0.0771 - acc: 1.0000 - val_loss: 0.4121 - val_acc: 0.8500\n",
            "Epoch 167/300\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.38813\n",
            "60/60 - 0s - loss: 0.1242 - acc: 0.9500 - val_loss: 0.4095 - val_acc: 0.7500\n",
            "Epoch 168/300\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.38813\n",
            "60/60 - 0s - loss: 0.0883 - acc: 1.0000 - val_loss: 0.4138 - val_acc: 0.8000\n",
            "Epoch 169/300\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 0.38813\n",
            "60/60 - 0s - loss: 0.0814 - acc: 1.0000 - val_loss: 0.4105 - val_acc: 0.8000\n",
            "Epoch 170/300\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.38813\n",
            "60/60 - 0s - loss: 0.0966 - acc: 0.9833 - val_loss: 0.4122 - val_acc: 0.8000\n",
            "Epoch 171/300\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.38813\n",
            "60/60 - 0s - loss: 0.0895 - acc: 0.9833 - val_loss: 0.4077 - val_acc: 0.8000\n",
            "Epoch 172/300\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.38813\n",
            "60/60 - 0s - loss: 0.1192 - acc: 0.9833 - val_loss: 0.4051 - val_acc: 0.8500\n",
            "Epoch 173/300\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.38813\n",
            "60/60 - 0s - loss: 0.0786 - acc: 1.0000 - val_loss: 0.4047 - val_acc: 0.8500\n",
            "Epoch 174/300\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.38813\n",
            "60/60 - 0s - loss: 0.1369 - acc: 0.9500 - val_loss: 0.4158 - val_acc: 0.8500\n",
            "Epoch 175/300\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.38813\n",
            "60/60 - 0s - loss: 0.1051 - acc: 1.0000 - val_loss: 0.4225 - val_acc: 0.8500\n",
            "Epoch 176/300\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 0.38813\n",
            "60/60 - 0s - loss: 0.0895 - acc: 1.0000 - val_loss: 0.4281 - val_acc: 0.8000\n",
            "Epoch 177/300\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.38813\n",
            "60/60 - 0s - loss: 0.0770 - acc: 1.0000 - val_loss: 0.4245 - val_acc: 0.8500\n",
            "Epoch 178/300\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.38813\n",
            "60/60 - 0s - loss: 0.0878 - acc: 1.0000 - val_loss: 0.4177 - val_acc: 0.9000\n",
            "Epoch 179/300\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.38813\n",
            "60/60 - 0s - loss: 0.1053 - acc: 0.9833 - val_loss: 0.4047 - val_acc: 0.8500\n",
            "Epoch 180/300\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.38813\n",
            "60/60 - 0s - loss: 0.0698 - acc: 1.0000 - val_loss: 0.3951 - val_acc: 0.8500\n",
            "Epoch 181/300\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.38813\n",
            "60/60 - 0s - loss: 0.0851 - acc: 1.0000 - val_loss: 0.3915 - val_acc: 0.9000\n",
            "Epoch 182/300\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.38813\n",
            "60/60 - 0s - loss: 0.0708 - acc: 1.0000 - val_loss: 0.3908 - val_acc: 0.9000\n",
            "Epoch 183/300\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.38813\n",
            "60/60 - 0s - loss: 0.0947 - acc: 1.0000 - val_loss: 0.3914 - val_acc: 0.8500\n",
            "Epoch 184/300\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 0.38813\n",
            "60/60 - 0s - loss: 0.0713 - acc: 1.0000 - val_loss: 0.3978 - val_acc: 0.8500\n",
            "Epoch 185/300\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.38813\n",
            "60/60 - 0s - loss: 0.1143 - acc: 0.9500 - val_loss: 0.3912 - val_acc: 0.8000\n",
            "Epoch 186/300\n",
            "\n",
            "Epoch 00186: val_loss improved from 0.38813 to 0.38222, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.0621 - acc: 1.0000 - val_loss: 0.3822 - val_acc: 0.8000\n",
            "Epoch 187/300\n",
            "\n",
            "Epoch 00187: val_loss improved from 0.38222 to 0.37660, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.0559 - acc: 1.0000 - val_loss: 0.3766 - val_acc: 0.8500\n",
            "Epoch 188/300\n",
            "\n",
            "Epoch 00188: val_loss improved from 0.37660 to 0.37484, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.0778 - acc: 1.0000 - val_loss: 0.3748 - val_acc: 0.8500\n",
            "Epoch 189/300\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.37484\n",
            "60/60 - 0s - loss: 0.1133 - acc: 0.9833 - val_loss: 0.3795 - val_acc: 0.8500\n",
            "Epoch 190/300\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.37484\n",
            "60/60 - 0s - loss: 0.0813 - acc: 0.9833 - val_loss: 0.3881 - val_acc: 0.8000\n",
            "Epoch 191/300\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.37484\n",
            "60/60 - 0s - loss: 0.1095 - acc: 0.9833 - val_loss: 0.3982 - val_acc: 0.8000\n",
            "Epoch 192/300\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 0.37484\n",
            "60/60 - 0s - loss: 0.0690 - acc: 1.0000 - val_loss: 0.4085 - val_acc: 0.8000\n",
            "Epoch 193/300\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.37484\n",
            "60/60 - 0s - loss: 0.0993 - acc: 1.0000 - val_loss: 0.4088 - val_acc: 0.7500\n",
            "Epoch 194/300\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.37484\n",
            "60/60 - 0s - loss: 0.0987 - acc: 0.9833 - val_loss: 0.4133 - val_acc: 0.7500\n",
            "Epoch 195/300\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.37484\n",
            "60/60 - 0s - loss: 0.0789 - acc: 0.9833 - val_loss: 0.4280 - val_acc: 0.7500\n",
            "Epoch 196/300\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.37484\n",
            "60/60 - 0s - loss: 0.0771 - acc: 1.0000 - val_loss: 0.4385 - val_acc: 0.7500\n",
            "Epoch 197/300\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.37484\n",
            "60/60 - 0s - loss: 0.0864 - acc: 0.9833 - val_loss: 0.4410 - val_acc: 0.8000\n",
            "Epoch 198/300\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.37484\n",
            "60/60 - 0s - loss: 0.0819 - acc: 1.0000 - val_loss: 0.4374 - val_acc: 0.8000\n",
            "Epoch 199/300\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.37484\n",
            "60/60 - 0s - loss: 0.1157 - acc: 0.9833 - val_loss: 0.4351 - val_acc: 0.8500\n",
            "Epoch 200/300\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.37484\n",
            "60/60 - 0s - loss: 0.0781 - acc: 1.0000 - val_loss: 0.4364 - val_acc: 0.8500\n",
            "Epoch 201/300\n",
            "\n",
            "Epoch 00201: val_loss did not improve from 0.37484\n",
            "60/60 - 0s - loss: 0.0654 - acc: 1.0000 - val_loss: 0.4377 - val_acc: 0.8500\n",
            "Epoch 202/300\n",
            "\n",
            "Epoch 00202: val_loss did not improve from 0.37484\n",
            "60/60 - 0s - loss: 0.0714 - acc: 1.0000 - val_loss: 0.4295 - val_acc: 0.8500\n",
            "Epoch 203/300\n",
            "\n",
            "Epoch 00203: val_loss did not improve from 0.37484\n",
            "60/60 - 0s - loss: 0.0795 - acc: 1.0000 - val_loss: 0.4245 - val_acc: 0.8500\n",
            "Epoch 204/300\n",
            "\n",
            "Epoch 00204: val_loss did not improve from 0.37484\n",
            "60/60 - 0s - loss: 0.0896 - acc: 1.0000 - val_loss: 0.4281 - val_acc: 0.7500\n",
            "Epoch 205/300\n",
            "\n",
            "Epoch 00205: val_loss did not improve from 0.37484\n",
            "60/60 - 0s - loss: 0.1141 - acc: 1.0000 - val_loss: 0.4329 - val_acc: 0.7500\n",
            "Epoch 206/300\n",
            "\n",
            "Epoch 00206: val_loss did not improve from 0.37484\n",
            "60/60 - 0s - loss: 0.0752 - acc: 0.9833 - val_loss: 0.4328 - val_acc: 0.7500\n",
            "Epoch 207/300\n",
            "\n",
            "Epoch 00207: val_loss did not improve from 0.37484\n",
            "60/60 - 0s - loss: 0.0724 - acc: 0.9833 - val_loss: 0.4321 - val_acc: 0.8000\n",
            "Epoch 208/300\n",
            "\n",
            "Epoch 00208: val_loss did not improve from 0.37484\n",
            "60/60 - 0s - loss: 0.0652 - acc: 1.0000 - val_loss: 0.4313 - val_acc: 0.7500\n",
            "Epoch 209/300\n",
            "\n",
            "Epoch 00209: val_loss did not improve from 0.37484\n",
            "60/60 - 0s - loss: 0.0778 - acc: 0.9833 - val_loss: 0.4314 - val_acc: 0.8500\n",
            "Epoch 210/300\n",
            "\n",
            "Epoch 00210: val_loss did not improve from 0.37484\n",
            "60/60 - 0s - loss: 0.0688 - acc: 1.0000 - val_loss: 0.4308 - val_acc: 0.8500\n",
            "Epoch 211/300\n",
            "\n",
            "Epoch 00211: val_loss did not improve from 0.37484\n",
            "60/60 - 0s - loss: 0.0665 - acc: 1.0000 - val_loss: 0.4252 - val_acc: 0.8500\n",
            "Epoch 212/300\n",
            "\n",
            "Epoch 00212: val_loss did not improve from 0.37484\n",
            "60/60 - 0s - loss: 0.0762 - acc: 1.0000 - val_loss: 0.4136 - val_acc: 0.8500\n",
            "Epoch 213/300\n",
            "\n",
            "Epoch 00213: val_loss did not improve from 0.37484\n",
            "60/60 - 0s - loss: 0.0913 - acc: 0.9833 - val_loss: 0.3954 - val_acc: 0.8500\n",
            "Epoch 214/300\n",
            "\n",
            "Epoch 00214: val_loss did not improve from 0.37484\n",
            "60/60 - 0s - loss: 0.1350 - acc: 0.9500 - val_loss: 0.3865 - val_acc: 0.8500\n",
            "Epoch 215/300\n",
            "\n",
            "Epoch 00215: val_loss did not improve from 0.37484\n",
            "60/60 - 0s - loss: 0.0744 - acc: 1.0000 - val_loss: 0.3835 - val_acc: 0.8500\n",
            "Epoch 216/300\n",
            "\n",
            "Epoch 00216: val_loss did not improve from 0.37484\n",
            "60/60 - 0s - loss: 0.0723 - acc: 1.0000 - val_loss: 0.3832 - val_acc: 0.8500\n",
            "Epoch 217/300\n",
            "\n",
            "Epoch 00217: val_loss did not improve from 0.37484\n",
            "60/60 - 0s - loss: 0.0717 - acc: 1.0000 - val_loss: 0.3886 - val_acc: 0.8500\n",
            "Epoch 218/300\n",
            "\n",
            "Epoch 00218: val_loss did not improve from 0.37484\n",
            "60/60 - 0s - loss: 0.0717 - acc: 1.0000 - val_loss: 0.3927 - val_acc: 0.8500\n",
            "Epoch 219/300\n",
            "\n",
            "Epoch 00219: val_loss did not improve from 0.37484\n",
            "60/60 - 0s - loss: 0.0519 - acc: 1.0000 - val_loss: 0.3986 - val_acc: 0.8500\n",
            "Epoch 220/300\n",
            "\n",
            "Epoch 00220: val_loss did not improve from 0.37484\n",
            "60/60 - 0s - loss: 0.0750 - acc: 1.0000 - val_loss: 0.4051 - val_acc: 0.8500\n",
            "Epoch 221/300\n",
            "\n",
            "Epoch 00221: val_loss did not improve from 0.37484\n",
            "60/60 - 0s - loss: 0.0614 - acc: 1.0000 - val_loss: 0.4096 - val_acc: 0.8500\n",
            "Epoch 222/300\n",
            "\n",
            "Epoch 00222: val_loss did not improve from 0.37484\n",
            "60/60 - 0s - loss: 0.0785 - acc: 0.9833 - val_loss: 0.4021 - val_acc: 0.8000\n",
            "Epoch 223/300\n",
            "\n",
            "Epoch 00223: val_loss did not improve from 0.37484\n",
            "60/60 - 0s - loss: 0.0804 - acc: 1.0000 - val_loss: 0.3817 - val_acc: 0.8500\n",
            "Epoch 224/300\n",
            "\n",
            "Epoch 00224: val_loss did not improve from 0.37484\n",
            "60/60 - 0s - loss: 0.0921 - acc: 0.9833 - val_loss: 0.3776 - val_acc: 0.8000\n",
            "Epoch 225/300\n",
            "\n",
            "Epoch 00225: val_loss improved from 0.37484 to 0.37326, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.0940 - acc: 0.9667 - val_loss: 0.3733 - val_acc: 0.8000\n",
            "Epoch 226/300\n",
            "\n",
            "Epoch 00226: val_loss did not improve from 0.37326\n",
            "60/60 - 0s - loss: 0.0887 - acc: 0.9833 - val_loss: 0.3838 - val_acc: 0.9000\n",
            "Epoch 227/300\n",
            "\n",
            "Epoch 00227: val_loss did not improve from 0.37326\n",
            "60/60 - 0s - loss: 0.0758 - acc: 1.0000 - val_loss: 0.3945 - val_acc: 0.8000\n",
            "Epoch 228/300\n",
            "\n",
            "Epoch 00228: val_loss did not improve from 0.37326\n",
            "60/60 - 0s - loss: 0.0671 - acc: 1.0000 - val_loss: 0.4067 - val_acc: 0.8000\n",
            "Epoch 229/300\n",
            "\n",
            "Epoch 00229: val_loss did not improve from 0.37326\n",
            "60/60 - 0s - loss: 0.0798 - acc: 0.9833 - val_loss: 0.4098 - val_acc: 0.7500\n",
            "Epoch 230/300\n",
            "\n",
            "Epoch 00230: val_loss did not improve from 0.37326\n",
            "60/60 - 0s - loss: 0.0781 - acc: 1.0000 - val_loss: 0.4109 - val_acc: 0.7500\n",
            "Epoch 231/300\n",
            "\n",
            "Epoch 00231: val_loss did not improve from 0.37326\n",
            "60/60 - 0s - loss: 0.1124 - acc: 1.0000 - val_loss: 0.4069 - val_acc: 0.7500\n",
            "Epoch 232/300\n",
            "\n",
            "Epoch 00232: val_loss did not improve from 0.37326\n",
            "60/60 - 0s - loss: 0.0671 - acc: 1.0000 - val_loss: 0.4004 - val_acc: 0.8500\n",
            "Epoch 233/300\n",
            "\n",
            "Epoch 00233: val_loss did not improve from 0.37326\n",
            "60/60 - 0s - loss: 0.0633 - acc: 1.0000 - val_loss: 0.4035 - val_acc: 0.8500\n",
            "Epoch 234/300\n",
            "\n",
            "Epoch 00234: val_loss did not improve from 0.37326\n",
            "60/60 - 0s - loss: 0.0889 - acc: 1.0000 - val_loss: 0.4071 - val_acc: 0.8500\n",
            "Epoch 235/300\n",
            "\n",
            "Epoch 00235: val_loss did not improve from 0.37326\n",
            "60/60 - 0s - loss: 0.0652 - acc: 1.0000 - val_loss: 0.4052 - val_acc: 0.8500\n",
            "Epoch 236/300\n",
            "\n",
            "Epoch 00236: val_loss did not improve from 0.37326\n",
            "60/60 - 0s - loss: 0.0949 - acc: 0.9833 - val_loss: 0.4066 - val_acc: 0.8500\n",
            "Epoch 237/300\n",
            "\n",
            "Epoch 00237: val_loss did not improve from 0.37326\n",
            "60/60 - 0s - loss: 0.0604 - acc: 1.0000 - val_loss: 0.4044 - val_acc: 0.8000\n",
            "Epoch 238/300\n",
            "\n",
            "Epoch 00238: val_loss did not improve from 0.37326\n",
            "60/60 - 0s - loss: 0.0732 - acc: 1.0000 - val_loss: 0.3991 - val_acc: 0.8000\n",
            "Epoch 239/300\n",
            "\n",
            "Epoch 00239: val_loss did not improve from 0.37326\n",
            "60/60 - 0s - loss: 0.0571 - acc: 1.0000 - val_loss: 0.3965 - val_acc: 0.8000\n",
            "Epoch 240/300\n",
            "\n",
            "Epoch 00240: val_loss did not improve from 0.37326\n",
            "60/60 - 0s - loss: 0.0602 - acc: 1.0000 - val_loss: 0.3997 - val_acc: 0.8500\n",
            "Epoch 241/300\n",
            "\n",
            "Epoch 00241: val_loss did not improve from 0.37326\n",
            "60/60 - 0s - loss: 0.0690 - acc: 1.0000 - val_loss: 0.4009 - val_acc: 0.8500\n",
            "Epoch 242/300\n",
            "\n",
            "Epoch 00242: val_loss did not improve from 0.37326\n",
            "60/60 - 0s - loss: 0.0598 - acc: 1.0000 - val_loss: 0.4066 - val_acc: 0.8500\n",
            "Epoch 243/300\n",
            "\n",
            "Epoch 00243: val_loss did not improve from 0.37326\n",
            "60/60 - 0s - loss: 0.0592 - acc: 0.9833 - val_loss: 0.4062 - val_acc: 0.8500\n",
            "Epoch 244/300\n",
            "\n",
            "Epoch 00244: val_loss did not improve from 0.37326\n",
            "60/60 - 0s - loss: 0.0749 - acc: 0.9833 - val_loss: 0.4026 - val_acc: 0.8500\n",
            "Epoch 245/300\n",
            "\n",
            "Epoch 00245: val_loss did not improve from 0.37326\n",
            "60/60 - 0s - loss: 0.0651 - acc: 1.0000 - val_loss: 0.3994 - val_acc: 0.7500\n",
            "Epoch 246/300\n",
            "\n",
            "Epoch 00246: val_loss did not improve from 0.37326\n",
            "60/60 - 0s - loss: 0.0499 - acc: 1.0000 - val_loss: 0.3979 - val_acc: 0.7500\n",
            "Epoch 247/300\n",
            "\n",
            "Epoch 00247: val_loss did not improve from 0.37326\n",
            "60/60 - 0s - loss: 0.0685 - acc: 1.0000 - val_loss: 0.3957 - val_acc: 0.7500\n",
            "Epoch 248/300\n",
            "\n",
            "Epoch 00248: val_loss did not improve from 0.37326\n",
            "60/60 - 0s - loss: 0.0508 - acc: 1.0000 - val_loss: 0.3913 - val_acc: 0.7500\n",
            "Epoch 249/300\n",
            "\n",
            "Epoch 00249: val_loss did not improve from 0.37326\n",
            "60/60 - 0s - loss: 0.1238 - acc: 0.9667 - val_loss: 0.3903 - val_acc: 0.8000\n",
            "Epoch 250/300\n",
            "\n",
            "Epoch 00250: val_loss did not improve from 0.37326\n",
            "60/60 - 0s - loss: 0.0653 - acc: 1.0000 - val_loss: 0.3882 - val_acc: 0.8500\n",
            "Epoch 251/300\n",
            "\n",
            "Epoch 00251: val_loss did not improve from 0.37326\n",
            "60/60 - 0s - loss: 0.0798 - acc: 1.0000 - val_loss: 0.3867 - val_acc: 0.8500\n",
            "Epoch 252/300\n",
            "\n",
            "Epoch 00252: val_loss did not improve from 0.37326\n",
            "60/60 - 0s - loss: 0.0696 - acc: 1.0000 - val_loss: 0.3839 - val_acc: 0.8500\n",
            "Epoch 253/300\n",
            "\n",
            "Epoch 00253: val_loss did not improve from 0.37326\n",
            "60/60 - 0s - loss: 0.0678 - acc: 1.0000 - val_loss: 0.3861 - val_acc: 0.8500\n",
            "Epoch 254/300\n",
            "\n",
            "Epoch 00254: val_loss did not improve from 0.37326\n",
            "60/60 - 0s - loss: 0.0700 - acc: 1.0000 - val_loss: 0.3808 - val_acc: 0.8500\n",
            "Epoch 255/300\n",
            "\n",
            "Epoch 00255: val_loss did not improve from 0.37326\n",
            "60/60 - 0s - loss: 0.0977 - acc: 0.9833 - val_loss: 0.3793 - val_acc: 0.8000\n",
            "Epoch 256/300\n",
            "\n",
            "Epoch 00256: val_loss did not improve from 0.37326\n",
            "60/60 - 0s - loss: 0.0756 - acc: 0.9833 - val_loss: 0.3854 - val_acc: 0.7000\n",
            "Epoch 257/300\n",
            "\n",
            "Epoch 00257: val_loss did not improve from 0.37326\n",
            "60/60 - 0s - loss: 0.0732 - acc: 1.0000 - val_loss: 0.3967 - val_acc: 0.8000\n",
            "Epoch 258/300\n",
            "\n",
            "Epoch 00258: val_loss did not improve from 0.37326\n",
            "60/60 - 0s - loss: 0.0818 - acc: 1.0000 - val_loss: 0.4074 - val_acc: 0.8000\n",
            "Epoch 259/300\n",
            "\n",
            "Epoch 00259: val_loss did not improve from 0.37326\n",
            "60/60 - 0s - loss: 0.0595 - acc: 1.0000 - val_loss: 0.4150 - val_acc: 0.8000\n",
            "Epoch 260/300\n",
            "\n",
            "Epoch 00260: val_loss did not improve from 0.37326\n",
            "60/60 - 0s - loss: 0.0526 - acc: 1.0000 - val_loss: 0.4105 - val_acc: 0.8000\n",
            "Epoch 261/300\n",
            "\n",
            "Epoch 00261: val_loss did not improve from 0.37326\n",
            "60/60 - 0s - loss: 0.0592 - acc: 1.0000 - val_loss: 0.4028 - val_acc: 0.8500\n",
            "Epoch 262/300\n",
            "\n",
            "Epoch 00262: val_loss did not improve from 0.37326\n",
            "60/60 - 0s - loss: 0.0562 - acc: 0.9833 - val_loss: 0.3969 - val_acc: 0.8500\n",
            "Epoch 263/300\n",
            "\n",
            "Epoch 00263: val_loss did not improve from 0.37326\n",
            "60/60 - 0s - loss: 0.0696 - acc: 0.9833 - val_loss: 0.3928 - val_acc: 0.8500\n",
            "Epoch 264/300\n",
            "\n",
            "Epoch 00264: val_loss did not improve from 0.37326\n",
            "60/60 - 0s - loss: 0.0687 - acc: 1.0000 - val_loss: 0.3874 - val_acc: 0.8500\n",
            "Epoch 265/300\n",
            "\n",
            "Epoch 00265: val_loss did not improve from 0.37326\n",
            "60/60 - 0s - loss: 0.0490 - acc: 1.0000 - val_loss: 0.3833 - val_acc: 0.8500\n",
            "Epoch 266/300\n",
            "\n",
            "Epoch 00266: val_loss did not improve from 0.37326\n",
            "60/60 - 0s - loss: 0.0656 - acc: 1.0000 - val_loss: 0.3833 - val_acc: 0.8500\n",
            "Epoch 267/300\n",
            "\n",
            "Epoch 00267: val_loss did not improve from 0.37326\n",
            "60/60 - 0s - loss: 0.0512 - acc: 1.0000 - val_loss: 0.3865 - val_acc: 0.8500\n",
            "Epoch 268/300\n",
            "\n",
            "Epoch 00268: val_loss did not improve from 0.37326\n",
            "60/60 - 0s - loss: 0.0683 - acc: 1.0000 - val_loss: 0.3889 - val_acc: 0.8500\n",
            "Epoch 269/300\n",
            "\n",
            "Epoch 00269: val_loss did not improve from 0.37326\n",
            "60/60 - 0s - loss: 0.0523 - acc: 1.0000 - val_loss: 0.4081 - val_acc: 0.8500\n",
            "Epoch 270/300\n",
            "\n",
            "Epoch 00270: val_loss did not improve from 0.37326\n",
            "60/60 - 0s - loss: 0.0613 - acc: 1.0000 - val_loss: 0.4155 - val_acc: 0.8000\n",
            "Epoch 271/300\n",
            "\n",
            "Epoch 00271: val_loss did not improve from 0.37326\n",
            "60/60 - 0s - loss: 0.0591 - acc: 1.0000 - val_loss: 0.4045 - val_acc: 0.8500\n",
            "Epoch 272/300\n",
            "\n",
            "Epoch 00272: val_loss did not improve from 0.37326\n",
            "60/60 - 0s - loss: 0.0529 - acc: 1.0000 - val_loss: 0.3988 - val_acc: 0.8500\n",
            "Epoch 273/300\n",
            "\n",
            "Epoch 00273: val_loss did not improve from 0.37326\n",
            "60/60 - 0s - loss: 0.0577 - acc: 1.0000 - val_loss: 0.3997 - val_acc: 0.8500\n",
            "Epoch 274/300\n",
            "\n",
            "Epoch 00274: val_loss did not improve from 0.37326\n",
            "60/60 - 0s - loss: 0.0950 - acc: 1.0000 - val_loss: 0.4024 - val_acc: 0.8500\n",
            "Epoch 275/300\n",
            "\n",
            "Epoch 00275: val_loss did not improve from 0.37326\n",
            "60/60 - 0s - loss: 0.1110 - acc: 0.9833 - val_loss: 0.4048 - val_acc: 0.8000\n",
            "Epoch 276/300\n",
            "\n",
            "Epoch 00276: val_loss did not improve from 0.37326\n",
            "60/60 - 0s - loss: 0.0517 - acc: 1.0000 - val_loss: 0.4018 - val_acc: 0.7500\n",
            "Epoch 277/300\n",
            "\n",
            "Epoch 00277: val_loss did not improve from 0.37326\n",
            "60/60 - 0s - loss: 0.0449 - acc: 1.0000 - val_loss: 0.4089 - val_acc: 0.7500\n",
            "Epoch 278/300\n",
            "\n",
            "Epoch 00278: val_loss did not improve from 0.37326\n",
            "60/60 - 0s - loss: 0.0740 - acc: 1.0000 - val_loss: 0.4090 - val_acc: 0.7500\n",
            "Epoch 279/300\n",
            "\n",
            "Epoch 00279: val_loss did not improve from 0.37326\n",
            "60/60 - 0s - loss: 0.0769 - acc: 1.0000 - val_loss: 0.4102 - val_acc: 0.7500\n",
            "Epoch 280/300\n",
            "\n",
            "Epoch 00280: val_loss did not improve from 0.37326\n",
            "60/60 - 0s - loss: 0.0597 - acc: 1.0000 - val_loss: 0.4153 - val_acc: 0.8000\n",
            "Epoch 281/300\n",
            "\n",
            "Epoch 00281: val_loss did not improve from 0.37326\n",
            "60/60 - 0s - loss: 0.0657 - acc: 1.0000 - val_loss: 0.4206 - val_acc: 0.8000\n",
            "Epoch 282/300\n",
            "\n",
            "Epoch 00282: val_loss did not improve from 0.37326\n",
            "60/60 - 0s - loss: 0.0698 - acc: 0.9833 - val_loss: 0.4195 - val_acc: 0.8000\n",
            "Epoch 283/300\n",
            "\n",
            "Epoch 00283: val_loss did not improve from 0.37326\n",
            "60/60 - 0s - loss: 0.0505 - acc: 1.0000 - val_loss: 0.4189 - val_acc: 0.8000\n",
            "Epoch 284/300\n",
            "\n",
            "Epoch 00284: val_loss did not improve from 0.37326\n",
            "60/60 - 0s - loss: 0.1045 - acc: 0.9667 - val_loss: 0.4116 - val_acc: 0.8000\n",
            "Epoch 285/300\n",
            "\n",
            "Epoch 00285: val_loss did not improve from 0.37326\n",
            "60/60 - 0s - loss: 0.0646 - acc: 1.0000 - val_loss: 0.4063 - val_acc: 0.8000\n",
            "Epoch 286/300\n",
            "\n",
            "Epoch 00286: val_loss did not improve from 0.37326\n",
            "60/60 - 0s - loss: 0.0411 - acc: 1.0000 - val_loss: 0.4019 - val_acc: 0.8500\n",
            "Epoch 287/300\n",
            "\n",
            "Epoch 00287: val_loss did not improve from 0.37326\n",
            "60/60 - 0s - loss: 0.0675 - acc: 1.0000 - val_loss: 0.4050 - val_acc: 0.8500\n",
            "Epoch 288/300\n",
            "\n",
            "Epoch 00288: val_loss did not improve from 0.37326\n",
            "60/60 - 0s - loss: 0.0683 - acc: 0.9833 - val_loss: 0.4141 - val_acc: 0.8500\n",
            "Epoch 289/300\n",
            "\n",
            "Epoch 00289: val_loss did not improve from 0.37326\n",
            "60/60 - 0s - loss: 0.0662 - acc: 1.0000 - val_loss: 0.4203 - val_acc: 0.8000\n",
            "Epoch 290/300\n",
            "\n",
            "Epoch 00290: val_loss did not improve from 0.37326\n",
            "60/60 - 0s - loss: 0.0763 - acc: 1.0000 - val_loss: 0.4216 - val_acc: 0.8000\n",
            "Epoch 291/300\n",
            "\n",
            "Epoch 00291: val_loss did not improve from 0.37326\n",
            "60/60 - 0s - loss: 0.0533 - acc: 1.0000 - val_loss: 0.4183 - val_acc: 0.8500\n",
            "Epoch 292/300\n",
            "\n",
            "Epoch 00292: val_loss did not improve from 0.37326\n",
            "60/60 - 0s - loss: 0.0505 - acc: 1.0000 - val_loss: 0.4147 - val_acc: 0.8500\n",
            "Epoch 293/300\n",
            "\n",
            "Epoch 00293: val_loss did not improve from 0.37326\n",
            "60/60 - 0s - loss: 0.0518 - acc: 1.0000 - val_loss: 0.4138 - val_acc: 0.8000\n",
            "Epoch 294/300\n",
            "\n",
            "Epoch 00294: val_loss did not improve from 0.37326\n",
            "60/60 - 0s - loss: 0.0439 - acc: 1.0000 - val_loss: 0.4139 - val_acc: 0.8000\n",
            "Epoch 295/300\n",
            "\n",
            "Epoch 00295: val_loss did not improve from 0.37326\n",
            "60/60 - 0s - loss: 0.0674 - acc: 1.0000 - val_loss: 0.4154 - val_acc: 0.8500\n",
            "Epoch 296/300\n",
            "\n",
            "Epoch 00296: val_loss did not improve from 0.37326\n",
            "60/60 - 0s - loss: 0.0837 - acc: 1.0000 - val_loss: 0.4248 - val_acc: 0.8000\n",
            "Epoch 297/300\n",
            "\n",
            "Epoch 00297: val_loss did not improve from 0.37326\n",
            "60/60 - 0s - loss: 0.0586 - acc: 1.0000 - val_loss: 0.4245 - val_acc: 0.8000\n",
            "Epoch 298/300\n",
            "\n",
            "Epoch 00298: val_loss did not improve from 0.37326\n",
            "60/60 - 0s - loss: 0.0497 - acc: 1.0000 - val_loss: 0.4228 - val_acc: 0.8000\n",
            "Epoch 299/300\n",
            "\n",
            "Epoch 00299: val_loss did not improve from 0.37326\n",
            "60/60 - 0s - loss: 0.0663 - acc: 1.0000 - val_loss: 0.4253 - val_acc: 0.8000\n",
            "Epoch 300/300\n",
            "\n",
            "Epoch 00300: val_loss did not improve from 0.37326\n",
            "60/60 - 0s - loss: 0.0681 - acc: 0.9833 - val_loss: 0.4274 - val_acc: 0.8000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIiWnQCAiUnL/DwrkFpHwhszmAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "# Evaluate on test data\n",
            "40/40 [==============================] - 0s 2ms/sample - loss: 0.7868 - acc: 0.6250\n",
            "test loss, test acc: [0.7868047171461512, 0.625]\n",
            "EEG_Deep/Data2A/parsed_P03T.mat\n",
            "Filtering of Training Data Finished\n",
            "EEG_Deep/Data2A/parsed_P03E.mat\n",
            "Filtering of Testing Data Finished\n",
            "(1, 2)\n",
            "Finding labels in training data\n",
            "Finding labels in testing data\n",
            "[1 1 2 1 1 2 2 2 1 1 1 1 1 2 1 2 2 1 1 2]\n",
            "(60, 12, 1536)\n",
            "(60,)\n",
            "(20, 12, 1536)\n",
            "(20,)\n",
            "[[1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]]\n",
            "X_train shape: (60, 1, 12, 1536)\n",
            "60 train samples\n",
            "20 val samples\n",
            "(40, 12, 1536)\n",
            "(40, 1)\n",
            "X_train shape: (40, 1, 12, 1536)\n",
            "40 train samples\n",
            "Train on 60 samples, validate on 20 samples\n",
            "Epoch 1/300\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.68060, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 1s - loss: 0.6593 - acc: 0.4500 - val_loss: 0.6806 - val_acc: 0.6000\n",
            "Epoch 2/300\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.68060 to 0.67785, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6702 - acc: 0.6333 - val_loss: 0.6779 - val_acc: 0.7500\n",
            "Epoch 3/300\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.67785 to 0.67652, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6603 - acc: 0.6000 - val_loss: 0.6765 - val_acc: 0.7500\n",
            "Epoch 4/300\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.6637 - acc: 0.6000 - val_loss: 0.6769 - val_acc: 0.6000\n",
            "Epoch 5/300\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.6339 - acc: 0.7000 - val_loss: 0.6783 - val_acc: 0.6000\n",
            "Epoch 6/300\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.6444 - acc: 0.6000 - val_loss: 0.6793 - val_acc: 0.5000\n",
            "Epoch 7/300\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.6356 - acc: 0.6833 - val_loss: 0.6805 - val_acc: 0.5000\n",
            "Epoch 8/300\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.6129 - acc: 0.7500 - val_loss: 0.6814 - val_acc: 0.5000\n",
            "Epoch 9/300\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.6058 - acc: 0.7333 - val_loss: 0.6808 - val_acc: 0.5000\n",
            "Epoch 10/300\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.6038 - acc: 0.7667 - val_loss: 0.6796 - val_acc: 0.5000\n",
            "Epoch 11/300\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.6042 - acc: 0.7667 - val_loss: 0.6782 - val_acc: 0.5000\n",
            "Epoch 12/300\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.5957 - acc: 0.7333 - val_loss: 0.6779 - val_acc: 0.5000\n",
            "Epoch 13/300\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.5826 - acc: 0.7667 - val_loss: 0.6790 - val_acc: 0.5000\n",
            "Epoch 14/300\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.5775 - acc: 0.7500 - val_loss: 0.6795 - val_acc: 0.5000\n",
            "Epoch 15/300\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.5778 - acc: 0.6833 - val_loss: 0.6804 - val_acc: 0.5000\n",
            "Epoch 16/300\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.5780 - acc: 0.8000 - val_loss: 0.6817 - val_acc: 0.5000\n",
            "Epoch 17/300\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.5573 - acc: 0.8167 - val_loss: 0.6820 - val_acc: 0.5000\n",
            "Epoch 18/300\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.5619 - acc: 0.8000 - val_loss: 0.6813 - val_acc: 0.5000\n",
            "Epoch 19/300\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.5497 - acc: 0.8333 - val_loss: 0.6809 - val_acc: 0.5000\n",
            "Epoch 20/300\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.5519 - acc: 0.8000 - val_loss: 0.6815 - val_acc: 0.5000\n",
            "Epoch 21/300\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.5303 - acc: 0.8167 - val_loss: 0.6813 - val_acc: 0.5000\n",
            "Epoch 22/300\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.5500 - acc: 0.7833 - val_loss: 0.6833 - val_acc: 0.4500\n",
            "Epoch 23/300\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.5505 - acc: 0.7833 - val_loss: 0.6835 - val_acc: 0.4500\n",
            "Epoch 24/300\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.5446 - acc: 0.8333 - val_loss: 0.6826 - val_acc: 0.4500\n",
            "Epoch 25/300\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.5216 - acc: 0.8667 - val_loss: 0.6811 - val_acc: 0.4500\n",
            "Epoch 26/300\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.5260 - acc: 0.8500 - val_loss: 0.6810 - val_acc: 0.4500\n",
            "Epoch 27/300\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.5059 - acc: 0.8667 - val_loss: 0.6804 - val_acc: 0.5000\n",
            "Epoch 28/300\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.5057 - acc: 0.8500 - val_loss: 0.6804 - val_acc: 0.5000\n",
            "Epoch 29/300\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.5091 - acc: 0.8333 - val_loss: 0.6818 - val_acc: 0.5500\n",
            "Epoch 30/300\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.4923 - acc: 0.9000 - val_loss: 0.6836 - val_acc: 0.5500\n",
            "Epoch 31/300\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.5003 - acc: 0.8667 - val_loss: 0.6850 - val_acc: 0.5500\n",
            "Epoch 32/300\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.5213 - acc: 0.8000 - val_loss: 0.6865 - val_acc: 0.5000\n",
            "Epoch 33/300\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.4872 - acc: 0.8500 - val_loss: 0.6876 - val_acc: 0.5000\n",
            "Epoch 34/300\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.4804 - acc: 0.8500 - val_loss: 0.6889 - val_acc: 0.5000\n",
            "Epoch 35/300\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.4849 - acc: 0.8500 - val_loss: 0.6893 - val_acc: 0.5000\n",
            "Epoch 36/300\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.4807 - acc: 0.8333 - val_loss: 0.6903 - val_acc: 0.5000\n",
            "Epoch 37/300\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.4640 - acc: 0.9000 - val_loss: 0.6902 - val_acc: 0.5000\n",
            "Epoch 38/300\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.4544 - acc: 0.9000 - val_loss: 0.6911 - val_acc: 0.5000\n",
            "Epoch 39/300\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.4768 - acc: 0.9500 - val_loss: 0.6906 - val_acc: 0.5000\n",
            "Epoch 40/300\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.4747 - acc: 0.8500 - val_loss: 0.6908 - val_acc: 0.5000\n",
            "Epoch 41/300\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.4595 - acc: 0.8833 - val_loss: 0.6916 - val_acc: 0.5500\n",
            "Epoch 42/300\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.4376 - acc: 0.8667 - val_loss: 0.6914 - val_acc: 0.5500\n",
            "Epoch 43/300\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.4399 - acc: 0.9333 - val_loss: 0.6920 - val_acc: 0.5500\n",
            "Epoch 44/300\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.4462 - acc: 0.8833 - val_loss: 0.6935 - val_acc: 0.5500\n",
            "Epoch 45/300\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.4347 - acc: 0.9333 - val_loss: 0.6952 - val_acc: 0.5000\n",
            "Epoch 46/300\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.4354 - acc: 0.9000 - val_loss: 0.6971 - val_acc: 0.5000\n",
            "Epoch 47/300\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.4271 - acc: 0.9000 - val_loss: 0.6986 - val_acc: 0.4500\n",
            "Epoch 48/300\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.4272 - acc: 0.8833 - val_loss: 0.6987 - val_acc: 0.5000\n",
            "Epoch 49/300\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.4193 - acc: 0.9333 - val_loss: 0.6982 - val_acc: 0.5000\n",
            "Epoch 50/300\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.4315 - acc: 0.9500 - val_loss: 0.6981 - val_acc: 0.5500\n",
            "Epoch 51/300\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.4064 - acc: 0.9167 - val_loss: 0.7016 - val_acc: 0.5500\n",
            "Epoch 52/300\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.4300 - acc: 0.9167 - val_loss: 0.7100 - val_acc: 0.5000\n",
            "Epoch 53/300\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.3991 - acc: 0.9167 - val_loss: 0.7186 - val_acc: 0.5000\n",
            "Epoch 54/300\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.4133 - acc: 0.9000 - val_loss: 0.7232 - val_acc: 0.4500\n",
            "Epoch 55/300\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.4012 - acc: 0.9333 - val_loss: 0.7269 - val_acc: 0.4500\n",
            "Epoch 56/300\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.3861 - acc: 0.9167 - val_loss: 0.7322 - val_acc: 0.4500\n",
            "Epoch 57/300\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.3947 - acc: 0.9000 - val_loss: 0.7414 - val_acc: 0.4500\n",
            "Epoch 58/300\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.3845 - acc: 0.9167 - val_loss: 0.7487 - val_acc: 0.4500\n",
            "Epoch 59/300\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.3883 - acc: 0.8500 - val_loss: 0.7586 - val_acc: 0.4500\n",
            "Epoch 60/300\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.3780 - acc: 0.9167 - val_loss: 0.7695 - val_acc: 0.4500\n",
            "Epoch 61/300\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.3994 - acc: 0.8500 - val_loss: 0.7770 - val_acc: 0.4500\n",
            "Epoch 62/300\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.3460 - acc: 0.9333 - val_loss: 0.7803 - val_acc: 0.4500\n",
            "Epoch 63/300\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.3789 - acc: 0.9167 - val_loss: 0.7801 - val_acc: 0.4500\n",
            "Epoch 64/300\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.3677 - acc: 0.9000 - val_loss: 0.7762 - val_acc: 0.4500\n",
            "Epoch 65/300\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.3963 - acc: 0.9000 - val_loss: 0.7834 - val_acc: 0.4500\n",
            "Epoch 66/300\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.3333 - acc: 0.9667 - val_loss: 0.8016 - val_acc: 0.4500\n",
            "Epoch 67/300\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.3417 - acc: 0.9500 - val_loss: 0.8251 - val_acc: 0.5000\n",
            "Epoch 68/300\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.3905 - acc: 0.8833 - val_loss: 0.8436 - val_acc: 0.5000\n",
            "Epoch 69/300\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.3424 - acc: 0.9167 - val_loss: 0.8472 - val_acc: 0.4500\n",
            "Epoch 70/300\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.3386 - acc: 0.9500 - val_loss: 0.8555 - val_acc: 0.4500\n",
            "Epoch 71/300\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.3716 - acc: 0.9000 - val_loss: 0.8738 - val_acc: 0.4000\n",
            "Epoch 72/300\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.3305 - acc: 0.9333 - val_loss: 0.8968 - val_acc: 0.4000\n",
            "Epoch 73/300\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.3305 - acc: 0.9167 - val_loss: 0.9288 - val_acc: 0.4500\n",
            "Epoch 74/300\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.3413 - acc: 0.9167 - val_loss: 0.9457 - val_acc: 0.4000\n",
            "Epoch 75/300\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.3399 - acc: 0.9333 - val_loss: 0.9729 - val_acc: 0.4000\n",
            "Epoch 76/300\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.3251 - acc: 0.9333 - val_loss: 0.9870 - val_acc: 0.4000\n",
            "Epoch 77/300\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.3196 - acc: 0.9333 - val_loss: 1.0045 - val_acc: 0.4000\n",
            "Epoch 78/300\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.2805 - acc: 0.9500 - val_loss: 0.9930 - val_acc: 0.4000\n",
            "Epoch 79/300\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.3447 - acc: 0.8833 - val_loss: 0.9597 - val_acc: 0.4500\n",
            "Epoch 80/300\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.2895 - acc: 0.9667 - val_loss: 0.9427 - val_acc: 0.4500\n",
            "Epoch 81/300\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.3048 - acc: 0.9167 - val_loss: 0.9366 - val_acc: 0.4500\n",
            "Epoch 82/300\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.3245 - acc: 0.9167 - val_loss: 0.9465 - val_acc: 0.4500\n",
            "Epoch 83/300\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.2969 - acc: 0.9500 - val_loss: 0.9692 - val_acc: 0.5000\n",
            "Epoch 84/300\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.3313 - acc: 0.9167 - val_loss: 0.9864 - val_acc: 0.4500\n",
            "Epoch 85/300\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.3088 - acc: 0.9167 - val_loss: 0.9782 - val_acc: 0.4500\n",
            "Epoch 86/300\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.2947 - acc: 0.9500 - val_loss: 0.9672 - val_acc: 0.4500\n",
            "Epoch 87/300\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.3363 - acc: 0.9167 - val_loss: 0.9673 - val_acc: 0.4500\n",
            "Epoch 88/300\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.2738 - acc: 0.9500 - val_loss: 0.9891 - val_acc: 0.4500\n",
            "Epoch 89/300\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.2941 - acc: 0.9167 - val_loss: 0.9950 - val_acc: 0.4500\n",
            "Epoch 90/300\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.3002 - acc: 0.9333 - val_loss: 0.9846 - val_acc: 0.5000\n",
            "Epoch 91/300\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.3131 - acc: 0.9000 - val_loss: 0.9754 - val_acc: 0.5500\n",
            "Epoch 92/300\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.2619 - acc: 0.9833 - val_loss: 0.9722 - val_acc: 0.5500\n",
            "Epoch 93/300\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.2745 - acc: 0.9333 - val_loss: 0.9705 - val_acc: 0.5500\n",
            "Epoch 94/300\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.2624 - acc: 0.9500 - val_loss: 0.9890 - val_acc: 0.5500\n",
            "Epoch 95/300\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.2709 - acc: 0.9167 - val_loss: 0.9930 - val_acc: 0.5500\n",
            "Epoch 96/300\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.2827 - acc: 0.9500 - val_loss: 0.9855 - val_acc: 0.5500\n",
            "Epoch 97/300\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.2776 - acc: 0.8833 - val_loss: 0.9638 - val_acc: 0.5500\n",
            "Epoch 98/300\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.2795 - acc: 0.9333 - val_loss: 0.9516 - val_acc: 0.5500\n",
            "Epoch 99/300\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.2964 - acc: 0.9333 - val_loss: 0.9615 - val_acc: 0.5500\n",
            "Epoch 100/300\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.2692 - acc: 0.9333 - val_loss: 0.9876 - val_acc: 0.5500\n",
            "Epoch 101/300\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.2780 - acc: 0.9333 - val_loss: 0.9940 - val_acc: 0.5500\n",
            "Epoch 102/300\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.2865 - acc: 0.9167 - val_loss: 0.9768 - val_acc: 0.5500\n",
            "Epoch 103/300\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.2490 - acc: 0.9667 - val_loss: 0.9743 - val_acc: 0.5500\n",
            "Epoch 104/300\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.2353 - acc: 0.9667 - val_loss: 0.9878 - val_acc: 0.5500\n",
            "Epoch 105/300\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.2584 - acc: 0.9667 - val_loss: 1.0245 - val_acc: 0.5000\n",
            "Epoch 106/300\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.2680 - acc: 0.9667 - val_loss: 1.0839 - val_acc: 0.4500\n",
            "Epoch 107/300\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.2628 - acc: 0.9333 - val_loss: 1.1124 - val_acc: 0.4500\n",
            "Epoch 108/300\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.3340 - acc: 0.8333 - val_loss: 1.0780 - val_acc: 0.5000\n",
            "Epoch 109/300\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.2942 - acc: 0.9500 - val_loss: 1.0297 - val_acc: 0.5500\n",
            "Epoch 110/300\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.2076 - acc: 0.9833 - val_loss: 1.0082 - val_acc: 0.5500\n",
            "Epoch 111/300\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.3155 - acc: 0.8833 - val_loss: 0.9832 - val_acc: 0.5500\n",
            "Epoch 112/300\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.2818 - acc: 0.9333 - val_loss: 0.9782 - val_acc: 0.5500\n",
            "Epoch 113/300\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.2560 - acc: 0.9500 - val_loss: 0.9912 - val_acc: 0.5500\n",
            "Epoch 114/300\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.2899 - acc: 0.9333 - val_loss: 0.9990 - val_acc: 0.5500\n",
            "Epoch 115/300\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.2997 - acc: 0.9167 - val_loss: 0.9759 - val_acc: 0.5500\n",
            "Epoch 116/300\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.2426 - acc: 0.9667 - val_loss: 0.9735 - val_acc: 0.5000\n",
            "Epoch 117/300\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.2873 - acc: 0.9333 - val_loss: 0.9907 - val_acc: 0.4500\n",
            "Epoch 118/300\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.2339 - acc: 0.9500 - val_loss: 0.9993 - val_acc: 0.4500\n",
            "Epoch 119/300\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.2393 - acc: 0.9667 - val_loss: 1.0073 - val_acc: 0.5000\n",
            "Epoch 120/300\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.2540 - acc: 0.9333 - val_loss: 0.9997 - val_acc: 0.5500\n",
            "Epoch 121/300\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.2930 - acc: 0.9667 - val_loss: 1.0047 - val_acc: 0.5500\n",
            "Epoch 122/300\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.2070 - acc: 0.9833 - val_loss: 1.0265 - val_acc: 0.5500\n",
            "Epoch 123/300\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.2583 - acc: 0.9333 - val_loss: 1.0547 - val_acc: 0.5500\n",
            "Epoch 124/300\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.2352 - acc: 0.9333 - val_loss: 1.0849 - val_acc: 0.5500\n",
            "Epoch 125/300\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.2193 - acc: 0.9667 - val_loss: 1.1338 - val_acc: 0.5500\n",
            "Epoch 126/300\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.2381 - acc: 0.9333 - val_loss: 1.1725 - val_acc: 0.5500\n",
            "Epoch 127/300\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.1990 - acc: 0.9833 - val_loss: 1.2138 - val_acc: 0.5500\n",
            "Epoch 128/300\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.1866 - acc: 0.9833 - val_loss: 1.2580 - val_acc: 0.5500\n",
            "Epoch 129/300\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.2331 - acc: 1.0000 - val_loss: 1.3030 - val_acc: 0.5500\n",
            "Epoch 130/300\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.2219 - acc: 0.9500 - val_loss: 1.3200 - val_acc: 0.4500\n",
            "Epoch 131/300\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.2157 - acc: 0.9667 - val_loss: 1.2863 - val_acc: 0.4500\n",
            "Epoch 132/300\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.2163 - acc: 0.9667 - val_loss: 1.2318 - val_acc: 0.4500\n",
            "Epoch 133/300\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.2129 - acc: 0.9333 - val_loss: 1.2180 - val_acc: 0.5500\n",
            "Epoch 134/300\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.2183 - acc: 0.9500 - val_loss: 1.2387 - val_acc: 0.5500\n",
            "Epoch 135/300\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.2208 - acc: 0.9333 - val_loss: 1.2340 - val_acc: 0.5500\n",
            "Epoch 136/300\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.2195 - acc: 0.9500 - val_loss: 1.1978 - val_acc: 0.5500\n",
            "Epoch 137/300\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.1747 - acc: 1.0000 - val_loss: 1.1547 - val_acc: 0.5500\n",
            "Epoch 138/300\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.2505 - acc: 0.9500 - val_loss: 1.1342 - val_acc: 0.5500\n",
            "Epoch 139/300\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.2408 - acc: 0.9000 - val_loss: 1.0980 - val_acc: 0.5500\n",
            "Epoch 140/300\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.2235 - acc: 0.9333 - val_loss: 1.1067 - val_acc: 0.5500\n",
            "Epoch 141/300\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.2342 - acc: 0.9167 - val_loss: 1.0989 - val_acc: 0.5000\n",
            "Epoch 142/300\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.2328 - acc: 0.9500 - val_loss: 1.0707 - val_acc: 0.5500\n",
            "Epoch 143/300\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.2169 - acc: 0.9833 - val_loss: 1.0497 - val_acc: 0.5500\n",
            "Epoch 144/300\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.2025 - acc: 0.9833 - val_loss: 1.0747 - val_acc: 0.5500\n",
            "Epoch 145/300\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.2020 - acc: 0.9500 - val_loss: 1.0877 - val_acc: 0.5500\n",
            "Epoch 146/300\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.2044 - acc: 0.9667 - val_loss: 1.0682 - val_acc: 0.5500\n",
            "Epoch 147/300\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.2226 - acc: 0.9333 - val_loss: 1.0427 - val_acc: 0.5500\n",
            "Epoch 148/300\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.1966 - acc: 0.9833 - val_loss: 1.0299 - val_acc: 0.5500\n",
            "Epoch 149/300\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.1981 - acc: 0.9667 - val_loss: 0.9855 - val_acc: 0.5500\n",
            "Epoch 150/300\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.2151 - acc: 0.9500 - val_loss: 0.9458 - val_acc: 0.5500\n",
            "Epoch 151/300\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.1863 - acc: 0.9667 - val_loss: 0.8928 - val_acc: 0.5500\n",
            "Epoch 152/300\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.2013 - acc: 0.9500 - val_loss: 0.8809 - val_acc: 0.5500\n",
            "Epoch 153/300\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.1948 - acc: 0.9333 - val_loss: 0.8890 - val_acc: 0.5500\n",
            "Epoch 154/300\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.2075 - acc: 0.9333 - val_loss: 0.9079 - val_acc: 0.5500\n",
            "Epoch 155/300\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.1959 - acc: 0.9833 - val_loss: 0.9317 - val_acc: 0.5500\n",
            "Epoch 156/300\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.1966 - acc: 0.9333 - val_loss: 0.9520 - val_acc: 0.5500\n",
            "Epoch 157/300\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.2090 - acc: 0.9333 - val_loss: 0.9954 - val_acc: 0.5500\n",
            "Epoch 158/300\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.2251 - acc: 0.9333 - val_loss: 1.0062 - val_acc: 0.5500\n",
            "Epoch 159/300\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.2333 - acc: 0.9167 - val_loss: 1.0067 - val_acc: 0.5500\n",
            "Epoch 160/300\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.1502 - acc: 0.9667 - val_loss: 1.0291 - val_acc: 0.5500\n",
            "Epoch 161/300\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.1857 - acc: 0.9833 - val_loss: 1.0546 - val_acc: 0.5500\n",
            "Epoch 162/300\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.1692 - acc: 1.0000 - val_loss: 1.0902 - val_acc: 0.5500\n",
            "Epoch 163/300\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.1685 - acc: 0.9833 - val_loss: 1.1280 - val_acc: 0.5500\n",
            "Epoch 164/300\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.1716 - acc: 0.9667 - val_loss: 1.1610 - val_acc: 0.5500\n",
            "Epoch 165/300\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.1906 - acc: 0.9500 - val_loss: 1.1838 - val_acc: 0.5500\n",
            "Epoch 166/300\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.1631 - acc: 0.9500 - val_loss: 1.2104 - val_acc: 0.5500\n",
            "Epoch 167/300\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.1858 - acc: 0.9833 - val_loss: 1.1962 - val_acc: 0.5500\n",
            "Epoch 168/300\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.1767 - acc: 0.9833 - val_loss: 1.2310 - val_acc: 0.5500\n",
            "Epoch 169/300\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.1798 - acc: 0.9333 - val_loss: 1.2484 - val_acc: 0.5500\n",
            "Epoch 170/300\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.1417 - acc: 1.0000 - val_loss: 1.2626 - val_acc: 0.5500\n",
            "Epoch 171/300\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.2023 - acc: 0.9333 - val_loss: 1.2958 - val_acc: 0.5500\n",
            "Epoch 172/300\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.1557 - acc: 0.9667 - val_loss: 1.2880 - val_acc: 0.5500\n",
            "Epoch 173/300\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.1775 - acc: 0.9500 - val_loss: 1.2559 - val_acc: 0.5500\n",
            "Epoch 174/300\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.1975 - acc: 0.9333 - val_loss: 1.2033 - val_acc: 0.5500\n",
            "Epoch 175/300\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.1582 - acc: 0.9500 - val_loss: 1.2173 - val_acc: 0.5500\n",
            "Epoch 176/300\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.1807 - acc: 0.9667 - val_loss: 1.2328 - val_acc: 0.5500\n",
            "Epoch 177/300\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.1605 - acc: 0.9667 - val_loss: 1.2183 - val_acc: 0.5500\n",
            "Epoch 178/300\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.1823 - acc: 0.9500 - val_loss: 1.1852 - val_acc: 0.5500\n",
            "Epoch 179/300\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.1929 - acc: 0.9333 - val_loss: 1.1952 - val_acc: 0.5500\n",
            "Epoch 180/300\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.1549 - acc: 0.9500 - val_loss: 1.1865 - val_acc: 0.5500\n",
            "Epoch 181/300\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.1437 - acc: 0.9833 - val_loss: 1.1479 - val_acc: 0.5000\n",
            "Epoch 182/300\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.1833 - acc: 0.9667 - val_loss: 1.1539 - val_acc: 0.4500\n",
            "Epoch 183/300\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.2461 - acc: 0.9000 - val_loss: 1.2078 - val_acc: 0.4500\n",
            "Epoch 184/300\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.1682 - acc: 0.9667 - val_loss: 1.2305 - val_acc: 0.5500\n",
            "Epoch 185/300\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.1993 - acc: 0.9333 - val_loss: 1.2360 - val_acc: 0.5500\n",
            "Epoch 186/300\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.1773 - acc: 0.9500 - val_loss: 1.2302 - val_acc: 0.5500\n",
            "Epoch 187/300\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.1753 - acc: 0.9667 - val_loss: 1.2332 - val_acc: 0.5500\n",
            "Epoch 188/300\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.1685 - acc: 0.9333 - val_loss: 1.2323 - val_acc: 0.5500\n",
            "Epoch 189/300\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.1740 - acc: 0.9833 - val_loss: 1.2348 - val_acc: 0.5500\n",
            "Epoch 190/300\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.1770 - acc: 0.9500 - val_loss: 1.2280 - val_acc: 0.5500\n",
            "Epoch 191/300\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.1401 - acc: 0.9833 - val_loss: 1.1882 - val_acc: 0.5500\n",
            "Epoch 192/300\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.1779 - acc: 0.9500 - val_loss: 1.1735 - val_acc: 0.5500\n",
            "Epoch 193/300\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.1667 - acc: 0.9500 - val_loss: 1.2034 - val_acc: 0.5500\n",
            "Epoch 194/300\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.1902 - acc: 0.9500 - val_loss: 1.1564 - val_acc: 0.5500\n",
            "Epoch 195/300\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.1764 - acc: 0.9500 - val_loss: 1.1052 - val_acc: 0.5500\n",
            "Epoch 196/300\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.1254 - acc: 0.9833 - val_loss: 1.1119 - val_acc: 0.5500\n",
            "Epoch 197/300\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.2141 - acc: 0.9500 - val_loss: 1.1066 - val_acc: 0.5500\n",
            "Epoch 198/300\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.2106 - acc: 0.9167 - val_loss: 1.0695 - val_acc: 0.5500\n",
            "Epoch 199/300\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.1630 - acc: 0.9667 - val_loss: 1.0645 - val_acc: 0.5500\n",
            "Epoch 200/300\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.1590 - acc: 0.9833 - val_loss: 1.0615 - val_acc: 0.5500\n",
            "Epoch 201/300\n",
            "\n",
            "Epoch 00201: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.2272 - acc: 0.9333 - val_loss: 1.0137 - val_acc: 0.5500\n",
            "Epoch 202/300\n",
            "\n",
            "Epoch 00202: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.1722 - acc: 0.9833 - val_loss: 0.9716 - val_acc: 0.5500\n",
            "Epoch 203/300\n",
            "\n",
            "Epoch 00203: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.1317 - acc: 0.9833 - val_loss: 0.9723 - val_acc: 0.5500\n",
            "Epoch 204/300\n",
            "\n",
            "Epoch 00204: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.1971 - acc: 0.9500 - val_loss: 1.0125 - val_acc: 0.5500\n",
            "Epoch 205/300\n",
            "\n",
            "Epoch 00205: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.1903 - acc: 0.9333 - val_loss: 1.0817 - val_acc: 0.5500\n",
            "Epoch 206/300\n",
            "\n",
            "Epoch 00206: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.1689 - acc: 0.9833 - val_loss: 1.1345 - val_acc: 0.5500\n",
            "Epoch 207/300\n",
            "\n",
            "Epoch 00207: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.1789 - acc: 0.9833 - val_loss: 1.1830 - val_acc: 0.5500\n",
            "Epoch 208/300\n",
            "\n",
            "Epoch 00208: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.1700 - acc: 0.9667 - val_loss: 1.2291 - val_acc: 0.5500\n",
            "Epoch 209/300\n",
            "\n",
            "Epoch 00209: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.1251 - acc: 0.9833 - val_loss: 1.3168 - val_acc: 0.5500\n",
            "Epoch 210/300\n",
            "\n",
            "Epoch 00210: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.1988 - acc: 0.9167 - val_loss: 1.3134 - val_acc: 0.5500\n",
            "Epoch 211/300\n",
            "\n",
            "Epoch 00211: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.1442 - acc: 0.9667 - val_loss: 1.2843 - val_acc: 0.5500\n",
            "Epoch 212/300\n",
            "\n",
            "Epoch 00212: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.1333 - acc: 0.9667 - val_loss: 1.2711 - val_acc: 0.5500\n",
            "Epoch 213/300\n",
            "\n",
            "Epoch 00213: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.1488 - acc: 0.9667 - val_loss: 1.2821 - val_acc: 0.5500\n",
            "Epoch 214/300\n",
            "\n",
            "Epoch 00214: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.1536 - acc: 0.9333 - val_loss: 1.2928 - val_acc: 0.5500\n",
            "Epoch 215/300\n",
            "\n",
            "Epoch 00215: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.1064 - acc: 0.9833 - val_loss: 1.3429 - val_acc: 0.5500\n",
            "Epoch 216/300\n",
            "\n",
            "Epoch 00216: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.1521 - acc: 0.9667 - val_loss: 1.3478 - val_acc: 0.5500\n",
            "Epoch 217/300\n",
            "\n",
            "Epoch 00217: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.1776 - acc: 0.9667 - val_loss: 1.3706 - val_acc: 0.5000\n",
            "Epoch 218/300\n",
            "\n",
            "Epoch 00218: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.1550 - acc: 0.9333 - val_loss: 1.3999 - val_acc: 0.5000\n",
            "Epoch 219/300\n",
            "\n",
            "Epoch 00219: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.1624 - acc: 0.9667 - val_loss: 1.3809 - val_acc: 0.5500\n",
            "Epoch 220/300\n",
            "\n",
            "Epoch 00220: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.1448 - acc: 0.9833 - val_loss: 1.3030 - val_acc: 0.5500\n",
            "Epoch 221/300\n",
            "\n",
            "Epoch 00221: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.1303 - acc: 0.9833 - val_loss: 1.2868 - val_acc: 0.5500\n",
            "Epoch 222/300\n",
            "\n",
            "Epoch 00222: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.2242 - acc: 0.9000 - val_loss: 1.3005 - val_acc: 0.5500\n",
            "Epoch 223/300\n",
            "\n",
            "Epoch 00223: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.1771 - acc: 0.9500 - val_loss: 1.3231 - val_acc: 0.5500\n",
            "Epoch 224/300\n",
            "\n",
            "Epoch 00224: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.2200 - acc: 0.9000 - val_loss: 1.2129 - val_acc: 0.5500\n",
            "Epoch 225/300\n",
            "\n",
            "Epoch 00225: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.1470 - acc: 0.9500 - val_loss: 1.1205 - val_acc: 0.5500\n",
            "Epoch 226/300\n",
            "\n",
            "Epoch 00226: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.1619 - acc: 0.9833 - val_loss: 1.0751 - val_acc: 0.5500\n",
            "Epoch 227/300\n",
            "\n",
            "Epoch 00227: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.1595 - acc: 0.9667 - val_loss: 1.0716 - val_acc: 0.5500\n",
            "Epoch 228/300\n",
            "\n",
            "Epoch 00228: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.1301 - acc: 0.9833 - val_loss: 1.0919 - val_acc: 0.5500\n",
            "Epoch 229/300\n",
            "\n",
            "Epoch 00229: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.1714 - acc: 0.9500 - val_loss: 1.1362 - val_acc: 0.5500\n",
            "Epoch 230/300\n",
            "\n",
            "Epoch 00230: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.2078 - acc: 0.9000 - val_loss: 1.1391 - val_acc: 0.5500\n",
            "Epoch 231/300\n",
            "\n",
            "Epoch 00231: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.1319 - acc: 1.0000 - val_loss: 1.1682 - val_acc: 0.5500\n",
            "Epoch 232/300\n",
            "\n",
            "Epoch 00232: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.1399 - acc: 0.9833 - val_loss: 1.2131 - val_acc: 0.5500\n",
            "Epoch 233/300\n",
            "\n",
            "Epoch 00233: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.1613 - acc: 0.9667 - val_loss: 1.1745 - val_acc: 0.5500\n",
            "Epoch 234/300\n",
            "\n",
            "Epoch 00234: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.1417 - acc: 1.0000 - val_loss: 1.1627 - val_acc: 0.5500\n",
            "Epoch 235/300\n",
            "\n",
            "Epoch 00235: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.1226 - acc: 0.9667 - val_loss: 1.1847 - val_acc: 0.5500\n",
            "Epoch 236/300\n",
            "\n",
            "Epoch 00236: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.1250 - acc: 1.0000 - val_loss: 1.2059 - val_acc: 0.5500\n",
            "Epoch 237/300\n",
            "\n",
            "Epoch 00237: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.1420 - acc: 0.9667 - val_loss: 1.1823 - val_acc: 0.5500\n",
            "Epoch 238/300\n",
            "\n",
            "Epoch 00238: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.1936 - acc: 0.9500 - val_loss: 1.0779 - val_acc: 0.5500\n",
            "Epoch 239/300\n",
            "\n",
            "Epoch 00239: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.1817 - acc: 0.9500 - val_loss: 0.9765 - val_acc: 0.5500\n",
            "Epoch 240/300\n",
            "\n",
            "Epoch 00240: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.1302 - acc: 0.9667 - val_loss: 0.9637 - val_acc: 0.5500\n",
            "Epoch 241/300\n",
            "\n",
            "Epoch 00241: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.1450 - acc: 0.9833 - val_loss: 1.0117 - val_acc: 0.5500\n",
            "Epoch 242/300\n",
            "\n",
            "Epoch 00242: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.1545 - acc: 0.9500 - val_loss: 1.0743 - val_acc: 0.5500\n",
            "Epoch 243/300\n",
            "\n",
            "Epoch 00243: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.1270 - acc: 0.9833 - val_loss: 1.1701 - val_acc: 0.5500\n",
            "Epoch 244/300\n",
            "\n",
            "Epoch 00244: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.1486 - acc: 0.9500 - val_loss: 1.2466 - val_acc: 0.5500\n",
            "Epoch 245/300\n",
            "\n",
            "Epoch 00245: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.1177 - acc: 0.9833 - val_loss: 1.3020 - val_acc: 0.5500\n",
            "Epoch 246/300\n",
            "\n",
            "Epoch 00246: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.1369 - acc: 0.9833 - val_loss: 1.2952 - val_acc: 0.5500\n",
            "Epoch 247/300\n",
            "\n",
            "Epoch 00247: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.1531 - acc: 0.9500 - val_loss: 1.2555 - val_acc: 0.5500\n",
            "Epoch 248/300\n",
            "\n",
            "Epoch 00248: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.1119 - acc: 1.0000 - val_loss: 1.2108 - val_acc: 0.5500\n",
            "Epoch 249/300\n",
            "\n",
            "Epoch 00249: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.1787 - acc: 0.9500 - val_loss: 1.1620 - val_acc: 0.5500\n",
            "Epoch 250/300\n",
            "\n",
            "Epoch 00250: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.1534 - acc: 0.9667 - val_loss: 1.1446 - val_acc: 0.5500\n",
            "Epoch 251/300\n",
            "\n",
            "Epoch 00251: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.1222 - acc: 1.0000 - val_loss: 1.1142 - val_acc: 0.5500\n",
            "Epoch 252/300\n",
            "\n",
            "Epoch 00252: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.1099 - acc: 1.0000 - val_loss: 1.0989 - val_acc: 0.5500\n",
            "Epoch 253/300\n",
            "\n",
            "Epoch 00253: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.1559 - acc: 0.9667 - val_loss: 1.1024 - val_acc: 0.5500\n",
            "Epoch 254/300\n",
            "\n",
            "Epoch 00254: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.1157 - acc: 1.0000 - val_loss: 1.0801 - val_acc: 0.5500\n",
            "Epoch 255/300\n",
            "\n",
            "Epoch 00255: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.1090 - acc: 0.9833 - val_loss: 1.0957 - val_acc: 0.5500\n",
            "Epoch 256/300\n",
            "\n",
            "Epoch 00256: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.2022 - acc: 0.9667 - val_loss: 1.1096 - val_acc: 0.5500\n",
            "Epoch 257/300\n",
            "\n",
            "Epoch 00257: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.1345 - acc: 0.9500 - val_loss: 1.1166 - val_acc: 0.5500\n",
            "Epoch 258/300\n",
            "\n",
            "Epoch 00258: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.1596 - acc: 0.9500 - val_loss: 1.1003 - val_acc: 0.5500\n",
            "Epoch 259/300\n",
            "\n",
            "Epoch 00259: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.1128 - acc: 1.0000 - val_loss: 1.0467 - val_acc: 0.5500\n",
            "Epoch 260/300\n",
            "\n",
            "Epoch 00260: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.1399 - acc: 0.9500 - val_loss: 1.0006 - val_acc: 0.5500\n",
            "Epoch 261/300\n",
            "\n",
            "Epoch 00261: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.1085 - acc: 0.9667 - val_loss: 0.9592 - val_acc: 0.5500\n",
            "Epoch 262/300\n",
            "\n",
            "Epoch 00262: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.1689 - acc: 0.9500 - val_loss: 0.9681 - val_acc: 0.5500\n",
            "Epoch 263/300\n",
            "\n",
            "Epoch 00263: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.1169 - acc: 0.9833 - val_loss: 0.9711 - val_acc: 0.5500\n",
            "Epoch 264/300\n",
            "\n",
            "Epoch 00264: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.1293 - acc: 0.9667 - val_loss: 0.9595 - val_acc: 0.5500\n",
            "Epoch 265/300\n",
            "\n",
            "Epoch 00265: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.1730 - acc: 0.9667 - val_loss: 0.9163 - val_acc: 0.5500\n",
            "Epoch 266/300\n",
            "\n",
            "Epoch 00266: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.1990 - acc: 0.9000 - val_loss: 0.8430 - val_acc: 0.6000\n",
            "Epoch 267/300\n",
            "\n",
            "Epoch 00267: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.1594 - acc: 0.9833 - val_loss: 0.7828 - val_acc: 0.6000\n",
            "Epoch 268/300\n",
            "\n",
            "Epoch 00268: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.1381 - acc: 0.9500 - val_loss: 0.7492 - val_acc: 0.6000\n",
            "Epoch 269/300\n",
            "\n",
            "Epoch 00269: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.1423 - acc: 0.9667 - val_loss: 0.7486 - val_acc: 0.6000\n",
            "Epoch 270/300\n",
            "\n",
            "Epoch 00270: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.1658 - acc: 0.9500 - val_loss: 0.7605 - val_acc: 0.6000\n",
            "Epoch 271/300\n",
            "\n",
            "Epoch 00271: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.1133 - acc: 0.9833 - val_loss: 0.8003 - val_acc: 0.6000\n",
            "Epoch 272/300\n",
            "\n",
            "Epoch 00272: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.1183 - acc: 1.0000 - val_loss: 0.8506 - val_acc: 0.6000\n",
            "Epoch 273/300\n",
            "\n",
            "Epoch 00273: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.1396 - acc: 0.9833 - val_loss: 0.8933 - val_acc: 0.6000\n",
            "Epoch 274/300\n",
            "\n",
            "Epoch 00274: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.0960 - acc: 0.9833 - val_loss: 0.9281 - val_acc: 0.6000\n",
            "Epoch 275/300\n",
            "\n",
            "Epoch 00275: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.0699 - acc: 1.0000 - val_loss: 0.9530 - val_acc: 0.5500\n",
            "Epoch 276/300\n",
            "\n",
            "Epoch 00276: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.1831 - acc: 0.9167 - val_loss: 0.9878 - val_acc: 0.5500\n",
            "Epoch 277/300\n",
            "\n",
            "Epoch 00277: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.1459 - acc: 0.9833 - val_loss: 0.9628 - val_acc: 0.5500\n",
            "Epoch 278/300\n",
            "\n",
            "Epoch 00278: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.1266 - acc: 0.9833 - val_loss: 0.9367 - val_acc: 0.5500\n",
            "Epoch 279/300\n",
            "\n",
            "Epoch 00279: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.1853 - acc: 0.9167 - val_loss: 0.9483 - val_acc: 0.5500\n",
            "Epoch 280/300\n",
            "\n",
            "Epoch 00280: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.1153 - acc: 0.9667 - val_loss: 0.9608 - val_acc: 0.5500\n",
            "Epoch 281/300\n",
            "\n",
            "Epoch 00281: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.1362 - acc: 0.9667 - val_loss: 0.9969 - val_acc: 0.5500\n",
            "Epoch 282/300\n",
            "\n",
            "Epoch 00282: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.1191 - acc: 0.9833 - val_loss: 0.9993 - val_acc: 0.5500\n",
            "Epoch 283/300\n",
            "\n",
            "Epoch 00283: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.1115 - acc: 0.9833 - val_loss: 1.0263 - val_acc: 0.5500\n",
            "Epoch 284/300\n",
            "\n",
            "Epoch 00284: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.0918 - acc: 0.9833 - val_loss: 1.0652 - val_acc: 0.5500\n",
            "Epoch 285/300\n",
            "\n",
            "Epoch 00285: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.1063 - acc: 0.9667 - val_loss: 1.0942 - val_acc: 0.5500\n",
            "Epoch 286/300\n",
            "\n",
            "Epoch 00286: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.0930 - acc: 0.9833 - val_loss: 1.1226 - val_acc: 0.5500\n",
            "Epoch 287/300\n",
            "\n",
            "Epoch 00287: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.1116 - acc: 0.9833 - val_loss: 1.1391 - val_acc: 0.5500\n",
            "Epoch 288/300\n",
            "\n",
            "Epoch 00288: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.1040 - acc: 0.9833 - val_loss: 1.1517 - val_acc: 0.5500\n",
            "Epoch 289/300\n",
            "\n",
            "Epoch 00289: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.1161 - acc: 0.9833 - val_loss: 1.1923 - val_acc: 0.5500\n",
            "Epoch 290/300\n",
            "\n",
            "Epoch 00290: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.1124 - acc: 0.9833 - val_loss: 1.2149 - val_acc: 0.5500\n",
            "Epoch 291/300\n",
            "\n",
            "Epoch 00291: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.1246 - acc: 0.9667 - val_loss: 1.1981 - val_acc: 0.5500\n",
            "Epoch 292/300\n",
            "\n",
            "Epoch 00292: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.1036 - acc: 0.9833 - val_loss: 1.1836 - val_acc: 0.5500\n",
            "Epoch 293/300\n",
            "\n",
            "Epoch 00293: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.1347 - acc: 0.9667 - val_loss: 1.1681 - val_acc: 0.5500\n",
            "Epoch 294/300\n",
            "\n",
            "Epoch 00294: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.1196 - acc: 0.9833 - val_loss: 1.1631 - val_acc: 0.5500\n",
            "Epoch 295/300\n",
            "\n",
            "Epoch 00295: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.1170 - acc: 0.9667 - val_loss: 1.1752 - val_acc: 0.5500\n",
            "Epoch 296/300\n",
            "\n",
            "Epoch 00296: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.1144 - acc: 0.9667 - val_loss: 1.1503 - val_acc: 0.5500\n",
            "Epoch 297/300\n",
            "\n",
            "Epoch 00297: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.1099 - acc: 0.9667 - val_loss: 1.1066 - val_acc: 0.5500\n",
            "Epoch 298/300\n",
            "\n",
            "Epoch 00298: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.1113 - acc: 0.9667 - val_loss: 1.0888 - val_acc: 0.5500\n",
            "Epoch 299/300\n",
            "\n",
            "Epoch 00299: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.1137 - acc: 0.9667 - val_loss: 1.0511 - val_acc: 0.5500\n",
            "Epoch 300/300\n",
            "\n",
            "Epoch 00300: val_loss did not improve from 0.67652\n",
            "60/60 - 0s - loss: 0.1246 - acc: 0.9667 - val_loss: 1.0325 - val_acc: 0.6000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nwzAyzv8DmbdJaUlsbR0AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "# Evaluate on test data\n",
            "40/40 [==============================] - 0s 2ms/sample - loss: 1.1852 - acc: 0.4750\n",
            "test loss, test acc: [1.1851710435701535, 0.475]\n",
            "EEG_Deep/Data2A/parsed_P04T.mat\n",
            "Filtering of Training Data Finished\n",
            "EEG_Deep/Data2A/parsed_P04E.mat\n",
            "Filtering of Testing Data Finished\n",
            "(1, 2)\n",
            "Finding labels in training data\n",
            "Finding labels in testing data\n",
            "[1 2 1 1 2 1 1 2 2 1 1 2 1 2 1 1 2 1 2 1]\n",
            "(60, 12, 1536)\n",
            "(60,)\n",
            "(20, 12, 1536)\n",
            "(20,)\n",
            "[[1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]]\n",
            "X_train shape: (60, 1, 12, 1536)\n",
            "60 train samples\n",
            "20 val samples\n",
            "(40, 12, 1536)\n",
            "(40, 1)\n",
            "X_train shape: (40, 1, 12, 1536)\n",
            "40 train samples\n",
            "Train on 60 samples, validate on 20 samples\n",
            "Epoch 1/300\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.69605, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 1s - loss: 0.6752 - acc: 0.5500 - val_loss: 0.6961 - val_acc: 0.4500\n",
            "Epoch 2/300\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.69605\n",
            "60/60 - 0s - loss: 0.6671 - acc: 0.7000 - val_loss: 0.6968 - val_acc: 0.4500\n",
            "Epoch 3/300\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.69605\n",
            "60/60 - 0s - loss: 0.6667 - acc: 0.6833 - val_loss: 0.6975 - val_acc: 0.4500\n",
            "Epoch 4/300\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.69605\n",
            "60/60 - 0s - loss: 0.6718 - acc: 0.5333 - val_loss: 0.6972 - val_acc: 0.3500\n",
            "Epoch 5/300\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.69605\n",
            "60/60 - 0s - loss: 0.6367 - acc: 0.7333 - val_loss: 0.6965 - val_acc: 0.3000\n",
            "Epoch 6/300\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.69605 to 0.69561, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6455 - acc: 0.7667 - val_loss: 0.6956 - val_acc: 0.3000\n",
            "Epoch 7/300\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.69561 to 0.69502, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6213 - acc: 0.7333 - val_loss: 0.6950 - val_acc: 0.3500\n",
            "Epoch 8/300\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.69502 to 0.69467, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6085 - acc: 0.8000 - val_loss: 0.6947 - val_acc: 0.3500\n",
            "Epoch 9/300\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.69467 to 0.69388, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6191 - acc: 0.7667 - val_loss: 0.6939 - val_acc: 0.3500\n",
            "Epoch 10/300\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.69388\n",
            "60/60 - 0s - loss: 0.6009 - acc: 0.8667 - val_loss: 0.6939 - val_acc: 0.3500\n",
            "Epoch 11/300\n",
            "\n",
            "Epoch 00011: val_loss improved from 0.69388 to 0.69347, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6009 - acc: 0.8333 - val_loss: 0.6935 - val_acc: 0.3500\n",
            "Epoch 12/300\n",
            "\n",
            "Epoch 00012: val_loss improved from 0.69347 to 0.69290, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5872 - acc: 0.7500 - val_loss: 0.6929 - val_acc: 0.3500\n",
            "Epoch 13/300\n",
            "\n",
            "Epoch 00013: val_loss improved from 0.69290 to 0.69149, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5752 - acc: 0.8167 - val_loss: 0.6915 - val_acc: 0.3500\n",
            "Epoch 14/300\n",
            "\n",
            "Epoch 00014: val_loss improved from 0.69149 to 0.68999, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5776 - acc: 0.8500 - val_loss: 0.6900 - val_acc: 0.3500\n",
            "Epoch 15/300\n",
            "\n",
            "Epoch 00015: val_loss improved from 0.68999 to 0.68792, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5711 - acc: 0.8000 - val_loss: 0.6879 - val_acc: 0.3500\n",
            "Epoch 16/300\n",
            "\n",
            "Epoch 00016: val_loss improved from 0.68792 to 0.68639, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5722 - acc: 0.8000 - val_loss: 0.6864 - val_acc: 0.3500\n",
            "Epoch 17/300\n",
            "\n",
            "Epoch 00017: val_loss improved from 0.68639 to 0.68527, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5529 - acc: 0.8833 - val_loss: 0.6853 - val_acc: 0.3500\n",
            "Epoch 18/300\n",
            "\n",
            "Epoch 00018: val_loss improved from 0.68527 to 0.68489, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5337 - acc: 0.9000 - val_loss: 0.6849 - val_acc: 0.3500\n",
            "Epoch 19/300\n",
            "\n",
            "Epoch 00019: val_loss improved from 0.68489 to 0.68386, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5129 - acc: 0.8833 - val_loss: 0.6839 - val_acc: 0.4000\n",
            "Epoch 20/300\n",
            "\n",
            "Epoch 00020: val_loss improved from 0.68386 to 0.68265, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5055 - acc: 0.9500 - val_loss: 0.6826 - val_acc: 0.4000\n",
            "Epoch 21/300\n",
            "\n",
            "Epoch 00021: val_loss improved from 0.68265 to 0.68198, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5307 - acc: 0.8833 - val_loss: 0.6820 - val_acc: 0.4000\n",
            "Epoch 22/300\n",
            "\n",
            "Epoch 00022: val_loss improved from 0.68198 to 0.68133, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5220 - acc: 0.8333 - val_loss: 0.6813 - val_acc: 0.4000\n",
            "Epoch 23/300\n",
            "\n",
            "Epoch 00023: val_loss improved from 0.68133 to 0.68086, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5032 - acc: 0.8667 - val_loss: 0.6809 - val_acc: 0.4000\n",
            "Epoch 24/300\n",
            "\n",
            "Epoch 00024: val_loss improved from 0.68086 to 0.68059, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5031 - acc: 0.8833 - val_loss: 0.6806 - val_acc: 0.4500\n",
            "Epoch 25/300\n",
            "\n",
            "Epoch 00025: val_loss improved from 0.68059 to 0.67880, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4839 - acc: 0.8833 - val_loss: 0.6788 - val_acc: 0.4000\n",
            "Epoch 26/300\n",
            "\n",
            "Epoch 00026: val_loss improved from 0.67880 to 0.67613, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4956 - acc: 0.9000 - val_loss: 0.6761 - val_acc: 0.4000\n",
            "Epoch 27/300\n",
            "\n",
            "Epoch 00027: val_loss improved from 0.67613 to 0.67485, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4826 - acc: 0.8500 - val_loss: 0.6748 - val_acc: 0.4500\n",
            "Epoch 28/300\n",
            "\n",
            "Epoch 00028: val_loss improved from 0.67485 to 0.67290, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4736 - acc: 0.8833 - val_loss: 0.6729 - val_acc: 0.4500\n",
            "Epoch 29/300\n",
            "\n",
            "Epoch 00029: val_loss improved from 0.67290 to 0.66967, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4604 - acc: 0.9000 - val_loss: 0.6697 - val_acc: 0.4500\n",
            "Epoch 30/300\n",
            "\n",
            "Epoch 00030: val_loss improved from 0.66967 to 0.66583, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4608 - acc: 0.9000 - val_loss: 0.6658 - val_acc: 0.5000\n",
            "Epoch 31/300\n",
            "\n",
            "Epoch 00031: val_loss improved from 0.66583 to 0.66246, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4026 - acc: 0.9333 - val_loss: 0.6625 - val_acc: 0.5000\n",
            "Epoch 32/300\n",
            "\n",
            "Epoch 00032: val_loss improved from 0.66246 to 0.66151, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4437 - acc: 0.8500 - val_loss: 0.6615 - val_acc: 0.5000\n",
            "Epoch 33/300\n",
            "\n",
            "Epoch 00033: val_loss improved from 0.66151 to 0.65854, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4254 - acc: 0.9167 - val_loss: 0.6585 - val_acc: 0.5000\n",
            "Epoch 34/300\n",
            "\n",
            "Epoch 00034: val_loss improved from 0.65854 to 0.65398, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4136 - acc: 0.9167 - val_loss: 0.6540 - val_acc: 0.5500\n",
            "Epoch 35/300\n",
            "\n",
            "Epoch 00035: val_loss improved from 0.65398 to 0.64865, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4171 - acc: 0.9333 - val_loss: 0.6487 - val_acc: 0.5500\n",
            "Epoch 36/300\n",
            "\n",
            "Epoch 00036: val_loss improved from 0.64865 to 0.64533, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3949 - acc: 0.9500 - val_loss: 0.6453 - val_acc: 0.6500\n",
            "Epoch 37/300\n",
            "\n",
            "Epoch 00037: val_loss improved from 0.64533 to 0.64005, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3540 - acc: 0.9333 - val_loss: 0.6400 - val_acc: 0.6500\n",
            "Epoch 38/300\n",
            "\n",
            "Epoch 00038: val_loss improved from 0.64005 to 0.63712, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3968 - acc: 0.9167 - val_loss: 0.6371 - val_acc: 0.7000\n",
            "Epoch 39/300\n",
            "\n",
            "Epoch 00039: val_loss improved from 0.63712 to 0.63380, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3631 - acc: 0.9333 - val_loss: 0.6338 - val_acc: 0.7000\n",
            "Epoch 40/300\n",
            "\n",
            "Epoch 00040: val_loss improved from 0.63380 to 0.63004, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3499 - acc: 0.9667 - val_loss: 0.6300 - val_acc: 0.7000\n",
            "Epoch 41/300\n",
            "\n",
            "Epoch 00041: val_loss improved from 0.63004 to 0.62345, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3178 - acc: 0.9667 - val_loss: 0.6235 - val_acc: 0.7000\n",
            "Epoch 42/300\n",
            "\n",
            "Epoch 00042: val_loss improved from 0.62345 to 0.61904, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3490 - acc: 0.9333 - val_loss: 0.6190 - val_acc: 0.6500\n",
            "Epoch 43/300\n",
            "\n",
            "Epoch 00043: val_loss improved from 0.61904 to 0.60961, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3258 - acc: 0.9333 - val_loss: 0.6096 - val_acc: 0.7000\n",
            "Epoch 44/300\n",
            "\n",
            "Epoch 00044: val_loss improved from 0.60961 to 0.58874, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2991 - acc: 0.9500 - val_loss: 0.5887 - val_acc: 0.7500\n",
            "Epoch 45/300\n",
            "\n",
            "Epoch 00045: val_loss improved from 0.58874 to 0.56520, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2983 - acc: 0.9167 - val_loss: 0.5652 - val_acc: 0.7500\n",
            "Epoch 46/300\n",
            "\n",
            "Epoch 00046: val_loss improved from 0.56520 to 0.55087, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3292 - acc: 0.9333 - val_loss: 0.5509 - val_acc: 0.8000\n",
            "Epoch 47/300\n",
            "\n",
            "Epoch 00047: val_loss improved from 0.55087 to 0.54151, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3387 - acc: 0.9000 - val_loss: 0.5415 - val_acc: 0.8000\n",
            "Epoch 48/300\n",
            "\n",
            "Epoch 00048: val_loss improved from 0.54151 to 0.53979, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2864 - acc: 0.9000 - val_loss: 0.5398 - val_acc: 0.8000\n",
            "Epoch 49/300\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.53979\n",
            "60/60 - 0s - loss: 0.2496 - acc: 0.9333 - val_loss: 0.5415 - val_acc: 0.7500\n",
            "Epoch 50/300\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.53979\n",
            "60/60 - 0s - loss: 0.2524 - acc: 0.9500 - val_loss: 0.5474 - val_acc: 0.7000\n",
            "Epoch 51/300\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.53979\n",
            "60/60 - 0s - loss: 0.2337 - acc: 0.9500 - val_loss: 0.5491 - val_acc: 0.7000\n",
            "Epoch 52/300\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.53979\n",
            "60/60 - 0s - loss: 0.2136 - acc: 0.9667 - val_loss: 0.5481 - val_acc: 0.7000\n",
            "Epoch 53/300\n",
            "\n",
            "Epoch 00053: val_loss improved from 0.53979 to 0.52810, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2388 - acc: 0.9667 - val_loss: 0.5281 - val_acc: 0.7500\n",
            "Epoch 54/300\n",
            "\n",
            "Epoch 00054: val_loss improved from 0.52810 to 0.51427, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2165 - acc: 0.9833 - val_loss: 0.5143 - val_acc: 0.7500\n",
            "Epoch 55/300\n",
            "\n",
            "Epoch 00055: val_loss improved from 0.51427 to 0.50178, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2151 - acc: 0.9500 - val_loss: 0.5018 - val_acc: 0.7500\n",
            "Epoch 56/300\n",
            "\n",
            "Epoch 00056: val_loss improved from 0.50178 to 0.48701, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2280 - acc: 0.9000 - val_loss: 0.4870 - val_acc: 0.8000\n",
            "Epoch 57/300\n",
            "\n",
            "Epoch 00057: val_loss improved from 0.48701 to 0.48363, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1972 - acc: 0.9500 - val_loss: 0.4836 - val_acc: 0.8000\n",
            "Epoch 58/300\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.48363\n",
            "60/60 - 0s - loss: 0.2208 - acc: 0.9500 - val_loss: 0.4939 - val_acc: 0.7500\n",
            "Epoch 59/300\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.48363\n",
            "60/60 - 0s - loss: 0.1954 - acc: 0.9500 - val_loss: 0.5071 - val_acc: 0.7500\n",
            "Epoch 60/300\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.48363\n",
            "60/60 - 0s - loss: 0.1940 - acc: 0.9667 - val_loss: 0.5362 - val_acc: 0.7500\n",
            "Epoch 61/300\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.48363\n",
            "60/60 - 0s - loss: 0.1560 - acc: 0.9667 - val_loss: 0.5687 - val_acc: 0.7500\n",
            "Epoch 62/300\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.48363\n",
            "60/60 - 0s - loss: 0.1660 - acc: 0.9500 - val_loss: 0.6067 - val_acc: 0.7500\n",
            "Epoch 63/300\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.48363\n",
            "60/60 - 0s - loss: 0.1782 - acc: 0.9667 - val_loss: 0.6378 - val_acc: 0.7500\n",
            "Epoch 64/300\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.48363\n",
            "60/60 - 0s - loss: 0.1720 - acc: 0.9500 - val_loss: 0.6413 - val_acc: 0.7500\n",
            "Epoch 65/300\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.48363\n",
            "60/60 - 0s - loss: 0.1746 - acc: 0.9333 - val_loss: 0.6158 - val_acc: 0.7500\n",
            "Epoch 66/300\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.48363\n",
            "60/60 - 0s - loss: 0.1472 - acc: 1.0000 - val_loss: 0.5975 - val_acc: 0.7500\n",
            "Epoch 67/300\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.48363\n",
            "60/60 - 0s - loss: 0.1643 - acc: 0.9667 - val_loss: 0.5816 - val_acc: 0.7500\n",
            "Epoch 68/300\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.48363\n",
            "60/60 - 0s - loss: 0.1650 - acc: 0.9667 - val_loss: 0.5558 - val_acc: 0.7500\n",
            "Epoch 69/300\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.48363\n",
            "60/60 - 0s - loss: 0.2529 - acc: 0.9000 - val_loss: 0.5234 - val_acc: 0.7500\n",
            "Epoch 70/300\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.48363\n",
            "60/60 - 0s - loss: 0.1822 - acc: 0.9500 - val_loss: 0.5190 - val_acc: 0.7500\n",
            "Epoch 71/300\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.48363\n",
            "60/60 - 0s - loss: 0.1412 - acc: 0.9667 - val_loss: 0.5272 - val_acc: 0.7500\n",
            "Epoch 72/300\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.48363\n",
            "60/60 - 0s - loss: 0.1919 - acc: 0.9500 - val_loss: 0.5389 - val_acc: 0.7500\n",
            "Epoch 73/300\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.48363\n",
            "60/60 - 0s - loss: 0.1968 - acc: 0.9500 - val_loss: 0.5390 - val_acc: 0.7500\n",
            "Epoch 74/300\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.48363\n",
            "60/60 - 0s - loss: 0.1508 - acc: 0.9833 - val_loss: 0.5513 - val_acc: 0.7500\n",
            "Epoch 75/300\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.48363\n",
            "60/60 - 0s - loss: 0.1484 - acc: 0.9500 - val_loss: 0.5559 - val_acc: 0.7500\n",
            "Epoch 76/300\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.48363\n",
            "60/60 - 0s - loss: 0.1902 - acc: 0.9500 - val_loss: 0.5756 - val_acc: 0.7500\n",
            "Epoch 77/300\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.48363\n",
            "60/60 - 0s - loss: 0.1650 - acc: 0.9667 - val_loss: 0.6037 - val_acc: 0.7500\n",
            "Epoch 78/300\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.48363\n",
            "60/60 - 0s - loss: 0.1621 - acc: 0.9667 - val_loss: 0.6417 - val_acc: 0.7500\n",
            "Epoch 79/300\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.48363\n",
            "60/60 - 0s - loss: 0.1826 - acc: 0.9667 - val_loss: 0.6795 - val_acc: 0.7500\n",
            "Epoch 80/300\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.48363\n",
            "60/60 - 0s - loss: 0.1322 - acc: 0.9500 - val_loss: 0.6903 - val_acc: 0.7500\n",
            "Epoch 81/300\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.48363\n",
            "60/60 - 0s - loss: 0.1546 - acc: 0.9500 - val_loss: 0.7016 - val_acc: 0.7500\n",
            "Epoch 82/300\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.48363\n",
            "60/60 - 0s - loss: 0.1508 - acc: 0.9667 - val_loss: 0.7097 - val_acc: 0.7500\n",
            "Epoch 83/300\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.48363\n",
            "60/60 - 0s - loss: 0.1438 - acc: 0.9667 - val_loss: 0.7093 - val_acc: 0.7500\n",
            "Epoch 84/300\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.48363\n",
            "60/60 - 0s - loss: 0.1842 - acc: 0.9333 - val_loss: 0.7286 - val_acc: 0.7500\n",
            "Epoch 85/300\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.48363\n",
            "60/60 - 0s - loss: 0.1319 - acc: 0.9833 - val_loss: 0.7350 - val_acc: 0.7500\n",
            "Epoch 86/300\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.48363\n",
            "60/60 - 0s - loss: 0.1359 - acc: 0.9833 - val_loss: 0.7271 - val_acc: 0.7500\n",
            "Epoch 87/300\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.48363\n",
            "60/60 - 0s - loss: 0.2006 - acc: 0.9333 - val_loss: 0.6926 - val_acc: 0.7500\n",
            "Epoch 88/300\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.48363\n",
            "60/60 - 0s - loss: 0.1866 - acc: 0.9500 - val_loss: 0.6581 - val_acc: 0.7500\n",
            "Epoch 89/300\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.48363\n",
            "60/60 - 0s - loss: 0.1182 - acc: 0.9667 - val_loss: 0.6568 - val_acc: 0.7500\n",
            "Epoch 90/300\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.48363\n",
            "60/60 - 0s - loss: 0.1206 - acc: 0.9833 - val_loss: 0.6713 - val_acc: 0.7500\n",
            "Epoch 91/300\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.48363\n",
            "60/60 - 0s - loss: 0.1154 - acc: 0.9833 - val_loss: 0.6832 - val_acc: 0.7500\n",
            "Epoch 92/300\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.48363\n",
            "60/60 - 0s - loss: 0.2553 - acc: 0.8667 - val_loss: 0.6873 - val_acc: 0.7500\n",
            "Epoch 93/300\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.48363\n",
            "60/60 - 0s - loss: 0.1574 - acc: 0.9500 - val_loss: 0.6890 - val_acc: 0.7500\n",
            "Epoch 94/300\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.48363\n",
            "60/60 - 0s - loss: 0.1845 - acc: 0.9500 - val_loss: 0.6939 - val_acc: 0.7500\n",
            "Epoch 95/300\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.48363\n",
            "60/60 - 0s - loss: 0.1611 - acc: 0.9500 - val_loss: 0.6827 - val_acc: 0.7500\n",
            "Epoch 96/300\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.48363\n",
            "60/60 - 0s - loss: 0.1347 - acc: 0.9667 - val_loss: 0.6732 - val_acc: 0.7500\n",
            "Epoch 97/300\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.48363\n",
            "60/60 - 0s - loss: 0.1010 - acc: 0.9833 - val_loss: 0.6650 - val_acc: 0.7500\n",
            "Epoch 98/300\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.48363\n",
            "60/60 - 0s - loss: 0.1343 - acc: 0.9833 - val_loss: 0.6755 - val_acc: 0.7500\n",
            "Epoch 99/300\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.48363\n",
            "60/60 - 0s - loss: 0.1334 - acc: 0.9833 - val_loss: 0.6748 - val_acc: 0.7500\n",
            "Epoch 100/300\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.48363\n",
            "60/60 - 0s - loss: 0.1191 - acc: 0.9833 - val_loss: 0.6897 - val_acc: 0.7500\n",
            "Epoch 101/300\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.48363\n",
            "60/60 - 0s - loss: 0.0793 - acc: 1.0000 - val_loss: 0.6946 - val_acc: 0.7500\n",
            "Epoch 102/300\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.48363\n",
            "60/60 - 0s - loss: 0.1220 - acc: 0.9833 - val_loss: 0.6779 - val_acc: 0.7500\n",
            "Epoch 103/300\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.48363\n",
            "60/60 - 0s - loss: 0.0754 - acc: 1.0000 - val_loss: 0.6870 - val_acc: 0.8000\n",
            "Epoch 104/300\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.48363\n",
            "60/60 - 0s - loss: 0.0970 - acc: 0.9833 - val_loss: 0.6865 - val_acc: 0.8000\n",
            "Epoch 105/300\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 0.48363\n",
            "60/60 - 0s - loss: 0.1125 - acc: 0.9500 - val_loss: 0.6920 - val_acc: 0.8000\n",
            "Epoch 106/300\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 0.48363\n",
            "60/60 - 0s - loss: 0.1388 - acc: 0.9500 - val_loss: 0.6706 - val_acc: 0.8000\n",
            "Epoch 107/300\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.48363\n",
            "60/60 - 0s - loss: 0.1146 - acc: 0.9667 - val_loss: 0.6650 - val_acc: 0.8000\n",
            "Epoch 108/300\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 0.48363\n",
            "60/60 - 0s - loss: 0.1346 - acc: 0.9500 - val_loss: 0.6511 - val_acc: 0.8000\n",
            "Epoch 109/300\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.48363\n",
            "60/60 - 0s - loss: 0.1073 - acc: 0.9667 - val_loss: 0.6395 - val_acc: 0.8000\n",
            "Epoch 110/300\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.48363\n",
            "60/60 - 0s - loss: 0.1460 - acc: 0.9667 - val_loss: 0.6339 - val_acc: 0.8000\n",
            "Epoch 111/300\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 0.48363\n",
            "60/60 - 0s - loss: 0.1215 - acc: 0.9667 - val_loss: 0.6168 - val_acc: 0.8000\n",
            "Epoch 112/300\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 0.48363\n",
            "60/60 - 0s - loss: 0.1298 - acc: 0.9667 - val_loss: 0.6030 - val_acc: 0.8000\n",
            "Epoch 113/300\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.48363\n",
            "60/60 - 0s - loss: 0.0845 - acc: 0.9833 - val_loss: 0.5829 - val_acc: 0.8000\n",
            "Epoch 114/300\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.48363\n",
            "60/60 - 0s - loss: 0.1239 - acc: 0.9667 - val_loss: 0.5812 - val_acc: 0.8000\n",
            "Epoch 115/300\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.48363\n",
            "60/60 - 0s - loss: 0.1243 - acc: 0.9833 - val_loss: 0.5650 - val_acc: 0.8000\n",
            "Epoch 116/300\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.48363\n",
            "60/60 - 0s - loss: 0.1815 - acc: 0.9500 - val_loss: 0.5775 - val_acc: 0.8000\n",
            "Epoch 117/300\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.48363\n",
            "60/60 - 0s - loss: 0.1604 - acc: 0.9333 - val_loss: 0.5618 - val_acc: 0.8000\n",
            "Epoch 118/300\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.48363\n",
            "60/60 - 0s - loss: 0.1089 - acc: 1.0000 - val_loss: 0.5796 - val_acc: 0.8000\n",
            "Epoch 119/300\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 0.48363\n",
            "60/60 - 0s - loss: 0.0969 - acc: 0.9833 - val_loss: 0.5780 - val_acc: 0.8000\n",
            "Epoch 120/300\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.48363\n",
            "60/60 - 0s - loss: 0.0872 - acc: 1.0000 - val_loss: 0.5955 - val_acc: 0.8000\n",
            "Epoch 121/300\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.48363\n",
            "60/60 - 0s - loss: 0.0879 - acc: 1.0000 - val_loss: 0.6195 - val_acc: 0.8000\n",
            "Epoch 122/300\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.48363\n",
            "60/60 - 0s - loss: 0.1191 - acc: 0.9667 - val_loss: 0.6261 - val_acc: 0.8000\n",
            "Epoch 123/300\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 0.48363\n",
            "60/60 - 0s - loss: 0.1165 - acc: 0.9667 - val_loss: 0.6083 - val_acc: 0.8000\n",
            "Epoch 124/300\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 0.48363\n",
            "60/60 - 0s - loss: 0.0960 - acc: 0.9667 - val_loss: 0.6197 - val_acc: 0.8000\n",
            "Epoch 125/300\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 0.48363\n",
            "60/60 - 0s - loss: 0.0729 - acc: 1.0000 - val_loss: 0.6492 - val_acc: 0.7500\n",
            "Epoch 126/300\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 0.48363\n",
            "60/60 - 0s - loss: 0.0888 - acc: 1.0000 - val_loss: 0.6533 - val_acc: 0.7500\n",
            "Epoch 127/300\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 0.48363\n",
            "60/60 - 0s - loss: 0.1810 - acc: 0.9500 - val_loss: 0.6516 - val_acc: 0.8000\n",
            "Epoch 128/300\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 0.48363\n",
            "60/60 - 0s - loss: 0.0880 - acc: 0.9833 - val_loss: 0.6595 - val_acc: 0.7500\n",
            "Epoch 129/300\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.48363\n",
            "60/60 - 0s - loss: 0.0931 - acc: 0.9833 - val_loss: 0.6622 - val_acc: 0.8000\n",
            "Epoch 130/300\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 0.48363\n",
            "60/60 - 0s - loss: 0.0903 - acc: 1.0000 - val_loss: 0.6606 - val_acc: 0.8000\n",
            "Epoch 131/300\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 0.48363\n",
            "60/60 - 0s - loss: 0.1354 - acc: 0.9500 - val_loss: 0.6425 - val_acc: 0.8000\n",
            "Epoch 132/300\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 0.48363\n",
            "60/60 - 0s - loss: 0.0804 - acc: 0.9667 - val_loss: 0.6288 - val_acc: 0.8000\n",
            "Epoch 133/300\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.48363\n",
            "60/60 - 0s - loss: 0.1061 - acc: 0.9833 - val_loss: 0.6141 - val_acc: 0.8000\n",
            "Epoch 134/300\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.48363\n",
            "60/60 - 0s - loss: 0.0871 - acc: 0.9833 - val_loss: 0.6111 - val_acc: 0.8000\n",
            "Epoch 135/300\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 0.48363\n",
            "60/60 - 0s - loss: 0.0945 - acc: 0.9833 - val_loss: 0.6228 - val_acc: 0.8000\n",
            "Epoch 136/300\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.48363\n",
            "60/60 - 0s - loss: 0.0957 - acc: 0.9833 - val_loss: 0.6295 - val_acc: 0.8000\n",
            "Epoch 137/300\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 0.48363\n",
            "60/60 - 0s - loss: 0.1067 - acc: 0.9833 - val_loss: 0.6446 - val_acc: 0.8000\n",
            "Epoch 138/300\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.48363\n",
            "60/60 - 0s - loss: 0.0835 - acc: 0.9833 - val_loss: 0.6606 - val_acc: 0.8000\n",
            "Epoch 139/300\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.48363\n",
            "60/60 - 0s - loss: 0.0996 - acc: 0.9833 - val_loss: 0.6663 - val_acc: 0.8000\n",
            "Epoch 140/300\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 0.48363\n",
            "60/60 - 0s - loss: 0.1102 - acc: 0.9667 - val_loss: 0.6517 - val_acc: 0.8000\n",
            "Epoch 141/300\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.48363\n",
            "60/60 - 0s - loss: 0.0814 - acc: 1.0000 - val_loss: 0.6071 - val_acc: 0.8000\n",
            "Epoch 142/300\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.48363\n",
            "60/60 - 0s - loss: 0.1207 - acc: 0.9667 - val_loss: 0.5657 - val_acc: 0.8000\n",
            "Epoch 143/300\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 0.48363\n",
            "60/60 - 0s - loss: 0.1339 - acc: 0.9667 - val_loss: 0.5272 - val_acc: 0.8000\n",
            "Epoch 144/300\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 0.48363\n",
            "60/60 - 0s - loss: 0.1543 - acc: 0.9167 - val_loss: 0.4920 - val_acc: 0.8000\n",
            "Epoch 145/300\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.48363\n",
            "60/60 - 0s - loss: 0.1070 - acc: 0.9833 - val_loss: 0.5018 - val_acc: 0.8000\n",
            "Epoch 146/300\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.48363\n",
            "60/60 - 0s - loss: 0.0935 - acc: 0.9833 - val_loss: 0.5338 - val_acc: 0.8000\n",
            "Epoch 147/300\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.48363\n",
            "60/60 - 0s - loss: 0.0837 - acc: 1.0000 - val_loss: 0.5639 - val_acc: 0.8000\n",
            "Epoch 148/300\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 0.48363\n",
            "60/60 - 0s - loss: 0.0942 - acc: 1.0000 - val_loss: 0.5876 - val_acc: 0.8000\n",
            "Epoch 149/300\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.48363\n",
            "60/60 - 0s - loss: 0.0783 - acc: 1.0000 - val_loss: 0.5993 - val_acc: 0.8000\n",
            "Epoch 150/300\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.48363\n",
            "60/60 - 0s - loss: 0.0908 - acc: 1.0000 - val_loss: 0.6049 - val_acc: 0.8000\n",
            "Epoch 151/300\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.48363\n",
            "60/60 - 0s - loss: 0.1351 - acc: 0.9833 - val_loss: 0.5974 - val_acc: 0.8000\n",
            "Epoch 152/300\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.48363\n",
            "60/60 - 0s - loss: 0.0755 - acc: 1.0000 - val_loss: 0.6077 - val_acc: 0.8000\n",
            "Epoch 153/300\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 0.48363\n",
            "60/60 - 0s - loss: 0.2074 - acc: 0.9333 - val_loss: 0.5714 - val_acc: 0.8000\n",
            "Epoch 154/300\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.48363\n",
            "60/60 - 0s - loss: 0.1692 - acc: 0.9500 - val_loss: 0.4999 - val_acc: 0.8000\n",
            "Epoch 155/300\n",
            "\n",
            "Epoch 00155: val_loss improved from 0.48363 to 0.46084, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1017 - acc: 0.9833 - val_loss: 0.4608 - val_acc: 0.8000\n",
            "Epoch 156/300\n",
            "\n",
            "Epoch 00156: val_loss improved from 0.46084 to 0.43866, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1061 - acc: 0.9833 - val_loss: 0.4387 - val_acc: 0.8500\n",
            "Epoch 157/300\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.43866\n",
            "60/60 - 0s - loss: 0.1204 - acc: 0.9667 - val_loss: 0.4440 - val_acc: 0.8500\n",
            "Epoch 158/300\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.43866\n",
            "60/60 - 0s - loss: 0.1090 - acc: 0.9500 - val_loss: 0.4774 - val_acc: 0.8000\n",
            "Epoch 159/300\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.43866\n",
            "60/60 - 0s - loss: 0.1104 - acc: 0.9333 - val_loss: 0.4956 - val_acc: 0.8000\n",
            "Epoch 160/300\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.43866\n",
            "60/60 - 0s - loss: 0.0905 - acc: 0.9833 - val_loss: 0.5096 - val_acc: 0.8000\n",
            "Epoch 161/300\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.43866\n",
            "60/60 - 0s - loss: 0.0757 - acc: 1.0000 - val_loss: 0.5264 - val_acc: 0.8000\n",
            "Epoch 162/300\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.43866\n",
            "60/60 - 0s - loss: 0.0751 - acc: 1.0000 - val_loss: 0.5387 - val_acc: 0.8000\n",
            "Epoch 163/300\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.43866\n",
            "60/60 - 0s - loss: 0.0735 - acc: 1.0000 - val_loss: 0.5401 - val_acc: 0.8000\n",
            "Epoch 164/300\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.43866\n",
            "60/60 - 0s - loss: 0.1090 - acc: 0.9833 - val_loss: 0.5286 - val_acc: 0.8000\n",
            "Epoch 165/300\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.43866\n",
            "60/60 - 0s - loss: 0.1112 - acc: 0.9500 - val_loss: 0.5256 - val_acc: 0.8000\n",
            "Epoch 166/300\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 0.43866\n",
            "60/60 - 0s - loss: 0.0880 - acc: 0.9833 - val_loss: 0.5456 - val_acc: 0.8000\n",
            "Epoch 167/300\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.43866\n",
            "60/60 - 0s - loss: 0.0948 - acc: 1.0000 - val_loss: 0.5571 - val_acc: 0.8000\n",
            "Epoch 168/300\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.43866\n",
            "60/60 - 0s - loss: 0.0607 - acc: 1.0000 - val_loss: 0.5681 - val_acc: 0.8000\n",
            "Epoch 169/300\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 0.43866\n",
            "60/60 - 0s - loss: 0.0661 - acc: 0.9833 - val_loss: 0.5701 - val_acc: 0.8000\n",
            "Epoch 170/300\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.43866\n",
            "60/60 - 0s - loss: 0.1099 - acc: 0.9667 - val_loss: 0.5786 - val_acc: 0.8000\n",
            "Epoch 171/300\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.43866\n",
            "60/60 - 0s - loss: 0.1086 - acc: 0.9667 - val_loss: 0.5724 - val_acc: 0.8000\n",
            "Epoch 172/300\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.43866\n",
            "60/60 - 0s - loss: 0.1541 - acc: 0.9333 - val_loss: 0.5466 - val_acc: 0.8000\n",
            "Epoch 173/300\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.43866\n",
            "60/60 - 0s - loss: 0.0879 - acc: 0.9667 - val_loss: 0.5314 - val_acc: 0.8000\n",
            "Epoch 174/300\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.43866\n",
            "60/60 - 0s - loss: 0.1193 - acc: 0.9667 - val_loss: 0.5167 - val_acc: 0.8000\n",
            "Epoch 175/300\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.43866\n",
            "60/60 - 0s - loss: 0.0679 - acc: 1.0000 - val_loss: 0.5048 - val_acc: 0.8000\n",
            "Epoch 176/300\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 0.43866\n",
            "60/60 - 0s - loss: 0.0930 - acc: 0.9833 - val_loss: 0.5008 - val_acc: 0.8000\n",
            "Epoch 177/300\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.43866\n",
            "60/60 - 0s - loss: 0.0751 - acc: 1.0000 - val_loss: 0.5019 - val_acc: 0.8000\n",
            "Epoch 178/300\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.43866\n",
            "60/60 - 0s - loss: 0.0990 - acc: 0.9667 - val_loss: 0.5113 - val_acc: 0.8000\n",
            "Epoch 179/300\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.43866\n",
            "60/60 - 0s - loss: 0.0980 - acc: 0.9833 - val_loss: 0.5197 - val_acc: 0.8000\n",
            "Epoch 180/300\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.43866\n",
            "60/60 - 0s - loss: 0.0884 - acc: 0.9833 - val_loss: 0.5315 - val_acc: 0.8000\n",
            "Epoch 181/300\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.43866\n",
            "60/60 - 0s - loss: 0.0853 - acc: 0.9833 - val_loss: 0.5303 - val_acc: 0.8000\n",
            "Epoch 182/300\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.43866\n",
            "60/60 - 0s - loss: 0.1074 - acc: 0.9833 - val_loss: 0.5312 - val_acc: 0.8000\n",
            "Epoch 183/300\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.43866\n",
            "60/60 - 0s - loss: 0.0749 - acc: 1.0000 - val_loss: 0.5576 - val_acc: 0.8000\n",
            "Epoch 184/300\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 0.43866\n",
            "60/60 - 0s - loss: 0.1030 - acc: 0.9667 - val_loss: 0.5802 - val_acc: 0.8000\n",
            "Epoch 185/300\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.43866\n",
            "60/60 - 0s - loss: 0.1139 - acc: 0.9500 - val_loss: 0.5619 - val_acc: 0.8000\n",
            "Epoch 186/300\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.43866\n",
            "60/60 - 0s - loss: 0.0804 - acc: 0.9833 - val_loss: 0.5336 - val_acc: 0.8000\n",
            "Epoch 187/300\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 0.43866\n",
            "60/60 - 0s - loss: 0.0892 - acc: 0.9833 - val_loss: 0.5129 - val_acc: 0.8000\n",
            "Epoch 188/300\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.43866\n",
            "60/60 - 0s - loss: 0.0869 - acc: 0.9833 - val_loss: 0.5215 - val_acc: 0.8000\n",
            "Epoch 189/300\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.43866\n",
            "60/60 - 0s - loss: 0.0875 - acc: 0.9833 - val_loss: 0.5437 - val_acc: 0.8000\n",
            "Epoch 190/300\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.43866\n",
            "60/60 - 0s - loss: 0.0778 - acc: 1.0000 - val_loss: 0.5757 - val_acc: 0.8000\n",
            "Epoch 191/300\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.43866\n",
            "60/60 - 0s - loss: 0.1253 - acc: 0.9667 - val_loss: 0.5781 - val_acc: 0.8000\n",
            "Epoch 192/300\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 0.43866\n",
            "60/60 - 0s - loss: 0.0535 - acc: 1.0000 - val_loss: 0.5744 - val_acc: 0.8000\n",
            "Epoch 193/300\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.43866\n",
            "60/60 - 0s - loss: 0.0699 - acc: 0.9833 - val_loss: 0.5671 - val_acc: 0.8000\n",
            "Epoch 194/300\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.43866\n",
            "60/60 - 0s - loss: 0.0596 - acc: 1.0000 - val_loss: 0.5704 - val_acc: 0.8000\n",
            "Epoch 195/300\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.43866\n",
            "60/60 - 0s - loss: 0.0619 - acc: 1.0000 - val_loss: 0.5788 - val_acc: 0.8000\n",
            "Epoch 196/300\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.43866\n",
            "60/60 - 0s - loss: 0.0994 - acc: 0.9833 - val_loss: 0.5824 - val_acc: 0.8000\n",
            "Epoch 197/300\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.43866\n",
            "60/60 - 0s - loss: 0.1103 - acc: 0.9500 - val_loss: 0.5825 - val_acc: 0.8000\n",
            "Epoch 198/300\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.43866\n",
            "60/60 - 0s - loss: 0.0666 - acc: 0.9833 - val_loss: 0.5904 - val_acc: 0.8000\n",
            "Epoch 199/300\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.43866\n",
            "60/60 - 0s - loss: 0.0798 - acc: 1.0000 - val_loss: 0.5811 - val_acc: 0.8000\n",
            "Epoch 200/300\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.43866\n",
            "60/60 - 0s - loss: 0.0741 - acc: 0.9833 - val_loss: 0.5773 - val_acc: 0.8000\n",
            "Epoch 201/300\n",
            "\n",
            "Epoch 00201: val_loss did not improve from 0.43866\n",
            "60/60 - 0s - loss: 0.0881 - acc: 0.9833 - val_loss: 0.5859 - val_acc: 0.8000\n",
            "Epoch 202/300\n",
            "\n",
            "Epoch 00202: val_loss did not improve from 0.43866\n",
            "60/60 - 0s - loss: 0.0792 - acc: 0.9833 - val_loss: 0.5993 - val_acc: 0.8000\n",
            "Epoch 203/300\n",
            "\n",
            "Epoch 00203: val_loss did not improve from 0.43866\n",
            "60/60 - 0s - loss: 0.1056 - acc: 0.9500 - val_loss: 0.6298 - val_acc: 0.8000\n",
            "Epoch 204/300\n",
            "\n",
            "Epoch 00204: val_loss did not improve from 0.43866\n",
            "60/60 - 0s - loss: 0.0708 - acc: 0.9833 - val_loss: 0.6436 - val_acc: 0.8000\n",
            "Epoch 205/300\n",
            "\n",
            "Epoch 00205: val_loss did not improve from 0.43866\n",
            "60/60 - 0s - loss: 0.0715 - acc: 0.9833 - val_loss: 0.6329 - val_acc: 0.8000\n",
            "Epoch 206/300\n",
            "\n",
            "Epoch 00206: val_loss did not improve from 0.43866\n",
            "60/60 - 0s - loss: 0.1022 - acc: 0.9833 - val_loss: 0.6029 - val_acc: 0.8000\n",
            "Epoch 207/300\n",
            "\n",
            "Epoch 00207: val_loss did not improve from 0.43866\n",
            "60/60 - 0s - loss: 0.0556 - acc: 1.0000 - val_loss: 0.5717 - val_acc: 0.8000\n",
            "Epoch 208/300\n",
            "\n",
            "Epoch 00208: val_loss did not improve from 0.43866\n",
            "60/60 - 0s - loss: 0.0549 - acc: 1.0000 - val_loss: 0.5551 - val_acc: 0.8000\n",
            "Epoch 209/300\n",
            "\n",
            "Epoch 00209: val_loss did not improve from 0.43866\n",
            "60/60 - 0s - loss: 0.1252 - acc: 0.9500 - val_loss: 0.5379 - val_acc: 0.8000\n",
            "Epoch 210/300\n",
            "\n",
            "Epoch 00210: val_loss did not improve from 0.43866\n",
            "60/60 - 0s - loss: 0.0643 - acc: 1.0000 - val_loss: 0.5340 - val_acc: 0.8000\n",
            "Epoch 211/300\n",
            "\n",
            "Epoch 00211: val_loss did not improve from 0.43866\n",
            "60/60 - 0s - loss: 0.0462 - acc: 1.0000 - val_loss: 0.5380 - val_acc: 0.8000\n",
            "Epoch 212/300\n",
            "\n",
            "Epoch 00212: val_loss did not improve from 0.43866\n",
            "60/60 - 0s - loss: 0.0824 - acc: 1.0000 - val_loss: 0.5391 - val_acc: 0.8000\n",
            "Epoch 213/300\n",
            "\n",
            "Epoch 00213: val_loss did not improve from 0.43866\n",
            "60/60 - 0s - loss: 0.0664 - acc: 0.9833 - val_loss: 0.5290 - val_acc: 0.8000\n",
            "Epoch 214/300\n",
            "\n",
            "Epoch 00214: val_loss did not improve from 0.43866\n",
            "60/60 - 0s - loss: 0.0765 - acc: 0.9833 - val_loss: 0.5328 - val_acc: 0.8000\n",
            "Epoch 215/300\n",
            "\n",
            "Epoch 00215: val_loss did not improve from 0.43866\n",
            "60/60 - 0s - loss: 0.0629 - acc: 1.0000 - val_loss: 0.5392 - val_acc: 0.8000\n",
            "Epoch 216/300\n",
            "\n",
            "Epoch 00216: val_loss did not improve from 0.43866\n",
            "60/60 - 0s - loss: 0.0398 - acc: 1.0000 - val_loss: 0.5379 - val_acc: 0.8000\n",
            "Epoch 217/300\n",
            "\n",
            "Epoch 00217: val_loss did not improve from 0.43866\n",
            "60/60 - 0s - loss: 0.0701 - acc: 0.9833 - val_loss: 0.5254 - val_acc: 0.8000\n",
            "Epoch 218/300\n",
            "\n",
            "Epoch 00218: val_loss did not improve from 0.43866\n",
            "60/60 - 0s - loss: 0.0497 - acc: 1.0000 - val_loss: 0.5181 - val_acc: 0.8000\n",
            "Epoch 219/300\n",
            "\n",
            "Epoch 00219: val_loss did not improve from 0.43866\n",
            "60/60 - 0s - loss: 0.0425 - acc: 1.0000 - val_loss: 0.5089 - val_acc: 0.8000\n",
            "Epoch 220/300\n",
            "\n",
            "Epoch 00220: val_loss did not improve from 0.43866\n",
            "60/60 - 0s - loss: 0.0500 - acc: 1.0000 - val_loss: 0.4980 - val_acc: 0.8000\n",
            "Epoch 221/300\n",
            "\n",
            "Epoch 00221: val_loss did not improve from 0.43866\n",
            "60/60 - 0s - loss: 0.0630 - acc: 1.0000 - val_loss: 0.4956 - val_acc: 0.8000\n",
            "Epoch 222/300\n",
            "\n",
            "Epoch 00222: val_loss did not improve from 0.43866\n",
            "60/60 - 0s - loss: 0.0608 - acc: 1.0000 - val_loss: 0.5124 - val_acc: 0.8000\n",
            "Epoch 223/300\n",
            "\n",
            "Epoch 00223: val_loss did not improve from 0.43866\n",
            "60/60 - 0s - loss: 0.1017 - acc: 0.9500 - val_loss: 0.5273 - val_acc: 0.8000\n",
            "Epoch 224/300\n",
            "\n",
            "Epoch 00224: val_loss did not improve from 0.43866\n",
            "60/60 - 0s - loss: 0.0453 - acc: 1.0000 - val_loss: 0.5384 - val_acc: 0.8000\n",
            "Epoch 225/300\n",
            "\n",
            "Epoch 00225: val_loss did not improve from 0.43866\n",
            "60/60 - 0s - loss: 0.0475 - acc: 1.0000 - val_loss: 0.5525 - val_acc: 0.8000\n",
            "Epoch 226/300\n",
            "\n",
            "Epoch 00226: val_loss did not improve from 0.43866\n",
            "60/60 - 0s - loss: 0.0516 - acc: 1.0000 - val_loss: 0.5580 - val_acc: 0.8000\n",
            "Epoch 227/300\n",
            "\n",
            "Epoch 00227: val_loss did not improve from 0.43866\n",
            "60/60 - 0s - loss: 0.0803 - acc: 0.9833 - val_loss: 0.5699 - val_acc: 0.8000\n",
            "Epoch 228/300\n",
            "\n",
            "Epoch 00228: val_loss did not improve from 0.43866\n",
            "60/60 - 0s - loss: 0.0550 - acc: 0.9833 - val_loss: 0.5793 - val_acc: 0.8000\n",
            "Epoch 229/300\n",
            "\n",
            "Epoch 00229: val_loss did not improve from 0.43866\n",
            "60/60 - 0s - loss: 0.0612 - acc: 1.0000 - val_loss: 0.5812 - val_acc: 0.8000\n",
            "Epoch 230/300\n",
            "\n",
            "Epoch 00230: val_loss did not improve from 0.43866\n",
            "60/60 - 0s - loss: 0.0901 - acc: 0.9667 - val_loss: 0.5987 - val_acc: 0.8000\n",
            "Epoch 231/300\n",
            "\n",
            "Epoch 00231: val_loss did not improve from 0.43866\n",
            "60/60 - 0s - loss: 0.0658 - acc: 0.9833 - val_loss: 0.6062 - val_acc: 0.8000\n",
            "Epoch 232/300\n",
            "\n",
            "Epoch 00232: val_loss did not improve from 0.43866\n",
            "60/60 - 0s - loss: 0.0757 - acc: 0.9667 - val_loss: 0.5937 - val_acc: 0.8000\n",
            "Epoch 233/300\n",
            "\n",
            "Epoch 00233: val_loss did not improve from 0.43866\n",
            "60/60 - 0s - loss: 0.0571 - acc: 0.9833 - val_loss: 0.5860 - val_acc: 0.8000\n",
            "Epoch 234/300\n",
            "\n",
            "Epoch 00234: val_loss did not improve from 0.43866\n",
            "60/60 - 0s - loss: 0.0662 - acc: 1.0000 - val_loss: 0.5834 - val_acc: 0.8000\n",
            "Epoch 235/300\n",
            "\n",
            "Epoch 00235: val_loss did not improve from 0.43866\n",
            "60/60 - 0s - loss: 0.0803 - acc: 0.9833 - val_loss: 0.5366 - val_acc: 0.8000\n",
            "Epoch 236/300\n",
            "\n",
            "Epoch 00236: val_loss did not improve from 0.43866\n",
            "60/60 - 0s - loss: 0.1167 - acc: 0.9333 - val_loss: 0.4928 - val_acc: 0.8000\n",
            "Epoch 237/300\n",
            "\n",
            "Epoch 00237: val_loss did not improve from 0.43866\n",
            "60/60 - 0s - loss: 0.0697 - acc: 1.0000 - val_loss: 0.4843 - val_acc: 0.8000\n",
            "Epoch 238/300\n",
            "\n",
            "Epoch 00238: val_loss did not improve from 0.43866\n",
            "60/60 - 0s - loss: 0.0518 - acc: 1.0000 - val_loss: 0.4811 - val_acc: 0.8000\n",
            "Epoch 239/300\n",
            "\n",
            "Epoch 00239: val_loss did not improve from 0.43866\n",
            "60/60 - 0s - loss: 0.0882 - acc: 0.9667 - val_loss: 0.4741 - val_acc: 0.8000\n",
            "Epoch 240/300\n",
            "\n",
            "Epoch 00240: val_loss did not improve from 0.43866\n",
            "60/60 - 0s - loss: 0.1198 - acc: 0.9500 - val_loss: 0.4550 - val_acc: 0.7000\n",
            "Epoch 241/300\n",
            "\n",
            "Epoch 00241: val_loss did not improve from 0.43866\n",
            "60/60 - 0s - loss: 0.0500 - acc: 1.0000 - val_loss: 0.4491 - val_acc: 0.7500\n",
            "Epoch 242/300\n",
            "\n",
            "Epoch 00242: val_loss improved from 0.43866 to 0.43581, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.0789 - acc: 1.0000 - val_loss: 0.4358 - val_acc: 0.7500\n",
            "Epoch 243/300\n",
            "\n",
            "Epoch 00243: val_loss improved from 0.43581 to 0.43082, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.0962 - acc: 0.9667 - val_loss: 0.4308 - val_acc: 0.7500\n",
            "Epoch 244/300\n",
            "\n",
            "Epoch 00244: val_loss improved from 0.43082 to 0.42243, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.0512 - acc: 1.0000 - val_loss: 0.4224 - val_acc: 0.7500\n",
            "Epoch 245/300\n",
            "\n",
            "Epoch 00245: val_loss improved from 0.42243 to 0.40891, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.0555 - acc: 0.9833 - val_loss: 0.4089 - val_acc: 0.7500\n",
            "Epoch 246/300\n",
            "\n",
            "Epoch 00246: val_loss improved from 0.40891 to 0.39677, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.0446 - acc: 1.0000 - val_loss: 0.3968 - val_acc: 0.7500\n",
            "Epoch 247/300\n",
            "\n",
            "Epoch 00247: val_loss improved from 0.39677 to 0.38819, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.0613 - acc: 0.9833 - val_loss: 0.3882 - val_acc: 0.8000\n",
            "Epoch 248/300\n",
            "\n",
            "Epoch 00248: val_loss improved from 0.38819 to 0.37373, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.0795 - acc: 0.9833 - val_loss: 0.3737 - val_acc: 0.8000\n",
            "Epoch 249/300\n",
            "\n",
            "Epoch 00249: val_loss improved from 0.37373 to 0.36836, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.0385 - acc: 1.0000 - val_loss: 0.3684 - val_acc: 0.8000\n",
            "Epoch 250/300\n",
            "\n",
            "Epoch 00250: val_loss improved from 0.36836 to 0.36367, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.0751 - acc: 0.9667 - val_loss: 0.3637 - val_acc: 0.8000\n",
            "Epoch 251/300\n",
            "\n",
            "Epoch 00251: val_loss improved from 0.36367 to 0.36081, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.0743 - acc: 0.9833 - val_loss: 0.3608 - val_acc: 0.8000\n",
            "Epoch 252/300\n",
            "\n",
            "Epoch 00252: val_loss did not improve from 0.36081\n",
            "60/60 - 0s - loss: 0.0437 - acc: 1.0000 - val_loss: 0.3736 - val_acc: 0.8500\n",
            "Epoch 253/300\n",
            "\n",
            "Epoch 00253: val_loss did not improve from 0.36081\n",
            "60/60 - 0s - loss: 0.0615 - acc: 1.0000 - val_loss: 0.3917 - val_acc: 0.8000\n",
            "Epoch 254/300\n",
            "\n",
            "Epoch 00254: val_loss did not improve from 0.36081\n",
            "60/60 - 0s - loss: 0.0768 - acc: 1.0000 - val_loss: 0.4129 - val_acc: 0.8000\n",
            "Epoch 255/300\n",
            "\n",
            "Epoch 00255: val_loss did not improve from 0.36081\n",
            "60/60 - 0s - loss: 0.0906 - acc: 0.9833 - val_loss: 0.4189 - val_acc: 0.8000\n",
            "Epoch 256/300\n",
            "\n",
            "Epoch 00256: val_loss did not improve from 0.36081\n",
            "60/60 - 0s - loss: 0.0362 - acc: 1.0000 - val_loss: 0.4202 - val_acc: 0.8000\n",
            "Epoch 257/300\n",
            "\n",
            "Epoch 00257: val_loss did not improve from 0.36081\n",
            "60/60 - 0s - loss: 0.0489 - acc: 1.0000 - val_loss: 0.4255 - val_acc: 0.8000\n",
            "Epoch 258/300\n",
            "\n",
            "Epoch 00258: val_loss did not improve from 0.36081\n",
            "60/60 - 0s - loss: 0.0851 - acc: 0.9667 - val_loss: 0.4263 - val_acc: 0.8500\n",
            "Epoch 259/300\n",
            "\n",
            "Epoch 00259: val_loss did not improve from 0.36081\n",
            "60/60 - 0s - loss: 0.0401 - acc: 1.0000 - val_loss: 0.4371 - val_acc: 0.8500\n",
            "Epoch 260/300\n",
            "\n",
            "Epoch 00260: val_loss did not improve from 0.36081\n",
            "60/60 - 0s - loss: 0.0433 - acc: 1.0000 - val_loss: 0.4530 - val_acc: 0.8500\n",
            "Epoch 261/300\n",
            "\n",
            "Epoch 00261: val_loss did not improve from 0.36081\n",
            "60/60 - 0s - loss: 0.0652 - acc: 0.9667 - val_loss: 0.4603 - val_acc: 0.8500\n",
            "Epoch 262/300\n",
            "\n",
            "Epoch 00262: val_loss did not improve from 0.36081\n",
            "60/60 - 0s - loss: 0.0534 - acc: 1.0000 - val_loss: 0.4708 - val_acc: 0.8500\n",
            "Epoch 263/300\n",
            "\n",
            "Epoch 00263: val_loss did not improve from 0.36081\n",
            "60/60 - 0s - loss: 0.0512 - acc: 1.0000 - val_loss: 0.4758 - val_acc: 0.8000\n",
            "Epoch 264/300\n",
            "\n",
            "Epoch 00264: val_loss did not improve from 0.36081\n",
            "60/60 - 0s - loss: 0.0637 - acc: 0.9833 - val_loss: 0.4922 - val_acc: 0.8000\n",
            "Epoch 265/300\n",
            "\n",
            "Epoch 00265: val_loss did not improve from 0.36081\n",
            "60/60 - 0s - loss: 0.0392 - acc: 1.0000 - val_loss: 0.5155 - val_acc: 0.8000\n",
            "Epoch 266/300\n",
            "\n",
            "Epoch 00266: val_loss did not improve from 0.36081\n",
            "60/60 - 0s - loss: 0.0451 - acc: 1.0000 - val_loss: 0.5373 - val_acc: 0.8000\n",
            "Epoch 267/300\n",
            "\n",
            "Epoch 00267: val_loss did not improve from 0.36081\n",
            "60/60 - 0s - loss: 0.0687 - acc: 1.0000 - val_loss: 0.5091 - val_acc: 0.8000\n",
            "Epoch 268/300\n",
            "\n",
            "Epoch 00268: val_loss did not improve from 0.36081\n",
            "60/60 - 0s - loss: 0.0384 - acc: 1.0000 - val_loss: 0.4956 - val_acc: 0.8000\n",
            "Epoch 269/300\n",
            "\n",
            "Epoch 00269: val_loss did not improve from 0.36081\n",
            "60/60 - 0s - loss: 0.0482 - acc: 0.9833 - val_loss: 0.4882 - val_acc: 0.8500\n",
            "Epoch 270/300\n",
            "\n",
            "Epoch 00270: val_loss did not improve from 0.36081\n",
            "60/60 - 0s - loss: 0.0648 - acc: 0.9833 - val_loss: 0.4749 - val_acc: 0.8500\n",
            "Epoch 271/300\n",
            "\n",
            "Epoch 00271: val_loss did not improve from 0.36081\n",
            "60/60 - 0s - loss: 0.0961 - acc: 0.9667 - val_loss: 0.4731 - val_acc: 0.8500\n",
            "Epoch 272/300\n",
            "\n",
            "Epoch 00272: val_loss did not improve from 0.36081\n",
            "60/60 - 0s - loss: 0.0540 - acc: 1.0000 - val_loss: 0.4763 - val_acc: 0.8000\n",
            "Epoch 273/300\n",
            "\n",
            "Epoch 00273: val_loss did not improve from 0.36081\n",
            "60/60 - 0s - loss: 0.0444 - acc: 1.0000 - val_loss: 0.4742 - val_acc: 0.8000\n",
            "Epoch 274/300\n",
            "\n",
            "Epoch 00274: val_loss did not improve from 0.36081\n",
            "60/60 - 0s - loss: 0.0537 - acc: 0.9833 - val_loss: 0.4759 - val_acc: 0.8000\n",
            "Epoch 275/300\n",
            "\n",
            "Epoch 00275: val_loss did not improve from 0.36081\n",
            "60/60 - 0s - loss: 0.0870 - acc: 0.9833 - val_loss: 0.4775 - val_acc: 0.8000\n",
            "Epoch 276/300\n",
            "\n",
            "Epoch 00276: val_loss did not improve from 0.36081\n",
            "60/60 - 0s - loss: 0.0396 - acc: 1.0000 - val_loss: 0.4972 - val_acc: 0.8000\n",
            "Epoch 277/300\n",
            "\n",
            "Epoch 00277: val_loss did not improve from 0.36081\n",
            "60/60 - 0s - loss: 0.0525 - acc: 1.0000 - val_loss: 0.5042 - val_acc: 0.8000\n",
            "Epoch 278/300\n",
            "\n",
            "Epoch 00278: val_loss did not improve from 0.36081\n",
            "60/60 - 0s - loss: 0.0876 - acc: 0.9667 - val_loss: 0.4810 - val_acc: 0.8000\n",
            "Epoch 279/300\n",
            "\n",
            "Epoch 00279: val_loss did not improve from 0.36081\n",
            "60/60 - 0s - loss: 0.0366 - acc: 1.0000 - val_loss: 0.4854 - val_acc: 0.8000\n",
            "Epoch 280/300\n",
            "\n",
            "Epoch 00280: val_loss did not improve from 0.36081\n",
            "60/60 - 0s - loss: 0.0549 - acc: 1.0000 - val_loss: 0.4856 - val_acc: 0.8000\n",
            "Epoch 281/300\n",
            "\n",
            "Epoch 00281: val_loss did not improve from 0.36081\n",
            "60/60 - 0s - loss: 0.0495 - acc: 1.0000 - val_loss: 0.4660 - val_acc: 0.8000\n",
            "Epoch 282/300\n",
            "\n",
            "Epoch 00282: val_loss did not improve from 0.36081\n",
            "60/60 - 0s - loss: 0.0426 - acc: 1.0000 - val_loss: 0.4447 - val_acc: 0.8000\n",
            "Epoch 283/300\n",
            "\n",
            "Epoch 00283: val_loss did not improve from 0.36081\n",
            "60/60 - 0s - loss: 0.0622 - acc: 1.0000 - val_loss: 0.4249 - val_acc: 0.8500\n",
            "Epoch 284/300\n",
            "\n",
            "Epoch 00284: val_loss did not improve from 0.36081\n",
            "60/60 - 0s - loss: 0.0437 - acc: 1.0000 - val_loss: 0.4006 - val_acc: 0.8500\n",
            "Epoch 285/300\n",
            "\n",
            "Epoch 00285: val_loss did not improve from 0.36081\n",
            "60/60 - 0s - loss: 0.0450 - acc: 1.0000 - val_loss: 0.3870 - val_acc: 0.8500\n",
            "Epoch 286/300\n",
            "\n",
            "Epoch 00286: val_loss did not improve from 0.36081\n",
            "60/60 - 0s - loss: 0.0537 - acc: 0.9833 - val_loss: 0.3931 - val_acc: 0.8500\n",
            "Epoch 287/300\n",
            "\n",
            "Epoch 00287: val_loss did not improve from 0.36081\n",
            "60/60 - 0s - loss: 0.0865 - acc: 0.9833 - val_loss: 0.4301 - val_acc: 0.8500\n",
            "Epoch 288/300\n",
            "\n",
            "Epoch 00288: val_loss did not improve from 0.36081\n",
            "60/60 - 0s - loss: 0.0296 - acc: 1.0000 - val_loss: 0.4819 - val_acc: 0.8500\n",
            "Epoch 289/300\n",
            "\n",
            "Epoch 00289: val_loss did not improve from 0.36081\n",
            "60/60 - 0s - loss: 0.0548 - acc: 0.9833 - val_loss: 0.5141 - val_acc: 0.8000\n",
            "Epoch 290/300\n",
            "\n",
            "Epoch 00290: val_loss did not improve from 0.36081\n",
            "60/60 - 0s - loss: 0.0430 - acc: 1.0000 - val_loss: 0.5404 - val_acc: 0.8000\n",
            "Epoch 291/300\n",
            "\n",
            "Epoch 00291: val_loss did not improve from 0.36081\n",
            "60/60 - 0s - loss: 0.0453 - acc: 1.0000 - val_loss: 0.5531 - val_acc: 0.8000\n",
            "Epoch 292/300\n",
            "\n",
            "Epoch 00292: val_loss did not improve from 0.36081\n",
            "60/60 - 0s - loss: 0.0604 - acc: 0.9833 - val_loss: 0.5546 - val_acc: 0.8000\n",
            "Epoch 293/300\n",
            "\n",
            "Epoch 00293: val_loss did not improve from 0.36081\n",
            "60/60 - 0s - loss: 0.0490 - acc: 1.0000 - val_loss: 0.5387 - val_acc: 0.8000\n",
            "Epoch 294/300\n",
            "\n",
            "Epoch 00294: val_loss did not improve from 0.36081\n",
            "60/60 - 0s - loss: 0.0474 - acc: 1.0000 - val_loss: 0.5260 - val_acc: 0.8000\n",
            "Epoch 295/300\n",
            "\n",
            "Epoch 00295: val_loss did not improve from 0.36081\n",
            "60/60 - 0s - loss: 0.0385 - acc: 1.0000 - val_loss: 0.5137 - val_acc: 0.8500\n",
            "Epoch 296/300\n",
            "\n",
            "Epoch 00296: val_loss did not improve from 0.36081\n",
            "60/60 - 0s - loss: 0.0710 - acc: 0.9667 - val_loss: 0.4989 - val_acc: 0.8500\n",
            "Epoch 297/300\n",
            "\n",
            "Epoch 00297: val_loss did not improve from 0.36081\n",
            "60/60 - 0s - loss: 0.0479 - acc: 1.0000 - val_loss: 0.4902 - val_acc: 0.8500\n",
            "Epoch 298/300\n",
            "\n",
            "Epoch 00298: val_loss did not improve from 0.36081\n",
            "60/60 - 0s - loss: 0.0301 - acc: 1.0000 - val_loss: 0.5012 - val_acc: 0.8500\n",
            "Epoch 299/300\n",
            "\n",
            "Epoch 00299: val_loss did not improve from 0.36081\n",
            "60/60 - 0s - loss: 0.0478 - acc: 1.0000 - val_loss: 0.4986 - val_acc: 0.8500\n",
            "Epoch 300/300\n",
            "\n",
            "Epoch 00300: val_loss did not improve from 0.36081\n",
            "60/60 - 0s - loss: 0.0441 - acc: 0.9833 - val_loss: 0.4822 - val_acc: 0.8500\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nA7zre9sIV9HcAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "# Evaluate on test data\n",
            "40/40 [==============================] - 0s 2ms/sample - loss: 0.6602 - acc: 0.6750\n",
            "test loss, test acc: [0.6601789049862419, 0.675]\n",
            "EEG_Deep/Data2A/parsed_P05T.mat\n",
            "Filtering of Training Data Finished\n",
            "EEG_Deep/Data2A/parsed_P05E.mat\n",
            "Filtering of Testing Data Finished\n",
            "(1, 2)\n",
            "Finding labels in training data\n",
            "Finding labels in testing data\n",
            "[1 1 1 2 1 1 1 1 2 1 1 1 2 1 2 1 1 2 2 2]\n",
            "(60, 12, 1536)\n",
            "(60,)\n",
            "(20, 12, 1536)\n",
            "(20,)\n",
            "[[1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]]\n",
            "X_train shape: (60, 1, 12, 1536)\n",
            "60 train samples\n",
            "20 val samples\n",
            "(40, 12, 1536)\n",
            "(40, 1)\n",
            "X_train shape: (40, 1, 12, 1536)\n",
            "40 train samples\n",
            "Train on 60 samples, validate on 20 samples\n",
            "Epoch 1/300\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.68545, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 1s - loss: 0.7324 - acc: 0.5000 - val_loss: 0.6855 - val_acc: 0.7500\n",
            "Epoch 2/300\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.68545\n",
            "60/60 - 0s - loss: 0.6719 - acc: 0.5333 - val_loss: 0.6880 - val_acc: 0.6500\n",
            "Epoch 3/300\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.68545\n",
            "60/60 - 0s - loss: 0.6621 - acc: 0.6500 - val_loss: 0.6905 - val_acc: 0.4500\n",
            "Epoch 4/300\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.68545\n",
            "60/60 - 0s - loss: 0.6506 - acc: 0.6167 - val_loss: 0.6920 - val_acc: 0.4500\n",
            "Epoch 5/300\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.68545\n",
            "60/60 - 0s - loss: 0.6441 - acc: 0.6667 - val_loss: 0.6937 - val_acc: 0.4500\n",
            "Epoch 6/300\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.68545\n",
            "60/60 - 0s - loss: 0.6397 - acc: 0.6500 - val_loss: 0.6950 - val_acc: 0.4500\n",
            "Epoch 7/300\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.68545\n",
            "60/60 - 0s - loss: 0.6289 - acc: 0.6500 - val_loss: 0.6961 - val_acc: 0.4500\n",
            "Epoch 8/300\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.68545\n",
            "60/60 - 0s - loss: 0.6191 - acc: 0.6833 - val_loss: 0.6977 - val_acc: 0.4500\n",
            "Epoch 9/300\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.68545\n",
            "60/60 - 0s - loss: 0.6099 - acc: 0.6833 - val_loss: 0.6989 - val_acc: 0.4500\n",
            "Epoch 10/300\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.68545\n",
            "60/60 - 0s - loss: 0.5989 - acc: 0.7500 - val_loss: 0.7008 - val_acc: 0.4000\n",
            "Epoch 11/300\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.68545\n",
            "60/60 - 0s - loss: 0.6099 - acc: 0.6833 - val_loss: 0.7027 - val_acc: 0.4000\n",
            "Epoch 12/300\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.68545\n",
            "60/60 - 0s - loss: 0.6025 - acc: 0.7000 - val_loss: 0.7030 - val_acc: 0.4000\n",
            "Epoch 13/300\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.68545\n",
            "60/60 - 0s - loss: 0.5770 - acc: 0.8333 - val_loss: 0.7037 - val_acc: 0.4000\n",
            "Epoch 14/300\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.68545\n",
            "60/60 - 0s - loss: 0.5870 - acc: 0.7667 - val_loss: 0.7048 - val_acc: 0.4000\n",
            "Epoch 15/300\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.68545\n",
            "60/60 - 0s - loss: 0.5645 - acc: 0.8500 - val_loss: 0.7062 - val_acc: 0.4000\n",
            "Epoch 16/300\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.68545\n",
            "60/60 - 0s - loss: 0.5743 - acc: 0.8000 - val_loss: 0.7065 - val_acc: 0.4000\n",
            "Epoch 17/300\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.68545\n",
            "60/60 - 0s - loss: 0.5618 - acc: 0.8500 - val_loss: 0.7094 - val_acc: 0.4000\n",
            "Epoch 18/300\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.68545\n",
            "60/60 - 0s - loss: 0.5501 - acc: 0.8167 - val_loss: 0.7111 - val_acc: 0.4000\n",
            "Epoch 19/300\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.68545\n",
            "60/60 - 0s - loss: 0.5429 - acc: 0.8500 - val_loss: 0.7125 - val_acc: 0.4000\n",
            "Epoch 20/300\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.68545\n",
            "60/60 - 0s - loss: 0.5270 - acc: 0.8500 - val_loss: 0.7144 - val_acc: 0.4000\n",
            "Epoch 21/300\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.68545\n",
            "60/60 - 0s - loss: 0.5407 - acc: 0.8333 - val_loss: 0.7169 - val_acc: 0.4000\n",
            "Epoch 22/300\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.68545\n",
            "60/60 - 0s - loss: 0.5146 - acc: 0.9167 - val_loss: 0.7216 - val_acc: 0.4000\n",
            "Epoch 23/300\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.68545\n",
            "60/60 - 0s - loss: 0.5361 - acc: 0.8500 - val_loss: 0.7256 - val_acc: 0.4000\n",
            "Epoch 24/300\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.68545\n",
            "60/60 - 0s - loss: 0.5100 - acc: 0.8500 - val_loss: 0.7295 - val_acc: 0.4000\n",
            "Epoch 25/300\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.68545\n",
            "60/60 - 0s - loss: 0.5113 - acc: 0.8833 - val_loss: 0.7297 - val_acc: 0.4000\n",
            "Epoch 26/300\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.68545\n",
            "60/60 - 0s - loss: 0.4790 - acc: 0.9167 - val_loss: 0.7313 - val_acc: 0.4000\n",
            "Epoch 27/300\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.68545\n",
            "60/60 - 0s - loss: 0.4737 - acc: 0.9000 - val_loss: 0.7336 - val_acc: 0.4000\n",
            "Epoch 28/300\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.68545\n",
            "60/60 - 0s - loss: 0.4869 - acc: 0.8500 - val_loss: 0.7354 - val_acc: 0.4000\n",
            "Epoch 29/300\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.68545\n",
            "60/60 - 0s - loss: 0.5098 - acc: 0.8000 - val_loss: 0.7360 - val_acc: 0.4000\n",
            "Epoch 30/300\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.68545\n",
            "60/60 - 0s - loss: 0.5048 - acc: 0.8333 - val_loss: 0.7323 - val_acc: 0.4000\n",
            "Epoch 31/300\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.68545\n",
            "60/60 - 0s - loss: 0.4576 - acc: 0.8833 - val_loss: 0.7288 - val_acc: 0.4500\n",
            "Epoch 32/300\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.68545\n",
            "60/60 - 0s - loss: 0.4725 - acc: 0.9000 - val_loss: 0.7288 - val_acc: 0.4500\n",
            "Epoch 33/300\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.68545\n",
            "60/60 - 0s - loss: 0.4669 - acc: 0.9000 - val_loss: 0.7307 - val_acc: 0.4500\n",
            "Epoch 34/300\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.68545\n",
            "60/60 - 0s - loss: 0.4459 - acc: 0.9000 - val_loss: 0.7349 - val_acc: 0.4500\n",
            "Epoch 35/300\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.68545\n",
            "60/60 - 0s - loss: 0.4672 - acc: 0.8500 - val_loss: 0.7366 - val_acc: 0.4500\n",
            "Epoch 36/300\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.68545\n",
            "60/60 - 0s - loss: 0.4308 - acc: 0.9167 - val_loss: 0.7431 - val_acc: 0.4500\n",
            "Epoch 37/300\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.68545\n",
            "60/60 - 0s - loss: 0.4583 - acc: 0.8500 - val_loss: 0.7416 - val_acc: 0.4500\n",
            "Epoch 38/300\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.68545\n",
            "60/60 - 0s - loss: 0.4276 - acc: 0.9000 - val_loss: 0.7410 - val_acc: 0.4500\n",
            "Epoch 39/300\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.68545\n",
            "60/60 - 0s - loss: 0.4590 - acc: 0.8500 - val_loss: 0.7427 - val_acc: 0.4500\n",
            "Epoch 40/300\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.68545\n",
            "60/60 - 0s - loss: 0.4149 - acc: 0.9833 - val_loss: 0.7486 - val_acc: 0.4500\n",
            "Epoch 41/300\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.68545\n",
            "60/60 - 0s - loss: 0.4642 - acc: 0.7833 - val_loss: 0.7540 - val_acc: 0.4500\n",
            "Epoch 42/300\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.68545\n",
            "60/60 - 0s - loss: 0.4359 - acc: 0.8500 - val_loss: 0.7582 - val_acc: 0.4500\n",
            "Epoch 43/300\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.68545\n",
            "60/60 - 0s - loss: 0.4560 - acc: 0.8667 - val_loss: 0.7591 - val_acc: 0.4000\n",
            "Epoch 44/300\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.68545\n",
            "60/60 - 0s - loss: 0.4265 - acc: 0.9000 - val_loss: 0.7560 - val_acc: 0.4000\n",
            "Epoch 45/300\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.68545\n",
            "60/60 - 0s - loss: 0.4019 - acc: 0.9667 - val_loss: 0.7538 - val_acc: 0.4000\n",
            "Epoch 46/300\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.68545\n",
            "60/60 - 0s - loss: 0.4012 - acc: 0.9500 - val_loss: 0.7491 - val_acc: 0.4000\n",
            "Epoch 47/300\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.68545\n",
            "60/60 - 0s - loss: 0.3960 - acc: 0.9833 - val_loss: 0.7483 - val_acc: 0.4000\n",
            "Epoch 48/300\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.68545\n",
            "60/60 - 0s - loss: 0.4118 - acc: 0.9333 - val_loss: 0.7509 - val_acc: 0.4000\n",
            "Epoch 49/300\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.68545\n",
            "60/60 - 0s - loss: 0.4396 - acc: 0.8833 - val_loss: 0.7527 - val_acc: 0.4500\n",
            "Epoch 50/300\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.68545\n",
            "60/60 - 0s - loss: 0.3908 - acc: 0.9167 - val_loss: 0.7482 - val_acc: 0.4500\n",
            "Epoch 51/300\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.68545\n",
            "60/60 - 0s - loss: 0.4374 - acc: 0.8667 - val_loss: 0.7366 - val_acc: 0.4500\n",
            "Epoch 52/300\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.68545\n",
            "60/60 - 0s - loss: 0.3491 - acc: 0.9833 - val_loss: 0.7316 - val_acc: 0.4500\n",
            "Epoch 53/300\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.68545\n",
            "60/60 - 0s - loss: 0.3823 - acc: 0.9500 - val_loss: 0.7242 - val_acc: 0.4500\n",
            "Epoch 54/300\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.68545\n",
            "60/60 - 0s - loss: 0.3473 - acc: 0.9833 - val_loss: 0.7234 - val_acc: 0.4500\n",
            "Epoch 55/300\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.68545\n",
            "60/60 - 0s - loss: 0.3573 - acc: 0.9333 - val_loss: 0.7204 - val_acc: 0.4500\n",
            "Epoch 56/300\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.68545\n",
            "60/60 - 0s - loss: 0.3797 - acc: 0.9500 - val_loss: 0.7191 - val_acc: 0.4500\n",
            "Epoch 57/300\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.68545\n",
            "60/60 - 0s - loss: 0.3783 - acc: 0.9000 - val_loss: 0.7098 - val_acc: 0.4500\n",
            "Epoch 58/300\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.68545\n",
            "60/60 - 0s - loss: 0.3708 - acc: 0.9500 - val_loss: 0.7060 - val_acc: 0.4000\n",
            "Epoch 59/300\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.68545\n",
            "60/60 - 0s - loss: 0.3608 - acc: 0.9000 - val_loss: 0.6955 - val_acc: 0.4500\n",
            "Epoch 60/300\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.68545\n",
            "60/60 - 0s - loss: 0.3438 - acc: 0.9500 - val_loss: 0.7044 - val_acc: 0.4500\n",
            "Epoch 61/300\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.68545\n",
            "60/60 - 0s - loss: 0.3729 - acc: 0.9000 - val_loss: 0.7211 - val_acc: 0.4000\n",
            "Epoch 62/300\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.68545\n",
            "60/60 - 0s - loss: 0.3405 - acc: 0.9333 - val_loss: 0.7308 - val_acc: 0.4000\n",
            "Epoch 63/300\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.68545\n",
            "60/60 - 0s - loss: 0.3657 - acc: 0.9333 - val_loss: 0.7406 - val_acc: 0.4000\n",
            "Epoch 64/300\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.68545\n",
            "60/60 - 0s - loss: 0.3555 - acc: 0.9167 - val_loss: 0.7414 - val_acc: 0.3500\n",
            "Epoch 65/300\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.68545\n",
            "60/60 - 0s - loss: 0.3191 - acc: 0.9167 - val_loss: 0.7246 - val_acc: 0.4500\n",
            "Epoch 66/300\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.68545\n",
            "60/60 - 0s - loss: 0.3249 - acc: 0.9667 - val_loss: 0.6985 - val_acc: 0.5500\n",
            "Epoch 67/300\n",
            "\n",
            "Epoch 00067: val_loss improved from 0.68545 to 0.67158, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3375 - acc: 0.9500 - val_loss: 0.6716 - val_acc: 0.6000\n",
            "Epoch 68/300\n",
            "\n",
            "Epoch 00068: val_loss improved from 0.67158 to 0.64772, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2978 - acc: 0.9833 - val_loss: 0.6477 - val_acc: 0.7000\n",
            "Epoch 69/300\n",
            "\n",
            "Epoch 00069: val_loss improved from 0.64772 to 0.63469, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2973 - acc: 0.9167 - val_loss: 0.6347 - val_acc: 0.7000\n",
            "Epoch 70/300\n",
            "\n",
            "Epoch 00070: val_loss improved from 0.63469 to 0.62491, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3426 - acc: 0.9000 - val_loss: 0.6249 - val_acc: 0.7000\n",
            "Epoch 71/300\n",
            "\n",
            "Epoch 00071: val_loss improved from 0.62491 to 0.61309, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3076 - acc: 0.9500 - val_loss: 0.6131 - val_acc: 0.7000\n",
            "Epoch 72/300\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.61309\n",
            "60/60 - 0s - loss: 0.3008 - acc: 0.9000 - val_loss: 0.6132 - val_acc: 0.6500\n",
            "Epoch 73/300\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.61309\n",
            "60/60 - 0s - loss: 0.3108 - acc: 0.9667 - val_loss: 0.6209 - val_acc: 0.6500\n",
            "Epoch 74/300\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.61309\n",
            "60/60 - 0s - loss: 0.2664 - acc: 0.9333 - val_loss: 0.6270 - val_acc: 0.6500\n",
            "Epoch 75/300\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.61309\n",
            "60/60 - 0s - loss: 0.2756 - acc: 0.9333 - val_loss: 0.6201 - val_acc: 0.6500\n",
            "Epoch 76/300\n",
            "\n",
            "Epoch 00076: val_loss improved from 0.61309 to 0.61056, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3070 - acc: 0.9000 - val_loss: 0.6106 - val_acc: 0.6500\n",
            "Epoch 77/300\n",
            "\n",
            "Epoch 00077: val_loss improved from 0.61056 to 0.59732, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2665 - acc: 0.9333 - val_loss: 0.5973 - val_acc: 0.6500\n",
            "Epoch 78/300\n",
            "\n",
            "Epoch 00078: val_loss improved from 0.59732 to 0.59549, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2746 - acc: 0.9333 - val_loss: 0.5955 - val_acc: 0.6500\n",
            "Epoch 79/300\n",
            "\n",
            "Epoch 00079: val_loss improved from 0.59549 to 0.59297, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2741 - acc: 0.9667 - val_loss: 0.5930 - val_acc: 0.6500\n",
            "Epoch 80/300\n",
            "\n",
            "Epoch 00080: val_loss improved from 0.59297 to 0.57165, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2527 - acc: 0.9333 - val_loss: 0.5716 - val_acc: 0.7000\n",
            "Epoch 81/300\n",
            "\n",
            "Epoch 00081: val_loss improved from 0.57165 to 0.55288, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2727 - acc: 0.9333 - val_loss: 0.5529 - val_acc: 0.7000\n",
            "Epoch 82/300\n",
            "\n",
            "Epoch 00082: val_loss improved from 0.55288 to 0.53176, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2524 - acc: 0.9500 - val_loss: 0.5318 - val_acc: 0.7000\n",
            "Epoch 83/300\n",
            "\n",
            "Epoch 00083: val_loss improved from 0.53176 to 0.51739, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2385 - acc: 0.9167 - val_loss: 0.5174 - val_acc: 0.7000\n",
            "Epoch 84/300\n",
            "\n",
            "Epoch 00084: val_loss improved from 0.51739 to 0.50760, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2799 - acc: 0.9500 - val_loss: 0.5076 - val_acc: 0.7000\n",
            "Epoch 85/300\n",
            "\n",
            "Epoch 00085: val_loss improved from 0.50760 to 0.50424, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2786 - acc: 0.9167 - val_loss: 0.5042 - val_acc: 0.7500\n",
            "Epoch 86/300\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.50424\n",
            "60/60 - 0s - loss: 0.2512 - acc: 0.9500 - val_loss: 0.5046 - val_acc: 0.7500\n",
            "Epoch 87/300\n",
            "\n",
            "Epoch 00087: val_loss improved from 0.50424 to 0.49904, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2255 - acc: 0.9667 - val_loss: 0.4990 - val_acc: 0.7500\n",
            "Epoch 88/300\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.49904\n",
            "60/60 - 0s - loss: 0.2286 - acc: 0.9667 - val_loss: 0.5004 - val_acc: 0.7000\n",
            "Epoch 89/300\n",
            "\n",
            "Epoch 00089: val_loss improved from 0.49904 to 0.49461, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2240 - acc: 0.9667 - val_loss: 0.4946 - val_acc: 0.7500\n",
            "Epoch 90/300\n",
            "\n",
            "Epoch 00090: val_loss improved from 0.49461 to 0.48389, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2412 - acc: 0.9667 - val_loss: 0.4839 - val_acc: 0.7500\n",
            "Epoch 91/300\n",
            "\n",
            "Epoch 00091: val_loss improved from 0.48389 to 0.47683, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2489 - acc: 0.9167 - val_loss: 0.4768 - val_acc: 0.7500\n",
            "Epoch 92/300\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.47683\n",
            "60/60 - 0s - loss: 0.2373 - acc: 0.9167 - val_loss: 0.4778 - val_acc: 0.7500\n",
            "Epoch 93/300\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.47683\n",
            "60/60 - 0s - loss: 0.2031 - acc: 0.9667 - val_loss: 0.4830 - val_acc: 0.7500\n",
            "Epoch 94/300\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.47683\n",
            "60/60 - 0s - loss: 0.2100 - acc: 0.9667 - val_loss: 0.4889 - val_acc: 0.7500\n",
            "Epoch 95/300\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.47683\n",
            "60/60 - 0s - loss: 0.1994 - acc: 0.9500 - val_loss: 0.4871 - val_acc: 0.7500\n",
            "Epoch 96/300\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.47683\n",
            "60/60 - 0s - loss: 0.2137 - acc: 0.9667 - val_loss: 0.4865 - val_acc: 0.7500\n",
            "Epoch 97/300\n",
            "\n",
            "Epoch 00097: val_loss improved from 0.47683 to 0.47171, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2100 - acc: 0.9333 - val_loss: 0.4717 - val_acc: 0.7500\n",
            "Epoch 98/300\n",
            "\n",
            "Epoch 00098: val_loss improved from 0.47171 to 0.46060, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2437 - acc: 0.9333 - val_loss: 0.4606 - val_acc: 0.7500\n",
            "Epoch 99/300\n",
            "\n",
            "Epoch 00099: val_loss improved from 0.46060 to 0.45068, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2086 - acc: 0.9500 - val_loss: 0.4507 - val_acc: 0.7500\n",
            "Epoch 100/300\n",
            "\n",
            "Epoch 00100: val_loss improved from 0.45068 to 0.42529, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2010 - acc: 0.9500 - val_loss: 0.4253 - val_acc: 0.8000\n",
            "Epoch 101/300\n",
            "\n",
            "Epoch 00101: val_loss improved from 0.42529 to 0.41194, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2255 - acc: 0.9333 - val_loss: 0.4119 - val_acc: 0.9000\n",
            "Epoch 102/300\n",
            "\n",
            "Epoch 00102: val_loss improved from 0.41194 to 0.40532, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1794 - acc: 0.9500 - val_loss: 0.4053 - val_acc: 0.9000\n",
            "Epoch 103/300\n",
            "\n",
            "Epoch 00103: val_loss improved from 0.40532 to 0.39916, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1802 - acc: 0.9500 - val_loss: 0.3992 - val_acc: 0.9000\n",
            "Epoch 104/300\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.39916\n",
            "60/60 - 0s - loss: 0.1645 - acc: 0.9500 - val_loss: 0.3999 - val_acc: 0.9000\n",
            "Epoch 105/300\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 0.39916\n",
            "60/60 - 0s - loss: 0.2062 - acc: 0.9500 - val_loss: 0.4115 - val_acc: 0.9000\n",
            "Epoch 106/300\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 0.39916\n",
            "60/60 - 0s - loss: 0.2117 - acc: 0.9167 - val_loss: 0.4237 - val_acc: 0.8500\n",
            "Epoch 107/300\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.39916\n",
            "60/60 - 0s - loss: 0.2250 - acc: 0.9167 - val_loss: 0.4236 - val_acc: 0.8500\n",
            "Epoch 108/300\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 0.39916\n",
            "60/60 - 0s - loss: 0.1795 - acc: 0.9333 - val_loss: 0.4210 - val_acc: 0.8500\n",
            "Epoch 109/300\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.39916\n",
            "60/60 - 0s - loss: 0.1816 - acc: 0.9667 - val_loss: 0.4320 - val_acc: 0.8000\n",
            "Epoch 110/300\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.39916\n",
            "60/60 - 0s - loss: 0.1772 - acc: 0.9667 - val_loss: 0.4268 - val_acc: 0.8000\n",
            "Epoch 111/300\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 0.39916\n",
            "60/60 - 0s - loss: 0.1459 - acc: 0.9667 - val_loss: 0.4102 - val_acc: 0.8500\n",
            "Epoch 112/300\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 0.39916\n",
            "60/60 - 0s - loss: 0.1851 - acc: 0.9333 - val_loss: 0.4007 - val_acc: 0.9000\n",
            "Epoch 113/300\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.39916\n",
            "60/60 - 0s - loss: 0.2475 - acc: 0.9333 - val_loss: 0.4072 - val_acc: 0.9000\n",
            "Epoch 114/300\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.39916\n",
            "60/60 - 0s - loss: 0.1714 - acc: 0.9667 - val_loss: 0.4095 - val_acc: 0.8500\n",
            "Epoch 115/300\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.39916\n",
            "60/60 - 0s - loss: 0.1896 - acc: 0.9167 - val_loss: 0.4134 - val_acc: 0.8500\n",
            "Epoch 116/300\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.39916\n",
            "60/60 - 0s - loss: 0.1697 - acc: 0.9667 - val_loss: 0.4168 - val_acc: 0.8500\n",
            "Epoch 117/300\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.39916\n",
            "60/60 - 0s - loss: 0.2365 - acc: 0.9167 - val_loss: 0.4090 - val_acc: 0.8500\n",
            "Epoch 118/300\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.39916\n",
            "60/60 - 0s - loss: 0.1789 - acc: 0.9500 - val_loss: 0.4109 - val_acc: 0.8500\n",
            "Epoch 119/300\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 0.39916\n",
            "60/60 - 0s - loss: 0.1650 - acc: 0.9667 - val_loss: 0.4030 - val_acc: 0.8500\n",
            "Epoch 120/300\n",
            "\n",
            "Epoch 00120: val_loss improved from 0.39916 to 0.39610, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1653 - acc: 0.9667 - val_loss: 0.3961 - val_acc: 0.8500\n",
            "Epoch 121/300\n",
            "\n",
            "Epoch 00121: val_loss improved from 0.39610 to 0.39067, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1642 - acc: 1.0000 - val_loss: 0.3907 - val_acc: 0.8500\n",
            "Epoch 122/300\n",
            "\n",
            "Epoch 00122: val_loss improved from 0.39067 to 0.37948, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1795 - acc: 0.9500 - val_loss: 0.3795 - val_acc: 0.8500\n",
            "Epoch 123/300\n",
            "\n",
            "Epoch 00123: val_loss improved from 0.37948 to 0.37846, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1910 - acc: 0.9333 - val_loss: 0.3785 - val_acc: 0.8500\n",
            "Epoch 124/300\n",
            "\n",
            "Epoch 00124: val_loss improved from 0.37846 to 0.37622, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1506 - acc: 1.0000 - val_loss: 0.3762 - val_acc: 0.8500\n",
            "Epoch 125/300\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 0.37622\n",
            "60/60 - 0s - loss: 0.1533 - acc: 0.9833 - val_loss: 0.3794 - val_acc: 0.8500\n",
            "Epoch 126/300\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 0.37622\n",
            "60/60 - 0s - loss: 0.1800 - acc: 0.9500 - val_loss: 0.3949 - val_acc: 0.8500\n",
            "Epoch 127/300\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 0.37622\n",
            "60/60 - 0s - loss: 0.1704 - acc: 0.9333 - val_loss: 0.4078 - val_acc: 0.8000\n",
            "Epoch 128/300\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 0.37622\n",
            "60/60 - 0s - loss: 0.1534 - acc: 0.9667 - val_loss: 0.3939 - val_acc: 0.8500\n",
            "Epoch 129/300\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.37622\n",
            "60/60 - 0s - loss: 0.1470 - acc: 0.9833 - val_loss: 0.3870 - val_acc: 0.8500\n",
            "Epoch 130/300\n",
            "\n",
            "Epoch 00130: val_loss improved from 0.37622 to 0.35888, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1647 - acc: 0.9667 - val_loss: 0.3589 - val_acc: 0.8500\n",
            "Epoch 131/300\n",
            "\n",
            "Epoch 00131: val_loss improved from 0.35888 to 0.35190, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1656 - acc: 0.9500 - val_loss: 0.3519 - val_acc: 0.9000\n",
            "Epoch 132/300\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 0.35190\n",
            "60/60 - 0s - loss: 0.1182 - acc: 0.9833 - val_loss: 0.3542 - val_acc: 0.9000\n",
            "Epoch 133/300\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.35190\n",
            "60/60 - 0s - loss: 0.1347 - acc: 0.9667 - val_loss: 0.3545 - val_acc: 0.8500\n",
            "Epoch 134/300\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.35190\n",
            "60/60 - 0s - loss: 0.1693 - acc: 0.9500 - val_loss: 0.3570 - val_acc: 0.8500\n",
            "Epoch 135/300\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 0.35190\n",
            "60/60 - 0s - loss: 0.1344 - acc: 0.9667 - val_loss: 0.3619 - val_acc: 0.8500\n",
            "Epoch 136/300\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.35190\n",
            "60/60 - 0s - loss: 0.1353 - acc: 1.0000 - val_loss: 0.3754 - val_acc: 0.8500\n",
            "Epoch 137/300\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 0.35190\n",
            "60/60 - 0s - loss: 0.1327 - acc: 0.9833 - val_loss: 0.3758 - val_acc: 0.8500\n",
            "Epoch 138/300\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.35190\n",
            "60/60 - 0s - loss: 0.1329 - acc: 1.0000 - val_loss: 0.3700 - val_acc: 0.8500\n",
            "Epoch 139/300\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.35190\n",
            "60/60 - 0s - loss: 0.1457 - acc: 0.9667 - val_loss: 0.3591 - val_acc: 0.8500\n",
            "Epoch 140/300\n",
            "\n",
            "Epoch 00140: val_loss improved from 0.35190 to 0.34442, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1650 - acc: 0.9500 - val_loss: 0.3444 - val_acc: 0.9000\n",
            "Epoch 141/300\n",
            "\n",
            "Epoch 00141: val_loss improved from 0.34442 to 0.33202, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1231 - acc: 0.9667 - val_loss: 0.3320 - val_acc: 0.9000\n",
            "Epoch 142/300\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.33202\n",
            "60/60 - 0s - loss: 0.1378 - acc: 0.9667 - val_loss: 0.3355 - val_acc: 0.9000\n",
            "Epoch 143/300\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 0.33202\n",
            "60/60 - 0s - loss: 0.1564 - acc: 0.9833 - val_loss: 0.3674 - val_acc: 0.8500\n",
            "Epoch 144/300\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 0.33202\n",
            "60/60 - 0s - loss: 0.1585 - acc: 0.9500 - val_loss: 0.4143 - val_acc: 0.8500\n",
            "Epoch 145/300\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.33202\n",
            "60/60 - 0s - loss: 0.1732 - acc: 0.9500 - val_loss: 0.3952 - val_acc: 0.8500\n",
            "Epoch 146/300\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.33202\n",
            "60/60 - 0s - loss: 0.1161 - acc: 1.0000 - val_loss: 0.3847 - val_acc: 0.9000\n",
            "Epoch 147/300\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.33202\n",
            "60/60 - 0s - loss: 0.1741 - acc: 0.9167 - val_loss: 0.4116 - val_acc: 0.8500\n",
            "Epoch 148/300\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 0.33202\n",
            "60/60 - 0s - loss: 0.1391 - acc: 0.9833 - val_loss: 0.4028 - val_acc: 0.8500\n",
            "Epoch 149/300\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.33202\n",
            "60/60 - 0s - loss: 0.1475 - acc: 0.9667 - val_loss: 0.3904 - val_acc: 0.8500\n",
            "Epoch 150/300\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.33202\n",
            "60/60 - 0s - loss: 0.1605 - acc: 0.9667 - val_loss: 0.3759 - val_acc: 0.9000\n",
            "Epoch 151/300\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.33202\n",
            "60/60 - 0s - loss: 0.1103 - acc: 1.0000 - val_loss: 0.3708 - val_acc: 0.9000\n",
            "Epoch 152/300\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.33202\n",
            "60/60 - 0s - loss: 0.1034 - acc: 0.9833 - val_loss: 0.3785 - val_acc: 0.9000\n",
            "Epoch 153/300\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 0.33202\n",
            "60/60 - 0s - loss: 0.1604 - acc: 0.9333 - val_loss: 0.3793 - val_acc: 0.9000\n",
            "Epoch 154/300\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.33202\n",
            "60/60 - 0s - loss: 0.1067 - acc: 0.9833 - val_loss: 0.3748 - val_acc: 0.9000\n",
            "Epoch 155/300\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.33202\n",
            "60/60 - 0s - loss: 0.1290 - acc: 0.9833 - val_loss: 0.3756 - val_acc: 0.9000\n",
            "Epoch 156/300\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.33202\n",
            "60/60 - 0s - loss: 0.1521 - acc: 0.9667 - val_loss: 0.3815 - val_acc: 0.8500\n",
            "Epoch 157/300\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.33202\n",
            "60/60 - 0s - loss: 0.1275 - acc: 0.9833 - val_loss: 0.3868 - val_acc: 0.8500\n",
            "Epoch 158/300\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.33202\n",
            "60/60 - 0s - loss: 0.1247 - acc: 0.9667 - val_loss: 0.4058 - val_acc: 0.8500\n",
            "Epoch 159/300\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.33202\n",
            "60/60 - 0s - loss: 0.1208 - acc: 0.9667 - val_loss: 0.3932 - val_acc: 0.8500\n",
            "Epoch 160/300\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.33202\n",
            "60/60 - 0s - loss: 0.1204 - acc: 0.9833 - val_loss: 0.3656 - val_acc: 0.8500\n",
            "Epoch 161/300\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.33202\n",
            "60/60 - 0s - loss: 0.1534 - acc: 0.9667 - val_loss: 0.3514 - val_acc: 0.8500\n",
            "Epoch 162/300\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.33202\n",
            "60/60 - 0s - loss: 0.1382 - acc: 0.9667 - val_loss: 0.3926 - val_acc: 0.8500\n",
            "Epoch 163/300\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.33202\n",
            "60/60 - 0s - loss: 0.1484 - acc: 0.9667 - val_loss: 0.3962 - val_acc: 0.8500\n",
            "Epoch 164/300\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.33202\n",
            "60/60 - 0s - loss: 0.1446 - acc: 0.9667 - val_loss: 0.4094 - val_acc: 0.8500\n",
            "Epoch 165/300\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.33202\n",
            "60/60 - 0s - loss: 0.1232 - acc: 0.9667 - val_loss: 0.4114 - val_acc: 0.8500\n",
            "Epoch 166/300\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 0.33202\n",
            "60/60 - 0s - loss: 0.1283 - acc: 0.9833 - val_loss: 0.4029 - val_acc: 0.8500\n",
            "Epoch 167/300\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.33202\n",
            "60/60 - 0s - loss: 0.1681 - acc: 0.9333 - val_loss: 0.3472 - val_acc: 0.8500\n",
            "Epoch 168/300\n",
            "\n",
            "Epoch 00168: val_loss improved from 0.33202 to 0.33016, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1031 - acc: 0.9833 - val_loss: 0.3302 - val_acc: 0.9000\n",
            "Epoch 169/300\n",
            "\n",
            "Epoch 00169: val_loss improved from 0.33016 to 0.32152, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1547 - acc: 0.9500 - val_loss: 0.3215 - val_acc: 0.9000\n",
            "Epoch 170/300\n",
            "\n",
            "Epoch 00170: val_loss improved from 0.32152 to 0.31827, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.0888 - acc: 1.0000 - val_loss: 0.3183 - val_acc: 0.9000\n",
            "Epoch 171/300\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.31827\n",
            "60/60 - 0s - loss: 0.1135 - acc: 0.9833 - val_loss: 0.3249 - val_acc: 0.9000\n",
            "Epoch 172/300\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.31827\n",
            "60/60 - 0s - loss: 0.1133 - acc: 0.9667 - val_loss: 0.3510 - val_acc: 0.9000\n",
            "Epoch 173/300\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.31827\n",
            "60/60 - 0s - loss: 0.1042 - acc: 0.9833 - val_loss: 0.3779 - val_acc: 0.8500\n",
            "Epoch 174/300\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.31827\n",
            "60/60 - 0s - loss: 0.1464 - acc: 0.9500 - val_loss: 0.4163 - val_acc: 0.8000\n",
            "Epoch 175/300\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.31827\n",
            "60/60 - 0s - loss: 0.1180 - acc: 0.9667 - val_loss: 0.4163 - val_acc: 0.8000\n",
            "Epoch 176/300\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 0.31827\n",
            "60/60 - 0s - loss: 0.1302 - acc: 0.9667 - val_loss: 0.4009 - val_acc: 0.8000\n",
            "Epoch 177/300\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.31827\n",
            "60/60 - 0s - loss: 0.1568 - acc: 0.9333 - val_loss: 0.4003 - val_acc: 0.8000\n",
            "Epoch 178/300\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.31827\n",
            "60/60 - 0s - loss: 0.1222 - acc: 0.9833 - val_loss: 0.4145 - val_acc: 0.8000\n",
            "Epoch 179/300\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.31827\n",
            "60/60 - 0s - loss: 0.1106 - acc: 0.9833 - val_loss: 0.3982 - val_acc: 0.8000\n",
            "Epoch 180/300\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.31827\n",
            "60/60 - 0s - loss: 0.1420 - acc: 0.9667 - val_loss: 0.3838 - val_acc: 0.8000\n",
            "Epoch 181/300\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.31827\n",
            "60/60 - 0s - loss: 0.1812 - acc: 0.9333 - val_loss: 0.3915 - val_acc: 0.8000\n",
            "Epoch 182/300\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.31827\n",
            "60/60 - 0s - loss: 0.1105 - acc: 1.0000 - val_loss: 0.3968 - val_acc: 0.9000\n",
            "Epoch 183/300\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.31827\n",
            "60/60 - 0s - loss: 0.2011 - acc: 0.9500 - val_loss: 0.3882 - val_acc: 0.9000\n",
            "Epoch 184/300\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 0.31827\n",
            "60/60 - 0s - loss: 0.1033 - acc: 0.9833 - val_loss: 0.3859 - val_acc: 0.9000\n",
            "Epoch 185/300\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.31827\n",
            "60/60 - 0s - loss: 0.0940 - acc: 0.9833 - val_loss: 0.3939 - val_acc: 0.9000\n",
            "Epoch 186/300\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.31827\n",
            "60/60 - 0s - loss: 0.1719 - acc: 0.9500 - val_loss: 0.4439 - val_acc: 0.8500\n",
            "Epoch 187/300\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 0.31827\n",
            "60/60 - 0s - loss: 0.0983 - acc: 1.0000 - val_loss: 0.5025 - val_acc: 0.8500\n",
            "Epoch 188/300\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.31827\n",
            "60/60 - 0s - loss: 0.1419 - acc: 0.9667 - val_loss: 0.4786 - val_acc: 0.8500\n",
            "Epoch 189/300\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.31827\n",
            "60/60 - 0s - loss: 0.1207 - acc: 0.9833 - val_loss: 0.3763 - val_acc: 0.9000\n",
            "Epoch 190/300\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.31827\n",
            "60/60 - 0s - loss: 0.1225 - acc: 0.9667 - val_loss: 0.3268 - val_acc: 0.9500\n",
            "Epoch 191/300\n",
            "\n",
            "Epoch 00191: val_loss improved from 0.31827 to 0.31266, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1389 - acc: 0.9333 - val_loss: 0.3127 - val_acc: 0.9000\n",
            "Epoch 192/300\n",
            "\n",
            "Epoch 00192: val_loss improved from 0.31266 to 0.30495, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1053 - acc: 0.9667 - val_loss: 0.3050 - val_acc: 0.9000\n",
            "Epoch 193/300\n",
            "\n",
            "Epoch 00193: val_loss improved from 0.30495 to 0.29771, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1033 - acc: 0.9667 - val_loss: 0.2977 - val_acc: 0.9000\n",
            "Epoch 194/300\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.29771\n",
            "60/60 - 0s - loss: 0.1173 - acc: 0.9500 - val_loss: 0.3098 - val_acc: 0.9500\n",
            "Epoch 195/300\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.29771\n",
            "60/60 - 0s - loss: 0.1040 - acc: 0.9833 - val_loss: 0.3262 - val_acc: 0.9000\n",
            "Epoch 196/300\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.29771\n",
            "60/60 - 0s - loss: 0.1160 - acc: 1.0000 - val_loss: 0.3394 - val_acc: 0.9000\n",
            "Epoch 197/300\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.29771\n",
            "60/60 - 0s - loss: 0.1117 - acc: 0.9833 - val_loss: 0.3392 - val_acc: 0.9000\n",
            "Epoch 198/300\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.29771\n",
            "60/60 - 0s - loss: 0.1299 - acc: 0.9500 - val_loss: 0.3464 - val_acc: 0.9000\n",
            "Epoch 199/300\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.29771\n",
            "60/60 - 0s - loss: 0.1268 - acc: 0.9333 - val_loss: 0.3205 - val_acc: 0.9000\n",
            "Epoch 200/300\n",
            "\n",
            "Epoch 00200: val_loss improved from 0.29771 to 0.29238, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.0905 - acc: 0.9833 - val_loss: 0.2924 - val_acc: 0.9500\n",
            "Epoch 201/300\n",
            "\n",
            "Epoch 00201: val_loss improved from 0.29238 to 0.28498, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1705 - acc: 0.9333 - val_loss: 0.2850 - val_acc: 0.9500\n",
            "Epoch 202/300\n",
            "\n",
            "Epoch 00202: val_loss did not improve from 0.28498\n",
            "60/60 - 0s - loss: 0.1111 - acc: 0.9667 - val_loss: 0.2891 - val_acc: 0.9500\n",
            "Epoch 203/300\n",
            "\n",
            "Epoch 00203: val_loss did not improve from 0.28498\n",
            "60/60 - 0s - loss: 0.1083 - acc: 0.9833 - val_loss: 0.2994 - val_acc: 0.9000\n",
            "Epoch 204/300\n",
            "\n",
            "Epoch 00204: val_loss did not improve from 0.28498\n",
            "60/60 - 0s - loss: 0.0939 - acc: 1.0000 - val_loss: 0.3116 - val_acc: 0.9000\n",
            "Epoch 205/300\n",
            "\n",
            "Epoch 00205: val_loss did not improve from 0.28498\n",
            "60/60 - 0s - loss: 0.0818 - acc: 1.0000 - val_loss: 0.3287 - val_acc: 0.9000\n",
            "Epoch 206/300\n",
            "\n",
            "Epoch 00206: val_loss did not improve from 0.28498\n",
            "60/60 - 0s - loss: 0.1866 - acc: 0.9500 - val_loss: 0.3227 - val_acc: 0.9000\n",
            "Epoch 207/300\n",
            "\n",
            "Epoch 00207: val_loss did not improve from 0.28498\n",
            "60/60 - 0s - loss: 0.1064 - acc: 0.9667 - val_loss: 0.3037 - val_acc: 0.9000\n",
            "Epoch 208/300\n",
            "\n",
            "Epoch 00208: val_loss did not improve from 0.28498\n",
            "60/60 - 0s - loss: 0.1631 - acc: 0.9500 - val_loss: 0.3118 - val_acc: 0.9000\n",
            "Epoch 209/300\n",
            "\n",
            "Epoch 00209: val_loss did not improve from 0.28498\n",
            "60/60 - 0s - loss: 0.1004 - acc: 0.9833 - val_loss: 0.3219 - val_acc: 0.9000\n",
            "Epoch 210/300\n",
            "\n",
            "Epoch 00210: val_loss did not improve from 0.28498\n",
            "60/60 - 0s - loss: 0.0894 - acc: 1.0000 - val_loss: 0.3330 - val_acc: 0.9000\n",
            "Epoch 211/300\n",
            "\n",
            "Epoch 00211: val_loss did not improve from 0.28498\n",
            "60/60 - 0s - loss: 0.1227 - acc: 0.9667 - val_loss: 0.3337 - val_acc: 0.8500\n",
            "Epoch 212/300\n",
            "\n",
            "Epoch 00212: val_loss did not improve from 0.28498\n",
            "60/60 - 0s - loss: 0.0830 - acc: 0.9833 - val_loss: 0.3414 - val_acc: 0.9000\n",
            "Epoch 213/300\n",
            "\n",
            "Epoch 00213: val_loss did not improve from 0.28498\n",
            "60/60 - 0s - loss: 0.1226 - acc: 0.9833 - val_loss: 0.3487 - val_acc: 0.9000\n",
            "Epoch 214/300\n",
            "\n",
            "Epoch 00214: val_loss did not improve from 0.28498\n",
            "60/60 - 0s - loss: 0.0663 - acc: 1.0000 - val_loss: 0.3621 - val_acc: 0.9000\n",
            "Epoch 215/300\n",
            "\n",
            "Epoch 00215: val_loss did not improve from 0.28498\n",
            "60/60 - 0s - loss: 0.0883 - acc: 1.0000 - val_loss: 0.3615 - val_acc: 0.9000\n",
            "Epoch 216/300\n",
            "\n",
            "Epoch 00216: val_loss did not improve from 0.28498\n",
            "60/60 - 0s - loss: 0.1184 - acc: 0.9667 - val_loss: 0.3404 - val_acc: 0.9000\n",
            "Epoch 217/300\n",
            "\n",
            "Epoch 00217: val_loss did not improve from 0.28498\n",
            "60/60 - 0s - loss: 0.0763 - acc: 1.0000 - val_loss: 0.3050 - val_acc: 0.9000\n",
            "Epoch 218/300\n",
            "\n",
            "Epoch 00218: val_loss did not improve from 0.28498\n",
            "60/60 - 0s - loss: 0.0926 - acc: 0.9833 - val_loss: 0.2974 - val_acc: 0.9500\n",
            "Epoch 219/300\n",
            "\n",
            "Epoch 00219: val_loss did not improve from 0.28498\n",
            "60/60 - 0s - loss: 0.0960 - acc: 0.9833 - val_loss: 0.3000 - val_acc: 0.9500\n",
            "Epoch 220/300\n",
            "\n",
            "Epoch 00220: val_loss did not improve from 0.28498\n",
            "60/60 - 0s - loss: 0.0790 - acc: 0.9833 - val_loss: 0.2902 - val_acc: 0.9500\n",
            "Epoch 221/300\n",
            "\n",
            "Epoch 00221: val_loss improved from 0.28498 to 0.27664, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1272 - acc: 0.9500 - val_loss: 0.2766 - val_acc: 0.9500\n",
            "Epoch 222/300\n",
            "\n",
            "Epoch 00222: val_loss did not improve from 0.27664\n",
            "60/60 - 0s - loss: 0.1024 - acc: 0.9833 - val_loss: 0.2804 - val_acc: 0.9500\n",
            "Epoch 223/300\n",
            "\n",
            "Epoch 00223: val_loss did not improve from 0.27664\n",
            "60/60 - 0s - loss: 0.0905 - acc: 1.0000 - val_loss: 0.2971 - val_acc: 0.9500\n",
            "Epoch 224/300\n",
            "\n",
            "Epoch 00224: val_loss did not improve from 0.27664\n",
            "60/60 - 0s - loss: 0.0934 - acc: 0.9833 - val_loss: 0.3189 - val_acc: 0.9000\n",
            "Epoch 225/300\n",
            "\n",
            "Epoch 00225: val_loss did not improve from 0.27664\n",
            "60/60 - 0s - loss: 0.0717 - acc: 1.0000 - val_loss: 0.3579 - val_acc: 0.9000\n",
            "Epoch 226/300\n",
            "\n",
            "Epoch 00226: val_loss did not improve from 0.27664\n",
            "60/60 - 0s - loss: 0.0862 - acc: 1.0000 - val_loss: 0.4008 - val_acc: 0.9000\n",
            "Epoch 227/300\n",
            "\n",
            "Epoch 00227: val_loss did not improve from 0.27664\n",
            "60/60 - 0s - loss: 0.1047 - acc: 0.9833 - val_loss: 0.4119 - val_acc: 0.9000\n",
            "Epoch 228/300\n",
            "\n",
            "Epoch 00228: val_loss did not improve from 0.27664\n",
            "60/60 - 0s - loss: 0.1148 - acc: 0.9500 - val_loss: 0.3761 - val_acc: 0.9000\n",
            "Epoch 229/300\n",
            "\n",
            "Epoch 00229: val_loss did not improve from 0.27664\n",
            "60/60 - 0s - loss: 0.1452 - acc: 0.9500 - val_loss: 0.3349 - val_acc: 0.9000\n",
            "Epoch 230/300\n",
            "\n",
            "Epoch 00230: val_loss did not improve from 0.27664\n",
            "60/60 - 0s - loss: 0.1006 - acc: 0.9667 - val_loss: 0.3358 - val_acc: 0.9000\n",
            "Epoch 231/300\n",
            "\n",
            "Epoch 00231: val_loss did not improve from 0.27664\n",
            "60/60 - 0s - loss: 0.0904 - acc: 1.0000 - val_loss: 0.3552 - val_acc: 0.9000\n",
            "Epoch 232/300\n",
            "\n",
            "Epoch 00232: val_loss did not improve from 0.27664\n",
            "60/60 - 0s - loss: 0.0644 - acc: 1.0000 - val_loss: 0.3465 - val_acc: 0.9000\n",
            "Epoch 233/300\n",
            "\n",
            "Epoch 00233: val_loss did not improve from 0.27664\n",
            "60/60 - 0s - loss: 0.0968 - acc: 0.9833 - val_loss: 0.3303 - val_acc: 0.9500\n",
            "Epoch 234/300\n",
            "\n",
            "Epoch 00234: val_loss did not improve from 0.27664\n",
            "60/60 - 0s - loss: 0.1036 - acc: 0.9833 - val_loss: 0.3142 - val_acc: 0.9500\n",
            "Epoch 235/300\n",
            "\n",
            "Epoch 00235: val_loss did not improve from 0.27664\n",
            "60/60 - 0s - loss: 0.0877 - acc: 1.0000 - val_loss: 0.3141 - val_acc: 0.9500\n",
            "Epoch 236/300\n",
            "\n",
            "Epoch 00236: val_loss did not improve from 0.27664\n",
            "60/60 - 0s - loss: 0.0842 - acc: 0.9833 - val_loss: 0.3216 - val_acc: 0.9000\n",
            "Epoch 237/300\n",
            "\n",
            "Epoch 00237: val_loss did not improve from 0.27664\n",
            "60/60 - 0s - loss: 0.0829 - acc: 1.0000 - val_loss: 0.3208 - val_acc: 0.9000\n",
            "Epoch 238/300\n",
            "\n",
            "Epoch 00238: val_loss did not improve from 0.27664\n",
            "60/60 - 0s - loss: 0.1232 - acc: 0.9667 - val_loss: 0.3478 - val_acc: 0.9000\n",
            "Epoch 239/300\n",
            "\n",
            "Epoch 00239: val_loss did not improve from 0.27664\n",
            "60/60 - 0s - loss: 0.0729 - acc: 0.9833 - val_loss: 0.3393 - val_acc: 0.8500\n",
            "Epoch 240/300\n",
            "\n",
            "Epoch 00240: val_loss did not improve from 0.27664\n",
            "60/60 - 0s - loss: 0.0813 - acc: 1.0000 - val_loss: 0.3133 - val_acc: 0.9000\n",
            "Epoch 241/300\n",
            "\n",
            "Epoch 00241: val_loss did not improve from 0.27664\n",
            "60/60 - 0s - loss: 0.1039 - acc: 1.0000 - val_loss: 0.2956 - val_acc: 0.9000\n",
            "Epoch 242/300\n",
            "\n",
            "Epoch 00242: val_loss did not improve from 0.27664\n",
            "60/60 - 0s - loss: 0.0852 - acc: 1.0000 - val_loss: 0.2930 - val_acc: 0.9500\n",
            "Epoch 243/300\n",
            "\n",
            "Epoch 00243: val_loss did not improve from 0.27664\n",
            "60/60 - 0s - loss: 0.0958 - acc: 1.0000 - val_loss: 0.2957 - val_acc: 0.9000\n",
            "Epoch 244/300\n",
            "\n",
            "Epoch 00244: val_loss did not improve from 0.27664\n",
            "60/60 - 0s - loss: 0.0931 - acc: 1.0000 - val_loss: 0.3144 - val_acc: 0.9000\n",
            "Epoch 245/300\n",
            "\n",
            "Epoch 00245: val_loss did not improve from 0.27664\n",
            "60/60 - 0s - loss: 0.0874 - acc: 1.0000 - val_loss: 0.3578 - val_acc: 0.8500\n",
            "Epoch 246/300\n",
            "\n",
            "Epoch 00246: val_loss did not improve from 0.27664\n",
            "60/60 - 0s - loss: 0.1039 - acc: 0.9667 - val_loss: 0.3624 - val_acc: 0.9000\n",
            "Epoch 247/300\n",
            "\n",
            "Epoch 00247: val_loss did not improve from 0.27664\n",
            "60/60 - 0s - loss: 0.1214 - acc: 0.9500 - val_loss: 0.3401 - val_acc: 0.9000\n",
            "Epoch 248/300\n",
            "\n",
            "Epoch 00248: val_loss did not improve from 0.27664\n",
            "60/60 - 0s - loss: 0.0986 - acc: 1.0000 - val_loss: 0.2955 - val_acc: 0.9500\n",
            "Epoch 249/300\n",
            "\n",
            "Epoch 00249: val_loss improved from 0.27664 to 0.27525, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1044 - acc: 0.9833 - val_loss: 0.2753 - val_acc: 0.9000\n",
            "Epoch 250/300\n",
            "\n",
            "Epoch 00250: val_loss improved from 0.27525 to 0.26828, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.0938 - acc: 1.0000 - val_loss: 0.2683 - val_acc: 0.9500\n",
            "Epoch 251/300\n",
            "\n",
            "Epoch 00251: val_loss improved from 0.26828 to 0.26802, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.0866 - acc: 0.9667 - val_loss: 0.2680 - val_acc: 0.9500\n",
            "Epoch 252/300\n",
            "\n",
            "Epoch 00252: val_loss did not improve from 0.26802\n",
            "60/60 - 0s - loss: 0.0546 - acc: 1.0000 - val_loss: 0.2691 - val_acc: 0.9500\n",
            "Epoch 253/300\n",
            "\n",
            "Epoch 00253: val_loss did not improve from 0.26802\n",
            "60/60 - 0s - loss: 0.0889 - acc: 0.9833 - val_loss: 0.2801 - val_acc: 0.9500\n",
            "Epoch 254/300\n",
            "\n",
            "Epoch 00254: val_loss did not improve from 0.26802\n",
            "60/60 - 0s - loss: 0.0930 - acc: 1.0000 - val_loss: 0.3403 - val_acc: 0.9000\n",
            "Epoch 255/300\n",
            "\n",
            "Epoch 00255: val_loss did not improve from 0.26802\n",
            "60/60 - 0s - loss: 0.0813 - acc: 1.0000 - val_loss: 0.3956 - val_acc: 0.9000\n",
            "Epoch 256/300\n",
            "\n",
            "Epoch 00256: val_loss did not improve from 0.26802\n",
            "60/60 - 0s - loss: 0.1011 - acc: 0.9833 - val_loss: 0.3878 - val_acc: 0.8500\n",
            "Epoch 257/300\n",
            "\n",
            "Epoch 00257: val_loss did not improve from 0.26802\n",
            "60/60 - 0s - loss: 0.0724 - acc: 1.0000 - val_loss: 0.3658 - val_acc: 0.9000\n",
            "Epoch 258/300\n",
            "\n",
            "Epoch 00258: val_loss did not improve from 0.26802\n",
            "60/60 - 0s - loss: 0.0814 - acc: 0.9833 - val_loss: 0.3386 - val_acc: 0.9000\n",
            "Epoch 259/300\n",
            "\n",
            "Epoch 00259: val_loss did not improve from 0.26802\n",
            "60/60 - 0s - loss: 0.1120 - acc: 0.9833 - val_loss: 0.3367 - val_acc: 0.9000\n",
            "Epoch 260/300\n",
            "\n",
            "Epoch 00260: val_loss did not improve from 0.26802\n",
            "60/60 - 0s - loss: 0.0763 - acc: 1.0000 - val_loss: 0.3413 - val_acc: 0.9000\n",
            "Epoch 261/300\n",
            "\n",
            "Epoch 00261: val_loss did not improve from 0.26802\n",
            "60/60 - 0s - loss: 0.0599 - acc: 1.0000 - val_loss: 0.3375 - val_acc: 0.9500\n",
            "Epoch 262/300\n",
            "\n",
            "Epoch 00262: val_loss did not improve from 0.26802\n",
            "60/60 - 0s - loss: 0.0683 - acc: 0.9833 - val_loss: 0.3296 - val_acc: 0.9500\n",
            "Epoch 263/300\n",
            "\n",
            "Epoch 00263: val_loss did not improve from 0.26802\n",
            "60/60 - 0s - loss: 0.0934 - acc: 0.9667 - val_loss: 0.3368 - val_acc: 0.9500\n",
            "Epoch 264/300\n",
            "\n",
            "Epoch 00264: val_loss did not improve from 0.26802\n",
            "60/60 - 0s - loss: 0.0919 - acc: 0.9833 - val_loss: 0.3282 - val_acc: 0.9500\n",
            "Epoch 265/300\n",
            "\n",
            "Epoch 00265: val_loss did not improve from 0.26802\n",
            "60/60 - 0s - loss: 0.0795 - acc: 0.9833 - val_loss: 0.3099 - val_acc: 0.9500\n",
            "Epoch 266/300\n",
            "\n",
            "Epoch 00266: val_loss did not improve from 0.26802\n",
            "60/60 - 0s - loss: 0.0938 - acc: 0.9667 - val_loss: 0.2850 - val_acc: 0.9500\n",
            "Epoch 267/300\n",
            "\n",
            "Epoch 00267: val_loss improved from 0.26802 to 0.26681, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.0744 - acc: 0.9833 - val_loss: 0.2668 - val_acc: 0.9500\n",
            "Epoch 268/300\n",
            "\n",
            "Epoch 00268: val_loss did not improve from 0.26681\n",
            "60/60 - 0s - loss: 0.0731 - acc: 1.0000 - val_loss: 0.2701 - val_acc: 0.9500\n",
            "Epoch 269/300\n",
            "\n",
            "Epoch 00269: val_loss did not improve from 0.26681\n",
            "60/60 - 0s - loss: 0.0872 - acc: 0.9833 - val_loss: 0.2847 - val_acc: 0.9500\n",
            "Epoch 270/300\n",
            "\n",
            "Epoch 00270: val_loss did not improve from 0.26681\n",
            "60/60 - 0s - loss: 0.0679 - acc: 1.0000 - val_loss: 0.3044 - val_acc: 0.9500\n",
            "Epoch 271/300\n",
            "\n",
            "Epoch 00271: val_loss did not improve from 0.26681\n",
            "60/60 - 0s - loss: 0.1111 - acc: 0.9833 - val_loss: 0.3326 - val_acc: 0.9000\n",
            "Epoch 272/300\n",
            "\n",
            "Epoch 00272: val_loss did not improve from 0.26681\n",
            "60/60 - 0s - loss: 0.0782 - acc: 0.9667 - val_loss: 0.3260 - val_acc: 0.9000\n",
            "Epoch 273/300\n",
            "\n",
            "Epoch 00273: val_loss did not improve from 0.26681\n",
            "60/60 - 0s - loss: 0.0460 - acc: 1.0000 - val_loss: 0.3183 - val_acc: 0.9000\n",
            "Epoch 274/300\n",
            "\n",
            "Epoch 00274: val_loss did not improve from 0.26681\n",
            "60/60 - 0s - loss: 0.0896 - acc: 0.9833 - val_loss: 0.3268 - val_acc: 0.9000\n",
            "Epoch 275/300\n",
            "\n",
            "Epoch 00275: val_loss did not improve from 0.26681\n",
            "60/60 - 0s - loss: 0.0814 - acc: 0.9833 - val_loss: 0.3387 - val_acc: 0.9000\n",
            "Epoch 276/300\n",
            "\n",
            "Epoch 00276: val_loss did not improve from 0.26681\n",
            "60/60 - 0s - loss: 0.1407 - acc: 0.9667 - val_loss: 0.3949 - val_acc: 0.8000\n",
            "Epoch 277/300\n",
            "\n",
            "Epoch 00277: val_loss did not improve from 0.26681\n",
            "60/60 - 0s - loss: 0.0880 - acc: 1.0000 - val_loss: 0.4149 - val_acc: 0.8000\n",
            "Epoch 278/300\n",
            "\n",
            "Epoch 00278: val_loss did not improve from 0.26681\n",
            "60/60 - 0s - loss: 0.1290 - acc: 0.9667 - val_loss: 0.3587 - val_acc: 0.9000\n",
            "Epoch 279/300\n",
            "\n",
            "Epoch 00279: val_loss did not improve from 0.26681\n",
            "60/60 - 0s - loss: 0.0712 - acc: 1.0000 - val_loss: 0.2999 - val_acc: 0.9500\n",
            "Epoch 280/300\n",
            "\n",
            "Epoch 00280: val_loss did not improve from 0.26681\n",
            "60/60 - 0s - loss: 0.0570 - acc: 1.0000 - val_loss: 0.2737 - val_acc: 0.9500\n",
            "Epoch 281/300\n",
            "\n",
            "Epoch 00281: val_loss improved from 0.26681 to 0.25380, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.0580 - acc: 1.0000 - val_loss: 0.2538 - val_acc: 0.9500\n",
            "Epoch 282/300\n",
            "\n",
            "Epoch 00282: val_loss did not improve from 0.25380\n",
            "60/60 - 0s - loss: 0.0830 - acc: 0.9833 - val_loss: 0.2598 - val_acc: 0.9500\n",
            "Epoch 283/300\n",
            "\n",
            "Epoch 00283: val_loss did not improve from 0.25380\n",
            "60/60 - 0s - loss: 0.0670 - acc: 1.0000 - val_loss: 0.2612 - val_acc: 0.9500\n",
            "Epoch 284/300\n",
            "\n",
            "Epoch 00284: val_loss did not improve from 0.25380\n",
            "60/60 - 0s - loss: 0.0975 - acc: 0.9833 - val_loss: 0.2553 - val_acc: 0.9500\n",
            "Epoch 285/300\n",
            "\n",
            "Epoch 00285: val_loss improved from 0.25380 to 0.25347, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.0581 - acc: 1.0000 - val_loss: 0.2535 - val_acc: 0.9500\n",
            "Epoch 286/300\n",
            "\n",
            "Epoch 00286: val_loss did not improve from 0.25347\n",
            "60/60 - 0s - loss: 0.1128 - acc: 0.9500 - val_loss: 0.2595 - val_acc: 0.9500\n",
            "Epoch 287/300\n",
            "\n",
            "Epoch 00287: val_loss did not improve from 0.25347\n",
            "60/60 - 0s - loss: 0.0726 - acc: 1.0000 - val_loss: 0.2701 - val_acc: 0.9500\n",
            "Epoch 288/300\n",
            "\n",
            "Epoch 00288: val_loss did not improve from 0.25347\n",
            "60/60 - 0s - loss: 0.0794 - acc: 0.9833 - val_loss: 0.2802 - val_acc: 0.9500\n",
            "Epoch 289/300\n",
            "\n",
            "Epoch 00289: val_loss did not improve from 0.25347\n",
            "60/60 - 0s - loss: 0.0755 - acc: 0.9667 - val_loss: 0.3039 - val_acc: 0.9500\n",
            "Epoch 290/300\n",
            "\n",
            "Epoch 00290: val_loss did not improve from 0.25347\n",
            "60/60 - 0s - loss: 0.0757 - acc: 0.9833 - val_loss: 0.3319 - val_acc: 0.9500\n",
            "Epoch 291/300\n",
            "\n",
            "Epoch 00291: val_loss did not improve from 0.25347\n",
            "60/60 - 0s - loss: 0.0585 - acc: 0.9833 - val_loss: 0.3610 - val_acc: 0.8500\n",
            "Epoch 292/300\n",
            "\n",
            "Epoch 00292: val_loss did not improve from 0.25347\n",
            "60/60 - 0s - loss: 0.0592 - acc: 1.0000 - val_loss: 0.3763 - val_acc: 0.8500\n",
            "Epoch 293/300\n",
            "\n",
            "Epoch 00293: val_loss did not improve from 0.25347\n",
            "60/60 - 0s - loss: 0.1110 - acc: 0.9500 - val_loss: 0.3702 - val_acc: 0.8500\n",
            "Epoch 294/300\n",
            "\n",
            "Epoch 00294: val_loss did not improve from 0.25347\n",
            "60/60 - 0s - loss: 0.0684 - acc: 1.0000 - val_loss: 0.3505 - val_acc: 0.9000\n",
            "Epoch 295/300\n",
            "\n",
            "Epoch 00295: val_loss did not improve from 0.25347\n",
            "60/60 - 0s - loss: 0.0677 - acc: 1.0000 - val_loss: 0.3202 - val_acc: 0.9000\n",
            "Epoch 296/300\n",
            "\n",
            "Epoch 00296: val_loss did not improve from 0.25347\n",
            "60/60 - 0s - loss: 0.0963 - acc: 0.9500 - val_loss: 0.3070 - val_acc: 0.9000\n",
            "Epoch 297/300\n",
            "\n",
            "Epoch 00297: val_loss did not improve from 0.25347\n",
            "60/60 - 0s - loss: 0.0839 - acc: 1.0000 - val_loss: 0.3018 - val_acc: 0.9000\n",
            "Epoch 298/300\n",
            "\n",
            "Epoch 00298: val_loss did not improve from 0.25347\n",
            "60/60 - 0s - loss: 0.1183 - acc: 0.9667 - val_loss: 0.3170 - val_acc: 0.9000\n",
            "Epoch 299/300\n",
            "\n",
            "Epoch 00299: val_loss did not improve from 0.25347\n",
            "60/60 - 0s - loss: 0.0615 - acc: 0.9833 - val_loss: 0.3292 - val_acc: 0.9000\n",
            "Epoch 300/300\n",
            "\n",
            "Epoch 00300: val_loss did not improve from 0.25347\n",
            "60/60 - 0s - loss: 0.0811 - acc: 1.0000 - val_loss: 0.3645 - val_acc: 0.9000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngYhI5JQIREQip0QgIhI5JQIRkcj9PxB8RSkaEgn3AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "# Evaluate on test data\n",
            "40/40 [==============================] - 0s 2ms/sample - loss: 0.4919 - acc: 0.8000\n",
            "test loss, test acc: [0.4918532849345866, 0.8]\n",
            "EEG_Deep/Data2A/parsed_P06T.mat\n",
            "Filtering of Training Data Finished\n",
            "EEG_Deep/Data2A/parsed_P06E.mat\n",
            "Filtering of Testing Data Finished\n",
            "(1, 2)\n",
            "Finding labels in training data\n",
            "Finding labels in testing data\n",
            "[1 2 2 1 1 2 1 1 2 2 2 1 1 1 1 2 1 2 1 1]\n",
            "(60, 12, 1536)\n",
            "(60,)\n",
            "(20, 12, 1536)\n",
            "(20,)\n",
            "[[1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]]\n",
            "X_train shape: (60, 1, 12, 1536)\n",
            "60 train samples\n",
            "20 val samples\n",
            "(40, 12, 1536)\n",
            "(40, 1)\n",
            "X_train shape: (40, 1, 12, 1536)\n",
            "40 train samples\n",
            "Train on 60 samples, validate on 20 samples\n",
            "Epoch 1/300\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.69139, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 1s - loss: 0.7326 - acc: 0.5333 - val_loss: 0.6914 - val_acc: 0.4000\n",
            "Epoch 2/300\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.69139 to 0.69102, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6637 - acc: 0.6500 - val_loss: 0.6910 - val_acc: 0.5000\n",
            "Epoch 3/300\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.69102 to 0.69024, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6526 - acc: 0.7333 - val_loss: 0.6902 - val_acc: 0.5000\n",
            "Epoch 4/300\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.69024 to 0.68927, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6184 - acc: 0.8000 - val_loss: 0.6893 - val_acc: 0.4500\n",
            "Epoch 5/300\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.68927 to 0.68829, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5903 - acc: 0.8167 - val_loss: 0.6883 - val_acc: 0.4500\n",
            "Epoch 6/300\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.68829 to 0.68706, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5942 - acc: 0.8333 - val_loss: 0.6871 - val_acc: 0.4000\n",
            "Epoch 7/300\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.68706 to 0.68554, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5673 - acc: 0.8833 - val_loss: 0.6855 - val_acc: 0.4000\n",
            "Epoch 8/300\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.68554 to 0.68428, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5404 - acc: 0.9500 - val_loss: 0.6843 - val_acc: 0.4000\n",
            "Epoch 9/300\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.68428 to 0.68307, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5409 - acc: 0.9500 - val_loss: 0.6831 - val_acc: 0.4000\n",
            "Epoch 10/300\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.68307 to 0.68175, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5337 - acc: 0.9000 - val_loss: 0.6817 - val_acc: 0.4000\n",
            "Epoch 11/300\n",
            "\n",
            "Epoch 00011: val_loss improved from 0.68175 to 0.67997, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5075 - acc: 0.9500 - val_loss: 0.6800 - val_acc: 0.4000\n",
            "Epoch 12/300\n",
            "\n",
            "Epoch 00012: val_loss improved from 0.67997 to 0.67842, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4881 - acc: 0.9000 - val_loss: 0.6784 - val_acc: 0.4000\n",
            "Epoch 13/300\n",
            "\n",
            "Epoch 00013: val_loss improved from 0.67842 to 0.67802, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4789 - acc: 0.9167 - val_loss: 0.6780 - val_acc: 0.4000\n",
            "Epoch 14/300\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.67802\n",
            "60/60 - 0s - loss: 0.4746 - acc: 0.9167 - val_loss: 0.6794 - val_acc: 0.4000\n",
            "Epoch 15/300\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.67802\n",
            "60/60 - 0s - loss: 0.4823 - acc: 0.8833 - val_loss: 0.6810 - val_acc: 0.4000\n",
            "Epoch 16/300\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.67802\n",
            "60/60 - 0s - loss: 0.4240 - acc: 0.9500 - val_loss: 0.6841 - val_acc: 0.4000\n",
            "Epoch 17/300\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.67802\n",
            "60/60 - 0s - loss: 0.3998 - acc: 0.9500 - val_loss: 0.6899 - val_acc: 0.4000\n",
            "Epoch 18/300\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.67802\n",
            "60/60 - 0s - loss: 0.4008 - acc: 0.9333 - val_loss: 0.6959 - val_acc: 0.4000\n",
            "Epoch 19/300\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.67802\n",
            "60/60 - 0s - loss: 0.3557 - acc: 0.9667 - val_loss: 0.7006 - val_acc: 0.4000\n",
            "Epoch 20/300\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.67802\n",
            "60/60 - 0s - loss: 0.3545 - acc: 0.9667 - val_loss: 0.7056 - val_acc: 0.4000\n",
            "Epoch 21/300\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.67802\n",
            "60/60 - 0s - loss: 0.3561 - acc: 0.9333 - val_loss: 0.7114 - val_acc: 0.4000\n",
            "Epoch 22/300\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.67802\n",
            "60/60 - 0s - loss: 0.3302 - acc: 0.9500 - val_loss: 0.7218 - val_acc: 0.4000\n",
            "Epoch 23/300\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.67802\n",
            "60/60 - 0s - loss: 0.2859 - acc: 0.9667 - val_loss: 0.7398 - val_acc: 0.4000\n",
            "Epoch 24/300\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.67802\n",
            "60/60 - 0s - loss: 0.3116 - acc: 0.9167 - val_loss: 0.7614 - val_acc: 0.4000\n",
            "Epoch 25/300\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.67802\n",
            "60/60 - 0s - loss: 0.2842 - acc: 0.9500 - val_loss: 0.7859 - val_acc: 0.4000\n",
            "Epoch 26/300\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.67802\n",
            "60/60 - 0s - loss: 0.3159 - acc: 0.9000 - val_loss: 0.8141 - val_acc: 0.4000\n",
            "Epoch 27/300\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.67802\n",
            "60/60 - 0s - loss: 0.2710 - acc: 0.9167 - val_loss: 0.8571 - val_acc: 0.4000\n",
            "Epoch 28/300\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.67802\n",
            "60/60 - 0s - loss: 0.2627 - acc: 0.9000 - val_loss: 0.8961 - val_acc: 0.4000\n",
            "Epoch 29/300\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.67802\n",
            "60/60 - 0s - loss: 0.2445 - acc: 0.9500 - val_loss: 0.9214 - val_acc: 0.4000\n",
            "Epoch 30/300\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.67802\n",
            "60/60 - 0s - loss: 0.2671 - acc: 0.9167 - val_loss: 0.9395 - val_acc: 0.4000\n",
            "Epoch 31/300\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.67802\n",
            "60/60 - 0s - loss: 0.2637 - acc: 0.9333 - val_loss: 0.9499 - val_acc: 0.4000\n",
            "Epoch 32/300\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.67802\n",
            "60/60 - 0s - loss: 0.2689 - acc: 0.9167 - val_loss: 0.9603 - val_acc: 0.4000\n",
            "Epoch 33/300\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.67802\n",
            "60/60 - 0s - loss: 0.2276 - acc: 0.9667 - val_loss: 0.9648 - val_acc: 0.4000\n",
            "Epoch 34/300\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.67802\n",
            "60/60 - 0s - loss: 0.3045 - acc: 0.8833 - val_loss: 0.9678 - val_acc: 0.4000\n",
            "Epoch 35/300\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.67802\n",
            "60/60 - 0s - loss: 0.2384 - acc: 0.9333 - val_loss: 0.9710 - val_acc: 0.4000\n",
            "Epoch 36/300\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.67802\n",
            "60/60 - 0s - loss: 0.2233 - acc: 0.9667 - val_loss: 1.0002 - val_acc: 0.4000\n",
            "Epoch 37/300\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.67802\n",
            "60/60 - 0s - loss: 0.2039 - acc: 0.9667 - val_loss: 1.0504 - val_acc: 0.4000\n",
            "Epoch 38/300\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.67802\n",
            "60/60 - 0s - loss: 0.2315 - acc: 0.9333 - val_loss: 1.0931 - val_acc: 0.4000\n",
            "Epoch 39/300\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.67802\n",
            "60/60 - 0s - loss: 0.2135 - acc: 0.9333 - val_loss: 1.1240 - val_acc: 0.4000\n",
            "Epoch 40/300\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.67802\n",
            "60/60 - 0s - loss: 0.1892 - acc: 0.9500 - val_loss: 1.1246 - val_acc: 0.4000\n",
            "Epoch 41/300\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.67802\n",
            "60/60 - 0s - loss: 0.2603 - acc: 0.9167 - val_loss: 1.1266 - val_acc: 0.4000\n",
            "Epoch 42/300\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.67802\n",
            "60/60 - 0s - loss: 0.1931 - acc: 0.9333 - val_loss: 1.1155 - val_acc: 0.4000\n",
            "Epoch 43/300\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.67802\n",
            "60/60 - 0s - loss: 0.1552 - acc: 0.9667 - val_loss: 1.1251 - val_acc: 0.4000\n",
            "Epoch 44/300\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.67802\n",
            "60/60 - 0s - loss: 0.1816 - acc: 0.9500 - val_loss: 1.1343 - val_acc: 0.4000\n",
            "Epoch 45/300\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.67802\n",
            "60/60 - 0s - loss: 0.2080 - acc: 0.9333 - val_loss: 1.1400 - val_acc: 0.4000\n",
            "Epoch 46/300\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.67802\n",
            "60/60 - 0s - loss: 0.1622 - acc: 0.9667 - val_loss: 1.1422 - val_acc: 0.4000\n",
            "Epoch 47/300\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.67802\n",
            "60/60 - 0s - loss: 0.1552 - acc: 0.9833 - val_loss: 1.1518 - val_acc: 0.4000\n",
            "Epoch 48/300\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.67802\n",
            "60/60 - 0s - loss: 0.1915 - acc: 0.9667 - val_loss: 1.1865 - val_acc: 0.4000\n",
            "Epoch 49/300\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.67802\n",
            "60/60 - 0s - loss: 0.1326 - acc: 1.0000 - val_loss: 1.2237 - val_acc: 0.4000\n",
            "Epoch 50/300\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.67802\n",
            "60/60 - 0s - loss: 0.1762 - acc: 0.9500 - val_loss: 1.2634 - val_acc: 0.4000\n",
            "Epoch 51/300\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.67802\n",
            "60/60 - 0s - loss: 0.2296 - acc: 0.9333 - val_loss: 1.2886 - val_acc: 0.4000\n",
            "Epoch 52/300\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.67802\n",
            "60/60 - 0s - loss: 0.1745 - acc: 0.9167 - val_loss: 1.2757 - val_acc: 0.4000\n",
            "Epoch 53/300\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.67802\n",
            "60/60 - 0s - loss: 0.1638 - acc: 0.9667 - val_loss: 1.2460 - val_acc: 0.4000\n",
            "Epoch 54/300\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.67802\n",
            "60/60 - 0s - loss: 0.1694 - acc: 0.9667 - val_loss: 1.2210 - val_acc: 0.4000\n",
            "Epoch 55/300\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.67802\n",
            "60/60 - 0s - loss: 0.1500 - acc: 1.0000 - val_loss: 1.2193 - val_acc: 0.4000\n",
            "Epoch 56/300\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.67802\n",
            "60/60 - 0s - loss: 0.1411 - acc: 0.9833 - val_loss: 1.2206 - val_acc: 0.4000\n",
            "Epoch 57/300\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.67802\n",
            "60/60 - 0s - loss: 0.1033 - acc: 1.0000 - val_loss: 1.2216 - val_acc: 0.4000\n",
            "Epoch 58/300\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.67802\n",
            "60/60 - 0s - loss: 0.1679 - acc: 0.9500 - val_loss: 1.2089 - val_acc: 0.4500\n",
            "Epoch 59/300\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.67802\n",
            "60/60 - 0s - loss: 0.1189 - acc: 0.9667 - val_loss: 1.1959 - val_acc: 0.4500\n",
            "Epoch 60/300\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.67802\n",
            "60/60 - 0s - loss: 0.1113 - acc: 1.0000 - val_loss: 1.1965 - val_acc: 0.5000\n",
            "Epoch 61/300\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.67802\n",
            "60/60 - 0s - loss: 0.1631 - acc: 0.9833 - val_loss: 1.1955 - val_acc: 0.5500\n",
            "Epoch 62/300\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.67802\n",
            "60/60 - 0s - loss: 0.1122 - acc: 0.9667 - val_loss: 1.1946 - val_acc: 0.5500\n",
            "Epoch 63/300\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.67802\n",
            "60/60 - 0s - loss: 0.1230 - acc: 0.9667 - val_loss: 1.1906 - val_acc: 0.5500\n",
            "Epoch 64/300\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.67802\n",
            "60/60 - 0s - loss: 0.1496 - acc: 0.9500 - val_loss: 1.1757 - val_acc: 0.5500\n",
            "Epoch 65/300\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.67802\n",
            "60/60 - 0s - loss: 0.1313 - acc: 0.9667 - val_loss: 1.1487 - val_acc: 0.5500\n",
            "Epoch 66/300\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.67802\n",
            "60/60 - 0s - loss: 0.1451 - acc: 0.9500 - val_loss: 1.1088 - val_acc: 0.6000\n",
            "Epoch 67/300\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.67802\n",
            "60/60 - 0s - loss: 0.1239 - acc: 1.0000 - val_loss: 1.0811 - val_acc: 0.6000\n",
            "Epoch 68/300\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.67802\n",
            "60/60 - 0s - loss: 0.1699 - acc: 0.9333 - val_loss: 1.0622 - val_acc: 0.6000\n",
            "Epoch 69/300\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.67802\n",
            "60/60 - 0s - loss: 0.0928 - acc: 1.0000 - val_loss: 1.0538 - val_acc: 0.6000\n",
            "Epoch 70/300\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.67802\n",
            "60/60 - 0s - loss: 0.1452 - acc: 0.9833 - val_loss: 1.0329 - val_acc: 0.6000\n",
            "Epoch 71/300\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.67802\n",
            "60/60 - 0s - loss: 0.1391 - acc: 1.0000 - val_loss: 1.0331 - val_acc: 0.6000\n",
            "Epoch 72/300\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.67802\n",
            "60/60 - 0s - loss: 0.1479 - acc: 0.9500 - val_loss: 1.0120 - val_acc: 0.6000\n",
            "Epoch 73/300\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.67802\n",
            "60/60 - 0s - loss: 0.1069 - acc: 0.9833 - val_loss: 0.9930 - val_acc: 0.6000\n",
            "Epoch 74/300\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.67802\n",
            "60/60 - 0s - loss: 0.1065 - acc: 1.0000 - val_loss: 0.9768 - val_acc: 0.6000\n",
            "Epoch 75/300\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.67802\n",
            "60/60 - 0s - loss: 0.1761 - acc: 0.9333 - val_loss: 0.9634 - val_acc: 0.6000\n",
            "Epoch 76/300\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.67802\n",
            "60/60 - 0s - loss: 0.0872 - acc: 1.0000 - val_loss: 0.9139 - val_acc: 0.6000\n",
            "Epoch 77/300\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.67802\n",
            "60/60 - 0s - loss: 0.1329 - acc: 0.9500 - val_loss: 0.8813 - val_acc: 0.6000\n",
            "Epoch 78/300\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.67802\n",
            "60/60 - 0s - loss: 0.1076 - acc: 0.9833 - val_loss: 0.8586 - val_acc: 0.6500\n",
            "Epoch 79/300\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.67802\n",
            "60/60 - 0s - loss: 0.1219 - acc: 0.9667 - val_loss: 0.8605 - val_acc: 0.6000\n",
            "Epoch 80/300\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.67802\n",
            "60/60 - 0s - loss: 0.1133 - acc: 1.0000 - val_loss: 0.8687 - val_acc: 0.6000\n",
            "Epoch 81/300\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.67802\n",
            "60/60 - 0s - loss: 0.1126 - acc: 0.9833 - val_loss: 0.8744 - val_acc: 0.6000\n",
            "Epoch 82/300\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.67802\n",
            "60/60 - 0s - loss: 0.1086 - acc: 0.9833 - val_loss: 0.8654 - val_acc: 0.6000\n",
            "Epoch 83/300\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.67802\n",
            "60/60 - 0s - loss: 0.1226 - acc: 0.9667 - val_loss: 0.8392 - val_acc: 0.6500\n",
            "Epoch 84/300\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.67802\n",
            "60/60 - 0s - loss: 0.0830 - acc: 1.0000 - val_loss: 0.8309 - val_acc: 0.6500\n",
            "Epoch 85/300\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.67802\n",
            "60/60 - 0s - loss: 0.1046 - acc: 1.0000 - val_loss: 0.8356 - val_acc: 0.6500\n",
            "Epoch 86/300\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.67802\n",
            "60/60 - 0s - loss: 0.1017 - acc: 1.0000 - val_loss: 0.8188 - val_acc: 0.7000\n",
            "Epoch 87/300\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.67802\n",
            "60/60 - 0s - loss: 0.1231 - acc: 0.9833 - val_loss: 0.7969 - val_acc: 0.7000\n",
            "Epoch 88/300\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.67802\n",
            "60/60 - 0s - loss: 0.1480 - acc: 0.9333 - val_loss: 0.7757 - val_acc: 0.7000\n",
            "Epoch 89/300\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.67802\n",
            "60/60 - 0s - loss: 0.1807 - acc: 0.9500 - val_loss: 0.7448 - val_acc: 0.7000\n",
            "Epoch 90/300\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.67802\n",
            "60/60 - 0s - loss: 0.1187 - acc: 0.9833 - val_loss: 0.7186 - val_acc: 0.7000\n",
            "Epoch 91/300\n",
            "\n",
            "Epoch 00091: val_loss improved from 0.67802 to 0.67712, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1385 - acc: 0.9833 - val_loss: 0.6771 - val_acc: 0.7000\n",
            "Epoch 92/300\n",
            "\n",
            "Epoch 00092: val_loss improved from 0.67712 to 0.62764, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1708 - acc: 0.9333 - val_loss: 0.6276 - val_acc: 0.7000\n",
            "Epoch 93/300\n",
            "\n",
            "Epoch 00093: val_loss improved from 0.62764 to 0.57637, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1099 - acc: 0.9833 - val_loss: 0.5764 - val_acc: 0.7000\n",
            "Epoch 94/300\n",
            "\n",
            "Epoch 00094: val_loss improved from 0.57637 to 0.54769, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.0886 - acc: 0.9833 - val_loss: 0.5477 - val_acc: 0.7000\n",
            "Epoch 95/300\n",
            "\n",
            "Epoch 00095: val_loss improved from 0.54769 to 0.52912, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1507 - acc: 0.9500 - val_loss: 0.5291 - val_acc: 0.7000\n",
            "Epoch 96/300\n",
            "\n",
            "Epoch 00096: val_loss improved from 0.52912 to 0.51663, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.0860 - acc: 1.0000 - val_loss: 0.5166 - val_acc: 0.7000\n",
            "Epoch 97/300\n",
            "\n",
            "Epoch 00097: val_loss improved from 0.51663 to 0.49825, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1058 - acc: 0.9833 - val_loss: 0.4983 - val_acc: 0.7000\n",
            "Epoch 98/300\n",
            "\n",
            "Epoch 00098: val_loss improved from 0.49825 to 0.49192, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1187 - acc: 1.0000 - val_loss: 0.4919 - val_acc: 0.7500\n",
            "Epoch 99/300\n",
            "\n",
            "Epoch 00099: val_loss improved from 0.49192 to 0.48580, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1247 - acc: 1.0000 - val_loss: 0.4858 - val_acc: 0.7500\n",
            "Epoch 100/300\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.48580\n",
            "60/60 - 0s - loss: 0.1185 - acc: 0.9500 - val_loss: 0.5058 - val_acc: 0.7000\n",
            "Epoch 101/300\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.48580\n",
            "60/60 - 0s - loss: 0.0903 - acc: 1.0000 - val_loss: 0.5351 - val_acc: 0.7000\n",
            "Epoch 102/300\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.48580\n",
            "60/60 - 0s - loss: 0.1491 - acc: 0.9500 - val_loss: 0.5283 - val_acc: 0.7000\n",
            "Epoch 103/300\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.48580\n",
            "60/60 - 0s - loss: 0.0829 - acc: 1.0000 - val_loss: 0.5121 - val_acc: 0.7500\n",
            "Epoch 104/300\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.48580\n",
            "60/60 - 0s - loss: 0.0685 - acc: 1.0000 - val_loss: 0.4947 - val_acc: 0.7500\n",
            "Epoch 105/300\n",
            "\n",
            "Epoch 00105: val_loss improved from 0.48580 to 0.47709, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.0716 - acc: 1.0000 - val_loss: 0.4771 - val_acc: 0.7500\n",
            "Epoch 106/300\n",
            "\n",
            "Epoch 00106: val_loss improved from 0.47709 to 0.45263, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.0946 - acc: 1.0000 - val_loss: 0.4526 - val_acc: 0.7500\n",
            "Epoch 107/300\n",
            "\n",
            "Epoch 00107: val_loss improved from 0.45263 to 0.42807, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.0888 - acc: 0.9833 - val_loss: 0.4281 - val_acc: 0.7500\n",
            "Epoch 108/300\n",
            "\n",
            "Epoch 00108: val_loss improved from 0.42807 to 0.41089, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1198 - acc: 0.9667 - val_loss: 0.4109 - val_acc: 0.8000\n",
            "Epoch 109/300\n",
            "\n",
            "Epoch 00109: val_loss improved from 0.41089 to 0.39484, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1103 - acc: 1.0000 - val_loss: 0.3948 - val_acc: 0.8000\n",
            "Epoch 110/300\n",
            "\n",
            "Epoch 00110: val_loss improved from 0.39484 to 0.38148, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.0815 - acc: 1.0000 - val_loss: 0.3815 - val_acc: 0.8000\n",
            "Epoch 111/300\n",
            "\n",
            "Epoch 00111: val_loss improved from 0.38148 to 0.37544, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.0684 - acc: 1.0000 - val_loss: 0.3754 - val_acc: 0.8500\n",
            "Epoch 112/300\n",
            "\n",
            "Epoch 00112: val_loss improved from 0.37544 to 0.37098, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.0818 - acc: 0.9833 - val_loss: 0.3710 - val_acc: 0.8500\n",
            "Epoch 113/300\n",
            "\n",
            "Epoch 00113: val_loss improved from 0.37098 to 0.36921, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.0653 - acc: 1.0000 - val_loss: 0.3692 - val_acc: 0.8500\n",
            "Epoch 114/300\n",
            "\n",
            "Epoch 00114: val_loss improved from 0.36921 to 0.36725, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.0604 - acc: 0.9833 - val_loss: 0.3672 - val_acc: 0.8500\n",
            "Epoch 115/300\n",
            "\n",
            "Epoch 00115: val_loss improved from 0.36725 to 0.36493, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.0719 - acc: 1.0000 - val_loss: 0.3649 - val_acc: 0.8500\n",
            "Epoch 116/300\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.36493\n",
            "60/60 - 0s - loss: 0.0621 - acc: 1.0000 - val_loss: 0.3661 - val_acc: 0.8500\n",
            "Epoch 117/300\n",
            "\n",
            "Epoch 00117: val_loss improved from 0.36493 to 0.36475, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.0736 - acc: 0.9833 - val_loss: 0.3647 - val_acc: 0.8500\n",
            "Epoch 118/300\n",
            "\n",
            "Epoch 00118: val_loss improved from 0.36475 to 0.36025, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.0805 - acc: 0.9833 - val_loss: 0.3602 - val_acc: 0.8500\n",
            "Epoch 119/300\n",
            "\n",
            "Epoch 00119: val_loss improved from 0.36025 to 0.35977, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1461 - acc: 0.9500 - val_loss: 0.3598 - val_acc: 0.8500\n",
            "Epoch 120/300\n",
            "\n",
            "Epoch 00120: val_loss improved from 0.35977 to 0.35144, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.0950 - acc: 0.9667 - val_loss: 0.3514 - val_acc: 0.8500\n",
            "Epoch 121/300\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.35144\n",
            "60/60 - 0s - loss: 0.0947 - acc: 0.9667 - val_loss: 0.3673 - val_acc: 0.8500\n",
            "Epoch 122/300\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.35144\n",
            "60/60 - 0s - loss: 0.1013 - acc: 0.9667 - val_loss: 0.3705 - val_acc: 0.8500\n",
            "Epoch 123/300\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 0.35144\n",
            "60/60 - 0s - loss: 0.0938 - acc: 0.9833 - val_loss: 0.3628 - val_acc: 0.8500\n",
            "Epoch 124/300\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 0.35144\n",
            "60/60 - 0s - loss: 0.0648 - acc: 0.9833 - val_loss: 0.3540 - val_acc: 0.8500\n",
            "Epoch 125/300\n",
            "\n",
            "Epoch 00125: val_loss improved from 0.35144 to 0.33189, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.0816 - acc: 0.9667 - val_loss: 0.3319 - val_acc: 0.8500\n",
            "Epoch 126/300\n",
            "\n",
            "Epoch 00126: val_loss improved from 0.33189 to 0.31894, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.0765 - acc: 1.0000 - val_loss: 0.3189 - val_acc: 0.8500\n",
            "Epoch 127/300\n",
            "\n",
            "Epoch 00127: val_loss improved from 0.31894 to 0.31202, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.0978 - acc: 1.0000 - val_loss: 0.3120 - val_acc: 0.8500\n",
            "Epoch 128/300\n",
            "\n",
            "Epoch 00128: val_loss improved from 0.31202 to 0.30442, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.0668 - acc: 0.9833 - val_loss: 0.3044 - val_acc: 0.8500\n",
            "Epoch 129/300\n",
            "\n",
            "Epoch 00129: val_loss improved from 0.30442 to 0.29609, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.0613 - acc: 1.0000 - val_loss: 0.2961 - val_acc: 0.8500\n",
            "Epoch 130/300\n",
            "\n",
            "Epoch 00130: val_loss improved from 0.29609 to 0.29251, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.0764 - acc: 1.0000 - val_loss: 0.2925 - val_acc: 0.8500\n",
            "Epoch 131/300\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 0.29251\n",
            "60/60 - 0s - loss: 0.0572 - acc: 1.0000 - val_loss: 0.2933 - val_acc: 0.8500\n",
            "Epoch 132/300\n",
            "\n",
            "Epoch 00132: val_loss improved from 0.29251 to 0.28978, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.0792 - acc: 0.9833 - val_loss: 0.2898 - val_acc: 0.8500\n",
            "Epoch 133/300\n",
            "\n",
            "Epoch 00133: val_loss improved from 0.28978 to 0.28423, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.0781 - acc: 0.9833 - val_loss: 0.2842 - val_acc: 0.8500\n",
            "Epoch 134/300\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.28423\n",
            "60/60 - 0s - loss: 0.1117 - acc: 0.9667 - val_loss: 0.2859 - val_acc: 0.8500\n",
            "Epoch 135/300\n",
            "\n",
            "Epoch 00135: val_loss improved from 0.28423 to 0.28402, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1311 - acc: 0.9500 - val_loss: 0.2840 - val_acc: 0.8500\n",
            "Epoch 136/300\n",
            "\n",
            "Epoch 00136: val_loss improved from 0.28402 to 0.27866, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1210 - acc: 0.9667 - val_loss: 0.2787 - val_acc: 0.8500\n",
            "Epoch 137/300\n",
            "\n",
            "Epoch 00137: val_loss improved from 0.27866 to 0.27467, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.0805 - acc: 0.9833 - val_loss: 0.2747 - val_acc: 0.8500\n",
            "Epoch 138/300\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.27467\n",
            "60/60 - 0s - loss: 0.0884 - acc: 0.9833 - val_loss: 0.2821 - val_acc: 0.8500\n",
            "Epoch 139/300\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.27467\n",
            "60/60 - 0s - loss: 0.0532 - acc: 1.0000 - val_loss: 0.2880 - val_acc: 0.8500\n",
            "Epoch 140/300\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 0.27467\n",
            "60/60 - 0s - loss: 0.0586 - acc: 1.0000 - val_loss: 0.2919 - val_acc: 0.8500\n",
            "Epoch 141/300\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.27467\n",
            "60/60 - 0s - loss: 0.0886 - acc: 0.9833 - val_loss: 0.2998 - val_acc: 0.8500\n",
            "Epoch 142/300\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.27467\n",
            "60/60 - 0s - loss: 0.0812 - acc: 0.9833 - val_loss: 0.3133 - val_acc: 0.8500\n",
            "Epoch 143/300\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 0.27467\n",
            "60/60 - 0s - loss: 0.0721 - acc: 0.9833 - val_loss: 0.3272 - val_acc: 0.8500\n",
            "Epoch 144/300\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 0.27467\n",
            "60/60 - 0s - loss: 0.0730 - acc: 0.9833 - val_loss: 0.3285 - val_acc: 0.8500\n",
            "Epoch 145/300\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.27467\n",
            "60/60 - 0s - loss: 0.0884 - acc: 0.9833 - val_loss: 0.3244 - val_acc: 0.8500\n",
            "Epoch 146/300\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.27467\n",
            "60/60 - 0s - loss: 0.0449 - acc: 1.0000 - val_loss: 0.3236 - val_acc: 0.8500\n",
            "Epoch 147/300\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.27467\n",
            "60/60 - 0s - loss: 0.0633 - acc: 1.0000 - val_loss: 0.3258 - val_acc: 0.8500\n",
            "Epoch 148/300\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 0.27467\n",
            "60/60 - 0s - loss: 0.1548 - acc: 0.9667 - val_loss: 0.3177 - val_acc: 0.8500\n",
            "Epoch 149/300\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.27467\n",
            "60/60 - 0s - loss: 0.0552 - acc: 1.0000 - val_loss: 0.2917 - val_acc: 0.8500\n",
            "Epoch 150/300\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.27467\n",
            "60/60 - 0s - loss: 0.0704 - acc: 1.0000 - val_loss: 0.2788 - val_acc: 0.8500\n",
            "Epoch 151/300\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.27467\n",
            "60/60 - 0s - loss: 0.0614 - acc: 1.0000 - val_loss: 0.2749 - val_acc: 0.8500\n",
            "Epoch 152/300\n",
            "\n",
            "Epoch 00152: val_loss improved from 0.27467 to 0.27445, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.0765 - acc: 0.9833 - val_loss: 0.2745 - val_acc: 0.8500\n",
            "Epoch 153/300\n",
            "\n",
            "Epoch 00153: val_loss improved from 0.27445 to 0.26971, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.0515 - acc: 0.9833 - val_loss: 0.2697 - val_acc: 0.8500\n",
            "Epoch 154/300\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.26971\n",
            "60/60 - 0s - loss: 0.0768 - acc: 1.0000 - val_loss: 0.2775 - val_acc: 0.8500\n",
            "Epoch 155/300\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.26971\n",
            "60/60 - 0s - loss: 0.0450 - acc: 1.0000 - val_loss: 0.2851 - val_acc: 0.8500\n",
            "Epoch 156/300\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.26971\n",
            "60/60 - 0s - loss: 0.1154 - acc: 0.9667 - val_loss: 0.2799 - val_acc: 0.8500\n",
            "Epoch 157/300\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.26971\n",
            "60/60 - 0s - loss: 0.0730 - acc: 1.0000 - val_loss: 0.2715 - val_acc: 0.8500\n",
            "Epoch 158/300\n",
            "\n",
            "Epoch 00158: val_loss improved from 0.26971 to 0.26661, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.0873 - acc: 0.9833 - val_loss: 0.2666 - val_acc: 0.8500\n",
            "Epoch 159/300\n",
            "\n",
            "Epoch 00159: val_loss improved from 0.26661 to 0.26494, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.0592 - acc: 0.9833 - val_loss: 0.2649 - val_acc: 0.8500\n",
            "Epoch 160/300\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.26494\n",
            "60/60 - 0s - loss: 0.0707 - acc: 0.9833 - val_loss: 0.2684 - val_acc: 0.8500\n",
            "Epoch 161/300\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.26494\n",
            "60/60 - 0s - loss: 0.0858 - acc: 0.9833 - val_loss: 0.2748 - val_acc: 0.8500\n",
            "Epoch 162/300\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.26494\n",
            "60/60 - 0s - loss: 0.0572 - acc: 1.0000 - val_loss: 0.2888 - val_acc: 0.8500\n",
            "Epoch 163/300\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.26494\n",
            "60/60 - 0s - loss: 0.0586 - acc: 0.9833 - val_loss: 0.2918 - val_acc: 0.8500\n",
            "Epoch 164/300\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.26494\n",
            "60/60 - 0s - loss: 0.0924 - acc: 0.9833 - val_loss: 0.2966 - val_acc: 0.8500\n",
            "Epoch 165/300\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.26494\n",
            "60/60 - 0s - loss: 0.0474 - acc: 1.0000 - val_loss: 0.2980 - val_acc: 0.8500\n",
            "Epoch 166/300\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 0.26494\n",
            "60/60 - 0s - loss: 0.0478 - acc: 1.0000 - val_loss: 0.2961 - val_acc: 0.8500\n",
            "Epoch 167/300\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.26494\n",
            "60/60 - 0s - loss: 0.0462 - acc: 1.0000 - val_loss: 0.2922 - val_acc: 0.8500\n",
            "Epoch 168/300\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.26494\n",
            "60/60 - 0s - loss: 0.0477 - acc: 1.0000 - val_loss: 0.2871 - val_acc: 0.8500\n",
            "Epoch 169/300\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 0.26494\n",
            "60/60 - 0s - loss: 0.0469 - acc: 1.0000 - val_loss: 0.2786 - val_acc: 0.8500\n",
            "Epoch 170/300\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.26494\n",
            "60/60 - 0s - loss: 0.0572 - acc: 1.0000 - val_loss: 0.2725 - val_acc: 0.9000\n",
            "Epoch 171/300\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.26494\n",
            "60/60 - 0s - loss: 0.0442 - acc: 1.0000 - val_loss: 0.2673 - val_acc: 0.9000\n",
            "Epoch 172/300\n",
            "\n",
            "Epoch 00172: val_loss improved from 0.26494 to 0.26013, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.0379 - acc: 1.0000 - val_loss: 0.2601 - val_acc: 0.9000\n",
            "Epoch 173/300\n",
            "\n",
            "Epoch 00173: val_loss improved from 0.26013 to 0.25815, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.0530 - acc: 1.0000 - val_loss: 0.2581 - val_acc: 0.9000\n",
            "Epoch 174/300\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.25815\n",
            "60/60 - 0s - loss: 0.0830 - acc: 0.9667 - val_loss: 0.2584 - val_acc: 0.9000\n",
            "Epoch 175/300\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.25815\n",
            "60/60 - 0s - loss: 0.0946 - acc: 0.9667 - val_loss: 0.2672 - val_acc: 0.9000\n",
            "Epoch 176/300\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 0.25815\n",
            "60/60 - 0s - loss: 0.0531 - acc: 1.0000 - val_loss: 0.2653 - val_acc: 0.8500\n",
            "Epoch 177/300\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.25815\n",
            "60/60 - 0s - loss: 0.0632 - acc: 1.0000 - val_loss: 0.2611 - val_acc: 0.9000\n",
            "Epoch 178/300\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.25815\n",
            "60/60 - 0s - loss: 0.0669 - acc: 1.0000 - val_loss: 0.2597 - val_acc: 0.9000\n",
            "Epoch 179/300\n",
            "\n",
            "Epoch 00179: val_loss improved from 0.25815 to 0.25610, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.0479 - acc: 1.0000 - val_loss: 0.2561 - val_acc: 0.9000\n",
            "Epoch 180/300\n",
            "\n",
            "Epoch 00180: val_loss improved from 0.25610 to 0.24330, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.0958 - acc: 0.9667 - val_loss: 0.2433 - val_acc: 0.9000\n",
            "Epoch 181/300\n",
            "\n",
            "Epoch 00181: val_loss improved from 0.24330 to 0.22672, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.0360 - acc: 1.0000 - val_loss: 0.2267 - val_acc: 0.9000\n",
            "Epoch 182/300\n",
            "\n",
            "Epoch 00182: val_loss improved from 0.22672 to 0.21943, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.0461 - acc: 1.0000 - val_loss: 0.2194 - val_acc: 0.9000\n",
            "Epoch 183/300\n",
            "\n",
            "Epoch 00183: val_loss improved from 0.21943 to 0.21562, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.0645 - acc: 0.9833 - val_loss: 0.2156 - val_acc: 0.9000\n",
            "Epoch 184/300\n",
            "\n",
            "Epoch 00184: val_loss improved from 0.21562 to 0.21203, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.0597 - acc: 1.0000 - val_loss: 0.2120 - val_acc: 0.9000\n",
            "Epoch 185/300\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.21203\n",
            "60/60 - 0s - loss: 0.0773 - acc: 0.9667 - val_loss: 0.2130 - val_acc: 0.9000\n",
            "Epoch 186/300\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.21203\n",
            "60/60 - 0s - loss: 0.0540 - acc: 0.9833 - val_loss: 0.2181 - val_acc: 0.9000\n",
            "Epoch 187/300\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 0.21203\n",
            "60/60 - 0s - loss: 0.0356 - acc: 1.0000 - val_loss: 0.2221 - val_acc: 0.9000\n",
            "Epoch 188/300\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.21203\n",
            "60/60 - 0s - loss: 0.0503 - acc: 1.0000 - val_loss: 0.2235 - val_acc: 0.9000\n",
            "Epoch 189/300\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.21203\n",
            "60/60 - 0s - loss: 0.0448 - acc: 1.0000 - val_loss: 0.2256 - val_acc: 0.9000\n",
            "Epoch 190/300\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.21203\n",
            "60/60 - 0s - loss: 0.0952 - acc: 0.9833 - val_loss: 0.2321 - val_acc: 0.9000\n",
            "Epoch 191/300\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.21203\n",
            "60/60 - 0s - loss: 0.0611 - acc: 1.0000 - val_loss: 0.2417 - val_acc: 0.8500\n",
            "Epoch 192/300\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 0.21203\n",
            "60/60 - 0s - loss: 0.0704 - acc: 1.0000 - val_loss: 0.2734 - val_acc: 0.8500\n",
            "Epoch 193/300\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.21203\n",
            "60/60 - 0s - loss: 0.0401 - acc: 1.0000 - val_loss: 0.2982 - val_acc: 0.8500\n",
            "Epoch 194/300\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.21203\n",
            "60/60 - 0s - loss: 0.0738 - acc: 0.9833 - val_loss: 0.3021 - val_acc: 0.8500\n",
            "Epoch 195/300\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.21203\n",
            "60/60 - 0s - loss: 0.0614 - acc: 0.9833 - val_loss: 0.3059 - val_acc: 0.8500\n",
            "Epoch 196/300\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.21203\n",
            "60/60 - 0s - loss: 0.0476 - acc: 1.0000 - val_loss: 0.2939 - val_acc: 0.8500\n",
            "Epoch 197/300\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.21203\n",
            "60/60 - 0s - loss: 0.0277 - acc: 1.0000 - val_loss: 0.2810 - val_acc: 0.8500\n",
            "Epoch 198/300\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.21203\n",
            "60/60 - 0s - loss: 0.0600 - acc: 1.0000 - val_loss: 0.2586 - val_acc: 0.8500\n",
            "Epoch 199/300\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.21203\n",
            "60/60 - 0s - loss: 0.0555 - acc: 1.0000 - val_loss: 0.2288 - val_acc: 0.9000\n",
            "Epoch 200/300\n",
            "\n",
            "Epoch 00200: val_loss improved from 0.21203 to 0.21091, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.0663 - acc: 0.9667 - val_loss: 0.2109 - val_acc: 0.9000\n",
            "Epoch 201/300\n",
            "\n",
            "Epoch 00201: val_loss improved from 0.21091 to 0.19711, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.0443 - acc: 1.0000 - val_loss: 0.1971 - val_acc: 0.9000\n",
            "Epoch 202/300\n",
            "\n",
            "Epoch 00202: val_loss improved from 0.19711 to 0.19281, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.0429 - acc: 1.0000 - val_loss: 0.1928 - val_acc: 0.9000\n",
            "Epoch 203/300\n",
            "\n",
            "Epoch 00203: val_loss did not improve from 0.19281\n",
            "60/60 - 0s - loss: 0.0585 - acc: 1.0000 - val_loss: 0.1974 - val_acc: 0.9000\n",
            "Epoch 204/300\n",
            "\n",
            "Epoch 00204: val_loss did not improve from 0.19281\n",
            "60/60 - 0s - loss: 0.0507 - acc: 1.0000 - val_loss: 0.2026 - val_acc: 0.9000\n",
            "Epoch 205/300\n",
            "\n",
            "Epoch 00205: val_loss did not improve from 0.19281\n",
            "60/60 - 0s - loss: 0.0412 - acc: 1.0000 - val_loss: 0.2064 - val_acc: 0.9000\n",
            "Epoch 206/300\n",
            "\n",
            "Epoch 00206: val_loss did not improve from 0.19281\n",
            "60/60 - 0s - loss: 0.0395 - acc: 1.0000 - val_loss: 0.2104 - val_acc: 0.9000\n",
            "Epoch 207/300\n",
            "\n",
            "Epoch 00207: val_loss did not improve from 0.19281\n",
            "60/60 - 0s - loss: 0.0602 - acc: 1.0000 - val_loss: 0.2067 - val_acc: 0.9000\n",
            "Epoch 208/300\n",
            "\n",
            "Epoch 00208: val_loss did not improve from 0.19281\n",
            "60/60 - 0s - loss: 0.0299 - acc: 1.0000 - val_loss: 0.2110 - val_acc: 0.9000\n",
            "Epoch 209/300\n",
            "\n",
            "Epoch 00209: val_loss did not improve from 0.19281\n",
            "60/60 - 0s - loss: 0.0588 - acc: 1.0000 - val_loss: 0.2134 - val_acc: 0.9000\n",
            "Epoch 210/300\n",
            "\n",
            "Epoch 00210: val_loss did not improve from 0.19281\n",
            "60/60 - 0s - loss: 0.0454 - acc: 0.9833 - val_loss: 0.2126 - val_acc: 0.9000\n",
            "Epoch 211/300\n",
            "\n",
            "Epoch 00211: val_loss did not improve from 0.19281\n",
            "60/60 - 0s - loss: 0.0734 - acc: 0.9833 - val_loss: 0.1992 - val_acc: 0.9000\n",
            "Epoch 212/300\n",
            "\n",
            "Epoch 00212: val_loss improved from 0.19281 to 0.19033, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.0503 - acc: 0.9833 - val_loss: 0.1903 - val_acc: 0.9000\n",
            "Epoch 213/300\n",
            "\n",
            "Epoch 00213: val_loss improved from 0.19033 to 0.18967, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.0726 - acc: 0.9667 - val_loss: 0.1897 - val_acc: 0.9000\n",
            "Epoch 214/300\n",
            "\n",
            "Epoch 00214: val_loss improved from 0.18967 to 0.18726, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.0763 - acc: 0.9833 - val_loss: 0.1873 - val_acc: 0.9000\n",
            "Epoch 215/300\n",
            "\n",
            "Epoch 00215: val_loss did not improve from 0.18726\n",
            "60/60 - 0s - loss: 0.0522 - acc: 0.9833 - val_loss: 0.1897 - val_acc: 0.9000\n",
            "Epoch 216/300\n",
            "\n",
            "Epoch 00216: val_loss did not improve from 0.18726\n",
            "60/60 - 0s - loss: 0.0663 - acc: 0.9833 - val_loss: 0.1895 - val_acc: 0.9000\n",
            "Epoch 217/300\n",
            "\n",
            "Epoch 00217: val_loss improved from 0.18726 to 0.18355, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.0619 - acc: 1.0000 - val_loss: 0.1836 - val_acc: 0.9000\n",
            "Epoch 218/300\n",
            "\n",
            "Epoch 00218: val_loss improved from 0.18355 to 0.18003, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.0608 - acc: 1.0000 - val_loss: 0.1800 - val_acc: 0.9000\n",
            "Epoch 219/300\n",
            "\n",
            "Epoch 00219: val_loss did not improve from 0.18003\n",
            "60/60 - 0s - loss: 0.0411 - acc: 1.0000 - val_loss: 0.1859 - val_acc: 0.9000\n",
            "Epoch 220/300\n",
            "\n",
            "Epoch 00220: val_loss did not improve from 0.18003\n",
            "60/60 - 0s - loss: 0.0680 - acc: 1.0000 - val_loss: 0.2024 - val_acc: 0.9000\n",
            "Epoch 221/300\n",
            "\n",
            "Epoch 00221: val_loss did not improve from 0.18003\n",
            "60/60 - 0s - loss: 0.0504 - acc: 0.9833 - val_loss: 0.2191 - val_acc: 0.9000\n",
            "Epoch 222/300\n",
            "\n",
            "Epoch 00222: val_loss did not improve from 0.18003\n",
            "60/60 - 0s - loss: 0.0417 - acc: 0.9833 - val_loss: 0.2249 - val_acc: 0.9000\n",
            "Epoch 223/300\n",
            "\n",
            "Epoch 00223: val_loss did not improve from 0.18003\n",
            "60/60 - 0s - loss: 0.0269 - acc: 1.0000 - val_loss: 0.2291 - val_acc: 0.9000\n",
            "Epoch 224/300\n",
            "\n",
            "Epoch 00224: val_loss did not improve from 0.18003\n",
            "60/60 - 0s - loss: 0.0533 - acc: 1.0000 - val_loss: 0.2278 - val_acc: 0.9000\n",
            "Epoch 225/300\n",
            "\n",
            "Epoch 00225: val_loss did not improve from 0.18003\n",
            "60/60 - 0s - loss: 0.0576 - acc: 0.9833 - val_loss: 0.2174 - val_acc: 0.9000\n",
            "Epoch 226/300\n",
            "\n",
            "Epoch 00226: val_loss did not improve from 0.18003\n",
            "60/60 - 0s - loss: 0.0398 - acc: 1.0000 - val_loss: 0.2136 - val_acc: 0.9000\n",
            "Epoch 227/300\n",
            "\n",
            "Epoch 00227: val_loss did not improve from 0.18003\n",
            "60/60 - 0s - loss: 0.0460 - acc: 0.9833 - val_loss: 0.2086 - val_acc: 0.9000\n",
            "Epoch 228/300\n",
            "\n",
            "Epoch 00228: val_loss did not improve from 0.18003\n",
            "60/60 - 0s - loss: 0.0532 - acc: 0.9833 - val_loss: 0.2012 - val_acc: 0.9000\n",
            "Epoch 229/300\n",
            "\n",
            "Epoch 00229: val_loss did not improve from 0.18003\n",
            "60/60 - 0s - loss: 0.0445 - acc: 0.9833 - val_loss: 0.1938 - val_acc: 0.9000\n",
            "Epoch 230/300\n",
            "\n",
            "Epoch 00230: val_loss did not improve from 0.18003\n",
            "60/60 - 0s - loss: 0.0453 - acc: 0.9833 - val_loss: 0.1877 - val_acc: 0.9000\n",
            "Epoch 231/300\n",
            "\n",
            "Epoch 00231: val_loss did not improve from 0.18003\n",
            "60/60 - 0s - loss: 0.0410 - acc: 1.0000 - val_loss: 0.1905 - val_acc: 0.9000\n",
            "Epoch 232/300\n",
            "\n",
            "Epoch 00232: val_loss did not improve from 0.18003\n",
            "60/60 - 0s - loss: 0.0293 - acc: 1.0000 - val_loss: 0.1940 - val_acc: 0.9000\n",
            "Epoch 233/300\n",
            "\n",
            "Epoch 00233: val_loss did not improve from 0.18003\n",
            "60/60 - 0s - loss: 0.0353 - acc: 1.0000 - val_loss: 0.1929 - val_acc: 0.9000\n",
            "Epoch 234/300\n",
            "\n",
            "Epoch 00234: val_loss did not improve from 0.18003\n",
            "60/60 - 0s - loss: 0.0289 - acc: 1.0000 - val_loss: 0.1980 - val_acc: 0.9000\n",
            "Epoch 235/300\n",
            "\n",
            "Epoch 00235: val_loss did not improve from 0.18003\n",
            "60/60 - 0s - loss: 0.0711 - acc: 0.9833 - val_loss: 0.2046 - val_acc: 0.9000\n",
            "Epoch 236/300\n",
            "\n",
            "Epoch 00236: val_loss did not improve from 0.18003\n",
            "60/60 - 0s - loss: 0.0873 - acc: 0.9500 - val_loss: 0.1953 - val_acc: 0.9000\n",
            "Epoch 237/300\n",
            "\n",
            "Epoch 00237: val_loss improved from 0.18003 to 0.17598, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.0516 - acc: 0.9833 - val_loss: 0.1760 - val_acc: 0.9500\n",
            "Epoch 238/300\n",
            "\n",
            "Epoch 00238: val_loss improved from 0.17598 to 0.16950, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.0240 - acc: 1.0000 - val_loss: 0.1695 - val_acc: 0.9500\n",
            "Epoch 239/300\n",
            "\n",
            "Epoch 00239: val_loss improved from 0.16950 to 0.16736, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.0626 - acc: 0.9833 - val_loss: 0.1674 - val_acc: 0.9500\n",
            "Epoch 240/300\n",
            "\n",
            "Epoch 00240: val_loss improved from 0.16736 to 0.16382, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.0378 - acc: 1.0000 - val_loss: 0.1638 - val_acc: 0.9500\n",
            "Epoch 241/300\n",
            "\n",
            "Epoch 00241: val_loss improved from 0.16382 to 0.15962, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.0484 - acc: 1.0000 - val_loss: 0.1596 - val_acc: 0.9500\n",
            "Epoch 242/300\n",
            "\n",
            "Epoch 00242: val_loss did not improve from 0.15962\n",
            "60/60 - 0s - loss: 0.0466 - acc: 0.9833 - val_loss: 0.1598 - val_acc: 0.9500\n",
            "Epoch 243/300\n",
            "\n",
            "Epoch 00243: val_loss did not improve from 0.15962\n",
            "60/60 - 0s - loss: 0.0387 - acc: 1.0000 - val_loss: 0.1667 - val_acc: 0.9500\n",
            "Epoch 244/300\n",
            "\n",
            "Epoch 00244: val_loss did not improve from 0.15962\n",
            "60/60 - 0s - loss: 0.0596 - acc: 0.9833 - val_loss: 0.1831 - val_acc: 0.9000\n",
            "Epoch 245/300\n",
            "\n",
            "Epoch 00245: val_loss did not improve from 0.15962\n",
            "60/60 - 0s - loss: 0.0348 - acc: 1.0000 - val_loss: 0.2012 - val_acc: 0.9000\n",
            "Epoch 246/300\n",
            "\n",
            "Epoch 00246: val_loss did not improve from 0.15962\n",
            "60/60 - 0s - loss: 0.0330 - acc: 1.0000 - val_loss: 0.2102 - val_acc: 0.9000\n",
            "Epoch 247/300\n",
            "\n",
            "Epoch 00247: val_loss did not improve from 0.15962\n",
            "60/60 - 0s - loss: 0.0406 - acc: 1.0000 - val_loss: 0.2263 - val_acc: 0.9000\n",
            "Epoch 248/300\n",
            "\n",
            "Epoch 00248: val_loss did not improve from 0.15962\n",
            "60/60 - 0s - loss: 0.0620 - acc: 1.0000 - val_loss: 0.2313 - val_acc: 0.9000\n",
            "Epoch 249/300\n",
            "\n",
            "Epoch 00249: val_loss did not improve from 0.15962\n",
            "60/60 - 0s - loss: 0.0364 - acc: 1.0000 - val_loss: 0.2388 - val_acc: 0.9000\n",
            "Epoch 250/300\n",
            "\n",
            "Epoch 00250: val_loss did not improve from 0.15962\n",
            "60/60 - 0s - loss: 0.0254 - acc: 1.0000 - val_loss: 0.2459 - val_acc: 0.9000\n",
            "Epoch 251/300\n",
            "\n",
            "Epoch 00251: val_loss did not improve from 0.15962\n",
            "60/60 - 0s - loss: 0.0411 - acc: 1.0000 - val_loss: 0.2477 - val_acc: 0.9000\n",
            "Epoch 252/300\n",
            "\n",
            "Epoch 00252: val_loss did not improve from 0.15962\n",
            "60/60 - 0s - loss: 0.0302 - acc: 1.0000 - val_loss: 0.2610 - val_acc: 0.9000\n",
            "Epoch 253/300\n",
            "\n",
            "Epoch 00253: val_loss did not improve from 0.15962\n",
            "60/60 - 0s - loss: 0.0424 - acc: 1.0000 - val_loss: 0.2782 - val_acc: 0.9000\n",
            "Epoch 254/300\n",
            "\n",
            "Epoch 00254: val_loss did not improve from 0.15962\n",
            "60/60 - 0s - loss: 0.0474 - acc: 1.0000 - val_loss: 0.2643 - val_acc: 0.9000\n",
            "Epoch 255/300\n",
            "\n",
            "Epoch 00255: val_loss did not improve from 0.15962\n",
            "60/60 - 0s - loss: 0.0227 - acc: 1.0000 - val_loss: 0.2383 - val_acc: 0.9000\n",
            "Epoch 256/300\n",
            "\n",
            "Epoch 00256: val_loss did not improve from 0.15962\n",
            "60/60 - 0s - loss: 0.0479 - acc: 1.0000 - val_loss: 0.2365 - val_acc: 0.9000\n",
            "Epoch 257/300\n",
            "\n",
            "Epoch 00257: val_loss did not improve from 0.15962\n",
            "60/60 - 0s - loss: 0.0266 - acc: 1.0000 - val_loss: 0.2386 - val_acc: 0.9000\n",
            "Epoch 258/300\n",
            "\n",
            "Epoch 00258: val_loss did not improve from 0.15962\n",
            "60/60 - 0s - loss: 0.0468 - acc: 1.0000 - val_loss: 0.2444 - val_acc: 0.9000\n",
            "Epoch 259/300\n",
            "\n",
            "Epoch 00259: val_loss did not improve from 0.15962\n",
            "60/60 - 0s - loss: 0.0312 - acc: 1.0000 - val_loss: 0.2652 - val_acc: 0.9000\n",
            "Epoch 260/300\n",
            "\n",
            "Epoch 00260: val_loss did not improve from 0.15962\n",
            "60/60 - 0s - loss: 0.0549 - acc: 1.0000 - val_loss: 0.2891 - val_acc: 0.9000\n",
            "Epoch 261/300\n",
            "\n",
            "Epoch 00261: val_loss did not improve from 0.15962\n",
            "60/60 - 0s - loss: 0.0329 - acc: 1.0000 - val_loss: 0.2886 - val_acc: 0.9000\n",
            "Epoch 262/300\n",
            "\n",
            "Epoch 00262: val_loss did not improve from 0.15962\n",
            "60/60 - 0s - loss: 0.0222 - acc: 1.0000 - val_loss: 0.2763 - val_acc: 0.9000\n",
            "Epoch 263/300\n",
            "\n",
            "Epoch 00263: val_loss did not improve from 0.15962\n",
            "60/60 - 0s - loss: 0.0781 - acc: 0.9833 - val_loss: 0.2644 - val_acc: 0.9000\n",
            "Epoch 264/300\n",
            "\n",
            "Epoch 00264: val_loss did not improve from 0.15962\n",
            "60/60 - 0s - loss: 0.0700 - acc: 0.9667 - val_loss: 0.2299 - val_acc: 0.9000\n",
            "Epoch 265/300\n",
            "\n",
            "Epoch 00265: val_loss did not improve from 0.15962\n",
            "60/60 - 0s - loss: 0.0470 - acc: 1.0000 - val_loss: 0.1918 - val_acc: 0.9000\n",
            "Epoch 266/300\n",
            "\n",
            "Epoch 00266: val_loss did not improve from 0.15962\n",
            "60/60 - 0s - loss: 0.0331 - acc: 1.0000 - val_loss: 0.1831 - val_acc: 0.9000\n",
            "Epoch 267/300\n",
            "\n",
            "Epoch 00267: val_loss did not improve from 0.15962\n",
            "60/60 - 0s - loss: 0.0931 - acc: 0.9500 - val_loss: 0.1925 - val_acc: 0.9000\n",
            "Epoch 268/300\n",
            "\n",
            "Epoch 00268: val_loss did not improve from 0.15962\n",
            "60/60 - 0s - loss: 0.0325 - acc: 1.0000 - val_loss: 0.2138 - val_acc: 0.9000\n",
            "Epoch 269/300\n",
            "\n",
            "Epoch 00269: val_loss did not improve from 0.15962\n",
            "60/60 - 0s - loss: 0.0390 - acc: 1.0000 - val_loss: 0.2199 - val_acc: 0.9000\n",
            "Epoch 270/300\n",
            "\n",
            "Epoch 00270: val_loss did not improve from 0.15962\n",
            "60/60 - 0s - loss: 0.0555 - acc: 1.0000 - val_loss: 0.2238 - val_acc: 0.9000\n",
            "Epoch 271/300\n",
            "\n",
            "Epoch 00271: val_loss did not improve from 0.15962\n",
            "60/60 - 0s - loss: 0.0280 - acc: 1.0000 - val_loss: 0.2427 - val_acc: 0.9000\n",
            "Epoch 272/300\n",
            "\n",
            "Epoch 00272: val_loss did not improve from 0.15962\n",
            "60/60 - 0s - loss: 0.0396 - acc: 1.0000 - val_loss: 0.2529 - val_acc: 0.9000\n",
            "Epoch 273/300\n",
            "\n",
            "Epoch 00273: val_loss did not improve from 0.15962\n",
            "60/60 - 0s - loss: 0.0573 - acc: 1.0000 - val_loss: 0.2677 - val_acc: 0.9000\n",
            "Epoch 274/300\n",
            "\n",
            "Epoch 00274: val_loss did not improve from 0.15962\n",
            "60/60 - 0s - loss: 0.0251 - acc: 1.0000 - val_loss: 0.2763 - val_acc: 0.9000\n",
            "Epoch 275/300\n",
            "\n",
            "Epoch 00275: val_loss did not improve from 0.15962\n",
            "60/60 - 0s - loss: 0.0256 - acc: 1.0000 - val_loss: 0.2816 - val_acc: 0.9000\n",
            "Epoch 276/300\n",
            "\n",
            "Epoch 00276: val_loss did not improve from 0.15962\n",
            "60/60 - 0s - loss: 0.0263 - acc: 1.0000 - val_loss: 0.2790 - val_acc: 0.9000\n",
            "Epoch 277/300\n",
            "\n",
            "Epoch 00277: val_loss did not improve from 0.15962\n",
            "60/60 - 0s - loss: 0.0276 - acc: 1.0000 - val_loss: 0.2853 - val_acc: 0.9000\n",
            "Epoch 278/300\n",
            "\n",
            "Epoch 00278: val_loss did not improve from 0.15962\n",
            "60/60 - 0s - loss: 0.0193 - acc: 1.0000 - val_loss: 0.2931 - val_acc: 0.9000\n",
            "Epoch 279/300\n",
            "\n",
            "Epoch 00279: val_loss did not improve from 0.15962\n",
            "60/60 - 0s - loss: 0.0413 - acc: 0.9833 - val_loss: 0.2872 - val_acc: 0.9000\n",
            "Epoch 280/300\n",
            "\n",
            "Epoch 00280: val_loss did not improve from 0.15962\n",
            "60/60 - 0s - loss: 0.0221 - acc: 1.0000 - val_loss: 0.2703 - val_acc: 0.9000\n",
            "Epoch 281/300\n",
            "\n",
            "Epoch 00281: val_loss did not improve from 0.15962\n",
            "60/60 - 0s - loss: 0.0295 - acc: 1.0000 - val_loss: 0.2621 - val_acc: 0.9000\n",
            "Epoch 282/300\n",
            "\n",
            "Epoch 00282: val_loss did not improve from 0.15962\n",
            "60/60 - 0s - loss: 0.0415 - acc: 0.9833 - val_loss: 0.2525 - val_acc: 0.9000\n",
            "Epoch 283/300\n",
            "\n",
            "Epoch 00283: val_loss did not improve from 0.15962\n",
            "60/60 - 0s - loss: 0.0169 - acc: 1.0000 - val_loss: 0.2489 - val_acc: 0.9000\n",
            "Epoch 284/300\n",
            "\n",
            "Epoch 00284: val_loss did not improve from 0.15962\n",
            "60/60 - 0s - loss: 0.0410 - acc: 1.0000 - val_loss: 0.2326 - val_acc: 0.9000\n",
            "Epoch 285/300\n",
            "\n",
            "Epoch 00285: val_loss did not improve from 0.15962\n",
            "60/60 - 0s - loss: 0.0436 - acc: 1.0000 - val_loss: 0.1983 - val_acc: 0.9000\n",
            "Epoch 286/300\n",
            "\n",
            "Epoch 00286: val_loss did not improve from 0.15962\n",
            "60/60 - 0s - loss: 0.0334 - acc: 1.0000 - val_loss: 0.1827 - val_acc: 0.9000\n",
            "Epoch 287/300\n",
            "\n",
            "Epoch 00287: val_loss did not improve from 0.15962\n",
            "60/60 - 0s - loss: 0.0713 - acc: 0.9833 - val_loss: 0.1671 - val_acc: 0.9000\n",
            "Epoch 288/300\n",
            "\n",
            "Epoch 00288: val_loss did not improve from 0.15962\n",
            "60/60 - 0s - loss: 0.0244 - acc: 1.0000 - val_loss: 0.1604 - val_acc: 0.9000\n",
            "Epoch 289/300\n",
            "\n",
            "Epoch 00289: val_loss did not improve from 0.15962\n",
            "60/60 - 0s - loss: 0.0358 - acc: 1.0000 - val_loss: 0.1602 - val_acc: 0.9000\n",
            "Epoch 290/300\n",
            "\n",
            "Epoch 00290: val_loss did not improve from 0.15962\n",
            "60/60 - 0s - loss: 0.0488 - acc: 0.9833 - val_loss: 0.1692 - val_acc: 0.9000\n",
            "Epoch 291/300\n",
            "\n",
            "Epoch 00291: val_loss did not improve from 0.15962\n",
            "60/60 - 0s - loss: 0.0290 - acc: 1.0000 - val_loss: 0.1848 - val_acc: 0.9000\n",
            "Epoch 292/300\n",
            "\n",
            "Epoch 00292: val_loss did not improve from 0.15962\n",
            "60/60 - 0s - loss: 0.0290 - acc: 1.0000 - val_loss: 0.1978 - val_acc: 0.9000\n",
            "Epoch 293/300\n",
            "\n",
            "Epoch 00293: val_loss did not improve from 0.15962\n",
            "60/60 - 0s - loss: 0.0527 - acc: 1.0000 - val_loss: 0.2095 - val_acc: 0.9000\n",
            "Epoch 294/300\n",
            "\n",
            "Epoch 00294: val_loss did not improve from 0.15962\n",
            "60/60 - 0s - loss: 0.0192 - acc: 1.0000 - val_loss: 0.2059 - val_acc: 0.9000\n",
            "Epoch 295/300\n",
            "\n",
            "Epoch 00295: val_loss did not improve from 0.15962\n",
            "60/60 - 0s - loss: 0.0399 - acc: 1.0000 - val_loss: 0.2110 - val_acc: 0.9000\n",
            "Epoch 296/300\n",
            "\n",
            "Epoch 00296: val_loss did not improve from 0.15962\n",
            "60/60 - 0s - loss: 0.0243 - acc: 1.0000 - val_loss: 0.2385 - val_acc: 0.9000\n",
            "Epoch 297/300\n",
            "\n",
            "Epoch 00297: val_loss did not improve from 0.15962\n",
            "60/60 - 0s - loss: 0.0289 - acc: 1.0000 - val_loss: 0.2560 - val_acc: 0.9000\n",
            "Epoch 298/300\n",
            "\n",
            "Epoch 00298: val_loss did not improve from 0.15962\n",
            "60/60 - 0s - loss: 0.0703 - acc: 0.9500 - val_loss: 0.2479 - val_acc: 0.9000\n",
            "Epoch 299/300\n",
            "\n",
            "Epoch 00299: val_loss did not improve from 0.15962\n",
            "60/60 - 0s - loss: 0.0276 - acc: 1.0000 - val_loss: 0.2498 - val_acc: 0.9000\n",
            "Epoch 300/300\n",
            "\n",
            "Epoch 00300: val_loss did not improve from 0.15962\n",
            "60/60 - 0s - loss: 0.0342 - acc: 1.0000 - val_loss: 0.2493 - val_acc: 0.9000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "# Evaluate on test data\n",
            "40/40 [==============================] - 0s 2ms/sample - loss: 0.8943 - acc: 0.6500\n",
            "test loss, test acc: [0.894308906653896, 0.65]\n",
            "EEG_Deep/Data2A/parsed_P07T.mat\n",
            "Filtering of Training Data Finished\n",
            "EEG_Deep/Data2A/parsed_P07E.mat\n",
            "Filtering of Testing Data Finished\n",
            "(1, 2)\n",
            "Finding labels in training data\n",
            "Finding labels in testing data\n",
            "[2 1 2 2 2 1 1 1 2 2 1 2 2 2 1 1 1 1 2 2]\n",
            "(60, 12, 1536)\n",
            "(60,)\n",
            "(20, 12, 1536)\n",
            "(20,)\n",
            "[[0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]]\n",
            "X_train shape: (60, 1, 12, 1536)\n",
            "60 train samples\n",
            "20 val samples\n",
            "(40, 12, 1536)\n",
            "(40, 1)\n",
            "X_train shape: (40, 1, 12, 1536)\n",
            "40 train samples\n",
            "Train on 60 samples, validate on 20 samples\n",
            "Epoch 1/300\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.69143, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 1s - loss: 0.7164 - acc: 0.4167 - val_loss: 0.6914 - val_acc: 0.6000\n",
            "Epoch 2/300\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.69143\n",
            "60/60 - 0s - loss: 0.6907 - acc: 0.4667 - val_loss: 0.6918 - val_acc: 0.6000\n",
            "Epoch 3/300\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.69143\n",
            "60/60 - 0s - loss: 0.6793 - acc: 0.6000 - val_loss: 0.6915 - val_acc: 0.5500\n",
            "Epoch 4/300\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.69143\n",
            "60/60 - 0s - loss: 0.6568 - acc: 0.6833 - val_loss: 0.6922 - val_acc: 0.5500\n",
            "Epoch 5/300\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.69143\n",
            "60/60 - 0s - loss: 0.6569 - acc: 0.5833 - val_loss: 0.6927 - val_acc: 0.5500\n",
            "Epoch 6/300\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.69143\n",
            "60/60 - 0s - loss: 0.6324 - acc: 0.7833 - val_loss: 0.6936 - val_acc: 0.5500\n",
            "Epoch 7/300\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.69143\n",
            "60/60 - 0s - loss: 0.6364 - acc: 0.7500 - val_loss: 0.6948 - val_acc: 0.5000\n",
            "Epoch 8/300\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.69143\n",
            "60/60 - 0s - loss: 0.6122 - acc: 0.8000 - val_loss: 0.6952 - val_acc: 0.4500\n",
            "Epoch 9/300\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.69143\n",
            "60/60 - 0s - loss: 0.6191 - acc: 0.7000 - val_loss: 0.6954 - val_acc: 0.3500\n",
            "Epoch 10/300\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.69143\n",
            "60/60 - 0s - loss: 0.5791 - acc: 0.8333 - val_loss: 0.6960 - val_acc: 0.4500\n",
            "Epoch 11/300\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.69143\n",
            "60/60 - 0s - loss: 0.5872 - acc: 0.7000 - val_loss: 0.6967 - val_acc: 0.4500\n",
            "Epoch 12/300\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.69143\n",
            "60/60 - 0s - loss: 0.5669 - acc: 0.7833 - val_loss: 0.6978 - val_acc: 0.4500\n",
            "Epoch 13/300\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.69143\n",
            "60/60 - 0s - loss: 0.5693 - acc: 0.7833 - val_loss: 0.6981 - val_acc: 0.4500\n",
            "Epoch 14/300\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.69143\n",
            "60/60 - 0s - loss: 0.5649 - acc: 0.7833 - val_loss: 0.6988 - val_acc: 0.4500\n",
            "Epoch 15/300\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.69143\n",
            "60/60 - 0s - loss: 0.5527 - acc: 0.7500 - val_loss: 0.6992 - val_acc: 0.4500\n",
            "Epoch 16/300\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.69143\n",
            "60/60 - 0s - loss: 0.5289 - acc: 0.7667 - val_loss: 0.6998 - val_acc: 0.4500\n",
            "Epoch 17/300\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.69143\n",
            "60/60 - 0s - loss: 0.5357 - acc: 0.8500 - val_loss: 0.7015 - val_acc: 0.5000\n",
            "Epoch 18/300\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.69143\n",
            "60/60 - 0s - loss: 0.5012 - acc: 0.8500 - val_loss: 0.7021 - val_acc: 0.5000\n",
            "Epoch 19/300\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.69143\n",
            "60/60 - 0s - loss: 0.5234 - acc: 0.7833 - val_loss: 0.7038 - val_acc: 0.4500\n",
            "Epoch 20/300\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.69143\n",
            "60/60 - 0s - loss: 0.4997 - acc: 0.8000 - val_loss: 0.7058 - val_acc: 0.4500\n",
            "Epoch 21/300\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.69143\n",
            "60/60 - 0s - loss: 0.5139 - acc: 0.8167 - val_loss: 0.7058 - val_acc: 0.4500\n",
            "Epoch 22/300\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.69143\n",
            "60/60 - 0s - loss: 0.5118 - acc: 0.7667 - val_loss: 0.7081 - val_acc: 0.4500\n",
            "Epoch 23/300\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.69143\n",
            "60/60 - 0s - loss: 0.4886 - acc: 0.8500 - val_loss: 0.7100 - val_acc: 0.4000\n",
            "Epoch 24/300\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.69143\n",
            "60/60 - 0s - loss: 0.4542 - acc: 0.8667 - val_loss: 0.7115 - val_acc: 0.4000\n",
            "Epoch 25/300\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.69143\n",
            "60/60 - 0s - loss: 0.4773 - acc: 0.8167 - val_loss: 0.7127 - val_acc: 0.4500\n",
            "Epoch 26/300\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.69143\n",
            "60/60 - 0s - loss: 0.4683 - acc: 0.8333 - val_loss: 0.7147 - val_acc: 0.4500\n",
            "Epoch 27/300\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.69143\n",
            "60/60 - 0s - loss: 0.4915 - acc: 0.8333 - val_loss: 0.7173 - val_acc: 0.4000\n",
            "Epoch 28/300\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.69143\n",
            "60/60 - 0s - loss: 0.4716 - acc: 0.8667 - val_loss: 0.7200 - val_acc: 0.4500\n",
            "Epoch 29/300\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.69143\n",
            "60/60 - 0s - loss: 0.4593 - acc: 0.8333 - val_loss: 0.7209 - val_acc: 0.4500\n",
            "Epoch 30/300\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.69143\n",
            "60/60 - 0s - loss: 0.4275 - acc: 0.9000 - val_loss: 0.7202 - val_acc: 0.4500\n",
            "Epoch 31/300\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.69143\n",
            "60/60 - 0s - loss: 0.4308 - acc: 0.8167 - val_loss: 0.7215 - val_acc: 0.4500\n",
            "Epoch 32/300\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.69143\n",
            "60/60 - 0s - loss: 0.4557 - acc: 0.8167 - val_loss: 0.7239 - val_acc: 0.4500\n",
            "Epoch 33/300\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.69143\n",
            "60/60 - 0s - loss: 0.4406 - acc: 0.8333 - val_loss: 0.7235 - val_acc: 0.4500\n",
            "Epoch 34/300\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.69143\n",
            "60/60 - 0s - loss: 0.4153 - acc: 0.8333 - val_loss: 0.7239 - val_acc: 0.4500\n",
            "Epoch 35/300\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.69143\n",
            "60/60 - 0s - loss: 0.4251 - acc: 0.7833 - val_loss: 0.7234 - val_acc: 0.5000\n",
            "Epoch 36/300\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.69143\n",
            "60/60 - 0s - loss: 0.4500 - acc: 0.8167 - val_loss: 0.7233 - val_acc: 0.5000\n",
            "Epoch 37/300\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.69143\n",
            "60/60 - 0s - loss: 0.4217 - acc: 0.9333 - val_loss: 0.7217 - val_acc: 0.5000\n",
            "Epoch 38/300\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.69143\n",
            "60/60 - 0s - loss: 0.4326 - acc: 0.8500 - val_loss: 0.7220 - val_acc: 0.5000\n",
            "Epoch 39/300\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.69143\n",
            "60/60 - 0s - loss: 0.4056 - acc: 0.9000 - val_loss: 0.7212 - val_acc: 0.4500\n",
            "Epoch 40/300\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.69143\n",
            "60/60 - 0s - loss: 0.4200 - acc: 0.8500 - val_loss: 0.7201 - val_acc: 0.4500\n",
            "Epoch 41/300\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.69143\n",
            "60/60 - 0s - loss: 0.4050 - acc: 0.8333 - val_loss: 0.7170 - val_acc: 0.4500\n",
            "Epoch 42/300\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.69143\n",
            "60/60 - 0s - loss: 0.4102 - acc: 0.8667 - val_loss: 0.7145 - val_acc: 0.4500\n",
            "Epoch 43/300\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.69143\n",
            "60/60 - 0s - loss: 0.4208 - acc: 0.9000 - val_loss: 0.7103 - val_acc: 0.5000\n",
            "Epoch 44/300\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.69143\n",
            "60/60 - 0s - loss: 0.4033 - acc: 0.9167 - val_loss: 0.7112 - val_acc: 0.5000\n",
            "Epoch 45/300\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.69143\n",
            "60/60 - 0s - loss: 0.3926 - acc: 0.8833 - val_loss: 0.7095 - val_acc: 0.5500\n",
            "Epoch 46/300\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.69143\n",
            "60/60 - 0s - loss: 0.3947 - acc: 0.9333 - val_loss: 0.7078 - val_acc: 0.6500\n",
            "Epoch 47/300\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.69143\n",
            "60/60 - 0s - loss: 0.3884 - acc: 0.9167 - val_loss: 0.7045 - val_acc: 0.7000\n",
            "Epoch 48/300\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.69143\n",
            "60/60 - 0s - loss: 0.3677 - acc: 0.9500 - val_loss: 0.7028 - val_acc: 0.7000\n",
            "Epoch 49/300\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.69143\n",
            "60/60 - 0s - loss: 0.3638 - acc: 0.9333 - val_loss: 0.7006 - val_acc: 0.7000\n",
            "Epoch 50/300\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.69143\n",
            "60/60 - 0s - loss: 0.3852 - acc: 0.8667 - val_loss: 0.7022 - val_acc: 0.7000\n",
            "Epoch 51/300\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.69143\n",
            "60/60 - 0s - loss: 0.3923 - acc: 0.8833 - val_loss: 0.7010 - val_acc: 0.7000\n",
            "Epoch 52/300\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.69143\n",
            "60/60 - 0s - loss: 0.3853 - acc: 0.8833 - val_loss: 0.6983 - val_acc: 0.7000\n",
            "Epoch 53/300\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.69143\n",
            "60/60 - 0s - loss: 0.3633 - acc: 0.9000 - val_loss: 0.6961 - val_acc: 0.7000\n",
            "Epoch 54/300\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.69143\n",
            "60/60 - 0s - loss: 0.3345 - acc: 0.9833 - val_loss: 0.6938 - val_acc: 0.7000\n",
            "Epoch 55/300\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.69143\n",
            "60/60 - 0s - loss: 0.3478 - acc: 0.9167 - val_loss: 0.6950 - val_acc: 0.7000\n",
            "Epoch 56/300\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.69143\n",
            "60/60 - 0s - loss: 0.3745 - acc: 0.9000 - val_loss: 0.6973 - val_acc: 0.6500\n",
            "Epoch 57/300\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.69143\n",
            "60/60 - 0s - loss: 0.3518 - acc: 0.9167 - val_loss: 0.6958 - val_acc: 0.6500\n",
            "Epoch 58/300\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.69143\n",
            "60/60 - 0s - loss: 0.3418 - acc: 0.9167 - val_loss: 0.6933 - val_acc: 0.6500\n",
            "Epoch 59/300\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.69143\n",
            "60/60 - 0s - loss: 0.3401 - acc: 0.9333 - val_loss: 0.6919 - val_acc: 0.6500\n",
            "Epoch 60/300\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.69143\n",
            "60/60 - 0s - loss: 0.3565 - acc: 0.9167 - val_loss: 0.6922 - val_acc: 0.6000\n",
            "Epoch 61/300\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.69143\n",
            "60/60 - 0s - loss: 0.3490 - acc: 0.9167 - val_loss: 0.6938 - val_acc: 0.6000\n",
            "Epoch 62/300\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.69143\n",
            "60/60 - 0s - loss: 0.3662 - acc: 0.9000 - val_loss: 0.6935 - val_acc: 0.6000\n",
            "Epoch 63/300\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.69143\n",
            "60/60 - 0s - loss: 0.3444 - acc: 0.9167 - val_loss: 0.6930 - val_acc: 0.6000\n",
            "Epoch 64/300\n",
            "\n",
            "Epoch 00064: val_loss improved from 0.69143 to 0.68924, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3355 - acc: 0.9167 - val_loss: 0.6892 - val_acc: 0.6000\n",
            "Epoch 65/300\n",
            "\n",
            "Epoch 00065: val_loss improved from 0.68924 to 0.68730, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3186 - acc: 0.9667 - val_loss: 0.6873 - val_acc: 0.6000\n",
            "Epoch 66/300\n",
            "\n",
            "Epoch 00066: val_loss improved from 0.68730 to 0.68702, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3398 - acc: 0.9667 - val_loss: 0.6870 - val_acc: 0.6000\n",
            "Epoch 67/300\n",
            "\n",
            "Epoch 00067: val_loss improved from 0.68702 to 0.68422, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3253 - acc: 0.9167 - val_loss: 0.6842 - val_acc: 0.6000\n",
            "Epoch 68/300\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.68422\n",
            "60/60 - 0s - loss: 0.3298 - acc: 0.9167 - val_loss: 0.6864 - val_acc: 0.6000\n",
            "Epoch 69/300\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.68422\n",
            "60/60 - 0s - loss: 0.3258 - acc: 0.9167 - val_loss: 0.6879 - val_acc: 0.6000\n",
            "Epoch 70/300\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.68422\n",
            "60/60 - 0s - loss: 0.3146 - acc: 0.8833 - val_loss: 0.6911 - val_acc: 0.6000\n",
            "Epoch 71/300\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.68422\n",
            "60/60 - 0s - loss: 0.3173 - acc: 0.9333 - val_loss: 0.6926 - val_acc: 0.6000\n",
            "Epoch 72/300\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.68422\n",
            "60/60 - 0s - loss: 0.2937 - acc: 0.9667 - val_loss: 0.6938 - val_acc: 0.6000\n",
            "Epoch 73/300\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.68422\n",
            "60/60 - 0s - loss: 0.3535 - acc: 0.9000 - val_loss: 0.6895 - val_acc: 0.6000\n",
            "Epoch 74/300\n",
            "\n",
            "Epoch 00074: val_loss improved from 0.68422 to 0.68371, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3118 - acc: 0.9333 - val_loss: 0.6837 - val_acc: 0.6000\n",
            "Epoch 75/300\n",
            "\n",
            "Epoch 00075: val_loss improved from 0.68371 to 0.67882, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3297 - acc: 0.9500 - val_loss: 0.6788 - val_acc: 0.6000\n",
            "Epoch 76/300\n",
            "\n",
            "Epoch 00076: val_loss improved from 0.67882 to 0.67351, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3058 - acc: 0.9167 - val_loss: 0.6735 - val_acc: 0.6000\n",
            "Epoch 77/300\n",
            "\n",
            "Epoch 00077: val_loss improved from 0.67351 to 0.67274, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3052 - acc: 0.9500 - val_loss: 0.6727 - val_acc: 0.6000\n",
            "Epoch 78/300\n",
            "\n",
            "Epoch 00078: val_loss improved from 0.67274 to 0.66977, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3014 - acc: 0.9000 - val_loss: 0.6698 - val_acc: 0.6000\n",
            "Epoch 79/300\n",
            "\n",
            "Epoch 00079: val_loss improved from 0.66977 to 0.66884, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2930 - acc: 0.9833 - val_loss: 0.6688 - val_acc: 0.6500\n",
            "Epoch 80/300\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.66884\n",
            "60/60 - 0s - loss: 0.3165 - acc: 0.9667 - val_loss: 0.6731 - val_acc: 0.6000\n",
            "Epoch 81/300\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.66884\n",
            "60/60 - 0s - loss: 0.3114 - acc: 0.9500 - val_loss: 0.6727 - val_acc: 0.6000\n",
            "Epoch 82/300\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.66884\n",
            "60/60 - 0s - loss: 0.2999 - acc: 0.9667 - val_loss: 0.6747 - val_acc: 0.6000\n",
            "Epoch 83/300\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.66884\n",
            "60/60 - 0s - loss: 0.3005 - acc: 0.9833 - val_loss: 0.6699 - val_acc: 0.6000\n",
            "Epoch 84/300\n",
            "\n",
            "Epoch 00084: val_loss improved from 0.66884 to 0.66536, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2822 - acc: 0.9667 - val_loss: 0.6654 - val_acc: 0.6000\n",
            "Epoch 85/300\n",
            "\n",
            "Epoch 00085: val_loss improved from 0.66536 to 0.66349, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3204 - acc: 0.9167 - val_loss: 0.6635 - val_acc: 0.6000\n",
            "Epoch 86/300\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.66349\n",
            "60/60 - 0s - loss: 0.2626 - acc: 0.9833 - val_loss: 0.6644 - val_acc: 0.6000\n",
            "Epoch 87/300\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.66349\n",
            "60/60 - 0s - loss: 0.2924 - acc: 0.9500 - val_loss: 0.6657 - val_acc: 0.6000\n",
            "Epoch 88/300\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.66349\n",
            "60/60 - 0s - loss: 0.3110 - acc: 0.9333 - val_loss: 0.6644 - val_acc: 0.6000\n",
            "Epoch 89/300\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.66349\n",
            "60/60 - 0s - loss: 0.3160 - acc: 0.9333 - val_loss: 0.6677 - val_acc: 0.6000\n",
            "Epoch 90/300\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.66349\n",
            "60/60 - 0s - loss: 0.2552 - acc: 0.9667 - val_loss: 0.6710 - val_acc: 0.6000\n",
            "Epoch 91/300\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.66349\n",
            "60/60 - 0s - loss: 0.2702 - acc: 0.9833 - val_loss: 0.6749 - val_acc: 0.6000\n",
            "Epoch 92/300\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.66349\n",
            "60/60 - 0s - loss: 0.2920 - acc: 0.9833 - val_loss: 0.6806 - val_acc: 0.5500\n",
            "Epoch 93/300\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.66349\n",
            "60/60 - 0s - loss: 0.2579 - acc: 0.9833 - val_loss: 0.6819 - val_acc: 0.5000\n",
            "Epoch 94/300\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.66349\n",
            "60/60 - 0s - loss: 0.2695 - acc: 0.9833 - val_loss: 0.6796 - val_acc: 0.5500\n",
            "Epoch 95/300\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.66349\n",
            "60/60 - 0s - loss: 0.2636 - acc: 0.9333 - val_loss: 0.6719 - val_acc: 0.5000\n",
            "Epoch 96/300\n",
            "\n",
            "Epoch 00096: val_loss improved from 0.66349 to 0.66307, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2527 - acc: 0.9667 - val_loss: 0.6631 - val_acc: 0.5500\n",
            "Epoch 97/300\n",
            "\n",
            "Epoch 00097: val_loss improved from 0.66307 to 0.65948, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2263 - acc: 1.0000 - val_loss: 0.6595 - val_acc: 0.5500\n",
            "Epoch 98/300\n",
            "\n",
            "Epoch 00098: val_loss improved from 0.65948 to 0.65668, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2593 - acc: 0.9833 - val_loss: 0.6567 - val_acc: 0.5500\n",
            "Epoch 99/300\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.65668\n",
            "60/60 - 0s - loss: 0.2823 - acc: 0.9667 - val_loss: 0.6597 - val_acc: 0.5500\n",
            "Epoch 100/300\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.65668\n",
            "60/60 - 0s - loss: 0.2661 - acc: 0.9500 - val_loss: 0.6649 - val_acc: 0.5000\n",
            "Epoch 101/300\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.65668\n",
            "60/60 - 0s - loss: 0.2712 - acc: 0.9500 - val_loss: 0.6670 - val_acc: 0.5000\n",
            "Epoch 102/300\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.65668\n",
            "60/60 - 0s - loss: 0.2586 - acc: 0.9500 - val_loss: 0.6643 - val_acc: 0.5500\n",
            "Epoch 103/300\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.65668\n",
            "60/60 - 0s - loss: 0.2435 - acc: 0.9333 - val_loss: 0.6643 - val_acc: 0.5500\n",
            "Epoch 104/300\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.65668\n",
            "60/60 - 0s - loss: 0.2462 - acc: 0.9500 - val_loss: 0.6640 - val_acc: 0.5000\n",
            "Epoch 105/300\n",
            "\n",
            "Epoch 00105: val_loss improved from 0.65668 to 0.65562, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2696 - acc: 0.9333 - val_loss: 0.6556 - val_acc: 0.5000\n",
            "Epoch 106/300\n",
            "\n",
            "Epoch 00106: val_loss improved from 0.65562 to 0.64755, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2659 - acc: 0.9167 - val_loss: 0.6475 - val_acc: 0.6000\n",
            "Epoch 107/300\n",
            "\n",
            "Epoch 00107: val_loss improved from 0.64755 to 0.64347, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2450 - acc: 0.9667 - val_loss: 0.6435 - val_acc: 0.5500\n",
            "Epoch 108/300\n",
            "\n",
            "Epoch 00108: val_loss improved from 0.64347 to 0.64058, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2652 - acc: 0.9500 - val_loss: 0.6406 - val_acc: 0.5000\n",
            "Epoch 109/300\n",
            "\n",
            "Epoch 00109: val_loss improved from 0.64058 to 0.63864, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2378 - acc: 0.9833 - val_loss: 0.6386 - val_acc: 0.5000\n",
            "Epoch 110/300\n",
            "\n",
            "Epoch 00110: val_loss improved from 0.63864 to 0.63437, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2638 - acc: 0.9667 - val_loss: 0.6344 - val_acc: 0.5500\n",
            "Epoch 111/300\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 0.63437\n",
            "60/60 - 0s - loss: 0.2165 - acc: 0.9833 - val_loss: 0.6380 - val_acc: 0.5500\n",
            "Epoch 112/300\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 0.63437\n",
            "60/60 - 0s - loss: 0.2230 - acc: 0.9833 - val_loss: 0.6497 - val_acc: 0.6000\n",
            "Epoch 113/300\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.63437\n",
            "60/60 - 0s - loss: 0.2295 - acc: 0.9833 - val_loss: 0.6584 - val_acc: 0.6000\n",
            "Epoch 114/300\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.63437\n",
            "60/60 - 0s - loss: 0.2834 - acc: 0.9167 - val_loss: 0.6577 - val_acc: 0.6000\n",
            "Epoch 115/300\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.63437\n",
            "60/60 - 0s - loss: 0.2250 - acc: 0.9833 - val_loss: 0.6586 - val_acc: 0.6000\n",
            "Epoch 116/300\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.63437\n",
            "60/60 - 0s - loss: 0.2170 - acc: 0.9833 - val_loss: 0.6587 - val_acc: 0.5500\n",
            "Epoch 117/300\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.63437\n",
            "60/60 - 0s - loss: 0.2430 - acc: 0.9667 - val_loss: 0.6539 - val_acc: 0.6000\n",
            "Epoch 118/300\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.63437\n",
            "60/60 - 0s - loss: 0.2189 - acc: 1.0000 - val_loss: 0.6530 - val_acc: 0.5500\n",
            "Epoch 119/300\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 0.63437\n",
            "60/60 - 0s - loss: 0.2037 - acc: 0.9833 - val_loss: 0.6471 - val_acc: 0.6000\n",
            "Epoch 120/300\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.63437\n",
            "60/60 - 0s - loss: 0.2167 - acc: 0.9833 - val_loss: 0.6456 - val_acc: 0.6000\n",
            "Epoch 121/300\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.63437\n",
            "60/60 - 0s - loss: 0.2088 - acc: 0.9833 - val_loss: 0.6458 - val_acc: 0.6000\n",
            "Epoch 122/300\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.63437\n",
            "60/60 - 0s - loss: 0.2376 - acc: 0.9500 - val_loss: 0.6489 - val_acc: 0.6500\n",
            "Epoch 123/300\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 0.63437\n",
            "60/60 - 0s - loss: 0.1998 - acc: 0.9833 - val_loss: 0.6511 - val_acc: 0.6500\n",
            "Epoch 124/300\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 0.63437\n",
            "60/60 - 0s - loss: 0.2371 - acc: 0.9833 - val_loss: 0.6583 - val_acc: 0.6500\n",
            "Epoch 125/300\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 0.63437\n",
            "60/60 - 0s - loss: 0.2028 - acc: 0.9667 - val_loss: 0.6624 - val_acc: 0.6500\n",
            "Epoch 126/300\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 0.63437\n",
            "60/60 - 0s - loss: 0.1891 - acc: 1.0000 - val_loss: 0.6600 - val_acc: 0.6500\n",
            "Epoch 127/300\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 0.63437\n",
            "60/60 - 0s - loss: 0.2455 - acc: 0.9333 - val_loss: 0.6558 - val_acc: 0.6500\n",
            "Epoch 128/300\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 0.63437\n",
            "60/60 - 0s - loss: 0.2352 - acc: 0.9167 - val_loss: 0.6464 - val_acc: 0.7000\n",
            "Epoch 129/300\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.63437\n",
            "60/60 - 0s - loss: 0.2107 - acc: 0.9667 - val_loss: 0.6400 - val_acc: 0.7000\n",
            "Epoch 130/300\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 0.63437\n",
            "60/60 - 0s - loss: 0.2081 - acc: 0.9833 - val_loss: 0.6460 - val_acc: 0.6000\n",
            "Epoch 131/300\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 0.63437\n",
            "60/60 - 0s - loss: 0.2160 - acc: 0.9500 - val_loss: 0.6449 - val_acc: 0.6000\n",
            "Epoch 132/300\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 0.63437\n",
            "60/60 - 0s - loss: 0.2248 - acc: 0.9500 - val_loss: 0.6414 - val_acc: 0.6500\n",
            "Epoch 133/300\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.63437\n",
            "60/60 - 0s - loss: 0.2076 - acc: 0.9667 - val_loss: 0.6395 - val_acc: 0.7000\n",
            "Epoch 134/300\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.63437\n",
            "60/60 - 0s - loss: 0.2204 - acc: 0.9833 - val_loss: 0.6421 - val_acc: 0.6500\n",
            "Epoch 135/300\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 0.63437\n",
            "60/60 - 0s - loss: 0.2161 - acc: 0.9500 - val_loss: 0.6450 - val_acc: 0.6500\n",
            "Epoch 136/300\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.63437\n",
            "60/60 - 0s - loss: 0.1858 - acc: 0.9833 - val_loss: 0.6430 - val_acc: 0.6500\n",
            "Epoch 137/300\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 0.63437\n",
            "60/60 - 0s - loss: 0.2337 - acc: 0.9667 - val_loss: 0.6429 - val_acc: 0.6000\n",
            "Epoch 138/300\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.63437\n",
            "60/60 - 0s - loss: 0.1841 - acc: 0.9833 - val_loss: 0.6363 - val_acc: 0.6000\n",
            "Epoch 139/300\n",
            "\n",
            "Epoch 00139: val_loss improved from 0.63437 to 0.63034, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2425 - acc: 0.9500 - val_loss: 0.6303 - val_acc: 0.6500\n",
            "Epoch 140/300\n",
            "\n",
            "Epoch 00140: val_loss improved from 0.63034 to 0.62650, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2114 - acc: 0.9833 - val_loss: 0.6265 - val_acc: 0.6500\n",
            "Epoch 141/300\n",
            "\n",
            "Epoch 00141: val_loss improved from 0.62650 to 0.61941, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2219 - acc: 0.9500 - val_loss: 0.6194 - val_acc: 0.7000\n",
            "Epoch 142/300\n",
            "\n",
            "Epoch 00142: val_loss improved from 0.61941 to 0.61620, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1919 - acc: 0.9833 - val_loss: 0.6162 - val_acc: 0.7500\n",
            "Epoch 143/300\n",
            "\n",
            "Epoch 00143: val_loss improved from 0.61620 to 0.61055, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2092 - acc: 0.9667 - val_loss: 0.6106 - val_acc: 0.8000\n",
            "Epoch 144/300\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 0.61055\n",
            "60/60 - 0s - loss: 0.1974 - acc: 0.9667 - val_loss: 0.6141 - val_acc: 0.7500\n",
            "Epoch 145/300\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.61055\n",
            "60/60 - 0s - loss: 0.2178 - acc: 0.9500 - val_loss: 0.6148 - val_acc: 0.7000\n",
            "Epoch 146/300\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.61055\n",
            "60/60 - 0s - loss: 0.2167 - acc: 0.9833 - val_loss: 0.6166 - val_acc: 0.7000\n",
            "Epoch 147/300\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.61055\n",
            "60/60 - 0s - loss: 0.1953 - acc: 0.9833 - val_loss: 0.6204 - val_acc: 0.7500\n",
            "Epoch 148/300\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 0.61055\n",
            "60/60 - 0s - loss: 0.2132 - acc: 0.9667 - val_loss: 0.6217 - val_acc: 0.7500\n",
            "Epoch 149/300\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.61055\n",
            "60/60 - 0s - loss: 0.1839 - acc: 0.9833 - val_loss: 0.6238 - val_acc: 0.7000\n",
            "Epoch 150/300\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.61055\n",
            "60/60 - 0s - loss: 0.2103 - acc: 0.9833 - val_loss: 0.6198 - val_acc: 0.7000\n",
            "Epoch 151/300\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.61055\n",
            "60/60 - 0s - loss: 0.2165 - acc: 0.9667 - val_loss: 0.6171 - val_acc: 0.7500\n",
            "Epoch 152/300\n",
            "\n",
            "Epoch 00152: val_loss improved from 0.61055 to 0.60883, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2184 - acc: 0.9333 - val_loss: 0.6088 - val_acc: 0.7000\n",
            "Epoch 153/300\n",
            "\n",
            "Epoch 00153: val_loss improved from 0.60883 to 0.60379, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2171 - acc: 0.9333 - val_loss: 0.6038 - val_acc: 0.7000\n",
            "Epoch 154/300\n",
            "\n",
            "Epoch 00154: val_loss improved from 0.60379 to 0.60008, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2202 - acc: 0.9833 - val_loss: 0.6001 - val_acc: 0.7500\n",
            "Epoch 155/300\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.60008\n",
            "60/60 - 0s - loss: 0.2072 - acc: 0.9667 - val_loss: 0.6076 - val_acc: 0.7000\n",
            "Epoch 156/300\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.60008\n",
            "60/60 - 0s - loss: 0.1766 - acc: 0.9833 - val_loss: 0.6130 - val_acc: 0.7000\n",
            "Epoch 157/300\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.60008\n",
            "60/60 - 0s - loss: 0.1883 - acc: 0.9833 - val_loss: 0.6206 - val_acc: 0.7000\n",
            "Epoch 158/300\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.60008\n",
            "60/60 - 0s - loss: 0.1878 - acc: 0.9667 - val_loss: 0.6276 - val_acc: 0.6000\n",
            "Epoch 159/300\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.60008\n",
            "60/60 - 0s - loss: 0.1950 - acc: 0.9500 - val_loss: 0.6282 - val_acc: 0.6000\n",
            "Epoch 160/300\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.60008\n",
            "60/60 - 0s - loss: 0.2110 - acc: 0.9667 - val_loss: 0.6269 - val_acc: 0.6000\n",
            "Epoch 161/300\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.60008\n",
            "60/60 - 0s - loss: 0.1822 - acc: 1.0000 - val_loss: 0.6265 - val_acc: 0.6500\n",
            "Epoch 162/300\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.60008\n",
            "60/60 - 0s - loss: 0.1727 - acc: 0.9667 - val_loss: 0.6213 - val_acc: 0.6500\n",
            "Epoch 163/300\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.60008\n",
            "60/60 - 0s - loss: 0.1998 - acc: 0.9833 - val_loss: 0.6154 - val_acc: 0.7000\n",
            "Epoch 164/300\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.60008\n",
            "60/60 - 0s - loss: 0.1724 - acc: 1.0000 - val_loss: 0.6157 - val_acc: 0.7000\n",
            "Epoch 165/300\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.60008\n",
            "60/60 - 0s - loss: 0.1827 - acc: 0.9833 - val_loss: 0.6172 - val_acc: 0.7000\n",
            "Epoch 166/300\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 0.60008\n",
            "60/60 - 0s - loss: 0.2117 - acc: 0.9500 - val_loss: 0.6163 - val_acc: 0.7000\n",
            "Epoch 167/300\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.60008\n",
            "60/60 - 0s - loss: 0.1960 - acc: 0.9667 - val_loss: 0.6195 - val_acc: 0.6500\n",
            "Epoch 168/300\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.60008\n",
            "60/60 - 0s - loss: 0.1714 - acc: 0.9667 - val_loss: 0.6221 - val_acc: 0.6500\n",
            "Epoch 169/300\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 0.60008\n",
            "60/60 - 0s - loss: 0.1830 - acc: 0.9833 - val_loss: 0.6281 - val_acc: 0.7500\n",
            "Epoch 170/300\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.60008\n",
            "60/60 - 0s - loss: 0.1729 - acc: 0.9833 - val_loss: 0.6358 - val_acc: 0.6500\n",
            "Epoch 171/300\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.60008\n",
            "60/60 - 0s - loss: 0.1987 - acc: 0.9833 - val_loss: 0.6356 - val_acc: 0.6000\n",
            "Epoch 172/300\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.60008\n",
            "60/60 - 0s - loss: 0.1740 - acc: 1.0000 - val_loss: 0.6296 - val_acc: 0.6000\n",
            "Epoch 173/300\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.60008\n",
            "60/60 - 0s - loss: 0.2283 - acc: 0.9333 - val_loss: 0.6186 - val_acc: 0.6500\n",
            "Epoch 174/300\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.60008\n",
            "60/60 - 0s - loss: 0.1770 - acc: 0.9833 - val_loss: 0.6103 - val_acc: 0.7000\n",
            "Epoch 175/300\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.60008\n",
            "60/60 - 0s - loss: 0.1831 - acc: 1.0000 - val_loss: 0.6056 - val_acc: 0.7000\n",
            "Epoch 176/300\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 0.60008\n",
            "60/60 - 0s - loss: 0.1774 - acc: 0.9667 - val_loss: 0.6062 - val_acc: 0.7500\n",
            "Epoch 177/300\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.60008\n",
            "60/60 - 0s - loss: 0.1767 - acc: 1.0000 - val_loss: 0.6071 - val_acc: 0.7000\n",
            "Epoch 178/300\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.60008\n",
            "60/60 - 0s - loss: 0.1648 - acc: 1.0000 - val_loss: 0.6060 - val_acc: 0.6000\n",
            "Epoch 179/300\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.60008\n",
            "60/60 - 0s - loss: 0.1886 - acc: 1.0000 - val_loss: 0.6105 - val_acc: 0.6500\n",
            "Epoch 180/300\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.60008\n",
            "60/60 - 0s - loss: 0.1548 - acc: 0.9833 - val_loss: 0.6157 - val_acc: 0.6500\n",
            "Epoch 181/300\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.60008\n",
            "60/60 - 0s - loss: 0.1885 - acc: 0.9667 - val_loss: 0.6139 - val_acc: 0.7000\n",
            "Epoch 182/300\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.60008\n",
            "60/60 - 0s - loss: 0.1938 - acc: 0.9833 - val_loss: 0.6130 - val_acc: 0.7000\n",
            "Epoch 183/300\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.60008\n",
            "60/60 - 0s - loss: 0.1530 - acc: 1.0000 - val_loss: 0.6109 - val_acc: 0.7000\n",
            "Epoch 184/300\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 0.60008\n",
            "60/60 - 0s - loss: 0.1672 - acc: 1.0000 - val_loss: 0.6082 - val_acc: 0.6500\n",
            "Epoch 185/300\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.60008\n",
            "60/60 - 0s - loss: 0.1692 - acc: 0.9833 - val_loss: 0.6045 - val_acc: 0.6500\n",
            "Epoch 186/300\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.60008\n",
            "60/60 - 0s - loss: 0.1560 - acc: 1.0000 - val_loss: 0.6012 - val_acc: 0.6500\n",
            "Epoch 187/300\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 0.60008\n",
            "60/60 - 0s - loss: 0.1428 - acc: 1.0000 - val_loss: 0.6042 - val_acc: 0.6500\n",
            "Epoch 188/300\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.60008\n",
            "60/60 - 0s - loss: 0.1551 - acc: 0.9833 - val_loss: 0.6054 - val_acc: 0.7000\n",
            "Epoch 189/300\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.60008\n",
            "60/60 - 0s - loss: 0.1548 - acc: 0.9667 - val_loss: 0.6141 - val_acc: 0.7000\n",
            "Epoch 190/300\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.60008\n",
            "60/60 - 0s - loss: 0.1734 - acc: 0.9500 - val_loss: 0.6172 - val_acc: 0.6500\n",
            "Epoch 191/300\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.60008\n",
            "60/60 - 0s - loss: 0.1673 - acc: 0.9833 - val_loss: 0.6211 - val_acc: 0.6500\n",
            "Epoch 192/300\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 0.60008\n",
            "60/60 - 0s - loss: 0.1963 - acc: 0.9667 - val_loss: 0.6262 - val_acc: 0.6500\n",
            "Epoch 193/300\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.60008\n",
            "60/60 - 0s - loss: 0.1533 - acc: 1.0000 - val_loss: 0.6229 - val_acc: 0.7000\n",
            "Epoch 194/300\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.60008\n",
            "60/60 - 0s - loss: 0.1581 - acc: 1.0000 - val_loss: 0.6242 - val_acc: 0.6500\n",
            "Epoch 195/300\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.60008\n",
            "60/60 - 0s - loss: 0.1741 - acc: 0.9667 - val_loss: 0.6246 - val_acc: 0.6500\n",
            "Epoch 196/300\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.60008\n",
            "60/60 - 0s - loss: 0.1562 - acc: 1.0000 - val_loss: 0.6236 - val_acc: 0.6500\n",
            "Epoch 197/300\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.60008\n",
            "60/60 - 0s - loss: 0.1542 - acc: 1.0000 - val_loss: 0.6139 - val_acc: 0.6500\n",
            "Epoch 198/300\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.60008\n",
            "60/60 - 0s - loss: 0.1867 - acc: 0.9667 - val_loss: 0.6101 - val_acc: 0.7000\n",
            "Epoch 199/300\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.60008\n",
            "60/60 - 0s - loss: 0.1776 - acc: 0.9833 - val_loss: 0.6100 - val_acc: 0.7000\n",
            "Epoch 200/300\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.60008\n",
            "60/60 - 0s - loss: 0.1734 - acc: 1.0000 - val_loss: 0.6151 - val_acc: 0.7000\n",
            "Epoch 201/300\n",
            "\n",
            "Epoch 00201: val_loss did not improve from 0.60008\n",
            "60/60 - 0s - loss: 0.1520 - acc: 1.0000 - val_loss: 0.6160 - val_acc: 0.7500\n",
            "Epoch 202/300\n",
            "\n",
            "Epoch 00202: val_loss did not improve from 0.60008\n",
            "60/60 - 0s - loss: 0.1839 - acc: 0.9833 - val_loss: 0.6238 - val_acc: 0.8000\n",
            "Epoch 203/300\n",
            "\n",
            "Epoch 00203: val_loss did not improve from 0.60008\n",
            "60/60 - 0s - loss: 0.1657 - acc: 0.9667 - val_loss: 0.6246 - val_acc: 0.7500\n",
            "Epoch 204/300\n",
            "\n",
            "Epoch 00204: val_loss did not improve from 0.60008\n",
            "60/60 - 0s - loss: 0.1481 - acc: 1.0000 - val_loss: 0.6202 - val_acc: 0.7500\n",
            "Epoch 205/300\n",
            "\n",
            "Epoch 00205: val_loss did not improve from 0.60008\n",
            "60/60 - 0s - loss: 0.1606 - acc: 1.0000 - val_loss: 0.6175 - val_acc: 0.7000\n",
            "Epoch 206/300\n",
            "\n",
            "Epoch 00206: val_loss did not improve from 0.60008\n",
            "60/60 - 0s - loss: 0.1368 - acc: 1.0000 - val_loss: 0.6124 - val_acc: 0.7000\n",
            "Epoch 207/300\n",
            "\n",
            "Epoch 00207: val_loss did not improve from 0.60008\n",
            "60/60 - 0s - loss: 0.1786 - acc: 0.9667 - val_loss: 0.6040 - val_acc: 0.7000\n",
            "Epoch 208/300\n",
            "\n",
            "Epoch 00208: val_loss improved from 0.60008 to 0.59075, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1457 - acc: 1.0000 - val_loss: 0.5907 - val_acc: 0.7500\n",
            "Epoch 209/300\n",
            "\n",
            "Epoch 00209: val_loss improved from 0.59075 to 0.58607, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1610 - acc: 1.0000 - val_loss: 0.5861 - val_acc: 0.7500\n",
            "Epoch 210/300\n",
            "\n",
            "Epoch 00210: val_loss improved from 0.58607 to 0.58334, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1704 - acc: 0.9833 - val_loss: 0.5833 - val_acc: 0.7500\n",
            "Epoch 211/300\n",
            "\n",
            "Epoch 00211: val_loss did not improve from 0.58334\n",
            "60/60 - 0s - loss: 0.2039 - acc: 0.9833 - val_loss: 0.5887 - val_acc: 0.7500\n",
            "Epoch 212/300\n",
            "\n",
            "Epoch 00212: val_loss did not improve from 0.58334\n",
            "60/60 - 0s - loss: 0.1488 - acc: 1.0000 - val_loss: 0.5969 - val_acc: 0.7500\n",
            "Epoch 213/300\n",
            "\n",
            "Epoch 00213: val_loss did not improve from 0.58334\n",
            "60/60 - 0s - loss: 0.1502 - acc: 0.9833 - val_loss: 0.6021 - val_acc: 0.7500\n",
            "Epoch 214/300\n",
            "\n",
            "Epoch 00214: val_loss did not improve from 0.58334\n",
            "60/60 - 0s - loss: 0.1557 - acc: 0.9833 - val_loss: 0.6080 - val_acc: 0.7000\n",
            "Epoch 215/300\n",
            "\n",
            "Epoch 00215: val_loss did not improve from 0.58334\n",
            "60/60 - 0s - loss: 0.1270 - acc: 1.0000 - val_loss: 0.6074 - val_acc: 0.7000\n",
            "Epoch 216/300\n",
            "\n",
            "Epoch 00216: val_loss did not improve from 0.58334\n",
            "60/60 - 0s - loss: 0.1513 - acc: 1.0000 - val_loss: 0.6092 - val_acc: 0.7000\n",
            "Epoch 217/300\n",
            "\n",
            "Epoch 00217: val_loss did not improve from 0.58334\n",
            "60/60 - 0s - loss: 0.1788 - acc: 0.9667 - val_loss: 0.6110 - val_acc: 0.6500\n",
            "Epoch 218/300\n",
            "\n",
            "Epoch 00218: val_loss did not improve from 0.58334\n",
            "60/60 - 0s - loss: 0.1735 - acc: 0.9500 - val_loss: 0.6173 - val_acc: 0.7000\n",
            "Epoch 219/300\n",
            "\n",
            "Epoch 00219: val_loss did not improve from 0.58334\n",
            "60/60 - 0s - loss: 0.1588 - acc: 0.9833 - val_loss: 0.6137 - val_acc: 0.7000\n",
            "Epoch 220/300\n",
            "\n",
            "Epoch 00220: val_loss did not improve from 0.58334\n",
            "60/60 - 0s - loss: 0.1849 - acc: 1.0000 - val_loss: 0.6160 - val_acc: 0.6500\n",
            "Epoch 221/300\n",
            "\n",
            "Epoch 00221: val_loss did not improve from 0.58334\n",
            "60/60 - 0s - loss: 0.1350 - acc: 1.0000 - val_loss: 0.6256 - val_acc: 0.5500\n",
            "Epoch 222/300\n",
            "\n",
            "Epoch 00222: val_loss did not improve from 0.58334\n",
            "60/60 - 0s - loss: 0.1293 - acc: 1.0000 - val_loss: 0.6238 - val_acc: 0.5500\n",
            "Epoch 223/300\n",
            "\n",
            "Epoch 00223: val_loss did not improve from 0.58334\n",
            "60/60 - 0s - loss: 0.1701 - acc: 0.9500 - val_loss: 0.6280 - val_acc: 0.5500\n",
            "Epoch 224/300\n",
            "\n",
            "Epoch 00224: val_loss did not improve from 0.58334\n",
            "60/60 - 0s - loss: 0.1571 - acc: 1.0000 - val_loss: 0.6190 - val_acc: 0.6000\n",
            "Epoch 225/300\n",
            "\n",
            "Epoch 00225: val_loss did not improve from 0.58334\n",
            "60/60 - 0s - loss: 0.1308 - acc: 1.0000 - val_loss: 0.6196 - val_acc: 0.7000\n",
            "Epoch 226/300\n",
            "\n",
            "Epoch 00226: val_loss did not improve from 0.58334\n",
            "60/60 - 0s - loss: 0.1492 - acc: 1.0000 - val_loss: 0.6240 - val_acc: 0.6500\n",
            "Epoch 227/300\n",
            "\n",
            "Epoch 00227: val_loss did not improve from 0.58334\n",
            "60/60 - 0s - loss: 0.1333 - acc: 0.9833 - val_loss: 0.6259 - val_acc: 0.6000\n",
            "Epoch 228/300\n",
            "\n",
            "Epoch 00228: val_loss did not improve from 0.58334\n",
            "60/60 - 0s - loss: 0.1404 - acc: 0.9833 - val_loss: 0.6238 - val_acc: 0.6000\n",
            "Epoch 229/300\n",
            "\n",
            "Epoch 00229: val_loss did not improve from 0.58334\n",
            "60/60 - 0s - loss: 0.1489 - acc: 0.9833 - val_loss: 0.6220 - val_acc: 0.6000\n",
            "Epoch 230/300\n",
            "\n",
            "Epoch 00230: val_loss did not improve from 0.58334\n",
            "60/60 - 0s - loss: 0.1206 - acc: 0.9833 - val_loss: 0.6243 - val_acc: 0.6000\n",
            "Epoch 231/300\n",
            "\n",
            "Epoch 00231: val_loss did not improve from 0.58334\n",
            "60/60 - 0s - loss: 0.1600 - acc: 1.0000 - val_loss: 0.6366 - val_acc: 0.5500\n",
            "Epoch 232/300\n",
            "\n",
            "Epoch 00232: val_loss did not improve from 0.58334\n",
            "60/60 - 0s - loss: 0.1589 - acc: 1.0000 - val_loss: 0.6404 - val_acc: 0.5500\n",
            "Epoch 233/300\n",
            "\n",
            "Epoch 00233: val_loss did not improve from 0.58334\n",
            "60/60 - 0s - loss: 0.1489 - acc: 1.0000 - val_loss: 0.6387 - val_acc: 0.5500\n",
            "Epoch 234/300\n",
            "\n",
            "Epoch 00234: val_loss did not improve from 0.58334\n",
            "60/60 - 0s - loss: 0.1228 - acc: 1.0000 - val_loss: 0.6448 - val_acc: 0.6000\n",
            "Epoch 235/300\n",
            "\n",
            "Epoch 00235: val_loss did not improve from 0.58334\n",
            "60/60 - 0s - loss: 0.1587 - acc: 0.9833 - val_loss: 0.6505 - val_acc: 0.6000\n",
            "Epoch 236/300\n",
            "\n",
            "Epoch 00236: val_loss did not improve from 0.58334\n",
            "60/60 - 0s - loss: 0.1375 - acc: 0.9833 - val_loss: 0.6566 - val_acc: 0.6000\n",
            "Epoch 237/300\n",
            "\n",
            "Epoch 00237: val_loss did not improve from 0.58334\n",
            "60/60 - 0s - loss: 0.1466 - acc: 0.9833 - val_loss: 0.6636 - val_acc: 0.5500\n",
            "Epoch 238/300\n",
            "\n",
            "Epoch 00238: val_loss did not improve from 0.58334\n",
            "60/60 - 0s - loss: 0.1550 - acc: 0.9667 - val_loss: 0.6586 - val_acc: 0.5500\n",
            "Epoch 239/300\n",
            "\n",
            "Epoch 00239: val_loss did not improve from 0.58334\n",
            "60/60 - 0s - loss: 0.1458 - acc: 1.0000 - val_loss: 0.6552 - val_acc: 0.6000\n",
            "Epoch 240/300\n",
            "\n",
            "Epoch 00240: val_loss did not improve from 0.58334\n",
            "60/60 - 0s - loss: 0.1459 - acc: 1.0000 - val_loss: 0.6437 - val_acc: 0.6000\n",
            "Epoch 241/300\n",
            "\n",
            "Epoch 00241: val_loss did not improve from 0.58334\n",
            "60/60 - 0s - loss: 0.1390 - acc: 0.9833 - val_loss: 0.6412 - val_acc: 0.6500\n",
            "Epoch 242/300\n",
            "\n",
            "Epoch 00242: val_loss did not improve from 0.58334\n",
            "60/60 - 0s - loss: 0.1253 - acc: 0.9833 - val_loss: 0.6346 - val_acc: 0.6500\n",
            "Epoch 243/300\n",
            "\n",
            "Epoch 00243: val_loss did not improve from 0.58334\n",
            "60/60 - 0s - loss: 0.1373 - acc: 0.9833 - val_loss: 0.6256 - val_acc: 0.6000\n",
            "Epoch 244/300\n",
            "\n",
            "Epoch 00244: val_loss did not improve from 0.58334\n",
            "60/60 - 0s - loss: 0.1290 - acc: 1.0000 - val_loss: 0.6222 - val_acc: 0.6500\n",
            "Epoch 245/300\n",
            "\n",
            "Epoch 00245: val_loss did not improve from 0.58334\n",
            "60/60 - 0s - loss: 0.1432 - acc: 1.0000 - val_loss: 0.6221 - val_acc: 0.6500\n",
            "Epoch 246/300\n",
            "\n",
            "Epoch 00246: val_loss did not improve from 0.58334\n",
            "60/60 - 0s - loss: 0.1556 - acc: 0.9667 - val_loss: 0.6319 - val_acc: 0.6500\n",
            "Epoch 247/300\n",
            "\n",
            "Epoch 00247: val_loss did not improve from 0.58334\n",
            "60/60 - 0s - loss: 0.1412 - acc: 0.9833 - val_loss: 0.6446 - val_acc: 0.6000\n",
            "Epoch 248/300\n",
            "\n",
            "Epoch 00248: val_loss did not improve from 0.58334\n",
            "60/60 - 0s - loss: 0.1634 - acc: 0.9667 - val_loss: 0.6500 - val_acc: 0.7000\n",
            "Epoch 249/300\n",
            "\n",
            "Epoch 00249: val_loss did not improve from 0.58334\n",
            "60/60 - 0s - loss: 0.1495 - acc: 0.9833 - val_loss: 0.6492 - val_acc: 0.6500\n",
            "Epoch 250/300\n",
            "\n",
            "Epoch 00250: val_loss did not improve from 0.58334\n",
            "60/60 - 0s - loss: 0.1475 - acc: 0.9667 - val_loss: 0.6544 - val_acc: 0.6000\n",
            "Epoch 251/300\n",
            "\n",
            "Epoch 00251: val_loss did not improve from 0.58334\n",
            "60/60 - 0s - loss: 0.1638 - acc: 0.9833 - val_loss: 0.6530 - val_acc: 0.6500\n",
            "Epoch 252/300\n",
            "\n",
            "Epoch 00252: val_loss did not improve from 0.58334\n",
            "60/60 - 0s - loss: 0.1335 - acc: 1.0000 - val_loss: 0.6459 - val_acc: 0.6500\n",
            "Epoch 253/300\n",
            "\n",
            "Epoch 00253: val_loss did not improve from 0.58334\n",
            "60/60 - 0s - loss: 0.1242 - acc: 1.0000 - val_loss: 0.6433 - val_acc: 0.6000\n",
            "Epoch 254/300\n",
            "\n",
            "Epoch 00254: val_loss did not improve from 0.58334\n",
            "60/60 - 0s - loss: 0.1403 - acc: 0.9833 - val_loss: 0.6455 - val_acc: 0.5500\n",
            "Epoch 255/300\n",
            "\n",
            "Epoch 00255: val_loss did not improve from 0.58334\n",
            "60/60 - 0s - loss: 0.1318 - acc: 1.0000 - val_loss: 0.6428 - val_acc: 0.5500\n",
            "Epoch 256/300\n",
            "\n",
            "Epoch 00256: val_loss did not improve from 0.58334\n",
            "60/60 - 0s - loss: 0.1545 - acc: 1.0000 - val_loss: 0.6215 - val_acc: 0.5500\n",
            "Epoch 257/300\n",
            "\n",
            "Epoch 00257: val_loss did not improve from 0.58334\n",
            "60/60 - 0s - loss: 0.1334 - acc: 1.0000 - val_loss: 0.6004 - val_acc: 0.6500\n",
            "Epoch 258/300\n",
            "\n",
            "Epoch 00258: val_loss did not improve from 0.58334\n",
            "60/60 - 0s - loss: 0.1500 - acc: 0.9833 - val_loss: 0.5920 - val_acc: 0.6500\n",
            "Epoch 259/300\n",
            "\n",
            "Epoch 00259: val_loss did not improve from 0.58334\n",
            "60/60 - 0s - loss: 0.1500 - acc: 1.0000 - val_loss: 0.5907 - val_acc: 0.6500\n",
            "Epoch 260/300\n",
            "\n",
            "Epoch 00260: val_loss did not improve from 0.58334\n",
            "60/60 - 0s - loss: 0.1125 - acc: 1.0000 - val_loss: 0.6068 - val_acc: 0.6000\n",
            "Epoch 261/300\n",
            "\n",
            "Epoch 00261: val_loss did not improve from 0.58334\n",
            "60/60 - 0s - loss: 0.1148 - acc: 0.9833 - val_loss: 0.6167 - val_acc: 0.5500\n",
            "Epoch 262/300\n",
            "\n",
            "Epoch 00262: val_loss did not improve from 0.58334\n",
            "60/60 - 0s - loss: 0.1532 - acc: 1.0000 - val_loss: 0.6153 - val_acc: 0.6000\n",
            "Epoch 263/300\n",
            "\n",
            "Epoch 00263: val_loss did not improve from 0.58334\n",
            "60/60 - 0s - loss: 0.0968 - acc: 1.0000 - val_loss: 0.6197 - val_acc: 0.6000\n",
            "Epoch 264/300\n",
            "\n",
            "Epoch 00264: val_loss did not improve from 0.58334\n",
            "60/60 - 0s - loss: 0.1137 - acc: 1.0000 - val_loss: 0.6245 - val_acc: 0.6000\n",
            "Epoch 265/300\n",
            "\n",
            "Epoch 00265: val_loss did not improve from 0.58334\n",
            "60/60 - 0s - loss: 0.1351 - acc: 0.9833 - val_loss: 0.6203 - val_acc: 0.5500\n",
            "Epoch 266/300\n",
            "\n",
            "Epoch 00266: val_loss did not improve from 0.58334\n",
            "60/60 - 0s - loss: 0.1463 - acc: 0.9833 - val_loss: 0.6070 - val_acc: 0.5500\n",
            "Epoch 267/300\n",
            "\n",
            "Epoch 00267: val_loss did not improve from 0.58334\n",
            "60/60 - 0s - loss: 0.1222 - acc: 1.0000 - val_loss: 0.6147 - val_acc: 0.6500\n",
            "Epoch 268/300\n",
            "\n",
            "Epoch 00268: val_loss did not improve from 0.58334\n",
            "60/60 - 0s - loss: 0.1575 - acc: 0.9667 - val_loss: 0.6196 - val_acc: 0.7000\n",
            "Epoch 269/300\n",
            "\n",
            "Epoch 00269: val_loss did not improve from 0.58334\n",
            "60/60 - 0s - loss: 0.1253 - acc: 1.0000 - val_loss: 0.6171 - val_acc: 0.7000\n",
            "Epoch 270/300\n",
            "\n",
            "Epoch 00270: val_loss did not improve from 0.58334\n",
            "60/60 - 0s - loss: 0.1056 - acc: 1.0000 - val_loss: 0.6133 - val_acc: 0.7000\n",
            "Epoch 271/300\n",
            "\n",
            "Epoch 00271: val_loss did not improve from 0.58334\n",
            "60/60 - 0s - loss: 0.1424 - acc: 1.0000 - val_loss: 0.6070 - val_acc: 0.7000\n",
            "Epoch 272/300\n",
            "\n",
            "Epoch 00272: val_loss did not improve from 0.58334\n",
            "60/60 - 0s - loss: 0.1231 - acc: 1.0000 - val_loss: 0.6046 - val_acc: 0.7000\n",
            "Epoch 273/300\n",
            "\n",
            "Epoch 00273: val_loss did not improve from 0.58334\n",
            "60/60 - 0s - loss: 0.1297 - acc: 0.9667 - val_loss: 0.6048 - val_acc: 0.7000\n",
            "Epoch 274/300\n",
            "\n",
            "Epoch 00274: val_loss did not improve from 0.58334\n",
            "60/60 - 0s - loss: 0.1210 - acc: 0.9833 - val_loss: 0.6101 - val_acc: 0.6000\n",
            "Epoch 275/300\n",
            "\n",
            "Epoch 00275: val_loss did not improve from 0.58334\n",
            "60/60 - 0s - loss: 0.1268 - acc: 1.0000 - val_loss: 0.6258 - val_acc: 0.6000\n",
            "Epoch 276/300\n",
            "\n",
            "Epoch 00276: val_loss did not improve from 0.58334\n",
            "60/60 - 0s - loss: 0.1151 - acc: 1.0000 - val_loss: 0.6292 - val_acc: 0.6000\n",
            "Epoch 277/300\n",
            "\n",
            "Epoch 00277: val_loss did not improve from 0.58334\n",
            "60/60 - 0s - loss: 0.1409 - acc: 1.0000 - val_loss: 0.6325 - val_acc: 0.6000\n",
            "Epoch 278/300\n",
            "\n",
            "Epoch 00278: val_loss did not improve from 0.58334\n",
            "60/60 - 0s - loss: 0.1201 - acc: 1.0000 - val_loss: 0.6436 - val_acc: 0.6000\n",
            "Epoch 279/300\n",
            "\n",
            "Epoch 00279: val_loss did not improve from 0.58334\n",
            "60/60 - 0s - loss: 0.1109 - acc: 1.0000 - val_loss: 0.6404 - val_acc: 0.6000\n",
            "Epoch 280/300\n",
            "\n",
            "Epoch 00280: val_loss did not improve from 0.58334\n",
            "60/60 - 0s - loss: 0.1474 - acc: 0.9833 - val_loss: 0.6398 - val_acc: 0.6000\n",
            "Epoch 281/300\n",
            "\n",
            "Epoch 00281: val_loss did not improve from 0.58334\n",
            "60/60 - 0s - loss: 0.0910 - acc: 1.0000 - val_loss: 0.6407 - val_acc: 0.6000\n",
            "Epoch 282/300\n",
            "\n",
            "Epoch 00282: val_loss did not improve from 0.58334\n",
            "60/60 - 0s - loss: 0.1166 - acc: 1.0000 - val_loss: 0.6398 - val_acc: 0.6000\n",
            "Epoch 283/300\n",
            "\n",
            "Epoch 00283: val_loss did not improve from 0.58334\n",
            "60/60 - 0s - loss: 0.1010 - acc: 1.0000 - val_loss: 0.6331 - val_acc: 0.6500\n",
            "Epoch 284/300\n",
            "\n",
            "Epoch 00284: val_loss did not improve from 0.58334\n",
            "60/60 - 0s - loss: 0.1221 - acc: 1.0000 - val_loss: 0.6357 - val_acc: 0.5000\n",
            "Epoch 285/300\n",
            "\n",
            "Epoch 00285: val_loss did not improve from 0.58334\n",
            "60/60 - 0s - loss: 0.1246 - acc: 1.0000 - val_loss: 0.6349 - val_acc: 0.5000\n",
            "Epoch 286/300\n",
            "\n",
            "Epoch 00286: val_loss did not improve from 0.58334\n",
            "60/60 - 0s - loss: 0.1625 - acc: 0.9833 - val_loss: 0.6458 - val_acc: 0.5500\n",
            "Epoch 287/300\n",
            "\n",
            "Epoch 00287: val_loss did not improve from 0.58334\n",
            "60/60 - 0s - loss: 0.1216 - acc: 0.9833 - val_loss: 0.6810 - val_acc: 0.5500\n",
            "Epoch 288/300\n",
            "\n",
            "Epoch 00288: val_loss did not improve from 0.58334\n",
            "60/60 - 0s - loss: 0.1227 - acc: 0.9833 - val_loss: 0.7122 - val_acc: 0.6000\n",
            "Epoch 289/300\n",
            "\n",
            "Epoch 00289: val_loss did not improve from 0.58334\n",
            "60/60 - 0s - loss: 0.1407 - acc: 0.9833 - val_loss: 0.7040 - val_acc: 0.6000\n",
            "Epoch 290/300\n",
            "\n",
            "Epoch 00290: val_loss did not improve from 0.58334\n",
            "60/60 - 0s - loss: 0.1394 - acc: 1.0000 - val_loss: 0.6756 - val_acc: 0.5500\n",
            "Epoch 291/300\n",
            "\n",
            "Epoch 00291: val_loss did not improve from 0.58334\n",
            "60/60 - 0s - loss: 0.1357 - acc: 1.0000 - val_loss: 0.6293 - val_acc: 0.5500\n",
            "Epoch 292/300\n",
            "\n",
            "Epoch 00292: val_loss did not improve from 0.58334\n",
            "60/60 - 0s - loss: 0.1373 - acc: 1.0000 - val_loss: 0.6092 - val_acc: 0.7000\n",
            "Epoch 293/300\n",
            "\n",
            "Epoch 00293: val_loss did not improve from 0.58334\n",
            "60/60 - 0s - loss: 0.1150 - acc: 1.0000 - val_loss: 0.6112 - val_acc: 0.6000\n",
            "Epoch 294/300\n",
            "\n",
            "Epoch 00294: val_loss did not improve from 0.58334\n",
            "60/60 - 0s - loss: 0.1356 - acc: 0.9833 - val_loss: 0.6155 - val_acc: 0.6000\n",
            "Epoch 295/300\n",
            "\n",
            "Epoch 00295: val_loss did not improve from 0.58334\n",
            "60/60 - 0s - loss: 0.1316 - acc: 1.0000 - val_loss: 0.6238 - val_acc: 0.6000\n",
            "Epoch 296/300\n",
            "\n",
            "Epoch 00296: val_loss did not improve from 0.58334\n",
            "60/60 - 0s - loss: 0.1216 - acc: 0.9833 - val_loss: 0.6350 - val_acc: 0.5500\n",
            "Epoch 297/300\n",
            "\n",
            "Epoch 00297: val_loss did not improve from 0.58334\n",
            "60/60 - 0s - loss: 0.1368 - acc: 0.9833 - val_loss: 0.6510 - val_acc: 0.5500\n",
            "Epoch 298/300\n",
            "\n",
            "Epoch 00298: val_loss did not improve from 0.58334\n",
            "60/60 - 0s - loss: 0.1217 - acc: 1.0000 - val_loss: 0.6582 - val_acc: 0.5500\n",
            "Epoch 299/300\n",
            "\n",
            "Epoch 00299: val_loss did not improve from 0.58334\n",
            "60/60 - 0s - loss: 0.1332 - acc: 0.9833 - val_loss: 0.6585 - val_acc: 0.5500\n",
            "Epoch 300/300\n",
            "\n",
            "Epoch 00300: val_loss did not improve from 0.58334\n",
            "60/60 - 0s - loss: 0.1264 - acc: 1.0000 - val_loss: 0.6526 - val_acc: 0.6000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFCXl/AvMxACgPUBHIQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "# Evaluate on test data\n",
            "40/40 [==============================] - 0s 2ms/sample - loss: 0.5640 - acc: 0.7250\n",
            "test loss, test acc: [0.5640123024582863, 0.725]\n",
            "EEG_Deep/Data2A/parsed_P08T.mat\n",
            "Filtering of Training Data Finished\n",
            "EEG_Deep/Data2A/parsed_P08E.mat\n",
            "Filtering of Testing Data Finished\n",
            "(1, 2)\n",
            "Finding labels in training data\n",
            "Finding labels in testing data\n",
            "[2 2 2 2 1 2 1 2 2 2 2 2 1 1 1 1 2 2 1 2]\n",
            "(60, 12, 1536)\n",
            "(60,)\n",
            "(20, 12, 1536)\n",
            "(20,)\n",
            "[[0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]]\n",
            "X_train shape: (60, 1, 12, 1536)\n",
            "60 train samples\n",
            "20 val samples\n",
            "(40, 12, 1536)\n",
            "(40, 1)\n",
            "X_train shape: (40, 1, 12, 1536)\n",
            "40 train samples\n",
            "Train on 60 samples, validate on 20 samples\n",
            "Epoch 1/300\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.69267, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 1s - loss: 0.6804 - acc: 0.4833 - val_loss: 0.6927 - val_acc: 0.5500\n",
            "Epoch 2/300\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.69267 to 0.69075, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6758 - acc: 0.5500 - val_loss: 0.6908 - val_acc: 0.5500\n",
            "Epoch 3/300\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.69075 to 0.68880, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6568 - acc: 0.6500 - val_loss: 0.6888 - val_acc: 0.6000\n",
            "Epoch 4/300\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.68880 to 0.68789, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6534 - acc: 0.6833 - val_loss: 0.6879 - val_acc: 0.6000\n",
            "Epoch 5/300\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.68789 to 0.68742, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6238 - acc: 0.6500 - val_loss: 0.6874 - val_acc: 0.5500\n",
            "Epoch 6/300\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.68742 to 0.68703, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6387 - acc: 0.6167 - val_loss: 0.6870 - val_acc: 0.5000\n",
            "Epoch 7/300\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.68703 to 0.68671, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5940 - acc: 0.7333 - val_loss: 0.6867 - val_acc: 0.5000\n",
            "Epoch 8/300\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.68671\n",
            "60/60 - 0s - loss: 0.5933 - acc: 0.7667 - val_loss: 0.6868 - val_acc: 0.5000\n",
            "Epoch 9/300\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.68671\n",
            "60/60 - 0s - loss: 0.5620 - acc: 0.8000 - val_loss: 0.6872 - val_acc: 0.5000\n",
            "Epoch 10/300\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.68671\n",
            "60/60 - 0s - loss: 0.5559 - acc: 0.9000 - val_loss: 0.6879 - val_acc: 0.5000\n",
            "Epoch 11/300\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.68671\n",
            "60/60 - 0s - loss: 0.5399 - acc: 0.8333 - val_loss: 0.6881 - val_acc: 0.5500\n",
            "Epoch 12/300\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.68671\n",
            "60/60 - 0s - loss: 0.5451 - acc: 0.7667 - val_loss: 0.6889 - val_acc: 0.5000\n",
            "Epoch 13/300\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.68671\n",
            "60/60 - 0s - loss: 0.5123 - acc: 0.8833 - val_loss: 0.6899 - val_acc: 0.5000\n",
            "Epoch 14/300\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.68671\n",
            "60/60 - 0s - loss: 0.5011 - acc: 0.8667 - val_loss: 0.6908 - val_acc: 0.4500\n",
            "Epoch 15/300\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.68671\n",
            "60/60 - 0s - loss: 0.4815 - acc: 0.9333 - val_loss: 0.6910 - val_acc: 0.5000\n",
            "Epoch 16/300\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.68671\n",
            "60/60 - 0s - loss: 0.4927 - acc: 0.8500 - val_loss: 0.6915 - val_acc: 0.5000\n",
            "Epoch 17/300\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.68671\n",
            "60/60 - 0s - loss: 0.4705 - acc: 0.9333 - val_loss: 0.6905 - val_acc: 0.4500\n",
            "Epoch 18/300\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.68671\n",
            "60/60 - 0s - loss: 0.4628 - acc: 0.8833 - val_loss: 0.6888 - val_acc: 0.4500\n",
            "Epoch 19/300\n",
            "\n",
            "Epoch 00019: val_loss improved from 0.68671 to 0.68586, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4800 - acc: 0.9000 - val_loss: 0.6859 - val_acc: 0.4500\n",
            "Epoch 20/300\n",
            "\n",
            "Epoch 00020: val_loss improved from 0.68586 to 0.68574, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4494 - acc: 0.9167 - val_loss: 0.6857 - val_acc: 0.4500\n",
            "Epoch 21/300\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.68574\n",
            "60/60 - 0s - loss: 0.4129 - acc: 0.9833 - val_loss: 0.6867 - val_acc: 0.4500\n",
            "Epoch 22/300\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.68574\n",
            "60/60 - 0s - loss: 0.4161 - acc: 0.9167 - val_loss: 0.6865 - val_acc: 0.4500\n",
            "Epoch 23/300\n",
            "\n",
            "Epoch 00023: val_loss improved from 0.68574 to 0.68511, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4184 - acc: 0.9000 - val_loss: 0.6851 - val_acc: 0.4500\n",
            "Epoch 24/300\n",
            "\n",
            "Epoch 00024: val_loss improved from 0.68511 to 0.68347, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3924 - acc: 0.9500 - val_loss: 0.6835 - val_acc: 0.4500\n",
            "Epoch 25/300\n",
            "\n",
            "Epoch 00025: val_loss improved from 0.68347 to 0.68240, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3931 - acc: 0.9500 - val_loss: 0.6824 - val_acc: 0.4500\n",
            "Epoch 26/300\n",
            "\n",
            "Epoch 00026: val_loss improved from 0.68240 to 0.68148, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3971 - acc: 0.9000 - val_loss: 0.6815 - val_acc: 0.4500\n",
            "Epoch 27/300\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.68148\n",
            "60/60 - 0s - loss: 0.3931 - acc: 0.9167 - val_loss: 0.6817 - val_acc: 0.5000\n",
            "Epoch 28/300\n",
            "\n",
            "Epoch 00028: val_loss improved from 0.68148 to 0.67976, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3698 - acc: 0.9000 - val_loss: 0.6798 - val_acc: 0.5000\n",
            "Epoch 29/300\n",
            "\n",
            "Epoch 00029: val_loss improved from 0.67976 to 0.67832, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3720 - acc: 0.9500 - val_loss: 0.6783 - val_acc: 0.5000\n",
            "Epoch 30/300\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.67832\n",
            "60/60 - 0s - loss: 0.3606 - acc: 0.9167 - val_loss: 0.6791 - val_acc: 0.5500\n",
            "Epoch 31/300\n",
            "\n",
            "Epoch 00031: val_loss improved from 0.67832 to 0.67456, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3795 - acc: 0.9500 - val_loss: 0.6746 - val_acc: 0.5500\n",
            "Epoch 32/300\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.3659 - acc: 0.9333 - val_loss: 0.6749 - val_acc: 0.5000\n",
            "Epoch 33/300\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.3296 - acc: 0.9667 - val_loss: 0.6784 - val_acc: 0.5500\n",
            "Epoch 34/300\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.3518 - acc: 0.9500 - val_loss: 0.6803 - val_acc: 0.5500\n",
            "Epoch 35/300\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.3193 - acc: 0.9833 - val_loss: 0.6862 - val_acc: 0.5500\n",
            "Epoch 36/300\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.3570 - acc: 0.9500 - val_loss: 0.6913 - val_acc: 0.5500\n",
            "Epoch 37/300\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.3389 - acc: 0.9833 - val_loss: 0.6943 - val_acc: 0.5500\n",
            "Epoch 38/300\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.3425 - acc: 0.9500 - val_loss: 0.6981 - val_acc: 0.5500\n",
            "Epoch 39/300\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.3249 - acc: 0.9667 - val_loss: 0.6971 - val_acc: 0.5500\n",
            "Epoch 40/300\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.3176 - acc: 0.9833 - val_loss: 0.6961 - val_acc: 0.5500\n",
            "Epoch 41/300\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.3084 - acc: 0.9833 - val_loss: 0.6968 - val_acc: 0.5500\n",
            "Epoch 42/300\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.3424 - acc: 0.9167 - val_loss: 0.6962 - val_acc: 0.5500\n",
            "Epoch 43/300\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.3024 - acc: 0.9333 - val_loss: 0.6915 - val_acc: 0.5500\n",
            "Epoch 44/300\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.3437 - acc: 0.9167 - val_loss: 0.6890 - val_acc: 0.5000\n",
            "Epoch 45/300\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.3446 - acc: 0.9333 - val_loss: 0.6886 - val_acc: 0.5000\n",
            "Epoch 46/300\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.2944 - acc: 0.9667 - val_loss: 0.6903 - val_acc: 0.5000\n",
            "Epoch 47/300\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.3111 - acc: 0.9500 - val_loss: 0.6897 - val_acc: 0.5000\n",
            "Epoch 48/300\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.2861 - acc: 1.0000 - val_loss: 0.6929 - val_acc: 0.5000\n",
            "Epoch 49/300\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.3182 - acc: 1.0000 - val_loss: 0.6923 - val_acc: 0.5000\n",
            "Epoch 50/300\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.3049 - acc: 0.9500 - val_loss: 0.6932 - val_acc: 0.5000\n",
            "Epoch 51/300\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.2855 - acc: 0.9500 - val_loss: 0.7005 - val_acc: 0.5000\n",
            "Epoch 52/300\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.2923 - acc: 0.9833 - val_loss: 0.7022 - val_acc: 0.4500\n",
            "Epoch 53/300\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.2819 - acc: 0.9833 - val_loss: 0.7002 - val_acc: 0.4500\n",
            "Epoch 54/300\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.2874 - acc: 0.9500 - val_loss: 0.6992 - val_acc: 0.5000\n",
            "Epoch 55/300\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.2850 - acc: 0.9667 - val_loss: 0.7004 - val_acc: 0.5000\n",
            "Epoch 56/300\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.3016 - acc: 0.9667 - val_loss: 0.6956 - val_acc: 0.5000\n",
            "Epoch 57/300\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.2899 - acc: 0.9833 - val_loss: 0.7023 - val_acc: 0.4500\n",
            "Epoch 58/300\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.2814 - acc: 0.9667 - val_loss: 0.7056 - val_acc: 0.5000\n",
            "Epoch 59/300\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.2954 - acc: 0.9833 - val_loss: 0.7129 - val_acc: 0.5000\n",
            "Epoch 60/300\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.2911 - acc: 0.9833 - val_loss: 0.7209 - val_acc: 0.5000\n",
            "Epoch 61/300\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.3073 - acc: 1.0000 - val_loss: 0.7271 - val_acc: 0.5000\n",
            "Epoch 62/300\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.2787 - acc: 0.9667 - val_loss: 0.7308 - val_acc: 0.5000\n",
            "Epoch 63/300\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.2840 - acc: 0.9833 - val_loss: 0.7274 - val_acc: 0.4500\n",
            "Epoch 64/300\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.2745 - acc: 0.9500 - val_loss: 0.7296 - val_acc: 0.4500\n",
            "Epoch 65/300\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.2649 - acc: 1.0000 - val_loss: 0.7230 - val_acc: 0.4500\n",
            "Epoch 66/300\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.2548 - acc: 0.9667 - val_loss: 0.7172 - val_acc: 0.4500\n",
            "Epoch 67/300\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.2645 - acc: 0.9833 - val_loss: 0.7212 - val_acc: 0.4500\n",
            "Epoch 68/300\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.2564 - acc: 0.9833 - val_loss: 0.7247 - val_acc: 0.4500\n",
            "Epoch 69/300\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.2723 - acc: 0.9833 - val_loss: 0.7242 - val_acc: 0.4500\n",
            "Epoch 70/300\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.2647 - acc: 1.0000 - val_loss: 0.7263 - val_acc: 0.4500\n",
            "Epoch 71/300\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.2400 - acc: 1.0000 - val_loss: 0.7312 - val_acc: 0.4500\n",
            "Epoch 72/300\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.2442 - acc: 1.0000 - val_loss: 0.7425 - val_acc: 0.4500\n",
            "Epoch 73/300\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.2384 - acc: 1.0000 - val_loss: 0.7529 - val_acc: 0.4500\n",
            "Epoch 74/300\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.2578 - acc: 1.0000 - val_loss: 0.7660 - val_acc: 0.4500\n",
            "Epoch 75/300\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.2642 - acc: 0.9500 - val_loss: 0.7671 - val_acc: 0.4500\n",
            "Epoch 76/300\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.2663 - acc: 0.9333 - val_loss: 0.7672 - val_acc: 0.4500\n",
            "Epoch 77/300\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.2452 - acc: 0.9833 - val_loss: 0.7646 - val_acc: 0.4500\n",
            "Epoch 78/300\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.2401 - acc: 0.9833 - val_loss: 0.7676 - val_acc: 0.4500\n",
            "Epoch 79/300\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.2780 - acc: 0.9500 - val_loss: 0.7782 - val_acc: 0.4500\n",
            "Epoch 80/300\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.2389 - acc: 1.0000 - val_loss: 0.7783 - val_acc: 0.4500\n",
            "Epoch 81/300\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.2407 - acc: 0.9833 - val_loss: 0.7750 - val_acc: 0.4500\n",
            "Epoch 82/300\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.2510 - acc: 1.0000 - val_loss: 0.7728 - val_acc: 0.4500\n",
            "Epoch 83/300\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.2539 - acc: 0.9833 - val_loss: 0.7633 - val_acc: 0.4500\n",
            "Epoch 84/300\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.2322 - acc: 0.9833 - val_loss: 0.7700 - val_acc: 0.4500\n",
            "Epoch 85/300\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.2436 - acc: 0.9667 - val_loss: 0.7681 - val_acc: 0.4500\n",
            "Epoch 86/300\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.2313 - acc: 0.9833 - val_loss: 0.7729 - val_acc: 0.4500\n",
            "Epoch 87/300\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.2187 - acc: 0.9833 - val_loss: 0.7749 - val_acc: 0.4500\n",
            "Epoch 88/300\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.2391 - acc: 0.9667 - val_loss: 0.7702 - val_acc: 0.5000\n",
            "Epoch 89/300\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.2406 - acc: 0.9833 - val_loss: 0.7659 - val_acc: 0.5000\n",
            "Epoch 90/300\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.2523 - acc: 1.0000 - val_loss: 0.7797 - val_acc: 0.5000\n",
            "Epoch 91/300\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.2576 - acc: 0.9833 - val_loss: 0.7903 - val_acc: 0.5000\n",
            "Epoch 92/300\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.2470 - acc: 0.9667 - val_loss: 0.8024 - val_acc: 0.5000\n",
            "Epoch 93/300\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.2270 - acc: 0.9833 - val_loss: 0.8141 - val_acc: 0.5000\n",
            "Epoch 94/300\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.2402 - acc: 0.9667 - val_loss: 0.8110 - val_acc: 0.4500\n",
            "Epoch 95/300\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.2356 - acc: 1.0000 - val_loss: 0.7994 - val_acc: 0.4500\n",
            "Epoch 96/300\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.2184 - acc: 0.9833 - val_loss: 0.7974 - val_acc: 0.4500\n",
            "Epoch 97/300\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.2244 - acc: 1.0000 - val_loss: 0.7985 - val_acc: 0.4500\n",
            "Epoch 98/300\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.2439 - acc: 0.9667 - val_loss: 0.8036 - val_acc: 0.4500\n",
            "Epoch 99/300\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.2079 - acc: 1.0000 - val_loss: 0.8161 - val_acc: 0.4500\n",
            "Epoch 100/300\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.2299 - acc: 0.9500 - val_loss: 0.8349 - val_acc: 0.4500\n",
            "Epoch 101/300\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.2038 - acc: 1.0000 - val_loss: 0.8422 - val_acc: 0.4500\n",
            "Epoch 102/300\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.2131 - acc: 1.0000 - val_loss: 0.8457 - val_acc: 0.4500\n",
            "Epoch 103/300\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.2052 - acc: 1.0000 - val_loss: 0.8362 - val_acc: 0.4500\n",
            "Epoch 104/300\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.2170 - acc: 0.9833 - val_loss: 0.8528 - val_acc: 0.4500\n",
            "Epoch 105/300\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.2123 - acc: 0.9833 - val_loss: 0.8629 - val_acc: 0.4500\n",
            "Epoch 106/300\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1855 - acc: 1.0000 - val_loss: 0.8698 - val_acc: 0.4500\n",
            "Epoch 107/300\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.2134 - acc: 0.9500 - val_loss: 0.8861 - val_acc: 0.4500\n",
            "Epoch 108/300\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1972 - acc: 0.9833 - val_loss: 0.9041 - val_acc: 0.4500\n",
            "Epoch 109/300\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.2137 - acc: 0.9833 - val_loss: 0.8989 - val_acc: 0.4500\n",
            "Epoch 110/300\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.2224 - acc: 0.9667 - val_loss: 0.8990 - val_acc: 0.4500\n",
            "Epoch 111/300\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.2229 - acc: 0.9667 - val_loss: 0.8875 - val_acc: 0.4500\n",
            "Epoch 112/300\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.2239 - acc: 1.0000 - val_loss: 0.8886 - val_acc: 0.4500\n",
            "Epoch 113/300\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.2158 - acc: 1.0000 - val_loss: 0.8945 - val_acc: 0.4500\n",
            "Epoch 114/300\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.2124 - acc: 0.9833 - val_loss: 0.8948 - val_acc: 0.4500\n",
            "Epoch 115/300\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.2029 - acc: 1.0000 - val_loss: 0.8983 - val_acc: 0.4500\n",
            "Epoch 116/300\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1836 - acc: 0.9833 - val_loss: 0.9217 - val_acc: 0.4500\n",
            "Epoch 117/300\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.2133 - acc: 0.9833 - val_loss: 0.9234 - val_acc: 0.4500\n",
            "Epoch 118/300\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.2044 - acc: 0.9500 - val_loss: 0.9106 - val_acc: 0.4500\n",
            "Epoch 119/300\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.2158 - acc: 0.9833 - val_loss: 0.9105 - val_acc: 0.4500\n",
            "Epoch 120/300\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1944 - acc: 0.9833 - val_loss: 0.9150 - val_acc: 0.4500\n",
            "Epoch 121/300\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1913 - acc: 1.0000 - val_loss: 0.9046 - val_acc: 0.4500\n",
            "Epoch 122/300\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1897 - acc: 1.0000 - val_loss: 0.9132 - val_acc: 0.4500\n",
            "Epoch 123/300\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.2215 - acc: 0.9667 - val_loss: 0.9303 - val_acc: 0.4500\n",
            "Epoch 124/300\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1725 - acc: 1.0000 - val_loss: 0.9459 - val_acc: 0.4500\n",
            "Epoch 125/300\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1791 - acc: 0.9833 - val_loss: 0.9461 - val_acc: 0.4500\n",
            "Epoch 126/300\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.2079 - acc: 0.9667 - val_loss: 0.9454 - val_acc: 0.4500\n",
            "Epoch 127/300\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1760 - acc: 0.9833 - val_loss: 0.9386 - val_acc: 0.4500\n",
            "Epoch 128/300\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1890 - acc: 0.9833 - val_loss: 0.9195 - val_acc: 0.4500\n",
            "Epoch 129/300\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1804 - acc: 1.0000 - val_loss: 0.9190 - val_acc: 0.4500\n",
            "Epoch 130/300\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1808 - acc: 1.0000 - val_loss: 0.9130 - val_acc: 0.4500\n",
            "Epoch 131/300\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1734 - acc: 0.9833 - val_loss: 0.9040 - val_acc: 0.5000\n",
            "Epoch 132/300\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1866 - acc: 0.9833 - val_loss: 0.8937 - val_acc: 0.5000\n",
            "Epoch 133/300\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1835 - acc: 1.0000 - val_loss: 0.8744 - val_acc: 0.4500\n",
            "Epoch 134/300\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1890 - acc: 1.0000 - val_loss: 0.8521 - val_acc: 0.4500\n",
            "Epoch 135/300\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1857 - acc: 1.0000 - val_loss: 0.8628 - val_acc: 0.4500\n",
            "Epoch 136/300\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1941 - acc: 1.0000 - val_loss: 0.8751 - val_acc: 0.4500\n",
            "Epoch 137/300\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1868 - acc: 0.9667 - val_loss: 0.8835 - val_acc: 0.4500\n",
            "Epoch 138/300\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1942 - acc: 0.9833 - val_loss: 0.8850 - val_acc: 0.4500\n",
            "Epoch 139/300\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1960 - acc: 0.9833 - val_loss: 0.8895 - val_acc: 0.4500\n",
            "Epoch 140/300\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1748 - acc: 1.0000 - val_loss: 0.8943 - val_acc: 0.4500\n",
            "Epoch 141/300\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1767 - acc: 1.0000 - val_loss: 0.9186 - val_acc: 0.4500\n",
            "Epoch 142/300\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1930 - acc: 0.9667 - val_loss: 0.9522 - val_acc: 0.4500\n",
            "Epoch 143/300\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1728 - acc: 1.0000 - val_loss: 0.9780 - val_acc: 0.4500\n",
            "Epoch 144/300\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1721 - acc: 1.0000 - val_loss: 0.9594 - val_acc: 0.4500\n",
            "Epoch 145/300\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1794 - acc: 1.0000 - val_loss: 0.9438 - val_acc: 0.4500\n",
            "Epoch 146/300\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1824 - acc: 0.9833 - val_loss: 0.9123 - val_acc: 0.4500\n",
            "Epoch 147/300\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1669 - acc: 1.0000 - val_loss: 0.8974 - val_acc: 0.4500\n",
            "Epoch 148/300\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1691 - acc: 1.0000 - val_loss: 0.8820 - val_acc: 0.4500\n",
            "Epoch 149/300\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1528 - acc: 1.0000 - val_loss: 0.9100 - val_acc: 0.4500\n",
            "Epoch 150/300\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1671 - acc: 1.0000 - val_loss: 0.9467 - val_acc: 0.4500\n",
            "Epoch 151/300\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1816 - acc: 0.9833 - val_loss: 0.9517 - val_acc: 0.4000\n",
            "Epoch 152/300\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1894 - acc: 0.9833 - val_loss: 0.9244 - val_acc: 0.4500\n",
            "Epoch 153/300\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1682 - acc: 1.0000 - val_loss: 0.9067 - val_acc: 0.4500\n",
            "Epoch 154/300\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1582 - acc: 1.0000 - val_loss: 0.9061 - val_acc: 0.4500\n",
            "Epoch 155/300\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1655 - acc: 1.0000 - val_loss: 0.9219 - val_acc: 0.4500\n",
            "Epoch 156/300\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1803 - acc: 0.9833 - val_loss: 0.9143 - val_acc: 0.4500\n",
            "Epoch 157/300\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1724 - acc: 0.9833 - val_loss: 0.9198 - val_acc: 0.4500\n",
            "Epoch 158/300\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1541 - acc: 1.0000 - val_loss: 0.9271 - val_acc: 0.4000\n",
            "Epoch 159/300\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1891 - acc: 0.9667 - val_loss: 0.9458 - val_acc: 0.4500\n",
            "Epoch 160/300\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1689 - acc: 1.0000 - val_loss: 0.9388 - val_acc: 0.4500\n",
            "Epoch 161/300\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1679 - acc: 0.9833 - val_loss: 0.9066 - val_acc: 0.4500\n",
            "Epoch 162/300\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1618 - acc: 1.0000 - val_loss: 0.8994 - val_acc: 0.4500\n",
            "Epoch 163/300\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1830 - acc: 1.0000 - val_loss: 0.9043 - val_acc: 0.4500\n",
            "Epoch 164/300\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1683 - acc: 1.0000 - val_loss: 0.9119 - val_acc: 0.4000\n",
            "Epoch 165/300\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1395 - acc: 1.0000 - val_loss: 0.9241 - val_acc: 0.4000\n",
            "Epoch 166/300\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1728 - acc: 0.9833 - val_loss: 0.9227 - val_acc: 0.4000\n",
            "Epoch 167/300\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1698 - acc: 1.0000 - val_loss: 0.9062 - val_acc: 0.4500\n",
            "Epoch 168/300\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1624 - acc: 1.0000 - val_loss: 0.9101 - val_acc: 0.4500\n",
            "Epoch 169/300\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1406 - acc: 1.0000 - val_loss: 0.8935 - val_acc: 0.4500\n",
            "Epoch 170/300\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1596 - acc: 0.9833 - val_loss: 0.9151 - val_acc: 0.4000\n",
            "Epoch 171/300\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1644 - acc: 1.0000 - val_loss: 0.9097 - val_acc: 0.4000\n",
            "Epoch 172/300\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1586 - acc: 1.0000 - val_loss: 0.9261 - val_acc: 0.4000\n",
            "Epoch 173/300\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1533 - acc: 0.9833 - val_loss: 0.9297 - val_acc: 0.4000\n",
            "Epoch 174/300\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1611 - acc: 1.0000 - val_loss: 0.9238 - val_acc: 0.3500\n",
            "Epoch 175/300\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1533 - acc: 1.0000 - val_loss: 0.9072 - val_acc: 0.4000\n",
            "Epoch 176/300\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1598 - acc: 1.0000 - val_loss: 0.9032 - val_acc: 0.3500\n",
            "Epoch 177/300\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1464 - acc: 1.0000 - val_loss: 0.8956 - val_acc: 0.4000\n",
            "Epoch 178/300\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1737 - acc: 0.9833 - val_loss: 0.9021 - val_acc: 0.3500\n",
            "Epoch 179/300\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1515 - acc: 1.0000 - val_loss: 0.9068 - val_acc: 0.4000\n",
            "Epoch 180/300\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1539 - acc: 1.0000 - val_loss: 0.9421 - val_acc: 0.4000\n",
            "Epoch 181/300\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1523 - acc: 0.9833 - val_loss: 0.9413 - val_acc: 0.4000\n",
            "Epoch 182/300\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1369 - acc: 1.0000 - val_loss: 0.9283 - val_acc: 0.4000\n",
            "Epoch 183/300\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1541 - acc: 1.0000 - val_loss: 0.9251 - val_acc: 0.4000\n",
            "Epoch 184/300\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1498 - acc: 1.0000 - val_loss: 0.8817 - val_acc: 0.5000\n",
            "Epoch 185/300\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1360 - acc: 1.0000 - val_loss: 0.8554 - val_acc: 0.5000\n",
            "Epoch 186/300\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1441 - acc: 1.0000 - val_loss: 0.8711 - val_acc: 0.5000\n",
            "Epoch 187/300\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1528 - acc: 1.0000 - val_loss: 0.8873 - val_acc: 0.5000\n",
            "Epoch 188/300\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1443 - acc: 1.0000 - val_loss: 0.9016 - val_acc: 0.5000\n",
            "Epoch 189/300\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1465 - acc: 1.0000 - val_loss: 0.8740 - val_acc: 0.5000\n",
            "Epoch 190/300\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1475 - acc: 1.0000 - val_loss: 0.8736 - val_acc: 0.4500\n",
            "Epoch 191/300\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1444 - acc: 0.9833 - val_loss: 0.9006 - val_acc: 0.4000\n",
            "Epoch 192/300\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1469 - acc: 1.0000 - val_loss: 0.9286 - val_acc: 0.4000\n",
            "Epoch 193/300\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1428 - acc: 1.0000 - val_loss: 0.9382 - val_acc: 0.4000\n",
            "Epoch 194/300\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1381 - acc: 0.9833 - val_loss: 0.9323 - val_acc: 0.4500\n",
            "Epoch 195/300\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1552 - acc: 1.0000 - val_loss: 0.9521 - val_acc: 0.4000\n",
            "Epoch 196/300\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1570 - acc: 0.9833 - val_loss: 0.9527 - val_acc: 0.4000\n",
            "Epoch 197/300\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1398 - acc: 1.0000 - val_loss: 0.9475 - val_acc: 0.4000\n",
            "Epoch 198/300\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1258 - acc: 1.0000 - val_loss: 0.9099 - val_acc: 0.4500\n",
            "Epoch 199/300\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1214 - acc: 1.0000 - val_loss: 0.9084 - val_acc: 0.5000\n",
            "Epoch 200/300\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1285 - acc: 1.0000 - val_loss: 0.9133 - val_acc: 0.4000\n",
            "Epoch 201/300\n",
            "\n",
            "Epoch 00201: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1464 - acc: 1.0000 - val_loss: 0.9229 - val_acc: 0.4000\n",
            "Epoch 202/300\n",
            "\n",
            "Epoch 00202: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1497 - acc: 1.0000 - val_loss: 0.9579 - val_acc: 0.4000\n",
            "Epoch 203/300\n",
            "\n",
            "Epoch 00203: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1498 - acc: 1.0000 - val_loss: 0.9560 - val_acc: 0.4000\n",
            "Epoch 204/300\n",
            "\n",
            "Epoch 00204: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1539 - acc: 1.0000 - val_loss: 0.9634 - val_acc: 0.4000\n",
            "Epoch 205/300\n",
            "\n",
            "Epoch 00205: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1350 - acc: 0.9833 - val_loss: 0.9712 - val_acc: 0.3500\n",
            "Epoch 206/300\n",
            "\n",
            "Epoch 00206: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1223 - acc: 1.0000 - val_loss: 0.9816 - val_acc: 0.4000\n",
            "Epoch 207/300\n",
            "\n",
            "Epoch 00207: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1327 - acc: 1.0000 - val_loss: 0.9638 - val_acc: 0.4000\n",
            "Epoch 208/300\n",
            "\n",
            "Epoch 00208: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1276 - acc: 1.0000 - val_loss: 0.9447 - val_acc: 0.5000\n",
            "Epoch 209/300\n",
            "\n",
            "Epoch 00209: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1454 - acc: 1.0000 - val_loss: 0.9239 - val_acc: 0.5000\n",
            "Epoch 210/300\n",
            "\n",
            "Epoch 00210: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1191 - acc: 1.0000 - val_loss: 0.9159 - val_acc: 0.4500\n",
            "Epoch 211/300\n",
            "\n",
            "Epoch 00211: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1459 - acc: 1.0000 - val_loss: 0.9184 - val_acc: 0.4500\n",
            "Epoch 212/300\n",
            "\n",
            "Epoch 00212: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1349 - acc: 0.9833 - val_loss: 0.9256 - val_acc: 0.4500\n",
            "Epoch 213/300\n",
            "\n",
            "Epoch 00213: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1285 - acc: 1.0000 - val_loss: 0.9179 - val_acc: 0.4500\n",
            "Epoch 214/300\n",
            "\n",
            "Epoch 00214: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1184 - acc: 1.0000 - val_loss: 0.8886 - val_acc: 0.5000\n",
            "Epoch 215/300\n",
            "\n",
            "Epoch 00215: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1293 - acc: 1.0000 - val_loss: 0.9185 - val_acc: 0.4500\n",
            "Epoch 216/300\n",
            "\n",
            "Epoch 00216: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1446 - acc: 1.0000 - val_loss: 0.9562 - val_acc: 0.4500\n",
            "Epoch 217/300\n",
            "\n",
            "Epoch 00217: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1339 - acc: 1.0000 - val_loss: 0.9743 - val_acc: 0.4000\n",
            "Epoch 218/300\n",
            "\n",
            "Epoch 00218: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1606 - acc: 0.9333 - val_loss: 0.9560 - val_acc: 0.4000\n",
            "Epoch 219/300\n",
            "\n",
            "Epoch 00219: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1238 - acc: 1.0000 - val_loss: 0.9525 - val_acc: 0.4500\n",
            "Epoch 220/300\n",
            "\n",
            "Epoch 00220: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1139 - acc: 1.0000 - val_loss: 0.9330 - val_acc: 0.4000\n",
            "Epoch 221/300\n",
            "\n",
            "Epoch 00221: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1110 - acc: 1.0000 - val_loss: 0.9328 - val_acc: 0.5000\n",
            "Epoch 222/300\n",
            "\n",
            "Epoch 00222: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1326 - acc: 0.9833 - val_loss: 0.9300 - val_acc: 0.4500\n",
            "Epoch 223/300\n",
            "\n",
            "Epoch 00223: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1141 - acc: 1.0000 - val_loss: 0.9290 - val_acc: 0.5000\n",
            "Epoch 224/300\n",
            "\n",
            "Epoch 00224: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1117 - acc: 1.0000 - val_loss: 0.9123 - val_acc: 0.5000\n",
            "Epoch 225/300\n",
            "\n",
            "Epoch 00225: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1343 - acc: 1.0000 - val_loss: 0.9219 - val_acc: 0.4500\n",
            "Epoch 226/300\n",
            "\n",
            "Epoch 00226: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1298 - acc: 1.0000 - val_loss: 0.9233 - val_acc: 0.5000\n",
            "Epoch 227/300\n",
            "\n",
            "Epoch 00227: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1251 - acc: 1.0000 - val_loss: 0.8953 - val_acc: 0.4500\n",
            "Epoch 228/300\n",
            "\n",
            "Epoch 00228: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1313 - acc: 1.0000 - val_loss: 0.8829 - val_acc: 0.4500\n",
            "Epoch 229/300\n",
            "\n",
            "Epoch 00229: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1400 - acc: 1.0000 - val_loss: 0.8945 - val_acc: 0.4500\n",
            "Epoch 230/300\n",
            "\n",
            "Epoch 00230: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1039 - acc: 1.0000 - val_loss: 0.9272 - val_acc: 0.4500\n",
            "Epoch 231/300\n",
            "\n",
            "Epoch 00231: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1381 - acc: 1.0000 - val_loss: 0.9757 - val_acc: 0.3500\n",
            "Epoch 232/300\n",
            "\n",
            "Epoch 00232: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1190 - acc: 1.0000 - val_loss: 0.9810 - val_acc: 0.3500\n",
            "Epoch 233/300\n",
            "\n",
            "Epoch 00233: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1264 - acc: 1.0000 - val_loss: 0.9529 - val_acc: 0.4000\n",
            "Epoch 234/300\n",
            "\n",
            "Epoch 00234: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1333 - acc: 1.0000 - val_loss: 0.9274 - val_acc: 0.4000\n",
            "Epoch 235/300\n",
            "\n",
            "Epoch 00235: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1151 - acc: 1.0000 - val_loss: 0.9514 - val_acc: 0.4000\n",
            "Epoch 236/300\n",
            "\n",
            "Epoch 00236: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1307 - acc: 1.0000 - val_loss: 0.9474 - val_acc: 0.4000\n",
            "Epoch 237/300\n",
            "\n",
            "Epoch 00237: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1144 - acc: 1.0000 - val_loss: 0.9682 - val_acc: 0.4000\n",
            "Epoch 238/300\n",
            "\n",
            "Epoch 00238: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1195 - acc: 1.0000 - val_loss: 0.9765 - val_acc: 0.4000\n",
            "Epoch 239/300\n",
            "\n",
            "Epoch 00239: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1059 - acc: 1.0000 - val_loss: 0.9580 - val_acc: 0.4000\n",
            "Epoch 240/300\n",
            "\n",
            "Epoch 00240: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1117 - acc: 0.9833 - val_loss: 0.9557 - val_acc: 0.4000\n",
            "Epoch 241/300\n",
            "\n",
            "Epoch 00241: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1363 - acc: 0.9833 - val_loss: 0.9792 - val_acc: 0.4000\n",
            "Epoch 242/300\n",
            "\n",
            "Epoch 00242: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1333 - acc: 1.0000 - val_loss: 0.9787 - val_acc: 0.3500\n",
            "Epoch 243/300\n",
            "\n",
            "Epoch 00243: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1237 - acc: 1.0000 - val_loss: 0.9728 - val_acc: 0.3500\n",
            "Epoch 244/300\n",
            "\n",
            "Epoch 00244: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1273 - acc: 1.0000 - val_loss: 0.9715 - val_acc: 0.3500\n",
            "Epoch 245/300\n",
            "\n",
            "Epoch 00245: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1387 - acc: 1.0000 - val_loss: 0.9476 - val_acc: 0.4000\n",
            "Epoch 246/300\n",
            "\n",
            "Epoch 00246: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1100 - acc: 1.0000 - val_loss: 0.9519 - val_acc: 0.4000\n",
            "Epoch 247/300\n",
            "\n",
            "Epoch 00247: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1150 - acc: 0.9833 - val_loss: 0.9480 - val_acc: 0.4000\n",
            "Epoch 248/300\n",
            "\n",
            "Epoch 00248: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1317 - acc: 0.9833 - val_loss: 0.9610 - val_acc: 0.3500\n",
            "Epoch 249/300\n",
            "\n",
            "Epoch 00249: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1220 - acc: 1.0000 - val_loss: 0.9650 - val_acc: 0.3500\n",
            "Epoch 250/300\n",
            "\n",
            "Epoch 00250: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1294 - acc: 0.9833 - val_loss: 0.9683 - val_acc: 0.3500\n",
            "Epoch 251/300\n",
            "\n",
            "Epoch 00251: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1196 - acc: 1.0000 - val_loss: 0.9611 - val_acc: 0.3500\n",
            "Epoch 252/300\n",
            "\n",
            "Epoch 00252: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1312 - acc: 1.0000 - val_loss: 0.9806 - val_acc: 0.3500\n",
            "Epoch 253/300\n",
            "\n",
            "Epoch 00253: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1199 - acc: 1.0000 - val_loss: 1.0065 - val_acc: 0.3500\n",
            "Epoch 254/300\n",
            "\n",
            "Epoch 00254: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1195 - acc: 1.0000 - val_loss: 1.0284 - val_acc: 0.3500\n",
            "Epoch 255/300\n",
            "\n",
            "Epoch 00255: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1356 - acc: 1.0000 - val_loss: 1.0283 - val_acc: 0.3500\n",
            "Epoch 256/300\n",
            "\n",
            "Epoch 00256: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1274 - acc: 1.0000 - val_loss: 0.9844 - val_acc: 0.3500\n",
            "Epoch 257/300\n",
            "\n",
            "Epoch 00257: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1110 - acc: 1.0000 - val_loss: 0.9542 - val_acc: 0.4000\n",
            "Epoch 258/300\n",
            "\n",
            "Epoch 00258: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1016 - acc: 1.0000 - val_loss: 0.9477 - val_acc: 0.3500\n",
            "Epoch 259/300\n",
            "\n",
            "Epoch 00259: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1156 - acc: 1.0000 - val_loss: 0.9561 - val_acc: 0.4000\n",
            "Epoch 260/300\n",
            "\n",
            "Epoch 00260: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1223 - acc: 0.9833 - val_loss: 0.9481 - val_acc: 0.4000\n",
            "Epoch 261/300\n",
            "\n",
            "Epoch 00261: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1080 - acc: 1.0000 - val_loss: 0.9724 - val_acc: 0.3500\n",
            "Epoch 262/300\n",
            "\n",
            "Epoch 00262: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1107 - acc: 1.0000 - val_loss: 1.0172 - val_acc: 0.3500\n",
            "Epoch 263/300\n",
            "\n",
            "Epoch 00263: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1107 - acc: 1.0000 - val_loss: 1.0108 - val_acc: 0.3500\n",
            "Epoch 264/300\n",
            "\n",
            "Epoch 00264: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1046 - acc: 1.0000 - val_loss: 1.0186 - val_acc: 0.4000\n",
            "Epoch 265/300\n",
            "\n",
            "Epoch 00265: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1002 - acc: 1.0000 - val_loss: 0.9801 - val_acc: 0.4000\n",
            "Epoch 266/300\n",
            "\n",
            "Epoch 00266: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1037 - acc: 1.0000 - val_loss: 0.9557 - val_acc: 0.5000\n",
            "Epoch 267/300\n",
            "\n",
            "Epoch 00267: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1206 - acc: 0.9667 - val_loss: 0.9596 - val_acc: 0.5000\n",
            "Epoch 268/300\n",
            "\n",
            "Epoch 00268: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1192 - acc: 1.0000 - val_loss: 0.9741 - val_acc: 0.4000\n",
            "Epoch 269/300\n",
            "\n",
            "Epoch 00269: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.0991 - acc: 1.0000 - val_loss: 0.9782 - val_acc: 0.4500\n",
            "Epoch 270/300\n",
            "\n",
            "Epoch 00270: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1134 - acc: 1.0000 - val_loss: 0.9541 - val_acc: 0.4500\n",
            "Epoch 271/300\n",
            "\n",
            "Epoch 00271: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1039 - acc: 1.0000 - val_loss: 0.9631 - val_acc: 0.4500\n",
            "Epoch 272/300\n",
            "\n",
            "Epoch 00272: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.0967 - acc: 1.0000 - val_loss: 0.9912 - val_acc: 0.4500\n",
            "Epoch 273/300\n",
            "\n",
            "Epoch 00273: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1179 - acc: 0.9833 - val_loss: 1.0048 - val_acc: 0.3500\n",
            "Epoch 274/300\n",
            "\n",
            "Epoch 00274: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.0930 - acc: 1.0000 - val_loss: 1.0102 - val_acc: 0.3500\n",
            "Epoch 275/300\n",
            "\n",
            "Epoch 00275: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1016 - acc: 1.0000 - val_loss: 0.9835 - val_acc: 0.4000\n",
            "Epoch 276/300\n",
            "\n",
            "Epoch 00276: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1176 - acc: 0.9833 - val_loss: 0.9607 - val_acc: 0.4000\n",
            "Epoch 277/300\n",
            "\n",
            "Epoch 00277: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1059 - acc: 1.0000 - val_loss: 0.9771 - val_acc: 0.3500\n",
            "Epoch 278/300\n",
            "\n",
            "Epoch 00278: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1053 - acc: 1.0000 - val_loss: 0.9740 - val_acc: 0.4000\n",
            "Epoch 279/300\n",
            "\n",
            "Epoch 00279: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.0984 - acc: 1.0000 - val_loss: 0.9374 - val_acc: 0.5000\n",
            "Epoch 280/300\n",
            "\n",
            "Epoch 00280: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.0981 - acc: 1.0000 - val_loss: 0.9569 - val_acc: 0.4000\n",
            "Epoch 281/300\n",
            "\n",
            "Epoch 00281: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.0930 - acc: 1.0000 - val_loss: 0.9887 - val_acc: 0.3500\n",
            "Epoch 282/300\n",
            "\n",
            "Epoch 00282: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1161 - acc: 1.0000 - val_loss: 1.0099 - val_acc: 0.3500\n",
            "Epoch 283/300\n",
            "\n",
            "Epoch 00283: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1120 - acc: 1.0000 - val_loss: 1.0027 - val_acc: 0.4000\n",
            "Epoch 284/300\n",
            "\n",
            "Epoch 00284: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1146 - acc: 0.9833 - val_loss: 0.9592 - val_acc: 0.4500\n",
            "Epoch 285/300\n",
            "\n",
            "Epoch 00285: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1195 - acc: 0.9833 - val_loss: 0.9833 - val_acc: 0.4000\n",
            "Epoch 286/300\n",
            "\n",
            "Epoch 00286: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1264 - acc: 0.9833 - val_loss: 0.9840 - val_acc: 0.3500\n",
            "Epoch 287/300\n",
            "\n",
            "Epoch 00287: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1065 - acc: 1.0000 - val_loss: 0.9957 - val_acc: 0.3500\n",
            "Epoch 288/300\n",
            "\n",
            "Epoch 00288: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1113 - acc: 1.0000 - val_loss: 1.0443 - val_acc: 0.3500\n",
            "Epoch 289/300\n",
            "\n",
            "Epoch 00289: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.0922 - acc: 1.0000 - val_loss: 1.1005 - val_acc: 0.4000\n",
            "Epoch 290/300\n",
            "\n",
            "Epoch 00290: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1021 - acc: 1.0000 - val_loss: 1.1033 - val_acc: 0.4000\n",
            "Epoch 291/300\n",
            "\n",
            "Epoch 00291: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1005 - acc: 1.0000 - val_loss: 1.0573 - val_acc: 0.4000\n",
            "Epoch 292/300\n",
            "\n",
            "Epoch 00292: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1086 - acc: 1.0000 - val_loss: 1.0121 - val_acc: 0.4000\n",
            "Epoch 293/300\n",
            "\n",
            "Epoch 00293: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.0940 - acc: 1.0000 - val_loss: 1.0139 - val_acc: 0.4000\n",
            "Epoch 294/300\n",
            "\n",
            "Epoch 00294: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1107 - acc: 1.0000 - val_loss: 1.0303 - val_acc: 0.3500\n",
            "Epoch 295/300\n",
            "\n",
            "Epoch 00295: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.0904 - acc: 1.0000 - val_loss: 1.0183 - val_acc: 0.4000\n",
            "Epoch 296/300\n",
            "\n",
            "Epoch 00296: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.0938 - acc: 1.0000 - val_loss: 1.0123 - val_acc: 0.4500\n",
            "Epoch 297/300\n",
            "\n",
            "Epoch 00297: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.0965 - acc: 1.0000 - val_loss: 0.9885 - val_acc: 0.5000\n",
            "Epoch 298/300\n",
            "\n",
            "Epoch 00298: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.1059 - acc: 1.0000 - val_loss: 0.9954 - val_acc: 0.5000\n",
            "Epoch 299/300\n",
            "\n",
            "Epoch 00299: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.0857 - acc: 1.0000 - val_loss: 1.0240 - val_acc: 0.4000\n",
            "Epoch 300/300\n",
            "\n",
            "Epoch 00300: val_loss did not improve from 0.67456\n",
            "60/60 - 0s - loss: 0.0993 - acc: 1.0000 - val_loss: 1.0284 - val_acc: 0.4500\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "# Evaluate on test data\n",
            "40/40 [==============================] - 0s 2ms/sample - loss: 1.2323 - acc: 0.4750\n",
            "test loss, test acc: [1.232273648586124, 0.475]\n",
            "EEG_Deep/Data2A/parsed_P09T.mat\n",
            "Filtering of Training Data Finished\n",
            "EEG_Deep/Data2A/parsed_P09E.mat\n",
            "Filtering of Testing Data Finished\n",
            "(1, 2)\n",
            "Finding labels in training data\n",
            "Finding labels in testing data\n",
            "[2 1 1 2 1 2 1 1 2 1 2 2 1 1 1 1 2 1 2 1]\n",
            "(60, 12, 1536)\n",
            "(60,)\n",
            "(20, 12, 1536)\n",
            "(20,)\n",
            "[[0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]]\n",
            "X_train shape: (60, 1, 12, 1536)\n",
            "60 train samples\n",
            "20 val samples\n",
            "(40, 12, 1536)\n",
            "(40, 1)\n",
            "X_train shape: (40, 1, 12, 1536)\n",
            "40 train samples\n",
            "Train on 60 samples, validate on 20 samples\n",
            "Epoch 1/300\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.69395, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 1s - loss: 0.6954 - acc: 0.5500 - val_loss: 0.6939 - val_acc: 0.4500\n",
            "Epoch 2/300\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.6719 - acc: 0.5833 - val_loss: 0.6952 - val_acc: 0.3500\n",
            "Epoch 3/300\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.6478 - acc: 0.6667 - val_loss: 0.6953 - val_acc: 0.3500\n",
            "Epoch 4/300\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.6353 - acc: 0.7000 - val_loss: 0.6966 - val_acc: 0.4000\n",
            "Epoch 5/300\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.6085 - acc: 0.7667 - val_loss: 0.6981 - val_acc: 0.4000\n",
            "Epoch 6/300\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.5851 - acc: 0.7833 - val_loss: 0.6990 - val_acc: 0.3000\n",
            "Epoch 7/300\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.5960 - acc: 0.7667 - val_loss: 0.6996 - val_acc: 0.3500\n",
            "Epoch 8/300\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.5746 - acc: 0.8167 - val_loss: 0.7006 - val_acc: 0.3500\n",
            "Epoch 9/300\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.5589 - acc: 0.8000 - val_loss: 0.7018 - val_acc: 0.3500\n",
            "Epoch 10/300\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.5498 - acc: 0.8167 - val_loss: 0.7032 - val_acc: 0.4000\n",
            "Epoch 11/300\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.5596 - acc: 0.8000 - val_loss: 0.7044 - val_acc: 0.4000\n",
            "Epoch 12/300\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.5210 - acc: 0.8167 - val_loss: 0.7059 - val_acc: 0.5000\n",
            "Epoch 13/300\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.5178 - acc: 0.8000 - val_loss: 0.7070 - val_acc: 0.5000\n",
            "Epoch 14/300\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.4981 - acc: 0.8500 - val_loss: 0.7094 - val_acc: 0.5000\n",
            "Epoch 15/300\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.5123 - acc: 0.7833 - val_loss: 0.7111 - val_acc: 0.5000\n",
            "Epoch 16/300\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.4962 - acc: 0.8667 - val_loss: 0.7145 - val_acc: 0.4500\n",
            "Epoch 17/300\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.5038 - acc: 0.8333 - val_loss: 0.7184 - val_acc: 0.4500\n",
            "Epoch 18/300\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.4829 - acc: 0.8000 - val_loss: 0.7207 - val_acc: 0.4500\n",
            "Epoch 19/300\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.4809 - acc: 0.8333 - val_loss: 0.7246 - val_acc: 0.4500\n",
            "Epoch 20/300\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.4697 - acc: 0.8333 - val_loss: 0.7271 - val_acc: 0.4000\n",
            "Epoch 21/300\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.4593 - acc: 0.8833 - val_loss: 0.7316 - val_acc: 0.4000\n",
            "Epoch 22/300\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.4592 - acc: 0.8667 - val_loss: 0.7348 - val_acc: 0.4000\n",
            "Epoch 23/300\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.4384 - acc: 0.9000 - val_loss: 0.7379 - val_acc: 0.4000\n",
            "Epoch 24/300\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.4460 - acc: 0.8833 - val_loss: 0.7398 - val_acc: 0.4000\n",
            "Epoch 25/300\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.4329 - acc: 0.8333 - val_loss: 0.7435 - val_acc: 0.4000\n",
            "Epoch 26/300\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.4292 - acc: 0.8667 - val_loss: 0.7459 - val_acc: 0.4000\n",
            "Epoch 27/300\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.4230 - acc: 0.8833 - val_loss: 0.7481 - val_acc: 0.4000\n",
            "Epoch 28/300\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.3962 - acc: 0.9333 - val_loss: 0.7502 - val_acc: 0.4000\n",
            "Epoch 29/300\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.4078 - acc: 0.9000 - val_loss: 0.7501 - val_acc: 0.4000\n",
            "Epoch 30/300\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.3995 - acc: 0.9000 - val_loss: 0.7500 - val_acc: 0.4000\n",
            "Epoch 31/300\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.3950 - acc: 0.8833 - val_loss: 0.7480 - val_acc: 0.4000\n",
            "Epoch 32/300\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.4074 - acc: 0.8833 - val_loss: 0.7470 - val_acc: 0.4000\n",
            "Epoch 33/300\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.3809 - acc: 0.9000 - val_loss: 0.7479 - val_acc: 0.4000\n",
            "Epoch 34/300\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.4175 - acc: 0.8833 - val_loss: 0.7487 - val_acc: 0.4000\n",
            "Epoch 35/300\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.3949 - acc: 0.9000 - val_loss: 0.7515 - val_acc: 0.4000\n",
            "Epoch 36/300\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.3982 - acc: 0.9000 - val_loss: 0.7543 - val_acc: 0.4000\n",
            "Epoch 37/300\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.3819 - acc: 0.9000 - val_loss: 0.7557 - val_acc: 0.4000\n",
            "Epoch 38/300\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.4146 - acc: 0.8500 - val_loss: 0.7553 - val_acc: 0.4000\n",
            "Epoch 39/300\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.3537 - acc: 0.8833 - val_loss: 0.7547 - val_acc: 0.4500\n",
            "Epoch 40/300\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.3855 - acc: 0.8500 - val_loss: 0.7571 - val_acc: 0.4000\n",
            "Epoch 41/300\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.3776 - acc: 0.9167 - val_loss: 0.7582 - val_acc: 0.4000\n",
            "Epoch 42/300\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.3833 - acc: 0.9000 - val_loss: 0.7572 - val_acc: 0.4500\n",
            "Epoch 43/300\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.3835 - acc: 0.8833 - val_loss: 0.7571 - val_acc: 0.4500\n",
            "Epoch 44/300\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.3351 - acc: 0.9667 - val_loss: 0.7566 - val_acc: 0.4500\n",
            "Epoch 45/300\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.3908 - acc: 0.8500 - val_loss: 0.7551 - val_acc: 0.4500\n",
            "Epoch 46/300\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.3457 - acc: 0.9667 - val_loss: 0.7553 - val_acc: 0.4500\n",
            "Epoch 47/300\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.3833 - acc: 0.8833 - val_loss: 0.7565 - val_acc: 0.4500\n",
            "Epoch 48/300\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.3688 - acc: 0.9167 - val_loss: 0.7597 - val_acc: 0.4500\n",
            "Epoch 49/300\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.3687 - acc: 0.9000 - val_loss: 0.7654 - val_acc: 0.4500\n",
            "Epoch 50/300\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.3394 - acc: 0.9333 - val_loss: 0.7701 - val_acc: 0.5000\n",
            "Epoch 51/300\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.3486 - acc: 0.9000 - val_loss: 0.7731 - val_acc: 0.5000\n",
            "Epoch 52/300\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.3467 - acc: 0.9333 - val_loss: 0.7733 - val_acc: 0.5000\n",
            "Epoch 53/300\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.3553 - acc: 0.9167 - val_loss: 0.7721 - val_acc: 0.5000\n",
            "Epoch 54/300\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.3473 - acc: 0.9167 - val_loss: 0.7733 - val_acc: 0.5000\n",
            "Epoch 55/300\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.3401 - acc: 0.9167 - val_loss: 0.7699 - val_acc: 0.5000\n",
            "Epoch 56/300\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.3368 - acc: 0.9167 - val_loss: 0.7690 - val_acc: 0.5000\n",
            "Epoch 57/300\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.3279 - acc: 0.9667 - val_loss: 0.7605 - val_acc: 0.4000\n",
            "Epoch 58/300\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.3623 - acc: 0.9000 - val_loss: 0.7549 - val_acc: 0.4000\n",
            "Epoch 59/300\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.3442 - acc: 0.9167 - val_loss: 0.7559 - val_acc: 0.4000\n",
            "Epoch 60/300\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.3229 - acc: 0.9500 - val_loss: 0.7594 - val_acc: 0.4500\n",
            "Epoch 61/300\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.3182 - acc: 0.9667 - val_loss: 0.7655 - val_acc: 0.4500\n",
            "Epoch 62/300\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.3534 - acc: 0.9000 - val_loss: 0.7725 - val_acc: 0.4500\n",
            "Epoch 63/300\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.3243 - acc: 0.9667 - val_loss: 0.7758 - val_acc: 0.4000\n",
            "Epoch 64/300\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.3515 - acc: 0.8833 - val_loss: 0.7757 - val_acc: 0.4000\n",
            "Epoch 65/300\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.3042 - acc: 0.9500 - val_loss: 0.7769 - val_acc: 0.4000\n",
            "Epoch 66/300\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.3326 - acc: 0.9000 - val_loss: 0.7802 - val_acc: 0.4500\n",
            "Epoch 67/300\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.3386 - acc: 0.9167 - val_loss: 0.7785 - val_acc: 0.4000\n",
            "Epoch 68/300\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.3515 - acc: 0.9167 - val_loss: 0.7773 - val_acc: 0.4500\n",
            "Epoch 69/300\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.3115 - acc: 0.9667 - val_loss: 0.7798 - val_acc: 0.5000\n",
            "Epoch 70/300\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.3111 - acc: 0.9167 - val_loss: 0.7887 - val_acc: 0.5500\n",
            "Epoch 71/300\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.3243 - acc: 0.9333 - val_loss: 0.7938 - val_acc: 0.5000\n",
            "Epoch 72/300\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.3113 - acc: 0.9667 - val_loss: 0.7925 - val_acc: 0.5000\n",
            "Epoch 73/300\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.3004 - acc: 0.9167 - val_loss: 0.7915 - val_acc: 0.5000\n",
            "Epoch 74/300\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.3296 - acc: 0.9167 - val_loss: 0.7822 - val_acc: 0.5000\n",
            "Epoch 75/300\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.2969 - acc: 0.9500 - val_loss: 0.7679 - val_acc: 0.5500\n",
            "Epoch 76/300\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.3337 - acc: 0.9167 - val_loss: 0.7644 - val_acc: 0.5000\n",
            "Epoch 77/300\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.3290 - acc: 0.9333 - val_loss: 0.7647 - val_acc: 0.5000\n",
            "Epoch 78/300\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.2836 - acc: 0.9833 - val_loss: 0.7726 - val_acc: 0.5500\n",
            "Epoch 79/300\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.3056 - acc: 0.9667 - val_loss: 0.7736 - val_acc: 0.5500\n",
            "Epoch 80/300\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.3280 - acc: 0.8833 - val_loss: 0.7738 - val_acc: 0.5500\n",
            "Epoch 81/300\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.3071 - acc: 0.9167 - val_loss: 0.7740 - val_acc: 0.5500\n",
            "Epoch 82/300\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.3137 - acc: 0.9500 - val_loss: 0.7797 - val_acc: 0.5500\n",
            "Epoch 83/300\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.3330 - acc: 0.8833 - val_loss: 0.7810 - val_acc: 0.5000\n",
            "Epoch 84/300\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.3261 - acc: 0.9167 - val_loss: 0.7836 - val_acc: 0.5000\n",
            "Epoch 85/300\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.3120 - acc: 0.9500 - val_loss: 0.7845 - val_acc: 0.5000\n",
            "Epoch 86/300\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.3402 - acc: 0.9167 - val_loss: 0.7846 - val_acc: 0.5000\n",
            "Epoch 87/300\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.3145 - acc: 0.9500 - val_loss: 0.7788 - val_acc: 0.5500\n",
            "Epoch 88/300\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.3135 - acc: 0.9500 - val_loss: 0.7783 - val_acc: 0.5000\n",
            "Epoch 89/300\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.3110 - acc: 0.9333 - val_loss: 0.7793 - val_acc: 0.5000\n",
            "Epoch 90/300\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.3045 - acc: 0.9667 - val_loss: 0.7809 - val_acc: 0.5000\n",
            "Epoch 91/300\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.3249 - acc: 0.9333 - val_loss: 0.7881 - val_acc: 0.5000\n",
            "Epoch 92/300\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.3236 - acc: 0.9667 - val_loss: 0.7894 - val_acc: 0.5000\n",
            "Epoch 93/300\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.2842 - acc: 0.9667 - val_loss: 0.7896 - val_acc: 0.5000\n",
            "Epoch 94/300\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.3171 - acc: 0.9000 - val_loss: 0.7811 - val_acc: 0.5000\n",
            "Epoch 95/300\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.3052 - acc: 0.9000 - val_loss: 0.7794 - val_acc: 0.5000\n",
            "Epoch 96/300\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.2841 - acc: 0.9333 - val_loss: 0.7727 - val_acc: 0.5000\n",
            "Epoch 97/300\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.2992 - acc: 0.9167 - val_loss: 0.7667 - val_acc: 0.5000\n",
            "Epoch 98/300\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.2671 - acc: 0.9833 - val_loss: 0.7669 - val_acc: 0.4500\n",
            "Epoch 99/300\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.2887 - acc: 0.9500 - val_loss: 0.7652 - val_acc: 0.4500\n",
            "Epoch 100/300\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.3122 - acc: 0.9000 - val_loss: 0.7591 - val_acc: 0.5500\n",
            "Epoch 101/300\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.2769 - acc: 0.9667 - val_loss: 0.7560 - val_acc: 0.5000\n",
            "Epoch 102/300\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.2885 - acc: 0.9500 - val_loss: 0.7608 - val_acc: 0.5000\n",
            "Epoch 103/300\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.2877 - acc: 0.9500 - val_loss: 0.7599 - val_acc: 0.5000\n",
            "Epoch 104/300\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.2661 - acc: 0.9333 - val_loss: 0.7645 - val_acc: 0.5000\n",
            "Epoch 105/300\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.2930 - acc: 0.9833 - val_loss: 0.7583 - val_acc: 0.6000\n",
            "Epoch 106/300\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.3025 - acc: 0.9167 - val_loss: 0.7575 - val_acc: 0.6000\n",
            "Epoch 107/300\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.2586 - acc: 0.9500 - val_loss: 0.7644 - val_acc: 0.5500\n",
            "Epoch 108/300\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.2876 - acc: 0.9667 - val_loss: 0.7603 - val_acc: 0.5500\n",
            "Epoch 109/300\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.2981 - acc: 0.9500 - val_loss: 0.7548 - val_acc: 0.6000\n",
            "Epoch 110/300\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.3030 - acc: 0.9167 - val_loss: 0.7495 - val_acc: 0.6000\n",
            "Epoch 111/300\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.2796 - acc: 0.9500 - val_loss: 0.7486 - val_acc: 0.5500\n",
            "Epoch 112/300\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.2477 - acc: 0.9667 - val_loss: 0.7590 - val_acc: 0.5500\n",
            "Epoch 113/300\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.2633 - acc: 0.9667 - val_loss: 0.7649 - val_acc: 0.5500\n",
            "Epoch 114/300\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.2885 - acc: 0.9000 - val_loss: 0.7812 - val_acc: 0.5000\n",
            "Epoch 115/300\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.2884 - acc: 0.9500 - val_loss: 0.7893 - val_acc: 0.5000\n",
            "Epoch 116/300\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.3055 - acc: 0.9500 - val_loss: 0.7891 - val_acc: 0.5000\n",
            "Epoch 117/300\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.2578 - acc: 0.9833 - val_loss: 0.7794 - val_acc: 0.5000\n",
            "Epoch 118/300\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.2799 - acc: 0.9667 - val_loss: 0.7715 - val_acc: 0.6000\n",
            "Epoch 119/300\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.2581 - acc: 0.9833 - val_loss: 0.7644 - val_acc: 0.6000\n",
            "Epoch 120/300\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.2551 - acc: 1.0000 - val_loss: 0.7645 - val_acc: 0.5500\n",
            "Epoch 121/300\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.2709 - acc: 0.9667 - val_loss: 0.7686 - val_acc: 0.5500\n",
            "Epoch 122/300\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.2560 - acc: 0.9667 - val_loss: 0.7759 - val_acc: 0.5500\n",
            "Epoch 123/300\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.2686 - acc: 0.9667 - val_loss: 0.7712 - val_acc: 0.6000\n",
            "Epoch 124/300\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.2461 - acc: 0.9833 - val_loss: 0.7703 - val_acc: 0.6000\n",
            "Epoch 125/300\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.2658 - acc: 0.9333 - val_loss: 0.7630 - val_acc: 0.6000\n",
            "Epoch 126/300\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.2305 - acc: 0.9833 - val_loss: 0.7490 - val_acc: 0.6000\n",
            "Epoch 127/300\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.2606 - acc: 0.9667 - val_loss: 0.7484 - val_acc: 0.6000\n",
            "Epoch 128/300\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.2720 - acc: 0.9333 - val_loss: 0.7494 - val_acc: 0.6000\n",
            "Epoch 129/300\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.2613 - acc: 0.9333 - val_loss: 0.7373 - val_acc: 0.6000\n",
            "Epoch 130/300\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.2802 - acc: 0.9500 - val_loss: 0.7341 - val_acc: 0.6000\n",
            "Epoch 131/300\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.2749 - acc: 0.9167 - val_loss: 0.7348 - val_acc: 0.6000\n",
            "Epoch 132/300\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.2487 - acc: 0.9667 - val_loss: 0.7358 - val_acc: 0.5500\n",
            "Epoch 133/300\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.2629 - acc: 0.9833 - val_loss: 0.7430 - val_acc: 0.5500\n",
            "Epoch 134/300\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.2528 - acc: 0.9333 - val_loss: 0.7470 - val_acc: 0.5500\n",
            "Epoch 135/300\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.2516 - acc: 0.9500 - val_loss: 0.7403 - val_acc: 0.5500\n",
            "Epoch 136/300\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.2264 - acc: 0.9667 - val_loss: 0.7369 - val_acc: 0.6000\n",
            "Epoch 137/300\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.2252 - acc: 0.9667 - val_loss: 0.7460 - val_acc: 0.6000\n",
            "Epoch 138/300\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.2868 - acc: 0.9500 - val_loss: 0.7458 - val_acc: 0.6000\n",
            "Epoch 139/300\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.2412 - acc: 0.9500 - val_loss: 0.7614 - val_acc: 0.5500\n",
            "Epoch 140/300\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.2442 - acc: 0.9833 - val_loss: 0.7725 - val_acc: 0.5500\n",
            "Epoch 141/300\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.2334 - acc: 0.9833 - val_loss: 0.7680 - val_acc: 0.5000\n",
            "Epoch 142/300\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.2666 - acc: 0.9167 - val_loss: 0.7654 - val_acc: 0.5500\n",
            "Epoch 143/300\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.2343 - acc: 0.9667 - val_loss: 0.7563 - val_acc: 0.6000\n",
            "Epoch 144/300\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.2533 - acc: 0.9833 - val_loss: 0.7481 - val_acc: 0.6000\n",
            "Epoch 145/300\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.2289 - acc: 0.9667 - val_loss: 0.7483 - val_acc: 0.6000\n",
            "Epoch 146/300\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.2756 - acc: 0.9333 - val_loss: 0.7495 - val_acc: 0.6500\n",
            "Epoch 147/300\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.2207 - acc: 0.9667 - val_loss: 0.7541 - val_acc: 0.6500\n",
            "Epoch 148/300\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.2528 - acc: 0.9667 - val_loss: 0.7554 - val_acc: 0.6000\n",
            "Epoch 149/300\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.2864 - acc: 0.9333 - val_loss: 0.7447 - val_acc: 0.6000\n",
            "Epoch 150/300\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.2347 - acc: 0.9833 - val_loss: 0.7279 - val_acc: 0.5500\n",
            "Epoch 151/300\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.2152 - acc: 1.0000 - val_loss: 0.7286 - val_acc: 0.5500\n",
            "Epoch 152/300\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.2376 - acc: 0.9667 - val_loss: 0.7367 - val_acc: 0.5500\n",
            "Epoch 153/300\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.2516 - acc: 0.9500 - val_loss: 0.7502 - val_acc: 0.6500\n",
            "Epoch 154/300\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.2443 - acc: 0.9500 - val_loss: 0.7457 - val_acc: 0.6000\n",
            "Epoch 155/300\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.2796 - acc: 0.9333 - val_loss: 0.7352 - val_acc: 0.5500\n",
            "Epoch 156/300\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.2338 - acc: 0.9667 - val_loss: 0.7322 - val_acc: 0.5500\n",
            "Epoch 157/300\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.2427 - acc: 0.9667 - val_loss: 0.7448 - val_acc: 0.6000\n",
            "Epoch 158/300\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.2098 - acc: 0.9833 - val_loss: 0.7476 - val_acc: 0.6500\n",
            "Epoch 159/300\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.2188 - acc: 0.9833 - val_loss: 0.7554 - val_acc: 0.6500\n",
            "Epoch 160/300\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.2103 - acc: 0.9833 - val_loss: 0.7480 - val_acc: 0.6500\n",
            "Epoch 161/300\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.2264 - acc: 0.9667 - val_loss: 0.7399 - val_acc: 0.6500\n",
            "Epoch 162/300\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.2436 - acc: 0.9667 - val_loss: 0.7420 - val_acc: 0.5500\n",
            "Epoch 163/300\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.2169 - acc: 0.9667 - val_loss: 0.7494 - val_acc: 0.5500\n",
            "Epoch 164/300\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.2470 - acc: 0.9500 - val_loss: 0.7401 - val_acc: 0.6000\n",
            "Epoch 165/300\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.2234 - acc: 0.9500 - val_loss: 0.7210 - val_acc: 0.6500\n",
            "Epoch 166/300\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.2601 - acc: 0.9333 - val_loss: 0.7047 - val_acc: 0.5500\n",
            "Epoch 167/300\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.2151 - acc: 0.9500 - val_loss: 0.7080 - val_acc: 0.5500\n",
            "Epoch 168/300\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.2263 - acc: 1.0000 - val_loss: 0.7171 - val_acc: 0.5500\n",
            "Epoch 169/300\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.2220 - acc: 0.9500 - val_loss: 0.7322 - val_acc: 0.5500\n",
            "Epoch 170/300\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.2084 - acc: 0.9833 - val_loss: 0.7372 - val_acc: 0.5500\n",
            "Epoch 171/300\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.2124 - acc: 0.9833 - val_loss: 0.7386 - val_acc: 0.5500\n",
            "Epoch 172/300\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.2284 - acc: 0.9667 - val_loss: 0.7331 - val_acc: 0.5500\n",
            "Epoch 173/300\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.2068 - acc: 1.0000 - val_loss: 0.7331 - val_acc: 0.5500\n",
            "Epoch 174/300\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.2252 - acc: 0.9500 - val_loss: 0.7313 - val_acc: 0.5500\n",
            "Epoch 175/300\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.2296 - acc: 0.9667 - val_loss: 0.7283 - val_acc: 0.5500\n",
            "Epoch 176/300\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.2612 - acc: 0.9167 - val_loss: 0.7437 - val_acc: 0.5500\n",
            "Epoch 177/300\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.2436 - acc: 0.9667 - val_loss: 0.7487 - val_acc: 0.5500\n",
            "Epoch 178/300\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.1808 - acc: 0.9667 - val_loss: 0.7567 - val_acc: 0.5500\n",
            "Epoch 179/300\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.2110 - acc: 0.9667 - val_loss: 0.7717 - val_acc: 0.5000\n",
            "Epoch 180/300\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.2198 - acc: 0.9667 - val_loss: 0.7829 - val_acc: 0.5000\n",
            "Epoch 181/300\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.2368 - acc: 0.9667 - val_loss: 0.7678 - val_acc: 0.5000\n",
            "Epoch 182/300\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.1928 - acc: 0.9833 - val_loss: 0.7445 - val_acc: 0.5000\n",
            "Epoch 183/300\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.2349 - acc: 0.9500 - val_loss: 0.7333 - val_acc: 0.5000\n",
            "Epoch 184/300\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.2138 - acc: 0.9667 - val_loss: 0.7463 - val_acc: 0.5000\n",
            "Epoch 185/300\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.2336 - acc: 0.9667 - val_loss: 0.7440 - val_acc: 0.5000\n",
            "Epoch 186/300\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.2202 - acc: 0.9500 - val_loss: 0.7311 - val_acc: 0.5000\n",
            "Epoch 187/300\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.2135 - acc: 1.0000 - val_loss: 0.7280 - val_acc: 0.5000\n",
            "Epoch 188/300\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.2144 - acc: 0.9500 - val_loss: 0.7303 - val_acc: 0.5000\n",
            "Epoch 189/300\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.2213 - acc: 0.9667 - val_loss: 0.7335 - val_acc: 0.5000\n",
            "Epoch 190/300\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.1955 - acc: 1.0000 - val_loss: 0.7507 - val_acc: 0.5500\n",
            "Epoch 191/300\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.2079 - acc: 0.9833 - val_loss: 0.7569 - val_acc: 0.5500\n",
            "Epoch 192/300\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.2084 - acc: 1.0000 - val_loss: 0.7622 - val_acc: 0.6000\n",
            "Epoch 193/300\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.1982 - acc: 0.9833 - val_loss: 0.7590 - val_acc: 0.6000\n",
            "Epoch 194/300\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.2028 - acc: 0.9833 - val_loss: 0.7399 - val_acc: 0.5500\n",
            "Epoch 195/300\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.2223 - acc: 0.9500 - val_loss: 0.7447 - val_acc: 0.5500\n",
            "Epoch 196/300\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.2172 - acc: 0.9667 - val_loss: 0.7516 - val_acc: 0.5000\n",
            "Epoch 197/300\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.2148 - acc: 0.9500 - val_loss: 0.7537 - val_acc: 0.5500\n",
            "Epoch 198/300\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.2035 - acc: 1.0000 - val_loss: 0.7565 - val_acc: 0.5500\n",
            "Epoch 199/300\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.1913 - acc: 1.0000 - val_loss: 0.7531 - val_acc: 0.6000\n",
            "Epoch 200/300\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.1953 - acc: 0.9833 - val_loss: 0.7394 - val_acc: 0.6000\n",
            "Epoch 201/300\n",
            "\n",
            "Epoch 00201: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.2117 - acc: 0.9667 - val_loss: 0.7111 - val_acc: 0.5500\n",
            "Epoch 202/300\n",
            "\n",
            "Epoch 00202: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.1858 - acc: 0.9833 - val_loss: 0.7142 - val_acc: 0.5500\n",
            "Epoch 203/300\n",
            "\n",
            "Epoch 00203: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.2299 - acc: 0.9333 - val_loss: 0.7388 - val_acc: 0.6000\n",
            "Epoch 204/300\n",
            "\n",
            "Epoch 00204: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.1993 - acc: 0.9833 - val_loss: 0.7465 - val_acc: 0.5500\n",
            "Epoch 205/300\n",
            "\n",
            "Epoch 00205: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.2220 - acc: 0.9500 - val_loss: 0.7364 - val_acc: 0.5500\n",
            "Epoch 206/300\n",
            "\n",
            "Epoch 00206: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.1946 - acc: 0.9667 - val_loss: 0.7278 - val_acc: 0.5500\n",
            "Epoch 207/300\n",
            "\n",
            "Epoch 00207: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.2012 - acc: 1.0000 - val_loss: 0.7160 - val_acc: 0.5500\n",
            "Epoch 208/300\n",
            "\n",
            "Epoch 00208: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.1794 - acc: 0.9667 - val_loss: 0.7220 - val_acc: 0.6000\n",
            "Epoch 209/300\n",
            "\n",
            "Epoch 00209: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.2017 - acc: 0.9667 - val_loss: 0.7238 - val_acc: 0.6500\n",
            "Epoch 210/300\n",
            "\n",
            "Epoch 00210: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.2172 - acc: 0.9833 - val_loss: 0.7089 - val_acc: 0.6000\n",
            "Epoch 211/300\n",
            "\n",
            "Epoch 00211: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.2143 - acc: 0.9667 - val_loss: 0.6990 - val_acc: 0.6000\n",
            "Epoch 212/300\n",
            "\n",
            "Epoch 00212: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.2111 - acc: 0.9833 - val_loss: 0.6940 - val_acc: 0.5500\n",
            "Epoch 213/300\n",
            "\n",
            "Epoch 00213: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.1950 - acc: 0.9833 - val_loss: 0.7042 - val_acc: 0.5500\n",
            "Epoch 214/300\n",
            "\n",
            "Epoch 00214: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.2050 - acc: 0.9833 - val_loss: 0.7127 - val_acc: 0.6000\n",
            "Epoch 215/300\n",
            "\n",
            "Epoch 00215: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.1702 - acc: 0.9833 - val_loss: 0.7147 - val_acc: 0.5500\n",
            "Epoch 216/300\n",
            "\n",
            "Epoch 00216: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.1709 - acc: 1.0000 - val_loss: 0.7205 - val_acc: 0.5500\n",
            "Epoch 217/300\n",
            "\n",
            "Epoch 00217: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.2198 - acc: 0.9333 - val_loss: 0.7324 - val_acc: 0.5500\n",
            "Epoch 218/300\n",
            "\n",
            "Epoch 00218: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.1774 - acc: 0.9833 - val_loss: 0.7234 - val_acc: 0.5500\n",
            "Epoch 219/300\n",
            "\n",
            "Epoch 00219: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.1876 - acc: 1.0000 - val_loss: 0.7206 - val_acc: 0.5500\n",
            "Epoch 220/300\n",
            "\n",
            "Epoch 00220: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.1592 - acc: 0.9833 - val_loss: 0.7228 - val_acc: 0.5500\n",
            "Epoch 221/300\n",
            "\n",
            "Epoch 00221: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.1618 - acc: 1.0000 - val_loss: 0.7205 - val_acc: 0.5500\n",
            "Epoch 222/300\n",
            "\n",
            "Epoch 00222: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.1630 - acc: 0.9833 - val_loss: 0.7191 - val_acc: 0.6000\n",
            "Epoch 223/300\n",
            "\n",
            "Epoch 00223: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.2008 - acc: 0.9500 - val_loss: 0.7280 - val_acc: 0.6000\n",
            "Epoch 224/300\n",
            "\n",
            "Epoch 00224: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.1915 - acc: 0.9333 - val_loss: 0.7291 - val_acc: 0.6000\n",
            "Epoch 225/300\n",
            "\n",
            "Epoch 00225: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.1838 - acc: 0.9833 - val_loss: 0.7350 - val_acc: 0.6000\n",
            "Epoch 226/300\n",
            "\n",
            "Epoch 00226: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.1447 - acc: 1.0000 - val_loss: 0.7541 - val_acc: 0.6000\n",
            "Epoch 227/300\n",
            "\n",
            "Epoch 00227: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.1924 - acc: 0.9833 - val_loss: 0.7580 - val_acc: 0.6000\n",
            "Epoch 228/300\n",
            "\n",
            "Epoch 00228: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.1668 - acc: 0.9833 - val_loss: 0.7382 - val_acc: 0.6000\n",
            "Epoch 229/300\n",
            "\n",
            "Epoch 00229: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.1850 - acc: 0.9833 - val_loss: 0.7260 - val_acc: 0.6000\n",
            "Epoch 230/300\n",
            "\n",
            "Epoch 00230: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.2201 - acc: 0.9833 - val_loss: 0.7321 - val_acc: 0.5500\n",
            "Epoch 231/300\n",
            "\n",
            "Epoch 00231: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.2139 - acc: 0.9333 - val_loss: 0.7437 - val_acc: 0.5500\n",
            "Epoch 232/300\n",
            "\n",
            "Epoch 00232: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.2021 - acc: 0.9667 - val_loss: 0.7235 - val_acc: 0.5500\n",
            "Epoch 233/300\n",
            "\n",
            "Epoch 00233: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.1732 - acc: 0.9833 - val_loss: 0.7199 - val_acc: 0.5500\n",
            "Epoch 234/300\n",
            "\n",
            "Epoch 00234: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.1816 - acc: 0.9667 - val_loss: 0.7221 - val_acc: 0.6000\n",
            "Epoch 235/300\n",
            "\n",
            "Epoch 00235: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.1693 - acc: 0.9667 - val_loss: 0.7186 - val_acc: 0.6000\n",
            "Epoch 236/300\n",
            "\n",
            "Epoch 00236: val_loss did not improve from 0.69395\n",
            "60/60 - 0s - loss: 0.2031 - acc: 0.9667 - val_loss: 0.7015 - val_acc: 0.6000\n",
            "Epoch 237/300\n",
            "\n",
            "Epoch 00237: val_loss improved from 0.69395 to 0.68608, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2008 - acc: 0.9667 - val_loss: 0.6861 - val_acc: 0.6000\n",
            "Epoch 238/300\n",
            "\n",
            "Epoch 00238: val_loss did not improve from 0.68608\n",
            "60/60 - 0s - loss: 0.1689 - acc: 0.9667 - val_loss: 0.6965 - val_acc: 0.5500\n",
            "Epoch 239/300\n",
            "\n",
            "Epoch 00239: val_loss did not improve from 0.68608\n",
            "60/60 - 0s - loss: 0.1749 - acc: 0.9833 - val_loss: 0.7143 - val_acc: 0.5500\n",
            "Epoch 240/300\n",
            "\n",
            "Epoch 00240: val_loss did not improve from 0.68608\n",
            "60/60 - 0s - loss: 0.1984 - acc: 0.9500 - val_loss: 0.7282 - val_acc: 0.5500\n",
            "Epoch 241/300\n",
            "\n",
            "Epoch 00241: val_loss did not improve from 0.68608\n",
            "60/60 - 0s - loss: 0.1435 - acc: 1.0000 - val_loss: 0.7253 - val_acc: 0.5500\n",
            "Epoch 242/300\n",
            "\n",
            "Epoch 00242: val_loss did not improve from 0.68608\n",
            "60/60 - 0s - loss: 0.1777 - acc: 1.0000 - val_loss: 0.7154 - val_acc: 0.5500\n",
            "Epoch 243/300\n",
            "\n",
            "Epoch 00243: val_loss did not improve from 0.68608\n",
            "60/60 - 0s - loss: 0.1833 - acc: 0.9833 - val_loss: 0.7340 - val_acc: 0.5500\n",
            "Epoch 244/300\n",
            "\n",
            "Epoch 00244: val_loss did not improve from 0.68608\n",
            "60/60 - 0s - loss: 0.1812 - acc: 0.9833 - val_loss: 0.7271 - val_acc: 0.5500\n",
            "Epoch 245/300\n",
            "\n",
            "Epoch 00245: val_loss did not improve from 0.68608\n",
            "60/60 - 0s - loss: 0.1781 - acc: 0.9667 - val_loss: 0.7095 - val_acc: 0.6000\n",
            "Epoch 246/300\n",
            "\n",
            "Epoch 00246: val_loss did not improve from 0.68608\n",
            "60/60 - 0s - loss: 0.1974 - acc: 0.9833 - val_loss: 0.7090 - val_acc: 0.5500\n",
            "Epoch 247/300\n",
            "\n",
            "Epoch 00247: val_loss did not improve from 0.68608\n",
            "60/60 - 0s - loss: 0.1717 - acc: 0.9833 - val_loss: 0.7132 - val_acc: 0.6000\n",
            "Epoch 248/300\n",
            "\n",
            "Epoch 00248: val_loss did not improve from 0.68608\n",
            "60/60 - 0s - loss: 0.1726 - acc: 0.9833 - val_loss: 0.7149 - val_acc: 0.6000\n",
            "Epoch 249/300\n",
            "\n",
            "Epoch 00249: val_loss did not improve from 0.68608\n",
            "60/60 - 0s - loss: 0.1826 - acc: 1.0000 - val_loss: 0.7336 - val_acc: 0.5500\n",
            "Epoch 250/300\n",
            "\n",
            "Epoch 00250: val_loss did not improve from 0.68608\n",
            "60/60 - 0s - loss: 0.2267 - acc: 0.9333 - val_loss: 0.7391 - val_acc: 0.6000\n",
            "Epoch 251/300\n",
            "\n",
            "Epoch 00251: val_loss did not improve from 0.68608\n",
            "60/60 - 0s - loss: 0.1908 - acc: 0.9667 - val_loss: 0.7251 - val_acc: 0.6000\n",
            "Epoch 252/300\n",
            "\n",
            "Epoch 00252: val_loss did not improve from 0.68608\n",
            "60/60 - 0s - loss: 0.1554 - acc: 1.0000 - val_loss: 0.7389 - val_acc: 0.6000\n",
            "Epoch 253/300\n",
            "\n",
            "Epoch 00253: val_loss did not improve from 0.68608\n",
            "60/60 - 0s - loss: 0.1708 - acc: 1.0000 - val_loss: 0.7504 - val_acc: 0.5000\n",
            "Epoch 254/300\n",
            "\n",
            "Epoch 00254: val_loss did not improve from 0.68608\n",
            "60/60 - 0s - loss: 0.1848 - acc: 0.9500 - val_loss: 0.7363 - val_acc: 0.5000\n",
            "Epoch 255/300\n",
            "\n",
            "Epoch 00255: val_loss did not improve from 0.68608\n",
            "60/60 - 0s - loss: 0.1797 - acc: 0.9833 - val_loss: 0.7204 - val_acc: 0.5000\n",
            "Epoch 256/300\n",
            "\n",
            "Epoch 00256: val_loss did not improve from 0.68608\n",
            "60/60 - 0s - loss: 0.1892 - acc: 0.9833 - val_loss: 0.7240 - val_acc: 0.5500\n",
            "Epoch 257/300\n",
            "\n",
            "Epoch 00257: val_loss did not improve from 0.68608\n",
            "60/60 - 0s - loss: 0.1785 - acc: 0.9833 - val_loss: 0.7257 - val_acc: 0.5500\n",
            "Epoch 258/300\n",
            "\n",
            "Epoch 00258: val_loss did not improve from 0.68608\n",
            "60/60 - 0s - loss: 0.1417 - acc: 0.9833 - val_loss: 0.7282 - val_acc: 0.5500\n",
            "Epoch 259/300\n",
            "\n",
            "Epoch 00259: val_loss did not improve from 0.68608\n",
            "60/60 - 0s - loss: 0.1718 - acc: 0.9667 - val_loss: 0.7252 - val_acc: 0.5500\n",
            "Epoch 260/300\n",
            "\n",
            "Epoch 00260: val_loss did not improve from 0.68608\n",
            "60/60 - 0s - loss: 0.1569 - acc: 0.9833 - val_loss: 0.7275 - val_acc: 0.6000\n",
            "Epoch 261/300\n",
            "\n",
            "Epoch 00261: val_loss did not improve from 0.68608\n",
            "60/60 - 0s - loss: 0.1662 - acc: 0.9833 - val_loss: 0.7159 - val_acc: 0.6500\n",
            "Epoch 262/300\n",
            "\n",
            "Epoch 00262: val_loss did not improve from 0.68608\n",
            "60/60 - 0s - loss: 0.1630 - acc: 0.9667 - val_loss: 0.7152 - val_acc: 0.6500\n",
            "Epoch 263/300\n",
            "\n",
            "Epoch 00263: val_loss did not improve from 0.68608\n",
            "60/60 - 0s - loss: 0.1552 - acc: 0.9833 - val_loss: 0.7005 - val_acc: 0.6000\n",
            "Epoch 264/300\n",
            "\n",
            "Epoch 00264: val_loss did not improve from 0.68608\n",
            "60/60 - 0s - loss: 0.1650 - acc: 0.9833 - val_loss: 0.6914 - val_acc: 0.6500\n",
            "Epoch 265/300\n",
            "\n",
            "Epoch 00265: val_loss improved from 0.68608 to 0.68304, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1507 - acc: 1.0000 - val_loss: 0.6830 - val_acc: 0.6500\n",
            "Epoch 266/300\n",
            "\n",
            "Epoch 00266: val_loss did not improve from 0.68304\n",
            "60/60 - 0s - loss: 0.1459 - acc: 0.9833 - val_loss: 0.6863 - val_acc: 0.6500\n",
            "Epoch 267/300\n",
            "\n",
            "Epoch 00267: val_loss did not improve from 0.68304\n",
            "60/60 - 0s - loss: 0.1703 - acc: 0.9833 - val_loss: 0.7028 - val_acc: 0.6500\n",
            "Epoch 268/300\n",
            "\n",
            "Epoch 00268: val_loss did not improve from 0.68304\n",
            "60/60 - 0s - loss: 0.1276 - acc: 0.9833 - val_loss: 0.7223 - val_acc: 0.6000\n",
            "Epoch 269/300\n",
            "\n",
            "Epoch 00269: val_loss did not improve from 0.68304\n",
            "60/60 - 0s - loss: 0.1822 - acc: 0.9667 - val_loss: 0.7252 - val_acc: 0.6000\n",
            "Epoch 270/300\n",
            "\n",
            "Epoch 00270: val_loss did not improve from 0.68304\n",
            "60/60 - 0s - loss: 0.1952 - acc: 0.9667 - val_loss: 0.7259 - val_acc: 0.6000\n",
            "Epoch 271/300\n",
            "\n",
            "Epoch 00271: val_loss did not improve from 0.68304\n",
            "60/60 - 0s - loss: 0.1413 - acc: 0.9833 - val_loss: 0.7187 - val_acc: 0.5500\n",
            "Epoch 272/300\n",
            "\n",
            "Epoch 00272: val_loss did not improve from 0.68304\n",
            "60/60 - 0s - loss: 0.1889 - acc: 0.9833 - val_loss: 0.7036 - val_acc: 0.5500\n",
            "Epoch 273/300\n",
            "\n",
            "Epoch 00273: val_loss did not improve from 0.68304\n",
            "60/60 - 0s - loss: 0.1620 - acc: 0.9833 - val_loss: 0.7273 - val_acc: 0.5500\n",
            "Epoch 274/300\n",
            "\n",
            "Epoch 00274: val_loss did not improve from 0.68304\n",
            "60/60 - 0s - loss: 0.1693 - acc: 0.9833 - val_loss: 0.7452 - val_acc: 0.5500\n",
            "Epoch 275/300\n",
            "\n",
            "Epoch 00275: val_loss did not improve from 0.68304\n",
            "60/60 - 0s - loss: 0.1743 - acc: 0.9667 - val_loss: 0.7298 - val_acc: 0.6000\n",
            "Epoch 276/300\n",
            "\n",
            "Epoch 00276: val_loss did not improve from 0.68304\n",
            "60/60 - 0s - loss: 0.1635 - acc: 0.9667 - val_loss: 0.7048 - val_acc: 0.6000\n",
            "Epoch 277/300\n",
            "\n",
            "Epoch 00277: val_loss improved from 0.68304 to 0.67395, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1405 - acc: 1.0000 - val_loss: 0.6740 - val_acc: 0.6000\n",
            "Epoch 278/300\n",
            "\n",
            "Epoch 00278: val_loss improved from 0.67395 to 0.67267, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1612 - acc: 0.9833 - val_loss: 0.6727 - val_acc: 0.6000\n",
            "Epoch 279/300\n",
            "\n",
            "Epoch 00279: val_loss did not improve from 0.67267\n",
            "60/60 - 0s - loss: 0.1867 - acc: 0.9667 - val_loss: 0.6960 - val_acc: 0.5500\n",
            "Epoch 280/300\n",
            "\n",
            "Epoch 00280: val_loss did not improve from 0.67267\n",
            "60/60 - 0s - loss: 0.1348 - acc: 0.9833 - val_loss: 0.7063 - val_acc: 0.5500\n",
            "Epoch 281/300\n",
            "\n",
            "Epoch 00281: val_loss did not improve from 0.67267\n",
            "60/60 - 0s - loss: 0.1772 - acc: 0.9833 - val_loss: 0.6893 - val_acc: 0.6500\n",
            "Epoch 282/300\n",
            "\n",
            "Epoch 00282: val_loss did not improve from 0.67267\n",
            "60/60 - 0s - loss: 0.1506 - acc: 0.9667 - val_loss: 0.6931 - val_acc: 0.6500\n",
            "Epoch 283/300\n",
            "\n",
            "Epoch 00283: val_loss did not improve from 0.67267\n",
            "60/60 - 0s - loss: 0.1418 - acc: 1.0000 - val_loss: 0.6892 - val_acc: 0.6500\n",
            "Epoch 284/300\n",
            "\n",
            "Epoch 00284: val_loss did not improve from 0.67267\n",
            "60/60 - 0s - loss: 0.1149 - acc: 1.0000 - val_loss: 0.6888 - val_acc: 0.6500\n",
            "Epoch 285/300\n",
            "\n",
            "Epoch 00285: val_loss did not improve from 0.67267\n",
            "60/60 - 0s - loss: 0.1570 - acc: 1.0000 - val_loss: 0.6848 - val_acc: 0.6500\n",
            "Epoch 286/300\n",
            "\n",
            "Epoch 00286: val_loss did not improve from 0.67267\n",
            "60/60 - 0s - loss: 0.1133 - acc: 1.0000 - val_loss: 0.6914 - val_acc: 0.6000\n",
            "Epoch 287/300\n",
            "\n",
            "Epoch 00287: val_loss did not improve from 0.67267\n",
            "60/60 - 0s - loss: 0.1354 - acc: 0.9833 - val_loss: 0.7143 - val_acc: 0.6000\n",
            "Epoch 288/300\n",
            "\n",
            "Epoch 00288: val_loss did not improve from 0.67267\n",
            "60/60 - 0s - loss: 0.1811 - acc: 0.9667 - val_loss: 0.7163 - val_acc: 0.6000\n",
            "Epoch 289/300\n",
            "\n",
            "Epoch 00289: val_loss did not improve from 0.67267\n",
            "60/60 - 0s - loss: 0.1562 - acc: 0.9833 - val_loss: 0.7180 - val_acc: 0.6500\n",
            "Epoch 290/300\n",
            "\n",
            "Epoch 00290: val_loss did not improve from 0.67267\n",
            "60/60 - 0s - loss: 0.1723 - acc: 0.9833 - val_loss: 0.6945 - val_acc: 0.6000\n",
            "Epoch 291/300\n",
            "\n",
            "Epoch 00291: val_loss improved from 0.67267 to 0.67032, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1539 - acc: 0.9500 - val_loss: 0.6703 - val_acc: 0.6000\n",
            "Epoch 292/300\n",
            "\n",
            "Epoch 00292: val_loss improved from 0.67032 to 0.65033, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1548 - acc: 0.9500 - val_loss: 0.6503 - val_acc: 0.6500\n",
            "Epoch 293/300\n",
            "\n",
            "Epoch 00293: val_loss improved from 0.65033 to 0.64904, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1458 - acc: 0.9833 - val_loss: 0.6490 - val_acc: 0.7000\n",
            "Epoch 294/300\n",
            "\n",
            "Epoch 00294: val_loss improved from 0.64904 to 0.64370, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1382 - acc: 0.9833 - val_loss: 0.6437 - val_acc: 0.7000\n",
            "Epoch 295/300\n",
            "\n",
            "Epoch 00295: val_loss did not improve from 0.64370\n",
            "60/60 - 0s - loss: 0.1330 - acc: 1.0000 - val_loss: 0.6452 - val_acc: 0.7000\n",
            "Epoch 296/300\n",
            "\n",
            "Epoch 00296: val_loss did not improve from 0.64370\n",
            "60/60 - 0s - loss: 0.1439 - acc: 0.9833 - val_loss: 0.6526 - val_acc: 0.7000\n",
            "Epoch 297/300\n",
            "\n",
            "Epoch 00297: val_loss did not improve from 0.64370\n",
            "60/60 - 0s - loss: 0.1779 - acc: 0.9667 - val_loss: 0.6568 - val_acc: 0.7000\n",
            "Epoch 298/300\n",
            "\n",
            "Epoch 00298: val_loss improved from 0.64370 to 0.62961, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1917 - acc: 0.9500 - val_loss: 0.6296 - val_acc: 0.7000\n",
            "Epoch 299/300\n",
            "\n",
            "Epoch 00299: val_loss improved from 0.62961 to 0.62889, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1439 - acc: 0.9833 - val_loss: 0.6289 - val_acc: 0.7000\n",
            "Epoch 300/300\n",
            "\n",
            "Epoch 00300: val_loss did not improve from 0.62889\n",
            "60/60 - 0s - loss: 0.1419 - acc: 1.0000 - val_loss: 0.6319 - val_acc: 0.7000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "# Evaluate on test data\n",
            "40/40 [==============================] - 0s 2ms/sample - loss: 0.7281 - acc: 0.6000\n",
            "test loss, test acc: [0.7281265735626221, 0.6]\n",
            "EEG_Deep/Data2A/parsed_P010T.mat\n",
            "Filtering of Training Data Finished\n",
            "EEG_Deep/Data2A/parsed_P010E.mat\n",
            "Filtering of Testing Data Finished\n",
            "(1, 2)\n",
            "Finding labels in training data\n",
            "Finding labels in testing data\n",
            "[1 2 2 2 2 1 1 2 2 2 2 1 2 2 1 1 1 2 1 1]\n",
            "(60, 12, 1536)\n",
            "(60,)\n",
            "(20, 12, 1536)\n",
            "(20,)\n",
            "[[1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]]\n",
            "X_train shape: (60, 1, 12, 1536)\n",
            "60 train samples\n",
            "20 val samples\n",
            "(40, 12, 1536)\n",
            "(40, 1)\n",
            "X_train shape: (40, 1, 12, 1536)\n",
            "40 train samples\n",
            "Train on 60 samples, validate on 20 samples\n",
            "Epoch 1/300\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.69157, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 1s - loss: 0.7322 - acc: 0.5167 - val_loss: 0.6916 - val_acc: 0.5500\n",
            "Epoch 2/300\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.69157 to 0.68747, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6780 - acc: 0.5500 - val_loss: 0.6875 - val_acc: 0.5500\n",
            "Epoch 3/300\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.68747 to 0.68329, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6347 - acc: 0.6500 - val_loss: 0.6833 - val_acc: 0.6500\n",
            "Epoch 4/300\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.68329 to 0.67923, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6089 - acc: 0.6167 - val_loss: 0.6792 - val_acc: 0.7000\n",
            "Epoch 5/300\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.67923 to 0.67475, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5743 - acc: 0.7667 - val_loss: 0.6748 - val_acc: 0.7500\n",
            "Epoch 6/300\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.67475 to 0.67049, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5400 - acc: 0.8000 - val_loss: 0.6705 - val_acc: 0.8500\n",
            "Epoch 7/300\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.67049 to 0.66469, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5117 - acc: 0.9000 - val_loss: 0.6647 - val_acc: 0.8500\n",
            "Epoch 8/300\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.66469 to 0.65724, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5244 - acc: 0.8167 - val_loss: 0.6572 - val_acc: 0.9000\n",
            "Epoch 9/300\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.65724 to 0.64847, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5023 - acc: 0.8333 - val_loss: 0.6485 - val_acc: 0.9000\n",
            "Epoch 10/300\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.64847 to 0.63999, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4499 - acc: 0.8500 - val_loss: 0.6400 - val_acc: 0.9000\n",
            "Epoch 11/300\n",
            "\n",
            "Epoch 00011: val_loss improved from 0.63999 to 0.63109, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4293 - acc: 0.8667 - val_loss: 0.6311 - val_acc: 0.9000\n",
            "Epoch 12/300\n",
            "\n",
            "Epoch 00012: val_loss improved from 0.63109 to 0.62225, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4137 - acc: 0.8167 - val_loss: 0.6223 - val_acc: 0.9000\n",
            "Epoch 13/300\n",
            "\n",
            "Epoch 00013: val_loss improved from 0.62225 to 0.61245, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4000 - acc: 0.8833 - val_loss: 0.6124 - val_acc: 0.9000\n",
            "Epoch 14/300\n",
            "\n",
            "Epoch 00014: val_loss improved from 0.61245 to 0.60321, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3625 - acc: 0.9167 - val_loss: 0.6032 - val_acc: 0.9000\n",
            "Epoch 15/300\n",
            "\n",
            "Epoch 00015: val_loss improved from 0.60321 to 0.59530, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4001 - acc: 0.8833 - val_loss: 0.5953 - val_acc: 0.9000\n",
            "Epoch 16/300\n",
            "\n",
            "Epoch 00016: val_loss improved from 0.59530 to 0.58758, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3361 - acc: 0.8833 - val_loss: 0.5876 - val_acc: 0.9000\n",
            "Epoch 17/300\n",
            "\n",
            "Epoch 00017: val_loss improved from 0.58758 to 0.58194, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3699 - acc: 0.8833 - val_loss: 0.5819 - val_acc: 0.9000\n",
            "Epoch 18/300\n",
            "\n",
            "Epoch 00018: val_loss improved from 0.58194 to 0.57783, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3010 - acc: 0.9500 - val_loss: 0.5778 - val_acc: 0.9000\n",
            "Epoch 19/300\n",
            "\n",
            "Epoch 00019: val_loss improved from 0.57783 to 0.57608, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4426 - acc: 0.7833 - val_loss: 0.5761 - val_acc: 0.9000\n",
            "Epoch 20/300\n",
            "\n",
            "Epoch 00020: val_loss improved from 0.57608 to 0.57440, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3309 - acc: 0.8667 - val_loss: 0.5744 - val_acc: 0.9000\n",
            "Epoch 21/300\n",
            "\n",
            "Epoch 00021: val_loss improved from 0.57440 to 0.57429, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3304 - acc: 0.8667 - val_loss: 0.5743 - val_acc: 0.9000\n",
            "Epoch 22/300\n",
            "\n",
            "Epoch 00022: val_loss improved from 0.57429 to 0.57032, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3151 - acc: 0.9000 - val_loss: 0.5703 - val_acc: 0.9000\n",
            "Epoch 23/300\n",
            "\n",
            "Epoch 00023: val_loss improved from 0.57032 to 0.56378, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3554 - acc: 0.8333 - val_loss: 0.5638 - val_acc: 0.9000\n",
            "Epoch 24/300\n",
            "\n",
            "Epoch 00024: val_loss improved from 0.56378 to 0.55640, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3320 - acc: 0.9000 - val_loss: 0.5564 - val_acc: 0.9000\n",
            "Epoch 25/300\n",
            "\n",
            "Epoch 00025: val_loss improved from 0.55640 to 0.55299, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3476 - acc: 0.9167 - val_loss: 0.5530 - val_acc: 0.9000\n",
            "Epoch 26/300\n",
            "\n",
            "Epoch 00026: val_loss improved from 0.55299 to 0.54980, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3158 - acc: 0.9333 - val_loss: 0.5498 - val_acc: 0.8500\n",
            "Epoch 27/300\n",
            "\n",
            "Epoch 00027: val_loss improved from 0.54980 to 0.54933, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2975 - acc: 0.9333 - val_loss: 0.5493 - val_acc: 0.8500\n",
            "Epoch 28/300\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.4206 - acc: 0.8167 - val_loss: 0.5509 - val_acc: 0.8500\n",
            "Epoch 29/300\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.3452 - acc: 0.8667 - val_loss: 0.5571 - val_acc: 0.8000\n",
            "Epoch 30/300\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.3175 - acc: 0.9167 - val_loss: 0.5612 - val_acc: 0.7500\n",
            "Epoch 31/300\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.2843 - acc: 0.8833 - val_loss: 0.5658 - val_acc: 0.7500\n",
            "Epoch 32/300\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.3333 - acc: 0.8833 - val_loss: 0.5652 - val_acc: 0.7500\n",
            "Epoch 33/300\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.2682 - acc: 0.9167 - val_loss: 0.5682 - val_acc: 0.7500\n",
            "Epoch 34/300\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.3009 - acc: 0.8833 - val_loss: 0.5751 - val_acc: 0.7000\n",
            "Epoch 35/300\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.2852 - acc: 0.9500 - val_loss: 0.5768 - val_acc: 0.7000\n",
            "Epoch 36/300\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.2881 - acc: 0.9167 - val_loss: 0.5780 - val_acc: 0.7000\n",
            "Epoch 37/300\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.3281 - acc: 0.9000 - val_loss: 0.5788 - val_acc: 0.7000\n",
            "Epoch 38/300\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.2829 - acc: 0.9333 - val_loss: 0.5876 - val_acc: 0.7000\n",
            "Epoch 39/300\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.2771 - acc: 0.9167 - val_loss: 0.6053 - val_acc: 0.7000\n",
            "Epoch 40/300\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.2725 - acc: 0.9500 - val_loss: 0.6298 - val_acc: 0.6500\n",
            "Epoch 41/300\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.2898 - acc: 0.8500 - val_loss: 0.6672 - val_acc: 0.5500\n",
            "Epoch 42/300\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.2650 - acc: 0.9333 - val_loss: 0.6966 - val_acc: 0.5000\n",
            "Epoch 43/300\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.2572 - acc: 0.9667 - val_loss: 0.7066 - val_acc: 0.5000\n",
            "Epoch 44/300\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.2334 - acc: 0.9833 - val_loss: 0.7075 - val_acc: 0.5000\n",
            "Epoch 45/300\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.2574 - acc: 0.9500 - val_loss: 0.7009 - val_acc: 0.5000\n",
            "Epoch 46/300\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.2756 - acc: 0.9333 - val_loss: 0.7002 - val_acc: 0.5000\n",
            "Epoch 47/300\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.2687 - acc: 0.9667 - val_loss: 0.6964 - val_acc: 0.5000\n",
            "Epoch 48/300\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.2929 - acc: 0.8833 - val_loss: 0.7044 - val_acc: 0.5000\n",
            "Epoch 49/300\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.2692 - acc: 0.9333 - val_loss: 0.7173 - val_acc: 0.5000\n",
            "Epoch 50/300\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.2640 - acc: 0.9000 - val_loss: 0.7137 - val_acc: 0.5500\n",
            "Epoch 51/300\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.2265 - acc: 0.9500 - val_loss: 0.7153 - val_acc: 0.5500\n",
            "Epoch 52/300\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.2413 - acc: 0.9500 - val_loss: 0.7193 - val_acc: 0.5500\n",
            "Epoch 53/300\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.3056 - acc: 0.8500 - val_loss: 0.7294 - val_acc: 0.5000\n",
            "Epoch 54/300\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.2296 - acc: 0.9500 - val_loss: 0.7574 - val_acc: 0.5000\n",
            "Epoch 55/300\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1936 - acc: 0.9667 - val_loss: 0.7816 - val_acc: 0.5000\n",
            "Epoch 56/300\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.2329 - acc: 0.9833 - val_loss: 0.7843 - val_acc: 0.5000\n",
            "Epoch 57/300\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.2484 - acc: 0.9333 - val_loss: 0.7688 - val_acc: 0.5000\n",
            "Epoch 58/300\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.2360 - acc: 0.9333 - val_loss: 0.7933 - val_acc: 0.5000\n",
            "Epoch 59/300\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.2269 - acc: 0.9333 - val_loss: 0.8183 - val_acc: 0.5000\n",
            "Epoch 60/300\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.2378 - acc: 0.9167 - val_loss: 0.8378 - val_acc: 0.5000\n",
            "Epoch 61/300\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.2153 - acc: 0.9500 - val_loss: 0.8456 - val_acc: 0.5000\n",
            "Epoch 62/300\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.2396 - acc: 0.9333 - val_loss: 0.8449 - val_acc: 0.5000\n",
            "Epoch 63/300\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.2807 - acc: 0.9000 - val_loss: 0.8054 - val_acc: 0.5000\n",
            "Epoch 64/300\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.2790 - acc: 0.9167 - val_loss: 0.7815 - val_acc: 0.5000\n",
            "Epoch 65/300\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.2575 - acc: 0.9333 - val_loss: 0.7756 - val_acc: 0.5000\n",
            "Epoch 66/300\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.2052 - acc: 0.9833 - val_loss: 0.7888 - val_acc: 0.5000\n",
            "Epoch 67/300\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.2472 - acc: 0.9167 - val_loss: 0.8097 - val_acc: 0.5000\n",
            "Epoch 68/300\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.2741 - acc: 0.9167 - val_loss: 0.8181 - val_acc: 0.5000\n",
            "Epoch 69/300\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.2209 - acc: 0.9833 - val_loss: 0.8121 - val_acc: 0.5000\n",
            "Epoch 70/300\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1868 - acc: 0.9833 - val_loss: 0.8073 - val_acc: 0.5000\n",
            "Epoch 71/300\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.2208 - acc: 0.9500 - val_loss: 0.8017 - val_acc: 0.5000\n",
            "Epoch 72/300\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.2605 - acc: 0.9333 - val_loss: 0.8017 - val_acc: 0.5000\n",
            "Epoch 73/300\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.2127 - acc: 0.9833 - val_loss: 0.7854 - val_acc: 0.5500\n",
            "Epoch 74/300\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1997 - acc: 0.9667 - val_loss: 0.7921 - val_acc: 0.5500\n",
            "Epoch 75/300\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1949 - acc: 0.9833 - val_loss: 0.7924 - val_acc: 0.5000\n",
            "Epoch 76/300\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1971 - acc: 0.9667 - val_loss: 0.7768 - val_acc: 0.5500\n",
            "Epoch 77/300\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.2163 - acc: 0.9333 - val_loss: 0.7752 - val_acc: 0.5500\n",
            "Epoch 78/300\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.2344 - acc: 0.9333 - val_loss: 0.7840 - val_acc: 0.5500\n",
            "Epoch 79/300\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1918 - acc: 0.9833 - val_loss: 0.8099 - val_acc: 0.5000\n",
            "Epoch 80/300\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.2261 - acc: 0.9167 - val_loss: 0.8339 - val_acc: 0.5000\n",
            "Epoch 81/300\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1955 - acc: 0.9667 - val_loss: 0.8413 - val_acc: 0.5000\n",
            "Epoch 82/300\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1848 - acc: 0.9500 - val_loss: 0.8510 - val_acc: 0.5000\n",
            "Epoch 83/300\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.2282 - acc: 0.9167 - val_loss: 0.8458 - val_acc: 0.5000\n",
            "Epoch 84/300\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.2170 - acc: 0.9500 - val_loss: 0.8658 - val_acc: 0.5000\n",
            "Epoch 85/300\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1849 - acc: 0.9667 - val_loss: 0.8624 - val_acc: 0.5000\n",
            "Epoch 86/300\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.2514 - acc: 0.9333 - val_loss: 0.8569 - val_acc: 0.5000\n",
            "Epoch 87/300\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.2282 - acc: 0.9667 - val_loss: 0.8560 - val_acc: 0.5000\n",
            "Epoch 88/300\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1736 - acc: 0.9667 - val_loss: 0.8940 - val_acc: 0.5000\n",
            "Epoch 89/300\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1945 - acc: 0.9667 - val_loss: 0.9451 - val_acc: 0.5000\n",
            "Epoch 90/300\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1806 - acc: 0.9833 - val_loss: 0.9791 - val_acc: 0.5000\n",
            "Epoch 91/300\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.2020 - acc: 0.9333 - val_loss: 0.9536 - val_acc: 0.5000\n",
            "Epoch 92/300\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.2078 - acc: 0.9333 - val_loss: 0.9067 - val_acc: 0.5000\n",
            "Epoch 93/300\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.2157 - acc: 0.9167 - val_loss: 0.8515 - val_acc: 0.5000\n",
            "Epoch 94/300\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.2191 - acc: 0.9333 - val_loss: 0.8209 - val_acc: 0.5000\n",
            "Epoch 95/300\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1929 - acc: 1.0000 - val_loss: 0.8312 - val_acc: 0.5000\n",
            "Epoch 96/300\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1852 - acc: 0.9167 - val_loss: 0.8301 - val_acc: 0.5000\n",
            "Epoch 97/300\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1851 - acc: 0.9833 - val_loss: 0.7876 - val_acc: 0.5000\n",
            "Epoch 98/300\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1771 - acc: 1.0000 - val_loss: 0.7515 - val_acc: 0.6000\n",
            "Epoch 99/300\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1666 - acc: 1.0000 - val_loss: 0.7313 - val_acc: 0.6000\n",
            "Epoch 100/300\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.2430 - acc: 0.8833 - val_loss: 0.7169 - val_acc: 0.6000\n",
            "Epoch 101/300\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1735 - acc: 0.9500 - val_loss: 0.7369 - val_acc: 0.6000\n",
            "Epoch 102/300\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1815 - acc: 1.0000 - val_loss: 0.7464 - val_acc: 0.5500\n",
            "Epoch 103/300\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.2307 - acc: 0.9833 - val_loss: 0.7649 - val_acc: 0.5500\n",
            "Epoch 104/300\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.2063 - acc: 0.9500 - val_loss: 0.7773 - val_acc: 0.5500\n",
            "Epoch 105/300\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.2008 - acc: 0.9333 - val_loss: 0.8037 - val_acc: 0.5500\n",
            "Epoch 106/300\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.2040 - acc: 0.9500 - val_loss: 0.8392 - val_acc: 0.5000\n",
            "Epoch 107/300\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1849 - acc: 0.9667 - val_loss: 0.8525 - val_acc: 0.5000\n",
            "Epoch 108/300\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.2028 - acc: 1.0000 - val_loss: 0.8568 - val_acc: 0.5000\n",
            "Epoch 109/300\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1775 - acc: 1.0000 - val_loss: 0.8455 - val_acc: 0.5000\n",
            "Epoch 110/300\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1802 - acc: 0.9500 - val_loss: 0.8075 - val_acc: 0.5000\n",
            "Epoch 111/300\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1964 - acc: 0.9500 - val_loss: 0.7651 - val_acc: 0.5500\n",
            "Epoch 112/300\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.2265 - acc: 0.9000 - val_loss: 0.7477 - val_acc: 0.6000\n",
            "Epoch 113/300\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1725 - acc: 1.0000 - val_loss: 0.7460 - val_acc: 0.6000\n",
            "Epoch 114/300\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1937 - acc: 0.9667 - val_loss: 0.7432 - val_acc: 0.6000\n",
            "Epoch 115/300\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.2615 - acc: 0.8833 - val_loss: 0.7221 - val_acc: 0.6000\n",
            "Epoch 116/300\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.2313 - acc: 0.9500 - val_loss: 0.7309 - val_acc: 0.5500\n",
            "Epoch 117/300\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1721 - acc: 1.0000 - val_loss: 0.7452 - val_acc: 0.5500\n",
            "Epoch 118/300\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1746 - acc: 0.9833 - val_loss: 0.7656 - val_acc: 0.5500\n",
            "Epoch 119/300\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1626 - acc: 1.0000 - val_loss: 0.7814 - val_acc: 0.5000\n",
            "Epoch 120/300\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1812 - acc: 0.9833 - val_loss: 0.7836 - val_acc: 0.5000\n",
            "Epoch 121/300\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1720 - acc: 0.9833 - val_loss: 0.7927 - val_acc: 0.5000\n",
            "Epoch 122/300\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1723 - acc: 0.9833 - val_loss: 0.8064 - val_acc: 0.5000\n",
            "Epoch 123/300\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1586 - acc: 1.0000 - val_loss: 0.8252 - val_acc: 0.5000\n",
            "Epoch 124/300\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1885 - acc: 0.9667 - val_loss: 0.8426 - val_acc: 0.5000\n",
            "Epoch 125/300\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.2088 - acc: 0.9500 - val_loss: 0.8414 - val_acc: 0.5000\n",
            "Epoch 126/300\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.2037 - acc: 0.9333 - val_loss: 0.8481 - val_acc: 0.5000\n",
            "Epoch 127/300\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1688 - acc: 0.9833 - val_loss: 0.8020 - val_acc: 0.5000\n",
            "Epoch 128/300\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1988 - acc: 0.9167 - val_loss: 0.7574 - val_acc: 0.5500\n",
            "Epoch 129/300\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1709 - acc: 0.9833 - val_loss: 0.7278 - val_acc: 0.5500\n",
            "Epoch 130/300\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1529 - acc: 1.0000 - val_loss: 0.7185 - val_acc: 0.5500\n",
            "Epoch 131/300\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1606 - acc: 1.0000 - val_loss: 0.6976 - val_acc: 0.6000\n",
            "Epoch 132/300\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1541 - acc: 0.9833 - val_loss: 0.6840 - val_acc: 0.6500\n",
            "Epoch 133/300\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.2211 - acc: 0.9333 - val_loss: 0.6805 - val_acc: 0.7000\n",
            "Epoch 134/300\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1689 - acc: 0.9667 - val_loss: 0.7095 - val_acc: 0.5500\n",
            "Epoch 135/300\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1628 - acc: 1.0000 - val_loss: 0.7422 - val_acc: 0.5500\n",
            "Epoch 136/300\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1485 - acc: 1.0000 - val_loss: 0.7639 - val_acc: 0.5000\n",
            "Epoch 137/300\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1995 - acc: 0.9667 - val_loss: 0.8089 - val_acc: 0.5000\n",
            "Epoch 138/300\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1535 - acc: 0.9833 - val_loss: 0.8185 - val_acc: 0.5000\n",
            "Epoch 139/300\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1549 - acc: 0.9833 - val_loss: 0.7898 - val_acc: 0.5000\n",
            "Epoch 140/300\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.2453 - acc: 0.8833 - val_loss: 0.7598 - val_acc: 0.5500\n",
            "Epoch 141/300\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1836 - acc: 0.9333 - val_loss: 0.7054 - val_acc: 0.6000\n",
            "Epoch 142/300\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1262 - acc: 1.0000 - val_loss: 0.6860 - val_acc: 0.6000\n",
            "Epoch 143/300\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1811 - acc: 0.9667 - val_loss: 0.6849 - val_acc: 0.6000\n",
            "Epoch 144/300\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1380 - acc: 0.9833 - val_loss: 0.6869 - val_acc: 0.6000\n",
            "Epoch 145/300\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.2133 - acc: 0.9500 - val_loss: 0.6897 - val_acc: 0.5500\n",
            "Epoch 146/300\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1704 - acc: 0.9833 - val_loss: 0.7118 - val_acc: 0.5000\n",
            "Epoch 147/300\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.2094 - acc: 0.9333 - val_loss: 0.7359 - val_acc: 0.5000\n",
            "Epoch 148/300\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1857 - acc: 0.9667 - val_loss: 0.7430 - val_acc: 0.5000\n",
            "Epoch 149/300\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1477 - acc: 1.0000 - val_loss: 0.7432 - val_acc: 0.5000\n",
            "Epoch 150/300\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1806 - acc: 0.9833 - val_loss: 0.7492 - val_acc: 0.5000\n",
            "Epoch 151/300\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1468 - acc: 1.0000 - val_loss: 0.7672 - val_acc: 0.5000\n",
            "Epoch 152/300\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1471 - acc: 1.0000 - val_loss: 0.8000 - val_acc: 0.5000\n",
            "Epoch 153/300\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.2099 - acc: 0.9667 - val_loss: 0.8168 - val_acc: 0.5500\n",
            "Epoch 154/300\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1323 - acc: 1.0000 - val_loss: 0.8041 - val_acc: 0.5500\n",
            "Epoch 155/300\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1340 - acc: 1.0000 - val_loss: 0.7967 - val_acc: 0.6000\n",
            "Epoch 156/300\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1452 - acc: 0.9667 - val_loss: 0.7803 - val_acc: 0.6000\n",
            "Epoch 157/300\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1430 - acc: 0.9667 - val_loss: 0.7678 - val_acc: 0.6000\n",
            "Epoch 158/300\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1534 - acc: 1.0000 - val_loss: 0.7489 - val_acc: 0.6500\n",
            "Epoch 159/300\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1654 - acc: 0.9667 - val_loss: 0.7398 - val_acc: 0.6500\n",
            "Epoch 160/300\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1287 - acc: 0.9667 - val_loss: 0.7544 - val_acc: 0.6500\n",
            "Epoch 161/300\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1388 - acc: 1.0000 - val_loss: 0.7513 - val_acc: 0.6000\n",
            "Epoch 162/300\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1589 - acc: 0.9500 - val_loss: 0.7516 - val_acc: 0.6000\n",
            "Epoch 163/300\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1439 - acc: 0.9833 - val_loss: 0.7678 - val_acc: 0.5500\n",
            "Epoch 164/300\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1557 - acc: 0.9667 - val_loss: 0.7656 - val_acc: 0.5500\n",
            "Epoch 165/300\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1669 - acc: 0.9667 - val_loss: 0.7570 - val_acc: 0.5000\n",
            "Epoch 166/300\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1434 - acc: 1.0000 - val_loss: 0.7409 - val_acc: 0.6000\n",
            "Epoch 167/300\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1325 - acc: 0.9833 - val_loss: 0.6997 - val_acc: 0.6500\n",
            "Epoch 168/300\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1661 - acc: 0.9500 - val_loss: 0.6777 - val_acc: 0.6500\n",
            "Epoch 169/300\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1663 - acc: 0.9667 - val_loss: 0.6810 - val_acc: 0.6500\n",
            "Epoch 170/300\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1138 - acc: 1.0000 - val_loss: 0.7000 - val_acc: 0.6000\n",
            "Epoch 171/300\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1721 - acc: 0.9333 - val_loss: 0.7185 - val_acc: 0.5500\n",
            "Epoch 172/300\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1429 - acc: 0.9667 - val_loss: 0.7301 - val_acc: 0.5000\n",
            "Epoch 173/300\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1482 - acc: 1.0000 - val_loss: 0.7717 - val_acc: 0.5000\n",
            "Epoch 174/300\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1212 - acc: 1.0000 - val_loss: 0.7975 - val_acc: 0.5000\n",
            "Epoch 175/300\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1465 - acc: 0.9833 - val_loss: 0.8351 - val_acc: 0.5000\n",
            "Epoch 176/300\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1259 - acc: 0.9667 - val_loss: 0.8903 - val_acc: 0.5000\n",
            "Epoch 177/300\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1246 - acc: 0.9667 - val_loss: 0.9176 - val_acc: 0.5000\n",
            "Epoch 178/300\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1303 - acc: 1.0000 - val_loss: 0.9495 - val_acc: 0.5000\n",
            "Epoch 179/300\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1398 - acc: 0.9833 - val_loss: 0.9488 - val_acc: 0.5000\n",
            "Epoch 180/300\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1417 - acc: 1.0000 - val_loss: 0.9186 - val_acc: 0.5000\n",
            "Epoch 181/300\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1280 - acc: 0.9667 - val_loss: 0.8801 - val_acc: 0.5000\n",
            "Epoch 182/300\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1299 - acc: 1.0000 - val_loss: 0.8450 - val_acc: 0.5000\n",
            "Epoch 183/300\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1552 - acc: 0.9500 - val_loss: 0.8500 - val_acc: 0.5000\n",
            "Epoch 184/300\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1229 - acc: 1.0000 - val_loss: 0.8673 - val_acc: 0.5000\n",
            "Epoch 185/300\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1410 - acc: 0.9833 - val_loss: 0.8808 - val_acc: 0.5000\n",
            "Epoch 186/300\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1317 - acc: 0.9833 - val_loss: 0.8965 - val_acc: 0.5000\n",
            "Epoch 187/300\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1290 - acc: 0.9833 - val_loss: 0.9023 - val_acc: 0.4500\n",
            "Epoch 188/300\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1670 - acc: 0.9667 - val_loss: 0.9032 - val_acc: 0.4500\n",
            "Epoch 189/300\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1710 - acc: 0.9167 - val_loss: 0.8769 - val_acc: 0.5000\n",
            "Epoch 190/300\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1328 - acc: 0.9833 - val_loss: 0.8798 - val_acc: 0.4500\n",
            "Epoch 191/300\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1429 - acc: 0.9833 - val_loss: 0.9034 - val_acc: 0.4500\n",
            "Epoch 192/300\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1599 - acc: 0.9500 - val_loss: 0.9477 - val_acc: 0.4500\n",
            "Epoch 193/300\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1553 - acc: 0.9833 - val_loss: 0.9756 - val_acc: 0.4500\n",
            "Epoch 194/300\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1203 - acc: 1.0000 - val_loss: 0.9434 - val_acc: 0.4500\n",
            "Epoch 195/300\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1490 - acc: 1.0000 - val_loss: 0.8402 - val_acc: 0.5500\n",
            "Epoch 196/300\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1308 - acc: 1.0000 - val_loss: 0.7598 - val_acc: 0.6000\n",
            "Epoch 197/300\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1276 - acc: 1.0000 - val_loss: 0.6860 - val_acc: 0.6500\n",
            "Epoch 198/300\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1153 - acc: 1.0000 - val_loss: 0.6581 - val_acc: 0.7000\n",
            "Epoch 199/300\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1386 - acc: 0.9833 - val_loss: 0.6636 - val_acc: 0.7000\n",
            "Epoch 200/300\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1411 - acc: 0.9833 - val_loss: 0.6784 - val_acc: 0.7000\n",
            "Epoch 201/300\n",
            "\n",
            "Epoch 00201: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.0996 - acc: 1.0000 - val_loss: 0.7042 - val_acc: 0.6000\n",
            "Epoch 202/300\n",
            "\n",
            "Epoch 00202: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1273 - acc: 1.0000 - val_loss: 0.7273 - val_acc: 0.6000\n",
            "Epoch 203/300\n",
            "\n",
            "Epoch 00203: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1661 - acc: 0.9833 - val_loss: 0.7550 - val_acc: 0.6000\n",
            "Epoch 204/300\n",
            "\n",
            "Epoch 00204: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1530 - acc: 0.9500 - val_loss: 0.7728 - val_acc: 0.5500\n",
            "Epoch 205/300\n",
            "\n",
            "Epoch 00205: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1372 - acc: 0.9667 - val_loss: 0.7620 - val_acc: 0.5500\n",
            "Epoch 206/300\n",
            "\n",
            "Epoch 00206: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1609 - acc: 0.9167 - val_loss: 0.7318 - val_acc: 0.5500\n",
            "Epoch 207/300\n",
            "\n",
            "Epoch 00207: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1304 - acc: 0.9833 - val_loss: 0.7202 - val_acc: 0.5500\n",
            "Epoch 208/300\n",
            "\n",
            "Epoch 00208: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.2806 - acc: 0.8833 - val_loss: 0.7098 - val_acc: 0.5500\n",
            "Epoch 209/300\n",
            "\n",
            "Epoch 00209: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.2605 - acc: 0.9000 - val_loss: 0.6639 - val_acc: 0.5500\n",
            "Epoch 210/300\n",
            "\n",
            "Epoch 00210: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1337 - acc: 1.0000 - val_loss: 0.5976 - val_acc: 0.6500\n",
            "Epoch 211/300\n",
            "\n",
            "Epoch 00211: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1498 - acc: 1.0000 - val_loss: 0.6022 - val_acc: 0.6500\n",
            "Epoch 212/300\n",
            "\n",
            "Epoch 00212: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1532 - acc: 1.0000 - val_loss: 0.6203 - val_acc: 0.6500\n",
            "Epoch 213/300\n",
            "\n",
            "Epoch 00213: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1372 - acc: 1.0000 - val_loss: 0.6367 - val_acc: 0.6500\n",
            "Epoch 214/300\n",
            "\n",
            "Epoch 00214: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1215 - acc: 1.0000 - val_loss: 0.6646 - val_acc: 0.6500\n",
            "Epoch 215/300\n",
            "\n",
            "Epoch 00215: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1704 - acc: 0.9500 - val_loss: 0.7015 - val_acc: 0.6000\n",
            "Epoch 216/300\n",
            "\n",
            "Epoch 00216: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1471 - acc: 0.9833 - val_loss: 0.7087 - val_acc: 0.6000\n",
            "Epoch 217/300\n",
            "\n",
            "Epoch 00217: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1884 - acc: 0.9667 - val_loss: 0.7089 - val_acc: 0.5500\n",
            "Epoch 218/300\n",
            "\n",
            "Epoch 00218: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1250 - acc: 1.0000 - val_loss: 0.7185 - val_acc: 0.5500\n",
            "Epoch 219/300\n",
            "\n",
            "Epoch 00219: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1812 - acc: 0.9500 - val_loss: 0.7354 - val_acc: 0.5500\n",
            "Epoch 220/300\n",
            "\n",
            "Epoch 00220: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1088 - acc: 1.0000 - val_loss: 0.7447 - val_acc: 0.5500\n",
            "Epoch 221/300\n",
            "\n",
            "Epoch 00221: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1409 - acc: 1.0000 - val_loss: 0.7624 - val_acc: 0.5500\n",
            "Epoch 222/300\n",
            "\n",
            "Epoch 00222: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1439 - acc: 0.9667 - val_loss: 0.8113 - val_acc: 0.5500\n",
            "Epoch 223/300\n",
            "\n",
            "Epoch 00223: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1075 - acc: 0.9833 - val_loss: 0.8492 - val_acc: 0.5500\n",
            "Epoch 224/300\n",
            "\n",
            "Epoch 00224: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1338 - acc: 0.9833 - val_loss: 0.8671 - val_acc: 0.5000\n",
            "Epoch 225/300\n",
            "\n",
            "Epoch 00225: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1055 - acc: 1.0000 - val_loss: 0.8662 - val_acc: 0.5000\n",
            "Epoch 226/300\n",
            "\n",
            "Epoch 00226: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1188 - acc: 0.9833 - val_loss: 0.8639 - val_acc: 0.5000\n",
            "Epoch 227/300\n",
            "\n",
            "Epoch 00227: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1415 - acc: 0.9667 - val_loss: 0.8467 - val_acc: 0.5000\n",
            "Epoch 228/300\n",
            "\n",
            "Epoch 00228: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.0902 - acc: 1.0000 - val_loss: 0.8529 - val_acc: 0.5000\n",
            "Epoch 229/300\n",
            "\n",
            "Epoch 00229: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1100 - acc: 1.0000 - val_loss: 0.8702 - val_acc: 0.4500\n",
            "Epoch 230/300\n",
            "\n",
            "Epoch 00230: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1561 - acc: 0.9333 - val_loss: 0.9084 - val_acc: 0.4500\n",
            "Epoch 231/300\n",
            "\n",
            "Epoch 00231: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.0906 - acc: 1.0000 - val_loss: 0.9016 - val_acc: 0.4500\n",
            "Epoch 232/300\n",
            "\n",
            "Epoch 00232: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1494 - acc: 0.9833 - val_loss: 0.8623 - val_acc: 0.4500\n",
            "Epoch 233/300\n",
            "\n",
            "Epoch 00233: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1431 - acc: 0.9667 - val_loss: 0.7659 - val_acc: 0.5500\n",
            "Epoch 234/300\n",
            "\n",
            "Epoch 00234: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1551 - acc: 0.9667 - val_loss: 0.6920 - val_acc: 0.5500\n",
            "Epoch 235/300\n",
            "\n",
            "Epoch 00235: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1032 - acc: 1.0000 - val_loss: 0.6810 - val_acc: 0.6000\n",
            "Epoch 236/300\n",
            "\n",
            "Epoch 00236: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1204 - acc: 1.0000 - val_loss: 0.6700 - val_acc: 0.6500\n",
            "Epoch 237/300\n",
            "\n",
            "Epoch 00237: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1083 - acc: 1.0000 - val_loss: 0.6890 - val_acc: 0.6500\n",
            "Epoch 238/300\n",
            "\n",
            "Epoch 00238: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1127 - acc: 1.0000 - val_loss: 0.7106 - val_acc: 0.6000\n",
            "Epoch 239/300\n",
            "\n",
            "Epoch 00239: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1259 - acc: 0.9833 - val_loss: 0.7284 - val_acc: 0.6000\n",
            "Epoch 240/300\n",
            "\n",
            "Epoch 00240: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.0985 - acc: 1.0000 - val_loss: 0.7274 - val_acc: 0.6000\n",
            "Epoch 241/300\n",
            "\n",
            "Epoch 00241: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1676 - acc: 0.9333 - val_loss: 0.6917 - val_acc: 0.6000\n",
            "Epoch 242/300\n",
            "\n",
            "Epoch 00242: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1245 - acc: 0.9667 - val_loss: 0.6517 - val_acc: 0.6000\n",
            "Epoch 243/300\n",
            "\n",
            "Epoch 00243: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.0917 - acc: 1.0000 - val_loss: 0.6358 - val_acc: 0.6500\n",
            "Epoch 244/300\n",
            "\n",
            "Epoch 00244: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1160 - acc: 1.0000 - val_loss: 0.6438 - val_acc: 0.6500\n",
            "Epoch 245/300\n",
            "\n",
            "Epoch 00245: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1188 - acc: 0.9833 - val_loss: 0.6652 - val_acc: 0.6000\n",
            "Epoch 246/300\n",
            "\n",
            "Epoch 00246: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1412 - acc: 0.9833 - val_loss: 0.6820 - val_acc: 0.6000\n",
            "Epoch 247/300\n",
            "\n",
            "Epoch 00247: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1451 - acc: 0.9333 - val_loss: 0.6929 - val_acc: 0.6000\n",
            "Epoch 248/300\n",
            "\n",
            "Epoch 00248: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1354 - acc: 0.9667 - val_loss: 0.6831 - val_acc: 0.6000\n",
            "Epoch 249/300\n",
            "\n",
            "Epoch 00249: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.0844 - acc: 1.0000 - val_loss: 0.6848 - val_acc: 0.5500\n",
            "Epoch 250/300\n",
            "\n",
            "Epoch 00250: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1064 - acc: 1.0000 - val_loss: 0.6866 - val_acc: 0.5500\n",
            "Epoch 251/300\n",
            "\n",
            "Epoch 00251: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1006 - acc: 0.9833 - val_loss: 0.7077 - val_acc: 0.5500\n",
            "Epoch 252/300\n",
            "\n",
            "Epoch 00252: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1086 - acc: 1.0000 - val_loss: 0.7390 - val_acc: 0.5500\n",
            "Epoch 253/300\n",
            "\n",
            "Epoch 00253: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.0948 - acc: 0.9833 - val_loss: 0.7593 - val_acc: 0.5500\n",
            "Epoch 254/300\n",
            "\n",
            "Epoch 00254: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1373 - acc: 0.9833 - val_loss: 0.7560 - val_acc: 0.5500\n",
            "Epoch 255/300\n",
            "\n",
            "Epoch 00255: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.0933 - acc: 1.0000 - val_loss: 0.7261 - val_acc: 0.6000\n",
            "Epoch 256/300\n",
            "\n",
            "Epoch 00256: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1281 - acc: 0.9833 - val_loss: 0.7141 - val_acc: 0.5500\n",
            "Epoch 257/300\n",
            "\n",
            "Epoch 00257: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1432 - acc: 0.9667 - val_loss: 0.7180 - val_acc: 0.5500\n",
            "Epoch 258/300\n",
            "\n",
            "Epoch 00258: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1094 - acc: 1.0000 - val_loss: 0.7238 - val_acc: 0.6000\n",
            "Epoch 259/300\n",
            "\n",
            "Epoch 00259: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1132 - acc: 1.0000 - val_loss: 0.7673 - val_acc: 0.5500\n",
            "Epoch 260/300\n",
            "\n",
            "Epoch 00260: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1137 - acc: 1.0000 - val_loss: 0.7890 - val_acc: 0.5000\n",
            "Epoch 261/300\n",
            "\n",
            "Epoch 00261: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.0982 - acc: 1.0000 - val_loss: 0.8493 - val_acc: 0.4500\n",
            "Epoch 262/300\n",
            "\n",
            "Epoch 00262: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1758 - acc: 0.9167 - val_loss: 0.8842 - val_acc: 0.4500\n",
            "Epoch 263/300\n",
            "\n",
            "Epoch 00263: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.0852 - acc: 1.0000 - val_loss: 0.8962 - val_acc: 0.4500\n",
            "Epoch 264/300\n",
            "\n",
            "Epoch 00264: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1080 - acc: 0.9500 - val_loss: 0.8581 - val_acc: 0.4500\n",
            "Epoch 265/300\n",
            "\n",
            "Epoch 00265: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.0956 - acc: 1.0000 - val_loss: 0.7707 - val_acc: 0.5000\n",
            "Epoch 266/300\n",
            "\n",
            "Epoch 00266: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1417 - acc: 0.9667 - val_loss: 0.7353 - val_acc: 0.5500\n",
            "Epoch 267/300\n",
            "\n",
            "Epoch 00267: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1099 - acc: 0.9667 - val_loss: 0.7123 - val_acc: 0.5500\n",
            "Epoch 268/300\n",
            "\n",
            "Epoch 00268: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1000 - acc: 1.0000 - val_loss: 0.6969 - val_acc: 0.5500\n",
            "Epoch 269/300\n",
            "\n",
            "Epoch 00269: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1074 - acc: 1.0000 - val_loss: 0.6757 - val_acc: 0.5500\n",
            "Epoch 270/300\n",
            "\n",
            "Epoch 00270: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1232 - acc: 1.0000 - val_loss: 0.6683 - val_acc: 0.6500\n",
            "Epoch 271/300\n",
            "\n",
            "Epoch 00271: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.0963 - acc: 1.0000 - val_loss: 0.6834 - val_acc: 0.6000\n",
            "Epoch 272/300\n",
            "\n",
            "Epoch 00272: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.0967 - acc: 1.0000 - val_loss: 0.6702 - val_acc: 0.6000\n",
            "Epoch 273/300\n",
            "\n",
            "Epoch 00273: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1125 - acc: 0.9833 - val_loss: 0.6393 - val_acc: 0.6000\n",
            "Epoch 274/300\n",
            "\n",
            "Epoch 00274: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.0845 - acc: 1.0000 - val_loss: 0.6085 - val_acc: 0.6500\n",
            "Epoch 275/300\n",
            "\n",
            "Epoch 00275: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.0972 - acc: 1.0000 - val_loss: 0.5944 - val_acc: 0.6500\n",
            "Epoch 276/300\n",
            "\n",
            "Epoch 00276: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1143 - acc: 0.9667 - val_loss: 0.6014 - val_acc: 0.6500\n",
            "Epoch 277/300\n",
            "\n",
            "Epoch 00277: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.0803 - acc: 1.0000 - val_loss: 0.6149 - val_acc: 0.7000\n",
            "Epoch 278/300\n",
            "\n",
            "Epoch 00278: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1135 - acc: 0.9500 - val_loss: 0.5989 - val_acc: 0.7500\n",
            "Epoch 279/300\n",
            "\n",
            "Epoch 00279: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.0980 - acc: 1.0000 - val_loss: 0.5701 - val_acc: 0.8000\n",
            "Epoch 280/300\n",
            "\n",
            "Epoch 00280: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.0847 - acc: 1.0000 - val_loss: 0.5602 - val_acc: 0.8000\n",
            "Epoch 281/300\n",
            "\n",
            "Epoch 00281: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1125 - acc: 0.9833 - val_loss: 0.5600 - val_acc: 0.8000\n",
            "Epoch 282/300\n",
            "\n",
            "Epoch 00282: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.0976 - acc: 0.9833 - val_loss: 0.5820 - val_acc: 0.7000\n",
            "Epoch 283/300\n",
            "\n",
            "Epoch 00283: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1115 - acc: 0.9833 - val_loss: 0.6138 - val_acc: 0.6500\n",
            "Epoch 284/300\n",
            "\n",
            "Epoch 00284: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.0962 - acc: 1.0000 - val_loss: 0.6310 - val_acc: 0.6500\n",
            "Epoch 285/300\n",
            "\n",
            "Epoch 00285: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1069 - acc: 0.9667 - val_loss: 0.6212 - val_acc: 0.6500\n",
            "Epoch 286/300\n",
            "\n",
            "Epoch 00286: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.0857 - acc: 1.0000 - val_loss: 0.6026 - val_acc: 0.6500\n",
            "Epoch 287/300\n",
            "\n",
            "Epoch 00287: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1034 - acc: 1.0000 - val_loss: 0.5951 - val_acc: 0.6500\n",
            "Epoch 288/300\n",
            "\n",
            "Epoch 00288: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.0913 - acc: 0.9833 - val_loss: 0.6136 - val_acc: 0.6500\n",
            "Epoch 289/300\n",
            "\n",
            "Epoch 00289: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1128 - acc: 0.9833 - val_loss: 0.6273 - val_acc: 0.6500\n",
            "Epoch 290/300\n",
            "\n",
            "Epoch 00290: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.0909 - acc: 1.0000 - val_loss: 0.5787 - val_acc: 0.6500\n",
            "Epoch 291/300\n",
            "\n",
            "Epoch 00291: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.0697 - acc: 1.0000 - val_loss: 0.5616 - val_acc: 0.6500\n",
            "Epoch 292/300\n",
            "\n",
            "Epoch 00292: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.0698 - acc: 1.0000 - val_loss: 0.5898 - val_acc: 0.6500\n",
            "Epoch 293/300\n",
            "\n",
            "Epoch 00293: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1109 - acc: 1.0000 - val_loss: 0.5976 - val_acc: 0.6500\n",
            "Epoch 294/300\n",
            "\n",
            "Epoch 00294: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1029 - acc: 0.9833 - val_loss: 0.6338 - val_acc: 0.6000\n",
            "Epoch 295/300\n",
            "\n",
            "Epoch 00295: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1091 - acc: 0.9833 - val_loss: 0.7096 - val_acc: 0.5500\n",
            "Epoch 296/300\n",
            "\n",
            "Epoch 00296: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.0917 - acc: 0.9833 - val_loss: 0.7768 - val_acc: 0.4500\n",
            "Epoch 297/300\n",
            "\n",
            "Epoch 00297: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.0713 - acc: 1.0000 - val_loss: 0.8717 - val_acc: 0.4500\n",
            "Epoch 298/300\n",
            "\n",
            "Epoch 00298: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.1043 - acc: 0.9833 - val_loss: 0.9010 - val_acc: 0.4500\n",
            "Epoch 299/300\n",
            "\n",
            "Epoch 00299: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.0725 - acc: 1.0000 - val_loss: 0.7816 - val_acc: 0.5000\n",
            "Epoch 300/300\n",
            "\n",
            "Epoch 00300: val_loss did not improve from 0.54933\n",
            "60/60 - 0s - loss: 0.0919 - acc: 1.0000 - val_loss: 0.7292 - val_acc: 0.5000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAgAElEQVR4nOy9eZgcVbn4/zld1cv0dM9klsxM9oQk\nk5CwhBB2BGUTBEW5ooD7ctGroleuC/det4sb+tPrflW+igoqiCIKiqAoiGyyJgRCNkKWyTLJ7L1M\n7+f3R9WprurpmelJ0pNZzud55pnuWk91Vb3veZfzHiGlRKPRaDTTF9+RboBGo9FojixaEWg0Gs00\nRysCjUajmeZoRaDRaDTTHK0INBqNZpqjFYFGo9FMc7Qi0EwLhBALhRBSCGFWsO07hRAPj0e7NJqJ\ngFYEmgmHEGK7ECIjhGguWf6sLcwXHpmWaTRTE60INBOVl4Er1RchxLFA+Mg1Z2JQiUWj0YwVrQg0\nE5VbgLe7vr8DuNm9gRCiXghxsxDigBBihxDiU0IIn73OEEJ8TQjRJYTYBlxcZt8fCyH2CiF2CyG+\nIIQwKmmYEOLXQoh9Qoh+IcRDQoiVrnU1Qoiv2+3pF0I8LISosdedKYR4VAjRJ4TYJYR4p738QSHE\ne13H8LimbCvog0KILcAWe9m37GMMCCGeFkK8wrW9IYT4LyHES0KImL1+nhDie0KIr5dcy11CiI9W\nct2aqYtWBJqJyuNAnRDiaFtAXwH8vGSb7wD1wFHA2ViK4132un8FLgFOANYAbyzZ96dADlhib3MB\n8F4q40/AUqAFeAb4hWvd14ATgdOBRuATQEEIscDe7zvATGAVsLbC8wG8HjgFWGF/f9I+RiPwS+DX\nQoiQve5aLGvqNUAd8G4gCfwMuNKlLJuB8+z9NdMZKaX+038T6g/YjiWgPgV8GbgQ+AtgAhJYCBhA\nBljh2u99wIP2578B73etu8De1wRagTRQ41p/JfCA/fmdwMMVtnWGfdx6rI7VIHB8me3+E7hzmGM8\nCLzX9d1zfvv454zSjl51XmATcOkw270InG9//hBwz5G+3/rvyP9pf6NmInML8BCwiBK3ENAM+IEd\nrmU7gDn259nArpJ1igX2vnuFEGqZr2T7stjWyReBy7F69gVXe4JACHipzK7zhlleKZ62CSE+BrwH\n6zolVs9fBddHOtfPgLdiKda3At86hDZppgjaNaSZsEgpd2AFjV8D/LZkdReQxRLqivnAbvvzXiyB\n6F6n2IVlETRLKWfYf3VSypWMzlXApVgWSz2WdQIg7DalgMVl9ts1zHKABN5AeFuZbZwywXY84BPA\nm4AGKeUMoN9uw2jn+jlwqRDieOBo4HfDbKeZRmhFoJnovAfLLZJwL5RS5oHbgS8KIaK2D/5ainGE\n24EPCyHmCiEagOtc++4F/gx8XQhRJ4TwCSEWCyHOrqA9USwl0o0lvL/kOm4BuAn4XyHEbDtoe5oQ\nIogVRzhPCPEmIYQphGgSQqyyd10LXCaECAshltjXPFobcsABwBRCfAbLIlD8CPi8EGKpsDhOCNFk\nt7EDK75wC3CHlHKwgmvWTHG0ItBMaKSUL0kpnxpm9TVYveltwMNYQc+b7HX/D7gPWIcV0C21KN4O\nBIANWP713wCzKmjSzVhupt32vo+XrP8YsB5L2PYAXwF8UsqdWJbNf9jL1wLH2/t8Ayve0YnluvkF\nI3MfcC+w2W5LCq/r6H+xFOGfgQHgx0CNa/3PgGOxlIFGg5BST0yj0UwnhBBnYVlOC6QWABq0RaDR\nTCuEEH7gI8CPtBLQKLQi0GimCUKIo4E+LBfYN49wczQTCO0a0mg0mmmOtgg0Go1mmjPpBpQ1NzfL\nhQsXHulmaDQazaTi6aef7pJSziy3btIpgoULF/LUU8NlE2o0Go2mHEKIHcOt064hjUajmeZoRaDR\naDTTHK0INBqNZpoz6WIE5chms3R0dJBKpY50U8aNUCjE3Llz8fv9R7opGo1mkjMlFEFHRwfRaJSF\nCxfiKis8ZZFS0t3dTUdHB4sWLTrSzdFoNJOcqrmGhBA3CSH2CyGeH2a9EEJ8WwixVQjxnBBi9cGe\nK5VK0dTUNC2UAIAQgqampmllAWk0mupRzRjBT7FmlhqOi7Cm+1sKXA18/1BONl2UgGK6Xa9Go6ke\nVXMNSSkfEkIsHGGTS4Gb7cJXjwshZgghZtm14jVTjGd39uI3fBwzp55ndvby4Mb9nLq4idMXN7Ol\nM8aBeJrTFzd79tnZneSOZzo4amYtl66yJh7rjqf558s9vOZYq2J0LJXlLxs6ufCYNv7w3F4uP3Eu\nQgiy+QK/faaDN544D8PnVZpSSn766HbiqRxvO20BM8IBAH715E729qe44qT5tNWHPPs8uGk/RzVH\nmN8UppSntvdQEzCIp3I8srWLV7TPxPQJHti4n1OOauKMJc1s3R9jf8y6xh3dCXZ0JzmrvTi2Z19/\nirW7+rjwGGtOmr5khn9s6eK1x8+mL5nh54/vIBwwedcZlvszkytw57MdvP6EOdz5zG4uWz2X3z27\nm0tPmM3vn93D61bNJuQ3Rrwndz7bwcsHXNM8CMGlq2azeGaEv77YybpdfVx4zCxWzK4ru//aXX38\n7cVOz7IT5jfwquUtAOzqSfLSgTiLZ0b4zdMdLGqu5fUnFO/jIy9187rjZ9ObsK4vGjJ580nzufu5\nPc59TGXz/PTR7STTOQCCfoN3nr6QP67fy2uPm03I7+OWx3fQl8zyllPm0xQJOm357TMd7OhO8sYT\n57JpX4xlbVHmNYb5++YDzGuo4aiZER7ctJ9ndvQ6+5xzdCvC+inwCUEmX2D1/Aae393Pn1/Y57nW\nlXPqaW+Ncuezu2lvjbBmQSPPdfRx3NwZ3PbkTmbVh3jzSdZ8SLFUlj+/0Mn5K1u55bEdBE0f7z5j\nEZl8gZseeZlUJg9AKGDwrtMXURMwkFLy66c7uPjYWdzxTAddsTTnHt3K8fNmjHhfD4YjGSOYg7eG\neoe9bIgiEEJcjWU1MH/+/NLVR5zu7m7OPfdcAPbt24dhGMycab3kTzzxBIFAYNRjvOtd7+K6665j\n2bJlVW3rkeLzf9hAbdDklvecwhf+sIFndvbxwKYD3H3NmZz/jYcA2H7DxZ59fvrodm565GUAXnvc\nbHw+wTW3PsujL3Xzz/86l9a6EF+5dyM/f3wn/9jSZb+QUVbNm8FfX+zkk3esZ35jLactbvIc9/nd\nA/zP3RsAiIZM3nnGInoTGT55x3oATJ/gQ+cs9ezzkdvWcslxs/jiG44dcm2f+t3z1Nf46R/MsnFf\njEde6ibk9/HI1m7aX9jHnz96Nt+8fwvP7uzjkevO4Qd/f4l7n9/Hs5+5wDnGzx/fwXcf2Mq6z15A\nfY2f3zzdwRf++CJnLGnmzy/s42t/3gzASQsbOXZuPf/YcoBP3rGeF/fG+Omj29k3kOKb929h64E4\nNz60jbUdfXypTFsVsVSWj/5qHWAJPQApoaM3ydcvP57/+PU6+pJZXtgzwI/feVLZY3zr/s08sOmA\nZ/+5DTU8vPwcAG58aBu/e3Y3V50ynx8+tA2Ai4+bhd/w8fPHd/KN+zezZkEDf9u4n6//xbq+ZDbP\nV+/dxOKZEU5c0MDj27q54U8bPed9rqOP+17oZHfvIJetnsNnfv8CACG/j6vPsiZmS2XzXHu7dX1d\n8TS/enIXl6+Zxxdffwwf+PnTnHN0K9+58gQ+/fvn2dUziBBW+9fv7ieRziMEBEwffcksd19zJt/6\n6xb+sqHTc63hgMFrjp3Fb57uAODfXrmYH/79JT5ybjvfvH8LAK9c1kJrXYhfP9XB9X/YwEd6lvKt\nv1rrTpg/g86BNF+9d5Pn+hY01nLxcbN4fvcAn/jNc2zYM8BPH90OQEtdaMopgoqRUt4I3AiwZs2a\nCVclr6mpibVr1wLwuc99jkgkwsc+9jHPNmqSaJ+vvDfuJz/5SdXbeSSJ2z06KSVbOuOA9bK66Ulk\naKwtKs2ueNr5nMjkiIb8HIhZy3qTGVrrQiTtntR9dm9t874Yq+bNYOO+2JBjKDZ1xlyf40O264pn\nPNtLKYmnc2WPBXAglmZvf4pkJuccK2RavfFu+1gHYmm64mmklByIpRlI5ZBSOi4+dV1b98c4cUEj\nB+xzJUrOu6kzxrFz653t/7je6jdt2DPg/IYA63b1lW2rYrN93T96+xrOW9EKwNt+/E82d8Y4EEvT\nl8wO+a1KGczmOXlRI7e/7zQAPnfXC9zxTIfnd4mlc06bAHoTGVrqQmzqHHCO776+zv6U3b4YJy5o\ncO7F3z/+StrqQ6z4zH3c94JlhUgpvb/Nvrjzudt1zvte6CRXkGzujLG7b5BEJs9m9XzEMrz3zEV8\n6pIVvOmHj5HM5ElkcuQLEr/hc47fFU9z5pJmfv7eUwD45T938l93rufBTQec87y0P05Bwp6+4qRv\nm/bFaK0Lsck+34t7Bzzt7RxI4ROw4XrLi77iM/eyqTPGxcxi4z5r2z88Z93jP374TFbOrh/2fhwK\nR3IcwW68c8rOpTjf7JRg69atrFixgre85S2sXLmSvXv3cvXVV7NmzRpWrlzJ9ddf72x75plnsnbt\nWnK5HDNmzOC6667j+OOP57TTTmP//v1H8CoOD8lMnlS2wN7+FDFbKWTzBc82m0uETnei+JIrRRIJ\nWX0XJahmRoPO8aEouJSy6S4jvLd0xggYPk5c0OCc0y38SwV+Jl8gX5COUHeTL0h6khn6B7Nk85KG\nsJ/ueMZpe08yY+2byJDOFUhk8nTFrWWDLkWott/stDvjXFdXPEM4YBAwfGyx26sEnVII6jo6ByxB\nurd/5EQCdZxlbVFn2bLWKFv3x3nRFlqnL26io3eQhP3bl5LJFQiaRRESDZnE05aCc1/TvoFiW9Tv\nrK5zS2fM87t2JdR6+zrte9EUCRI0DRY11zrbNtQGnOM1hP2e50ft1xD2O/dzc2fMEcjbuuL0D2YZ\nzOYdd1LQ9JHKFUjnCsTTOeLpHN3xjJWlF8/QFCl2Upa1RezrSdMQ9nvavKd/0HFHqmWb98ec7wHD\nRzhgsLkzxpb9MRY01RLyG4T8Bgubah0ltWV/sZPiE7B4ZqTsfTgcHEmL4C7gQ0KI24BTgP7DER/4\nn7tfcHpHh4sVs+v47Gsrmdd8KBs3buTmm29mzZo1ANxwww00NjaSy+V41atexRvf+EZWrFjh2ae/\nv5+zzz6bG264gWuvvZabbrqJ6667rtzhJw2DmTxB0+cI6gVNYbJ5S2BEQyaxVI7NnTFOParoxnEL\niHgqB/UQCZqedelseWWyqURgutnUGWNxS4QVs+r43drd1otuC626kDlE4A/aSqbcsXqTGdyV3E9b\n3MQ96/eRyFjHGkjl6E1mHMHUHU8754qncoQD1vUogaYEldo+mcnRnbCsH/fvV6qsdvQkgaIC6CnT\n1tLfoMZvMGdGcQbL9tYoqWzB8ftfctxsHn2pmy3746wq445I5wo0hL2KQEpLedUGi7+jWyn1JDKk\nc3le7krY1xtnMFtUND3xEkWQyBA0fdQGLAtLKSuwLBJ1jtMWN/G3jfspFCQ+n3CWn7642bGaYqkc\n/9hi9eCzecnTO3oAHAEf8hsciKVJZfPEUjn8hhUjiKVzdMfTNNUW4w9LW4sKVJ3DfQ8WNIUZGMyy\nuTPmsYJ39CSZXV9DczTI5s4YnQMplrZEXMeNOEpDPQsAC5trR435HArVTB+9FXgMWCaE6BBCvEcI\n8X4hxPvtTe7Bmmt2K9b8sh+oVluOJIsXL3aUAMCtt97K6tWrWb16NS+++CIbNmwYsk9NTQ0XXXQR\nACeeeCLbt28fr+ZWDWURqJ7oytl1ZGyLoNYWhqUWQVc8wyw7aDuQsoRFXcjqfTnC1NVb9QnrGJlc\nge22oCl184BlLbS3RmhvjRBL5dg3kHIEZ3tr1GOJqLZbxxpqXbgFrk/AyQsbAcuHrHrb+wfS9NoW\nTFc84wgpdU3u42zZ71Vgg5m8LYQCLGuLuiydUveV9d/tlugfzA5pb+lv4HMF0pe2WgLpj+v30lgb\n4HQ7tlJ6XxSZXIGgvyhCIkHr3sRSRReZatNs+z52J9K83JUgX5D4hHW9XfFMUcGXsYyaI0HHhaba\n6P5tAE49qolUtsCu3qTn3Kfa16Au84/r9zqfH3upG4BmlyJI5wqkspZFoO7P7l7LneS2COpCfufZ\nVOdw34No0GRpS5TNnXH29KdcrlForA3Q3hLhhT0DbO9O0u5SKu2tUbZ3JUhl82zpjDltbW8pblMN\nqpk1dOUo6yXwwcN93oPtuVeL2tqiKbtlyxa+9a1v8cQTTzBjxgze+ta3lh0LEAgEGMzkyEswDINc\nrrxp7mZ/LMU/t/Xw2uNnj7hdOpfnzmd286Y188gWCtz08HYKUvLuMxYR8vv41ZO7uHTVHO5et4eL\njm0jagveu9bt4YzFTZ6sjOF4eEsXLXVB5wGX0nKDpHN5NnfGaYkGmRkJOq4h9ZJsdvl4CwVJbzLD\nyQsb2et6kcIBr+89lioKu9MWN/HI1m7+5+4XyBWst7LHFiyFguT2p3Zxwco2dvcNclXrfKd9mzvj\ndMUzCAFLWiL8ZUMnv3m6g53dCS5fM490rmCfK8f9GzpZ2FzL/oEUzdGgRzksaKpltquHvbQ1ypPb\nex3hDlYwVimWF/cOsL0rwXkrWh2BpvzcbtdQTyLD/MYw7a1Rfr92D7FUdoiyUqhjA3z5nheZVV/D\n209bQENtgF/8cwf7B9Jcdcp8NnXGOLvdW5FY9XK74hlOPaqReY1hgqbPcVWAlf3lE4Lj580gky8Q\nMLwWAcC6jj6e3y0cQZrM5Dl+7gz29KdY39HPX1/c79yvZ3b0Mas+xNyGGjbui3liKr0Jy8Xmccm4\nhGYykydWyBENmhwzx/Kdf/uvW/nkhcscxXqabWGqZ0Nd2xMv9/DYNksRqJ5+0PSRsp/TfEGSt58h\npQibI96kj/bWKHv7U5x2VOOQexAN+VnSEuFXT+7i6/d5g8FNEUup/9oOMre3eRVBQcL1f9jAnv4U\nZyyx2u3ephpMimDxVGFgYIBoNEpdXR179+7lvvvu48ILyw+12NufIpevPC7+k0e28/0HX+Lco1sc\nd0M5Ht7SxXW/Xc/Rs+pIZfN85V4rI2NZa5RoyOS6367nl0/s5LmOfgazed5x+kL6k1k+fOuzfOri\no3nvK44asR1SSt76438CxSyglO2+SWcLdPQmWdAUxm/4yOQs37sS8tu7i6mM/YNZ8gXJwuYwj23r\ndgR+vsT/HE/n8AlLCF991mKe3dnHbU/uoi5k0lAbcATL83v6ue63652e9oKmMAttf/OuniTd8TQN\n4QAt0SA9yQwf+/U653wXHTPLadfVtzzFG06Yy+Pbulm9oIEL7EDrmgUNrF7Q4FGU7bbJr3rxgOPW\nAPjm/ZvZ0Z3kmc+cTyKTpy5k0hVPk0jnnOtLZq0YwQnzGxzFtWV/nO54hqUtEVK5PAHDx0vuNFCg\nqTbA7U/toiAtAf2GE+bw33daYzsLdsDa7W8Hy+12yqJGntrRy9ntLRg+waLmWrZ3J51tvvjHFwn6\nffzivaeSyRUIuGIEKn7z1Xs3Oq4fxZyGGkyf4KZHXqYgrd//wmNm8cjWbl7uTnDu8lY27ovRkyxa\nOpvt+IFbEZy4oIH5jWF29lgKNZ7O0RQJsLwtSnMkwB3PdLC8LUp3IkPI72PxzFpOXNDAG06YSyKd\n5/nd/Vywoo3OgTQv2C5klaAQ8tuKoMTdqO5fY623E3Te0S0UpGR+o/d3VL/lmUua+eUTO/n9uj20\n1YXI2797U22QU49qojZgYNqxKvf11df4+dWTuwgHDP71FUexty/FWUubh5zjcKIVwTiyevVqVqxY\nwfLly1mwYAFnnHHGsNumsgUklSsC1WuLufzO5VBB1ng6RzpX7D12xdPU2ub5cx39QNHPrlwM8WGC\nhm7KuWJUNk0qlyeZydNYG8Bv+sjmCyTsdQHTR08i42TSKEG4oMl6yeJ27zJj986LFkGOs9pn8tN3\nnQwUsy8APvTLZ5yXXV33fjtwWRs0HTdTLGUFBZtqAzTWBjw+/1S24OllFyQciKc5EE/TP5h1evI/\nfNuJNEWC7HApM9WL82QpuXrXO3uS5AqSp7ZbvuplbZYFsas36SjPZDpHTyJNcyRAu+0W2bwvRnci\nw3lHt/Dly47j2l+t9SiCptoAT3/6fKSUnPiF+9ncGfPcO9UGFWh38ys7A0jRHAk6VhVYz0BBWlZZ\nukQRRO3nZ2dPkkLJo1sX8tNYG2B/LM3ytij3/vtZPLPTyt+XEuY11jifLYFcsBVB2uM6aakL8dAn\nXsUrvvo3UlnbbRYJEg6YPPFf57H4v+9hIJWly/bpCyG4499OB+CNJ851jvP4tm5HWTkxAtMgmck7\nLkvn97LvX1OJRfC20xbyttMWAjjpw85vETI5b0Urm79wkbPsX77/qKUIIgGOmVPPC9cP7QTOnlHD\nus9e4Fn2ymUtQ7Y73GhFcJj53Oc+53xesmSJk1YK1mjgW265pex+Dz/8sPP5QHePk2b25je/mSuu\nuGLU86qHNZbK0Vp+/A9QFObJTN6jCLoTmSGCwVEuaesBH8x40z3LUc6frARpNi+Jp3LMbaghYPjI\n5iUD9suzsCnM5s44A4M56sN+R6EstBWB8junSxRBPJVjfuPQQV5gCTHlulHXrbJSwn6DkN+H6RPE\nUll6ElbPs9T1lckVHEWm2NmdIJMrELf38wmcQWnu/ZfMjFh+cNdv4v59VLBc+arbbVeSW1ns7U9R\nkJZwn9cQJuT3WT1nV6qtO+UWij1zIQRLWyJs6ox5MpSGc3WUoykSYNeuokWQzOQxDeH8NgGjGMBU\nbsRsGUs2EjJpigTZH0s7Lih3kHReQ/EeLmisZU/fIJs6LYVXrp1hv2kF0uMZFtiD/Hw+QSRoOop9\npOtb1hblzxs6CQcMp+OkYgSlqPvXXDu8W7QpEvAoAnUPPNvY96mpdvTffbzRZagnIGnXS5sr7VqV\nIZHO0dFrBQndPvNyqPXJTM7T0+2OZxxhq1AZDzGXr3c0lJBxP+xuIdSbzFDjN52epOqpq55/l0q7\ntAX2/MYwQuCknDoWgb3dQCrnCKBSGmsDxFI5MrmCc92qBx8OmAghiNgpj10Jqwepen21AYOWaJB0\nLj9EAarskFgqR5ctkFW6YG3AIGj6EMJSCo21Ace1UhcyPW4WxaMuRQBeV5IKfjZGgvh8gqUtUZ7a\n0UO+IB3fdqnyirqE0LK2KFs7454UUNX+UlfHcL+hOzCdzBRdJ0OCxWWEn6IuZDqCeZlt2URDfidr\nqa0+hN9WMOGgwdLWCM/u7COdKwzpiQPUBKzee3fC6zqKKkWQSA9RkG6UMnLv606FdaPuWbl2KEqV\nRLlnUt2nSuJs441WBBOQlKtXUkmcYIvL7zya+0YJ1MFMUcBFgybdibSzDqwsi4FUjs6BtOOWGYsi\ncL9U7v36B7OEA4bz0iufvfJXK6GjBPbMaJBIwHQEeVER2BZBOusRfG7Ui9uTKCo5pWBq7KCzSl1V\nvuhm+yVd0hol5Ddsi8B73cp1FLfTCt0CRwhBU22AhrClHNQ6wyc4apg8cOW+UhkxbldSR4+l4Jvt\n47S3Rnl+94Dn+koFlMrAsY4ZJZbOsc12HUWCptP+SnqmzZEg8XTOGfw3mMmRyReQUg4bLC5HJGg6\nv4U3S8a65sbagJMeGQ4YLGuLunz4QwVnOGCQsN1m7rTOaMhPPJ2lJ54ZUeCqoLP72COlZwZNn5Oo\nUI5SpRMNjmARVGCJjTdaEUxAUh6LoGDnNQ/f03e7G0p79aW4e/dKwM1tDNsWgTcDRx1buYZS2Tx3\nrdvjDFoCa0Tvju4Ef3xuL1+5dyP/2NJlncelVNyulYK0hLDfFiA9Tiwg7Pmusngawn6r165iBLb/\nti+ZZdBOSS330kExG6QrnnauWykapQgiQT+99oCwptpgUVi1RAiYPjL5AslseQUYS1mjZptKBFVT\nJOhyA1jrGsJ+R8moAUWlFLOYivdTWQRKqLW70icdi6DE5eDujSqBt67DGmk8t6GY1VSJQFLH/NPz\ne1lvJxBk7EFXgCdGUFsSmzJ9wlEO0ZDfaW9puiRYbir1m9T4rdTLkdoZDhhFt5lrfcRW7F0llkIp\ni5prMX3CUbBgxSaGu/6m2sCIhR4dpezcgzKKoGSbiYRWBBOQTK7guBry9tD40iwMN+70vtFcQ0qg\nDmbzDGZyCAFzZoToTmScdUc11/IOOwi2ozvhLO9OpPnwrc9y+Q8eA6xU1A/+4hl+8sh2rvvtc/zw\n7y+xfyBNwPB5RpiWulZq/IYjQLpLYgEqNrCnb5DmSBDT8Dm9dnVOxW47Z344l0RrXdA5lrKUVFZK\n2F+0CHa6TP+GcIDj583g3KNbCNiZTYO2Ijv1qEZOmF8cWBVP55zgn5szljRzxhIry+OkRVZq4cmL\nGjlpoZUdcuriJkdQHDOnDr8hWDm7jqbaALUBg509SXzCapsalavOcdriJsIBg6bagGNBrJhdx6Lm\nYk0lt2Jc2GwpWBV3mGfHU9y+8ZFQCuijv1rHa7/7MAVpxWmUQnZbfobtowerjs5Z7TOdgHwkZHLS\nwgZWzZvhtAGsQOiy1ihzZoSd9oQDhpNV0xD2exSHoibg/m3cFoHJvv4UmVxhRJ9+wPTx6pVtnOJK\n/QyWsQjOsQvonbyoccg6NycvauTUoxqdOFu5Z3L1/AaWtEScbLWJhA4WT0ByBUnINEhkchW5hjbv\njzOvsYZdPYMVWATFwG86l6fGb9AcCfJcRz+xVI6GsJ+/feyV5PJFF4zqve8fsHrrO20f87YDCXIF\nye4+67wff/UyPviqJdz40Et86Z6NJDJ5IkHTEyMAbNeQdczeZDEWAEXFsHl/3On9RoKmI8gzLreZ\nCgQPFyNYotI398cdZabcIo5rKGjyrJ290mT7+n//QSub64cPbSPtcg394r2n8o8tB3jnT550zrG7\nb3BIPv51Fy13Pl97fjvXnt/ufH/f2VZRtHO//iCdpHn/2Yu55Lji2I/GSIBEzyCLmmvJFgrEUjnb\nMrIUwXFzZ3gyowBm1dfwwK+6TL4AACAASURBVMdeyfcffIk/PLfXI4SUIFalKFRQdiT/uZty22Vy\nBec+lPrV1b368LlLedWyFi785kPO8rPbZ3LRsbM825+2uIn7PnoWYHUQwHo+VsyuK5tVowj7jbIu\nrkjQLMZVRrnG773FOwVKuRjB519/DP/f5cePeByAS1fN4dJVc7j8B4867Sjl+HkzuP/as0c91pFA\nWwQTkFxBWgFHBLnC0CyGUjbvi7F6vtXbHE0RuLOGkhlLETRFAvQkMgykso4QMQ2fUzdH7bM/5h3E\npFwYKquiscQ1MVxsQdXNgaLPvrE2QH2Nn+5EmkJBsqUz5rgHoiH/kBgBFBVBuZdO7TdnRg2b9hXd\nW2DFP9RLHw2ZTpZLqU85YPhI5wpOeQzDJxz3jiKblwcV/FO/0RC3kv19aWuEsN+6LhVvGP2Ypuc/\nWNfpN4TjzlOuoUrbXC7zxq0IAiXCU51b9cbV97oR4gcKxzU0gi9e4d7GEywO+V33c2wumNIYQcDw\njbmsg7qvw3VOJipaERwGuru7WbVqFatWraKtrY05c+Y43zOZkWu+uLnpppvYu3cv+bzENASGT4ya\nNdQ/mGXfQIrlbXXUBozRg8WOayjHYCZPTcCgsTZIriDtofHFB7gpErSCyKmh4wji6ZyjCFQWiuqZ\nKcHstj7c1ARMxyLojlvpl8rd0R3PsLtvkGQm77gEIiHTkzWkXB9dMWURDC9k2lsjVpzDpSBVxpA6\ndvF6vYIjYPqcYLESUuWEy8EE/xyBWbJvMbMm6gi7Sn3K6pgR1z0UQhAN+UnY90C5ZZorPGY5hZHJ\nF5w4VqkiUL+n+k3UszBSRpGixokRjFER1HpdQ4pSpT0apUK/kjaXoq53pGdyIqIVwWFAlaFeu3Yt\n73//+/noRz/qfK9kLgLFTTfdxO49e5FIDJ8P0xAe15CUQ5WC6o23t0Y8PefhKA0WhwOGI3y2dyc9\nD3+jXd0xXsbK2NIZc+rBOCZ6xNsLjKWHtwhU1lBv0qozI4SgKRKgO5F2FIyq8FjnihFk8gXqaixB\np+IJIyuCKNsOJJw0VfAKEXfPrVTgBm1FMJjNO8KpnLuhtFdfCVFHYA5nEUQd5VOpG6ecRQBei0ml\na1aqvFQ6bCmJtK0IDK/wVL9nqXVYSQ9ZXe9I2TnOti6hrap/gjc+Uunvpii9zoMR5sPdg4mOVgRj\nwCpHm3ZqkFTCz372M04++WRWrVrFBz7wAQqFAr3xQa56y1s59thjOeaYY/j2t7/Nz37+S9auXctV\nV17Jm179CmQ+i+kTpHLeUa1gZe/s7R8kmck5wri9NerkxCv6k1lufWKnR4EUxxHkSWbz1ARMR/gc\niKU9Jnyz7TIq527a3BkbMnhMKRRHESjro2RAVk3AFSxOZDxuku54xrmmJbZrKBI0ORBLc+sTO0ln\nC87xR3MNqd8lky94UjLdgkbta/qE409XqKwhZTkBBE2DaND09OQPxiKIBE1rIFqN95yNyiJoKyqC\nSnu2yhIoFULquxUPUoPQKjumSoctRbnahgjPoEkkaDq960jIRAiv4B4OFSyuqSCIre5HQ9iPaZQf\nyzBWReC2CKIhc8TnajgijlU2uRTB5GptJfzpOti3/vAes+1YuOgG4ukcu/sGGczmmdtQfjSrm+ef\nf54777yTRx99FNM0ufrqq7ntttswZ7Sxp3M/69db7ezr62NnHJauOIbvffe7RGYvoSYUJF3IegS7\nJdAF3QmrrHFvIsue2CA+YfX03Nk1AHc/t4dP/e55Tpg/g+Vtdc4EK1AMFof9Bm31RaHgfoAtwdxN\nY3joC7WtK8GunqRTahmG9gLdSsdN2G84yrQvmWWmLeja6kP8Y8sBNu0bYFZ9iHpbSB4318rU+c/f\nrscncFkEtiIYxSIAb2zB7XpQiq+xNuCpxAmW0E/n8iQz3rIdr1zeQiRocusTO+3faeyKYM3CRsst\nVnLOkxY28Oi8GSxqrnUEYqWKZvHMWpa0RFhZMrWkuqfhgEFjbYAT5s9wMpgq4exlM/nDur2elGD1\nnJW6hk5e1Ig7y3LNggZ29w4Ouc5y1IzFIgiUt6gcC8SljCrFnT76qmUtTtbZWFg9v4GTFjYMSaWd\n6Eyu1h5hlPCq1CK4//77efLJJ50y1IODg8ybN49z33A627Zs4cMf/jAXX3wxF1xwAcSt+j552xVk\n+nyeUZtQtAgKrvOnc3mCpuEZXq/ocipaxljeVkcyk3eOkczkSOcKtNX5WdBUi98QZPPS6yqJBOhN\nZukb9MY5AqaPbQcSFGSxPk7A9BX9wfZ/J1icLY0RFIfy9w9mnYyhJS0REpk8/9jSxco5xZmY1Ly9\nn/79CxRkMRNGlXYeKQ1ySUvEmYbQfX5FZBgXDeCkjyZdFgHAd648gX39qaIiOIhg8ZvWzONNa+YN\nWX7O8lbOWW4Vsqux73+lrqemSLBsVoq6pzV2kbM7PzB8jatyfPmy4zh+7gyu+22xgzWcInjH6Qt5\nx+kLne+XrZ7LZavnUgnurKHRCA8TP1HP3sFYaWpWOYBPX7KibC2m0Xj1yjZevbJtzPsdaaaeIrjo\nhqodWgkT3wgDS7zbS9797nfz+c9/3rP8+d39/O5vj/LSMw/zve99jzvuuIMPfearQLG6pmkIz4Op\njgdFRSSBRCbvvIx1Ib+nHr2Timm7ReKeQV7WwCA1uGtRcy2bO+Pe4Kn9kql0UcWCxrATm1Cllptd\nA26GuoasWISyDMIBw1FI+YJ0BICq39+dyDhlCBRuBVVXYx1fpZ6OFFysCRgsaAx7Sju4BY0Kjpfr\n1QdcMYLh6vn4DVFRRszBoBRc40EINTfqflQiYIc/hteFFbetvVJFcCiEDyJYXCrw3RbeWHF3vMoN\nLpvKTK+rPUQKdjXQytQAnHfeedx+++10dVmjbbu7u9mxYwddXQfI5fNcfvnlXH/99TzzzDMA1NZG\n6B+whtUbPjHkYXSEp6t7G0/lnJfRnW8PQyf5UK4awycYzBbTRwGnlK7bv6x6uqlscYBbOGDQWhdy\nMoWcofquF7I2YHrqAw1m8k4ePFg+YHdpAiUA3JNvLC0ZROR2WTkWQSJDwE7rHAl1LLVdjb94rNIs\nFzfurKFS4VQbMBDCEjgjjTg9FJSwqzTDZzicGMEhuCtK3W/qORuuPs/B4GQNVZI+6lcWgbfXPpKF\nNxrujlc1ZwObiEw9i6CKKPlb6Xt/7LHH8tnPfpbzzjuPQqGA3+/n//7v+3T2DPLZj19DyPQhhOAL\nX/oSAJe+6S1ce80H8AeCPPfs0/j9fnxCULBP7PwveIO/7px4t2uo1CJQ62ZGgnaMoOAI4dkzrNmW\nDNfFuXvJMyNB9g2kiARNmiLFUs3KB+9+IX0+4akPlMzmaaj1OyOBw36DQbPYTvXi14f9tNYF6RxI\neyYgUdemUDGCvsGsE0cYiWWtUf6yodO5Bo9FoARHGfdLsEywWCGE5Yo7mIyhSlEB1kMtUubECA5B\nuJUGoNWzdDgVQfhgXEMlClxZLpVUVi1FCX+fsJIHphNaEYyBoiIY/iG55mP/SSZfcHz3V111FVdd\ndZWzPpcvULN3gNvvfYij2+pI5/KkcgX29A3y6te+gTe96U2ksgUn7TTkN4r1/LP2zEmeLCCXRRAy\nSWbyfOEPG2irDzkxgp09SQYzeeflbakL8vKBBOlcweklzqq30grdE427BVBrnSVEoyHTY3YvbqnF\nJ4a+kLVBk588sp1LjpvNYCZHNOjH8AnLFRQw8KeGWgRgKZbOgbQzKljhHS1rfZayMuGmSjG01JVR\nBMo1NIxFkM1bAfZywilqK8VqMZz7Y6wo4XhIrqGSLBiVIFCaPnooOFlD/tHFUvG3KbEIgsMr9tFQ\nSi3kN6pm5U1UtGtoDKiJYoZ7RnL5Arv7Bu1p9srn87vjzLlCgT39KY9fP1+QHldHfY1J0DZZ98fS\n7OpNUigU4xTxdM5xsxw3t55oyOTmx3fwhT++yLauBPU1fqS0ipep0aXzGsLE0lYVSSUcLl01m9qA\nwb+4AntzZtQwvzFMbcDgJHsu3kioWDxNCOuFO2d5qzMloELVZvnfv2yiK26Vag6aPmdUr9/jGiq+\n+BesbOP8Fa3OJDkKd2qn+3MlboRTFjWxpCXizATl3qelLsjxc+tZs2BoFo1SsAOpbNkskLPaZ3Lm\nkurNHHX8vBkcN7feM8H8wRBxXEOHMUaQLh8sPhSOmVPP8rYocxtHv955jWGWt0VZ7ar9BFY66YkL\nGpwaT2PB5xMHNZp4KqAtgjHgWATDRAnc5aOHm12s4OrN5wpySC2hgixOtA0wMxoiEvSzZX+MXF4i\nkQjAb1obxVI5xww+Z3kr6z/3atbu6uP133vEmkC9NcoT23vojmfYsj9OwPSxrC3KH9fvBXC5hmqG\n1HapCRg89IlXAfDPbd386OGXqQuZjsuo0S598KN3rBlynd++8gSCpo+/vNjJwGCW1x0/m8e2deMT\nAiGERxG4BdTbTl3A205dMOR47hhBOGhg2qOuKymc1lYf4v5rz+b2J3d5rhms3t/vP3Rm2f2UgpWy\n/AChG/7luFHPfSictLCRu4Zp21ioOwzB4iExglT5cQSHworZddz772dVtG1dyF92W9PwOTOSHQxB\nv++wXtNkYcpccblRt9U6x3AWgXtCmeGa41EEeUm+pJaQlHJIVpL6qpSLRGIIgUTaMQLvC+6e+and\nHp3bk8iwuTPGkpkRj1CttJeoBK4VI1ATbIzssmhvjdKXzFKQ1ueQ6XPO5+5JVuLecQuigFF8WcfS\nyy3mqVfW/3FXo5xsA4TcqLZXko0zHCo4rhgufXSyE/Ib09IimBJ3MRQK0d3dXXVloNw6w50llS1g\nCIHp8w3bFrdrKJ3LDzlWQQ5VNKWKQUpJNjHAjr4ssXRuyMtYGzSd4mIqmNudSLN5X4z21ohHeFYq\nHGoCxYD0cFMklqLm7AWrXETQbzjnCwxjEQyH3/A5WVQB0+eZxKRS1LkrfdGDnklXJlcRMTfFcQQH\nr8xUcFxZq1NVEQTN6WkRTN5ujou5c+fS0dHBgQMHqnqe3mSGRDpPMmTSWyZbRZX6zRckfX4fA2VG\n5KayeadGzoDfx2DWaxEYPkHQ9JHcX9w3X5B09heDuBJJFpPv/LMXKcu/jMtao3T0DrJkpjWgantX\nkj39KdpdpQugckFa41gEfscVNVo2iyoj7TcEC5pqPS+YqjVkHbuyNkSCflLZ9EErgrHUsgHv73ow\nBcgmCu6RxYdCXciP6RP0Jq0R72IKZteE/EbZeQmmOpP36Xbh9/tZtGjRYT9uLl/g5sd28JZT5xM0\nDT76q7Xc+exe3nfWUfzna472bCul5KrP/4ULj2njka3dnLiggW+8+Wj+tH4v7W1RFs+McP+GTp7r\n7Ofbf90BWLNy7SiZwzZo+rhs9Vy+fFnx+PF0jks+e59nu3975WIG0nYpYGOoImhvi/LXjftpqQvS\nEA7w+DZ7XtyWqMc9VWkv0T2Ri1IAo+W3t9WFiIZM5syosXv0RfeC4RPOiN9KBVRdyKQrniZoFguh\nVZJhohhLCQPwKoLJVkTMzeEYUKaOEzB99CazTtryVMuuCfl9hLRFoHHz5PZerv/DBuY3hjlvRatT\nTrlcaegD8TS9ySztrVGeeLnHmcHpY79ex+tPmMMX33As1/12vTMVY2NtwFECZy5pZsPeAXoSGdK5\nwhDTtNyD6c7xLy1FAdbMSg9v6WJuQ5im2gAb9loD1Za1RckVpD31njUbWSXU1fg5eVEjqxc0UBsw\nOHNJM6eWZAqVIoTgdcfPpiVqjVE4Y0mTE2gXQjj1/isV5qpXHjR9Tq9tLMJtUXMtR8+qG1KLZzjc\n98Fdnnuy0VIX5IT5Mzh+3ozRNx6BVyxtJpuXvNyVIJbODTtF6GTmjMXNh5RdNVmZenfyMKLy8NUI\nXVUzR83e5WaLqwpowDTI5grk8gUSmTzd8Qz5gqQnkXZiBMfNrefBTZYr6xtvXsXfNnbyyTusWi6l\nPmzT8DlZMgp3oLacRXDSwkbuvsbKOFG+/Bq/wZwZNfh8gqc/ff6YfgvDJ7j9fac533/+3lMq2u+L\nbzjW+fzxVy/3rFOKoFJhrnq2lmto7MHiGeEAf/rIKyrefqpYBEHTGHN9oXL898UrSKRz/PTR7bZL\ncuoJzFJLf7ow/WygMdAdL06kDsVyyuUsAjUnbHtrlIAhyOYLTq51dyJNXzLjCRSvsntnaoL2oGd4\n+9DbUqoc3ANmRgvYNbsmPq+kCuR44bfbXakiUL7ugOFzygEcSibMaASGKW88nXE/a9MxqDpVqeqd\nFEJcKITYJITYKoS4rsz6BUKIvwohnhNCPCiEqKxM4TjRbU+jqEo1qKJp5aqPbtkfoyFsBVL9hlWa\nQGVWdMczzrEUx9ullRvCAUzD53nBymW1lL50HotglBdSbVtav+dIowLGlfbqVfZL0F+szHqofu+R\nmCoWweHEtGM7oBXBVKJqd1IIYQDfAy4CVgBXCiFWlGz2NeBmKeVxwPXAl6vVnoPBUQS2a0jFCLJl\nJpTftC9Ge2vUGSyVzUlHEXTF046bCawe//JZdrE2223j7n2We8FGsghKxxGUos5RWr/nSBNwAr4H\nbxGMhyKwxi1MPTfIwaBiOzD1UkenM9Xs5pwMbJVSbgMQQtwGXApscG2zArjW/vwA8LsqtqcssVSW\nux9/gSszv0XkvZOzn7u9k6VmktkdNfCnNt6X3EXSzHHU3gj8aSYAG/cNML+xlss6O1jUUge9s/Cb\nPgYHi5PKDKRyTnkHsAY0tdWFrFo1tpD2lsAtYxHY65sjQeLpLLXB4jajWwTFCdEnEmp0caUDvOrK\nxgiq9wgr4a+tAS9B04rtaEUwdajmEz4H2OX63gGURhjXAZcB3wLeAESFEE1Sym73RkKIq4GrAebP\nn39YG/m3jft5/M+/5qrAdyEQBV/x4T4lnWeNUcCXELDO5KJCDmlI/P0+WGfV1J+VyuLbbfBa8tQf\nSMJzSwgY55DNFzzzB6tgMlg9YCEErz9hDrPsqp9ui6BcjEAJpdcc20a3XX65uG7kF3LNggaOmVPH\nCfMqn5VqPFDXXKlraM3CRk47qolwoFh/qZoWgfpddXzAixUkzlVU+VUzOTjST/jHgO8KId4JPATs\nBvKlG0kpbwRuBFizZs1hHT48mMlTJxLWl2uegmhxdqHXfe1BtnUlaIkGeeK681jz338iky9w4co2\nfvC2E3mho5/Xfvdh3r56ATc/toOtte/BTPUTMAWZXMEzN8DmMnPmfv71xzjLvIJ9qHBTyuEdpy9k\n8cyIZ+RyuawhN0fPquMP11SeLTNeFC2CyoT5We0zOavdssQOJmtorKh7oi0CLyq2U1ohVjN5qeYT\nvhtwz8U3117mIKXcg2URIISIAP8ipeyrYpuGkMkXiGBX/wx688uVX78nkSGbLzhjA1TWkIodqGBy\n1oxgpgesGEG+4JTqheLkMFBe8HmDxWViBMpNYfvJla82ky+UHUcwGfAbwv4be/udkcXjkDU0mesM\nVYP99gj6iRZz0hw81ZQgTwJLhRCLhBAB4ArgLvcGQohmIYRqw38CN1WxPWXJ5ApERZKCMMFf41k+\nkMoxI+wnV5Dsc5V4yNmF4pQCUAohH4hAOmYrAunM2QvWnAAzwsW5Y0vxpI+OYBG4a94EXcHMyUjA\n9B10+mdxQFkVYwRlfnNNMWtuomWhaQ6eqkkQKWUO+BBwH/AicLuU8gUhxPVCiNfZm70S2CSE2Ay0\nAl+sVnuGI52zLIK8P+Kp9qbmw1VF23b1FktB5Essgh47u6gQqIPUgCt91DsngaoKWk74uf385Xr4\nQdMYMn2lk9UySYN2fsN30IK8WH20eteuFOxUHEF7OGifYMkHmoOnqk+4lPIe4J6SZZ9xff4N8Jtq\ntmE00rkCs8QgWTOCu9+n3ELtrRGeeLmH7z/4krNOzSFQOs4A2yII1vucGEF9jZ+BVBYpoaUuRH2N\nv6zwqyRGEA2Zntouk10RBAzfQfv4lWuomllDpmFNpKNjBOXRltLUYdo/4ZlcgSiDZMxawq7l/Umr\nN3/Koib+vvkA63f301YXIp3LD3EN9djWgwzVQfxl/PbI4lgqR12NyUkLG3ji5R5OX9xENGiyck79\nkHZ4s4aGCsdTj2oa4ktXCmCy5riftriJ+U3h0Tcsw6p5Mzh5YeNBzU07Fs47upVTRqmpNN14z5mL\n6B8sPwOfZnKiFYEdI0gbXjNXjSJe0BTmH584x1n+th//s1g6wrYaVAKPL1QH3TEnWBxLZYkE/fzo\nHScVDzxMiR63O6hcOugVJ8/nipO9qbPBSW4RvPcVRx30vicuaOD29582+oaHyI1vHzr72nTn05eU\njgvVTHYmpwQ5jGTyeSIMkja8VThVgbnSDB/TnoAdirEBha+m3okRZPOSgVSuYrfCaBZB2X0mebBY\no9FMDKa9BElnC0RJkvJ5FYEqMFfqgzZ8PqfEhCpGpzBr6iATQ+mO3kSm4kCj8kdD+fTRcuih/hqN\n5nAw7SVIJl8gIgYZ9Hl91co1VJqn7jcE+UIBKaWTNaQww1YhuVp7XEJPIjOmQONYff7F7af9bdRo\nNIeAjhFk80RJkhCWRTCYyfPTR7eTyVkB4dKsFsMnyOYlN/xpIynXNJN+Q2CErAFpYWmlmvYmM2Mq\nTxA0DTK5gmcax9G2t/5rRaDRaA6eaS9B8tk0AZEnKSyL4M8b9vGVezfyyEtdCDFUyJo+wY7uBD98\naBtgTTAD9tgARxFYJSsKcmwpdmou3kqn/5vs6aMajWZiMO0liJm1agAl7ORRVRxu/0CKcBmhbBo+\nZ4KZ7151AmcttWrfhAMmBK3BZzWF4uCzsZQnCBi+igPFoBWBRqM5PEx7CWJmLcEfwyovsckuDrc/\nli47WMl0zfAVNA3vhOhB2zooJJxt6sbkGvKNyc0TNHTWkEajOXSmvQTx5yzBPyAtRbDFVgTJTL5s\ncTjDpQhC/mKtnJqAUd4iGGOweCwWgRp7EKxi4TWNRjP1mfbBYmUR9BdqGMzk2dFTFOLlFIG/JN8/\n7LYI7BhBMF+sNBoNVh4jGGvQN6AtAo1GcxiY3hIk1sk1ye8BMFCo4aUDcVxl/svWwTE8rqFirZyQ\nv2gRzNlxJ583b2Kp6BizRTCW3r2KDVSaZaTRaDTlmN4Wwda/MFfupUdG2Clb2LQv5lldziIwPa4h\nwykgFw4YVtG5BWdQ07mJt5nPMkiQaOiKiptzdvtMZ66DSjh5URMdvYMVZxlpNBpNOaa3IshZcwy8\nOv1VZuZDbN4fI2D4aI4E2NOfosZfJljs6n2HTMOJEYQDplXG+l33sG57D/N/cgJRkmNyDX3onKVj\nav75K1o5f0XrmPbRaDSaUqa3ayhnlYhIY5LO5dm8L8bilgj1YauiZflgsXfeAOUacruRAoaPuKwh\nKgZ1CWONRjPhmd6KIG+ViMjgJ5UtsLkzTntrxBHeo7qGTFew2OXb9xs+YoSJktQTn2s0mgnP9FYE\ntkWQwU9vMsPuvkHaW6NO7n+5VE63ayjo93mzhmwCpiAma6jzDR7UfLwajUYznkxrKVXIDpKVBgV8\nTpG59taoMxp4NIvAnTXkHnymLII6MVjN5ms0Gs1hYVorgnw2TQbTkxK6rDXq1AcqrwiKFT+FEDSG\nA5zVPpM1CxucbfyuGIFGo9FMdKa1A7uQTZHBTzRk0pfMUuM3mNtQ4/j1y5aYsF1Dym1kGj5ufvfJ\nnm0CpmURREgO2V+j0WgmGtPaIihk06TxU2dbAEtbI/h8YsRgsbIeRhoF7Dd8xKkhLAehUBh2O41G\no5kITHNFkCIjTepqLMG/tMUaGRwdIUbgt11DI9UEChg+p3YRmdiw22k0Gs1EYHorglyajMsiWNZm\nTWCvYgQ1ZYS9sghGmk7Sbwjidllr0loRaDSaic20VgSUKIKlrZZFUMwaGj5GMNJ0koZPELfLWpMa\nOJwt1mg0msPOtA4Wy1yGNH7WLGygJ5lh9Xwr8+fYufWctLCBZW3RIfuYjmtoeB0qhGDlornQgbYI\nNBrNhGdaKwJyKTKYLG+r472vOMpZ3FoX4tfvP73sLkXX0MhVQj/w6tXwYyCtLQKNRjOxmd6uoXya\ntPQ7E7xUgllB1pC1gW1NaEWg0WgmOFVVBEKIC4UQm4QQW4UQ15VZP18I8YAQ4lkhxHNCiNdUsz1D\nyGXIYI5pYhcnRjDavAH2JDU6RqDRaCY6VVMEQggD+B5wEbACuFIIsaJks08Bt0spTwCuAP6vWu0p\n28a8FSMYy+TvToxghGAx4LIIdIxAo9FMbKoZIzgZ2Cql3AYghLgNuBTY4NpGAnbXmXpgTxXbMwRf\n3soaGosiqCR9FLAmqUFo15BGM93o2gKPfNMaTFo/F171X9ZcJROYaiqCOcAu1/cO4JSSbT4H/FkI\ncQ1QC5xX7kBCiKuBqwHmz59/+FqYT5ORfmrLpIkOh7+C9FHAuvHBOm0RaDTTjefvgGd/DqEZkOqD\nU94PtU1HulUjcqSDxVcCP5VSzgVeA9wihBjSJinljVLKNVLKNTNnzjxsJ/flrRhBY22g4n0qtgjA\nihPoGIFGM71Ix8BfC+d9zvpuz3sykammItgNzHN9n2svc/Me4HYAKeVjQAhormKbPIhCBmkGx+Qa\nUvMLjJY+ClhxAu0a0mimF6l+qxNoBq3vuemtCJ4ElgohFgkhAljB4LtKttkJnAsghDgaSxEcqGKb\nPJiFDKY/NKZ9Kik65xCs04pAo5lupGNWJ9CwPQ35zJFtTwVUTRFIKXPAh4D7gBexsoNeEEJcL4R4\nnb3ZfwD/KoRYB9wKvFNKKavVJg+FAiY5zMDYFIFZ4YAywLYIdIxAo5lWpAesTqBjEaSObHsqoKoj\ni6WU9wD3lCz7jOvzBuCMarZhWGwt7Q/WjGk30xi9xIRDqA56to25aRqNZhKjLALT7mTmprFFMOGx\nAziB0NgUwaz6EGe1zC3ArQAAIABJREFUz3TqEo2Itgg0mulHasDqBDquoSkQIxBCXCOEqEDqTS7y\nWevmhMZoEYT8Bje/+2SnUumI6BiBRjP9cCyCqRUsbgWeFELcbpeMmNgjIypkIGb11EM1Y1MEYyJY\nZ/kHJ4FpqNFoDhPpAQjWTy1FIKX8FLAUq5bmO4EtQogvCSEWV7ltVaUvFgcgHK6t3klUvSHtHtJo\npgeFPGTidtaQrQimgmsIwM7k2Wf/5YAG4DdCiK9WsW1Vpd9RBOHqnURXINVopheq0+cZRzDxPQKj\nZg0JIT4CvB3oAn4EfFxKmbVHAG8BPlHdJlaHWCIJQKS2ihZBUFkEWhFoNNMCpQg84wgmvkVQSfpo\nI3CZlHKHe6GUsiCEuKQ6zao+qUFLEVQ3RqArkGo00wrV6QvWudJHJ74iqMQ19CegR30RQtQJIU4B\nkFK+WK2GVR17kIfhr6Ii0HMSaDTTC7dFYNoWwRRRBN8H4q7vcXvZpEbafjvDH6zeSbRrSKOZXqhO\nX6h+ygWLhbvsg5SywBSY61jaN8cYY4mJMRHUWUMazbTCcQ25YgSTIFhciSLYJoT4sBDCb/99BJj0\ndROEba5V1SJwXEP91TuHRqOZOLhjBD6fpQymSK2h9wPfxppWUgJ/xZ4kZlJj1xoyA1WMEZhB60F4\n7nbofMFatubdsOgV1TunRqM5crhjBGC5hyZB9dFRFYGUcj9WCekphS9nZQ0ZgSqOIwBYeRnsfhr2\nrYfe7eAztSLQaKYqqQEQPgjYaelmYFIEiysZRxDCmkBmJdZ8AQBIKd9dxXZVHTNrx7+V+6ZaXPbD\n4ucfvEIHjjWaqUx6wLIGVCUeMzRlgsW3AG3Aq4G/Y800Numjn/5cnIw0irm+44Gew1ijmdqkY1ad\nIYURmDLB4iVSyk8DCSnlz4CLGToJ/aTDn42TEOGi5h4P9BzGGs3UJjVQjA+AFSecBMHiShRB1v7f\nJ4Q4BqgHWqrXpPEhkE8Qp8rxgVL0HMYazdQmPeB1NxuBqREsBm605yP4FNacwxHg01Vt1TgQyMVJ\njrsi0PMTaDRTmvQARFqL383Q5A8W24XlBqSUvcBDwFHj0qpxIJhP0C+OhEUQAynH1yWl0WjGh3QM\nmpYWv5uTI310RNeQPYp4UlYXHY1gPs6gb5wVQagOCjnIDo7veTUazfhQGiOYJAPKKokR3C+E+JgQ\nYp4QolH9Vb1lVSZYSJIUVSxBXfakuhqpRjOlSce8MQIzOCmyhiqJEbzZ/v9B1zLJJHcT1eTjDJrj\n7Rqy08rSAxBtHXlbjUYzucilrTEDpVlDk2AcQSUjixeNR0PGFSkJFZIM+o6URaADxhrNlMMpL+Ee\nRxCc/MFiACHE28stl1LefPibM07kUpjkSBvjrAj0/AQazdRFFZf0WARTpMQEcJLrcwg4F3gGmLyK\nwNbcqSNmEegYgUYz5XDPV6wwpo5r6Br3dyHEDOC2qrVoPLB75ONuEej5CTSaqYt7LgLFJAkWV5I1\nVEoCmNxxg7RSBJHxPa+OEWg0UxcnRlCSNTQVLAIhxN1YWUJgKY4VwO2VHFwIcSHwLcAAfiSlvKFk\n/TeAV9lfw0CLlHJGZU0/BGxBnDG1a0ij0RwmUmUsAiNojR0q5MFnHJl2VUAlMYKvuT7ngB1Syo7R\ndhJCGMD3gPOBDuBJIcRdUsoNahsp5Udd218DnFBpww8JWxBnjOgoGx5mDD/4w3rGMo1mKuLECFxZ\nQ6Y9A2IuDdWe++QQqEQR7AT2SilTAEKIGiHEQinl9lH2OxnYKqXcZu93G3ApsGGY7a8EPltRqw8V\nW3Pn/ONsEYDVW3jxLmuSGjfH/Ascc9n4t2e6UyjAA1+A1e+AhgVHujVHln98HXY/Y30+6b2w+FUj\nb6/xki6XNeSewH7iKoJKYgS/Bgqu73l72WjMAXa5vnfYy4YghFiAFXf42zDrrxZCPCWEeOrAgQMV\nnHoUbNdQ1j/OFgHAsZdbPsTe7cW/lx6AJ388/m3RwMBuSwBu+fORbsmR56Gvw87HYfN91vSqmrGR\njlmuINM1D7rfngo3kzwybaqQSiwCU0rphL2llBkhROAwt+MK4DdSyny5lVLKG4EbAdasWSPLbTMm\nbBMuN94xAoBXf3Hosl++GQb2jH9bNEVzPp8debupTiEP2QSc8WFLCUyCAOeEo7TOEEyaTMFKLIID\nQojXqS9CiEuBrgr22w3Mc32fay8rxxXArRUc8/CQ6meQID7DP26nHBFdnvrIoX73wjRXBE7qY52d\n8qgVwZgprTMELkUwsd/vSiyC9wO/EEJ81/7eAZQdbVzCk8BSIcQiLAVwBXBV6UZCiOVAA/BYRS0+\nHKRjJKjBNA4me7YKqPLUmvFHWwQWTupjVCuCgyVdziKYHCnjlQwoewk4VQgRsb/HKzmwlDInhPgQ\ncB9W+uhNUsoXhBDXA09JKe+yN70CuE1Keegun0pJDxAjjN+YIHMCqCks9TwF44/K4CqU9UpOH1Tq\nY6hu0oyGnXCkY94xBDBpyspUMo7gS8BXpZR99vcG4D+klJ8abV8p5T3APSXLPlPy/XNjafBhIR0j\nLmswfRPIIihkrV6YP3SkWzO9UD3hae8aclsEk2PC9QlHagAaFnqXTZKxQ5VIwouUEgCwZyt7TfWa\nNA6kBhiYSBbBJPEjTknUbz7tXUMqRlBvV8yc+JOpTDjSsRGCxRP73a5EERhCCCcfSghRAwRH2H7i\nk44RkzUYvommCCZ2r2FK4lgEuSPbjiNNaYxgEkyvOOFI9w8NFgcigJjw73YlweJfAH8VQvwEEMA7\ngZ9Vs1HVRqYHGCi0TJxgseNH1COOx52UtgiA4rMX0llDB4WU5S0Cn89aNtljBFLKrwgh1gHnYdUc\nug+Y3EMw0wPEqcE/YSyCyeFHnJLoGIGF2yIwtEUwZjIJkIWhwWKYFFmBlXaJO7GUwOXAOcCLVWtR\ntSkUIB0nRnjiWASTxI84JdExAov0AAjDqoVl6hjBmHEr0lKCdcXyExOUYS0CIUQ7Vv2fK7EGkP0K\nEFLKyV2AJBNHIInJGtomTLBYWwRHDGdA2TRPH1VuDSEmTQ39CYV6jtwF5xSTwDU0Upd4I1bv/xIp\n5ZlSyu9g1Rma3Ng3LEYYc6K4htTDM8EflilJSo8sBqzfQcWqjIAeRzBWRrIIQnUTvpM3kiK4DNgL\nPCCE+H9CiHOxgsWTG/uGxGUNxoRxDWmL4IihRxZbuAdDqWDxOI7xnPQ48xUPFyOY2J28YSWhlPJ3\nUsorgOXAA8C/Ay1CiO8LIS4YrwYedlLKIphAwWLDD2bNhPcjTkkc19B0Tx8d8CoCpFaOY2HUGMHE\n7uSN2iWWUiaklL+UUr4Wq3Dcs8Anq96yauGyCCZMsBgmRWbBlERbBBbuOjmGu4a+piLSrhIdpUzy\nGMEQpJS9UsobpZTnVqtBVcfudU+okcVQrDekGT9ymWJ2jI4RFIWYM6uWDhhXzGgWQW5wQnc2KhlQ\nNqXo6u6mGSZWrSHQFsGRwP17V9s1tOlP8OSPrM/zT4WzPj50mz3PwgNfBpmH+nlwyTfGrwihezCU\nYU83MppFsP43sK6kevyis+CMjxzetg32wt3/buXpX/y/EJlZXJfPwV3XQKoPzr8empce3nOXsvWv\n0P0SnHK19f3lf8Aj34Kebdb3cjECpWB/8UbwjSJy/eGh1zgOTCBJOD787oktACQJTpwSEwD+WmtQ\nimb8cMdk8lVWBGt/Cdsfgb3PwWP/V36bjffAlvugazM8/RNIdle3TW48MQK78OFoYwnW/gJ2PGYJ\n6sFe2LMWHv/B4W/bnmdhw++sKV47nvSu69sB634Jm+6xhHS1+f/bO/c4uao6wX9/Xd1dnVclJOER\nkmAAgxIIAkYQJ/gZYVVAF0QciOvMCOPKfBjZZZ0ZP4vLrPNyHs7O+PHDLiuLDIqOPF11wUEeIqPi\nyCMi4WkwBhwJKAmBdCehq7qqz/5x7qk6deveW7eq76O67/l+Pv3pqlu36p57b9X5nd/70Rvg/s+2\nnj/5ddj+LzBvCZzwO8EN6tecCoedooWtuVZBf+MveOf4UPrn4aNwGsGYaPWsxshgmYaGy3pV48iO\nNo0gZbW9Og6HrIc1G+FfrwwuOV4d10Xf3nEFfOP3dSTKguXpjgt0hFCj1tIIhj2NoJtpqF6FQ0+A\ni/5ZP7/zv8EjKVSfsU2m/ugbuyxLFpE59cn28UyOw5LV8NHALruaQ46F37uz+2fv3g5XnpCLibhw\nGsGCkk6FqDEyWM5il8STPeYHNzI/ffutMb2UF2kzVNBq297HPM+CSV8yVFxncb3a3p+3vAhqe5NP\nzrOvg/+atL2WwQTaqOmWnuYcg+oL9UuOxScHaCbMhgWlBlOqxDRDgxM+Ci6JJw/MD27e0vR9BMYZ\nG9WoZNKrXpl1yZFmCWqjERjTUJeFScMnCMy51WL1rup9fNBZmLHqW52njSnGZ45rm9RmSnMBkH0Y\neeEEwbyhOjXPIjZ4GoGr75Ip5sc8/4AMNYKIVV9eGoHdrxgs01CX72O91nIsQ2vcSU/I5jrIULhG\nMDSczfUyxfjMsYK6kvXLcFlrY04jSJ9Rpqihm9YPD5qPwJmGsiVLjcCsHJuCIGDVZ/bJur2hP/Qx\ntmlo0mcaSsm0MTmu6/rPOyDAR+A9X7wqOx+BfVw77DYJcgojL5wgkEaVqhEEA2Uacn1iM8eYGeYv\nTddZ3KjD1H5PEESs9uNoDWlg9yuG+M7iRq3TRwDJT8gm2S0oQ9c8r6zM5nrV/RpBQMP6mZBTGHnh\nBMFQo0ZNeaahQcojcBpB9lQntAAeXZBu+GjN+2F39RF4q8usbcV+jcD4COI4i0u2j2Bx++clNr7x\nlhDtEAR7dHmWeQdkZBoyPoIJqxlNghpBTnWJChc+OjRda5qGBip8tDTqfARZY1ZzQyPpagSTljM2\njkaQta3Y7lcMLbt/vxpB0p32zHUZmdcpQKsTnoBdnJGz2GgE41rLU42ENYJ86hIN0JI4G2xBMFjO\n4jH9pSp6XfwsMZNIaSRdZ7G94g6LCGpM6TIEZjLO0lbcETVkSkx0cxZPBjuL0/ARjIVoBJPjLQGb\niWnIuybV8eiyEv1Sdj6CTBiarg2mj6Bpl3V+gsyYtDWCFE1DdlRO2GTpn1SytBVPjuuFiPkONp3F\nERrB9LS+ZoHO4hSihpo+goDwUeOEr47rcaWJMQ1NjnfmXyRBTr0LCicIStO1ZvhotZ7yl6YXXMXH\n7DH23aFSyoLATPKVVslxv/nEbh5v9s0sj8CXFNXUCCK+i+Z7aguC0QXBIZ4zHl+Uj8AOuVU62StN\nbGdxKhrBIpdHkAXD01PIcJkLNqzm8OUL8h5OC6cRZI+ZYNI2DfmjcoJWfXlqBP6kqOEYixLzPbWd\nxSLplFw2AtuYy+yGOXES9ZJCKctZPN6asBN1FldajugMKZwgKKkpyuV5fOYDxw1W0blmNqcTBJlR\n9SaRoRHtn0nrx+e3wQdFhvj3ycr5CZ0aQSnGosS8Njzavj1pZ+d0Q2cqGx+BasDUa63XjZDIIglv\nuq4roJrjpKURqGntiM6QwgmCEVWjURrtvmPWxLHLOpLF+AhKXvBcWlqBP3M3Kh6+uU/GPgI7KUrE\ni2KLYRqyNQJI3qTVzdHeNBst7nwtaezr0eYjSDihzHx+hqQqCETkDBHZKiLbROTykH3OF5GnRORJ\nEbkhzfEADKsa00MDKAicaShb7BjwIR08kFoIaXUCpKTDHyHYfDIZJCwyzCPwmzeGx6IXJcZWbjRZ\nQ9Jx8G2Odl+i3fR0QFmOFCdQ+3qkGTUEmTuMU8sjEJEScBXwTuB54GERuU0p9ZS1z1rgk8BvKKVe\nEZGD0hqPYURNDaYgKMVw0DmSw44BL3mCIC2NwKy4TdnpsQrsfal9H3+rQ6MRBJWrTpqg7Ni4GoHf\nNBR0bjPBzsEwgtRsq+0FVHY+Avt6VMc7Nb0kyLrgoEeaCWUnAduUUtsBROQm4BzgKWufjwJXKaVe\nAVBKJfgNCmaEOsqvzg4CcRx0c4VHb9B/htefDhs/3v19Ox6Be/+8lWuxaAWce3VwM5Bu2Gq9EQBh\nkUN3XQEvbmk9P+Vj8IYzux9jy83wk6/oRjP2RFuuwM6n4Z5P6a5aEOAjqGhbcW0flBfGP69+CKqg\nOVyGn35Lj33DRXDsee2vm3j6DtPQIvjZ3fDdT8Npf5LA2KysbKN9fOsyGFvSWqHbGsF9fw2br9PR\nS6f/Kax6c+dn/uCzsOJN+nv3wOdhyWHwxvd0H4udV7HzGdj3sm4o1c/3L4wsNJsA0jQNrQR+aT1/\n3ttmcxRwlIj8UEQeEJEzgj5IRC4Wkc0isnnnzp0zGtSoqqGGB1gQFEEj2HKTnljVNOzcCj/+Urz3\nPXOX7galpmHiRXj8Ft3VqR9sm7xpHxgkCJSCB6+GV57Tx33+YXjyG/GOseVGfZ7LXg8nfri1fd37\n9H+7U1ltPyCtyS6rCqRNE5lPIzjxd/W4X/gJPHZr5/uapiGfRnDM+/X/B69JZnz2qvvgY+GoM/Vj\nNa3v2xHvgNdt1IuC9b8FCw/Srz37PS2Qgvjh5+Bx75x+dFX7oiQKI3iOPQ9WnggHvA7e8pGZnZ+f\nnHwEeZeYGAbWAr8JrAK+LyLrlVJtrbqUUtcA1wBs2LBhRqEdI0yhnLM4Xxo1OPR4+PDt8M9/BE98\nPd77qhO6CuVFd+jJ+NYL+58o7QnGRGgEmYbqk1pAbPg9OPUP4fMb4x+zOgGrNsDv+ATHUe/SXcju\n+yt9zNKIV9t/rGUGajMRrOj59GJT26cnTr/D8zcv139fPCv4fJumIZ+P4Oj3wql/BPd/Lhmzli2w\nxyrwH24K3/e8a1uP/zqkGqkRfHbRuLirb7NIO+ZcOPrfx3tPr2RdgtwjTY1gB7Daer7K22bzPHCb\nUmpKKfUs8AxaMKTCdKPBqDSQQTYNFaHekF2srJe46eqedmcq9K9C26aYKGdxUA5A3NVaVNMSv1Ow\nXmtfXWflNPSbpPyEOa2NRhD0WypXOsM8+8Uk2vXqkB0LiV4ygm9yT6dQ6EYj4pyTIicfQZqC4GFg\nrYgcLiKjwCbgNt8+30RrA4jIcrSpaHtaA6rVvEnWr84OAk1BUBCNwJxveZGegOMIQNuEMdOJ0p7g\nm87iANNQYFhnXEEQ0cbQbwuuT/oqeRoTQcqRQ/7z8xMWxhrmLDbvgWQmM9tH0AthiW12Q5mmUOhR\nI0hz/phrGoFSqg5cCtwFPA3copR6UkT+QkTO9na7C3hZRJ4C7gM+oZR6Oa0xVSe9Fcog+giMuaoI\nzmK7WNlYDxO6He8+04nSDv1r+ggCNAJ/9mgvcfKT4+F1aPy24Eat3cyS1YTgD1v1E6YBBWUWN9+T\nYDnq6rgXeju/t/eFJbbZLSabj2OO0yxW/OawJBkqafPnXPIRKKXuAO7wbfuU9VgBf+j9pc5UVQsC\nSfNG9kuRnMV1WyOwJsSFXaKHTbVQmPlEafsIosJH+y39MN3QdW+6agTGNFQNMQ2lPCH4w1b9hIWx\nNlfHQaahBFtWGq2qV19DeRFMvtq53dYIbD9BHJqmoZQtCjn0JChUZnHdMw0NDaJGUCRBYDc978WM\nYNvcZ2p+CNQIAkqAh/kIuvk0usWY+yf6Rq19dZ2VRhDHRxBkugsqOme/x/7smTAZ4WeJIkxgGw3S\nzgyuT8YzyUYJvyTJsuCgR6EEwZQnCGRkAAVBkaqP2vbwXiYN2+Y+uhCQmfkIRhdqVTzSNBSgEUxP\ndRfY3bJOO5zFkz6NIKVG8H7i+Ajs/Qz1iNVx0j6Cfko4hJm0zHk0qrB/V+f2KLJwFkO25UU8CiUI\n6lUdJjg0MsimoQI4i+0ImV5WvrbNXYQZNfGws2kjTUMBpR/s7VFjhfBJzO/jqFfbbc/GVpyZjyBE\nYJnr7b/OTXt5kI8gwYinfnsCd/MRAIzvCN4eRtQ5J0kODeyLJQhqehU3kIJgqKSdYkUIH21Y4aNx\nE2iCbO4zaeJhm5miwkc7NIKYk1xXjcAnABu1ztV1FvWGYo/Td3+iVseJ+gj6NQ1V9PfFHwlm37c9\nvQqCrExDTiNIlYZnGiqNDqBpCLoX+poLNLxSvs0M2rgTa4DNfSZONdvMFBU+OrlHR6yYfeJGK/n7\nAPsZHtMmqWb4aLVzgsliQqhaJrIgwjSgelUvXEoB8SajCfo3JvvVCLz31AJaWxraNIJeTENpO4ud\njyBV6lOeIBjEqCHQ5pK57iyu+3I54tqTg1au5Ur/4aN2KGo3H0HbMWNOct1W2sa0ZUcN+SeYLEwE\n3VbcYav7RoDgMpSGdQ2evH0EENDs3npuawRxrnOmzmKnEaRGwzMNDaxGUCrPfWex36RgWjf2Y3Of\nyYrZnuCjag35J8rYPgJf68kg7KSnRrUzPj0LjaDbijvM3h8kuGySCoHs20cQ1hva1giet7bHuM5R\nuRNJMlbRlVWDothSoliCwNMIhkfn5TySEAqhEQRkZ8ZZ+QatsMPKCMTBnuC75RHMSCOIEAS2j8PO\nrWgeKwMTQbcVd5RpKEqzTkKbqVf1wqFfHwF0jntyvCXA9uxoPY5znRtV7U8aSnnazCG7uFCCYLo+\n6IJgbO4LgqBiZXFWvkE29xlrBH7TUJCPYLx9ogyLogkar92MJgh7om8ErLCz8hFErbjDJqWGrzZS\n0PtmOvZuWc9RhPmeqhNQOVQ/np6CilcQOZazOEBYp0EOpagLJQjUlJ6ERsoD6iMolee+s7ipXvuy\naPv2EfTxY7H74EJ6GoHdjCYI+7yDnMVZ9C0O6k5mUxrRznK/L8YuHBhEEtpMt6znKKJ8BJVVrecL\nDtTfxTjXOUhYp0EOXcoKJQimjSAYHVBBUCjTkC+LttuXPsjmXq7oH2ev18wvVCLDR8fbtZCmT6OL\nkzpOtIvtIwiLGpral66tOO44g3wEUavjJDSCblnPUYStqqsTsGB5a0I3TW1i+Qgms9UIMswlyLsf\nQXY8dgsnP6m7QQ2sICiVu09qW++E+z+rSxysOA7e8w/JHb+2D269SNfnP+t/6C5Te1/Sk98Zf6O7\nOs2UoPjzsQpsexiufad+vvokePdftb8vzEcAcN0ZsPBg+K0vQVSOSHUvfO2iVitFv4/gWx/X57jS\n6moVVEG028Tx8LXw2E26kUoUYxV49Rf6uI2AFbZt5553QPRn9Ut1Irwwnj2On34LXnq6tW3XVlh6\nZPh7xiqwe3vrntosPUJ3lovSlvbsgC+f0zp+r5j33P853QjJ8MpzsPpkGF0Ar9X0OKO0l0e+DI98\nRT/e/XMvoz1lmkX7nGkoeYaG+bclJ3Nj/R3MX5J6a+T+GJ2vV4BR/PR2eOFR2PcSbP5ivDr+cdn1\nDPzsLnjuB/DYLfrHX52AX/wQtn8vmWMEaQTHbWr9OCdebP3wbIKqUB55Gqx9tzbpPPNtPfFEsXOr\n7lqlpnWnqzUb9fZ5S/UYAH5+X2t/0xzdb5ro5gg1Hb3efGH0eEynss3X6f9+m3vaTkOl9Pctyo8B\nugvXIcfp+2P+Dj1RdzEL49jz9PW13zO6AF57RQvJbuf0wk+0Frj8KL3g6ZWReXDSxbDsyPbjr9kI\nx74f3vox3d1s/fnRgv3xW7XQG12gr0HSHckCx+59x03DpAwojkZw7Pv5x58fxTf3vMAHRxLsMZok\n5YpesURRndAt8o7/EHznT/WXZXRBMsdvy7r0QutO/+9w04eSW50Epekf/V79B1oL+f7f60nYjs4I\nqkK5fC186BbY9h34p/Ni2O09c86Zn4HXva21fWgI3v9/dNcz+zNMc/ReNYLqBLzxvXDSR6PHc/ip\n8K5Pw91eb1+/RpB220LjE+lm7njrJfqvF448Tf/5+fGX4PbL9PcpyvZvggou+KfuGksQIlqrjRof\nn9CPH/jf4de4OgGrToLf/lrvY+iXHMrNFEcjAHbtq7FswQA2pTGENdOwMdUY05gkgrIuxxYnm+DS\nLTuzXAFUp2bkj95pe09MVTpWExbrM8IqiHZzhPYS+27v15FHkHIp6kZGcfE2cR2hUUXtkiZKsEd9\n79Iih94khRIEu/fWWLZwgAVBnNo5xlSRRmRBkEZQXpRshmu37MzQaI+I6Ja44XZxCsHZ1yAsO7ir\nRtBDfRx7vw7TUMrRI1llytqUYy5gsirwBtE1naK6zKWFWRBkGDhSKEHw8r4qyxYMaFYx6C9k/bXg\nMEaDWW2msVo0n1Uqw/gLrTEl2SijHpBHYBOVERrVRQv6S0rzH9v+jDDBERXWafrgxtYIrM/O2jSU\nhyCIW5nUaI5ZlIOJEuz9Fr2bCWZB4ARBOry8t8bSQdYI4qwAzco4jaQT81mVQ1uhlOUuURW90gjI\nI7AJM/NEmVtix/Z3SVDym8DCTElRE4fpgxvXnGDvFxQ+ao87aXIxDZlz6hJ+G5RvkhZhzYbqNa2Z\nZC0IcuhNUhhB0JhW7N5fY/mg+wggupBa2j6C4TGYv6x9TElmuHZbhYZNflG22maTmhgawciC6Eqb\nbT4CX79i/37T0wHH6DH2vc1HEBE+mgbGDp9mM3Y/cc1dUR3Qkqa8CFQDpl5r327GmJePwDmLk+eV\n/TWUgmULB9g01E1tNjX5xyrxV8G9YLQNM47hMT1JJOkj6OYsjvQRxKzkGcbknugftb92UZSPIMih\n3faePnwE/pX5yDwdMpuajyCDZux+4iZL1auAtMp/pEmYwJ1JQttMGBrSv48Me5MURhDs3qcnoIF2\nFndbAdpfzLR8BEYDsMeTqEbQZfLpx0cA8QvXdaurE8tHEKGN9Vofp00j8H03RdItRZ1V60WbuC1G\nTVG7XpvW90NoXaIZ1DqaKRmXmymMINi1V6uaS2eDaSjUcWWtNlPTCGxBYAmExJzFccJHaT9eswpl\nl0m8q2moizAxAs/YiqsTgGhzkn+/5usBx4D45gQzMUKwcEyz8FxQJdi0GRqKd6+6FbVLkigtFLLX\nCCDzcjOFEQRlVojqAAAOxklEQVQv79UT0PKBNg11qWxp95c1PW2T9hGMVVoOW/MDKVf0Sj4Jm2W3\nUr5BK8bmyjwisShu4bpIYVLxbMVeRqcRHP6xRuUt9GpOMBMjhDSCXzy3nMUQz4xXn8xuXFF+Kcje\nRwCZ9yYpkCDQF3XgE8ogwjTkc14lGdZpPt/WNsz/JJuRdyvlayZGW8DFmVxjFa7rkhzkX+mHCY6o\n+9Srj8D+vKDrkolGkLUgWNS9s1xWJZ8hwkfQx71MiuEYdccSpDCC4KDKGKeuXc6S+YMsCOL6CKyV\neuI+gkq7JgDJhjHGqeDon/zi2GqT8BH4NbIw53IcH0Evq8hm8buA7+ZYpf92nN3ISxDESZyMaoWZ\nNN26mRVAEBSm1tBZ61dw1voVeQ8jmuGyNpvE8RFA8qvFUGdxgo7poCqbfvyZnnFstXGb24Q1k7c/\nP7ZGEOQj8Lb1UqWyqRFk7CPIqhm7n/Ii2P9y9D7d+h0kSZhgNwI4Dx9BadQ5iwtLtygRf03+JCNK\nTEasXb5izPc/iWPVYzgB/ecVZ5XdTTvyN6MJ+wxoCaEw53KUYKyOw+ii8FyFIJrhukGmoYS1Ppvc\nTEMxvrf1anbO4tEwjWBCT8hRpc3TYi6ZhkTkDBHZKiLbROTygNcvFJGdIvKo9/cf0xzPrCAy3d23\nMk5ytWgyYgOjhhKMUIrjBOwwDcXRCLo4tONqFfa+YRpBVAhkP83WI53FGWgEWeYRQLxziqM5JkVp\nWEeG9ZLNnjbDY3NDIxCREnAVcCawDvigiKwL2PVmpdTx3t+1aY1n1hC1AvTX5E9ytWjbQ42tPBXT\nUAyNoCPD14wtwqzTzaEd188Alo8gxLkc5NA29FOtshyhEYxV9DWbSiG5yOR0ZG0a8ifuBZGlsxiC\nAy+6tfFMk4wTytL0EZwEbFNKbQcQkZuAc4CnUjzm7Kdcgefuh6tP7Xxt/AXaavKXK7D318H79opZ\nfYTlEQDc+5fwo6tmdpzdz8KBb4jep7xI92Uw57VvZ/t4wt4D8KX3tDqO2Rg1O85n/MvfwEPXwP5d\n4fuXF8FjN+umPTa7n4WD3hh+jKjjBmoE3rX/wmm9mZtAT6TnXKU7iX3zEviNy+AQq2tanqah+iRc\nvZFmDsXCg2DTDa2xNKrZhm0GmVnjtPFMi+FyS7utV3VPkL2/glP/GI55X/KHS/wTW6wEfmk9fx44\nOWC/80Tk7cAzwMeVUr/07yAiFwMXAxx22GEpDHWA2HARPB7SBKOyUrdxNBxzrp4wVUDNm3446Gjd\nwWn+cjjlUjjq3Xr7guW629OrHbemdyorYd3Z0fscd76e/E1iV2Wl3halSRz+dt3xK8quevAxcNgp\n4a+PLYGTL2k1B1q8GtadG7zvKZfCs9/v3F5Z2fsP9bjzYf7S4CzaI0+Do8+OrkgbRH0Stt8H//aA\n1iAfv0W34bQFQV7O4jecBS9uafVinnhBNxfa87zuKAaeszjLGkgB5qo4bTzTojTayiPY8zxsuwdk\nKLkmVD7yjhq6HbhRKVUVkd8Hrgc62hoppa4BrgHYsGFDgr0ZB5D1H9B/cVj9FvjgDemMw+4Z3K3b\nU9Ks2dhqIxmXxavg/OtndlwROPNv4+17yh/ovyRY8abwftDLjoQLAlp3dmNyHP52tTZ3GJOH3/Rh\nInOyKONgc8ixsOmrredbvw03bmoPk61nGD4KwWbW6jgsyWnhOTzWWtSY67LpBlgb0AM6AdJ0Fu8A\nVlvPV3nbmiilXlZKmSXctcCbcTgcM8d2aNvOb5usJ9swgvJUGrXsy2P3Wt8qTewSExkktqUpCB4G\n1orI4SIyCmwCbrN3EBE7sP9s4OkUx+NwFAfboW07v20aGZtfwggq+hYn8TBJBs1HYJeYyKAKamqm\nIaVUXUQuBe4CSsB1SqknReQvgM1KqduA/ywiZwN1YDdwYVrjcTgKhzF3hJqGatmHjgYRVJo686gh\nX7Zzr53mksZ2FmfQFyFVH4FS6g7gDt+2T1mPPwl8Ms0xOByFxYREhgmCRoZJW1EYh6w9EWetrZQr\nUJvQDuyhki48qBr5FJwDLQiMRtBrafM+cJnFDsdcxZg7zETS4SPIsMJnFH4fgVI5OIu9MdT2emPJ\nsQQ16PsyXdeCKYOxOEHgcMxVjAPUTCR+G3icch9ZUBqB4XktQdCYAlT2PgLo9KdEJTGmid3AvrpH\nX5+g/JiEcILA4ZirdPgIfBpBozoYPgJoz9TOo09CUHkRe3vWmPvSqGbiq3CCwOGYq/g1giBn8SBE\nDUF7aWrjJM3aWQyW0PQVeMwau4F9P2VLesQJAodjruL3EUzth0a99XrWIZpR2LV+mhpBxs5iGCCN\nwLsv9UmnETgcjhlQrkD9NXhtd2tbnklbUdjhm6bYWpZmq6aPwNMEMojUicTcl0Ytk8Q2JwgcjrmK\nmTzGrYT+tqStAckshnYfQdM0lHGtIRhAjcD5CBwOx0wwk8eeHa3VdZtGMECCYGxxa/LNxVns9xHk\nrBHYVVgnx1MvfucEgcMxVzHmDtWAyqH6sV8jGBRnse0jyMNZPLpAV/e0NYKR+bppTR6U7PBRpxE4\nHI5+sVezlZX6f1sZhwHSCIyPYHo6n4Y5Iu3mqck9+WkD4HMWOx+Bw+HoF3sVuXiV/t9WxiHjej5R\nlBcBSmf2GtNQ5i00K+0aQV7+AWjdl/27AZX6WPLuR+BwONLCtisbjeDuP4Ef/L1+PLV/cKKGjBnr\nC+9oteXMOuu5XIGnb4cXHtHNYJYfle3xbcx9uesKb2xOEDgcjn44YA285aMw+Sqc8NvazLDH6jJ3\n0DpYd05uw2vjyNNh/fktbWDsdDjw6GzH8LZL4Zk79eMD36A7w+XF8qPgzRfCa69oofD6f5fq4USp\n2dXwa8OGDWrz5s15D8PhcDhmFSLyY6XUhqDXnI/A4XA4Co4TBA6Hw1FwnCBwOByOguMEgcPhcBQc\nJwgcDoej4DhB4HA4HAXHCQKHw+EoOE4QOBwOR8GZdQllIrIT+EWfb18O7EpwOHnizmUwcecymLhz\ngdcppQ4MemHWCYKZICKbwzLrZhvuXAYTdy6DiTuXaJxpyOFwOAqOEwQOh8NRcIomCK7JewAJ4s5l\nMHHnMpi4c4mgUD4Ch8PhcHRSNI3A4XA4HD6cIHA4HI6CUxhBICJniMhWEdkmIpfnPZ5eEZHnRORx\nEXlURDZ725aKyD0i8jPv/wF5jzMIEblORF4SkSesbYFjF82V3n16TEROzG/knYScy5+JyA7v3jwq\nImdZr33SO5etIvLufEbdiYisFpH7ROQpEXlSRC7zts+6+xJxLrPxvoyJyEMissU7lz/3th8uIg96\nY75ZREa97WXv+Tbv9TV9HVgpNef/gBLwc+AIYBTYAqzLe1w9nsNzwHLftr8DLvceXw58Ju9xhoz9\n7cCJwBPdxg6cBXwbEOCtwIN5jz/GufwZ8McB+67zvmtl4HDvO1jK+xy8sa0ATvQeLwKe8cY76+5L\nxLnMxvsiwELv8QjwoHe9bwE2eduvBi7xHv8BcLX3eBNwcz/HLYpGcBKwTSm1XSlVA24CBqRZ64w4\nB7jee3w98L4cxxKKUur7wG7f5rCxnwN8WWkeAJaIyIpsRtqdkHMJ4xzgJqVUVSn1LLAN/V3MHaXU\ni0qpR7zHE8DTwEpm4X2JOJcwBvm+KKXUXu/piPengNOAr3nb/ffF3K+vAaeLiPR63KIIgpWA1bWb\n54n+ogwiCrhbRH4sIhd72w5WSr3oPf4VcHA+Q+uLsLHP1nt1qWcyuc4y0c2Kc/HMCSegV5+z+r74\nzgVm4X0RkZKIPAq8BNyD1lheVUrVvV3s8TbPxXt9D7Cs12MWRRDMBTYqpU4EzgQ+JiJvt19UWjec\nlbHAs3nsHp8HjgSOB14E/iHf4cRHRBYC/xf4L0qpcfu12XZfAs5lVt4XpVRDKXU8sAqtqbwx7WMW\nRRDsAFZbz1d522YNSqkd3v+XgG+gvyC/Nuq59/+l/EbYM2Fjn3X3Sin1a+/HOw18gZaZYaDPRURG\n0BPnV5VSX/c2z8r7EnQus/W+GJRSrwL3AaegTXHD3kv2eJvn4r2+GHi512MVRRA8DKz1PO+jaKfK\nbTmPKTYiskBEFpnHwLuAJ9Dn8GFvtw8D/y+fEfZF2NhvA37Xi1J5K7DHMlUMJD5b+bnoewP6XDZ5\nkR2HA2uBh7IeXxCeHfkfgaeVUp+1Xpp19yXsXGbpfTlQRJZ4j+cB70T7PO4DPuDt5r8v5n59APiu\np8n1Rt5e8qz+0FEPz6DtbVfkPZ4ex34EOsphC/CkGT/aFngv8DPgO8DSvMcaMv4b0ar5FNq++ZGw\nsaOjJq7y7tPjwIa8xx/jXL7ijfUx74e5wtr/Cu9ctgJn5j1+a1wb0Wafx4BHvb+zZuN9iTiX2Xhf\njgN+4o35CeBT3vYj0MJqG3ArUPa2j3nPt3mvH9HPcV2JCYfD4Sg4RTENORwOhyMEJwgcDoej4DhB\n4HA4HAXHCQKHw+EoOE4QOBwOR8FxgsDh8CEiDati5aOSYLVaEVljVy51OAaB4e67OByF4zWlU/wd\njkLgNAKHIyaie0L8nei+EA+JyOu97WtE5LtecbN7ReQwb/vBIvINr7b8FhF5m/dRJRH5gldv/m4v\ng9ThyA0nCByOTub5TEMXWK/tUUqtB/4X8Dlv2/8ErldKHQd8FbjS234l8D2l1JvQPQye9LavBa5S\nSh0DvAqcl/L5OByRuMxih8OHiOxVSi0M2P4ccJpSartX5OxXSqllIrILXb5gytv+olJquYjsBFYp\nparWZ6wB7lFKrfWe/1dgRCn16fTPzOEIxmkEDkdvqJDHvVC1HjdwvjpHzjhB4HD0xgXW/x95j/8V\nXdEW4EPAD7zH9wKXQLPZyOKsBulw9IJbiTgcnczzOkQZ7lRKmRDSA0TkMfSq/oPetv8EfFFEPgHs\nBC7ytl8GXCMiH0Gv/C9BVy51OAYK5yNwOGLi+Qg2KKV25T0WhyNJnGnI4XA4Co7TCBwOh6PgOI3A\n4XA4Co4TBA6Hw1FwnCBwOByOguMEgcPhcBQcJwgcDoej4Px/6tlE9y5zG88AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "# Evaluate on test data\n",
            "40/40 [==============================] - 0s 2ms/sample - loss: 0.8084 - acc: 0.6000\n",
            "test loss, test acc: [0.8083738882560283, 0.6]\n",
            "[[0.52836128]\n",
            " [0.78680472]\n",
            " [1.18517104]\n",
            " [0.6601789 ]\n",
            " [0.49185328]\n",
            " [0.89430891]\n",
            " [0.5640123 ]\n",
            " [1.23227365]\n",
            " [0.72812657]\n",
            " [0.80837389]]\n",
            "[[0.77499998]\n",
            " [0.625     ]\n",
            " [0.47499999]\n",
            " [0.67500001]\n",
            " [0.80000001]\n",
            " [0.64999998]\n",
            " [0.72500002]\n",
            " [0.47499999]\n",
            " [0.60000002]\n",
            " [0.60000002]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kd6FTGs7vBI3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "df_accl_all = pd.DataFrame({'Class1vs2': acc_all[:, 0]})\n",
        "df_accl_all.to_csv (r'EEG_Deep/df_accl_allPatient_8_24_2560:4096.csv', index = None, header=True) #Don't forget to add '.csv' at the end of the path\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}