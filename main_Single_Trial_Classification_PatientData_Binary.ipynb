{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "main_Single_Trial_Classification_PatientData_Binary",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sagihaider/EEG_Deep/blob/master/main_Single_Trial_Classification_PatientData_Binary.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X110PE9MjrlE",
        "colab_type": "code",
        "outputId": "3b212630-c45b-43e8-f5b8-dedf54c61d07",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        }
      },
      "source": [
        "!git clone https://github.com/sagihaider/EEG_Deep.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'EEG_Deep'...\n",
            "remote: Enumerating objects: 39, done.\u001b[K\n",
            "remote: Counting objects: 100% (39/39), done.\u001b[K\n",
            "remote: Compressing objects: 100% (35/35), done.\u001b[K\n",
            "remote: Total 192 (delta 20), reused 10 (delta 4), pack-reused 153\u001b[K\n",
            "Receiving objects: 100% (192/192), 857.72 MiB | 41.23 MiB/s, done.\n",
            "Resolving deltas: 100% (73/73), done.\n",
            "Checking out files: 100% (59/59), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ioLzMoiS60Qf",
        "colab_type": "code",
        "outputId": "d4917255-6904-404e-ec9f-3c3143796c34",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 65
        }
      },
      "source": [
        "import scipy.io as spio\n",
        "import numpy as np\n",
        "from importlib.machinery import SourceFileLoader\n",
        "\n",
        "# EEGNet-specific imports\n",
        "from EEG_Deep.EEGModels import EEGNet, ShallowConvNet, DeepConvNet\n",
        "from tensorflow.keras import utils as np_utils\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras import backend as K\n",
        "K.set_image_data_format('channels_first')\n",
        "\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# tools for plotting confusion matrices\n",
        "from matplotlib import pyplot as plt\n",
        "from scipy.signal import butter, lfilter"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mo80Jg_Pn5lP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Band-pass Filter\n",
        "def butter_bandpass_filter(data, lowcut, highcut, fs, order=4):\n",
        "    nyq = 0.5 * fs\n",
        "    low = lowcut / nyq\n",
        "    high = highcut / nyq\n",
        "    b, a = butter(order, [low, high], btype='band')\n",
        "    y = lfilter(b, a, data)\n",
        "    return y\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZjtnE_6RSpum",
        "colab_type": "code",
        "outputId": "22ad01c9-d5d2-49eb-f291-affa29349800",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from numpy import zeros\n",
        "K.clear_session()\n",
        "cols = 1\n",
        "rows = 10\n",
        "acc_all = zeros([rows, cols])\n",
        "loss_all = zeros([rows, cols])\n",
        "X_tr_c12 = np.empty([80, 12, 4096])\n",
        "X_ts_c12 = np.empty([80, 12, 4096])\n",
        "result=[]\n",
        "\n",
        "from itertools import combinations \n",
        "comb = combinations([1, 2], 2) \n",
        "  # Print the obtained combinations \n",
        "bincomb=[]\n",
        "for i in list(comb): \n",
        "    bincomb.append(i)\n",
        "\n",
        "for x in range(1,11):\n",
        "  fName = 'EEG_Deep/Data2A/parsed_P0' + str(x) + 'T.mat'  # Load Data\n",
        "  print(fName)\n",
        "  mat = spio.loadmat(fName)\n",
        "  r_X_tr = mat['RawEEGData']\n",
        "  r_y_tr = mat['Labels']\n",
        "\n",
        "  ### Filter Data ###\n",
        "  for t in range(r_X_tr.shape[0]):\n",
        "    tril = r_X_tr[t,:,:]\n",
        "    # tril = tril.transpose()\n",
        "    tril_filtered = butter_bandpass_filter(tril, \n",
        "                                              lowcut=8, \n",
        "                                              highcut=24, \n",
        "                                              fs=512,\n",
        "                                              order=4)\n",
        "    # tril_filtered = tril_filtered.transpose()\n",
        "    X_tr_c12[t,:,:] = tril_filtered\n",
        "\n",
        "  print(\"Filtering of Training Data Finished\")\n",
        "  ## Test Data Load \n",
        "\n",
        "  fName = 'EEG_Deep/Data2A/parsed_P0' + str(x) + 'E.mat'  # Load Data\n",
        "  print(fName)\n",
        "  mat = spio.loadmat(fName)\n",
        "  r_X_ts = mat['RawEEGData']\n",
        "  r_y_ts = mat['Labels']\n",
        "  for t in range(r_X_ts.shape[0]):\n",
        "    tril = r_X_ts[t,:,:]\n",
        "    # tril = tril.transpose()\n",
        "    tril_filtered = butter_bandpass_filter(tril, \n",
        "                                              lowcut=8, \n",
        "                                              highcut=24, \n",
        "                                              fs=512,\n",
        "                                              order=4)\n",
        "    # tril_filtered = tril_filtered.transpose()\n",
        "    X_ts_c12[t,:,:] = tril_filtered\n",
        "  \n",
        "  print(\"Filtering of Testing Data Finished\")    \n",
        "\n",
        "  for k, com in enumerate(bincomb):\n",
        "      print(com)\n",
        "      # # Find labels to specific class\n",
        "      print(\"Finding labels in training data\")\n",
        "      class1indx = list(np.where(r_y_tr == com[0]))\n",
        "      class2indx = list(np.where(r_y_tr == com[1]))\n",
        "      c1=list(class1indx[0])\n",
        "      c2=list(class2indx[0])\n",
        "      y_tr_c12 = c1 + c2\n",
        "      y_tr_c12.sort()\n",
        "      # print(y_tr_c12)\n",
        "      x_tr_12 = X_tr_c12[y_tr_c12,:,:]\n",
        "      y_tr_12 = r_y_tr[y_tr_c12]\n",
        "      # print(np.shape(x_tr_12))\n",
        "      # print(np.shape(y_tr_12))\n",
        "      # # Find labels to specific class\n",
        "      print(\"Finding labels in testing data\")\n",
        "      class1indx = list(np.where(r_y_ts == com[0]))\n",
        "      class2indx = list(np.where(r_y_ts == com[1]))\n",
        "      c1=list(class1indx[0])\n",
        "      c2=list(class2indx[0])\n",
        "      y_ts_c12 = c1 + c2\n",
        "      y_ts_c12.sort()\n",
        "      # print(y_ts_c12)\n",
        "      x_ts_12 = X_ts_c12[y_ts_c12,:,:]\n",
        "      y_ts_12 = r_y_ts[y_ts_c12]\n",
        "      # print(np.shape(x_ts_12))\n",
        "      # print(np.shape(y_ts_12))\n",
        "      del class1indx, class2indx, c1, c2\n",
        "\n",
        "      # shuffle the training data\n",
        "      indices = np.arange(x_tr_12.shape[0])\n",
        "      np.random.shuffle(indices)\n",
        "      x_tr_12 = x_tr_12[indices]\n",
        "      y_tr_12 = y_tr_12[indices]\n",
        "\n",
        "      # split data of each subject in training and validation\n",
        "      X_train = x_tr_12[0:60,:,2560:4096]\n",
        "      Y_train = y_tr_12[0:60].ravel()\n",
        "      X_val   = x_tr_12[60:,:,2560:4096]\n",
        "      Y_val   = y_tr_12[60:].ravel()\n",
        "      print(Y_val)\n",
        "      print(np.shape(X_train))\n",
        "      print(np.shape(Y_train))\n",
        "      print(np.shape(X_val))\n",
        "      print(np.shape(Y_val))\n",
        "  \n",
        "      # convert labels to one-hot encodings.\n",
        "      Y_train      = np_utils.to_categorical(Y_train-1, num_classes=2)\n",
        "      Y_val       = np_utils.to_categorical(Y_val-1, num_classes=2)\n",
        "      print(Y_val)\n",
        "\n",
        "      kernels, chans, samples = 1, 12, 1536\n",
        "      # convert data to NCHW (trials, kernels, channels, samples) format. Data \n",
        "      # contains 22 channels and 500 time-points. Set the number of kernels to 1.\n",
        "      X_train = X_train.reshape(X_train.shape[0], kernels, chans, samples)\n",
        "      X_val   = X_val.reshape(X_val.shape[0], kernels, chans, samples)\n",
        "   \n",
        "      print('X_train shape:', X_train.shape)\n",
        "      print(X_train.shape[0], 'train samples')\n",
        "      print(X_val.shape[0], 'val samples')\n",
        "\n",
        "      X_test      = x_ts_12[:,:,2560:4096]\n",
        "      Y_test      = y_ts_12[:]\n",
        "      print(np.shape(X_test))\n",
        "      print(np.shape(Y_test))\n",
        "\n",
        "      #convert labels to one-hot encodings.\n",
        "      Y_test      = np_utils.to_categorical(Y_test-1, num_classes=2)\n",
        "\n",
        "      # convert data to NCHW (trials, kernels, channels, samples) format. Data \n",
        "      # contains 22 channels and 500 time-points. Set the number of kernels to 1.\n",
        "      X_test      = X_test.reshape(X_test.shape[0], kernels, chans, samples)\n",
        "\n",
        "      print('X_train shape:', X_test.shape)\n",
        "      print(X_test.shape[0], 'train samples')\n",
        "\n",
        "      # configure the EEGNet-8,2,16 model with kernel length of 32 samples (other \n",
        "      # model configurations may do better, but this is a good starting point)\n",
        "      model = EEGNet(nb_classes = 2, Chans = 12, Samples = 1536,\n",
        "                     dropoutRate = 0.5, kernLength = 25, F1 = 8, \n",
        "                     D = 2, F2 = 16, norm_rate = 0.25, dropoutType = 'Dropout')\n",
        "      \n",
        "      # compile the model and set the optimizers\n",
        "      model.compile(loss='categorical_crossentropy', optimizer='adam', \n",
        "                    metrics = ['accuracy'])\n",
        "\n",
        "      # count number of parameters in the model\n",
        "      numParams    = model.count_params() \n",
        "\n",
        "      # set a valid path for your system to record model checkpoints\n",
        "      checkpointer = ModelCheckpoint(filepath='/tmp/checkpoint.h5', verbose=1,\n",
        "                                     save_best_only=True)\n",
        "  \n",
        "      # the syntax is {class_1:weight_1, class_2:weight_2,...}. Here just setting\n",
        "      # the weights all to be 1\n",
        "      class_weights = {0:1, 1:1}\n",
        "\n",
        "      history = model.fit(X_train, Y_train, batch_size = 16, epochs = 300, \n",
        "                      verbose = 2, validation_data=(X_val, Y_val),\n",
        "                      callbacks=[checkpointer], class_weight = class_weights)\n",
        "  \n",
        "      # Plot training & validation accuracy values\n",
        "      plt.plot(history.history['acc'])\n",
        "      plt.plot(history.history['val_acc'])\n",
        "      plt.title('Model accuracy')\n",
        "      plt.ylabel('Accuracy')\n",
        "      plt.xlabel('Epoch')\n",
        "      plt.legend(['Train', 'Test'], loc='upper left')\n",
        "      plt.show()\n",
        "      figName = 'Accuracy_A0' + str(x) + '.png'  \n",
        "      plt.savefig(figName)\n",
        "\n",
        "      print('\\n# Evaluate on test data')\n",
        "      results = model.evaluate(X_test, Y_test, batch_size=1)\n",
        "      print('test loss, test acc:', results)\n",
        "\n",
        "      loss_all[x - 1, k-1] = results[0]\n",
        "      acc_all[x - 1, k-1] = results[1]\n",
        "\n",
        "      from keras import backend as K \n",
        "      # Do some code, e.g. train and save model\n",
        "      K.clear_session()\n",
        "\n",
        "\n",
        "print(loss_all)\n",
        "print(acc_all)\n",
        "\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "EEG_Deep/Data2A/parsed_P01T.mat\n",
            "Filtering of Training Data Finished\n",
            "EEG_Deep/Data2A/parsed_P01E.mat\n",
            "Filtering of Testing Data Finished\n",
            "(1, 2)\n",
            "Finding labels in training data\n",
            "Finding labels in testing data\n",
            "[1 2 2 1 2 2 1 2 1 2 1 1 2 1 2 2 2 2 2 2]\n",
            "(60, 12, 1536)\n",
            "(60,)\n",
            "(20, 12, 1536)\n",
            "(20,)\n",
            "[[1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]]\n",
            "X_train shape: (60, 1, 12, 1536)\n",
            "60 train samples\n",
            "20 val samples\n",
            "(40, 12, 1536)\n",
            "(40, 1)\n",
            "X_train shape: (40, 1, 12, 1536)\n",
            "40 train samples\n",
            "Train on 60 samples, validate on 20 samples\n",
            "Epoch 1/300\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.69903, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 1s - loss: 0.7631 - acc: 0.4500 - val_loss: 0.6990 - val_acc: 0.3500\n",
            "Epoch 2/300\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.69903 to 0.69884, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6860 - acc: 0.6333 - val_loss: 0.6988 - val_acc: 0.3500\n",
            "Epoch 3/300\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.69884\n",
            "60/60 - 0s - loss: 0.6916 - acc: 0.5333 - val_loss: 0.7010 - val_acc: 0.3500\n",
            "Epoch 4/300\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.69884\n",
            "60/60 - 0s - loss: 0.6411 - acc: 0.6167 - val_loss: 0.7010 - val_acc: 0.4000\n",
            "Epoch 5/300\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.69884\n",
            "60/60 - 0s - loss: 0.6470 - acc: 0.6000 - val_loss: 0.7006 - val_acc: 0.4000\n",
            "Epoch 6/300\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.69884\n",
            "60/60 - 0s - loss: 0.6392 - acc: 0.5833 - val_loss: 0.7006 - val_acc: 0.4500\n",
            "Epoch 7/300\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.69884\n",
            "60/60 - 0s - loss: 0.6120 - acc: 0.7333 - val_loss: 0.7000 - val_acc: 0.4500\n",
            "Epoch 8/300\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.69884\n",
            "60/60 - 0s - loss: 0.6080 - acc: 0.8000 - val_loss: 0.6992 - val_acc: 0.4000\n",
            "Epoch 9/300\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.69884 to 0.69873, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.6164 - acc: 0.7167 - val_loss: 0.6987 - val_acc: 0.4500\n",
            "Epoch 10/300\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.69873 to 0.69866, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5959 - acc: 0.7333 - val_loss: 0.6987 - val_acc: 0.4500\n",
            "Epoch 11/300\n",
            "\n",
            "Epoch 00011: val_loss improved from 0.69866 to 0.69829, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5955 - acc: 0.8500 - val_loss: 0.6983 - val_acc: 0.4500\n",
            "Epoch 12/300\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.69829\n",
            "60/60 - 0s - loss: 0.5853 - acc: 0.8000 - val_loss: 0.6989 - val_acc: 0.4500\n",
            "Epoch 13/300\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.69829\n",
            "60/60 - 0s - loss: 0.5683 - acc: 0.8000 - val_loss: 0.6992 - val_acc: 0.5000\n",
            "Epoch 14/300\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.69829\n",
            "60/60 - 0s - loss: 0.5599 - acc: 0.8333 - val_loss: 0.6986 - val_acc: 0.5000\n",
            "Epoch 15/300\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.69829\n",
            "60/60 - 0s - loss: 0.5481 - acc: 0.8500 - val_loss: 0.6993 - val_acc: 0.5000\n",
            "Epoch 16/300\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.69829\n",
            "60/60 - 0s - loss: 0.5317 - acc: 0.8667 - val_loss: 0.6989 - val_acc: 0.4500\n",
            "Epoch 17/300\n",
            "\n",
            "Epoch 00017: val_loss improved from 0.69829 to 0.69657, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5340 - acc: 0.8833 - val_loss: 0.6966 - val_acc: 0.5500\n",
            "Epoch 18/300\n",
            "\n",
            "Epoch 00018: val_loss improved from 0.69657 to 0.69478, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5047 - acc: 0.8833 - val_loss: 0.6948 - val_acc: 0.6500\n",
            "Epoch 19/300\n",
            "\n",
            "Epoch 00019: val_loss improved from 0.69478 to 0.69293, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5077 - acc: 0.8833 - val_loss: 0.6929 - val_acc: 0.6500\n",
            "Epoch 20/300\n",
            "\n",
            "Epoch 00020: val_loss improved from 0.69293 to 0.69136, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.5274 - acc: 0.8833 - val_loss: 0.6914 - val_acc: 0.6000\n",
            "Epoch 21/300\n",
            "\n",
            "Epoch 00021: val_loss improved from 0.69136 to 0.69072, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4918 - acc: 0.9000 - val_loss: 0.6907 - val_acc: 0.5500\n",
            "Epoch 22/300\n",
            "\n",
            "Epoch 00022: val_loss improved from 0.69072 to 0.68993, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4946 - acc: 0.8667 - val_loss: 0.6899 - val_acc: 0.5000\n",
            "Epoch 23/300\n",
            "\n",
            "Epoch 00023: val_loss improved from 0.68993 to 0.68905, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4785 - acc: 0.8833 - val_loss: 0.6891 - val_acc: 0.5000\n",
            "Epoch 24/300\n",
            "\n",
            "Epoch 00024: val_loss improved from 0.68905 to 0.68722, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4548 - acc: 0.9333 - val_loss: 0.6872 - val_acc: 0.5000\n",
            "Epoch 25/300\n",
            "\n",
            "Epoch 00025: val_loss improved from 0.68722 to 0.68290, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4778 - acc: 0.8167 - val_loss: 0.6829 - val_acc: 0.5500\n",
            "Epoch 26/300\n",
            "\n",
            "Epoch 00026: val_loss improved from 0.68290 to 0.67815, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4581 - acc: 0.9167 - val_loss: 0.6781 - val_acc: 0.5500\n",
            "Epoch 27/300\n",
            "\n",
            "Epoch 00027: val_loss improved from 0.67815 to 0.67515, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4817 - acc: 0.9167 - val_loss: 0.6751 - val_acc: 0.5500\n",
            "Epoch 28/300\n",
            "\n",
            "Epoch 00028: val_loss improved from 0.67515 to 0.67153, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4869 - acc: 0.8833 - val_loss: 0.6715 - val_acc: 0.5500\n",
            "Epoch 29/300\n",
            "\n",
            "Epoch 00029: val_loss improved from 0.67153 to 0.66648, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4438 - acc: 0.9000 - val_loss: 0.6665 - val_acc: 0.6000\n",
            "Epoch 30/300\n",
            "\n",
            "Epoch 00030: val_loss improved from 0.66648 to 0.65993, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4261 - acc: 0.9333 - val_loss: 0.6599 - val_acc: 0.6500\n",
            "Epoch 31/300\n",
            "\n",
            "Epoch 00031: val_loss improved from 0.65993 to 0.65628, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4419 - acc: 0.9667 - val_loss: 0.6563 - val_acc: 0.6500\n",
            "Epoch 32/300\n",
            "\n",
            "Epoch 00032: val_loss improved from 0.65628 to 0.65344, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4284 - acc: 0.9500 - val_loss: 0.6534 - val_acc: 0.7000\n",
            "Epoch 33/300\n",
            "\n",
            "Epoch 00033: val_loss improved from 0.65344 to 0.64822, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4230 - acc: 0.9167 - val_loss: 0.6482 - val_acc: 0.7000\n",
            "Epoch 34/300\n",
            "\n",
            "Epoch 00034: val_loss improved from 0.64822 to 0.63876, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4191 - acc: 0.9333 - val_loss: 0.6388 - val_acc: 0.6500\n",
            "Epoch 35/300\n",
            "\n",
            "Epoch 00035: val_loss improved from 0.63876 to 0.62969, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4070 - acc: 0.9167 - val_loss: 0.6297 - val_acc: 0.7000\n",
            "Epoch 36/300\n",
            "\n",
            "Epoch 00036: val_loss improved from 0.62969 to 0.62504, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4242 - acc: 0.9333 - val_loss: 0.6250 - val_acc: 0.7500\n",
            "Epoch 37/300\n",
            "\n",
            "Epoch 00037: val_loss improved from 0.62504 to 0.62331, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3940 - acc: 0.9667 - val_loss: 0.6233 - val_acc: 0.7500\n",
            "Epoch 38/300\n",
            "\n",
            "Epoch 00038: val_loss improved from 0.62331 to 0.62277, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.4032 - acc: 0.9167 - val_loss: 0.6228 - val_acc: 0.7500\n",
            "Epoch 39/300\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.62277\n",
            "60/60 - 0s - loss: 0.3893 - acc: 0.9500 - val_loss: 0.6230 - val_acc: 0.6500\n",
            "Epoch 40/300\n",
            "\n",
            "Epoch 00040: val_loss improved from 0.62277 to 0.62186, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3773 - acc: 0.9667 - val_loss: 0.6219 - val_acc: 0.6500\n",
            "Epoch 41/300\n",
            "\n",
            "Epoch 00041: val_loss improved from 0.62186 to 0.61990, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3882 - acc: 0.9333 - val_loss: 0.6199 - val_acc: 0.6500\n",
            "Epoch 42/300\n",
            "\n",
            "Epoch 00042: val_loss improved from 0.61990 to 0.61282, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3786 - acc: 0.9333 - val_loss: 0.6128 - val_acc: 0.7500\n",
            "Epoch 43/300\n",
            "\n",
            "Epoch 00043: val_loss improved from 0.61282 to 0.60430, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3767 - acc: 0.9333 - val_loss: 0.6043 - val_acc: 0.7500\n",
            "Epoch 44/300\n",
            "\n",
            "Epoch 00044: val_loss improved from 0.60430 to 0.59716, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3820 - acc: 0.9500 - val_loss: 0.5972 - val_acc: 0.7500\n",
            "Epoch 45/300\n",
            "\n",
            "Epoch 00045: val_loss improved from 0.59716 to 0.59034, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3910 - acc: 0.8833 - val_loss: 0.5903 - val_acc: 0.7500\n",
            "Epoch 46/300\n",
            "\n",
            "Epoch 00046: val_loss improved from 0.59034 to 0.58739, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3765 - acc: 0.9333 - val_loss: 0.5874 - val_acc: 0.7500\n",
            "Epoch 47/300\n",
            "\n",
            "Epoch 00047: val_loss improved from 0.58739 to 0.58378, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3310 - acc: 0.9500 - val_loss: 0.5838 - val_acc: 0.7500\n",
            "Epoch 48/300\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.58378\n",
            "60/60 - 0s - loss: 0.3785 - acc: 0.9500 - val_loss: 0.5852 - val_acc: 0.7500\n",
            "Epoch 49/300\n",
            "\n",
            "Epoch 00049: val_loss improved from 0.58378 to 0.58314, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3508 - acc: 0.9333 - val_loss: 0.5831 - val_acc: 0.7500\n",
            "Epoch 50/300\n",
            "\n",
            "Epoch 00050: val_loss improved from 0.58314 to 0.58128, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3586 - acc: 0.9333 - val_loss: 0.5813 - val_acc: 0.7500\n",
            "Epoch 51/300\n",
            "\n",
            "Epoch 00051: val_loss improved from 0.58128 to 0.57992, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3572 - acc: 0.9333 - val_loss: 0.5799 - val_acc: 0.7500\n",
            "Epoch 52/300\n",
            "\n",
            "Epoch 00052: val_loss improved from 0.57992 to 0.57724, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3317 - acc: 0.9833 - val_loss: 0.5772 - val_acc: 0.7500\n",
            "Epoch 53/300\n",
            "\n",
            "Epoch 00053: val_loss improved from 0.57724 to 0.57222, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3298 - acc: 0.9833 - val_loss: 0.5722 - val_acc: 0.7500\n",
            "Epoch 54/300\n",
            "\n",
            "Epoch 00054: val_loss improved from 0.57222 to 0.56575, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3230 - acc: 0.9667 - val_loss: 0.5657 - val_acc: 0.7500\n",
            "Epoch 55/300\n",
            "\n",
            "Epoch 00055: val_loss improved from 0.56575 to 0.55717, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3551 - acc: 0.9000 - val_loss: 0.5572 - val_acc: 0.7500\n",
            "Epoch 56/300\n",
            "\n",
            "Epoch 00056: val_loss improved from 0.55717 to 0.54909, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3317 - acc: 0.9333 - val_loss: 0.5491 - val_acc: 0.7500\n",
            "Epoch 57/300\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.54909\n",
            "60/60 - 0s - loss: 0.3589 - acc: 0.9000 - val_loss: 0.5492 - val_acc: 0.7500\n",
            "Epoch 58/300\n",
            "\n",
            "Epoch 00058: val_loss improved from 0.54909 to 0.54772, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3511 - acc: 0.9500 - val_loss: 0.5477 - val_acc: 0.7500\n",
            "Epoch 59/300\n",
            "\n",
            "Epoch 00059: val_loss improved from 0.54772 to 0.54517, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3339 - acc: 0.9500 - val_loss: 0.5452 - val_acc: 0.7500\n",
            "Epoch 60/300\n",
            "\n",
            "Epoch 00060: val_loss improved from 0.54517 to 0.53916, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3448 - acc: 0.9000 - val_loss: 0.5392 - val_acc: 0.7500\n",
            "Epoch 61/300\n",
            "\n",
            "Epoch 00061: val_loss improved from 0.53916 to 0.53154, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3186 - acc: 0.9333 - val_loss: 0.5315 - val_acc: 0.7500\n",
            "Epoch 62/300\n",
            "\n",
            "Epoch 00062: val_loss improved from 0.53154 to 0.52287, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3455 - acc: 0.9333 - val_loss: 0.5229 - val_acc: 0.8000\n",
            "Epoch 63/300\n",
            "\n",
            "Epoch 00063: val_loss improved from 0.52287 to 0.51247, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2923 - acc: 0.9167 - val_loss: 0.5125 - val_acc: 0.7500\n",
            "Epoch 64/300\n",
            "\n",
            "Epoch 00064: val_loss improved from 0.51247 to 0.50504, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3111 - acc: 0.9167 - val_loss: 0.5050 - val_acc: 0.7500\n",
            "Epoch 65/300\n",
            "\n",
            "Epoch 00065: val_loss improved from 0.50504 to 0.49951, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2765 - acc: 0.9667 - val_loss: 0.4995 - val_acc: 0.8000\n",
            "Epoch 66/300\n",
            "\n",
            "Epoch 00066: val_loss improved from 0.49951 to 0.49479, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3082 - acc: 0.8833 - val_loss: 0.4948 - val_acc: 0.8000\n",
            "Epoch 67/300\n",
            "\n",
            "Epoch 00067: val_loss improved from 0.49479 to 0.49372, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2965 - acc: 0.9167 - val_loss: 0.4937 - val_acc: 0.8000\n",
            "Epoch 68/300\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.49372\n",
            "60/60 - 0s - loss: 0.3489 - acc: 0.9167 - val_loss: 0.4944 - val_acc: 0.8500\n",
            "Epoch 69/300\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.49372\n",
            "60/60 - 0s - loss: 0.2911 - acc: 0.9333 - val_loss: 0.5000 - val_acc: 0.8500\n",
            "Epoch 70/300\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.49372\n",
            "60/60 - 0s - loss: 0.3068 - acc: 0.9333 - val_loss: 0.5050 - val_acc: 0.9000\n",
            "Epoch 71/300\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.49372\n",
            "60/60 - 0s - loss: 0.3194 - acc: 0.9333 - val_loss: 0.5029 - val_acc: 0.9000\n",
            "Epoch 72/300\n",
            "\n",
            "Epoch 00072: val_loss improved from 0.49372 to 0.48895, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2997 - acc: 0.9333 - val_loss: 0.4890 - val_acc: 0.9000\n",
            "Epoch 73/300\n",
            "\n",
            "Epoch 00073: val_loss improved from 0.48895 to 0.47996, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.3088 - acc: 0.9167 - val_loss: 0.4800 - val_acc: 0.9000\n",
            "Epoch 74/300\n",
            "\n",
            "Epoch 00074: val_loss improved from 0.47996 to 0.47367, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2815 - acc: 0.9500 - val_loss: 0.4737 - val_acc: 0.9000\n",
            "Epoch 75/300\n",
            "\n",
            "Epoch 00075: val_loss improved from 0.47367 to 0.47223, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2903 - acc: 0.9000 - val_loss: 0.4722 - val_acc: 0.9000\n",
            "Epoch 76/300\n",
            "\n",
            "Epoch 00076: val_loss improved from 0.47223 to 0.46847, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.2662 - acc: 0.9333 - val_loss: 0.4685 - val_acc: 0.9000\n",
            "Epoch 77/300\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.46847\n",
            "60/60 - 0s - loss: 0.2936 - acc: 0.9167 - val_loss: 0.4735 - val_acc: 0.9500\n",
            "Epoch 78/300\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.46847\n",
            "60/60 - 0s - loss: 0.2691 - acc: 0.9333 - val_loss: 0.4805 - val_acc: 0.9500\n",
            "Epoch 79/300\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.46847\n",
            "60/60 - 0s - loss: 0.2807 - acc: 0.9667 - val_loss: 0.4907 - val_acc: 0.9000\n",
            "Epoch 80/300\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.46847\n",
            "60/60 - 0s - loss: 0.2635 - acc: 0.9667 - val_loss: 0.5031 - val_acc: 0.8000\n",
            "Epoch 81/300\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.46847\n",
            "60/60 - 0s - loss: 0.3203 - acc: 0.8833 - val_loss: 0.5233 - val_acc: 0.8000\n",
            "Epoch 82/300\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.46847\n",
            "60/60 - 0s - loss: 0.2693 - acc: 0.9333 - val_loss: 0.5446 - val_acc: 0.7500\n",
            "Epoch 83/300\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.46847\n",
            "60/60 - 0s - loss: 0.2409 - acc: 0.9667 - val_loss: 0.5663 - val_acc: 0.7000\n",
            "Epoch 84/300\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.46847\n",
            "60/60 - 0s - loss: 0.2892 - acc: 0.9333 - val_loss: 0.5691 - val_acc: 0.7500\n",
            "Epoch 85/300\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.46847\n",
            "60/60 - 0s - loss: 0.2581 - acc: 0.9833 - val_loss: 0.5643 - val_acc: 0.7500\n",
            "Epoch 86/300\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.46847\n",
            "60/60 - 0s - loss: 0.2821 - acc: 0.9167 - val_loss: 0.5686 - val_acc: 0.7000\n",
            "Epoch 87/300\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.46847\n",
            "60/60 - 0s - loss: 0.2873 - acc: 0.9667 - val_loss: 0.5660 - val_acc: 0.7000\n",
            "Epoch 88/300\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.46847\n",
            "60/60 - 0s - loss: 0.2282 - acc: 0.9500 - val_loss: 0.5722 - val_acc: 0.6500\n",
            "Epoch 89/300\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.46847\n",
            "60/60 - 0s - loss: 0.2489 - acc: 0.9833 - val_loss: 0.5781 - val_acc: 0.6000\n",
            "Epoch 90/300\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.46847\n",
            "60/60 - 0s - loss: 0.2128 - acc: 0.9833 - val_loss: 0.5885 - val_acc: 0.6000\n",
            "Epoch 91/300\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.46847\n",
            "60/60 - 0s - loss: 0.2601 - acc: 0.9167 - val_loss: 0.5927 - val_acc: 0.5500\n",
            "Epoch 92/300\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.46847\n",
            "60/60 - 0s - loss: 0.2351 - acc: 0.9833 - val_loss: 0.6096 - val_acc: 0.5000\n",
            "Epoch 93/300\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.46847\n",
            "60/60 - 0s - loss: 0.2372 - acc: 0.9500 - val_loss: 0.6461 - val_acc: 0.4500\n",
            "Epoch 94/300\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.46847\n",
            "60/60 - 0s - loss: 0.2146 - acc: 0.9833 - val_loss: 0.6766 - val_acc: 0.4000\n",
            "Epoch 95/300\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.46847\n",
            "60/60 - 0s - loss: 0.2729 - acc: 0.9333 - val_loss: 0.7306 - val_acc: 0.4500\n",
            "Epoch 96/300\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.46847\n",
            "60/60 - 0s - loss: 0.2296 - acc: 0.9667 - val_loss: 0.7822 - val_acc: 0.4000\n",
            "Epoch 97/300\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.46847\n",
            "60/60 - 0s - loss: 0.2981 - acc: 0.9333 - val_loss: 0.8091 - val_acc: 0.4000\n",
            "Epoch 98/300\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.46847\n",
            "60/60 - 0s - loss: 0.2121 - acc: 0.9833 - val_loss: 0.8224 - val_acc: 0.4000\n",
            "Epoch 99/300\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.46847\n",
            "60/60 - 0s - loss: 0.2420 - acc: 0.9167 - val_loss: 0.8440 - val_acc: 0.4000\n",
            "Epoch 100/300\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.46847\n",
            "60/60 - 0s - loss: 0.2287 - acc: 0.9500 - val_loss: 0.8566 - val_acc: 0.4000\n",
            "Epoch 101/300\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.46847\n",
            "60/60 - 0s - loss: 0.2193 - acc: 0.9333 - val_loss: 0.8652 - val_acc: 0.4000\n",
            "Epoch 102/300\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.46847\n",
            "60/60 - 0s - loss: 0.2729 - acc: 0.9500 - val_loss: 0.8458 - val_acc: 0.4000\n",
            "Epoch 103/300\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.46847\n",
            "60/60 - 0s - loss: 0.2002 - acc: 0.9667 - val_loss: 0.8382 - val_acc: 0.4000\n",
            "Epoch 104/300\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.46847\n",
            "60/60 - 0s - loss: 0.1981 - acc: 1.0000 - val_loss: 0.8772 - val_acc: 0.4000\n",
            "Epoch 105/300\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 0.46847\n",
            "60/60 - 0s - loss: 0.2430 - acc: 0.9333 - val_loss: 0.8992 - val_acc: 0.4000\n",
            "Epoch 106/300\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 0.46847\n",
            "60/60 - 0s - loss: 0.2174 - acc: 0.9833 - val_loss: 0.9018 - val_acc: 0.4000\n",
            "Epoch 107/300\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.46847\n",
            "60/60 - 0s - loss: 0.2354 - acc: 0.9500 - val_loss: 0.8941 - val_acc: 0.4000\n",
            "Epoch 108/300\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 0.46847\n",
            "60/60 - 0s - loss: 0.2084 - acc: 0.9500 - val_loss: 0.9365 - val_acc: 0.4000\n",
            "Epoch 109/300\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.46847\n",
            "60/60 - 0s - loss: 0.2383 - acc: 0.9500 - val_loss: 0.9735 - val_acc: 0.4000\n",
            "Epoch 110/300\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.46847\n",
            "60/60 - 0s - loss: 0.2274 - acc: 0.9500 - val_loss: 1.0238 - val_acc: 0.4000\n",
            "Epoch 111/300\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 0.46847\n",
            "60/60 - 0s - loss: 0.2240 - acc: 0.9333 - val_loss: 1.0324 - val_acc: 0.4000\n",
            "Epoch 112/300\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 0.46847\n",
            "60/60 - 0s - loss: 0.1793 - acc: 0.9667 - val_loss: 1.0568 - val_acc: 0.4000\n",
            "Epoch 113/300\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.46847\n",
            "60/60 - 0s - loss: 0.2102 - acc: 0.9000 - val_loss: 1.0885 - val_acc: 0.4000\n",
            "Epoch 114/300\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.46847\n",
            "60/60 - 0s - loss: 0.2035 - acc: 0.9667 - val_loss: 1.0886 - val_acc: 0.4000\n",
            "Epoch 115/300\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.46847\n",
            "60/60 - 0s - loss: 0.2244 - acc: 0.9333 - val_loss: 1.0802 - val_acc: 0.4000\n",
            "Epoch 116/300\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.46847\n",
            "60/60 - 0s - loss: 0.2042 - acc: 0.9667 - val_loss: 1.0560 - val_acc: 0.4000\n",
            "Epoch 117/300\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.46847\n",
            "60/60 - 0s - loss: 0.1900 - acc: 0.9667 - val_loss: 1.0189 - val_acc: 0.4000\n",
            "Epoch 118/300\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.46847\n",
            "60/60 - 0s - loss: 0.1905 - acc: 0.9833 - val_loss: 0.9733 - val_acc: 0.4000\n",
            "Epoch 119/300\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 0.46847\n",
            "60/60 - 0s - loss: 0.1952 - acc: 0.9667 - val_loss: 0.9571 - val_acc: 0.4000\n",
            "Epoch 120/300\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.46847\n",
            "60/60 - 0s - loss: 0.1723 - acc: 0.9667 - val_loss: 0.9111 - val_acc: 0.4500\n",
            "Epoch 121/300\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.46847\n",
            "60/60 - 0s - loss: 0.2096 - acc: 0.9333 - val_loss: 0.8549 - val_acc: 0.4500\n",
            "Epoch 122/300\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.46847\n",
            "60/60 - 0s - loss: 0.1844 - acc: 0.9833 - val_loss: 0.8195 - val_acc: 0.4500\n",
            "Epoch 123/300\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 0.46847\n",
            "60/60 - 0s - loss: 0.2049 - acc: 0.9667 - val_loss: 0.7648 - val_acc: 0.4000\n",
            "Epoch 124/300\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 0.46847\n",
            "60/60 - 0s - loss: 0.1832 - acc: 0.9667 - val_loss: 0.7607 - val_acc: 0.4000\n",
            "Epoch 125/300\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 0.46847\n",
            "60/60 - 0s - loss: 0.1891 - acc: 0.9667 - val_loss: 0.7459 - val_acc: 0.4000\n",
            "Epoch 126/300\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 0.46847\n",
            "60/60 - 0s - loss: 0.1932 - acc: 0.9667 - val_loss: 0.7710 - val_acc: 0.4000\n",
            "Epoch 127/300\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 0.46847\n",
            "60/60 - 0s - loss: 0.2076 - acc: 0.9333 - val_loss: 0.7357 - val_acc: 0.5000\n",
            "Epoch 128/300\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 0.46847\n",
            "60/60 - 0s - loss: 0.2096 - acc: 0.9500 - val_loss: 0.6968 - val_acc: 0.5000\n",
            "Epoch 129/300\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.46847\n",
            "60/60 - 0s - loss: 0.1437 - acc: 1.0000 - val_loss: 0.6617 - val_acc: 0.5000\n",
            "Epoch 130/300\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 0.46847\n",
            "60/60 - 0s - loss: 0.1978 - acc: 0.9667 - val_loss: 0.6626 - val_acc: 0.5500\n",
            "Epoch 131/300\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 0.46847\n",
            "60/60 - 0s - loss: 0.2019 - acc: 0.9667 - val_loss: 0.7047 - val_acc: 0.4500\n",
            "Epoch 132/300\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 0.46847\n",
            "60/60 - 0s - loss: 0.1616 - acc: 0.9833 - val_loss: 0.7507 - val_acc: 0.4000\n",
            "Epoch 133/300\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.46847\n",
            "60/60 - 0s - loss: 0.2181 - acc: 0.9500 - val_loss: 0.7847 - val_acc: 0.4000\n",
            "Epoch 134/300\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.46847\n",
            "60/60 - 0s - loss: 0.1664 - acc: 0.9667 - val_loss: 0.7651 - val_acc: 0.4000\n",
            "Epoch 135/300\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 0.46847\n",
            "60/60 - 0s - loss: 0.1958 - acc: 0.9333 - val_loss: 0.7240 - val_acc: 0.4000\n",
            "Epoch 136/300\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.46847\n",
            "60/60 - 0s - loss: 0.1713 - acc: 0.9667 - val_loss: 0.7204 - val_acc: 0.4000\n",
            "Epoch 137/300\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 0.46847\n",
            "60/60 - 0s - loss: 0.1957 - acc: 0.9500 - val_loss: 0.7129 - val_acc: 0.5500\n",
            "Epoch 138/300\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.46847\n",
            "60/60 - 0s - loss: 0.1613 - acc: 0.9833 - val_loss: 0.7072 - val_acc: 0.5500\n",
            "Epoch 139/300\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.46847\n",
            "60/60 - 0s - loss: 0.1553 - acc: 0.9667 - val_loss: 0.7343 - val_acc: 0.4500\n",
            "Epoch 140/300\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 0.46847\n",
            "60/60 - 0s - loss: 0.1928 - acc: 0.9333 - val_loss: 0.7469 - val_acc: 0.4500\n",
            "Epoch 141/300\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.46847\n",
            "60/60 - 0s - loss: 0.1314 - acc: 0.9833 - val_loss: 0.7572 - val_acc: 0.4500\n",
            "Epoch 142/300\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.46847\n",
            "60/60 - 0s - loss: 0.1804 - acc: 0.9500 - val_loss: 0.7784 - val_acc: 0.5000\n",
            "Epoch 143/300\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 0.46847\n",
            "60/60 - 0s - loss: 0.1571 - acc: 1.0000 - val_loss: 0.7826 - val_acc: 0.5000\n",
            "Epoch 144/300\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 0.46847\n",
            "60/60 - 0s - loss: 0.1750 - acc: 0.9667 - val_loss: 0.7621 - val_acc: 0.5000\n",
            "Epoch 145/300\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.46847\n",
            "60/60 - 0s - loss: 0.1891 - acc: 0.9500 - val_loss: 0.6942 - val_acc: 0.5500\n",
            "Epoch 146/300\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.46847\n",
            "60/60 - 0s - loss: 0.1585 - acc: 0.9667 - val_loss: 0.6325 - val_acc: 0.6000\n",
            "Epoch 147/300\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.46847\n",
            "60/60 - 0s - loss: 0.1610 - acc: 0.9833 - val_loss: 0.6075 - val_acc: 0.6000\n",
            "Epoch 148/300\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 0.46847\n",
            "60/60 - 0s - loss: 0.1733 - acc: 0.9667 - val_loss: 0.6186 - val_acc: 0.6000\n",
            "Epoch 149/300\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.46847\n",
            "60/60 - 0s - loss: 0.1934 - acc: 0.9500 - val_loss: 0.6942 - val_acc: 0.5500\n",
            "Epoch 150/300\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.46847\n",
            "60/60 - 0s - loss: 0.1776 - acc: 0.9500 - val_loss: 0.7261 - val_acc: 0.5000\n",
            "Epoch 151/300\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.46847\n",
            "60/60 - 0s - loss: 0.1783 - acc: 0.9500 - val_loss: 0.7044 - val_acc: 0.5500\n",
            "Epoch 152/300\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.46847\n",
            "60/60 - 0s - loss: 0.1433 - acc: 1.0000 - val_loss: 0.6846 - val_acc: 0.5500\n",
            "Epoch 153/300\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 0.46847\n",
            "60/60 - 0s - loss: 0.2097 - acc: 0.9500 - val_loss: 0.7110 - val_acc: 0.5500\n",
            "Epoch 154/300\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.46847\n",
            "60/60 - 0s - loss: 0.2563 - acc: 0.9167 - val_loss: 0.7535 - val_acc: 0.4000\n",
            "Epoch 155/300\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.46847\n",
            "60/60 - 0s - loss: 0.1620 - acc: 0.9667 - val_loss: 0.8205 - val_acc: 0.4500\n",
            "Epoch 156/300\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.46847\n",
            "60/60 - 0s - loss: 0.1555 - acc: 0.9833 - val_loss: 0.8502 - val_acc: 0.4500\n",
            "Epoch 157/300\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.46847\n",
            "60/60 - 0s - loss: 0.1650 - acc: 0.9667 - val_loss: 0.8648 - val_acc: 0.4500\n",
            "Epoch 158/300\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.46847\n",
            "60/60 - 0s - loss: 0.1456 - acc: 0.9667 - val_loss: 0.8369 - val_acc: 0.4500\n",
            "Epoch 159/300\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.46847\n",
            "60/60 - 0s - loss: 0.1267 - acc: 0.9833 - val_loss: 0.8065 - val_acc: 0.4000\n",
            "Epoch 160/300\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.46847\n",
            "60/60 - 0s - loss: 0.1806 - acc: 0.9500 - val_loss: 0.7899 - val_acc: 0.4000\n",
            "Epoch 161/300\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.46847\n",
            "60/60 - 0s - loss: 0.1562 - acc: 0.9833 - val_loss: 0.7580 - val_acc: 0.4500\n",
            "Epoch 162/300\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.46847\n",
            "60/60 - 0s - loss: 0.1752 - acc: 0.9167 - val_loss: 0.7233 - val_acc: 0.5500\n",
            "Epoch 163/300\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.46847\n",
            "60/60 - 0s - loss: 0.1562 - acc: 1.0000 - val_loss: 0.6950 - val_acc: 0.6000\n",
            "Epoch 164/300\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.46847\n",
            "60/60 - 0s - loss: 0.1489 - acc: 0.9667 - val_loss: 0.6427 - val_acc: 0.6000\n",
            "Epoch 165/300\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.46847\n",
            "60/60 - 0s - loss: 0.1003 - acc: 1.0000 - val_loss: 0.5935 - val_acc: 0.6500\n",
            "Epoch 166/300\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 0.46847\n",
            "60/60 - 0s - loss: 0.2126 - acc: 0.9167 - val_loss: 0.5560 - val_acc: 0.6500\n",
            "Epoch 167/300\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.46847\n",
            "60/60 - 0s - loss: 0.1217 - acc: 1.0000 - val_loss: 0.5542 - val_acc: 0.6500\n",
            "Epoch 168/300\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.46847\n",
            "60/60 - 0s - loss: 0.1387 - acc: 0.9833 - val_loss: 0.5450 - val_acc: 0.6500\n",
            "Epoch 169/300\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 0.46847\n",
            "60/60 - 0s - loss: 0.1088 - acc: 0.9833 - val_loss: 0.5308 - val_acc: 0.7000\n",
            "Epoch 170/300\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.46847\n",
            "60/60 - 0s - loss: 0.1323 - acc: 0.9667 - val_loss: 0.5163 - val_acc: 0.7000\n",
            "Epoch 171/300\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.46847\n",
            "60/60 - 0s - loss: 0.2115 - acc: 0.9167 - val_loss: 0.5234 - val_acc: 0.7000\n",
            "Epoch 172/300\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.46847\n",
            "60/60 - 0s - loss: 0.2032 - acc: 0.9333 - val_loss: 0.5828 - val_acc: 0.7000\n",
            "Epoch 173/300\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.46847\n",
            "60/60 - 0s - loss: 0.1528 - acc: 0.9500 - val_loss: 0.6212 - val_acc: 0.7000\n",
            "Epoch 174/300\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.46847\n",
            "60/60 - 0s - loss: 0.1700 - acc: 0.9833 - val_loss: 0.6375 - val_acc: 0.7000\n",
            "Epoch 175/300\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.46847\n",
            "60/60 - 0s - loss: 0.1249 - acc: 1.0000 - val_loss: 0.6291 - val_acc: 0.7000\n",
            "Epoch 176/300\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 0.46847\n",
            "60/60 - 0s - loss: 0.1482 - acc: 0.9833 - val_loss: 0.6120 - val_acc: 0.7000\n",
            "Epoch 177/300\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.46847\n",
            "60/60 - 0s - loss: 0.1239 - acc: 1.0000 - val_loss: 0.5912 - val_acc: 0.6500\n",
            "Epoch 178/300\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.46847\n",
            "60/60 - 0s - loss: 0.1316 - acc: 1.0000 - val_loss: 0.5942 - val_acc: 0.6500\n",
            "Epoch 179/300\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.46847\n",
            "60/60 - 0s - loss: 0.1869 - acc: 0.9667 - val_loss: 0.6079 - val_acc: 0.6500\n",
            "Epoch 180/300\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.46847\n",
            "60/60 - 0s - loss: 0.1201 - acc: 0.9833 - val_loss: 0.5931 - val_acc: 0.6500\n",
            "Epoch 181/300\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.46847\n",
            "60/60 - 0s - loss: 0.1204 - acc: 1.0000 - val_loss: 0.5838 - val_acc: 0.6500\n",
            "Epoch 182/300\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.46847\n",
            "60/60 - 0s - loss: 0.1204 - acc: 0.9667 - val_loss: 0.5550 - val_acc: 0.7000\n",
            "Epoch 183/300\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.46847\n",
            "60/60 - 0s - loss: 0.1519 - acc: 0.9500 - val_loss: 0.5501 - val_acc: 0.7000\n",
            "Epoch 184/300\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 0.46847\n",
            "60/60 - 0s - loss: 0.1154 - acc: 0.9833 - val_loss: 0.5626 - val_acc: 0.7000\n",
            "Epoch 185/300\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.46847\n",
            "60/60 - 0s - loss: 0.1406 - acc: 0.9667 - val_loss: 0.5473 - val_acc: 0.7000\n",
            "Epoch 186/300\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.46847\n",
            "60/60 - 0s - loss: 0.1333 - acc: 0.9833 - val_loss: 0.5268 - val_acc: 0.7000\n",
            "Epoch 187/300\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 0.46847\n",
            "60/60 - 0s - loss: 0.1278 - acc: 1.0000 - val_loss: 0.5296 - val_acc: 0.7000\n",
            "Epoch 188/300\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.46847\n",
            "60/60 - 0s - loss: 0.1171 - acc: 0.9667 - val_loss: 0.5279 - val_acc: 0.7000\n",
            "Epoch 189/300\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.46847\n",
            "60/60 - 0s - loss: 0.1075 - acc: 1.0000 - val_loss: 0.5234 - val_acc: 0.7000\n",
            "Epoch 190/300\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.46847\n",
            "60/60 - 0s - loss: 0.1237 - acc: 0.9833 - val_loss: 0.5161 - val_acc: 0.7000\n",
            "Epoch 191/300\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.46847\n",
            "60/60 - 0s - loss: 0.0967 - acc: 0.9833 - val_loss: 0.4959 - val_acc: 0.7000\n",
            "Epoch 192/300\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 0.46847\n",
            "60/60 - 0s - loss: 0.1027 - acc: 1.0000 - val_loss: 0.4776 - val_acc: 0.7000\n",
            "Epoch 193/300\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.46847\n",
            "60/60 - 0s - loss: 0.1355 - acc: 0.9833 - val_loss: 0.5011 - val_acc: 0.7000\n",
            "Epoch 194/300\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.46847\n",
            "60/60 - 0s - loss: 0.1354 - acc: 0.9500 - val_loss: 0.5435 - val_acc: 0.7000\n",
            "Epoch 195/300\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.46847\n",
            "60/60 - 0s - loss: 0.0859 - acc: 0.9833 - val_loss: 0.5570 - val_acc: 0.7000\n",
            "Epoch 196/300\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.46847\n",
            "60/60 - 0s - loss: 0.1073 - acc: 0.9833 - val_loss: 0.5291 - val_acc: 0.7000\n",
            "Epoch 197/300\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.46847\n",
            "60/60 - 0s - loss: 0.1734 - acc: 0.9500 - val_loss: 0.4958 - val_acc: 0.7000\n",
            "Epoch 198/300\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.46847\n",
            "60/60 - 0s - loss: 0.1546 - acc: 0.9667 - val_loss: 0.4685 - val_acc: 0.7000\n",
            "Epoch 199/300\n",
            "\n",
            "Epoch 00199: val_loss improved from 0.46847 to 0.46205, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.0976 - acc: 1.0000 - val_loss: 0.4621 - val_acc: 0.7000\n",
            "Epoch 200/300\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.46205\n",
            "60/60 - 0s - loss: 0.1333 - acc: 0.9500 - val_loss: 0.4757 - val_acc: 0.7000\n",
            "Epoch 201/300\n",
            "\n",
            "Epoch 00201: val_loss did not improve from 0.46205\n",
            "60/60 - 0s - loss: 0.1236 - acc: 0.9833 - val_loss: 0.5259 - val_acc: 0.7000\n",
            "Epoch 202/300\n",
            "\n",
            "Epoch 00202: val_loss did not improve from 0.46205\n",
            "60/60 - 0s - loss: 0.1390 - acc: 0.9667 - val_loss: 0.5691 - val_acc: 0.7000\n",
            "Epoch 203/300\n",
            "\n",
            "Epoch 00203: val_loss did not improve from 0.46205\n",
            "60/60 - 0s - loss: 0.1202 - acc: 1.0000 - val_loss: 0.5687 - val_acc: 0.7000\n",
            "Epoch 204/300\n",
            "\n",
            "Epoch 00204: val_loss did not improve from 0.46205\n",
            "60/60 - 0s - loss: 0.1357 - acc: 0.9833 - val_loss: 0.5727 - val_acc: 0.7000\n",
            "Epoch 205/300\n",
            "\n",
            "Epoch 00205: val_loss did not improve from 0.46205\n",
            "60/60 - 0s - loss: 0.1076 - acc: 0.9667 - val_loss: 0.5576 - val_acc: 0.7000\n",
            "Epoch 206/300\n",
            "\n",
            "Epoch 00206: val_loss did not improve from 0.46205\n",
            "60/60 - 0s - loss: 0.1033 - acc: 1.0000 - val_loss: 0.5431 - val_acc: 0.7000\n",
            "Epoch 207/300\n",
            "\n",
            "Epoch 00207: val_loss did not improve from 0.46205\n",
            "60/60 - 0s - loss: 0.1463 - acc: 0.9667 - val_loss: 0.5632 - val_acc: 0.7000\n",
            "Epoch 208/300\n",
            "\n",
            "Epoch 00208: val_loss did not improve from 0.46205\n",
            "60/60 - 0s - loss: 0.1173 - acc: 0.9833 - val_loss: 0.5387 - val_acc: 0.7000\n",
            "Epoch 209/300\n",
            "\n",
            "Epoch 00209: val_loss did not improve from 0.46205\n",
            "60/60 - 0s - loss: 0.1535 - acc: 0.9500 - val_loss: 0.5204 - val_acc: 0.7000\n",
            "Epoch 210/300\n",
            "\n",
            "Epoch 00210: val_loss did not improve from 0.46205\n",
            "60/60 - 0s - loss: 0.1257 - acc: 0.9667 - val_loss: 0.5009 - val_acc: 0.7500\n",
            "Epoch 211/300\n",
            "\n",
            "Epoch 00211: val_loss did not improve from 0.46205\n",
            "60/60 - 0s - loss: 0.1152 - acc: 0.9833 - val_loss: 0.4703 - val_acc: 0.7500\n",
            "Epoch 212/300\n",
            "\n",
            "Epoch 00212: val_loss did not improve from 0.46205\n",
            "60/60 - 0s - loss: 0.1128 - acc: 1.0000 - val_loss: 0.4648 - val_acc: 0.7500\n",
            "Epoch 213/300\n",
            "\n",
            "Epoch 00213: val_loss did not improve from 0.46205\n",
            "60/60 - 0s - loss: 0.1232 - acc: 1.0000 - val_loss: 0.4702 - val_acc: 0.7500\n",
            "Epoch 214/300\n",
            "\n",
            "Epoch 00214: val_loss improved from 0.46205 to 0.45169, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1281 - acc: 1.0000 - val_loss: 0.4517 - val_acc: 0.7500\n",
            "Epoch 215/300\n",
            "\n",
            "Epoch 00215: val_loss improved from 0.45169 to 0.44486, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1020 - acc: 0.9833 - val_loss: 0.4449 - val_acc: 0.7500\n",
            "Epoch 216/300\n",
            "\n",
            "Epoch 00216: val_loss did not improve from 0.44486\n",
            "60/60 - 0s - loss: 0.1655 - acc: 0.9500 - val_loss: 0.4494 - val_acc: 0.7500\n",
            "Epoch 217/300\n",
            "\n",
            "Epoch 00217: val_loss did not improve from 0.44486\n",
            "60/60 - 0s - loss: 0.1316 - acc: 0.9833 - val_loss: 0.4630 - val_acc: 0.7500\n",
            "Epoch 218/300\n",
            "\n",
            "Epoch 00218: val_loss did not improve from 0.44486\n",
            "60/60 - 0s - loss: 0.1153 - acc: 1.0000 - val_loss: 0.4609 - val_acc: 0.7500\n",
            "Epoch 219/300\n",
            "\n",
            "Epoch 00219: val_loss did not improve from 0.44486\n",
            "60/60 - 0s - loss: 0.1163 - acc: 0.9667 - val_loss: 0.4696 - val_acc: 0.7500\n",
            "Epoch 220/300\n",
            "\n",
            "Epoch 00220: val_loss did not improve from 0.44486\n",
            "60/60 - 0s - loss: 0.1031 - acc: 0.9667 - val_loss: 0.4778 - val_acc: 0.7500\n",
            "Epoch 221/300\n",
            "\n",
            "Epoch 00221: val_loss did not improve from 0.44486\n",
            "60/60 - 0s - loss: 0.1139 - acc: 1.0000 - val_loss: 0.5094 - val_acc: 0.7500\n",
            "Epoch 222/300\n",
            "\n",
            "Epoch 00222: val_loss did not improve from 0.44486\n",
            "60/60 - 0s - loss: 0.1022 - acc: 0.9833 - val_loss: 0.5317 - val_acc: 0.7500\n",
            "Epoch 223/300\n",
            "\n",
            "Epoch 00223: val_loss did not improve from 0.44486\n",
            "60/60 - 0s - loss: 0.0987 - acc: 1.0000 - val_loss: 0.5487 - val_acc: 0.7000\n",
            "Epoch 224/300\n",
            "\n",
            "Epoch 00224: val_loss did not improve from 0.44486\n",
            "60/60 - 0s - loss: 0.0795 - acc: 1.0000 - val_loss: 0.5662 - val_acc: 0.7000\n",
            "Epoch 225/300\n",
            "\n",
            "Epoch 00225: val_loss did not improve from 0.44486\n",
            "60/60 - 0s - loss: 0.0951 - acc: 1.0000 - val_loss: 0.5557 - val_acc: 0.7000\n",
            "Epoch 226/300\n",
            "\n",
            "Epoch 00226: val_loss did not improve from 0.44486\n",
            "60/60 - 0s - loss: 0.0920 - acc: 0.9833 - val_loss: 0.5190 - val_acc: 0.7500\n",
            "Epoch 227/300\n",
            "\n",
            "Epoch 00227: val_loss did not improve from 0.44486\n",
            "60/60 - 0s - loss: 0.0775 - acc: 1.0000 - val_loss: 0.4879 - val_acc: 0.7500\n",
            "Epoch 228/300\n",
            "\n",
            "Epoch 00228: val_loss did not improve from 0.44486\n",
            "60/60 - 0s - loss: 0.1473 - acc: 0.9500 - val_loss: 0.4837 - val_acc: 0.7500\n",
            "Epoch 229/300\n",
            "\n",
            "Epoch 00229: val_loss did not improve from 0.44486\n",
            "60/60 - 0s - loss: 0.1175 - acc: 0.9833 - val_loss: 0.4987 - val_acc: 0.7500\n",
            "Epoch 230/300\n",
            "\n",
            "Epoch 00230: val_loss did not improve from 0.44486\n",
            "60/60 - 0s - loss: 0.0982 - acc: 0.9833 - val_loss: 0.5039 - val_acc: 0.7500\n",
            "Epoch 231/300\n",
            "\n",
            "Epoch 00231: val_loss did not improve from 0.44486\n",
            "60/60 - 0s - loss: 0.1357 - acc: 0.9667 - val_loss: 0.5105 - val_acc: 0.7500\n",
            "Epoch 232/300\n",
            "\n",
            "Epoch 00232: val_loss did not improve from 0.44486\n",
            "60/60 - 0s - loss: 0.1235 - acc: 0.9833 - val_loss: 0.4956 - val_acc: 0.7500\n",
            "Epoch 233/300\n",
            "\n",
            "Epoch 00233: val_loss did not improve from 0.44486\n",
            "60/60 - 0s - loss: 0.0933 - acc: 1.0000 - val_loss: 0.4754 - val_acc: 0.7500\n",
            "Epoch 234/300\n",
            "\n",
            "Epoch 00234: val_loss did not improve from 0.44486\n",
            "60/60 - 0s - loss: 0.1231 - acc: 0.9667 - val_loss: 0.4736 - val_acc: 0.7500\n",
            "Epoch 235/300\n",
            "\n",
            "Epoch 00235: val_loss did not improve from 0.44486\n",
            "60/60 - 0s - loss: 0.1957 - acc: 0.9333 - val_loss: 0.5144 - val_acc: 0.7500\n",
            "Epoch 236/300\n",
            "\n",
            "Epoch 00236: val_loss did not improve from 0.44486\n",
            "60/60 - 0s - loss: 0.0903 - acc: 1.0000 - val_loss: 0.5175 - val_acc: 0.7500\n",
            "Epoch 237/300\n",
            "\n",
            "Epoch 00237: val_loss did not improve from 0.44486\n",
            "60/60 - 0s - loss: 0.0940 - acc: 1.0000 - val_loss: 0.5113 - val_acc: 0.7500\n",
            "Epoch 238/300\n",
            "\n",
            "Epoch 00238: val_loss did not improve from 0.44486\n",
            "60/60 - 0s - loss: 0.0899 - acc: 1.0000 - val_loss: 0.5098 - val_acc: 0.7500\n",
            "Epoch 239/300\n",
            "\n",
            "Epoch 00239: val_loss did not improve from 0.44486\n",
            "60/60 - 0s - loss: 0.1281 - acc: 0.9833 - val_loss: 0.4883 - val_acc: 0.7500\n",
            "Epoch 240/300\n",
            "\n",
            "Epoch 00240: val_loss did not improve from 0.44486\n",
            "60/60 - 0s - loss: 0.1271 - acc: 1.0000 - val_loss: 0.5104 - val_acc: 0.7500\n",
            "Epoch 241/300\n",
            "\n",
            "Epoch 00241: val_loss did not improve from 0.44486\n",
            "60/60 - 0s - loss: 0.1236 - acc: 0.9500 - val_loss: 0.5153 - val_acc: 0.7500\n",
            "Epoch 242/300\n",
            "\n",
            "Epoch 00242: val_loss did not improve from 0.44486\n",
            "60/60 - 0s - loss: 0.1405 - acc: 0.9667 - val_loss: 0.5115 - val_acc: 0.7500\n",
            "Epoch 243/300\n",
            "\n",
            "Epoch 00243: val_loss did not improve from 0.44486\n",
            "60/60 - 0s - loss: 0.0865 - acc: 1.0000 - val_loss: 0.5039 - val_acc: 0.7500\n",
            "Epoch 244/300\n",
            "\n",
            "Epoch 00244: val_loss did not improve from 0.44486\n",
            "60/60 - 0s - loss: 0.1258 - acc: 0.9833 - val_loss: 0.4887 - val_acc: 0.7500\n",
            "Epoch 245/300\n",
            "\n",
            "Epoch 00245: val_loss did not improve from 0.44486\n",
            "60/60 - 0s - loss: 0.0756 - acc: 1.0000 - val_loss: 0.4853 - val_acc: 0.7500\n",
            "Epoch 246/300\n",
            "\n",
            "Epoch 00246: val_loss did not improve from 0.44486\n",
            "60/60 - 0s - loss: 0.0928 - acc: 0.9833 - val_loss: 0.4988 - val_acc: 0.7500\n",
            "Epoch 247/300\n",
            "\n",
            "Epoch 00247: val_loss did not improve from 0.44486\n",
            "60/60 - 0s - loss: 0.0903 - acc: 1.0000 - val_loss: 0.5238 - val_acc: 0.7500\n",
            "Epoch 248/300\n",
            "\n",
            "Epoch 00248: val_loss did not improve from 0.44486\n",
            "60/60 - 0s - loss: 0.0919 - acc: 0.9833 - val_loss: 0.5261 - val_acc: 0.7500\n",
            "Epoch 249/300\n",
            "\n",
            "Epoch 00249: val_loss did not improve from 0.44486\n",
            "60/60 - 0s - loss: 0.1312 - acc: 0.9833 - val_loss: 0.5221 - val_acc: 0.7500\n",
            "Epoch 250/300\n",
            "\n",
            "Epoch 00250: val_loss did not improve from 0.44486\n",
            "60/60 - 0s - loss: 0.1082 - acc: 0.9833 - val_loss: 0.5001 - val_acc: 0.7500\n",
            "Epoch 251/300\n",
            "\n",
            "Epoch 00251: val_loss did not improve from 0.44486\n",
            "60/60 - 0s - loss: 0.1037 - acc: 0.9833 - val_loss: 0.4775 - val_acc: 0.7500\n",
            "Epoch 252/300\n",
            "\n",
            "Epoch 00252: val_loss improved from 0.44486 to 0.44236, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.0924 - acc: 1.0000 - val_loss: 0.4424 - val_acc: 0.7500\n",
            "Epoch 253/300\n",
            "\n",
            "Epoch 00253: val_loss did not improve from 0.44236\n",
            "60/60 - 0s - loss: 0.1784 - acc: 0.9667 - val_loss: 0.4547 - val_acc: 0.7500\n",
            "Epoch 254/300\n",
            "\n",
            "Epoch 00254: val_loss did not improve from 0.44236\n",
            "60/60 - 0s - loss: 0.0835 - acc: 1.0000 - val_loss: 0.4579 - val_acc: 0.8500\n",
            "Epoch 255/300\n",
            "\n",
            "Epoch 00255: val_loss improved from 0.44236 to 0.43809, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1555 - acc: 0.9833 - val_loss: 0.4381 - val_acc: 0.8500\n",
            "Epoch 256/300\n",
            "\n",
            "Epoch 00256: val_loss did not improve from 0.43809\n",
            "60/60 - 0s - loss: 0.1073 - acc: 0.9667 - val_loss: 0.4443 - val_acc: 0.8500\n",
            "Epoch 257/300\n",
            "\n",
            "Epoch 00257: val_loss did not improve from 0.43809\n",
            "60/60 - 0s - loss: 0.1208 - acc: 1.0000 - val_loss: 0.4642 - val_acc: 0.7500\n",
            "Epoch 258/300\n",
            "\n",
            "Epoch 00258: val_loss did not improve from 0.43809\n",
            "60/60 - 0s - loss: 0.0985 - acc: 0.9667 - val_loss: 0.4864 - val_acc: 0.7000\n",
            "Epoch 259/300\n",
            "\n",
            "Epoch 00259: val_loss did not improve from 0.43809\n",
            "60/60 - 0s - loss: 0.1073 - acc: 0.9667 - val_loss: 0.5057 - val_acc: 0.7000\n",
            "Epoch 260/300\n",
            "\n",
            "Epoch 00260: val_loss did not improve from 0.43809\n",
            "60/60 - 0s - loss: 0.0762 - acc: 1.0000 - val_loss: 0.5272 - val_acc: 0.7000\n",
            "Epoch 261/300\n",
            "\n",
            "Epoch 00261: val_loss did not improve from 0.43809\n",
            "60/60 - 0s - loss: 0.1042 - acc: 1.0000 - val_loss: 0.5533 - val_acc: 0.7000\n",
            "Epoch 262/300\n",
            "\n",
            "Epoch 00262: val_loss did not improve from 0.43809\n",
            "60/60 - 0s - loss: 0.1459 - acc: 0.9333 - val_loss: 0.5913 - val_acc: 0.7000\n",
            "Epoch 263/300\n",
            "\n",
            "Epoch 00263: val_loss did not improve from 0.43809\n",
            "60/60 - 0s - loss: 0.1185 - acc: 1.0000 - val_loss: 0.6241 - val_acc: 0.7000\n",
            "Epoch 264/300\n",
            "\n",
            "Epoch 00264: val_loss did not improve from 0.43809\n",
            "60/60 - 0s - loss: 0.1338 - acc: 0.9667 - val_loss: 0.6710 - val_acc: 0.6500\n",
            "Epoch 265/300\n",
            "\n",
            "Epoch 00265: val_loss did not improve from 0.43809\n",
            "60/60 - 0s - loss: 0.1078 - acc: 0.9833 - val_loss: 0.6355 - val_acc: 0.7000\n",
            "Epoch 266/300\n",
            "\n",
            "Epoch 00266: val_loss did not improve from 0.43809\n",
            "60/60 - 0s - loss: 0.0790 - acc: 1.0000 - val_loss: 0.5774 - val_acc: 0.7000\n",
            "Epoch 267/300\n",
            "\n",
            "Epoch 00267: val_loss did not improve from 0.43809\n",
            "60/60 - 0s - loss: 0.0918 - acc: 1.0000 - val_loss: 0.5468 - val_acc: 0.7500\n",
            "Epoch 268/300\n",
            "\n",
            "Epoch 00268: val_loss did not improve from 0.43809\n",
            "60/60 - 0s - loss: 0.1109 - acc: 0.9833 - val_loss: 0.5436 - val_acc: 0.7500\n",
            "Epoch 269/300\n",
            "\n",
            "Epoch 00269: val_loss did not improve from 0.43809\n",
            "60/60 - 0s - loss: 0.0779 - acc: 1.0000 - val_loss: 0.5425 - val_acc: 0.7500\n",
            "Epoch 270/300\n",
            "\n",
            "Epoch 00270: val_loss did not improve from 0.43809\n",
            "60/60 - 0s - loss: 0.1088 - acc: 0.9833 - val_loss: 0.5484 - val_acc: 0.7500\n",
            "Epoch 271/300\n",
            "\n",
            "Epoch 00271: val_loss did not improve from 0.43809\n",
            "60/60 - 0s - loss: 0.0886 - acc: 0.9833 - val_loss: 0.5686 - val_acc: 0.7500\n",
            "Epoch 272/300\n",
            "\n",
            "Epoch 00272: val_loss did not improve from 0.43809\n",
            "60/60 - 0s - loss: 0.1566 - acc: 0.9667 - val_loss: 0.5648 - val_acc: 0.7500\n",
            "Epoch 273/300\n",
            "\n",
            "Epoch 00273: val_loss did not improve from 0.43809\n",
            "60/60 - 0s - loss: 0.0665 - acc: 1.0000 - val_loss: 0.5545 - val_acc: 0.7500\n",
            "Epoch 274/300\n",
            "\n",
            "Epoch 00274: val_loss did not improve from 0.43809\n",
            "60/60 - 0s - loss: 0.0987 - acc: 1.0000 - val_loss: 0.5494 - val_acc: 0.7500\n",
            "Epoch 275/300\n",
            "\n",
            "Epoch 00275: val_loss did not improve from 0.43809\n",
            "60/60 - 0s - loss: 0.1208 - acc: 0.9667 - val_loss: 0.5379 - val_acc: 0.7500\n",
            "Epoch 276/300\n",
            "\n",
            "Epoch 00276: val_loss did not improve from 0.43809\n",
            "60/60 - 0s - loss: 0.0741 - acc: 1.0000 - val_loss: 0.5202 - val_acc: 0.7500\n",
            "Epoch 277/300\n",
            "\n",
            "Epoch 00277: val_loss did not improve from 0.43809\n",
            "60/60 - 0s - loss: 0.1388 - acc: 0.9833 - val_loss: 0.5046 - val_acc: 0.7500\n",
            "Epoch 278/300\n",
            "\n",
            "Epoch 00278: val_loss did not improve from 0.43809\n",
            "60/60 - 0s - loss: 0.0908 - acc: 1.0000 - val_loss: 0.4674 - val_acc: 0.8500\n",
            "Epoch 279/300\n",
            "\n",
            "Epoch 00279: val_loss did not improve from 0.43809\n",
            "60/60 - 0s - loss: 0.1411 - acc: 0.9500 - val_loss: 0.4425 - val_acc: 0.8500\n",
            "Epoch 280/300\n",
            "\n",
            "Epoch 00280: val_loss improved from 0.43809 to 0.43351, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.0959 - acc: 1.0000 - val_loss: 0.4335 - val_acc: 0.8500\n",
            "Epoch 281/300\n",
            "\n",
            "Epoch 00281: val_loss did not improve from 0.43351\n",
            "60/60 - 0s - loss: 0.1194 - acc: 0.9667 - val_loss: 0.4435 - val_acc: 0.8500\n",
            "Epoch 282/300\n",
            "\n",
            "Epoch 00282: val_loss did not improve from 0.43351\n",
            "60/60 - 0s - loss: 0.0944 - acc: 1.0000 - val_loss: 0.4363 - val_acc: 0.8500\n",
            "Epoch 283/300\n",
            "\n",
            "Epoch 00283: val_loss improved from 0.43351 to 0.43005, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.1113 - acc: 0.9667 - val_loss: 0.4300 - val_acc: 0.8500\n",
            "Epoch 284/300\n",
            "\n",
            "Epoch 00284: val_loss improved from 0.43005 to 0.42342, saving model to /tmp/checkpoint.h5\n",
            "60/60 - 0s - loss: 0.0917 - acc: 1.0000 - val_loss: 0.4234 - val_acc: 0.8500\n",
            "Epoch 285/300\n",
            "\n",
            "Epoch 00285: val_loss did not improve from 0.42342\n",
            "60/60 - 0s - loss: 0.0783 - acc: 1.0000 - val_loss: 0.4279 - val_acc: 0.8500\n",
            "Epoch 286/300\n",
            "\n",
            "Epoch 00286: val_loss did not improve from 0.42342\n",
            "60/60 - 0s - loss: 0.1205 - acc: 0.9833 - val_loss: 0.4557 - val_acc: 0.8000\n",
            "Epoch 287/300\n",
            "\n",
            "Epoch 00287: val_loss did not improve from 0.42342\n",
            "60/60 - 0s - loss: 0.1132 - acc: 0.9833 - val_loss: 0.4939 - val_acc: 0.8000\n",
            "Epoch 288/300\n",
            "\n",
            "Epoch 00288: val_loss did not improve from 0.42342\n",
            "60/60 - 0s - loss: 0.0909 - acc: 1.0000 - val_loss: 0.5232 - val_acc: 0.8000\n",
            "Epoch 289/300\n",
            "\n",
            "Epoch 00289: val_loss did not improve from 0.42342\n",
            "60/60 - 0s - loss: 0.0762 - acc: 0.9833 - val_loss: 0.5621 - val_acc: 0.7500\n",
            "Epoch 290/300\n",
            "\n",
            "Epoch 00290: val_loss did not improve from 0.42342\n",
            "60/60 - 0s - loss: 0.0661 - acc: 1.0000 - val_loss: 0.6096 - val_acc: 0.7000\n",
            "Epoch 291/300\n",
            "\n",
            "Epoch 00291: val_loss did not improve from 0.42342\n",
            "60/60 - 0s - loss: 0.1259 - acc: 0.9667 - val_loss: 0.6806 - val_acc: 0.6500\n",
            "Epoch 292/300\n",
            "\n",
            "Epoch 00292: val_loss did not improve from 0.42342\n",
            "60/60 - 0s - loss: 0.0780 - acc: 1.0000 - val_loss: 0.7916 - val_acc: 0.6000\n",
            "Epoch 293/300\n",
            "\n",
            "Epoch 00293: val_loss did not improve from 0.42342\n",
            "60/60 - 0s - loss: 0.0843 - acc: 0.9833 - val_loss: 0.8134 - val_acc: 0.6000\n",
            "Epoch 294/300\n",
            "\n",
            "Epoch 00294: val_loss did not improve from 0.42342\n",
            "60/60 - 0s - loss: 0.0830 - acc: 0.9833 - val_loss: 0.8111 - val_acc: 0.6000\n",
            "Epoch 295/300\n",
            "\n",
            "Epoch 00295: val_loss did not improve from 0.42342\n",
            "60/60 - 0s - loss: 0.0982 - acc: 0.9833 - val_loss: 0.7845 - val_acc: 0.6000\n",
            "Epoch 296/300\n",
            "\n",
            "Epoch 00296: val_loss did not improve from 0.42342\n",
            "60/60 - 0s - loss: 0.1141 - acc: 0.9667 - val_loss: 0.7765 - val_acc: 0.6000\n",
            "Epoch 297/300\n",
            "\n",
            "Epoch 00297: val_loss did not improve from 0.42342\n",
            "60/60 - 0s - loss: 0.0603 - acc: 1.0000 - val_loss: 0.7663 - val_acc: 0.6000\n",
            "Epoch 298/300\n",
            "\n",
            "Epoch 00298: val_loss did not improve from 0.42342\n",
            "60/60 - 0s - loss: 0.1441 - acc: 0.9333 - val_loss: 0.6986 - val_acc: 0.6500\n",
            "Epoch 299/300\n",
            "\n",
            "Epoch 00299: val_loss did not improve from 0.42342\n",
            "60/60 - 0s - loss: 0.1181 - acc: 0.9667 - val_loss: 0.6377 - val_acc: 0.7500\n",
            "Epoch 300/300\n",
            "\n",
            "Epoch 00300: val_loss did not improve from 0.42342\n",
            "60/60 - 0s - loss: 0.0905 - acc: 0.9833 - val_loss: 0.5999 - val_acc: 0.7500\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "# Evaluate on test data\n",
            "40/40 [==============================] - 0s 2ms/sample - loss: 0.4918 - acc: 0.7750\n",
            "test loss, test acc: [0.4918227424786892, 0.775]\n",
            "[[0.49182274]\n",
            " [0.        ]\n",
            " [0.        ]\n",
            " [0.        ]\n",
            " [0.        ]\n",
            " [0.        ]\n",
            " [0.        ]\n",
            " [0.        ]\n",
            " [0.        ]\n",
            " [0.        ]]\n",
            "[[0.77499998]\n",
            " [0.        ]\n",
            " [0.        ]\n",
            " [0.        ]\n",
            " [0.        ]\n",
            " [0.        ]\n",
            " [0.        ]\n",
            " [0.        ]\n",
            " [0.        ]\n",
            " [0.        ]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kd6FTGs7vBI3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "df_accl_all = pd.DataFrame({'Class1vs2': acc_all[:, 0]})\n",
        "df_accl_all.to_csv (r'EEG_Deep/df_accl_allPatient_8_24_2560:4096.csv', index = None, header=True) #Don't forget to add '.csv' at the end of the path\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}