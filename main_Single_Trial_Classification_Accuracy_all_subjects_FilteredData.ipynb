{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "main_Single_Trial_Classification_Accuracy_all_subjects_FilteredData",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sagihaider/EEG_Deep/blob/master/main_Single_Trial_Classification_Accuracy_all_subjects_FilteredData.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X110PE9MjrlE",
        "colab_type": "code",
        "outputId": "ebffdfa8-47b0-4baa-89ed-37742a3968e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "!git clone https://github.com/sagihaider/EEG_Deep.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'EEG_Deep'...\n",
            "remote: Enumerating objects: 16, done.\u001b[K\n",
            "remote: Counting objects: 100% (16/16), done.\u001b[K\n",
            "remote: Compressing objects: 100% (16/16), done.\u001b[K\n",
            "remote: Total 263 (delta 8), reused 0 (delta 0), pack-reused 247\u001b[K\n",
            "Receiving objects: 100% (263/263), 1.07 GiB | 38.80 MiB/s, done.\n",
            "Resolving deltas: 100% (112/112), done.\n",
            "Checking out files: 100% (72/72), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ioLzMoiS60Qf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 63
        },
        "outputId": "e6ec433b-6133-43b5-aa79-b22bbcdccb22"
      },
      "source": [
        "import scipy.io as spio\n",
        "import numpy as np\n",
        "from importlib.machinery import SourceFileLoader\n",
        "\n",
        "# EEGNet-specific imports\n",
        "from EEG_Deep.EEGModels import EEGNet, ShallowConvNet, DeepConvNet\n",
        "from tensorflow.keras import utils as np_utils\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras import backend as K\n",
        "K.set_image_data_format('channels_first')\n",
        "\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# tools for plotting confusion matrices\n",
        "from matplotlib import pyplot as plt\n",
        "from scipy.signal import butter, lfilter"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mo80Jg_Pn5lP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Band-pass Filter\n",
        "def butter_bandpass_filter(data, lowcut, highcut, fs, order=4):\n",
        "    nyq = 0.5 * fs\n",
        "    low = lowcut / nyq\n",
        "    high = highcut / nyq\n",
        "    b, a = butter(order, [low, high], btype='band')\n",
        "    y = lfilter(b, a, data)\n",
        "    return y\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZjtnE_6RSpum",
        "colab_type": "code",
        "outputId": "61330347-94db-40a2-81d7-4daa4f3df5e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from numpy import zeros\n",
        "K.clear_session()\n",
        "cols = 2\n",
        "rows = 9\n",
        "acc_all = zeros([rows, cols])\n",
        "X_tr = np.empty([288, 22, 1875])\n",
        "X_ts = np.empty([288, 22, 1875])\n",
        "\n",
        "\n",
        "for x in range(1,10):\n",
        "  fName = 'EEG_Deep/Data2A/Data_A0' + str(x) + 'T.mat'  # Load Data\n",
        "  print(fName)\n",
        "  mat = spio.loadmat(fName)\n",
        "  r_X_tr = mat['cleanRawEEGData']\n",
        "  y_tr = mat['cleanClassLabels']\n",
        "  \n",
        "  print(np.shape(r_X_tr))\n",
        "  print(np.shape(y_tr))\n",
        "\n",
        "  for t in range(r_X_tr.shape[0]):\n",
        "    tril = r_X_tr[t,:,:]\n",
        "    # tril = tril.transpose()\n",
        "    tril_filtered = butter_bandpass_filter(tril, \n",
        "                                              lowcut=4, \n",
        "                                              highcut=40, \n",
        "                                              fs=250,\n",
        "                                              order=4)\n",
        "    # tril_filtered = tril_filtered.transpose()\n",
        "    X_tr[t,:,:] = tril_filtered \n",
        "    \n",
        "  # split data of each subject in training and validation\n",
        "  X_train      = X_tr[0:240,:,500:1250]\n",
        "  Y_train      = y_tr[0:240]\n",
        "  X_val       = X_tr[241:,:,500:1250]\n",
        "  Y_val       = y_tr[241:]\n",
        "\n",
        "  print(np.shape(X_train))\n",
        "  print(np.shape(Y_train))\n",
        "  print(np.shape(X_val))\n",
        "  print(np.shape(Y_val))\n",
        "  \n",
        "  # convert labels to one-hot encodings.\n",
        "  Y_train      = np_utils.to_categorical(Y_train-1)\n",
        "  Y_val       = np_utils.to_categorical(Y_val-1)\n",
        "\n",
        "  kernels, chans, samples = 1, 22, 750\n",
        "  # convert data to NCHW (trials, kernels, channels, samples) format. Data \n",
        "  # contains 22 channels and 500 time-points. Set the number of kernels to 1.\n",
        "  X_train      = X_train.reshape(X_train.shape[0], kernels, chans, samples)\n",
        "  X_val       = X_val.reshape(X_val.shape[0], kernels, chans, samples)\n",
        "   \n",
        "  print('X_train shape:', X_train.shape)\n",
        "  print(X_train.shape[0], 'train samples')\n",
        "  print(X_val.shape[0], 'val samples')\n",
        "\n",
        "  # Load test data         \n",
        "  fName = 'EEG_Deep/Data2A/Data_A0' + str(x) + 'E.mat'  # Load Data\n",
        "  print(fName)\n",
        "  mat = spio.loadmat(fName)\n",
        "  r_X_ts = mat['cleanRawEEGData']\n",
        "  y_ts = mat['cleanClassLabels']\n",
        "\n",
        "  print(np.shape(r_X_ts))\n",
        "  print(np.shape(y_ts))\n",
        "\n",
        "  for t in range(r_X_ts.shape[0]):\n",
        "    tril = r_X_ts[t,:,:]\n",
        "    # tril = tril.transpose()\n",
        "    tril_filtered = butter_bandpass_filter(tril, \n",
        "                                              lowcut=4, \n",
        "                                              highcut=40, \n",
        "                                              fs=250,\n",
        "                                              order=4)\n",
        "    # tril_filtered = tril_filtered.transpose()\n",
        "    X_ts[t,:,:] = tril_filtered \n",
        "\n",
        "  X_test      = X_ts[:,:,500:1250]\n",
        "  Y_test      = y_ts[:]\n",
        "  print(np.shape(X_test))\n",
        "  print(np.shape(Y_test))\n",
        "\n",
        "  #convert labels to one-hot encodings.\n",
        "  Y_test      = np_utils.to_categorical(Y_test-1)\n",
        "\n",
        "  # convert data to NCHW (trials, kernels, channels, samples) format. Data \n",
        "  # contains 22 channels and 500 time-points. Set the number of kernels to 1.\n",
        "  X_test      = X_test.reshape(X_test.shape[0], kernels, chans, samples)\n",
        "\n",
        "  print('X_train shape:', X_test.shape)\n",
        "  print(X_test.shape[0], 'train samples')\n",
        "\n",
        "  # configure the EEGNet-8,2,16 model with kernel length of 32 samples (other \n",
        "  # model configurations may do better, but this is a good starting point)\n",
        "  model = EEGNet(nb_classes = 4, Chans = 22, Samples = 750, \n",
        "                 dropoutRate = 0.5, kernLength = 25, F1 = 8, \n",
        "                 D = 2, F2 = 16, norm_rate = 0.25, dropoutType = 'Dropout')\n",
        "\n",
        "  # compile the model and set the optimizers\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='adam', \n",
        "                metrics = ['accuracy'])\n",
        "\n",
        "  # count number of parameters in the model\n",
        "  numParams    = model.count_params() \n",
        "\n",
        "  # set a valid path for your system to record model checkpoints\n",
        "  checkpointer = ModelCheckpoint(filepath='/tmp/checkpoint.h5', verbose=1,\n",
        "                                 save_best_only=True)\n",
        "  \n",
        "  # the syntax is {class_1:weight_1, class_2:weight_2,...}. Here just setting\n",
        "  # the weights all to be 1\n",
        "  class_weights = {0:1, 1:1, 2:1, 3:1}\n",
        "\n",
        "################################################################################\n",
        "# fit the model. Due to very small sample sizes this can get\n",
        "# pretty noisy run-to-run, but most runs should be comparable to xDAWN + \n",
        "# Riemannian geometry classification (below)\n",
        "################################################################################\n",
        "  history = model.fit(X_train, Y_train, batch_size = 16, epochs = 300, \n",
        "                      verbose = 2, validation_data=(X_val, Y_val),\n",
        "                      callbacks=[checkpointer], class_weight = class_weights)\n",
        "  \n",
        "  # Plot training & validation accuracy values\n",
        "  plt.plot(history.history['acc'])\n",
        "  plt.plot(history.history['val_acc'])\n",
        "  plt.title('Model accuracy')\n",
        "  plt.ylabel('Accuracy')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.legend(['Train', 'Test'], loc='upper left')\n",
        "  plt.show()\n",
        "  figName = 'Accuracy_A0' + str(x) + '.png'  \n",
        "  plt.savefig(figName)\n",
        "\n",
        "  print('\\n# Evaluate on test data')\n",
        "  results = model.evaluate(X_test, Y_test, batch_size=1)\n",
        "  print('test loss, test acc:', results)\n",
        "\n",
        "  acc_all[x - 1, 0] = results[0]\n",
        "  acc_all[x - 1, 1] = results[1]\n",
        "\n",
        "  from keras import backend as K \n",
        "  # Do some code, e.g. train and save model\n",
        "  K.clear_session()\n",
        "\n",
        "\n",
        "print(acc_all)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "EEG_Deep/Data2A/Data_A01T.mat\n",
            "(288, 22, 1875)\n",
            "(288, 1)\n",
            "(240, 22, 750)\n",
            "(240, 1)\n",
            "(47, 22, 750)\n",
            "(47, 1)\n",
            "X_train shape: (240, 1, 22, 750)\n",
            "240 train samples\n",
            "47 val samples\n",
            "EEG_Deep/Data2A/Data_A01E.mat\n",
            "(288, 22, 1875)\n",
            "(288, 1)\n",
            "(288, 22, 750)\n",
            "(288, 1)\n",
            "X_train shape: (288, 1, 22, 750)\n",
            "288 train samples\n",
            "Train on 240 samples, validate on 47 samples\n",
            "Epoch 1/300\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 1.38619, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 1s - loss: 1.3988 - acc: 0.2042 - val_loss: 1.3862 - val_acc: 0.2340\n",
            "Epoch 2/300\n",
            "\n",
            "Epoch 00002: val_loss improved from 1.38619 to 1.38569, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.3829 - acc: 0.2958 - val_loss: 1.3857 - val_acc: 0.2128\n",
            "Epoch 3/300\n",
            "\n",
            "Epoch 00003: val_loss improved from 1.38569 to 1.38468, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.3577 - acc: 0.3458 - val_loss: 1.3847 - val_acc: 0.2128\n",
            "Epoch 4/300\n",
            "\n",
            "Epoch 00004: val_loss improved from 1.38468 to 1.38186, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.3469 - acc: 0.3500 - val_loss: 1.3819 - val_acc: 0.3191\n",
            "Epoch 5/300\n",
            "\n",
            "Epoch 00005: val_loss improved from 1.38186 to 1.37671, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.3438 - acc: 0.3792 - val_loss: 1.3767 - val_acc: 0.4043\n",
            "Epoch 6/300\n",
            "\n",
            "Epoch 00006: val_loss improved from 1.37671 to 1.36787, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.3120 - acc: 0.4292 - val_loss: 1.3679 - val_acc: 0.4894\n",
            "Epoch 7/300\n",
            "\n",
            "Epoch 00007: val_loss improved from 1.36787 to 1.34936, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.2770 - acc: 0.4792 - val_loss: 1.3494 - val_acc: 0.4043\n",
            "Epoch 8/300\n",
            "\n",
            "Epoch 00008: val_loss improved from 1.34936 to 1.31572, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.2149 - acc: 0.5292 - val_loss: 1.3157 - val_acc: 0.4043\n",
            "Epoch 9/300\n",
            "\n",
            "Epoch 00009: val_loss improved from 1.31572 to 1.27127, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.1442 - acc: 0.5750 - val_loss: 1.2713 - val_acc: 0.3830\n",
            "Epoch 10/300\n",
            "\n",
            "Epoch 00010: val_loss improved from 1.27127 to 1.23167, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.0280 - acc: 0.6292 - val_loss: 1.2317 - val_acc: 0.3830\n",
            "Epoch 11/300\n",
            "\n",
            "Epoch 00011: val_loss improved from 1.23167 to 1.21145, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.9586 - acc: 0.6542 - val_loss: 1.2114 - val_acc: 0.3404\n",
            "Epoch 12/300\n",
            "\n",
            "Epoch 00012: val_loss improved from 1.21145 to 1.20036, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.9217 - acc: 0.6208 - val_loss: 1.2004 - val_acc: 0.3830\n",
            "Epoch 13/300\n",
            "\n",
            "Epoch 00013: val_loss improved from 1.20036 to 1.19521, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.8791 - acc: 0.6625 - val_loss: 1.1952 - val_acc: 0.3617\n",
            "Epoch 14/300\n",
            "\n",
            "Epoch 00014: val_loss improved from 1.19521 to 1.17825, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.8751 - acc: 0.6458 - val_loss: 1.1783 - val_acc: 0.3617\n",
            "Epoch 15/300\n",
            "\n",
            "Epoch 00015: val_loss improved from 1.17825 to 1.14012, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.8252 - acc: 0.6750 - val_loss: 1.1401 - val_acc: 0.4468\n",
            "Epoch 16/300\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 1.14012\n",
            "240/240 - 0s - loss: 0.8377 - acc: 0.6417 - val_loss: 1.1520 - val_acc: 0.4043\n",
            "Epoch 17/300\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 1.14012\n",
            "240/240 - 0s - loss: 0.8000 - acc: 0.6875 - val_loss: 1.1455 - val_acc: 0.4043\n",
            "Epoch 18/300\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 1.14012\n",
            "240/240 - 0s - loss: 0.8115 - acc: 0.6625 - val_loss: 1.1424 - val_acc: 0.4043\n",
            "Epoch 19/300\n",
            "\n",
            "Epoch 00019: val_loss improved from 1.14012 to 1.08075, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.7999 - acc: 0.6625 - val_loss: 1.0808 - val_acc: 0.4043\n",
            "Epoch 20/300\n",
            "\n",
            "Epoch 00020: val_loss improved from 1.08075 to 1.04718, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.7948 - acc: 0.6792 - val_loss: 1.0472 - val_acc: 0.4255\n",
            "Epoch 21/300\n",
            "\n",
            "Epoch 00021: val_loss improved from 1.04718 to 0.98282, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.7751 - acc: 0.7125 - val_loss: 0.9828 - val_acc: 0.4894\n",
            "Epoch 22/300\n",
            "\n",
            "Epoch 00022: val_loss improved from 0.98282 to 0.95520, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.7744 - acc: 0.6875 - val_loss: 0.9552 - val_acc: 0.4255\n",
            "Epoch 23/300\n",
            "\n",
            "Epoch 00023: val_loss improved from 0.95520 to 0.89714, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.7933 - acc: 0.7000 - val_loss: 0.8971 - val_acc: 0.5106\n",
            "Epoch 24/300\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.89714\n",
            "240/240 - 0s - loss: 0.7742 - acc: 0.6750 - val_loss: 0.9273 - val_acc: 0.4681\n",
            "Epoch 25/300\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.89714\n",
            "240/240 - 0s - loss: 0.7449 - acc: 0.6958 - val_loss: 0.9431 - val_acc: 0.4681\n",
            "Epoch 26/300\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.89714\n",
            "240/240 - 0s - loss: 0.7537 - acc: 0.6667 - val_loss: 0.9440 - val_acc: 0.4681\n",
            "Epoch 27/300\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.89714\n",
            "240/240 - 0s - loss: 0.7471 - acc: 0.6958 - val_loss: 0.9195 - val_acc: 0.4681\n",
            "Epoch 28/300\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.89714\n",
            "240/240 - 0s - loss: 0.7414 - acc: 0.7250 - val_loss: 0.9138 - val_acc: 0.5745\n",
            "Epoch 29/300\n",
            "\n",
            "Epoch 00029: val_loss improved from 0.89714 to 0.86749, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.7480 - acc: 0.7125 - val_loss: 0.8675 - val_acc: 0.5957\n",
            "Epoch 30/300\n",
            "\n",
            "Epoch 00030: val_loss improved from 0.86749 to 0.85666, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.7133 - acc: 0.7208 - val_loss: 0.8567 - val_acc: 0.6596\n",
            "Epoch 31/300\n",
            "\n",
            "Epoch 00031: val_loss improved from 0.85666 to 0.84874, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.7334 - acc: 0.7125 - val_loss: 0.8487 - val_acc: 0.5957\n",
            "Epoch 32/300\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.84874\n",
            "240/240 - 0s - loss: 0.7122 - acc: 0.6875 - val_loss: 0.8643 - val_acc: 0.6383\n",
            "Epoch 33/300\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.84874\n",
            "240/240 - 0s - loss: 0.7138 - acc: 0.6958 - val_loss: 0.8579 - val_acc: 0.6809\n",
            "Epoch 34/300\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.84874\n",
            "240/240 - 0s - loss: 0.7080 - acc: 0.7375 - val_loss: 0.8616 - val_acc: 0.6170\n",
            "Epoch 35/300\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.84874\n",
            "240/240 - 0s - loss: 0.6866 - acc: 0.7375 - val_loss: 0.8707 - val_acc: 0.6596\n",
            "Epoch 36/300\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.84874\n",
            "240/240 - 0s - loss: 0.7328 - acc: 0.6875 - val_loss: 0.8645 - val_acc: 0.6596\n",
            "Epoch 37/300\n",
            "\n",
            "Epoch 00037: val_loss improved from 0.84874 to 0.82024, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.6868 - acc: 0.6875 - val_loss: 0.8202 - val_acc: 0.6809\n",
            "Epoch 38/300\n",
            "\n",
            "Epoch 00038: val_loss improved from 0.82024 to 0.80380, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.6977 - acc: 0.7083 - val_loss: 0.8038 - val_acc: 0.6809\n",
            "Epoch 39/300\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.80380\n",
            "240/240 - 0s - loss: 0.7085 - acc: 0.7000 - val_loss: 0.8250 - val_acc: 0.6596\n",
            "Epoch 40/300\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.80380\n",
            "240/240 - 0s - loss: 0.6673 - acc: 0.7500 - val_loss: 0.8397 - val_acc: 0.6383\n",
            "Epoch 41/300\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.80380\n",
            "240/240 - 0s - loss: 0.6424 - acc: 0.7583 - val_loss: 0.8053 - val_acc: 0.6596\n",
            "Epoch 42/300\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.80380\n",
            "240/240 - 0s - loss: 0.7020 - acc: 0.7292 - val_loss: 0.8311 - val_acc: 0.6809\n",
            "Epoch 43/300\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.80380\n",
            "240/240 - 0s - loss: 0.6297 - acc: 0.7667 - val_loss: 0.8301 - val_acc: 0.6596\n",
            "Epoch 44/300\n",
            "\n",
            "Epoch 00044: val_loss improved from 0.80380 to 0.78768, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.7531 - acc: 0.6750 - val_loss: 0.7877 - val_acc: 0.6596\n",
            "Epoch 45/300\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.78768\n",
            "240/240 - 0s - loss: 0.6771 - acc: 0.7292 - val_loss: 0.8047 - val_acc: 0.6383\n",
            "Epoch 46/300\n",
            "\n",
            "Epoch 00046: val_loss improved from 0.78768 to 0.77993, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.6643 - acc: 0.7333 - val_loss: 0.7799 - val_acc: 0.6809\n",
            "Epoch 47/300\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.77993\n",
            "240/240 - 0s - loss: 0.6639 - acc: 0.7458 - val_loss: 0.7878 - val_acc: 0.6596\n",
            "Epoch 48/300\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.77993\n",
            "240/240 - 0s - loss: 0.6444 - acc: 0.7750 - val_loss: 0.7971 - val_acc: 0.6809\n",
            "Epoch 49/300\n",
            "\n",
            "Epoch 00049: val_loss improved from 0.77993 to 0.77136, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.6915 - acc: 0.7292 - val_loss: 0.7714 - val_acc: 0.6383\n",
            "Epoch 50/300\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.77136\n",
            "240/240 - 0s - loss: 0.6436 - acc: 0.7583 - val_loss: 0.7741 - val_acc: 0.6809\n",
            "Epoch 51/300\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.77136\n",
            "240/240 - 0s - loss: 0.6814 - acc: 0.7000 - val_loss: 0.8089 - val_acc: 0.6383\n",
            "Epoch 52/300\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.77136\n",
            "240/240 - 0s - loss: 0.6890 - acc: 0.7208 - val_loss: 0.7973 - val_acc: 0.6809\n",
            "Epoch 53/300\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.77136\n",
            "240/240 - 0s - loss: 0.6334 - acc: 0.7917 - val_loss: 0.8041 - val_acc: 0.7021\n",
            "Epoch 54/300\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.77136\n",
            "240/240 - 0s - loss: 0.6202 - acc: 0.7708 - val_loss: 0.7967 - val_acc: 0.6596\n",
            "Epoch 55/300\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.77136\n",
            "240/240 - 0s - loss: 0.6545 - acc: 0.7292 - val_loss: 0.8081 - val_acc: 0.7234\n",
            "Epoch 56/300\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.77136\n",
            "240/240 - 0s - loss: 0.6564 - acc: 0.7417 - val_loss: 0.7814 - val_acc: 0.6809\n",
            "Epoch 57/300\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.77136\n",
            "240/240 - 0s - loss: 0.6202 - acc: 0.7792 - val_loss: 0.7844 - val_acc: 0.7021\n",
            "Epoch 58/300\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.77136\n",
            "240/240 - 0s - loss: 0.6246 - acc: 0.7583 - val_loss: 0.7816 - val_acc: 0.7021\n",
            "Epoch 59/300\n",
            "\n",
            "Epoch 00059: val_loss improved from 0.77136 to 0.74305, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.6234 - acc: 0.7667 - val_loss: 0.7431 - val_acc: 0.7234\n",
            "Epoch 60/300\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.74305\n",
            "240/240 - 0s - loss: 0.6727 - acc: 0.7417 - val_loss: 0.7788 - val_acc: 0.6596\n",
            "Epoch 61/300\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.74305\n",
            "240/240 - 0s - loss: 0.6176 - acc: 0.7458 - val_loss: 0.7792 - val_acc: 0.7447\n",
            "Epoch 62/300\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.74305\n",
            "240/240 - 0s - loss: 0.6236 - acc: 0.7833 - val_loss: 0.7825 - val_acc: 0.7021\n",
            "Epoch 63/300\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.74305\n",
            "240/240 - 0s - loss: 0.6192 - acc: 0.7583 - val_loss: 0.7794 - val_acc: 0.6809\n",
            "Epoch 64/300\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.74305\n",
            "240/240 - 0s - loss: 0.6420 - acc: 0.7667 - val_loss: 0.8036 - val_acc: 0.6383\n",
            "Epoch 65/300\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.74305\n",
            "240/240 - 0s - loss: 0.5956 - acc: 0.7458 - val_loss: 0.8092 - val_acc: 0.7021\n",
            "Epoch 66/300\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.74305\n",
            "240/240 - 0s - loss: 0.6217 - acc: 0.7542 - val_loss: 0.7519 - val_acc: 0.7021\n",
            "Epoch 67/300\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.74305\n",
            "240/240 - 0s - loss: 0.6036 - acc: 0.7667 - val_loss: 0.7652 - val_acc: 0.7234\n",
            "Epoch 68/300\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.74305\n",
            "240/240 - 0s - loss: 0.6223 - acc: 0.7333 - val_loss: 0.7926 - val_acc: 0.6596\n",
            "Epoch 69/300\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.74305\n",
            "240/240 - 0s - loss: 0.6080 - acc: 0.7750 - val_loss: 0.8017 - val_acc: 0.7021\n",
            "Epoch 70/300\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.74305\n",
            "240/240 - 0s - loss: 0.6128 - acc: 0.7542 - val_loss: 0.7902 - val_acc: 0.7021\n",
            "Epoch 71/300\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.74305\n",
            "240/240 - 0s - loss: 0.6258 - acc: 0.7708 - val_loss: 0.7720 - val_acc: 0.6596\n",
            "Epoch 72/300\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.74305\n",
            "240/240 - 0s - loss: 0.6208 - acc: 0.7417 - val_loss: 0.7733 - val_acc: 0.7447\n",
            "Epoch 73/300\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.74305\n",
            "240/240 - 0s - loss: 0.6124 - acc: 0.7542 - val_loss: 0.7605 - val_acc: 0.7234\n",
            "Epoch 74/300\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.74305\n",
            "240/240 - 0s - loss: 0.6507 - acc: 0.7208 - val_loss: 0.7762 - val_acc: 0.7447\n",
            "Epoch 75/300\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.74305\n",
            "240/240 - 0s - loss: 0.5909 - acc: 0.7708 - val_loss: 0.7727 - val_acc: 0.7021\n",
            "Epoch 76/300\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.74305\n",
            "240/240 - 0s - loss: 0.5616 - acc: 0.8125 - val_loss: 0.7978 - val_acc: 0.7447\n",
            "Epoch 77/300\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.74305\n",
            "240/240 - 0s - loss: 0.6017 - acc: 0.7708 - val_loss: 0.8075 - val_acc: 0.6383\n",
            "Epoch 78/300\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.74305\n",
            "240/240 - 0s - loss: 0.6251 - acc: 0.7500 - val_loss: 0.7927 - val_acc: 0.6383\n",
            "Epoch 79/300\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.74305\n",
            "240/240 - 0s - loss: 0.5782 - acc: 0.7625 - val_loss: 0.8053 - val_acc: 0.6383\n",
            "Epoch 80/300\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.74305\n",
            "240/240 - 0s - loss: 0.5933 - acc: 0.7583 - val_loss: 0.8046 - val_acc: 0.6596\n",
            "Epoch 81/300\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.74305\n",
            "240/240 - 0s - loss: 0.6079 - acc: 0.7583 - val_loss: 0.7997 - val_acc: 0.7021\n",
            "Epoch 82/300\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.74305\n",
            "240/240 - 0s - loss: 0.6079 - acc: 0.7583 - val_loss: 0.7997 - val_acc: 0.6809\n",
            "Epoch 83/300\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.74305\n",
            "240/240 - 0s - loss: 0.5789 - acc: 0.7792 - val_loss: 0.7812 - val_acc: 0.7447\n",
            "Epoch 84/300\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.74305\n",
            "240/240 - 0s - loss: 0.5956 - acc: 0.7875 - val_loss: 0.8003 - val_acc: 0.6596\n",
            "Epoch 85/300\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.74305\n",
            "240/240 - 0s - loss: 0.5831 - acc: 0.7833 - val_loss: 0.7799 - val_acc: 0.6170\n",
            "Epoch 86/300\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.74305\n",
            "240/240 - 0s - loss: 0.6268 - acc: 0.7542 - val_loss: 0.7536 - val_acc: 0.6809\n",
            "Epoch 87/300\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.74305\n",
            "240/240 - 0s - loss: 0.5845 - acc: 0.7542 - val_loss: 0.7594 - val_acc: 0.6596\n",
            "Epoch 88/300\n",
            "\n",
            "Epoch 00088: val_loss improved from 0.74305 to 0.73564, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.6109 - acc: 0.7625 - val_loss: 0.7356 - val_acc: 0.7021\n",
            "Epoch 89/300\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.73564\n",
            "240/240 - 0s - loss: 0.5910 - acc: 0.7625 - val_loss: 0.7491 - val_acc: 0.7021\n",
            "Epoch 90/300\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.73564\n",
            "240/240 - 0s - loss: 0.6198 - acc: 0.7292 - val_loss: 0.7450 - val_acc: 0.6383\n",
            "Epoch 91/300\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.73564\n",
            "240/240 - 0s - loss: 0.5675 - acc: 0.7750 - val_loss: 0.7903 - val_acc: 0.6809\n",
            "Epoch 92/300\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.73564\n",
            "240/240 - 0s - loss: 0.5635 - acc: 0.7500 - val_loss: 0.7958 - val_acc: 0.6809\n",
            "Epoch 93/300\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.73564\n",
            "240/240 - 0s - loss: 0.5450 - acc: 0.7958 - val_loss: 0.7672 - val_acc: 0.7021\n",
            "Epoch 94/300\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.73564\n",
            "240/240 - 0s - loss: 0.5447 - acc: 0.8125 - val_loss: 0.7751 - val_acc: 0.6596\n",
            "Epoch 95/300\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.73564\n",
            "240/240 - 0s - loss: 0.6187 - acc: 0.7375 - val_loss: 0.7465 - val_acc: 0.7234\n",
            "Epoch 96/300\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.73564\n",
            "240/240 - 0s - loss: 0.5734 - acc: 0.8042 - val_loss: 0.7476 - val_acc: 0.6596\n",
            "Epoch 97/300\n",
            "\n",
            "Epoch 00097: val_loss improved from 0.73564 to 0.71490, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.5745 - acc: 0.8000 - val_loss: 0.7149 - val_acc: 0.7234\n",
            "Epoch 98/300\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.71490\n",
            "240/240 - 0s - loss: 0.5782 - acc: 0.7583 - val_loss: 0.7337 - val_acc: 0.7021\n",
            "Epoch 99/300\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.71490\n",
            "240/240 - 0s - loss: 0.6063 - acc: 0.7667 - val_loss: 0.7415 - val_acc: 0.7447\n",
            "Epoch 100/300\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.71490\n",
            "240/240 - 0s - loss: 0.5543 - acc: 0.7833 - val_loss: 0.7378 - val_acc: 0.7021\n",
            "Epoch 101/300\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.71490\n",
            "240/240 - 0s - loss: 0.5542 - acc: 0.7917 - val_loss: 0.7362 - val_acc: 0.7021\n",
            "Epoch 102/300\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.71490\n",
            "240/240 - 0s - loss: 0.5701 - acc: 0.7583 - val_loss: 0.7231 - val_acc: 0.7234\n",
            "Epoch 103/300\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.71490\n",
            "240/240 - 0s - loss: 0.6024 - acc: 0.7583 - val_loss: 0.7570 - val_acc: 0.7234\n",
            "Epoch 104/300\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.71490\n",
            "240/240 - 0s - loss: 0.5732 - acc: 0.7458 - val_loss: 0.7539 - val_acc: 0.7234\n",
            "Epoch 105/300\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 0.71490\n",
            "240/240 - 0s - loss: 0.5621 - acc: 0.7708 - val_loss: 0.7324 - val_acc: 0.7021\n",
            "Epoch 106/300\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 0.71490\n",
            "240/240 - 0s - loss: 0.5737 - acc: 0.7792 - val_loss: 0.7367 - val_acc: 0.6809\n",
            "Epoch 107/300\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.71490\n",
            "240/240 - 0s - loss: 0.5808 - acc: 0.7792 - val_loss: 0.7372 - val_acc: 0.7021\n",
            "Epoch 108/300\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 0.71490\n",
            "240/240 - 0s - loss: 0.5619 - acc: 0.7875 - val_loss: 0.7783 - val_acc: 0.6596\n",
            "Epoch 109/300\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.71490\n",
            "240/240 - 0s - loss: 0.6160 - acc: 0.7417 - val_loss: 0.7313 - val_acc: 0.7234\n",
            "Epoch 110/300\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.71490\n",
            "240/240 - 0s - loss: 0.5725 - acc: 0.7667 - val_loss: 0.7350 - val_acc: 0.6809\n",
            "Epoch 111/300\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 0.71490\n",
            "240/240 - 0s - loss: 0.5682 - acc: 0.7750 - val_loss: 0.7583 - val_acc: 0.7234\n",
            "Epoch 112/300\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 0.71490\n",
            "240/240 - 0s - loss: 0.5911 - acc: 0.7625 - val_loss: 0.7777 - val_acc: 0.7447\n",
            "Epoch 113/300\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.71490\n",
            "240/240 - 0s - loss: 0.5459 - acc: 0.8125 - val_loss: 0.7633 - val_acc: 0.6809\n",
            "Epoch 114/300\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.71490\n",
            "240/240 - 0s - loss: 0.5494 - acc: 0.7667 - val_loss: 0.7711 - val_acc: 0.7021\n",
            "Epoch 115/300\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.71490\n",
            "240/240 - 0s - loss: 0.5325 - acc: 0.8083 - val_loss: 0.7762 - val_acc: 0.6596\n",
            "Epoch 116/300\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.71490\n",
            "240/240 - 0s - loss: 0.5652 - acc: 0.7917 - val_loss: 0.7803 - val_acc: 0.7234\n",
            "Epoch 117/300\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.71490\n",
            "240/240 - 0s - loss: 0.5540 - acc: 0.7917 - val_loss: 0.7888 - val_acc: 0.7447\n",
            "Epoch 118/300\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.71490\n",
            "240/240 - 0s - loss: 0.5021 - acc: 0.8125 - val_loss: 0.7854 - val_acc: 0.6383\n",
            "Epoch 119/300\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 0.71490\n",
            "240/240 - 0s - loss: 0.5727 - acc: 0.8042 - val_loss: 0.7934 - val_acc: 0.7234\n",
            "Epoch 120/300\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.71490\n",
            "240/240 - 0s - loss: 0.5521 - acc: 0.7792 - val_loss: 0.7772 - val_acc: 0.6809\n",
            "Epoch 121/300\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.71490\n",
            "240/240 - 0s - loss: 0.5588 - acc: 0.7917 - val_loss: 0.7541 - val_acc: 0.7447\n",
            "Epoch 122/300\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.71490\n",
            "240/240 - 0s - loss: 0.5481 - acc: 0.8083 - val_loss: 0.7447 - val_acc: 0.6809\n",
            "Epoch 123/300\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 0.71490\n",
            "240/240 - 0s - loss: 0.5373 - acc: 0.7792 - val_loss: 0.7480 - val_acc: 0.7021\n",
            "Epoch 124/300\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 0.71490\n",
            "240/240 - 0s - loss: 0.5400 - acc: 0.7792 - val_loss: 0.7691 - val_acc: 0.7234\n",
            "Epoch 125/300\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 0.71490\n",
            "240/240 - 0s - loss: 0.5254 - acc: 0.8083 - val_loss: 0.7608 - val_acc: 0.6596\n",
            "Epoch 126/300\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 0.71490\n",
            "240/240 - 0s - loss: 0.5730 - acc: 0.7667 - val_loss: 0.7530 - val_acc: 0.6596\n",
            "Epoch 127/300\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 0.71490\n",
            "240/240 - 0s - loss: 0.5485 - acc: 0.8083 - val_loss: 0.7597 - val_acc: 0.7021\n",
            "Epoch 128/300\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 0.71490\n",
            "240/240 - 0s - loss: 0.5839 - acc: 0.7833 - val_loss: 0.7389 - val_acc: 0.7234\n",
            "Epoch 129/300\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.71490\n",
            "240/240 - 0s - loss: 0.5679 - acc: 0.7750 - val_loss: 0.7442 - val_acc: 0.7021\n",
            "Epoch 130/300\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 0.71490\n",
            "240/240 - 0s - loss: 0.5430 - acc: 0.8042 - val_loss: 0.7350 - val_acc: 0.7234\n",
            "Epoch 131/300\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 0.71490\n",
            "240/240 - 0s - loss: 0.5119 - acc: 0.7958 - val_loss: 0.7304 - val_acc: 0.7021\n",
            "Epoch 132/300\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 0.71490\n",
            "240/240 - 0s - loss: 0.5271 - acc: 0.8250 - val_loss: 0.7178 - val_acc: 0.7021\n",
            "Epoch 133/300\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.71490\n",
            "240/240 - 0s - loss: 0.5424 - acc: 0.7792 - val_loss: 0.7449 - val_acc: 0.7021\n",
            "Epoch 134/300\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.71490\n",
            "240/240 - 0s - loss: 0.5504 - acc: 0.7750 - val_loss: 0.7481 - val_acc: 0.6809\n",
            "Epoch 135/300\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 0.71490\n",
            "240/240 - 0s - loss: 0.5427 - acc: 0.8167 - val_loss: 0.7512 - val_acc: 0.6809\n",
            "Epoch 136/300\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.71490\n",
            "240/240 - 0s - loss: 0.5398 - acc: 0.7833 - val_loss: 0.7290 - val_acc: 0.7447\n",
            "Epoch 137/300\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 0.71490\n",
            "240/240 - 0s - loss: 0.4940 - acc: 0.8417 - val_loss: 0.7295 - val_acc: 0.7021\n",
            "Epoch 138/300\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.71490\n",
            "240/240 - 0s - loss: 0.5390 - acc: 0.7583 - val_loss: 0.7341 - val_acc: 0.7447\n",
            "Epoch 139/300\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.71490\n",
            "240/240 - 0s - loss: 0.5148 - acc: 0.7792 - val_loss: 0.7432 - val_acc: 0.7021\n",
            "Epoch 140/300\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 0.71490\n",
            "240/240 - 0s - loss: 0.5524 - acc: 0.7625 - val_loss: 0.7645 - val_acc: 0.6383\n",
            "Epoch 141/300\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.71490\n",
            "240/240 - 0s - loss: 0.5680 - acc: 0.7625 - val_loss: 0.7442 - val_acc: 0.7021\n",
            "Epoch 142/300\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.71490\n",
            "240/240 - 0s - loss: 0.5357 - acc: 0.7708 - val_loss: 0.7240 - val_acc: 0.7021\n",
            "Epoch 143/300\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 0.71490\n",
            "240/240 - 0s - loss: 0.5204 - acc: 0.8042 - val_loss: 0.7379 - val_acc: 0.7234\n",
            "Epoch 144/300\n",
            "\n",
            "Epoch 00144: val_loss improved from 0.71490 to 0.71026, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.5492 - acc: 0.7708 - val_loss: 0.7103 - val_acc: 0.7021\n",
            "Epoch 145/300\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.71026\n",
            "240/240 - 0s - loss: 0.5264 - acc: 0.7875 - val_loss: 0.7249 - val_acc: 0.7021\n",
            "Epoch 146/300\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.71026\n",
            "240/240 - 0s - loss: 0.5436 - acc: 0.7917 - val_loss: 0.7163 - val_acc: 0.7234\n",
            "Epoch 147/300\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.71026\n",
            "240/240 - 0s - loss: 0.5332 - acc: 0.7833 - val_loss: 0.7438 - val_acc: 0.7447\n",
            "Epoch 148/300\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 0.71026\n",
            "240/240 - 0s - loss: 0.5000 - acc: 0.7875 - val_loss: 0.7374 - val_acc: 0.7660\n",
            "Epoch 149/300\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.71026\n",
            "240/240 - 0s - loss: 0.5307 - acc: 0.8000 - val_loss: 0.7190 - val_acc: 0.7447\n",
            "Epoch 150/300\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.71026\n",
            "240/240 - 0s - loss: 0.5116 - acc: 0.8167 - val_loss: 0.7566 - val_acc: 0.7021\n",
            "Epoch 151/300\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.71026\n",
            "240/240 - 0s - loss: 0.5545 - acc: 0.7875 - val_loss: 0.7659 - val_acc: 0.7660\n",
            "Epoch 152/300\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.71026\n",
            "240/240 - 0s - loss: 0.5164 - acc: 0.8125 - val_loss: 0.7736 - val_acc: 0.6809\n",
            "Epoch 153/300\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 0.71026\n",
            "240/240 - 0s - loss: 0.5524 - acc: 0.7708 - val_loss: 0.7829 - val_acc: 0.6809\n",
            "Epoch 154/300\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.71026\n",
            "240/240 - 0s - loss: 0.5284 - acc: 0.7958 - val_loss: 0.7361 - val_acc: 0.7021\n",
            "Epoch 155/300\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.71026\n",
            "240/240 - 0s - loss: 0.5193 - acc: 0.7917 - val_loss: 0.7203 - val_acc: 0.7234\n",
            "Epoch 156/300\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.71026\n",
            "240/240 - 0s - loss: 0.5288 - acc: 0.7667 - val_loss: 0.7510 - val_acc: 0.6596\n",
            "Epoch 157/300\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.71026\n",
            "240/240 - 0s - loss: 0.4836 - acc: 0.8083 - val_loss: 0.7169 - val_acc: 0.7447\n",
            "Epoch 158/300\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.71026\n",
            "240/240 - 0s - loss: 0.5017 - acc: 0.8083 - val_loss: 0.7467 - val_acc: 0.6596\n",
            "Epoch 159/300\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.71026\n",
            "240/240 - 0s - loss: 0.4980 - acc: 0.8000 - val_loss: 0.7346 - val_acc: 0.6809\n",
            "Epoch 160/300\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.71026\n",
            "240/240 - 0s - loss: 0.4873 - acc: 0.8208 - val_loss: 0.7127 - val_acc: 0.7447\n",
            "Epoch 161/300\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.71026\n",
            "240/240 - 0s - loss: 0.5247 - acc: 0.7958 - val_loss: 0.7286 - val_acc: 0.7021\n",
            "Epoch 162/300\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.71026\n",
            "240/240 - 0s - loss: 0.5327 - acc: 0.7708 - val_loss: 0.7314 - val_acc: 0.7021\n",
            "Epoch 163/300\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.71026\n",
            "240/240 - 0s - loss: 0.5391 - acc: 0.7958 - val_loss: 0.7232 - val_acc: 0.7021\n",
            "Epoch 164/300\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.71026\n",
            "240/240 - 0s - loss: 0.4994 - acc: 0.8208 - val_loss: 0.7145 - val_acc: 0.7234\n",
            "Epoch 165/300\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.71026\n",
            "240/240 - 0s - loss: 0.5170 - acc: 0.7583 - val_loss: 0.7310 - val_acc: 0.7234\n",
            "Epoch 166/300\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 0.71026\n",
            "240/240 - 0s - loss: 0.5295 - acc: 0.7750 - val_loss: 0.7246 - val_acc: 0.7021\n",
            "Epoch 167/300\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.71026\n",
            "240/240 - 0s - loss: 0.5209 - acc: 0.7750 - val_loss: 0.7210 - val_acc: 0.7234\n",
            "Epoch 168/300\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.71026\n",
            "240/240 - 0s - loss: 0.4692 - acc: 0.8292 - val_loss: 0.7276 - val_acc: 0.6596\n",
            "Epoch 169/300\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 0.71026\n",
            "240/240 - 0s - loss: 0.5346 - acc: 0.8042 - val_loss: 0.7129 - val_acc: 0.7447\n",
            "Epoch 170/300\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.71026\n",
            "240/240 - 0s - loss: 0.5186 - acc: 0.7917 - val_loss: 0.7396 - val_acc: 0.6809\n",
            "Epoch 171/300\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.71026\n",
            "240/240 - 0s - loss: 0.5299 - acc: 0.7875 - val_loss: 0.7259 - val_acc: 0.7021\n",
            "Epoch 172/300\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.71026\n",
            "240/240 - 0s - loss: 0.5072 - acc: 0.8333 - val_loss: 0.7458 - val_acc: 0.7021\n",
            "Epoch 173/300\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.71026\n",
            "240/240 - 0s - loss: 0.5281 - acc: 0.7917 - val_loss: 0.7331 - val_acc: 0.7234\n",
            "Epoch 174/300\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.71026\n",
            "240/240 - 0s - loss: 0.4993 - acc: 0.7875 - val_loss: 0.7234 - val_acc: 0.6383\n",
            "Epoch 175/300\n",
            "\n",
            "Epoch 00175: val_loss improved from 0.71026 to 0.70178, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.5024 - acc: 0.8083 - val_loss: 0.7018 - val_acc: 0.7234\n",
            "Epoch 176/300\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 0.70178\n",
            "240/240 - 0s - loss: 0.4722 - acc: 0.8375 - val_loss: 0.7099 - val_acc: 0.7021\n",
            "Epoch 177/300\n",
            "\n",
            "Epoch 00177: val_loss improved from 0.70178 to 0.69841, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.4936 - acc: 0.7792 - val_loss: 0.6984 - val_acc: 0.7660\n",
            "Epoch 178/300\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.69841\n",
            "240/240 - 0s - loss: 0.4794 - acc: 0.8167 - val_loss: 0.7116 - val_acc: 0.7021\n",
            "Epoch 179/300\n",
            "\n",
            "Epoch 00179: val_loss improved from 0.69841 to 0.67515, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.5309 - acc: 0.7875 - val_loss: 0.6751 - val_acc: 0.7447\n",
            "Epoch 180/300\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.67515\n",
            "240/240 - 0s - loss: 0.5270 - acc: 0.8125 - val_loss: 0.7215 - val_acc: 0.6596\n",
            "Epoch 181/300\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.67515\n",
            "240/240 - 0s - loss: 0.4819 - acc: 0.8333 - val_loss: 0.7132 - val_acc: 0.7872\n",
            "Epoch 182/300\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.67515\n",
            "240/240 - 0s - loss: 0.5008 - acc: 0.8042 - val_loss: 0.7380 - val_acc: 0.7234\n",
            "Epoch 183/300\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.67515\n",
            "240/240 - 0s - loss: 0.4652 - acc: 0.8208 - val_loss: 0.7398 - val_acc: 0.7447\n",
            "Epoch 184/300\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 0.67515\n",
            "240/240 - 0s - loss: 0.5377 - acc: 0.8042 - val_loss: 0.7065 - val_acc: 0.6809\n",
            "Epoch 185/300\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.67515\n",
            "240/240 - 0s - loss: 0.5423 - acc: 0.7750 - val_loss: 0.7058 - val_acc: 0.7447\n",
            "Epoch 186/300\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.67515\n",
            "240/240 - 0s - loss: 0.4840 - acc: 0.8458 - val_loss: 0.7072 - val_acc: 0.7021\n",
            "Epoch 187/300\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 0.67515\n",
            "240/240 - 0s - loss: 0.5006 - acc: 0.7958 - val_loss: 0.7408 - val_acc: 0.7447\n",
            "Epoch 188/300\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.67515\n",
            "240/240 - 0s - loss: 0.4633 - acc: 0.8292 - val_loss: 0.7414 - val_acc: 0.7234\n",
            "Epoch 189/300\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.67515\n",
            "240/240 - 0s - loss: 0.4851 - acc: 0.8333 - val_loss: 0.7431 - val_acc: 0.7234\n",
            "Epoch 190/300\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.67515\n",
            "240/240 - 0s - loss: 0.4954 - acc: 0.8083 - val_loss: 0.7084 - val_acc: 0.7447\n",
            "Epoch 191/300\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.67515\n",
            "240/240 - 0s - loss: 0.4731 - acc: 0.8042 - val_loss: 0.7070 - val_acc: 0.7234\n",
            "Epoch 192/300\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 0.67515\n",
            "240/240 - 0s - loss: 0.4753 - acc: 0.8250 - val_loss: 0.6969 - val_acc: 0.7447\n",
            "Epoch 193/300\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.67515\n",
            "240/240 - 0s - loss: 0.4689 - acc: 0.8125 - val_loss: 0.7380 - val_acc: 0.6809\n",
            "Epoch 194/300\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.67515\n",
            "240/240 - 0s - loss: 0.4850 - acc: 0.8208 - val_loss: 0.7173 - val_acc: 0.7234\n",
            "Epoch 195/300\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.67515\n",
            "240/240 - 0s - loss: 0.4813 - acc: 0.8042 - val_loss: 0.7021 - val_acc: 0.7234\n",
            "Epoch 196/300\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.67515\n",
            "240/240 - 0s - loss: 0.4492 - acc: 0.8250 - val_loss: 0.6869 - val_acc: 0.7021\n",
            "Epoch 197/300\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.67515\n",
            "240/240 - 0s - loss: 0.4685 - acc: 0.8458 - val_loss: 0.6848 - val_acc: 0.7447\n",
            "Epoch 198/300\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.67515\n",
            "240/240 - 0s - loss: 0.4898 - acc: 0.8208 - val_loss: 0.7128 - val_acc: 0.7021\n",
            "Epoch 199/300\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.67515\n",
            "240/240 - 0s - loss: 0.4771 - acc: 0.8250 - val_loss: 0.7038 - val_acc: 0.7447\n",
            "Epoch 200/300\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.67515\n",
            "240/240 - 0s - loss: 0.4321 - acc: 0.8667 - val_loss: 0.7437 - val_acc: 0.6596\n",
            "Epoch 201/300\n",
            "\n",
            "Epoch 00201: val_loss did not improve from 0.67515\n",
            "240/240 - 0s - loss: 0.4928 - acc: 0.7958 - val_loss: 0.7176 - val_acc: 0.7234\n",
            "Epoch 202/300\n",
            "\n",
            "Epoch 00202: val_loss did not improve from 0.67515\n",
            "240/240 - 0s - loss: 0.4656 - acc: 0.8292 - val_loss: 0.7162 - val_acc: 0.6596\n",
            "Epoch 203/300\n",
            "\n",
            "Epoch 00203: val_loss did not improve from 0.67515\n",
            "240/240 - 0s - loss: 0.5203 - acc: 0.7875 - val_loss: 0.7186 - val_acc: 0.7021\n",
            "Epoch 204/300\n",
            "\n",
            "Epoch 00204: val_loss did not improve from 0.67515\n",
            "240/240 - 0s - loss: 0.4443 - acc: 0.8500 - val_loss: 0.7381 - val_acc: 0.6596\n",
            "Epoch 205/300\n",
            "\n",
            "Epoch 00205: val_loss did not improve from 0.67515\n",
            "240/240 - 0s - loss: 0.5014 - acc: 0.7958 - val_loss: 0.7155 - val_acc: 0.7021\n",
            "Epoch 206/300\n",
            "\n",
            "Epoch 00206: val_loss did not improve from 0.67515\n",
            "240/240 - 0s - loss: 0.4898 - acc: 0.7958 - val_loss: 0.7046 - val_acc: 0.7447\n",
            "Epoch 207/300\n",
            "\n",
            "Epoch 00207: val_loss did not improve from 0.67515\n",
            "240/240 - 0s - loss: 0.5077 - acc: 0.7917 - val_loss: 0.7293 - val_acc: 0.6809\n",
            "Epoch 208/300\n",
            "\n",
            "Epoch 00208: val_loss improved from 0.67515 to 0.65529, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.4800 - acc: 0.8083 - val_loss: 0.6553 - val_acc: 0.7234\n",
            "Epoch 209/300\n",
            "\n",
            "Epoch 00209: val_loss did not improve from 0.65529\n",
            "240/240 - 0s - loss: 0.4897 - acc: 0.7917 - val_loss: 0.6882 - val_acc: 0.7021\n",
            "Epoch 210/300\n",
            "\n",
            "Epoch 00210: val_loss did not improve from 0.65529\n",
            "240/240 - 0s - loss: 0.4763 - acc: 0.8125 - val_loss: 0.6834 - val_acc: 0.7447\n",
            "Epoch 211/300\n",
            "\n",
            "Epoch 00211: val_loss did not improve from 0.65529\n",
            "240/240 - 0s - loss: 0.4512 - acc: 0.8333 - val_loss: 0.7536 - val_acc: 0.6809\n",
            "Epoch 212/300\n",
            "\n",
            "Epoch 00212: val_loss did not improve from 0.65529\n",
            "240/240 - 0s - loss: 0.4729 - acc: 0.8125 - val_loss: 0.7635 - val_acc: 0.6809\n",
            "Epoch 213/300\n",
            "\n",
            "Epoch 00213: val_loss did not improve from 0.65529\n",
            "240/240 - 0s - loss: 0.4700 - acc: 0.8167 - val_loss: 0.7319 - val_acc: 0.7234\n",
            "Epoch 214/300\n",
            "\n",
            "Epoch 00214: val_loss did not improve from 0.65529\n",
            "240/240 - 0s - loss: 0.4890 - acc: 0.8167 - val_loss: 0.7494 - val_acc: 0.7447\n",
            "Epoch 215/300\n",
            "\n",
            "Epoch 00215: val_loss did not improve from 0.65529\n",
            "240/240 - 0s - loss: 0.4945 - acc: 0.8125 - val_loss: 0.7302 - val_acc: 0.7234\n",
            "Epoch 216/300\n",
            "\n",
            "Epoch 00216: val_loss did not improve from 0.65529\n",
            "240/240 - 0s - loss: 0.4248 - acc: 0.8542 - val_loss: 0.7721 - val_acc: 0.7447\n",
            "Epoch 217/300\n",
            "\n",
            "Epoch 00217: val_loss did not improve from 0.65529\n",
            "240/240 - 0s - loss: 0.4978 - acc: 0.8125 - val_loss: 0.7562 - val_acc: 0.7234\n",
            "Epoch 218/300\n",
            "\n",
            "Epoch 00218: val_loss did not improve from 0.65529\n",
            "240/240 - 0s - loss: 0.4233 - acc: 0.8708 - val_loss: 0.7032 - val_acc: 0.7234\n",
            "Epoch 219/300\n",
            "\n",
            "Epoch 00219: val_loss did not improve from 0.65529\n",
            "240/240 - 0s - loss: 0.4904 - acc: 0.8000 - val_loss: 0.7289 - val_acc: 0.6809\n",
            "Epoch 220/300\n",
            "\n",
            "Epoch 00220: val_loss did not improve from 0.65529\n",
            "240/240 - 0s - loss: 0.4499 - acc: 0.8292 - val_loss: 0.7111 - val_acc: 0.7660\n",
            "Epoch 221/300\n",
            "\n",
            "Epoch 00221: val_loss did not improve from 0.65529\n",
            "240/240 - 0s - loss: 0.4444 - acc: 0.8333 - val_loss: 0.7021 - val_acc: 0.7234\n",
            "Epoch 222/300\n",
            "\n",
            "Epoch 00222: val_loss did not improve from 0.65529\n",
            "240/240 - 0s - loss: 0.4620 - acc: 0.8292 - val_loss: 0.7062 - val_acc: 0.6809\n",
            "Epoch 223/300\n",
            "\n",
            "Epoch 00223: val_loss did not improve from 0.65529\n",
            "240/240 - 0s - loss: 0.4871 - acc: 0.7917 - val_loss: 0.6982 - val_acc: 0.7660\n",
            "Epoch 224/300\n",
            "\n",
            "Epoch 00224: val_loss did not improve from 0.65529\n",
            "240/240 - 0s - loss: 0.4165 - acc: 0.8542 - val_loss: 0.7375 - val_acc: 0.7021\n",
            "Epoch 225/300\n",
            "\n",
            "Epoch 00225: val_loss did not improve from 0.65529\n",
            "240/240 - 0s - loss: 0.4648 - acc: 0.8208 - val_loss: 0.6996 - val_acc: 0.7234\n",
            "Epoch 226/300\n",
            "\n",
            "Epoch 00226: val_loss did not improve from 0.65529\n",
            "240/240 - 0s - loss: 0.4768 - acc: 0.7958 - val_loss: 0.6887 - val_acc: 0.7447\n",
            "Epoch 227/300\n",
            "\n",
            "Epoch 00227: val_loss did not improve from 0.65529\n",
            "240/240 - 0s - loss: 0.4865 - acc: 0.8083 - val_loss: 0.7334 - val_acc: 0.6596\n",
            "Epoch 228/300\n",
            "\n",
            "Epoch 00228: val_loss did not improve from 0.65529\n",
            "240/240 - 0s - loss: 0.4626 - acc: 0.8125 - val_loss: 0.7232 - val_acc: 0.7021\n",
            "Epoch 229/300\n",
            "\n",
            "Epoch 00229: val_loss did not improve from 0.65529\n",
            "240/240 - 0s - loss: 0.4501 - acc: 0.8208 - val_loss: 0.6784 - val_acc: 0.7660\n",
            "Epoch 230/300\n",
            "\n",
            "Epoch 00230: val_loss did not improve from 0.65529\n",
            "240/240 - 0s - loss: 0.5014 - acc: 0.7667 - val_loss: 0.6883 - val_acc: 0.7021\n",
            "Epoch 231/300\n",
            "\n",
            "Epoch 00231: val_loss did not improve from 0.65529\n",
            "240/240 - 0s - loss: 0.4963 - acc: 0.7792 - val_loss: 0.7477 - val_acc: 0.6809\n",
            "Epoch 232/300\n",
            "\n",
            "Epoch 00232: val_loss did not improve from 0.65529\n",
            "240/240 - 0s - loss: 0.4329 - acc: 0.8333 - val_loss: 0.6950 - val_acc: 0.7021\n",
            "Epoch 233/300\n",
            "\n",
            "Epoch 00233: val_loss did not improve from 0.65529\n",
            "240/240 - 0s - loss: 0.4257 - acc: 0.8583 - val_loss: 0.6929 - val_acc: 0.7660\n",
            "Epoch 234/300\n",
            "\n",
            "Epoch 00234: val_loss did not improve from 0.65529\n",
            "240/240 - 0s - loss: 0.3918 - acc: 0.8750 - val_loss: 0.7126 - val_acc: 0.7660\n",
            "Epoch 235/300\n",
            "\n",
            "Epoch 00235: val_loss did not improve from 0.65529\n",
            "240/240 - 0s - loss: 0.4831 - acc: 0.8250 - val_loss: 0.7400 - val_acc: 0.7447\n",
            "Epoch 236/300\n",
            "\n",
            "Epoch 00236: val_loss did not improve from 0.65529\n",
            "240/240 - 0s - loss: 0.4618 - acc: 0.8125 - val_loss: 0.6935 - val_acc: 0.7234\n",
            "Epoch 237/300\n",
            "\n",
            "Epoch 00237: val_loss did not improve from 0.65529\n",
            "240/240 - 0s - loss: 0.4562 - acc: 0.8083 - val_loss: 0.7002 - val_acc: 0.6809\n",
            "Epoch 238/300\n",
            "\n",
            "Epoch 00238: val_loss did not improve from 0.65529\n",
            "240/240 - 0s - loss: 0.4557 - acc: 0.8292 - val_loss: 0.7028 - val_acc: 0.7660\n",
            "Epoch 239/300\n",
            "\n",
            "Epoch 00239: val_loss improved from 0.65529 to 0.64813, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.4936 - acc: 0.7958 - val_loss: 0.6481 - val_acc: 0.7660\n",
            "Epoch 240/300\n",
            "\n",
            "Epoch 00240: val_loss did not improve from 0.64813\n",
            "240/240 - 0s - loss: 0.4538 - acc: 0.8500 - val_loss: 0.7080 - val_acc: 0.7660\n",
            "Epoch 241/300\n",
            "\n",
            "Epoch 00241: val_loss did not improve from 0.64813\n",
            "240/240 - 0s - loss: 0.4342 - acc: 0.8292 - val_loss: 0.7415 - val_acc: 0.7021\n",
            "Epoch 242/300\n",
            "\n",
            "Epoch 00242: val_loss did not improve from 0.64813\n",
            "240/240 - 0s - loss: 0.4179 - acc: 0.8667 - val_loss: 0.7462 - val_acc: 0.6170\n",
            "Epoch 243/300\n",
            "\n",
            "Epoch 00243: val_loss did not improve from 0.64813\n",
            "240/240 - 0s - loss: 0.4770 - acc: 0.8083 - val_loss: 0.7155 - val_acc: 0.6809\n",
            "Epoch 244/300\n",
            "\n",
            "Epoch 00244: val_loss did not improve from 0.64813\n",
            "240/240 - 0s - loss: 0.4770 - acc: 0.8167 - val_loss: 0.6952 - val_acc: 0.7660\n",
            "Epoch 245/300\n",
            "\n",
            "Epoch 00245: val_loss did not improve from 0.64813\n",
            "240/240 - 0s - loss: 0.4332 - acc: 0.8125 - val_loss: 0.7080 - val_acc: 0.7447\n",
            "Epoch 246/300\n",
            "\n",
            "Epoch 00246: val_loss did not improve from 0.64813\n",
            "240/240 - 0s - loss: 0.4317 - acc: 0.8292 - val_loss: 0.6969 - val_acc: 0.7447\n",
            "Epoch 247/300\n",
            "\n",
            "Epoch 00247: val_loss did not improve from 0.64813\n",
            "240/240 - 0s - loss: 0.4326 - acc: 0.8208 - val_loss: 0.6881 - val_acc: 0.7447\n",
            "Epoch 248/300\n",
            "\n",
            "Epoch 00248: val_loss did not improve from 0.64813\n",
            "240/240 - 0s - loss: 0.5011 - acc: 0.7750 - val_loss: 0.6731 - val_acc: 0.7021\n",
            "Epoch 249/300\n",
            "\n",
            "Epoch 00249: val_loss did not improve from 0.64813\n",
            "240/240 - 0s - loss: 0.4545 - acc: 0.8250 - val_loss: 0.6693 - val_acc: 0.7660\n",
            "Epoch 250/300\n",
            "\n",
            "Epoch 00250: val_loss did not improve from 0.64813\n",
            "240/240 - 0s - loss: 0.4291 - acc: 0.8458 - val_loss: 0.6840 - val_acc: 0.7234\n",
            "Epoch 251/300\n",
            "\n",
            "Epoch 00251: val_loss did not improve from 0.64813\n",
            "240/240 - 0s - loss: 0.4405 - acc: 0.8208 - val_loss: 0.7042 - val_acc: 0.7447\n",
            "Epoch 252/300\n",
            "\n",
            "Epoch 00252: val_loss did not improve from 0.64813\n",
            "240/240 - 0s - loss: 0.4343 - acc: 0.8333 - val_loss: 0.6791 - val_acc: 0.7234\n",
            "Epoch 253/300\n",
            "\n",
            "Epoch 00253: val_loss did not improve from 0.64813\n",
            "240/240 - 0s - loss: 0.4834 - acc: 0.8208 - val_loss: 0.7137 - val_acc: 0.6809\n",
            "Epoch 254/300\n",
            "\n",
            "Epoch 00254: val_loss did not improve from 0.64813\n",
            "240/240 - 0s - loss: 0.4422 - acc: 0.8208 - val_loss: 0.6740 - val_acc: 0.7234\n",
            "Epoch 255/300\n",
            "\n",
            "Epoch 00255: val_loss did not improve from 0.64813\n",
            "240/240 - 0s - loss: 0.3888 - acc: 0.8792 - val_loss: 0.6614 - val_acc: 0.7447\n",
            "Epoch 256/300\n",
            "\n",
            "Epoch 00256: val_loss did not improve from 0.64813\n",
            "240/240 - 0s - loss: 0.4264 - acc: 0.8542 - val_loss: 0.6640 - val_acc: 0.7234\n",
            "Epoch 257/300\n",
            "\n",
            "Epoch 00257: val_loss improved from 0.64813 to 0.63020, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.4607 - acc: 0.8333 - val_loss: 0.6302 - val_acc: 0.8085\n",
            "Epoch 258/300\n",
            "\n",
            "Epoch 00258: val_loss improved from 0.63020 to 0.61913, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.4494 - acc: 0.8417 - val_loss: 0.6191 - val_acc: 0.7447\n",
            "Epoch 259/300\n",
            "\n",
            "Epoch 00259: val_loss did not improve from 0.61913\n",
            "240/240 - 0s - loss: 0.4219 - acc: 0.8333 - val_loss: 0.6799 - val_acc: 0.6596\n",
            "Epoch 260/300\n",
            "\n",
            "Epoch 00260: val_loss did not improve from 0.61913\n",
            "240/240 - 0s - loss: 0.4097 - acc: 0.8458 - val_loss: 0.6581 - val_acc: 0.7234\n",
            "Epoch 261/300\n",
            "\n",
            "Epoch 00261: val_loss did not improve from 0.61913\n",
            "240/240 - 0s - loss: 0.4278 - acc: 0.8583 - val_loss: 0.6501 - val_acc: 0.7234\n",
            "Epoch 262/300\n",
            "\n",
            "Epoch 00262: val_loss did not improve from 0.61913\n",
            "240/240 - 0s - loss: 0.4501 - acc: 0.8458 - val_loss: 0.6586 - val_acc: 0.7447\n",
            "Epoch 263/300\n",
            "\n",
            "Epoch 00263: val_loss did not improve from 0.61913\n",
            "240/240 - 0s - loss: 0.4210 - acc: 0.8250 - val_loss: 0.6833 - val_acc: 0.7660\n",
            "Epoch 264/300\n",
            "\n",
            "Epoch 00264: val_loss did not improve from 0.61913\n",
            "240/240 - 0s - loss: 0.4177 - acc: 0.8500 - val_loss: 0.7081 - val_acc: 0.6809\n",
            "Epoch 265/300\n",
            "\n",
            "Epoch 00265: val_loss did not improve from 0.61913\n",
            "240/240 - 0s - loss: 0.4667 - acc: 0.8167 - val_loss: 0.7094 - val_acc: 0.7234\n",
            "Epoch 266/300\n",
            "\n",
            "Epoch 00266: val_loss did not improve from 0.61913\n",
            "240/240 - 0s - loss: 0.3969 - acc: 0.8583 - val_loss: 0.6896 - val_acc: 0.7447\n",
            "Epoch 267/300\n",
            "\n",
            "Epoch 00267: val_loss did not improve from 0.61913\n",
            "240/240 - 0s - loss: 0.4366 - acc: 0.8292 - val_loss: 0.6557 - val_acc: 0.8298\n",
            "Epoch 268/300\n",
            "\n",
            "Epoch 00268: val_loss did not improve from 0.61913\n",
            "240/240 - 0s - loss: 0.4037 - acc: 0.8583 - val_loss: 0.6501 - val_acc: 0.7872\n",
            "Epoch 269/300\n",
            "\n",
            "Epoch 00269: val_loss did not improve from 0.61913\n",
            "240/240 - 0s - loss: 0.4852 - acc: 0.7875 - val_loss: 0.6566 - val_acc: 0.7660\n",
            "Epoch 270/300\n",
            "\n",
            "Epoch 00270: val_loss did not improve from 0.61913\n",
            "240/240 - 0s - loss: 0.4716 - acc: 0.8417 - val_loss: 0.6618 - val_acc: 0.7660\n",
            "Epoch 271/300\n",
            "\n",
            "Epoch 00271: val_loss did not improve from 0.61913\n",
            "240/240 - 0s - loss: 0.4269 - acc: 0.8375 - val_loss: 0.6715 - val_acc: 0.7872\n",
            "Epoch 272/300\n",
            "\n",
            "Epoch 00272: val_loss did not improve from 0.61913\n",
            "240/240 - 0s - loss: 0.4553 - acc: 0.8167 - val_loss: 0.6662 - val_acc: 0.7872\n",
            "Epoch 273/300\n",
            "\n",
            "Epoch 00273: val_loss did not improve from 0.61913\n",
            "240/240 - 0s - loss: 0.4378 - acc: 0.8333 - val_loss: 0.6576 - val_acc: 0.7447\n",
            "Epoch 274/300\n",
            "\n",
            "Epoch 00274: val_loss did not improve from 0.61913\n",
            "240/240 - 0s - loss: 0.4825 - acc: 0.7708 - val_loss: 0.7159 - val_acc: 0.7021\n",
            "Epoch 275/300\n",
            "\n",
            "Epoch 00275: val_loss did not improve from 0.61913\n",
            "240/240 - 0s - loss: 0.4212 - acc: 0.8333 - val_loss: 0.6725 - val_acc: 0.8085\n",
            "Epoch 276/300\n",
            "\n",
            "Epoch 00276: val_loss did not improve from 0.61913\n",
            "240/240 - 0s - loss: 0.4669 - acc: 0.8250 - val_loss: 0.6607 - val_acc: 0.8298\n",
            "Epoch 277/300\n",
            "\n",
            "Epoch 00277: val_loss did not improve from 0.61913\n",
            "240/240 - 0s - loss: 0.4658 - acc: 0.8000 - val_loss: 0.6631 - val_acc: 0.7660\n",
            "Epoch 278/300\n",
            "\n",
            "Epoch 00278: val_loss improved from 0.61913 to 0.61725, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.4096 - acc: 0.8583 - val_loss: 0.6173 - val_acc: 0.7872\n",
            "Epoch 279/300\n",
            "\n",
            "Epoch 00279: val_loss did not improve from 0.61725\n",
            "240/240 - 0s - loss: 0.4473 - acc: 0.8583 - val_loss: 0.6681 - val_acc: 0.7447\n",
            "Epoch 280/300\n",
            "\n",
            "Epoch 00280: val_loss did not improve from 0.61725\n",
            "240/240 - 0s - loss: 0.4537 - acc: 0.8167 - val_loss: 0.7395 - val_acc: 0.6383\n",
            "Epoch 281/300\n",
            "\n",
            "Epoch 00281: val_loss did not improve from 0.61725\n",
            "240/240 - 0s - loss: 0.4782 - acc: 0.7917 - val_loss: 0.7120 - val_acc: 0.6809\n",
            "Epoch 282/300\n",
            "\n",
            "Epoch 00282: val_loss did not improve from 0.61725\n",
            "240/240 - 0s - loss: 0.4045 - acc: 0.8583 - val_loss: 0.7021 - val_acc: 0.6809\n",
            "Epoch 283/300\n",
            "\n",
            "Epoch 00283: val_loss did not improve from 0.61725\n",
            "240/240 - 0s - loss: 0.4733 - acc: 0.8042 - val_loss: 0.6937 - val_acc: 0.6809\n",
            "Epoch 284/300\n",
            "\n",
            "Epoch 00284: val_loss did not improve from 0.61725\n",
            "240/240 - 0s - loss: 0.4255 - acc: 0.8542 - val_loss: 0.6745 - val_acc: 0.6809\n",
            "Epoch 285/300\n",
            "\n",
            "Epoch 00285: val_loss did not improve from 0.61725\n",
            "240/240 - 0s - loss: 0.4530 - acc: 0.8250 - val_loss: 0.6557 - val_acc: 0.7447\n",
            "Epoch 286/300\n",
            "\n",
            "Epoch 00286: val_loss did not improve from 0.61725\n",
            "240/240 - 0s - loss: 0.4385 - acc: 0.8250 - val_loss: 0.6972 - val_acc: 0.6809\n",
            "Epoch 287/300\n",
            "\n",
            "Epoch 00287: val_loss did not improve from 0.61725\n",
            "240/240 - 0s - loss: 0.4652 - acc: 0.8167 - val_loss: 0.7214 - val_acc: 0.7447\n",
            "Epoch 288/300\n",
            "\n",
            "Epoch 00288: val_loss did not improve from 0.61725\n",
            "240/240 - 0s - loss: 0.4637 - acc: 0.7917 - val_loss: 0.7064 - val_acc: 0.7447\n",
            "Epoch 289/300\n",
            "\n",
            "Epoch 00289: val_loss did not improve from 0.61725\n",
            "240/240 - 0s - loss: 0.4378 - acc: 0.8208 - val_loss: 0.7296 - val_acc: 0.7447\n",
            "Epoch 290/300\n",
            "\n",
            "Epoch 00290: val_loss did not improve from 0.61725\n",
            "240/240 - 0s - loss: 0.4216 - acc: 0.8500 - val_loss: 0.7146 - val_acc: 0.6596\n",
            "Epoch 291/300\n",
            "\n",
            "Epoch 00291: val_loss did not improve from 0.61725\n",
            "240/240 - 0s - loss: 0.3957 - acc: 0.8792 - val_loss: 0.6834 - val_acc: 0.7234\n",
            "Epoch 292/300\n",
            "\n",
            "Epoch 00292: val_loss did not improve from 0.61725\n",
            "240/240 - 0s - loss: 0.4918 - acc: 0.8000 - val_loss: 0.7247 - val_acc: 0.6596\n",
            "Epoch 293/300\n",
            "\n",
            "Epoch 00293: val_loss did not improve from 0.61725\n",
            "240/240 - 0s - loss: 0.3964 - acc: 0.8333 - val_loss: 0.7076 - val_acc: 0.6809\n",
            "Epoch 294/300\n",
            "\n",
            "Epoch 00294: val_loss did not improve from 0.61725\n",
            "240/240 - 0s - loss: 0.4804 - acc: 0.7917 - val_loss: 0.6870 - val_acc: 0.7660\n",
            "Epoch 295/300\n",
            "\n",
            "Epoch 00295: val_loss did not improve from 0.61725\n",
            "240/240 - 0s - loss: 0.4387 - acc: 0.8167 - val_loss: 0.6908 - val_acc: 0.7660\n",
            "Epoch 296/300\n",
            "\n",
            "Epoch 00296: val_loss did not improve from 0.61725\n",
            "240/240 - 0s - loss: 0.4085 - acc: 0.8583 - val_loss: 0.7282 - val_acc: 0.7234\n",
            "Epoch 297/300\n",
            "\n",
            "Epoch 00297: val_loss did not improve from 0.61725\n",
            "240/240 - 0s - loss: 0.3749 - acc: 0.8708 - val_loss: 0.7521 - val_acc: 0.6809\n",
            "Epoch 298/300\n",
            "\n",
            "Epoch 00298: val_loss did not improve from 0.61725\n",
            "240/240 - 0s - loss: 0.4185 - acc: 0.8250 - val_loss: 0.6952 - val_acc: 0.6809\n",
            "Epoch 299/300\n",
            "\n",
            "Epoch 00299: val_loss did not improve from 0.61725\n",
            "240/240 - 0s - loss: 0.4394 - acc: 0.8375 - val_loss: 0.6831 - val_acc: 0.7234\n",
            "Epoch 300/300\n",
            "\n",
            "Epoch 00300: val_loss did not improve from 0.61725\n",
            "240/240 - 0s - loss: 0.3636 - acc: 0.8667 - val_loss: 0.6975 - val_acc: 0.7234\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "# Evaluate on test data\n",
            "288/288 [==============================] - 1s 2ms/sample - loss: 0.5998 - acc: 0.7535\n",
            "test loss, test acc: [0.5998049772218413, 0.7534722]\n",
            "EEG_Deep/Data2A/Data_A02T.mat\n",
            "(288, 22, 1875)\n",
            "(288, 1)\n",
            "(240, 22, 750)\n",
            "(240, 1)\n",
            "(47, 22, 750)\n",
            "(47, 1)\n",
            "X_train shape: (240, 1, 22, 750)\n",
            "240 train samples\n",
            "47 val samples\n",
            "EEG_Deep/Data2A/Data_A02E.mat\n",
            "(288, 22, 1875)\n",
            "(288, 1)\n",
            "(288, 22, 750)\n",
            "(288, 1)\n",
            "X_train shape: (288, 1, 22, 750)\n",
            "288 train samples\n",
            "Train on 240 samples, validate on 47 samples\n",
            "Epoch 1/300\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 1.38500, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 1s - loss: 1.4068 - acc: 0.2500 - val_loss: 1.3850 - val_acc: 0.2553\n",
            "Epoch 2/300\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 1.38500\n",
            "240/240 - 0s - loss: 1.3687 - acc: 0.3375 - val_loss: 1.3854 - val_acc: 0.2766\n",
            "Epoch 3/300\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 1.38500\n",
            "240/240 - 0s - loss: 1.3426 - acc: 0.4042 - val_loss: 1.3865 - val_acc: 0.2340\n",
            "Epoch 4/300\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 1.38500\n",
            "240/240 - 0s - loss: 1.3437 - acc: 0.3708 - val_loss: 1.3876 - val_acc: 0.2128\n",
            "Epoch 5/300\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 1.38500\n",
            "240/240 - 0s - loss: 1.3206 - acc: 0.4417 - val_loss: 1.3876 - val_acc: 0.1915\n",
            "Epoch 6/300\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 1.38500\n",
            "240/240 - 0s - loss: 1.3136 - acc: 0.4500 - val_loss: 1.3876 - val_acc: 0.1915\n",
            "Epoch 7/300\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 1.38500\n",
            "240/240 - 0s - loss: 1.3034 - acc: 0.4375 - val_loss: 1.3885 - val_acc: 0.2128\n",
            "Epoch 8/300\n",
            "\n",
            "Epoch 00008: val_loss improved from 1.38500 to 1.38491, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.2892 - acc: 0.4708 - val_loss: 1.3849 - val_acc: 0.2553\n",
            "Epoch 9/300\n",
            "\n",
            "Epoch 00009: val_loss improved from 1.38491 to 1.38113, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.2756 - acc: 0.4750 - val_loss: 1.3811 - val_acc: 0.2979\n",
            "Epoch 10/300\n",
            "\n",
            "Epoch 00010: val_loss improved from 1.38113 to 1.37429, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.2562 - acc: 0.4958 - val_loss: 1.3743 - val_acc: 0.2979\n",
            "Epoch 11/300\n",
            "\n",
            "Epoch 00011: val_loss improved from 1.37429 to 1.37105, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.2294 - acc: 0.5167 - val_loss: 1.3710 - val_acc: 0.2766\n",
            "Epoch 12/300\n",
            "\n",
            "Epoch 00012: val_loss improved from 1.37105 to 1.36647, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.2245 - acc: 0.5083 - val_loss: 1.3665 - val_acc: 0.3191\n",
            "Epoch 13/300\n",
            "\n",
            "Epoch 00013: val_loss improved from 1.36647 to 1.35539, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.1965 - acc: 0.5333 - val_loss: 1.3554 - val_acc: 0.3404\n",
            "Epoch 14/300\n",
            "\n",
            "Epoch 00014: val_loss improved from 1.35539 to 1.34772, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.1993 - acc: 0.5292 - val_loss: 1.3477 - val_acc: 0.3404\n",
            "Epoch 15/300\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 1.34772\n",
            "240/240 - 0s - loss: 1.1937 - acc: 0.5500 - val_loss: 1.3499 - val_acc: 0.3191\n",
            "Epoch 16/300\n",
            "\n",
            "Epoch 00016: val_loss improved from 1.34772 to 1.34072, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.1710 - acc: 0.5542 - val_loss: 1.3407 - val_acc: 0.2766\n",
            "Epoch 17/300\n",
            "\n",
            "Epoch 00017: val_loss improved from 1.34072 to 1.33873, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.1624 - acc: 0.5708 - val_loss: 1.3387 - val_acc: 0.3404\n",
            "Epoch 18/300\n",
            "\n",
            "Epoch 00018: val_loss improved from 1.33873 to 1.33645, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.1792 - acc: 0.5292 - val_loss: 1.3365 - val_acc: 0.2979\n",
            "Epoch 19/300\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 1.33645\n",
            "240/240 - 0s - loss: 1.1490 - acc: 0.5667 - val_loss: 1.3372 - val_acc: 0.2766\n",
            "Epoch 20/300\n",
            "\n",
            "Epoch 00020: val_loss improved from 1.33645 to 1.31835, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.1422 - acc: 0.5458 - val_loss: 1.3183 - val_acc: 0.3404\n",
            "Epoch 21/300\n",
            "\n",
            "Epoch 00021: val_loss improved from 1.31835 to 1.31567, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.1489 - acc: 0.5208 - val_loss: 1.3157 - val_acc: 0.3404\n",
            "Epoch 22/300\n",
            "\n",
            "Epoch 00022: val_loss improved from 1.31567 to 1.31148, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.1447 - acc: 0.5625 - val_loss: 1.3115 - val_acc: 0.3191\n",
            "Epoch 23/300\n",
            "\n",
            "Epoch 00023: val_loss improved from 1.31148 to 1.30824, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.1384 - acc: 0.5167 - val_loss: 1.3082 - val_acc: 0.3404\n",
            "Epoch 24/300\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 1.30824\n",
            "240/240 - 0s - loss: 1.1366 - acc: 0.5292 - val_loss: 1.3194 - val_acc: 0.3404\n",
            "Epoch 25/300\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 1.30824\n",
            "240/240 - 0s - loss: 1.1382 - acc: 0.5542 - val_loss: 1.3143 - val_acc: 0.3404\n",
            "Epoch 26/300\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 1.30824\n",
            "240/240 - 0s - loss: 1.1392 - acc: 0.5542 - val_loss: 1.3207 - val_acc: 0.3404\n",
            "Epoch 27/300\n",
            "\n",
            "Epoch 00027: val_loss improved from 1.30824 to 1.30529, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.1344 - acc: 0.5333 - val_loss: 1.3053 - val_acc: 0.3830\n",
            "Epoch 28/300\n",
            "\n",
            "Epoch 00028: val_loss improved from 1.30529 to 1.29748, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.1208 - acc: 0.5583 - val_loss: 1.2975 - val_acc: 0.4043\n",
            "Epoch 29/300\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 1.29748\n",
            "240/240 - 0s - loss: 1.1092 - acc: 0.5833 - val_loss: 1.3031 - val_acc: 0.3617\n",
            "Epoch 30/300\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 1.29748\n",
            "240/240 - 0s - loss: 1.1332 - acc: 0.5458 - val_loss: 1.3204 - val_acc: 0.3830\n",
            "Epoch 31/300\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 1.29748\n",
            "240/240 - 0s - loss: 1.1200 - acc: 0.6125 - val_loss: 1.3166 - val_acc: 0.3617\n",
            "Epoch 32/300\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 1.29748\n",
            "240/240 - 0s - loss: 1.1028 - acc: 0.5375 - val_loss: 1.3164 - val_acc: 0.3830\n",
            "Epoch 33/300\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 1.29748\n",
            "240/240 - 0s - loss: 1.1022 - acc: 0.5875 - val_loss: 1.3107 - val_acc: 0.3617\n",
            "Epoch 34/300\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 1.29748\n",
            "240/240 - 0s - loss: 1.0746 - acc: 0.6250 - val_loss: 1.3007 - val_acc: 0.4468\n",
            "Epoch 35/300\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 1.29748\n",
            "240/240 - 0s - loss: 1.0962 - acc: 0.5292 - val_loss: 1.3077 - val_acc: 0.3830\n",
            "Epoch 36/300\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 1.29748\n",
            "240/240 - 0s - loss: 1.1087 - acc: 0.5583 - val_loss: 1.3106 - val_acc: 0.3617\n",
            "Epoch 37/300\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 1.29748\n",
            "240/240 - 0s - loss: 1.0761 - acc: 0.5708 - val_loss: 1.3012 - val_acc: 0.3404\n",
            "Epoch 38/300\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 1.29748\n",
            "240/240 - 0s - loss: 1.1057 - acc: 0.5250 - val_loss: 1.2979 - val_acc: 0.3830\n",
            "Epoch 39/300\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 1.29748\n",
            "240/240 - 0s - loss: 1.0774 - acc: 0.6042 - val_loss: 1.3037 - val_acc: 0.3191\n",
            "Epoch 40/300\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 1.29748\n",
            "240/240 - 0s - loss: 1.0805 - acc: 0.6000 - val_loss: 1.3143 - val_acc: 0.3830\n",
            "Epoch 41/300\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 1.29748\n",
            "240/240 - 0s - loss: 1.0834 - acc: 0.5833 - val_loss: 1.3153 - val_acc: 0.3830\n",
            "Epoch 42/300\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 1.29748\n",
            "240/240 - 0s - loss: 1.0547 - acc: 0.6042 - val_loss: 1.3178 - val_acc: 0.3830\n",
            "Epoch 43/300\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 1.29748\n",
            "240/240 - 0s - loss: 1.0652 - acc: 0.5833 - val_loss: 1.3174 - val_acc: 0.3404\n",
            "Epoch 44/300\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 1.29748\n",
            "240/240 - 0s - loss: 1.0768 - acc: 0.5250 - val_loss: 1.3152 - val_acc: 0.3830\n",
            "Epoch 45/300\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 1.29748\n",
            "240/240 - 0s - loss: 1.0902 - acc: 0.5750 - val_loss: 1.3164 - val_acc: 0.3191\n",
            "Epoch 46/300\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 1.29748\n",
            "240/240 - 0s - loss: 1.0589 - acc: 0.5958 - val_loss: 1.3044 - val_acc: 0.3404\n",
            "Epoch 47/300\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 1.29748\n",
            "240/240 - 0s - loss: 1.0792 - acc: 0.5625 - val_loss: 1.3003 - val_acc: 0.3404\n",
            "Epoch 48/300\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 1.29748\n",
            "240/240 - 0s - loss: 1.0544 - acc: 0.6375 - val_loss: 1.3147 - val_acc: 0.3404\n",
            "Epoch 49/300\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 1.29748\n",
            "240/240 - 0s - loss: 1.0610 - acc: 0.5667 - val_loss: 1.3002 - val_acc: 0.3617\n",
            "Epoch 50/300\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 1.29748\n",
            "240/240 - 0s - loss: 1.0406 - acc: 0.5917 - val_loss: 1.2995 - val_acc: 0.3617\n",
            "Epoch 51/300\n",
            "\n",
            "Epoch 00051: val_loss improved from 1.29748 to 1.29562, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.0359 - acc: 0.6208 - val_loss: 1.2956 - val_acc: 0.3830\n",
            "Epoch 52/300\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 1.29562\n",
            "240/240 - 0s - loss: 1.0408 - acc: 0.5917 - val_loss: 1.3047 - val_acc: 0.4043\n",
            "Epoch 53/300\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 1.29562\n",
            "240/240 - 0s - loss: 1.0578 - acc: 0.6000 - val_loss: 1.3041 - val_acc: 0.3617\n",
            "Epoch 54/300\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 1.29562\n",
            "240/240 - 0s - loss: 1.0692 - acc: 0.5458 - val_loss: 1.3015 - val_acc: 0.4043\n",
            "Epoch 55/300\n",
            "\n",
            "Epoch 00055: val_loss improved from 1.29562 to 1.28494, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.0324 - acc: 0.6042 - val_loss: 1.2849 - val_acc: 0.3617\n",
            "Epoch 56/300\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 1.28494\n",
            "240/240 - 0s - loss: 1.0328 - acc: 0.6125 - val_loss: 1.2929 - val_acc: 0.3617\n",
            "Epoch 57/300\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 1.28494\n",
            "240/240 - 0s - loss: 1.0439 - acc: 0.6250 - val_loss: 1.2915 - val_acc: 0.4468\n",
            "Epoch 58/300\n",
            "\n",
            "Epoch 00058: val_loss improved from 1.28494 to 1.28477, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.0147 - acc: 0.6167 - val_loss: 1.2848 - val_acc: 0.3617\n",
            "Epoch 59/300\n",
            "\n",
            "Epoch 00059: val_loss improved from 1.28477 to 1.27673, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.0352 - acc: 0.6000 - val_loss: 1.2767 - val_acc: 0.4043\n",
            "Epoch 60/300\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 1.27673\n",
            "240/240 - 0s - loss: 1.0385 - acc: 0.6042 - val_loss: 1.2877 - val_acc: 0.3404\n",
            "Epoch 61/300\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 1.27673\n",
            "240/240 - 0s - loss: 1.0266 - acc: 0.6250 - val_loss: 1.2958 - val_acc: 0.3617\n",
            "Epoch 62/300\n",
            "\n",
            "Epoch 00062: val_loss improved from 1.27673 to 1.27360, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.9833 - acc: 0.6542 - val_loss: 1.2736 - val_acc: 0.3404\n",
            "Epoch 63/300\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 1.27360\n",
            "240/240 - 0s - loss: 1.0186 - acc: 0.6000 - val_loss: 1.2906 - val_acc: 0.3830\n",
            "Epoch 64/300\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 1.27360\n",
            "240/240 - 0s - loss: 1.0180 - acc: 0.6000 - val_loss: 1.2971 - val_acc: 0.3617\n",
            "Epoch 65/300\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 1.27360\n",
            "240/240 - 0s - loss: 1.0000 - acc: 0.6125 - val_loss: 1.2939 - val_acc: 0.4255\n",
            "Epoch 66/300\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 1.27360\n",
            "240/240 - 0s - loss: 1.0081 - acc: 0.5792 - val_loss: 1.2897 - val_acc: 0.3830\n",
            "Epoch 67/300\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 1.27360\n",
            "240/240 - 0s - loss: 1.0108 - acc: 0.6458 - val_loss: 1.2827 - val_acc: 0.3617\n",
            "Epoch 68/300\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 1.27360\n",
            "240/240 - 0s - loss: 0.9789 - acc: 0.6583 - val_loss: 1.2780 - val_acc: 0.3830\n",
            "Epoch 69/300\n",
            "\n",
            "Epoch 00069: val_loss improved from 1.27360 to 1.26752, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.0028 - acc: 0.6125 - val_loss: 1.2675 - val_acc: 0.4043\n",
            "Epoch 70/300\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 1.26752\n",
            "240/240 - 0s - loss: 1.0241 - acc: 0.5750 - val_loss: 1.2741 - val_acc: 0.4043\n",
            "Epoch 71/300\n",
            "\n",
            "Epoch 00071: val_loss improved from 1.26752 to 1.25872, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.9526 - acc: 0.6458 - val_loss: 1.2587 - val_acc: 0.4468\n",
            "Epoch 72/300\n",
            "\n",
            "Epoch 00072: val_loss improved from 1.25872 to 1.25777, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.9423 - acc: 0.6667 - val_loss: 1.2578 - val_acc: 0.4043\n",
            "Epoch 73/300\n",
            "\n",
            "Epoch 00073: val_loss improved from 1.25777 to 1.24320, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.9704 - acc: 0.6542 - val_loss: 1.2432 - val_acc: 0.4255\n",
            "Epoch 74/300\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 1.24320\n",
            "240/240 - 0s - loss: 0.9690 - acc: 0.6667 - val_loss: 1.2703 - val_acc: 0.3830\n",
            "Epoch 75/300\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 1.24320\n",
            "240/240 - 0s - loss: 0.9546 - acc: 0.6750 - val_loss: 1.2704 - val_acc: 0.3830\n",
            "Epoch 76/300\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 1.24320\n",
            "240/240 - 0s - loss: 0.9565 - acc: 0.6625 - val_loss: 1.2665 - val_acc: 0.4255\n",
            "Epoch 77/300\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 1.24320\n",
            "240/240 - 0s - loss: 0.9181 - acc: 0.7042 - val_loss: 1.2740 - val_acc: 0.3617\n",
            "Epoch 78/300\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 1.24320\n",
            "240/240 - 0s - loss: 0.9458 - acc: 0.6958 - val_loss: 1.2742 - val_acc: 0.3617\n",
            "Epoch 79/300\n",
            "\n",
            "Epoch 00079: val_loss improved from 1.24320 to 1.23695, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.9855 - acc: 0.6583 - val_loss: 1.2369 - val_acc: 0.4255\n",
            "Epoch 80/300\n",
            "\n",
            "Epoch 00080: val_loss improved from 1.23695 to 1.23396, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.9296 - acc: 0.6708 - val_loss: 1.2340 - val_acc: 0.4255\n",
            "Epoch 81/300\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 1.23396\n",
            "240/240 - 0s - loss: 0.9213 - acc: 0.7083 - val_loss: 1.2342 - val_acc: 0.3830\n",
            "Epoch 82/300\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 1.23396\n",
            "240/240 - 0s - loss: 0.9047 - acc: 0.7083 - val_loss: 1.2493 - val_acc: 0.3617\n",
            "Epoch 83/300\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 1.23396\n",
            "240/240 - 0s - loss: 0.9325 - acc: 0.6750 - val_loss: 1.2449 - val_acc: 0.3830\n",
            "Epoch 84/300\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 1.23396\n",
            "240/240 - 0s - loss: 0.9446 - acc: 0.6708 - val_loss: 1.2380 - val_acc: 0.3404\n",
            "Epoch 85/300\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 1.23396\n",
            "240/240 - 0s - loss: 0.9349 - acc: 0.6375 - val_loss: 1.2565 - val_acc: 0.2979\n",
            "Epoch 86/300\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 1.23396\n",
            "240/240 - 0s - loss: 0.9416 - acc: 0.6708 - val_loss: 1.2400 - val_acc: 0.4468\n",
            "Epoch 87/300\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 1.23396\n",
            "240/240 - 0s - loss: 0.9032 - acc: 0.6958 - val_loss: 1.2533 - val_acc: 0.3191\n",
            "Epoch 88/300\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 1.23396\n",
            "240/240 - 0s - loss: 0.9433 - acc: 0.6500 - val_loss: 1.2543 - val_acc: 0.3830\n",
            "Epoch 89/300\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 1.23396\n",
            "240/240 - 0s - loss: 0.9655 - acc: 0.6417 - val_loss: 1.2389 - val_acc: 0.3830\n",
            "Epoch 90/300\n",
            "\n",
            "Epoch 00090: val_loss improved from 1.23396 to 1.23302, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.9040 - acc: 0.6542 - val_loss: 1.2330 - val_acc: 0.3830\n",
            "Epoch 91/300\n",
            "\n",
            "Epoch 00091: val_loss improved from 1.23302 to 1.20827, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.9454 - acc: 0.6708 - val_loss: 1.2083 - val_acc: 0.4468\n",
            "Epoch 92/300\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 1.20827\n",
            "240/240 - 0s - loss: 0.9060 - acc: 0.6750 - val_loss: 1.2214 - val_acc: 0.3830\n",
            "Epoch 93/300\n",
            "\n",
            "Epoch 00093: val_loss improved from 1.20827 to 1.20281, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.9137 - acc: 0.6750 - val_loss: 1.2028 - val_acc: 0.4255\n",
            "Epoch 94/300\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 1.20281\n",
            "240/240 - 0s - loss: 0.9069 - acc: 0.6875 - val_loss: 1.2164 - val_acc: 0.3404\n",
            "Epoch 95/300\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 1.20281\n",
            "240/240 - 0s - loss: 0.9070 - acc: 0.7042 - val_loss: 1.2193 - val_acc: 0.3830\n",
            "Epoch 96/300\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 1.20281\n",
            "240/240 - 0s - loss: 0.9031 - acc: 0.7083 - val_loss: 1.2130 - val_acc: 0.3617\n",
            "Epoch 97/300\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 1.20281\n",
            "240/240 - 0s - loss: 0.8677 - acc: 0.7208 - val_loss: 1.2336 - val_acc: 0.4043\n",
            "Epoch 98/300\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 1.20281\n",
            "240/240 - 0s - loss: 0.8874 - acc: 0.6875 - val_loss: 1.2303 - val_acc: 0.4043\n",
            "Epoch 99/300\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 1.20281\n",
            "240/240 - 0s - loss: 0.8830 - acc: 0.7083 - val_loss: 1.2149 - val_acc: 0.3830\n",
            "Epoch 100/300\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 1.20281\n",
            "240/240 - 0s - loss: 0.8565 - acc: 0.7208 - val_loss: 1.2126 - val_acc: 0.4468\n",
            "Epoch 101/300\n",
            "\n",
            "Epoch 00101: val_loss improved from 1.20281 to 1.18261, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.9092 - acc: 0.6750 - val_loss: 1.1826 - val_acc: 0.4255\n",
            "Epoch 102/300\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 1.18261\n",
            "240/240 - 0s - loss: 0.8801 - acc: 0.7250 - val_loss: 1.2048 - val_acc: 0.3830\n",
            "Epoch 103/300\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 1.18261\n",
            "240/240 - 0s - loss: 0.8377 - acc: 0.7167 - val_loss: 1.1993 - val_acc: 0.3830\n",
            "Epoch 104/300\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 1.18261\n",
            "240/240 - 0s - loss: 0.8378 - acc: 0.7375 - val_loss: 1.1848 - val_acc: 0.4681\n",
            "Epoch 105/300\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 1.18261\n",
            "240/240 - 0s - loss: 0.9007 - acc: 0.6875 - val_loss: 1.1970 - val_acc: 0.4681\n",
            "Epoch 106/300\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 1.18261\n",
            "240/240 - 0s - loss: 0.8751 - acc: 0.6875 - val_loss: 1.1960 - val_acc: 0.3830\n",
            "Epoch 107/300\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 1.18261\n",
            "240/240 - 0s - loss: 0.8555 - acc: 0.7458 - val_loss: 1.1909 - val_acc: 0.3617\n",
            "Epoch 108/300\n",
            "\n",
            "Epoch 00108: val_loss improved from 1.18261 to 1.18253, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.8451 - acc: 0.7250 - val_loss: 1.1825 - val_acc: 0.3617\n",
            "Epoch 109/300\n",
            "\n",
            "Epoch 00109: val_loss improved from 1.18253 to 1.16674, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.8363 - acc: 0.7500 - val_loss: 1.1667 - val_acc: 0.4681\n",
            "Epoch 110/300\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 1.16674\n",
            "240/240 - 0s - loss: 0.8561 - acc: 0.6875 - val_loss: 1.1869 - val_acc: 0.4681\n",
            "Epoch 111/300\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 1.16674\n",
            "240/240 - 0s - loss: 0.8108 - acc: 0.7667 - val_loss: 1.1721 - val_acc: 0.4043\n",
            "Epoch 112/300\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 1.16674\n",
            "240/240 - 0s - loss: 0.8199 - acc: 0.7458 - val_loss: 1.1758 - val_acc: 0.3830\n",
            "Epoch 113/300\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 1.16674\n",
            "240/240 - 0s - loss: 0.8274 - acc: 0.7292 - val_loss: 1.1817 - val_acc: 0.3617\n",
            "Epoch 114/300\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 1.16674\n",
            "240/240 - 0s - loss: 0.8054 - acc: 0.7500 - val_loss: 1.1998 - val_acc: 0.4043\n",
            "Epoch 115/300\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 1.16674\n",
            "240/240 - 0s - loss: 0.8144 - acc: 0.7458 - val_loss: 1.1897 - val_acc: 0.3617\n",
            "Epoch 116/300\n",
            "\n",
            "Epoch 00116: val_loss improved from 1.16674 to 1.16355, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.8145 - acc: 0.7542 - val_loss: 1.1635 - val_acc: 0.4043\n",
            "Epoch 117/300\n",
            "\n",
            "Epoch 00117: val_loss improved from 1.16355 to 1.16187, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.8218 - acc: 0.7625 - val_loss: 1.1619 - val_acc: 0.4468\n",
            "Epoch 118/300\n",
            "\n",
            "Epoch 00118: val_loss improved from 1.16187 to 1.11345, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.8731 - acc: 0.6917 - val_loss: 1.1135 - val_acc: 0.4681\n",
            "Epoch 119/300\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 1.11345\n",
            "240/240 - 0s - loss: 0.8470 - acc: 0.7083 - val_loss: 1.1538 - val_acc: 0.3617\n",
            "Epoch 120/300\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 1.11345\n",
            "240/240 - 0s - loss: 0.8607 - acc: 0.6792 - val_loss: 1.1290 - val_acc: 0.4681\n",
            "Epoch 121/300\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 1.11345\n",
            "240/240 - 0s - loss: 0.8728 - acc: 0.7000 - val_loss: 1.1302 - val_acc: 0.4681\n",
            "Epoch 122/300\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 1.11345\n",
            "240/240 - 0s - loss: 0.8168 - acc: 0.7375 - val_loss: 1.1287 - val_acc: 0.4468\n",
            "Epoch 123/300\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 1.11345\n",
            "240/240 - 0s - loss: 0.8103 - acc: 0.7417 - val_loss: 1.1306 - val_acc: 0.4681\n",
            "Epoch 124/300\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 1.11345\n",
            "240/240 - 0s - loss: 0.8313 - acc: 0.7333 - val_loss: 1.1292 - val_acc: 0.4468\n",
            "Epoch 125/300\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 1.11345\n",
            "240/240 - 0s - loss: 0.8476 - acc: 0.6792 - val_loss: 1.1404 - val_acc: 0.4043\n",
            "Epoch 126/300\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 1.11345\n",
            "240/240 - 0s - loss: 0.8007 - acc: 0.7375 - val_loss: 1.1517 - val_acc: 0.4468\n",
            "Epoch 127/300\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 1.11345\n",
            "240/240 - 0s - loss: 0.8182 - acc: 0.7458 - val_loss: 1.1472 - val_acc: 0.4043\n",
            "Epoch 128/300\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 1.11345\n",
            "240/240 - 0s - loss: 0.8549 - acc: 0.6875 - val_loss: 1.1410 - val_acc: 0.4681\n",
            "Epoch 129/300\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 1.11345\n",
            "240/240 - 0s - loss: 0.8281 - acc: 0.7000 - val_loss: 1.1322 - val_acc: 0.4468\n",
            "Epoch 130/300\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 1.11345\n",
            "240/240 - 0s - loss: 0.7978 - acc: 0.7417 - val_loss: 1.1314 - val_acc: 0.4681\n",
            "Epoch 131/300\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 1.11345\n",
            "240/240 - 0s - loss: 0.7913 - acc: 0.7542 - val_loss: 1.1230 - val_acc: 0.4043\n",
            "Epoch 132/300\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 1.11345\n",
            "240/240 - 0s - loss: 0.8023 - acc: 0.7125 - val_loss: 1.1365 - val_acc: 0.4468\n",
            "Epoch 133/300\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 1.11345\n",
            "240/240 - 0s - loss: 0.8175 - acc: 0.7542 - val_loss: 1.1140 - val_acc: 0.4894\n",
            "Epoch 134/300\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 1.11345\n",
            "240/240 - 0s - loss: 0.7910 - acc: 0.7667 - val_loss: 1.1237 - val_acc: 0.4468\n",
            "Epoch 135/300\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 1.11345\n",
            "240/240 - 0s - loss: 0.8082 - acc: 0.7250 - val_loss: 1.1452 - val_acc: 0.4255\n",
            "Epoch 136/300\n",
            "\n",
            "Epoch 00136: val_loss improved from 1.11345 to 1.10769, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.8017 - acc: 0.7417 - val_loss: 1.1077 - val_acc: 0.5106\n",
            "Epoch 137/300\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 1.10769\n",
            "240/240 - 0s - loss: 0.7896 - acc: 0.7417 - val_loss: 1.1136 - val_acc: 0.4255\n",
            "Epoch 138/300\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 1.10769\n",
            "240/240 - 0s - loss: 0.8253 - acc: 0.7167 - val_loss: 1.1198 - val_acc: 0.4894\n",
            "Epoch 139/300\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 1.10769\n",
            "240/240 - 0s - loss: 0.7687 - acc: 0.7458 - val_loss: 1.1245 - val_acc: 0.4894\n",
            "Epoch 140/300\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 1.10769\n",
            "240/240 - 0s - loss: 0.7835 - acc: 0.7125 - val_loss: 1.1179 - val_acc: 0.5106\n",
            "Epoch 141/300\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 1.10769\n",
            "240/240 - 0s - loss: 0.7500 - acc: 0.7750 - val_loss: 1.1344 - val_acc: 0.4255\n",
            "Epoch 142/300\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 1.10769\n",
            "240/240 - 0s - loss: 0.8148 - acc: 0.7042 - val_loss: 1.1278 - val_acc: 0.4681\n",
            "Epoch 143/300\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 1.10769\n",
            "240/240 - 0s - loss: 0.7533 - acc: 0.7750 - val_loss: 1.1259 - val_acc: 0.4468\n",
            "Epoch 144/300\n",
            "\n",
            "Epoch 00144: val_loss improved from 1.10769 to 1.09938, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.7830 - acc: 0.7542 - val_loss: 1.0994 - val_acc: 0.4681\n",
            "Epoch 145/300\n",
            "\n",
            "Epoch 00145: val_loss improved from 1.09938 to 1.09698, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.7785 - acc: 0.7667 - val_loss: 1.0970 - val_acc: 0.4894\n",
            "Epoch 146/300\n",
            "\n",
            "Epoch 00146: val_loss improved from 1.09698 to 1.09417, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.7446 - acc: 0.7500 - val_loss: 1.0942 - val_acc: 0.4894\n",
            "Epoch 147/300\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 1.09417\n",
            "240/240 - 0s - loss: 0.7834 - acc: 0.7458 - val_loss: 1.1072 - val_acc: 0.4681\n",
            "Epoch 148/300\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 1.09417\n",
            "240/240 - 0s - loss: 0.8196 - acc: 0.6917 - val_loss: 1.1001 - val_acc: 0.5319\n",
            "Epoch 149/300\n",
            "\n",
            "Epoch 00149: val_loss improved from 1.09417 to 1.07317, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.7312 - acc: 0.7542 - val_loss: 1.0732 - val_acc: 0.5106\n",
            "Epoch 150/300\n",
            "\n",
            "Epoch 00150: val_loss improved from 1.07317 to 1.07033, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.7997 - acc: 0.7125 - val_loss: 1.0703 - val_acc: 0.4894\n",
            "Epoch 151/300\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 1.07033\n",
            "240/240 - 0s - loss: 0.7487 - acc: 0.7625 - val_loss: 1.0794 - val_acc: 0.4894\n",
            "Epoch 152/300\n",
            "\n",
            "Epoch 00152: val_loss improved from 1.07033 to 1.06849, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.7545 - acc: 0.7917 - val_loss: 1.0685 - val_acc: 0.5957\n",
            "Epoch 153/300\n",
            "\n",
            "Epoch 00153: val_loss improved from 1.06849 to 1.05628, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.7959 - acc: 0.7083 - val_loss: 1.0563 - val_acc: 0.5106\n",
            "Epoch 154/300\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 1.05628\n",
            "240/240 - 0s - loss: 0.7571 - acc: 0.7667 - val_loss: 1.0855 - val_acc: 0.5106\n",
            "Epoch 155/300\n",
            "\n",
            "Epoch 00155: val_loss improved from 1.05628 to 1.05543, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.7677 - acc: 0.7417 - val_loss: 1.0554 - val_acc: 0.5106\n",
            "Epoch 156/300\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 1.05543\n",
            "240/240 - 0s - loss: 0.7706 - acc: 0.7458 - val_loss: 1.0688 - val_acc: 0.5319\n",
            "Epoch 157/300\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 1.05543\n",
            "240/240 - 0s - loss: 0.7587 - acc: 0.7542 - val_loss: 1.1022 - val_acc: 0.5532\n",
            "Epoch 158/300\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 1.05543\n",
            "240/240 - 0s - loss: 0.7529 - acc: 0.7625 - val_loss: 1.0902 - val_acc: 0.5745\n",
            "Epoch 159/300\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 1.05543\n",
            "240/240 - 0s - loss: 0.7938 - acc: 0.7583 - val_loss: 1.0729 - val_acc: 0.5319\n",
            "Epoch 160/300\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 1.05543\n",
            "240/240 - 0s - loss: 0.7724 - acc: 0.7417 - val_loss: 1.1202 - val_acc: 0.3830\n",
            "Epoch 161/300\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 1.05543\n",
            "240/240 - 0s - loss: 0.7495 - acc: 0.7500 - val_loss: 1.0801 - val_acc: 0.4894\n",
            "Epoch 162/300\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 1.05543\n",
            "240/240 - 0s - loss: 0.7400 - acc: 0.7625 - val_loss: 1.0902 - val_acc: 0.4894\n",
            "Epoch 163/300\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 1.05543\n",
            "240/240 - 0s - loss: 0.7712 - acc: 0.7625 - val_loss: 1.0994 - val_acc: 0.5106\n",
            "Epoch 164/300\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 1.05543\n",
            "240/240 - 0s - loss: 0.7698 - acc: 0.7250 - val_loss: 1.0930 - val_acc: 0.4468\n",
            "Epoch 165/300\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 1.05543\n",
            "240/240 - 0s - loss: 0.7338 - acc: 0.7792 - val_loss: 1.0693 - val_acc: 0.5106\n",
            "Epoch 166/300\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 1.05543\n",
            "240/240 - 0s - loss: 0.7381 - acc: 0.7542 - val_loss: 1.0596 - val_acc: 0.5106\n",
            "Epoch 167/300\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 1.05543\n",
            "240/240 - 0s - loss: 0.7504 - acc: 0.7542 - val_loss: 1.1216 - val_acc: 0.4894\n",
            "Epoch 168/300\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 1.05543\n",
            "240/240 - 0s - loss: 0.7497 - acc: 0.7375 - val_loss: 1.0965 - val_acc: 0.4894\n",
            "Epoch 169/300\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 1.05543\n",
            "240/240 - 0s - loss: 0.7428 - acc: 0.7583 - val_loss: 1.0987 - val_acc: 0.4681\n",
            "Epoch 170/300\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 1.05543\n",
            "240/240 - 0s - loss: 0.7333 - acc: 0.7833 - val_loss: 1.0756 - val_acc: 0.5319\n",
            "Epoch 171/300\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 1.05543\n",
            "240/240 - 0s - loss: 0.7148 - acc: 0.7792 - val_loss: 1.1008 - val_acc: 0.5106\n",
            "Epoch 172/300\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 1.05543\n",
            "240/240 - 0s - loss: 0.7326 - acc: 0.7708 - val_loss: 1.0710 - val_acc: 0.4894\n",
            "Epoch 173/300\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 1.05543\n",
            "240/240 - 0s - loss: 0.7549 - acc: 0.7333 - val_loss: 1.1031 - val_acc: 0.4681\n",
            "Epoch 174/300\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 1.05543\n",
            "240/240 - 0s - loss: 0.7325 - acc: 0.7250 - val_loss: 1.0907 - val_acc: 0.4468\n",
            "Epoch 175/300\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 1.05543\n",
            "240/240 - 0s - loss: 0.7369 - acc: 0.7500 - val_loss: 1.0774 - val_acc: 0.5745\n",
            "Epoch 176/300\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 1.05543\n",
            "240/240 - 0s - loss: 0.7148 - acc: 0.7750 - val_loss: 1.0823 - val_acc: 0.5106\n",
            "Epoch 177/300\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 1.05543\n",
            "240/240 - 0s - loss: 0.7177 - acc: 0.7375 - val_loss: 1.0759 - val_acc: 0.4894\n",
            "Epoch 178/300\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 1.05543\n",
            "240/240 - 0s - loss: 0.7024 - acc: 0.8083 - val_loss: 1.0953 - val_acc: 0.4681\n",
            "Epoch 179/300\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 1.05543\n",
            "240/240 - 0s - loss: 0.7020 - acc: 0.7708 - val_loss: 1.0841 - val_acc: 0.4681\n",
            "Epoch 180/300\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 1.05543\n",
            "240/240 - 0s - loss: 0.7165 - acc: 0.7708 - val_loss: 1.1172 - val_acc: 0.4468\n",
            "Epoch 181/300\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 1.05543\n",
            "240/240 - 0s - loss: 0.7195 - acc: 0.7500 - val_loss: 1.0690 - val_acc: 0.5319\n",
            "Epoch 182/300\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 1.05543\n",
            "240/240 - 0s - loss: 0.7274 - acc: 0.7500 - val_loss: 1.0760 - val_acc: 0.5532\n",
            "Epoch 183/300\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 1.05543\n",
            "240/240 - 0s - loss: 0.7242 - acc: 0.7542 - val_loss: 1.0813 - val_acc: 0.5106\n",
            "Epoch 184/300\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 1.05543\n",
            "240/240 - 0s - loss: 0.6932 - acc: 0.7833 - val_loss: 1.0757 - val_acc: 0.5106\n",
            "Epoch 185/300\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 1.05543\n",
            "240/240 - 0s - loss: 0.7212 - acc: 0.7667 - val_loss: 1.0756 - val_acc: 0.4894\n",
            "Epoch 186/300\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 1.05543\n",
            "240/240 - 0s - loss: 0.7433 - acc: 0.7542 - val_loss: 1.0663 - val_acc: 0.4894\n",
            "Epoch 187/300\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 1.05543\n",
            "240/240 - 0s - loss: 0.7109 - acc: 0.8000 - val_loss: 1.1049 - val_acc: 0.4043\n",
            "Epoch 188/300\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 1.05543\n",
            "240/240 - 0s - loss: 0.7022 - acc: 0.7750 - val_loss: 1.0708 - val_acc: 0.5319\n",
            "Epoch 189/300\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 1.05543\n",
            "240/240 - 0s - loss: 0.7382 - acc: 0.7458 - val_loss: 1.1007 - val_acc: 0.4894\n",
            "Epoch 190/300\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 1.05543\n",
            "240/240 - 0s - loss: 0.7266 - acc: 0.7833 - val_loss: 1.0860 - val_acc: 0.4468\n",
            "Epoch 191/300\n",
            "\n",
            "Epoch 00191: val_loss improved from 1.05543 to 1.05497, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.7186 - acc: 0.8000 - val_loss: 1.0550 - val_acc: 0.5319\n",
            "Epoch 192/300\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 1.05497\n",
            "240/240 - 0s - loss: 0.6896 - acc: 0.8208 - val_loss: 1.0654 - val_acc: 0.4894\n",
            "Epoch 193/300\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 1.05497\n",
            "240/240 - 0s - loss: 0.6933 - acc: 0.7458 - val_loss: 1.0724 - val_acc: 0.4894\n",
            "Epoch 194/300\n",
            "\n",
            "Epoch 00194: val_loss improved from 1.05497 to 1.04481, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.7415 - acc: 0.7292 - val_loss: 1.0448 - val_acc: 0.5106\n",
            "Epoch 195/300\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 1.04481\n",
            "240/240 - 0s - loss: 0.7315 - acc: 0.7625 - val_loss: 1.0464 - val_acc: 0.5532\n",
            "Epoch 196/300\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 1.04481\n",
            "240/240 - 0s - loss: 0.7050 - acc: 0.7667 - val_loss: 1.0687 - val_acc: 0.4043\n",
            "Epoch 197/300\n",
            "\n",
            "Epoch 00197: val_loss improved from 1.04481 to 1.04354, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.7054 - acc: 0.7417 - val_loss: 1.0435 - val_acc: 0.5532\n",
            "Epoch 198/300\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 1.04354\n",
            "240/240 - 0s - loss: 0.7123 - acc: 0.7333 - val_loss: 1.0482 - val_acc: 0.5106\n",
            "Epoch 199/300\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 1.04354\n",
            "240/240 - 0s - loss: 0.6752 - acc: 0.7792 - val_loss: 1.0860 - val_acc: 0.4681\n",
            "Epoch 200/300\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 1.04354\n",
            "240/240 - 0s - loss: 0.6920 - acc: 0.7792 - val_loss: 1.0597 - val_acc: 0.4468\n",
            "Epoch 201/300\n",
            "\n",
            "Epoch 00201: val_loss did not improve from 1.04354\n",
            "240/240 - 0s - loss: 0.7059 - acc: 0.7708 - val_loss: 1.0977 - val_acc: 0.4894\n",
            "Epoch 202/300\n",
            "\n",
            "Epoch 00202: val_loss did not improve from 1.04354\n",
            "240/240 - 0s - loss: 0.7422 - acc: 0.7208 - val_loss: 1.0725 - val_acc: 0.4255\n",
            "Epoch 203/300\n",
            "\n",
            "Epoch 00203: val_loss did not improve from 1.04354\n",
            "240/240 - 0s - loss: 0.7333 - acc: 0.7667 - val_loss: 1.0544 - val_acc: 0.5319\n",
            "Epoch 204/300\n",
            "\n",
            "Epoch 00204: val_loss did not improve from 1.04354\n",
            "240/240 - 0s - loss: 0.7064 - acc: 0.7750 - val_loss: 1.0456 - val_acc: 0.5745\n",
            "Epoch 205/300\n",
            "\n",
            "Epoch 00205: val_loss did not improve from 1.04354\n",
            "240/240 - 0s - loss: 0.7199 - acc: 0.7292 - val_loss: 1.0813 - val_acc: 0.5319\n",
            "Epoch 206/300\n",
            "\n",
            "Epoch 00206: val_loss did not improve from 1.04354\n",
            "240/240 - 0s - loss: 0.7171 - acc: 0.7750 - val_loss: 1.0989 - val_acc: 0.4255\n",
            "Epoch 207/300\n",
            "\n",
            "Epoch 00207: val_loss did not improve from 1.04354\n",
            "240/240 - 0s - loss: 0.7337 - acc: 0.7458 - val_loss: 1.0830 - val_acc: 0.5106\n",
            "Epoch 208/300\n",
            "\n",
            "Epoch 00208: val_loss did not improve from 1.04354\n",
            "240/240 - 0s - loss: 0.6571 - acc: 0.7958 - val_loss: 1.0553 - val_acc: 0.5106\n",
            "Epoch 209/300\n",
            "\n",
            "Epoch 00209: val_loss did not improve from 1.04354\n",
            "240/240 - 0s - loss: 0.6975 - acc: 0.8000 - val_loss: 1.0951 - val_acc: 0.5319\n",
            "Epoch 210/300\n",
            "\n",
            "Epoch 00210: val_loss did not improve from 1.04354\n",
            "240/240 - 0s - loss: 0.6818 - acc: 0.7958 - val_loss: 1.0963 - val_acc: 0.4468\n",
            "Epoch 211/300\n",
            "\n",
            "Epoch 00211: val_loss did not improve from 1.04354\n",
            "240/240 - 0s - loss: 0.6785 - acc: 0.7833 - val_loss: 1.0653 - val_acc: 0.4894\n",
            "Epoch 212/300\n",
            "\n",
            "Epoch 00212: val_loss did not improve from 1.04354\n",
            "240/240 - 0s - loss: 0.6721 - acc: 0.7667 - val_loss: 1.0649 - val_acc: 0.5106\n",
            "Epoch 213/300\n",
            "\n",
            "Epoch 00213: val_loss improved from 1.04354 to 1.04141, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.6568 - acc: 0.7917 - val_loss: 1.0414 - val_acc: 0.4894\n",
            "Epoch 214/300\n",
            "\n",
            "Epoch 00214: val_loss improved from 1.04141 to 1.00505, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.7506 - acc: 0.7542 - val_loss: 1.0050 - val_acc: 0.5957\n",
            "Epoch 215/300\n",
            "\n",
            "Epoch 00215: val_loss did not improve from 1.00505\n",
            "240/240 - 0s - loss: 0.6563 - acc: 0.8042 - val_loss: 1.0228 - val_acc: 0.5745\n",
            "Epoch 216/300\n",
            "\n",
            "Epoch 00216: val_loss did not improve from 1.00505\n",
            "240/240 - 0s - loss: 0.6565 - acc: 0.8000 - val_loss: 1.0560 - val_acc: 0.4255\n",
            "Epoch 217/300\n",
            "\n",
            "Epoch 00217: val_loss did not improve from 1.00505\n",
            "240/240 - 0s - loss: 0.6870 - acc: 0.7875 - val_loss: 1.0402 - val_acc: 0.4681\n",
            "Epoch 218/300\n",
            "\n",
            "Epoch 00218: val_loss did not improve from 1.00505\n",
            "240/240 - 0s - loss: 0.6780 - acc: 0.7667 - val_loss: 1.0196 - val_acc: 0.4894\n",
            "Epoch 219/300\n",
            "\n",
            "Epoch 00219: val_loss improved from 1.00505 to 1.00317, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.6300 - acc: 0.8333 - val_loss: 1.0032 - val_acc: 0.5319\n",
            "Epoch 220/300\n",
            "\n",
            "Epoch 00220: val_loss did not improve from 1.00317\n",
            "240/240 - 0s - loss: 0.6375 - acc: 0.8042 - val_loss: 1.0296 - val_acc: 0.5532\n",
            "Epoch 221/300\n",
            "\n",
            "Epoch 00221: val_loss did not improve from 1.00317\n",
            "240/240 - 0s - loss: 0.6525 - acc: 0.7958 - val_loss: 1.0226 - val_acc: 0.5106\n",
            "Epoch 222/300\n",
            "\n",
            "Epoch 00222: val_loss did not improve from 1.00317\n",
            "240/240 - 0s - loss: 0.6514 - acc: 0.8125 - val_loss: 1.0226 - val_acc: 0.5106\n",
            "Epoch 223/300\n",
            "\n",
            "Epoch 00223: val_loss did not improve from 1.00317\n",
            "240/240 - 0s - loss: 0.6786 - acc: 0.8125 - val_loss: 1.0401 - val_acc: 0.5106\n",
            "Epoch 224/300\n",
            "\n",
            "Epoch 00224: val_loss did not improve from 1.00317\n",
            "240/240 - 0s - loss: 0.6936 - acc: 0.7583 - val_loss: 1.0096 - val_acc: 0.5319\n",
            "Epoch 225/300\n",
            "\n",
            "Epoch 00225: val_loss did not improve from 1.00317\n",
            "240/240 - 0s - loss: 0.6781 - acc: 0.7792 - val_loss: 1.0483 - val_acc: 0.4894\n",
            "Epoch 226/300\n",
            "\n",
            "Epoch 00226: val_loss did not improve from 1.00317\n",
            "240/240 - 0s - loss: 0.6504 - acc: 0.7833 - val_loss: 1.0407 - val_acc: 0.5319\n",
            "Epoch 227/300\n",
            "\n",
            "Epoch 00227: val_loss did not improve from 1.00317\n",
            "240/240 - 0s - loss: 0.6255 - acc: 0.8125 - val_loss: 1.0625 - val_acc: 0.4468\n",
            "Epoch 228/300\n",
            "\n",
            "Epoch 00228: val_loss did not improve from 1.00317\n",
            "240/240 - 0s - loss: 0.6535 - acc: 0.7750 - val_loss: 1.0355 - val_acc: 0.5745\n",
            "Epoch 229/300\n",
            "\n",
            "Epoch 00229: val_loss did not improve from 1.00317\n",
            "240/240 - 0s - loss: 0.6449 - acc: 0.7917 - val_loss: 1.0419 - val_acc: 0.5106\n",
            "Epoch 230/300\n",
            "\n",
            "Epoch 00230: val_loss did not improve from 1.00317\n",
            "240/240 - 0s - loss: 0.6975 - acc: 0.7208 - val_loss: 1.0622 - val_acc: 0.4681\n",
            "Epoch 231/300\n",
            "\n",
            "Epoch 00231: val_loss did not improve from 1.00317\n",
            "240/240 - 0s - loss: 0.6878 - acc: 0.7458 - val_loss: 1.0718 - val_acc: 0.5319\n",
            "Epoch 232/300\n",
            "\n",
            "Epoch 00232: val_loss did not improve from 1.00317\n",
            "240/240 - 0s - loss: 0.6771 - acc: 0.7708 - val_loss: 1.0795 - val_acc: 0.5319\n",
            "Epoch 233/300\n",
            "\n",
            "Epoch 00233: val_loss did not improve from 1.00317\n",
            "240/240 - 0s - loss: 0.6716 - acc: 0.8042 - val_loss: 1.0538 - val_acc: 0.4894\n",
            "Epoch 234/300\n",
            "\n",
            "Epoch 00234: val_loss did not improve from 1.00317\n",
            "240/240 - 0s - loss: 0.7169 - acc: 0.7333 - val_loss: 1.0836 - val_acc: 0.4043\n",
            "Epoch 235/300\n",
            "\n",
            "Epoch 00235: val_loss did not improve from 1.00317\n",
            "240/240 - 0s - loss: 0.6384 - acc: 0.7917 - val_loss: 1.0560 - val_acc: 0.4468\n",
            "Epoch 236/300\n",
            "\n",
            "Epoch 00236: val_loss did not improve from 1.00317\n",
            "240/240 - 0s - loss: 0.6564 - acc: 0.7792 - val_loss: 1.0232 - val_acc: 0.5319\n",
            "Epoch 237/300\n",
            "\n",
            "Epoch 00237: val_loss did not improve from 1.00317\n",
            "240/240 - 0s - loss: 0.6896 - acc: 0.7500 - val_loss: 1.0222 - val_acc: 0.5319\n",
            "Epoch 238/300\n",
            "\n",
            "Epoch 00238: val_loss did not improve from 1.00317\n",
            "240/240 - 0s - loss: 0.6501 - acc: 0.8083 - val_loss: 1.0457 - val_acc: 0.5319\n",
            "Epoch 239/300\n",
            "\n",
            "Epoch 00239: val_loss did not improve from 1.00317\n",
            "240/240 - 0s - loss: 0.6812 - acc: 0.7917 - val_loss: 1.0546 - val_acc: 0.5957\n",
            "Epoch 240/300\n",
            "\n",
            "Epoch 00240: val_loss did not improve from 1.00317\n",
            "240/240 - 0s - loss: 0.6744 - acc: 0.8208 - val_loss: 1.0435 - val_acc: 0.5106\n",
            "Epoch 241/300\n",
            "\n",
            "Epoch 00241: val_loss did not improve from 1.00317\n",
            "240/240 - 0s - loss: 0.6337 - acc: 0.7875 - val_loss: 1.0623 - val_acc: 0.5745\n",
            "Epoch 242/300\n",
            "\n",
            "Epoch 00242: val_loss did not improve from 1.00317\n",
            "240/240 - 0s - loss: 0.6583 - acc: 0.7792 - val_loss: 1.0599 - val_acc: 0.5532\n",
            "Epoch 243/300\n",
            "\n",
            "Epoch 00243: val_loss did not improve from 1.00317\n",
            "240/240 - 0s - loss: 0.6878 - acc: 0.7667 - val_loss: 1.0346 - val_acc: 0.4894\n",
            "Epoch 244/300\n",
            "\n",
            "Epoch 00244: val_loss did not improve from 1.00317\n",
            "240/240 - 0s - loss: 0.6482 - acc: 0.7500 - val_loss: 1.0334 - val_acc: 0.5319\n",
            "Epoch 245/300\n",
            "\n",
            "Epoch 00245: val_loss did not improve from 1.00317\n",
            "240/240 - 0s - loss: 0.6224 - acc: 0.8000 - val_loss: 1.0214 - val_acc: 0.5319\n",
            "Epoch 246/300\n",
            "\n",
            "Epoch 00246: val_loss did not improve from 1.00317\n",
            "240/240 - 0s - loss: 0.6543 - acc: 0.7667 - val_loss: 1.0251 - val_acc: 0.5319\n",
            "Epoch 247/300\n",
            "\n",
            "Epoch 00247: val_loss did not improve from 1.00317\n",
            "240/240 - 0s - loss: 0.6419 - acc: 0.7833 - val_loss: 1.0306 - val_acc: 0.5532\n",
            "Epoch 248/300\n",
            "\n",
            "Epoch 00248: val_loss did not improve from 1.00317\n",
            "240/240 - 0s - loss: 0.6460 - acc: 0.7917 - val_loss: 1.0232 - val_acc: 0.5106\n",
            "Epoch 249/300\n",
            "\n",
            "Epoch 00249: val_loss did not improve from 1.00317\n",
            "240/240 - 0s - loss: 0.6467 - acc: 0.7875 - val_loss: 1.0261 - val_acc: 0.4681\n",
            "Epoch 250/300\n",
            "\n",
            "Epoch 00250: val_loss did not improve from 1.00317\n",
            "240/240 - 0s - loss: 0.6988 - acc: 0.7750 - val_loss: 1.0707 - val_acc: 0.4894\n",
            "Epoch 251/300\n",
            "\n",
            "Epoch 00251: val_loss did not improve from 1.00317\n",
            "240/240 - 0s - loss: 0.6491 - acc: 0.7792 - val_loss: 1.0248 - val_acc: 0.5745\n",
            "Epoch 252/300\n",
            "\n",
            "Epoch 00252: val_loss did not improve from 1.00317\n",
            "240/240 - 0s - loss: 0.6572 - acc: 0.7500 - val_loss: 1.0339 - val_acc: 0.4894\n",
            "Epoch 253/300\n",
            "\n",
            "Epoch 00253: val_loss did not improve from 1.00317\n",
            "240/240 - 0s - loss: 0.6132 - acc: 0.8042 - val_loss: 1.0390 - val_acc: 0.5745\n",
            "Epoch 254/300\n",
            "\n",
            "Epoch 00254: val_loss did not improve from 1.00317\n",
            "240/240 - 0s - loss: 0.6496 - acc: 0.8083 - val_loss: 1.0342 - val_acc: 0.5106\n",
            "Epoch 255/300\n",
            "\n",
            "Epoch 00255: val_loss did not improve from 1.00317\n",
            "240/240 - 0s - loss: 0.6071 - acc: 0.8167 - val_loss: 1.0546 - val_acc: 0.4468\n",
            "Epoch 256/300\n",
            "\n",
            "Epoch 00256: val_loss did not improve from 1.00317\n",
            "240/240 - 0s - loss: 0.6331 - acc: 0.7917 - val_loss: 1.0248 - val_acc: 0.5957\n",
            "Epoch 257/300\n",
            "\n",
            "Epoch 00257: val_loss did not improve from 1.00317\n",
            "240/240 - 0s - loss: 0.6209 - acc: 0.7833 - val_loss: 1.0389 - val_acc: 0.5106\n",
            "Epoch 258/300\n",
            "\n",
            "Epoch 00258: val_loss did not improve from 1.00317\n",
            "240/240 - 0s - loss: 0.6398 - acc: 0.7792 - val_loss: 1.0137 - val_acc: 0.5319\n",
            "Epoch 259/300\n",
            "\n",
            "Epoch 00259: val_loss did not improve from 1.00317\n",
            "240/240 - 0s - loss: 0.6708 - acc: 0.7542 - val_loss: 1.0211 - val_acc: 0.5532\n",
            "Epoch 260/300\n",
            "\n",
            "Epoch 00260: val_loss did not improve from 1.00317\n",
            "240/240 - 0s - loss: 0.6168 - acc: 0.7833 - val_loss: 1.0591 - val_acc: 0.4681\n",
            "Epoch 261/300\n",
            "\n",
            "Epoch 00261: val_loss did not improve from 1.00317\n",
            "240/240 - 0s - loss: 0.6916 - acc: 0.7417 - val_loss: 1.0411 - val_acc: 0.4468\n",
            "Epoch 262/300\n",
            "\n",
            "Epoch 00262: val_loss improved from 1.00317 to 1.00102, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.6282 - acc: 0.7708 - val_loss: 1.0010 - val_acc: 0.5319\n",
            "Epoch 263/300\n",
            "\n",
            "Epoch 00263: val_loss did not improve from 1.00102\n",
            "240/240 - 0s - loss: 0.6150 - acc: 0.7917 - val_loss: 1.0347 - val_acc: 0.4894\n",
            "Epoch 264/300\n",
            "\n",
            "Epoch 00264: val_loss did not improve from 1.00102\n",
            "240/240 - 0s - loss: 0.5953 - acc: 0.8000 - val_loss: 1.0182 - val_acc: 0.5532\n",
            "Epoch 265/300\n",
            "\n",
            "Epoch 00265: val_loss did not improve from 1.00102\n",
            "240/240 - 0s - loss: 0.6370 - acc: 0.7750 - val_loss: 1.0338 - val_acc: 0.4894\n",
            "Epoch 266/300\n",
            "\n",
            "Epoch 00266: val_loss did not improve from 1.00102\n",
            "240/240 - 0s - loss: 0.6256 - acc: 0.8333 - val_loss: 1.0604 - val_acc: 0.5532\n",
            "Epoch 267/300\n",
            "\n",
            "Epoch 00267: val_loss did not improve from 1.00102\n",
            "240/240 - 0s - loss: 0.6709 - acc: 0.7583 - val_loss: 1.0731 - val_acc: 0.5532\n",
            "Epoch 268/300\n",
            "\n",
            "Epoch 00268: val_loss did not improve from 1.00102\n",
            "240/240 - 0s - loss: 0.6895 - acc: 0.7625 - val_loss: 1.0402 - val_acc: 0.4681\n",
            "Epoch 269/300\n",
            "\n",
            "Epoch 00269: val_loss did not improve from 1.00102\n",
            "240/240 - 0s - loss: 0.6251 - acc: 0.8083 - val_loss: 1.0430 - val_acc: 0.5106\n",
            "Epoch 270/300\n",
            "\n",
            "Epoch 00270: val_loss did not improve from 1.00102\n",
            "240/240 - 0s - loss: 0.6239 - acc: 0.7833 - val_loss: 1.0440 - val_acc: 0.5106\n",
            "Epoch 271/300\n",
            "\n",
            "Epoch 00271: val_loss did not improve from 1.00102\n",
            "240/240 - 0s - loss: 0.5880 - acc: 0.8250 - val_loss: 1.0121 - val_acc: 0.5319\n",
            "Epoch 272/300\n",
            "\n",
            "Epoch 00272: val_loss did not improve from 1.00102\n",
            "240/240 - 0s - loss: 0.6771 - acc: 0.7458 - val_loss: 1.0359 - val_acc: 0.4894\n",
            "Epoch 273/300\n",
            "\n",
            "Epoch 00273: val_loss did not improve from 1.00102\n",
            "240/240 - 0s - loss: 0.6628 - acc: 0.7792 - val_loss: 1.0042 - val_acc: 0.5319\n",
            "Epoch 274/300\n",
            "\n",
            "Epoch 00274: val_loss improved from 1.00102 to 0.99861, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.6493 - acc: 0.7708 - val_loss: 0.9986 - val_acc: 0.5106\n",
            "Epoch 275/300\n",
            "\n",
            "Epoch 00275: val_loss did not improve from 0.99861\n",
            "240/240 - 0s - loss: 0.6165 - acc: 0.7833 - val_loss: 1.0204 - val_acc: 0.4894\n",
            "Epoch 276/300\n",
            "\n",
            "Epoch 00276: val_loss did not improve from 0.99861\n",
            "240/240 - 0s - loss: 0.6657 - acc: 0.7792 - val_loss: 1.0323 - val_acc: 0.4681\n",
            "Epoch 277/300\n",
            "\n",
            "Epoch 00277: val_loss improved from 0.99861 to 0.98599, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.6391 - acc: 0.7833 - val_loss: 0.9860 - val_acc: 0.5532\n",
            "Epoch 278/300\n",
            "\n",
            "Epoch 00278: val_loss did not improve from 0.98599\n",
            "240/240 - 0s - loss: 0.6566 - acc: 0.7875 - val_loss: 1.0059 - val_acc: 0.4894\n",
            "Epoch 279/300\n",
            "\n",
            "Epoch 00279: val_loss did not improve from 0.98599\n",
            "240/240 - 0s - loss: 0.5858 - acc: 0.8375 - val_loss: 1.0130 - val_acc: 0.5319\n",
            "Epoch 280/300\n",
            "\n",
            "Epoch 00280: val_loss did not improve from 0.98599\n",
            "240/240 - 0s - loss: 0.6110 - acc: 0.7958 - val_loss: 1.0127 - val_acc: 0.5106\n",
            "Epoch 281/300\n",
            "\n",
            "Epoch 00281: val_loss did not improve from 0.98599\n",
            "240/240 - 0s - loss: 0.6281 - acc: 0.7833 - val_loss: 1.0523 - val_acc: 0.4681\n",
            "Epoch 282/300\n",
            "\n",
            "Epoch 00282: val_loss did not improve from 0.98599\n",
            "240/240 - 0s - loss: 0.6093 - acc: 0.7917 - val_loss: 1.0346 - val_acc: 0.5106\n",
            "Epoch 283/300\n",
            "\n",
            "Epoch 00283: val_loss did not improve from 0.98599\n",
            "240/240 - 0s - loss: 0.6185 - acc: 0.8042 - val_loss: 1.0233 - val_acc: 0.5319\n",
            "Epoch 284/300\n",
            "\n",
            "Epoch 00284: val_loss did not improve from 0.98599\n",
            "240/240 - 0s - loss: 0.6176 - acc: 0.7917 - val_loss: 1.0183 - val_acc: 0.5319\n",
            "Epoch 285/300\n",
            "\n",
            "Epoch 00285: val_loss did not improve from 0.98599\n",
            "240/240 - 0s - loss: 0.6237 - acc: 0.7917 - val_loss: 1.0150 - val_acc: 0.4681\n",
            "Epoch 286/300\n",
            "\n",
            "Epoch 00286: val_loss did not improve from 0.98599\n",
            "240/240 - 0s - loss: 0.6590 - acc: 0.7583 - val_loss: 1.0229 - val_acc: 0.5957\n",
            "Epoch 287/300\n",
            "\n",
            "Epoch 00287: val_loss did not improve from 0.98599\n",
            "240/240 - 0s - loss: 0.6115 - acc: 0.8208 - val_loss: 1.0457 - val_acc: 0.4681\n",
            "Epoch 288/300\n",
            "\n",
            "Epoch 00288: val_loss did not improve from 0.98599\n",
            "240/240 - 0s - loss: 0.6206 - acc: 0.8083 - val_loss: 1.0529 - val_acc: 0.5532\n",
            "Epoch 289/300\n",
            "\n",
            "Epoch 00289: val_loss did not improve from 0.98599\n",
            "240/240 - 0s - loss: 0.6460 - acc: 0.7792 - val_loss: 1.0431 - val_acc: 0.5106\n",
            "Epoch 290/300\n",
            "\n",
            "Epoch 00290: val_loss did not improve from 0.98599\n",
            "240/240 - 0s - loss: 0.6024 - acc: 0.7792 - val_loss: 1.0426 - val_acc: 0.5745\n",
            "Epoch 291/300\n",
            "\n",
            "Epoch 00291: val_loss did not improve from 0.98599\n",
            "240/240 - 0s - loss: 0.6336 - acc: 0.7875 - val_loss: 1.0605 - val_acc: 0.4894\n",
            "Epoch 292/300\n",
            "\n",
            "Epoch 00292: val_loss did not improve from 0.98599\n",
            "240/240 - 0s - loss: 0.5871 - acc: 0.8167 - val_loss: 1.0369 - val_acc: 0.4894\n",
            "Epoch 293/300\n",
            "\n",
            "Epoch 00293: val_loss did not improve from 0.98599\n",
            "240/240 - 0s - loss: 0.5959 - acc: 0.7958 - val_loss: 1.0576 - val_acc: 0.4681\n",
            "Epoch 294/300\n",
            "\n",
            "Epoch 00294: val_loss did not improve from 0.98599\n",
            "240/240 - 0s - loss: 0.6097 - acc: 0.7833 - val_loss: 1.0154 - val_acc: 0.5319\n",
            "Epoch 295/300\n",
            "\n",
            "Epoch 00295: val_loss did not improve from 0.98599\n",
            "240/240 - 0s - loss: 0.6716 - acc: 0.7583 - val_loss: 1.0321 - val_acc: 0.6383\n",
            "Epoch 296/300\n",
            "\n",
            "Epoch 00296: val_loss did not improve from 0.98599\n",
            "240/240 - 0s - loss: 0.6367 - acc: 0.7792 - val_loss: 1.0226 - val_acc: 0.5106\n",
            "Epoch 297/300\n",
            "\n",
            "Epoch 00297: val_loss did not improve from 0.98599\n",
            "240/240 - 0s - loss: 0.6240 - acc: 0.7875 - val_loss: 1.0169 - val_acc: 0.5319\n",
            "Epoch 298/300\n",
            "\n",
            "Epoch 00298: val_loss did not improve from 0.98599\n",
            "240/240 - 0s - loss: 0.6089 - acc: 0.7792 - val_loss: 1.0439 - val_acc: 0.4681\n",
            "Epoch 299/300\n",
            "\n",
            "Epoch 00299: val_loss did not improve from 0.98599\n",
            "240/240 - 0s - loss: 0.5809 - acc: 0.8333 - val_loss: 1.0139 - val_acc: 0.4894\n",
            "Epoch 300/300\n",
            "\n",
            "Epoch 00300: val_loss did not improve from 0.98599\n",
            "240/240 - 0s - loss: 0.5930 - acc: 0.7875 - val_loss: 1.0274 - val_acc: 0.5532\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "# Evaluate on test data\n",
            "288/288 [==============================] - 1s 2ms/sample - loss: 1.1775 - acc: 0.4792\n",
            "test loss, test acc: [1.1774940102639246, 0.47916666]\n",
            "EEG_Deep/Data2A/Data_A03T.mat\n",
            "(288, 22, 1875)\n",
            "(288, 1)\n",
            "(240, 22, 750)\n",
            "(240, 1)\n",
            "(47, 22, 750)\n",
            "(47, 1)\n",
            "X_train shape: (240, 1, 22, 750)\n",
            "240 train samples\n",
            "47 val samples\n",
            "EEG_Deep/Data2A/Data_A03E.mat\n",
            "(288, 22, 1875)\n",
            "(288, 1)\n",
            "(288, 22, 750)\n",
            "(288, 1)\n",
            "X_train shape: (288, 1, 22, 750)\n",
            "288 train samples\n",
            "Train on 240 samples, validate on 47 samples\n",
            "Epoch 1/300\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 1.38368, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 1s - loss: 1.3929 - acc: 0.2583 - val_loss: 1.3837 - val_acc: 0.2340\n",
            "Epoch 2/300\n",
            "\n",
            "Epoch 00002: val_loss improved from 1.38368 to 1.38258, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.3724 - acc: 0.2958 - val_loss: 1.3826 - val_acc: 0.2979\n",
            "Epoch 3/300\n",
            "\n",
            "Epoch 00003: val_loss improved from 1.38258 to 1.38031, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.3575 - acc: 0.3042 - val_loss: 1.3803 - val_acc: 0.3404\n",
            "Epoch 4/300\n",
            "\n",
            "Epoch 00004: val_loss improved from 1.38031 to 1.37458, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.3122 - acc: 0.4167 - val_loss: 1.3746 - val_acc: 0.3617\n",
            "Epoch 5/300\n",
            "\n",
            "Epoch 00005: val_loss improved from 1.37458 to 1.36330, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.2821 - acc: 0.4708 - val_loss: 1.3633 - val_acc: 0.4894\n",
            "Epoch 6/300\n",
            "\n",
            "Epoch 00006: val_loss improved from 1.36330 to 1.34481, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.2121 - acc: 0.5500 - val_loss: 1.3448 - val_acc: 0.5532\n",
            "Epoch 7/300\n",
            "\n",
            "Epoch 00007: val_loss improved from 1.34481 to 1.31862, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.1636 - acc: 0.5500 - val_loss: 1.3186 - val_acc: 0.4255\n",
            "Epoch 8/300\n",
            "\n",
            "Epoch 00008: val_loss improved from 1.31862 to 1.31784, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.1495 - acc: 0.5083 - val_loss: 1.3178 - val_acc: 0.3404\n",
            "Epoch 9/300\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 1.31784\n",
            "240/240 - 0s - loss: 1.0914 - acc: 0.5292 - val_loss: 1.3277 - val_acc: 0.2766\n",
            "Epoch 10/300\n",
            "\n",
            "Epoch 00010: val_loss improved from 1.31784 to 1.29733, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.0409 - acc: 0.6083 - val_loss: 1.2973 - val_acc: 0.3191\n",
            "Epoch 11/300\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 1.29733\n",
            "240/240 - 0s - loss: 0.9946 - acc: 0.6292 - val_loss: 1.3310 - val_acc: 0.2766\n",
            "Epoch 12/300\n",
            "\n",
            "Epoch 00012: val_loss improved from 1.29733 to 1.27444, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.9712 - acc: 0.6417 - val_loss: 1.2744 - val_acc: 0.4043\n",
            "Epoch 13/300\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 1.27444\n",
            "240/240 - 0s - loss: 0.9277 - acc: 0.6958 - val_loss: 1.2983 - val_acc: 0.4043\n",
            "Epoch 14/300\n",
            "\n",
            "Epoch 00014: val_loss improved from 1.27444 to 1.22679, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.9095 - acc: 0.6625 - val_loss: 1.2268 - val_acc: 0.4468\n",
            "Epoch 15/300\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 1.22679\n",
            "240/240 - 0s - loss: 0.8749 - acc: 0.6958 - val_loss: 1.3142 - val_acc: 0.4043\n",
            "Epoch 16/300\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 1.22679\n",
            "240/240 - 0s - loss: 0.8655 - acc: 0.6833 - val_loss: 1.2727 - val_acc: 0.4255\n",
            "Epoch 17/300\n",
            "\n",
            "Epoch 00017: val_loss improved from 1.22679 to 1.21973, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.8271 - acc: 0.7250 - val_loss: 1.2197 - val_acc: 0.4255\n",
            "Epoch 18/300\n",
            "\n",
            "Epoch 00018: val_loss improved from 1.21973 to 1.21619, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.8458 - acc: 0.6917 - val_loss: 1.2162 - val_acc: 0.4894\n",
            "Epoch 19/300\n",
            "\n",
            "Epoch 00019: val_loss improved from 1.21619 to 1.13632, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.8531 - acc: 0.7083 - val_loss: 1.1363 - val_acc: 0.4894\n",
            "Epoch 20/300\n",
            "\n",
            "Epoch 00020: val_loss improved from 1.13632 to 1.05383, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.8303 - acc: 0.6958 - val_loss: 1.0538 - val_acc: 0.5106\n",
            "Epoch 21/300\n",
            "\n",
            "Epoch 00021: val_loss improved from 1.05383 to 1.01480, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.7623 - acc: 0.7667 - val_loss: 1.0148 - val_acc: 0.5532\n",
            "Epoch 22/300\n",
            "\n",
            "Epoch 00022: val_loss improved from 1.01480 to 0.93149, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.7970 - acc: 0.7083 - val_loss: 0.9315 - val_acc: 0.6383\n",
            "Epoch 23/300\n",
            "\n",
            "Epoch 00023: val_loss improved from 0.93149 to 0.90435, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.7728 - acc: 0.7292 - val_loss: 0.9043 - val_acc: 0.6809\n",
            "Epoch 24/300\n",
            "\n",
            "Epoch 00024: val_loss improved from 0.90435 to 0.88844, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.7547 - acc: 0.7542 - val_loss: 0.8884 - val_acc: 0.6383\n",
            "Epoch 25/300\n",
            "\n",
            "Epoch 00025: val_loss improved from 0.88844 to 0.88556, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.7467 - acc: 0.7000 - val_loss: 0.8856 - val_acc: 0.6809\n",
            "Epoch 26/300\n",
            "\n",
            "Epoch 00026: val_loss improved from 0.88556 to 0.86117, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.7031 - acc: 0.7625 - val_loss: 0.8612 - val_acc: 0.7021\n",
            "Epoch 27/300\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.86117\n",
            "240/240 - 0s - loss: 0.7679 - acc: 0.7292 - val_loss: 0.8791 - val_acc: 0.5957\n",
            "Epoch 28/300\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.86117\n",
            "240/240 - 0s - loss: 0.7341 - acc: 0.7583 - val_loss: 0.9299 - val_acc: 0.5745\n",
            "Epoch 29/300\n",
            "\n",
            "Epoch 00029: val_loss improved from 0.86117 to 0.85849, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.7348 - acc: 0.7583 - val_loss: 0.8585 - val_acc: 0.6170\n",
            "Epoch 30/300\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.85849\n",
            "240/240 - 0s - loss: 0.7322 - acc: 0.7625 - val_loss: 0.8664 - val_acc: 0.6170\n",
            "Epoch 31/300\n",
            "\n",
            "Epoch 00031: val_loss improved from 0.85849 to 0.82147, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.7330 - acc: 0.7583 - val_loss: 0.8215 - val_acc: 0.6596\n",
            "Epoch 32/300\n",
            "\n",
            "Epoch 00032: val_loss improved from 0.82147 to 0.81425, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.7125 - acc: 0.7708 - val_loss: 0.8143 - val_acc: 0.7021\n",
            "Epoch 33/300\n",
            "\n",
            "Epoch 00033: val_loss improved from 0.81425 to 0.78556, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.6878 - acc: 0.7875 - val_loss: 0.7856 - val_acc: 0.7021\n",
            "Epoch 34/300\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.78556\n",
            "240/240 - 0s - loss: 0.7170 - acc: 0.7708 - val_loss: 0.7912 - val_acc: 0.6809\n",
            "Epoch 35/300\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.78556\n",
            "240/240 - 0s - loss: 0.6844 - acc: 0.7750 - val_loss: 0.8130 - val_acc: 0.7021\n",
            "Epoch 36/300\n",
            "\n",
            "Epoch 00036: val_loss improved from 0.78556 to 0.77865, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.6727 - acc: 0.7833 - val_loss: 0.7786 - val_acc: 0.6809\n",
            "Epoch 37/300\n",
            "\n",
            "Epoch 00037: val_loss improved from 0.77865 to 0.75545, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.6815 - acc: 0.7583 - val_loss: 0.7555 - val_acc: 0.7021\n",
            "Epoch 38/300\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.75545\n",
            "240/240 - 0s - loss: 0.6859 - acc: 0.7708 - val_loss: 0.7838 - val_acc: 0.7447\n",
            "Epoch 39/300\n",
            "\n",
            "Epoch 00039: val_loss improved from 0.75545 to 0.74885, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.6640 - acc: 0.7542 - val_loss: 0.7489 - val_acc: 0.7234\n",
            "Epoch 40/300\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.74885\n",
            "240/240 - 0s - loss: 0.6442 - acc: 0.7792 - val_loss: 0.8598 - val_acc: 0.6383\n",
            "Epoch 41/300\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.74885\n",
            "240/240 - 0s - loss: 0.7055 - acc: 0.7667 - val_loss: 0.7889 - val_acc: 0.6809\n",
            "Epoch 42/300\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.74885\n",
            "240/240 - 0s - loss: 0.6769 - acc: 0.7792 - val_loss: 0.8000 - val_acc: 0.7021\n",
            "Epoch 43/300\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.74885\n",
            "240/240 - 0s - loss: 0.6475 - acc: 0.7750 - val_loss: 0.7801 - val_acc: 0.7021\n",
            "Epoch 44/300\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.74885\n",
            "240/240 - 0s - loss: 0.6724 - acc: 0.7583 - val_loss: 0.7562 - val_acc: 0.7021\n",
            "Epoch 45/300\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.74885\n",
            "240/240 - 0s - loss: 0.6535 - acc: 0.7667 - val_loss: 0.7528 - val_acc: 0.6809\n",
            "Epoch 46/300\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.74885\n",
            "240/240 - 0s - loss: 0.6786 - acc: 0.7542 - val_loss: 0.8077 - val_acc: 0.7234\n",
            "Epoch 47/300\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.74885\n",
            "240/240 - 0s - loss: 0.6168 - acc: 0.8000 - val_loss: 0.8080 - val_acc: 0.6596\n",
            "Epoch 48/300\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.74885\n",
            "240/240 - 0s - loss: 0.6432 - acc: 0.8125 - val_loss: 0.7945 - val_acc: 0.6596\n",
            "Epoch 49/300\n",
            "\n",
            "Epoch 00049: val_loss improved from 0.74885 to 0.73905, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.6490 - acc: 0.7750 - val_loss: 0.7390 - val_acc: 0.7021\n",
            "Epoch 50/300\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.73905\n",
            "240/240 - 0s - loss: 0.6555 - acc: 0.7792 - val_loss: 0.7666 - val_acc: 0.7021\n",
            "Epoch 51/300\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.73905\n",
            "240/240 - 0s - loss: 0.6083 - acc: 0.7792 - val_loss: 0.7427 - val_acc: 0.7021\n",
            "Epoch 52/300\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.73905\n",
            "240/240 - 0s - loss: 0.6104 - acc: 0.7958 - val_loss: 0.8076 - val_acc: 0.6809\n",
            "Epoch 53/300\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.73905\n",
            "240/240 - 0s - loss: 0.6297 - acc: 0.7833 - val_loss: 0.7897 - val_acc: 0.6809\n",
            "Epoch 54/300\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.73905\n",
            "240/240 - 0s - loss: 0.6387 - acc: 0.7667 - val_loss: 0.7709 - val_acc: 0.7021\n",
            "Epoch 55/300\n",
            "\n",
            "Epoch 00055: val_loss improved from 0.73905 to 0.73726, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.6223 - acc: 0.8125 - val_loss: 0.7373 - val_acc: 0.7234\n",
            "Epoch 56/300\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.73726\n",
            "240/240 - 0s - loss: 0.6324 - acc: 0.7875 - val_loss: 0.7513 - val_acc: 0.7234\n",
            "Epoch 57/300\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.73726\n",
            "240/240 - 0s - loss: 0.5949 - acc: 0.7833 - val_loss: 0.7718 - val_acc: 0.7021\n",
            "Epoch 58/300\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.73726\n",
            "240/240 - 0s - loss: 0.6202 - acc: 0.7958 - val_loss: 0.7446 - val_acc: 0.7021\n",
            "Epoch 59/300\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.73726\n",
            "240/240 - 0s - loss: 0.6013 - acc: 0.8083 - val_loss: 0.7427 - val_acc: 0.7447\n",
            "Epoch 60/300\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.73726\n",
            "240/240 - 0s - loss: 0.5934 - acc: 0.8000 - val_loss: 0.7695 - val_acc: 0.7234\n",
            "Epoch 61/300\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.73726\n",
            "240/240 - 0s - loss: 0.6102 - acc: 0.7958 - val_loss: 0.7762 - val_acc: 0.7234\n",
            "Epoch 62/300\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.73726\n",
            "240/240 - 0s - loss: 0.6142 - acc: 0.8000 - val_loss: 0.7377 - val_acc: 0.7021\n",
            "Epoch 63/300\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.73726\n",
            "240/240 - 0s - loss: 0.6029 - acc: 0.7958 - val_loss: 0.7544 - val_acc: 0.7021\n",
            "Epoch 64/300\n",
            "\n",
            "Epoch 00064: val_loss improved from 0.73726 to 0.71179, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.5805 - acc: 0.8167 - val_loss: 0.7118 - val_acc: 0.7021\n",
            "Epoch 65/300\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.71179\n",
            "240/240 - 0s - loss: 0.5898 - acc: 0.8167 - val_loss: 0.7957 - val_acc: 0.6809\n",
            "Epoch 66/300\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.71179\n",
            "240/240 - 0s - loss: 0.5977 - acc: 0.7875 - val_loss: 0.7554 - val_acc: 0.6809\n",
            "Epoch 67/300\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.71179\n",
            "240/240 - 0s - loss: 0.6096 - acc: 0.7875 - val_loss: 0.7274 - val_acc: 0.7021\n",
            "Epoch 68/300\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.71179\n",
            "240/240 - 0s - loss: 0.6152 - acc: 0.7833 - val_loss: 0.7826 - val_acc: 0.7021\n",
            "Epoch 69/300\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.71179\n",
            "240/240 - 0s - loss: 0.5792 - acc: 0.7875 - val_loss: 0.7835 - val_acc: 0.6596\n",
            "Epoch 70/300\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.71179\n",
            "240/240 - 0s - loss: 0.5787 - acc: 0.8125 - val_loss: 0.7144 - val_acc: 0.6809\n",
            "Epoch 71/300\n",
            "\n",
            "Epoch 00071: val_loss improved from 0.71179 to 0.68214, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.5726 - acc: 0.7958 - val_loss: 0.6821 - val_acc: 0.7447\n",
            "Epoch 72/300\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.68214\n",
            "240/240 - 0s - loss: 0.6068 - acc: 0.7792 - val_loss: 0.7302 - val_acc: 0.7447\n",
            "Epoch 73/300\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.68214\n",
            "240/240 - 0s - loss: 0.5599 - acc: 0.8292 - val_loss: 0.7277 - val_acc: 0.7447\n",
            "Epoch 74/300\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.68214\n",
            "240/240 - 0s - loss: 0.5632 - acc: 0.8208 - val_loss: 0.7659 - val_acc: 0.6809\n",
            "Epoch 75/300\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.68214\n",
            "240/240 - 0s - loss: 0.5696 - acc: 0.8083 - val_loss: 0.7290 - val_acc: 0.7021\n",
            "Epoch 76/300\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.68214\n",
            "240/240 - 0s - loss: 0.5868 - acc: 0.8000 - val_loss: 0.6906 - val_acc: 0.7234\n",
            "Epoch 77/300\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.68214\n",
            "240/240 - 0s - loss: 0.5717 - acc: 0.8125 - val_loss: 0.7467 - val_acc: 0.7234\n",
            "Epoch 78/300\n",
            "\n",
            "Epoch 00078: val_loss improved from 0.68214 to 0.67247, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.5744 - acc: 0.7917 - val_loss: 0.6725 - val_acc: 0.7447\n",
            "Epoch 79/300\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.67247\n",
            "240/240 - 0s - loss: 0.5758 - acc: 0.7875 - val_loss: 0.7877 - val_acc: 0.6809\n",
            "Epoch 80/300\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.67247\n",
            "240/240 - 0s - loss: 0.5541 - acc: 0.8208 - val_loss: 0.7349 - val_acc: 0.7447\n",
            "Epoch 81/300\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.67247\n",
            "240/240 - 0s - loss: 0.5772 - acc: 0.8042 - val_loss: 0.6892 - val_acc: 0.7234\n",
            "Epoch 82/300\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.67247\n",
            "240/240 - 0s - loss: 0.5409 - acc: 0.8375 - val_loss: 0.7929 - val_acc: 0.7234\n",
            "Epoch 83/300\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.67247\n",
            "240/240 - 0s - loss: 0.5846 - acc: 0.8167 - val_loss: 0.7999 - val_acc: 0.7447\n",
            "Epoch 84/300\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.67247\n",
            "240/240 - 0s - loss: 0.5780 - acc: 0.8083 - val_loss: 0.7010 - val_acc: 0.7234\n",
            "Epoch 85/300\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.67247\n",
            "240/240 - 0s - loss: 0.5070 - acc: 0.8417 - val_loss: 0.8640 - val_acc: 0.7021\n",
            "Epoch 86/300\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.67247\n",
            "240/240 - 0s - loss: 0.5356 - acc: 0.8375 - val_loss: 0.7001 - val_acc: 0.7660\n",
            "Epoch 87/300\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.67247\n",
            "240/240 - 0s - loss: 0.5427 - acc: 0.8292 - val_loss: 0.7933 - val_acc: 0.7234\n",
            "Epoch 88/300\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.67247\n",
            "240/240 - 0s - loss: 0.5369 - acc: 0.8250 - val_loss: 0.7301 - val_acc: 0.7660\n",
            "Epoch 89/300\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.67247\n",
            "240/240 - 0s - loss: 0.5297 - acc: 0.8333 - val_loss: 0.6833 - val_acc: 0.7872\n",
            "Epoch 90/300\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.67247\n",
            "240/240 - 0s - loss: 0.5477 - acc: 0.8167 - val_loss: 0.7019 - val_acc: 0.7660\n",
            "Epoch 91/300\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.67247\n",
            "240/240 - 0s - loss: 0.4922 - acc: 0.8667 - val_loss: 0.8242 - val_acc: 0.6596\n",
            "Epoch 92/300\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.67247\n",
            "240/240 - 0s - loss: 0.4774 - acc: 0.8667 - val_loss: 0.7066 - val_acc: 0.8085\n",
            "Epoch 93/300\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.67247\n",
            "240/240 - 0s - loss: 0.5331 - acc: 0.8042 - val_loss: 0.7078 - val_acc: 0.7660\n",
            "Epoch 94/300\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.67247\n",
            "240/240 - 0s - loss: 0.5270 - acc: 0.8333 - val_loss: 0.6783 - val_acc: 0.7872\n",
            "Epoch 95/300\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.67247\n",
            "240/240 - 0s - loss: 0.5600 - acc: 0.8333 - val_loss: 0.7537 - val_acc: 0.7660\n",
            "Epoch 96/300\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.67247\n",
            "240/240 - 0s - loss: 0.5190 - acc: 0.8458 - val_loss: 0.7076 - val_acc: 0.7234\n",
            "Epoch 97/300\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.67247\n",
            "240/240 - 0s - loss: 0.5203 - acc: 0.8292 - val_loss: 0.6923 - val_acc: 0.7660\n",
            "Epoch 98/300\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.67247\n",
            "240/240 - 0s - loss: 0.5752 - acc: 0.7625 - val_loss: 0.6746 - val_acc: 0.7872\n",
            "Epoch 99/300\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.67247\n",
            "240/240 - 0s - loss: 0.5077 - acc: 0.8250 - val_loss: 0.7492 - val_acc: 0.7660\n",
            "Epoch 100/300\n",
            "\n",
            "Epoch 00100: val_loss improved from 0.67247 to 0.64905, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.5533 - acc: 0.8083 - val_loss: 0.6490 - val_acc: 0.7660\n",
            "Epoch 101/300\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.64905\n",
            "240/240 - 0s - loss: 0.4670 - acc: 0.8833 - val_loss: 0.6815 - val_acc: 0.7660\n",
            "Epoch 102/300\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.64905\n",
            "240/240 - 0s - loss: 0.5212 - acc: 0.8292 - val_loss: 0.7599 - val_acc: 0.7234\n",
            "Epoch 103/300\n",
            "\n",
            "Epoch 00103: val_loss improved from 0.64905 to 0.62788, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.5040 - acc: 0.8458 - val_loss: 0.6279 - val_acc: 0.7447\n",
            "Epoch 104/300\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.62788\n",
            "240/240 - 0s - loss: 0.4959 - acc: 0.8583 - val_loss: 0.7336 - val_acc: 0.7234\n",
            "Epoch 105/300\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 0.62788\n",
            "240/240 - 0s - loss: 0.4833 - acc: 0.8625 - val_loss: 0.7231 - val_acc: 0.7660\n",
            "Epoch 106/300\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 0.62788\n",
            "240/240 - 0s - loss: 0.5207 - acc: 0.8167 - val_loss: 0.7232 - val_acc: 0.7234\n",
            "Epoch 107/300\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.62788\n",
            "240/240 - 0s - loss: 0.5154 - acc: 0.8167 - val_loss: 0.7142 - val_acc: 0.7660\n",
            "Epoch 108/300\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 0.62788\n",
            "240/240 - 0s - loss: 0.5503 - acc: 0.8083 - val_loss: 0.6572 - val_acc: 0.8298\n",
            "Epoch 109/300\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.62788\n",
            "240/240 - 0s - loss: 0.4894 - acc: 0.8458 - val_loss: 0.7618 - val_acc: 0.7660\n",
            "Epoch 110/300\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.62788\n",
            "240/240 - 0s - loss: 0.5101 - acc: 0.8292 - val_loss: 0.6793 - val_acc: 0.8085\n",
            "Epoch 111/300\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 0.62788\n",
            "240/240 - 0s - loss: 0.4823 - acc: 0.8375 - val_loss: 0.7223 - val_acc: 0.7660\n",
            "Epoch 112/300\n",
            "\n",
            "Epoch 00112: val_loss improved from 0.62788 to 0.62275, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.4694 - acc: 0.8417 - val_loss: 0.6227 - val_acc: 0.7660\n",
            "Epoch 113/300\n",
            "\n",
            "Epoch 00113: val_loss improved from 0.62275 to 0.60881, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.4464 - acc: 0.8875 - val_loss: 0.6088 - val_acc: 0.7447\n",
            "Epoch 114/300\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.60881\n",
            "240/240 - 0s - loss: 0.4796 - acc: 0.8542 - val_loss: 0.6654 - val_acc: 0.7660\n",
            "Epoch 115/300\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.60881\n",
            "240/240 - 0s - loss: 0.4905 - acc: 0.8500 - val_loss: 0.6762 - val_acc: 0.7660\n",
            "Epoch 116/300\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.60881\n",
            "240/240 - 0s - loss: 0.4544 - acc: 0.8750 - val_loss: 0.6739 - val_acc: 0.8085\n",
            "Epoch 117/300\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.60881\n",
            "240/240 - 0s - loss: 0.5242 - acc: 0.8375 - val_loss: 0.6650 - val_acc: 0.7872\n",
            "Epoch 118/300\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.60881\n",
            "240/240 - 0s - loss: 0.4517 - acc: 0.8833 - val_loss: 0.7345 - val_acc: 0.7660\n",
            "Epoch 119/300\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 0.60881\n",
            "240/240 - 0s - loss: 0.4606 - acc: 0.8500 - val_loss: 0.6217 - val_acc: 0.8298\n",
            "Epoch 120/300\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.60881\n",
            "240/240 - 0s - loss: 0.4600 - acc: 0.8667 - val_loss: 0.7261 - val_acc: 0.7660\n",
            "Epoch 121/300\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.60881\n",
            "240/240 - 0s - loss: 0.4954 - acc: 0.8292 - val_loss: 0.6284 - val_acc: 0.8085\n",
            "Epoch 122/300\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.60881\n",
            "240/240 - 0s - loss: 0.4328 - acc: 0.8625 - val_loss: 0.7166 - val_acc: 0.7447\n",
            "Epoch 123/300\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 0.60881\n",
            "240/240 - 0s - loss: 0.4810 - acc: 0.8417 - val_loss: 0.6160 - val_acc: 0.8298\n",
            "Epoch 124/300\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 0.60881\n",
            "240/240 - 0s - loss: 0.4827 - acc: 0.8458 - val_loss: 0.6846 - val_acc: 0.7660\n",
            "Epoch 125/300\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 0.60881\n",
            "240/240 - 0s - loss: 0.4420 - acc: 0.8625 - val_loss: 0.7002 - val_acc: 0.7447\n",
            "Epoch 126/300\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 0.60881\n",
            "240/240 - 0s - loss: 0.4493 - acc: 0.9000 - val_loss: 0.7293 - val_acc: 0.7872\n",
            "Epoch 127/300\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 0.60881\n",
            "240/240 - 0s - loss: 0.4894 - acc: 0.8375 - val_loss: 0.6370 - val_acc: 0.7660\n",
            "Epoch 128/300\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 0.60881\n",
            "240/240 - 0s - loss: 0.4545 - acc: 0.8625 - val_loss: 0.6461 - val_acc: 0.7872\n",
            "Epoch 129/300\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.60881\n",
            "240/240 - 0s - loss: 0.4398 - acc: 0.8542 - val_loss: 0.6386 - val_acc: 0.7660\n",
            "Epoch 130/300\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 0.60881\n",
            "240/240 - 0s - loss: 0.4541 - acc: 0.8833 - val_loss: 0.6957 - val_acc: 0.7872\n",
            "Epoch 131/300\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 0.60881\n",
            "240/240 - 0s - loss: 0.4468 - acc: 0.8958 - val_loss: 0.7027 - val_acc: 0.7660\n",
            "Epoch 132/300\n",
            "\n",
            "Epoch 00132: val_loss improved from 0.60881 to 0.58035, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.4515 - acc: 0.8625 - val_loss: 0.5803 - val_acc: 0.8298\n",
            "Epoch 133/300\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.58035\n",
            "240/240 - 0s - loss: 0.4605 - acc: 0.8750 - val_loss: 0.5935 - val_acc: 0.7872\n",
            "Epoch 134/300\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.58035\n",
            "240/240 - 0s - loss: 0.4785 - acc: 0.8500 - val_loss: 0.6316 - val_acc: 0.8085\n",
            "Epoch 135/300\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 0.58035\n",
            "240/240 - 0s - loss: 0.4148 - acc: 0.8708 - val_loss: 0.6777 - val_acc: 0.7872\n",
            "Epoch 136/300\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.58035\n",
            "240/240 - 0s - loss: 0.4089 - acc: 0.8833 - val_loss: 0.6234 - val_acc: 0.8085\n",
            "Epoch 137/300\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 0.58035\n",
            "240/240 - 0s - loss: 0.4356 - acc: 0.8958 - val_loss: 0.6132 - val_acc: 0.7872\n",
            "Epoch 138/300\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.58035\n",
            "240/240 - 0s - loss: 0.4605 - acc: 0.8333 - val_loss: 0.6779 - val_acc: 0.7660\n",
            "Epoch 139/300\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.58035\n",
            "240/240 - 0s - loss: 0.4640 - acc: 0.8625 - val_loss: 0.6586 - val_acc: 0.8085\n",
            "Epoch 140/300\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 0.58035\n",
            "240/240 - 0s - loss: 0.4652 - acc: 0.8417 - val_loss: 0.6360 - val_acc: 0.8085\n",
            "Epoch 141/300\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.58035\n",
            "240/240 - 0s - loss: 0.4339 - acc: 0.8708 - val_loss: 0.6651 - val_acc: 0.7872\n",
            "Epoch 142/300\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.58035\n",
            "240/240 - 0s - loss: 0.4284 - acc: 0.8625 - val_loss: 0.6100 - val_acc: 0.7660\n",
            "Epoch 143/300\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 0.58035\n",
            "240/240 - 0s - loss: 0.4647 - acc: 0.8750 - val_loss: 0.6604 - val_acc: 0.7660\n",
            "Epoch 144/300\n",
            "\n",
            "Epoch 00144: val_loss improved from 0.58035 to 0.56983, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.4506 - acc: 0.8708 - val_loss: 0.5698 - val_acc: 0.7872\n",
            "Epoch 145/300\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.56983\n",
            "240/240 - 0s - loss: 0.4568 - acc: 0.8625 - val_loss: 0.6546 - val_acc: 0.7660\n",
            "Epoch 146/300\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.56983\n",
            "240/240 - 0s - loss: 0.4259 - acc: 0.8625 - val_loss: 0.6176 - val_acc: 0.7660\n",
            "Epoch 147/300\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.56983\n",
            "240/240 - 0s - loss: 0.4309 - acc: 0.8792 - val_loss: 0.6207 - val_acc: 0.7872\n",
            "Epoch 148/300\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 0.56983\n",
            "240/240 - 0s - loss: 0.3931 - acc: 0.9083 - val_loss: 0.6628 - val_acc: 0.7660\n",
            "Epoch 149/300\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.56983\n",
            "240/240 - 0s - loss: 0.4347 - acc: 0.8333 - val_loss: 0.5965 - val_acc: 0.8085\n",
            "Epoch 150/300\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.56983\n",
            "240/240 - 0s - loss: 0.4473 - acc: 0.8583 - val_loss: 0.6137 - val_acc: 0.7872\n",
            "Epoch 151/300\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.56983\n",
            "240/240 - 0s - loss: 0.4439 - acc: 0.8708 - val_loss: 0.6255 - val_acc: 0.8085\n",
            "Epoch 152/300\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.56983\n",
            "240/240 - 0s - loss: 0.4352 - acc: 0.8667 - val_loss: 0.6009 - val_acc: 0.8085\n",
            "Epoch 153/300\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 0.56983\n",
            "240/240 - 0s - loss: 0.4260 - acc: 0.8542 - val_loss: 0.7122 - val_acc: 0.7447\n",
            "Epoch 154/300\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.56983\n",
            "240/240 - 0s - loss: 0.4371 - acc: 0.8750 - val_loss: 0.7286 - val_acc: 0.7234\n",
            "Epoch 155/300\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.56983\n",
            "240/240 - 0s - loss: 0.4582 - acc: 0.8417 - val_loss: 0.6840 - val_acc: 0.7660\n",
            "Epoch 156/300\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.56983\n",
            "240/240 - 0s - loss: 0.4170 - acc: 0.8667 - val_loss: 0.6235 - val_acc: 0.7447\n",
            "Epoch 157/300\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.56983\n",
            "240/240 - 0s - loss: 0.4305 - acc: 0.8458 - val_loss: 0.5956 - val_acc: 0.7660\n",
            "Epoch 158/300\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.56983\n",
            "240/240 - 0s - loss: 0.4390 - acc: 0.8750 - val_loss: 0.6300 - val_acc: 0.7660\n",
            "Epoch 159/300\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.56983\n",
            "240/240 - 0s - loss: 0.4466 - acc: 0.8792 - val_loss: 0.5989 - val_acc: 0.7872\n",
            "Epoch 160/300\n",
            "\n",
            "Epoch 00160: val_loss improved from 0.56983 to 0.54544, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.4359 - acc: 0.8625 - val_loss: 0.5454 - val_acc: 0.7872\n",
            "Epoch 161/300\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.54544\n",
            "240/240 - 0s - loss: 0.4192 - acc: 0.8708 - val_loss: 0.5590 - val_acc: 0.7872\n",
            "Epoch 162/300\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.54544\n",
            "240/240 - 0s - loss: 0.4451 - acc: 0.8542 - val_loss: 0.5975 - val_acc: 0.7872\n",
            "Epoch 163/300\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.54544\n",
            "240/240 - 0s - loss: 0.4270 - acc: 0.8792 - val_loss: 0.6014 - val_acc: 0.7872\n",
            "Epoch 164/300\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.54544\n",
            "240/240 - 0s - loss: 0.4300 - acc: 0.8917 - val_loss: 0.6608 - val_acc: 0.7872\n",
            "Epoch 165/300\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.54544\n",
            "240/240 - 0s - loss: 0.4267 - acc: 0.8667 - val_loss: 0.7822 - val_acc: 0.7660\n",
            "Epoch 166/300\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 0.54544\n",
            "240/240 - 0s - loss: 0.4519 - acc: 0.8417 - val_loss: 0.5718 - val_acc: 0.7447\n",
            "Epoch 167/300\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.54544\n",
            "240/240 - 0s - loss: 0.3970 - acc: 0.8792 - val_loss: 0.5914 - val_acc: 0.7872\n",
            "Epoch 168/300\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.54544\n",
            "240/240 - 0s - loss: 0.4072 - acc: 0.8708 - val_loss: 0.5921 - val_acc: 0.7660\n",
            "Epoch 169/300\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 0.54544\n",
            "240/240 - 0s - loss: 0.4225 - acc: 0.8792 - val_loss: 0.5919 - val_acc: 0.7660\n",
            "Epoch 170/300\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.54544\n",
            "240/240 - 0s - loss: 0.4043 - acc: 0.8792 - val_loss: 0.6335 - val_acc: 0.7660\n",
            "Epoch 171/300\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.54544\n",
            "240/240 - 0s - loss: 0.4262 - acc: 0.8750 - val_loss: 0.6322 - val_acc: 0.7660\n",
            "Epoch 172/300\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.54544\n",
            "240/240 - 0s - loss: 0.4001 - acc: 0.8875 - val_loss: 0.5966 - val_acc: 0.8085\n",
            "Epoch 173/300\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.54544\n",
            "240/240 - 0s - loss: 0.3904 - acc: 0.8875 - val_loss: 0.6359 - val_acc: 0.7872\n",
            "Epoch 174/300\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.54544\n",
            "240/240 - 0s - loss: 0.3944 - acc: 0.8792 - val_loss: 0.5573 - val_acc: 0.8085\n",
            "Epoch 175/300\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.54544\n",
            "240/240 - 0s - loss: 0.3971 - acc: 0.8792 - val_loss: 0.7214 - val_acc: 0.7447\n",
            "Epoch 176/300\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 0.54544\n",
            "240/240 - 0s - loss: 0.4074 - acc: 0.8792 - val_loss: 0.5782 - val_acc: 0.8085\n",
            "Epoch 177/300\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.54544\n",
            "240/240 - 0s - loss: 0.4356 - acc: 0.8583 - val_loss: 0.5507 - val_acc: 0.7660\n",
            "Epoch 178/300\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.54544\n",
            "240/240 - 0s - loss: 0.3999 - acc: 0.8667 - val_loss: 0.5837 - val_acc: 0.7872\n",
            "Epoch 179/300\n",
            "\n",
            "Epoch 00179: val_loss improved from 0.54544 to 0.54343, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.4188 - acc: 0.8708 - val_loss: 0.5434 - val_acc: 0.7872\n",
            "Epoch 180/300\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.54343\n",
            "240/240 - 0s - loss: 0.3852 - acc: 0.8625 - val_loss: 0.6653 - val_acc: 0.7234\n",
            "Epoch 181/300\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.54343\n",
            "240/240 - 0s - loss: 0.3967 - acc: 0.8667 - val_loss: 0.5893 - val_acc: 0.7447\n",
            "Epoch 182/300\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.54343\n",
            "240/240 - 0s - loss: 0.3973 - acc: 0.8625 - val_loss: 0.6122 - val_acc: 0.7447\n",
            "Epoch 183/300\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.54343\n",
            "240/240 - 0s - loss: 0.4146 - acc: 0.8625 - val_loss: 0.6098 - val_acc: 0.7872\n",
            "Epoch 184/300\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 0.54343\n",
            "240/240 - 0s - loss: 0.4321 - acc: 0.8583 - val_loss: 0.5890 - val_acc: 0.8085\n",
            "Epoch 185/300\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.54343\n",
            "240/240 - 0s - loss: 0.4267 - acc: 0.8667 - val_loss: 0.5664 - val_acc: 0.7872\n",
            "Epoch 186/300\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.54343\n",
            "240/240 - 0s - loss: 0.4033 - acc: 0.9042 - val_loss: 0.7324 - val_acc: 0.7234\n",
            "Epoch 187/300\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 0.54343\n",
            "240/240 - 0s - loss: 0.4396 - acc: 0.8625 - val_loss: 0.6062 - val_acc: 0.7660\n",
            "Epoch 188/300\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.54343\n",
            "240/240 - 0s - loss: 0.3776 - acc: 0.8708 - val_loss: 0.6391 - val_acc: 0.7660\n",
            "Epoch 189/300\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.54343\n",
            "240/240 - 0s - loss: 0.3989 - acc: 0.8875 - val_loss: 0.5948 - val_acc: 0.7872\n",
            "Epoch 190/300\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.54343\n",
            "240/240 - 0s - loss: 0.4141 - acc: 0.8625 - val_loss: 0.6882 - val_acc: 0.7234\n",
            "Epoch 191/300\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.54343\n",
            "240/240 - 0s - loss: 0.4102 - acc: 0.8750 - val_loss: 0.7443 - val_acc: 0.7660\n",
            "Epoch 192/300\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 0.54343\n",
            "240/240 - 0s - loss: 0.3728 - acc: 0.9000 - val_loss: 0.6139 - val_acc: 0.7872\n",
            "Epoch 193/300\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.54343\n",
            "240/240 - 0s - loss: 0.4028 - acc: 0.8750 - val_loss: 0.5869 - val_acc: 0.8085\n",
            "Epoch 194/300\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.54343\n",
            "240/240 - 0s - loss: 0.4155 - acc: 0.8625 - val_loss: 0.5647 - val_acc: 0.8298\n",
            "Epoch 195/300\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.54343\n",
            "240/240 - 0s - loss: 0.4469 - acc: 0.8250 - val_loss: 0.6984 - val_acc: 0.7234\n",
            "Epoch 196/300\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.54343\n",
            "240/240 - 0s - loss: 0.4148 - acc: 0.8708 - val_loss: 0.5471 - val_acc: 0.8298\n",
            "Epoch 197/300\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.54343\n",
            "240/240 - 0s - loss: 0.4047 - acc: 0.8917 - val_loss: 0.7156 - val_acc: 0.7660\n",
            "Epoch 198/300\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.54343\n",
            "240/240 - 0s - loss: 0.4807 - acc: 0.8167 - val_loss: 0.6082 - val_acc: 0.7872\n",
            "Epoch 199/300\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.54343\n",
            "240/240 - 0s - loss: 0.3975 - acc: 0.8750 - val_loss: 0.6629 - val_acc: 0.8298\n",
            "Epoch 200/300\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.54343\n",
            "240/240 - 0s - loss: 0.4048 - acc: 0.8667 - val_loss: 0.5914 - val_acc: 0.7447\n",
            "Epoch 201/300\n",
            "\n",
            "Epoch 00201: val_loss did not improve from 0.54343\n",
            "240/240 - 0s - loss: 0.3623 - acc: 0.9042 - val_loss: 0.5869 - val_acc: 0.8085\n",
            "Epoch 202/300\n",
            "\n",
            "Epoch 00202: val_loss did not improve from 0.54343\n",
            "240/240 - 0s - loss: 0.3604 - acc: 0.8750 - val_loss: 0.6853 - val_acc: 0.7872\n",
            "Epoch 203/300\n",
            "\n",
            "Epoch 00203: val_loss did not improve from 0.54343\n",
            "240/240 - 0s - loss: 0.3944 - acc: 0.8667 - val_loss: 0.5696 - val_acc: 0.7872\n",
            "Epoch 204/300\n",
            "\n",
            "Epoch 00204: val_loss did not improve from 0.54343\n",
            "240/240 - 0s - loss: 0.3811 - acc: 0.9000 - val_loss: 0.7782 - val_acc: 0.7234\n",
            "Epoch 205/300\n",
            "\n",
            "Epoch 00205: val_loss did not improve from 0.54343\n",
            "240/240 - 0s - loss: 0.4375 - acc: 0.8417 - val_loss: 0.5969 - val_acc: 0.7872\n",
            "Epoch 206/300\n",
            "\n",
            "Epoch 00206: val_loss did not improve from 0.54343\n",
            "240/240 - 0s - loss: 0.3511 - acc: 0.9208 - val_loss: 0.6061 - val_acc: 0.8085\n",
            "Epoch 207/300\n",
            "\n",
            "Epoch 00207: val_loss did not improve from 0.54343\n",
            "240/240 - 0s - loss: 0.3895 - acc: 0.8792 - val_loss: 0.6233 - val_acc: 0.7447\n",
            "Epoch 208/300\n",
            "\n",
            "Epoch 00208: val_loss did not improve from 0.54343\n",
            "240/240 - 0s - loss: 0.4052 - acc: 0.8750 - val_loss: 0.6380 - val_acc: 0.7447\n",
            "Epoch 209/300\n",
            "\n",
            "Epoch 00209: val_loss did not improve from 0.54343\n",
            "240/240 - 0s - loss: 0.3582 - acc: 0.8917 - val_loss: 0.5683 - val_acc: 0.8085\n",
            "Epoch 210/300\n",
            "\n",
            "Epoch 00210: val_loss did not improve from 0.54343\n",
            "240/240 - 0s - loss: 0.3793 - acc: 0.9000 - val_loss: 0.6310 - val_acc: 0.7872\n",
            "Epoch 211/300\n",
            "\n",
            "Epoch 00211: val_loss did not improve from 0.54343\n",
            "240/240 - 0s - loss: 0.3786 - acc: 0.8875 - val_loss: 0.5558 - val_acc: 0.7872\n",
            "Epoch 212/300\n",
            "\n",
            "Epoch 00212: val_loss did not improve from 0.54343\n",
            "240/240 - 0s - loss: 0.3648 - acc: 0.8917 - val_loss: 0.5904 - val_acc: 0.7872\n",
            "Epoch 213/300\n",
            "\n",
            "Epoch 00213: val_loss improved from 0.54343 to 0.54206, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.4019 - acc: 0.8667 - val_loss: 0.5421 - val_acc: 0.8085\n",
            "Epoch 214/300\n",
            "\n",
            "Epoch 00214: val_loss did not improve from 0.54206\n",
            "240/240 - 0s - loss: 0.3758 - acc: 0.8875 - val_loss: 0.6893 - val_acc: 0.8085\n",
            "Epoch 215/300\n",
            "\n",
            "Epoch 00215: val_loss did not improve from 0.54206\n",
            "240/240 - 0s - loss: 0.4043 - acc: 0.8542 - val_loss: 0.6583 - val_acc: 0.7872\n",
            "Epoch 216/300\n",
            "\n",
            "Epoch 00216: val_loss did not improve from 0.54206\n",
            "240/240 - 0s - loss: 0.3518 - acc: 0.8792 - val_loss: 0.6161 - val_acc: 0.8085\n",
            "Epoch 217/300\n",
            "\n",
            "Epoch 00217: val_loss did not improve from 0.54206\n",
            "240/240 - 0s - loss: 0.3596 - acc: 0.9042 - val_loss: 0.5799 - val_acc: 0.7660\n",
            "Epoch 218/300\n",
            "\n",
            "Epoch 00218: val_loss did not improve from 0.54206\n",
            "240/240 - 0s - loss: 0.3282 - acc: 0.9167 - val_loss: 0.5597 - val_acc: 0.7660\n",
            "Epoch 219/300\n",
            "\n",
            "Epoch 00219: val_loss did not improve from 0.54206\n",
            "240/240 - 0s - loss: 0.3771 - acc: 0.8833 - val_loss: 0.5799 - val_acc: 0.8085\n",
            "Epoch 220/300\n",
            "\n",
            "Epoch 00220: val_loss did not improve from 0.54206\n",
            "240/240 - 0s - loss: 0.3313 - acc: 0.9250 - val_loss: 0.6847 - val_acc: 0.7660\n",
            "Epoch 221/300\n",
            "\n",
            "Epoch 00221: val_loss did not improve from 0.54206\n",
            "240/240 - 0s - loss: 0.3854 - acc: 0.8875 - val_loss: 0.6297 - val_acc: 0.7872\n",
            "Epoch 222/300\n",
            "\n",
            "Epoch 00222: val_loss did not improve from 0.54206\n",
            "240/240 - 0s - loss: 0.3423 - acc: 0.9208 - val_loss: 0.7296 - val_acc: 0.7021\n",
            "Epoch 223/300\n",
            "\n",
            "Epoch 00223: val_loss did not improve from 0.54206\n",
            "240/240 - 0s - loss: 0.3966 - acc: 0.8583 - val_loss: 0.6549 - val_acc: 0.7660\n",
            "Epoch 224/300\n",
            "\n",
            "Epoch 00224: val_loss did not improve from 0.54206\n",
            "240/240 - 0s - loss: 0.3936 - acc: 0.8833 - val_loss: 0.7587 - val_acc: 0.7447\n",
            "Epoch 225/300\n",
            "\n",
            "Epoch 00225: val_loss did not improve from 0.54206\n",
            "240/240 - 0s - loss: 0.3837 - acc: 0.8667 - val_loss: 0.6904 - val_acc: 0.7660\n",
            "Epoch 226/300\n",
            "\n",
            "Epoch 00226: val_loss did not improve from 0.54206\n",
            "240/240 - 0s - loss: 0.3812 - acc: 0.8750 - val_loss: 0.7128 - val_acc: 0.7872\n",
            "Epoch 227/300\n",
            "\n",
            "Epoch 00227: val_loss did not improve from 0.54206\n",
            "240/240 - 0s - loss: 0.3875 - acc: 0.8875 - val_loss: 0.6059 - val_acc: 0.7447\n",
            "Epoch 228/300\n",
            "\n",
            "Epoch 00228: val_loss did not improve from 0.54206\n",
            "240/240 - 0s - loss: 0.4155 - acc: 0.8458 - val_loss: 0.6031 - val_acc: 0.7660\n",
            "Epoch 229/300\n",
            "\n",
            "Epoch 00229: val_loss did not improve from 0.54206\n",
            "240/240 - 0s - loss: 0.3830 - acc: 0.9000 - val_loss: 0.6028 - val_acc: 0.7660\n",
            "Epoch 230/300\n",
            "\n",
            "Epoch 00230: val_loss did not improve from 0.54206\n",
            "240/240 - 0s - loss: 0.3625 - acc: 0.9083 - val_loss: 0.6686 - val_acc: 0.7447\n",
            "Epoch 231/300\n",
            "\n",
            "Epoch 00231: val_loss did not improve from 0.54206\n",
            "240/240 - 0s - loss: 0.3486 - acc: 0.8750 - val_loss: 0.6187 - val_acc: 0.7660\n",
            "Epoch 232/300\n",
            "\n",
            "Epoch 00232: val_loss did not improve from 0.54206\n",
            "240/240 - 0s - loss: 0.3597 - acc: 0.9000 - val_loss: 0.6333 - val_acc: 0.8085\n",
            "Epoch 233/300\n",
            "\n",
            "Epoch 00233: val_loss did not improve from 0.54206\n",
            "240/240 - 0s - loss: 0.3830 - acc: 0.8917 - val_loss: 0.5854 - val_acc: 0.8085\n",
            "Epoch 234/300\n",
            "\n",
            "Epoch 00234: val_loss did not improve from 0.54206\n",
            "240/240 - 0s - loss: 0.3440 - acc: 0.8958 - val_loss: 0.6388 - val_acc: 0.8085\n",
            "Epoch 235/300\n",
            "\n",
            "Epoch 00235: val_loss did not improve from 0.54206\n",
            "240/240 - 0s - loss: 0.3717 - acc: 0.8792 - val_loss: 0.6108 - val_acc: 0.7660\n",
            "Epoch 236/300\n",
            "\n",
            "Epoch 00236: val_loss did not improve from 0.54206\n",
            "240/240 - 0s - loss: 0.3366 - acc: 0.9125 - val_loss: 0.7603 - val_acc: 0.7660\n",
            "Epoch 237/300\n",
            "\n",
            "Epoch 00237: val_loss improved from 0.54206 to 0.52082, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.3843 - acc: 0.8708 - val_loss: 0.5208 - val_acc: 0.7872\n",
            "Epoch 238/300\n",
            "\n",
            "Epoch 00238: val_loss did not improve from 0.52082\n",
            "240/240 - 0s - loss: 0.3537 - acc: 0.8833 - val_loss: 0.6856 - val_acc: 0.7447\n",
            "Epoch 239/300\n",
            "\n",
            "Epoch 00239: val_loss improved from 0.52082 to 0.48814, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.3920 - acc: 0.8667 - val_loss: 0.4881 - val_acc: 0.8298\n",
            "Epoch 240/300\n",
            "\n",
            "Epoch 00240: val_loss did not improve from 0.48814\n",
            "240/240 - 0s - loss: 0.3650 - acc: 0.8708 - val_loss: 0.7078 - val_acc: 0.7447\n",
            "Epoch 241/300\n",
            "\n",
            "Epoch 00241: val_loss did not improve from 0.48814\n",
            "240/240 - 0s - loss: 0.3970 - acc: 0.8708 - val_loss: 0.6316 - val_acc: 0.7872\n",
            "Epoch 242/300\n",
            "\n",
            "Epoch 00242: val_loss did not improve from 0.48814\n",
            "240/240 - 0s - loss: 0.3459 - acc: 0.8875 - val_loss: 0.5787 - val_acc: 0.8085\n",
            "Epoch 243/300\n",
            "\n",
            "Epoch 00243: val_loss did not improve from 0.48814\n",
            "240/240 - 0s - loss: 0.3646 - acc: 0.9000 - val_loss: 0.6286 - val_acc: 0.7872\n",
            "Epoch 244/300\n",
            "\n",
            "Epoch 00244: val_loss did not improve from 0.48814\n",
            "240/240 - 0s - loss: 0.3492 - acc: 0.9000 - val_loss: 0.5686 - val_acc: 0.7660\n",
            "Epoch 245/300\n",
            "\n",
            "Epoch 00245: val_loss did not improve from 0.48814\n",
            "240/240 - 0s - loss: 0.3413 - acc: 0.8750 - val_loss: 0.5222 - val_acc: 0.8085\n",
            "Epoch 246/300\n",
            "\n",
            "Epoch 00246: val_loss did not improve from 0.48814\n",
            "240/240 - 0s - loss: 0.3636 - acc: 0.8833 - val_loss: 0.5172 - val_acc: 0.8085\n",
            "Epoch 247/300\n",
            "\n",
            "Epoch 00247: val_loss did not improve from 0.48814\n",
            "240/240 - 0s - loss: 0.3384 - acc: 0.8958 - val_loss: 0.5049 - val_acc: 0.8511\n",
            "Epoch 248/300\n",
            "\n",
            "Epoch 00248: val_loss did not improve from 0.48814\n",
            "240/240 - 0s - loss: 0.3406 - acc: 0.9083 - val_loss: 0.7044 - val_acc: 0.7447\n",
            "Epoch 249/300\n",
            "\n",
            "Epoch 00249: val_loss did not improve from 0.48814\n",
            "240/240 - 0s - loss: 0.3360 - acc: 0.9250 - val_loss: 0.6285 - val_acc: 0.8085\n",
            "Epoch 250/300\n",
            "\n",
            "Epoch 00250: val_loss did not improve from 0.48814\n",
            "240/240 - 0s - loss: 0.3777 - acc: 0.8625 - val_loss: 0.6679 - val_acc: 0.7447\n",
            "Epoch 251/300\n",
            "\n",
            "Epoch 00251: val_loss did not improve from 0.48814\n",
            "240/240 - 0s - loss: 0.3792 - acc: 0.8833 - val_loss: 0.5152 - val_acc: 0.8723\n",
            "Epoch 252/300\n",
            "\n",
            "Epoch 00252: val_loss did not improve from 0.48814\n",
            "240/240 - 0s - loss: 0.3446 - acc: 0.8875 - val_loss: 0.6012 - val_acc: 0.8085\n",
            "Epoch 253/300\n",
            "\n",
            "Epoch 00253: val_loss did not improve from 0.48814\n",
            "240/240 - 0s - loss: 0.3510 - acc: 0.9000 - val_loss: 0.6163 - val_acc: 0.8085\n",
            "Epoch 254/300\n",
            "\n",
            "Epoch 00254: val_loss did not improve from 0.48814\n",
            "240/240 - 0s - loss: 0.3896 - acc: 0.8542 - val_loss: 0.5988 - val_acc: 0.7872\n",
            "Epoch 255/300\n",
            "\n",
            "Epoch 00255: val_loss did not improve from 0.48814\n",
            "240/240 - 0s - loss: 0.3699 - acc: 0.9042 - val_loss: 0.6751 - val_acc: 0.7660\n",
            "Epoch 256/300\n",
            "\n",
            "Epoch 00256: val_loss did not improve from 0.48814\n",
            "240/240 - 0s - loss: 0.3756 - acc: 0.8792 - val_loss: 0.6484 - val_acc: 0.7447\n",
            "Epoch 257/300\n",
            "\n",
            "Epoch 00257: val_loss did not improve from 0.48814\n",
            "240/240 - 0s - loss: 0.3518 - acc: 0.8750 - val_loss: 0.7312 - val_acc: 0.7234\n",
            "Epoch 258/300\n",
            "\n",
            "Epoch 00258: val_loss did not improve from 0.48814\n",
            "240/240 - 0s - loss: 0.3665 - acc: 0.8667 - val_loss: 0.5880 - val_acc: 0.7660\n",
            "Epoch 259/300\n",
            "\n",
            "Epoch 00259: val_loss did not improve from 0.48814\n",
            "240/240 - 0s - loss: 0.3060 - acc: 0.9208 - val_loss: 0.5903 - val_acc: 0.7660\n",
            "Epoch 260/300\n",
            "\n",
            "Epoch 00260: val_loss did not improve from 0.48814\n",
            "240/240 - 0s - loss: 0.3249 - acc: 0.8958 - val_loss: 0.5864 - val_acc: 0.7234\n",
            "Epoch 261/300\n",
            "\n",
            "Epoch 00261: val_loss did not improve from 0.48814\n",
            "240/240 - 0s - loss: 0.3343 - acc: 0.8833 - val_loss: 0.5467 - val_acc: 0.7872\n",
            "Epoch 262/300\n",
            "\n",
            "Epoch 00262: val_loss did not improve from 0.48814\n",
            "240/240 - 0s - loss: 0.3141 - acc: 0.8917 - val_loss: 0.6302 - val_acc: 0.7234\n",
            "Epoch 263/300\n",
            "\n",
            "Epoch 00263: val_loss did not improve from 0.48814\n",
            "240/240 - 0s - loss: 0.3125 - acc: 0.9083 - val_loss: 0.6498 - val_acc: 0.7872\n",
            "Epoch 264/300\n",
            "\n",
            "Epoch 00264: val_loss did not improve from 0.48814\n",
            "240/240 - 0s - loss: 0.3120 - acc: 0.9125 - val_loss: 0.5620 - val_acc: 0.8085\n",
            "Epoch 265/300\n",
            "\n",
            "Epoch 00265: val_loss did not improve from 0.48814\n",
            "240/240 - 0s - loss: 0.3537 - acc: 0.8875 - val_loss: 0.6631 - val_acc: 0.7660\n",
            "Epoch 266/300\n",
            "\n",
            "Epoch 00266: val_loss did not improve from 0.48814\n",
            "240/240 - 0s - loss: 0.3557 - acc: 0.9000 - val_loss: 0.5515 - val_acc: 0.8298\n",
            "Epoch 267/300\n",
            "\n",
            "Epoch 00267: val_loss did not improve from 0.48814\n",
            "240/240 - 0s - loss: 0.3615 - acc: 0.8667 - val_loss: 0.6334 - val_acc: 0.7872\n",
            "Epoch 268/300\n",
            "\n",
            "Epoch 00268: val_loss did not improve from 0.48814\n",
            "240/240 - 0s - loss: 0.3363 - acc: 0.8917 - val_loss: 0.5825 - val_acc: 0.7872\n",
            "Epoch 269/300\n",
            "\n",
            "Epoch 00269: val_loss did not improve from 0.48814\n",
            "240/240 - 0s - loss: 0.3542 - acc: 0.8792 - val_loss: 0.5968 - val_acc: 0.7660\n",
            "Epoch 270/300\n",
            "\n",
            "Epoch 00270: val_loss did not improve from 0.48814\n",
            "240/240 - 0s - loss: 0.3516 - acc: 0.8958 - val_loss: 0.6660 - val_acc: 0.7660\n",
            "Epoch 271/300\n",
            "\n",
            "Epoch 00271: val_loss did not improve from 0.48814\n",
            "240/240 - 0s - loss: 0.3448 - acc: 0.8917 - val_loss: 0.5943 - val_acc: 0.8085\n",
            "Epoch 272/300\n",
            "\n",
            "Epoch 00272: val_loss did not improve from 0.48814\n",
            "240/240 - 0s - loss: 0.3308 - acc: 0.9000 - val_loss: 0.5880 - val_acc: 0.7234\n",
            "Epoch 273/300\n",
            "\n",
            "Epoch 00273: val_loss did not improve from 0.48814\n",
            "240/240 - 0s - loss: 0.3427 - acc: 0.9083 - val_loss: 0.5775 - val_acc: 0.7872\n",
            "Epoch 274/300\n",
            "\n",
            "Epoch 00274: val_loss did not improve from 0.48814\n",
            "240/240 - 0s - loss: 0.2996 - acc: 0.9083 - val_loss: 0.5502 - val_acc: 0.7872\n",
            "Epoch 275/300\n",
            "\n",
            "Epoch 00275: val_loss did not improve from 0.48814\n",
            "240/240 - 0s - loss: 0.3358 - acc: 0.9167 - val_loss: 0.5667 - val_acc: 0.7660\n",
            "Epoch 276/300\n",
            "\n",
            "Epoch 00276: val_loss did not improve from 0.48814\n",
            "240/240 - 0s - loss: 0.3606 - acc: 0.8917 - val_loss: 0.6188 - val_acc: 0.7660\n",
            "Epoch 277/300\n",
            "\n",
            "Epoch 00277: val_loss did not improve from 0.48814\n",
            "240/240 - 0s - loss: 0.3201 - acc: 0.9125 - val_loss: 0.5292 - val_acc: 0.8085\n",
            "Epoch 278/300\n",
            "\n",
            "Epoch 00278: val_loss did not improve from 0.48814\n",
            "240/240 - 0s - loss: 0.3350 - acc: 0.9083 - val_loss: 0.6592 - val_acc: 0.7021\n",
            "Epoch 279/300\n",
            "\n",
            "Epoch 00279: val_loss did not improve from 0.48814\n",
            "240/240 - 0s - loss: 0.3537 - acc: 0.8875 - val_loss: 0.6186 - val_acc: 0.7872\n",
            "Epoch 280/300\n",
            "\n",
            "Epoch 00280: val_loss did not improve from 0.48814\n",
            "240/240 - 0s - loss: 0.3576 - acc: 0.8917 - val_loss: 0.6529 - val_acc: 0.7447\n",
            "Epoch 281/300\n",
            "\n",
            "Epoch 00281: val_loss did not improve from 0.48814\n",
            "240/240 - 0s - loss: 0.3257 - acc: 0.9000 - val_loss: 0.5386 - val_acc: 0.7660\n",
            "Epoch 282/300\n",
            "\n",
            "Epoch 00282: val_loss did not improve from 0.48814\n",
            "240/240 - 0s - loss: 0.3398 - acc: 0.8833 - val_loss: 0.6443 - val_acc: 0.7234\n",
            "Epoch 283/300\n",
            "\n",
            "Epoch 00283: val_loss did not improve from 0.48814\n",
            "240/240 - 0s - loss: 0.3355 - acc: 0.9083 - val_loss: 0.6318 - val_acc: 0.7447\n",
            "Epoch 284/300\n",
            "\n",
            "Epoch 00284: val_loss did not improve from 0.48814\n",
            "240/240 - 0s - loss: 0.3392 - acc: 0.9083 - val_loss: 0.5880 - val_acc: 0.7872\n",
            "Epoch 285/300\n",
            "\n",
            "Epoch 00285: val_loss did not improve from 0.48814\n",
            "240/240 - 0s - loss: 0.3404 - acc: 0.8875 - val_loss: 0.5070 - val_acc: 0.7660\n",
            "Epoch 286/300\n",
            "\n",
            "Epoch 00286: val_loss improved from 0.48814 to 0.46495, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.3457 - acc: 0.8917 - val_loss: 0.4650 - val_acc: 0.8085\n",
            "Epoch 287/300\n",
            "\n",
            "Epoch 00287: val_loss did not improve from 0.46495\n",
            "240/240 - 0s - loss: 0.3561 - acc: 0.8833 - val_loss: 0.6021 - val_acc: 0.7872\n",
            "Epoch 288/300\n",
            "\n",
            "Epoch 00288: val_loss did not improve from 0.46495\n",
            "240/240 - 0s - loss: 0.3193 - acc: 0.9208 - val_loss: 0.5339 - val_acc: 0.7660\n",
            "Epoch 289/300\n",
            "\n",
            "Epoch 00289: val_loss did not improve from 0.46495\n",
            "240/240 - 0s - loss: 0.3474 - acc: 0.8750 - val_loss: 0.5458 - val_acc: 0.8298\n",
            "Epoch 290/300\n",
            "\n",
            "Epoch 00290: val_loss did not improve from 0.46495\n",
            "240/240 - 0s - loss: 0.3519 - acc: 0.8958 - val_loss: 0.5702 - val_acc: 0.8298\n",
            "Epoch 291/300\n",
            "\n",
            "Epoch 00291: val_loss did not improve from 0.46495\n",
            "240/240 - 0s - loss: 0.3567 - acc: 0.8792 - val_loss: 0.7242 - val_acc: 0.7021\n",
            "Epoch 292/300\n",
            "\n",
            "Epoch 00292: val_loss did not improve from 0.46495\n",
            "240/240 - 0s - loss: 0.3189 - acc: 0.9167 - val_loss: 0.6156 - val_acc: 0.8085\n",
            "Epoch 293/300\n",
            "\n",
            "Epoch 00293: val_loss did not improve from 0.46495\n",
            "240/240 - 0s - loss: 0.3453 - acc: 0.8958 - val_loss: 0.5773 - val_acc: 0.8085\n",
            "Epoch 294/300\n",
            "\n",
            "Epoch 00294: val_loss did not improve from 0.46495\n",
            "240/240 - 0s - loss: 0.3557 - acc: 0.8708 - val_loss: 0.5598 - val_acc: 0.8085\n",
            "Epoch 295/300\n",
            "\n",
            "Epoch 00295: val_loss did not improve from 0.46495\n",
            "240/240 - 0s - loss: 0.3040 - acc: 0.8875 - val_loss: 0.5996 - val_acc: 0.8085\n",
            "Epoch 296/300\n",
            "\n",
            "Epoch 00296: val_loss did not improve from 0.46495\n",
            "240/240 - 0s - loss: 0.3477 - acc: 0.8833 - val_loss: 0.7224 - val_acc: 0.7021\n",
            "Epoch 297/300\n",
            "\n",
            "Epoch 00297: val_loss did not improve from 0.46495\n",
            "240/240 - 0s - loss: 0.3330 - acc: 0.8833 - val_loss: 0.6315 - val_acc: 0.8298\n",
            "Epoch 298/300\n",
            "\n",
            "Epoch 00298: val_loss did not improve from 0.46495\n",
            "240/240 - 0s - loss: 0.3081 - acc: 0.9083 - val_loss: 0.5691 - val_acc: 0.7660\n",
            "Epoch 299/300\n",
            "\n",
            "Epoch 00299: val_loss did not improve from 0.46495\n",
            "240/240 - 0s - loss: 0.3142 - acc: 0.8958 - val_loss: 0.6297 - val_acc: 0.7234\n",
            "Epoch 300/300\n",
            "\n",
            "Epoch 00300: val_loss did not improve from 0.46495\n",
            "240/240 - 0s - loss: 0.3403 - acc: 0.9167 - val_loss: 0.5558 - val_acc: 0.7660\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUyAQEck5BQIRkZxTIBARybn/D45l6ngCUi/eAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "# Evaluate on test data\n",
            "288/288 [==============================] - 1s 2ms/sample - loss: 0.4990 - acc: 0.7847\n",
            "test loss, test acc: [0.49895158926180255, 0.7847222]\n",
            "EEG_Deep/Data2A/Data_A04T.mat\n",
            "(288, 22, 1875)\n",
            "(288, 1)\n",
            "(240, 22, 750)\n",
            "(240, 1)\n",
            "(47, 22, 750)\n",
            "(47, 1)\n",
            "X_train shape: (240, 1, 22, 750)\n",
            "240 train samples\n",
            "47 val samples\n",
            "EEG_Deep/Data2A/Data_A04E.mat\n",
            "(288, 22, 1875)\n",
            "(288, 1)\n",
            "(288, 22, 750)\n",
            "(288, 1)\n",
            "X_train shape: (288, 1, 22, 750)\n",
            "288 train samples\n",
            "Train on 240 samples, validate on 47 samples\n",
            "Epoch 1/300\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 1.38357, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 1s - loss: 1.3983 - acc: 0.2958 - val_loss: 1.3836 - val_acc: 0.3617\n",
            "Epoch 2/300\n",
            "\n",
            "Epoch 00002: val_loss improved from 1.38357 to 1.38115, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.3736 - acc: 0.2917 - val_loss: 1.3811 - val_acc: 0.3191\n",
            "Epoch 3/300\n",
            "\n",
            "Epoch 00003: val_loss improved from 1.38115 to 1.37806, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.3499 - acc: 0.4083 - val_loss: 1.3781 - val_acc: 0.3830\n",
            "Epoch 4/300\n",
            "\n",
            "Epoch 00004: val_loss improved from 1.37806 to 1.37350, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.3387 - acc: 0.4125 - val_loss: 1.3735 - val_acc: 0.5106\n",
            "Epoch 5/300\n",
            "\n",
            "Epoch 00005: val_loss improved from 1.37350 to 1.36737, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.3156 - acc: 0.4292 - val_loss: 1.3674 - val_acc: 0.4894\n",
            "Epoch 6/300\n",
            "\n",
            "Epoch 00006: val_loss improved from 1.36737 to 1.35789, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.3002 - acc: 0.4875 - val_loss: 1.3579 - val_acc: 0.4043\n",
            "Epoch 7/300\n",
            "\n",
            "Epoch 00007: val_loss improved from 1.35789 to 1.34774, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.2696 - acc: 0.4958 - val_loss: 1.3477 - val_acc: 0.4681\n",
            "Epoch 8/300\n",
            "\n",
            "Epoch 00008: val_loss improved from 1.34774 to 1.33082, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.2553 - acc: 0.4750 - val_loss: 1.3308 - val_acc: 0.5106\n",
            "Epoch 9/300\n",
            "\n",
            "Epoch 00009: val_loss improved from 1.33082 to 1.32431, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.2407 - acc: 0.4833 - val_loss: 1.3243 - val_acc: 0.4894\n",
            "Epoch 10/300\n",
            "\n",
            "Epoch 00010: val_loss improved from 1.32431 to 1.30251, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.2242 - acc: 0.5208 - val_loss: 1.3025 - val_acc: 0.4681\n",
            "Epoch 11/300\n",
            "\n",
            "Epoch 00011: val_loss improved from 1.30251 to 1.27737, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.2017 - acc: 0.4500 - val_loss: 1.2774 - val_acc: 0.5319\n",
            "Epoch 12/300\n",
            "\n",
            "Epoch 00012: val_loss improved from 1.27737 to 1.26161, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.1628 - acc: 0.5708 - val_loss: 1.2616 - val_acc: 0.5106\n",
            "Epoch 13/300\n",
            "\n",
            "Epoch 00013: val_loss improved from 1.26161 to 1.24561, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.1883 - acc: 0.5042 - val_loss: 1.2456 - val_acc: 0.5106\n",
            "Epoch 14/300\n",
            "\n",
            "Epoch 00014: val_loss improved from 1.24561 to 1.24243, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.1723 - acc: 0.5292 - val_loss: 1.2424 - val_acc: 0.5106\n",
            "Epoch 15/300\n",
            "\n",
            "Epoch 00015: val_loss improved from 1.24243 to 1.24207, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.1367 - acc: 0.5375 - val_loss: 1.2421 - val_acc: 0.4894\n",
            "Epoch 16/300\n",
            "\n",
            "Epoch 00016: val_loss improved from 1.24207 to 1.21925, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.1285 - acc: 0.5083 - val_loss: 1.2192 - val_acc: 0.4681\n",
            "Epoch 17/300\n",
            "\n",
            "Epoch 00017: val_loss improved from 1.21925 to 1.21039, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.1183 - acc: 0.6000 - val_loss: 1.2104 - val_acc: 0.5106\n",
            "Epoch 18/300\n",
            "\n",
            "Epoch 00018: val_loss improved from 1.21039 to 1.18495, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.1185 - acc: 0.5500 - val_loss: 1.1850 - val_acc: 0.5745\n",
            "Epoch 19/300\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 1.18495\n",
            "240/240 - 0s - loss: 1.1001 - acc: 0.5792 - val_loss: 1.1878 - val_acc: 0.5532\n",
            "Epoch 20/300\n",
            "\n",
            "Epoch 00020: val_loss improved from 1.18495 to 1.18074, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.0964 - acc: 0.5833 - val_loss: 1.1807 - val_acc: 0.5532\n",
            "Epoch 21/300\n",
            "\n",
            "Epoch 00021: val_loss improved from 1.18074 to 1.14886, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.0878 - acc: 0.5875 - val_loss: 1.1489 - val_acc: 0.6170\n",
            "Epoch 22/300\n",
            "\n",
            "Epoch 00022: val_loss improved from 1.14886 to 1.12060, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.1040 - acc: 0.5625 - val_loss: 1.1206 - val_acc: 0.6170\n",
            "Epoch 23/300\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 1.12060\n",
            "240/240 - 0s - loss: 1.0512 - acc: 0.6208 - val_loss: 1.1316 - val_acc: 0.5532\n",
            "Epoch 24/300\n",
            "\n",
            "Epoch 00024: val_loss improved from 1.12060 to 1.10131, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.0217 - acc: 0.6333 - val_loss: 1.1013 - val_acc: 0.6170\n",
            "Epoch 25/300\n",
            "\n",
            "Epoch 00025: val_loss improved from 1.10131 to 1.08801, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.0559 - acc: 0.5875 - val_loss: 1.0880 - val_acc: 0.5957\n",
            "Epoch 26/300\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 1.08801\n",
            "240/240 - 0s - loss: 1.0276 - acc: 0.6333 - val_loss: 1.1151 - val_acc: 0.5957\n",
            "Epoch 27/300\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 1.08801\n",
            "240/240 - 0s - loss: 1.0208 - acc: 0.6250 - val_loss: 1.0931 - val_acc: 0.6170\n",
            "Epoch 28/300\n",
            "\n",
            "Epoch 00028: val_loss improved from 1.08801 to 1.03906, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.0441 - acc: 0.5792 - val_loss: 1.0391 - val_acc: 0.6170\n",
            "Epoch 29/300\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 1.03906\n",
            "240/240 - 0s - loss: 1.0421 - acc: 0.6125 - val_loss: 1.0401 - val_acc: 0.6170\n",
            "Epoch 30/300\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 1.03906\n",
            "240/240 - 0s - loss: 1.0144 - acc: 0.6208 - val_loss: 1.0884 - val_acc: 0.5319\n",
            "Epoch 31/300\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 1.03906\n",
            "240/240 - 0s - loss: 1.0575 - acc: 0.5917 - val_loss: 1.0644 - val_acc: 0.5745\n",
            "Epoch 32/300\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 1.03906\n",
            "240/240 - 0s - loss: 1.0072 - acc: 0.6417 - val_loss: 1.0705 - val_acc: 0.5745\n",
            "Epoch 33/300\n",
            "\n",
            "Epoch 00033: val_loss improved from 1.03906 to 1.02129, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.0182 - acc: 0.6500 - val_loss: 1.0213 - val_acc: 0.6170\n",
            "Epoch 34/300\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 1.02129\n",
            "240/240 - 0s - loss: 1.0170 - acc: 0.6250 - val_loss: 1.0507 - val_acc: 0.6383\n",
            "Epoch 35/300\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 1.02129\n",
            "240/240 - 0s - loss: 1.0160 - acc: 0.5958 - val_loss: 1.0291 - val_acc: 0.5532\n",
            "Epoch 36/300\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 1.02129\n",
            "240/240 - 0s - loss: 1.0110 - acc: 0.6042 - val_loss: 1.0235 - val_acc: 0.6170\n",
            "Epoch 37/300\n",
            "\n",
            "Epoch 00037: val_loss improved from 1.02129 to 1.00838, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.9939 - acc: 0.6458 - val_loss: 1.0084 - val_acc: 0.5745\n",
            "Epoch 38/300\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 1.00838\n",
            "240/240 - 0s - loss: 1.0191 - acc: 0.6167 - val_loss: 1.0130 - val_acc: 0.5957\n",
            "Epoch 39/300\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 1.00838\n",
            "240/240 - 0s - loss: 0.9687 - acc: 0.6708 - val_loss: 1.0232 - val_acc: 0.5957\n",
            "Epoch 40/300\n",
            "\n",
            "Epoch 00040: val_loss improved from 1.00838 to 0.98717, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.0030 - acc: 0.6167 - val_loss: 0.9872 - val_acc: 0.6383\n",
            "Epoch 41/300\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.98717\n",
            "240/240 - 0s - loss: 1.0110 - acc: 0.6208 - val_loss: 0.9945 - val_acc: 0.5957\n",
            "Epoch 42/300\n",
            "\n",
            "Epoch 00042: val_loss improved from 0.98717 to 0.98479, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.9811 - acc: 0.6583 - val_loss: 0.9848 - val_acc: 0.6596\n",
            "Epoch 43/300\n",
            "\n",
            "Epoch 00043: val_loss improved from 0.98479 to 0.97915, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.9472 - acc: 0.6250 - val_loss: 0.9792 - val_acc: 0.6596\n",
            "Epoch 44/300\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.97915\n",
            "240/240 - 0s - loss: 0.9946 - acc: 0.6083 - val_loss: 0.9794 - val_acc: 0.6170\n",
            "Epoch 45/300\n",
            "\n",
            "Epoch 00045: val_loss improved from 0.97915 to 0.96215, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.9264 - acc: 0.6667 - val_loss: 0.9621 - val_acc: 0.6170\n",
            "Epoch 46/300\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.96215\n",
            "240/240 - 0s - loss: 0.9944 - acc: 0.6000 - val_loss: 0.9757 - val_acc: 0.6170\n",
            "Epoch 47/300\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.96215\n",
            "240/240 - 0s - loss: 0.9854 - acc: 0.5958 - val_loss: 0.9683 - val_acc: 0.6596\n",
            "Epoch 48/300\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.96215\n",
            "240/240 - 0s - loss: 0.9766 - acc: 0.6167 - val_loss: 0.9789 - val_acc: 0.6596\n",
            "Epoch 49/300\n",
            "\n",
            "Epoch 00049: val_loss improved from 0.96215 to 0.95946, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.9502 - acc: 0.6583 - val_loss: 0.9595 - val_acc: 0.6809\n",
            "Epoch 50/300\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.95946\n",
            "240/240 - 0s - loss: 0.9419 - acc: 0.6792 - val_loss: 0.9827 - val_acc: 0.6383\n",
            "Epoch 51/300\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.95946\n",
            "240/240 - 0s - loss: 0.9579 - acc: 0.6500 - val_loss: 0.9619 - val_acc: 0.6170\n",
            "Epoch 52/300\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.95946\n",
            "240/240 - 0s - loss: 0.9359 - acc: 0.6583 - val_loss: 0.9843 - val_acc: 0.6596\n",
            "Epoch 53/300\n",
            "\n",
            "Epoch 00053: val_loss improved from 0.95946 to 0.95648, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.9016 - acc: 0.6792 - val_loss: 0.9565 - val_acc: 0.6383\n",
            "Epoch 54/300\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.95648\n",
            "240/240 - 0s - loss: 0.9859 - acc: 0.5958 - val_loss: 0.9818 - val_acc: 0.6170\n",
            "Epoch 55/300\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.95648\n",
            "240/240 - 0s - loss: 0.9307 - acc: 0.6375 - val_loss: 0.9703 - val_acc: 0.5957\n",
            "Epoch 56/300\n",
            "\n",
            "Epoch 00056: val_loss improved from 0.95648 to 0.93955, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.9457 - acc: 0.6667 - val_loss: 0.9396 - val_acc: 0.6809\n",
            "Epoch 57/300\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.93955\n",
            "240/240 - 0s - loss: 0.9234 - acc: 0.6708 - val_loss: 0.9500 - val_acc: 0.6596\n",
            "Epoch 58/300\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.93955\n",
            "240/240 - 0s - loss: 0.9500 - acc: 0.6083 - val_loss: 0.9473 - val_acc: 0.6383\n",
            "Epoch 59/300\n",
            "\n",
            "Epoch 00059: val_loss improved from 0.93955 to 0.93883, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.9430 - acc: 0.6208 - val_loss: 0.9388 - val_acc: 0.6383\n",
            "Epoch 60/300\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.93883\n",
            "240/240 - 0s - loss: 0.9171 - acc: 0.6667 - val_loss: 0.9409 - val_acc: 0.5957\n",
            "Epoch 61/300\n",
            "\n",
            "Epoch 00061: val_loss improved from 0.93883 to 0.92736, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.8936 - acc: 0.6750 - val_loss: 0.9274 - val_acc: 0.6596\n",
            "Epoch 62/300\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.92736\n",
            "240/240 - 0s - loss: 0.9111 - acc: 0.6917 - val_loss: 0.9515 - val_acc: 0.6383\n",
            "Epoch 63/300\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.92736\n",
            "240/240 - 0s - loss: 0.8921 - acc: 0.6750 - val_loss: 0.9317 - val_acc: 0.6383\n",
            "Epoch 64/300\n",
            "\n",
            "Epoch 00064: val_loss improved from 0.92736 to 0.92094, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.8746 - acc: 0.6917 - val_loss: 0.9209 - val_acc: 0.6596\n",
            "Epoch 65/300\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.92094\n",
            "240/240 - 0s - loss: 0.9110 - acc: 0.6583 - val_loss: 0.9277 - val_acc: 0.6809\n",
            "Epoch 66/300\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.92094\n",
            "240/240 - 0s - loss: 0.9056 - acc: 0.6583 - val_loss: 0.9531 - val_acc: 0.6383\n",
            "Epoch 67/300\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.92094\n",
            "240/240 - 0s - loss: 0.8968 - acc: 0.6708 - val_loss: 0.9444 - val_acc: 0.6596\n",
            "Epoch 68/300\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.92094\n",
            "240/240 - 0s - loss: 0.8659 - acc: 0.6750 - val_loss: 0.9398 - val_acc: 0.6170\n",
            "Epoch 69/300\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.92094\n",
            "240/240 - 0s - loss: 0.9222 - acc: 0.6750 - val_loss: 0.9442 - val_acc: 0.6170\n",
            "Epoch 70/300\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.92094\n",
            "240/240 - 0s - loss: 0.8394 - acc: 0.7292 - val_loss: 0.9338 - val_acc: 0.6809\n",
            "Epoch 71/300\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.92094\n",
            "240/240 - 0s - loss: 0.8384 - acc: 0.7542 - val_loss: 0.9472 - val_acc: 0.6596\n",
            "Epoch 72/300\n",
            "\n",
            "Epoch 00072: val_loss improved from 0.92094 to 0.90336, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.8734 - acc: 0.6875 - val_loss: 0.9034 - val_acc: 0.6383\n",
            "Epoch 73/300\n",
            "\n",
            "Epoch 00073: val_loss improved from 0.90336 to 0.89249, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.8334 - acc: 0.7125 - val_loss: 0.8925 - val_acc: 0.6809\n",
            "Epoch 74/300\n",
            "\n",
            "Epoch 00074: val_loss improved from 0.89249 to 0.87052, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.8342 - acc: 0.7333 - val_loss: 0.8705 - val_acc: 0.7234\n",
            "Epoch 75/300\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.87052\n",
            "240/240 - 0s - loss: 0.8623 - acc: 0.6833 - val_loss: 0.8851 - val_acc: 0.6596\n",
            "Epoch 76/300\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.87052\n",
            "240/240 - 0s - loss: 0.8520 - acc: 0.7083 - val_loss: 0.9014 - val_acc: 0.6596\n",
            "Epoch 77/300\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.87052\n",
            "240/240 - 0s - loss: 0.8463 - acc: 0.7167 - val_loss: 0.8977 - val_acc: 0.6383\n",
            "Epoch 78/300\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.87052\n",
            "240/240 - 0s - loss: 0.8337 - acc: 0.7792 - val_loss: 0.9124 - val_acc: 0.7021\n",
            "Epoch 79/300\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.87052\n",
            "240/240 - 0s - loss: 0.8724 - acc: 0.6792 - val_loss: 0.9023 - val_acc: 0.5745\n",
            "Epoch 80/300\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.87052\n",
            "240/240 - 0s - loss: 0.8478 - acc: 0.6667 - val_loss: 0.9093 - val_acc: 0.6383\n",
            "Epoch 81/300\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.87052\n",
            "240/240 - 0s - loss: 0.8698 - acc: 0.6917 - val_loss: 0.9217 - val_acc: 0.7021\n",
            "Epoch 82/300\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.87052\n",
            "240/240 - 0s - loss: 0.8487 - acc: 0.6833 - val_loss: 0.9094 - val_acc: 0.6809\n",
            "Epoch 83/300\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.87052\n",
            "240/240 - 0s - loss: 0.9043 - acc: 0.6708 - val_loss: 0.9183 - val_acc: 0.6596\n",
            "Epoch 84/300\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.87052\n",
            "240/240 - 0s - loss: 0.8203 - acc: 0.7167 - val_loss: 0.9049 - val_acc: 0.6596\n",
            "Epoch 85/300\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.87052\n",
            "240/240 - 0s - loss: 0.8511 - acc: 0.6583 - val_loss: 0.8961 - val_acc: 0.7021\n",
            "Epoch 86/300\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.87052\n",
            "240/240 - 0s - loss: 0.8408 - acc: 0.7125 - val_loss: 0.8980 - val_acc: 0.6596\n",
            "Epoch 87/300\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.87052\n",
            "240/240 - 0s - loss: 0.8602 - acc: 0.7167 - val_loss: 0.9207 - val_acc: 0.6170\n",
            "Epoch 88/300\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.87052\n",
            "240/240 - 0s - loss: 0.8191 - acc: 0.7375 - val_loss: 0.9235 - val_acc: 0.6596\n",
            "Epoch 89/300\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.87052\n",
            "240/240 - 0s - loss: 0.8241 - acc: 0.7333 - val_loss: 0.8765 - val_acc: 0.6809\n",
            "Epoch 90/300\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.87052\n",
            "240/240 - 0s - loss: 0.8081 - acc: 0.7458 - val_loss: 0.8930 - val_acc: 0.6809\n",
            "Epoch 91/300\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.87052\n",
            "240/240 - 0s - loss: 0.8360 - acc: 0.7083 - val_loss: 0.8924 - val_acc: 0.6170\n",
            "Epoch 92/300\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.87052\n",
            "240/240 - 0s - loss: 0.8289 - acc: 0.7167 - val_loss: 0.9071 - val_acc: 0.6596\n",
            "Epoch 93/300\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.87052\n",
            "240/240 - 0s - loss: 0.8228 - acc: 0.7083 - val_loss: 0.8827 - val_acc: 0.7021\n",
            "Epoch 94/300\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.87052\n",
            "240/240 - 0s - loss: 0.8573 - acc: 0.7125 - val_loss: 0.9121 - val_acc: 0.5957\n",
            "Epoch 95/300\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.87052\n",
            "240/240 - 0s - loss: 0.8230 - acc: 0.7167 - val_loss: 0.9128 - val_acc: 0.6170\n",
            "Epoch 96/300\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.87052\n",
            "240/240 - 0s - loss: 0.8047 - acc: 0.7208 - val_loss: 0.9170 - val_acc: 0.6596\n",
            "Epoch 97/300\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.87052\n",
            "240/240 - 0s - loss: 0.8354 - acc: 0.6958 - val_loss: 0.9337 - val_acc: 0.5957\n",
            "Epoch 98/300\n",
            "\n",
            "Epoch 00098: val_loss improved from 0.87052 to 0.86926, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.8175 - acc: 0.7208 - val_loss: 0.8693 - val_acc: 0.6170\n",
            "Epoch 99/300\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.86926\n",
            "240/240 - 0s - loss: 0.7803 - acc: 0.7333 - val_loss: 0.8920 - val_acc: 0.6383\n",
            "Epoch 100/300\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.86926\n",
            "240/240 - 0s - loss: 0.7911 - acc: 0.7042 - val_loss: 0.8907 - val_acc: 0.6596\n",
            "Epoch 101/300\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.86926\n",
            "240/240 - 0s - loss: 0.7955 - acc: 0.6958 - val_loss: 0.8972 - val_acc: 0.5745\n",
            "Epoch 102/300\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.86926\n",
            "240/240 - 0s - loss: 0.8449 - acc: 0.7333 - val_loss: 0.8884 - val_acc: 0.6383\n",
            "Epoch 103/300\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.86926\n",
            "240/240 - 0s - loss: 0.7520 - acc: 0.7500 - val_loss: 0.8989 - val_acc: 0.5957\n",
            "Epoch 104/300\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.86926\n",
            "240/240 - 0s - loss: 0.7609 - acc: 0.7458 - val_loss: 0.8983 - val_acc: 0.6170\n",
            "Epoch 105/300\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 0.86926\n",
            "240/240 - 0s - loss: 0.7915 - acc: 0.7375 - val_loss: 0.9059 - val_acc: 0.6383\n",
            "Epoch 106/300\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 0.86926\n",
            "240/240 - 0s - loss: 0.8041 - acc: 0.6958 - val_loss: 0.9032 - val_acc: 0.6596\n",
            "Epoch 107/300\n",
            "\n",
            "Epoch 00107: val_loss improved from 0.86926 to 0.85581, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.7633 - acc: 0.7500 - val_loss: 0.8558 - val_acc: 0.6596\n",
            "Epoch 108/300\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 0.85581\n",
            "240/240 - 0s - loss: 0.7858 - acc: 0.7583 - val_loss: 0.8967 - val_acc: 0.6809\n",
            "Epoch 109/300\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.85581\n",
            "240/240 - 0s - loss: 0.8124 - acc: 0.6792 - val_loss: 0.9210 - val_acc: 0.6596\n",
            "Epoch 110/300\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.85581\n",
            "240/240 - 0s - loss: 0.8027 - acc: 0.7083 - val_loss: 0.9340 - val_acc: 0.6383\n",
            "Epoch 111/300\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 0.85581\n",
            "240/240 - 0s - loss: 0.7747 - acc: 0.7208 - val_loss: 0.9373 - val_acc: 0.5745\n",
            "Epoch 112/300\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 0.85581\n",
            "240/240 - 0s - loss: 0.7890 - acc: 0.7208 - val_loss: 0.8893 - val_acc: 0.7021\n",
            "Epoch 113/300\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.85581\n",
            "240/240 - 0s - loss: 0.8100 - acc: 0.6792 - val_loss: 0.8766 - val_acc: 0.6809\n",
            "Epoch 114/300\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.85581\n",
            "240/240 - 0s - loss: 0.7682 - acc: 0.7250 - val_loss: 0.8674 - val_acc: 0.6809\n",
            "Epoch 115/300\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.85581\n",
            "240/240 - 0s - loss: 0.7834 - acc: 0.7250 - val_loss: 0.8812 - val_acc: 0.6596\n",
            "Epoch 116/300\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.85581\n",
            "240/240 - 0s - loss: 0.7593 - acc: 0.7750 - val_loss: 0.8811 - val_acc: 0.6809\n",
            "Epoch 117/300\n",
            "\n",
            "Epoch 00117: val_loss improved from 0.85581 to 0.85265, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.7693 - acc: 0.7250 - val_loss: 0.8526 - val_acc: 0.6809\n",
            "Epoch 118/300\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.85265\n",
            "240/240 - 0s - loss: 0.7664 - acc: 0.7417 - val_loss: 0.8813 - val_acc: 0.7021\n",
            "Epoch 119/300\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 0.85265\n",
            "240/240 - 0s - loss: 0.7511 - acc: 0.7500 - val_loss: 0.8859 - val_acc: 0.6809\n",
            "Epoch 120/300\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.85265\n",
            "240/240 - 0s - loss: 0.7700 - acc: 0.7375 - val_loss: 0.8822 - val_acc: 0.6809\n",
            "Epoch 121/300\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.85265\n",
            "240/240 - 0s - loss: 0.7347 - acc: 0.7625 - val_loss: 0.8802 - val_acc: 0.6383\n",
            "Epoch 122/300\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.85265\n",
            "240/240 - 0s - loss: 0.7757 - acc: 0.7292 - val_loss: 0.8828 - val_acc: 0.6596\n",
            "Epoch 123/300\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 0.85265\n",
            "240/240 - 0s - loss: 0.8162 - acc: 0.6792 - val_loss: 0.8916 - val_acc: 0.6170\n",
            "Epoch 124/300\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 0.85265\n",
            "240/240 - 0s - loss: 0.7818 - acc: 0.7250 - val_loss: 0.9175 - val_acc: 0.6170\n",
            "Epoch 125/300\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 0.85265\n",
            "240/240 - 0s - loss: 0.7603 - acc: 0.7333 - val_loss: 0.9093 - val_acc: 0.6383\n",
            "Epoch 126/300\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 0.85265\n",
            "240/240 - 0s - loss: 0.7159 - acc: 0.7833 - val_loss: 0.8827 - val_acc: 0.6383\n",
            "Epoch 127/300\n",
            "\n",
            "Epoch 00127: val_loss improved from 0.85265 to 0.84652, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.7678 - acc: 0.7167 - val_loss: 0.8465 - val_acc: 0.6383\n",
            "Epoch 128/300\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 0.84652\n",
            "240/240 - 0s - loss: 0.7054 - acc: 0.7625 - val_loss: 0.8835 - val_acc: 0.6383\n",
            "Epoch 129/300\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.84652\n",
            "240/240 - 0s - loss: 0.7373 - acc: 0.7458 - val_loss: 0.8528 - val_acc: 0.6596\n",
            "Epoch 130/300\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 0.84652\n",
            "240/240 - 0s - loss: 0.7181 - acc: 0.7667 - val_loss: 0.8574 - val_acc: 0.6596\n",
            "Epoch 131/300\n",
            "\n",
            "Epoch 00131: val_loss improved from 0.84652 to 0.82619, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.7407 - acc: 0.7375 - val_loss: 0.8262 - val_acc: 0.6383\n",
            "Epoch 132/300\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 0.82619\n",
            "240/240 - 0s - loss: 0.7572 - acc: 0.7333 - val_loss: 0.8382 - val_acc: 0.6383\n",
            "Epoch 133/300\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.82619\n",
            "240/240 - 0s - loss: 0.7042 - acc: 0.7542 - val_loss: 0.8709 - val_acc: 0.6383\n",
            "Epoch 134/300\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.82619\n",
            "240/240 - 0s - loss: 0.7318 - acc: 0.7750 - val_loss: 0.8878 - val_acc: 0.6170\n",
            "Epoch 135/300\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 0.82619\n",
            "240/240 - 0s - loss: 0.6867 - acc: 0.8208 - val_loss: 0.8640 - val_acc: 0.6809\n",
            "Epoch 136/300\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.82619\n",
            "240/240 - 0s - loss: 0.7592 - acc: 0.7458 - val_loss: 0.8659 - val_acc: 0.6596\n",
            "Epoch 137/300\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 0.82619\n",
            "240/240 - 0s - loss: 0.6983 - acc: 0.7875 - val_loss: 0.9098 - val_acc: 0.5957\n",
            "Epoch 138/300\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.82619\n",
            "240/240 - 0s - loss: 0.6991 - acc: 0.7917 - val_loss: 0.8768 - val_acc: 0.6170\n",
            "Epoch 139/300\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.82619\n",
            "240/240 - 0s - loss: 0.7362 - acc: 0.7625 - val_loss: 0.8635 - val_acc: 0.6596\n",
            "Epoch 140/300\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 0.82619\n",
            "240/240 - 0s - loss: 0.7181 - acc: 0.7458 - val_loss: 0.8392 - val_acc: 0.6809\n",
            "Epoch 141/300\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.82619\n",
            "240/240 - 0s - loss: 0.7500 - acc: 0.7375 - val_loss: 0.8888 - val_acc: 0.6383\n",
            "Epoch 142/300\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.82619\n",
            "240/240 - 0s - loss: 0.7461 - acc: 0.7667 - val_loss: 0.8967 - val_acc: 0.6170\n",
            "Epoch 143/300\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 0.82619\n",
            "240/240 - 0s - loss: 0.7144 - acc: 0.7583 - val_loss: 0.8755 - val_acc: 0.6383\n",
            "Epoch 144/300\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 0.82619\n",
            "240/240 - 0s - loss: 0.7265 - acc: 0.7917 - val_loss: 0.8885 - val_acc: 0.6383\n",
            "Epoch 145/300\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.82619\n",
            "240/240 - 0s - loss: 0.6731 - acc: 0.8042 - val_loss: 0.8340 - val_acc: 0.6383\n",
            "Epoch 146/300\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.82619\n",
            "240/240 - 0s - loss: 0.7382 - acc: 0.7417 - val_loss: 0.8980 - val_acc: 0.5957\n",
            "Epoch 147/300\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.82619\n",
            "240/240 - 0s - loss: 0.7190 - acc: 0.7333 - val_loss: 0.8583 - val_acc: 0.6596\n",
            "Epoch 148/300\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 0.82619\n",
            "240/240 - 0s - loss: 0.7381 - acc: 0.7333 - val_loss: 0.8266 - val_acc: 0.7021\n",
            "Epoch 149/300\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.82619\n",
            "240/240 - 0s - loss: 0.7560 - acc: 0.6958 - val_loss: 0.8644 - val_acc: 0.6170\n",
            "Epoch 150/300\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.82619\n",
            "240/240 - 0s - loss: 0.6907 - acc: 0.7833 - val_loss: 0.8562 - val_acc: 0.6383\n",
            "Epoch 151/300\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.82619\n",
            "240/240 - 0s - loss: 0.6756 - acc: 0.7583 - val_loss: 0.8746 - val_acc: 0.5957\n",
            "Epoch 152/300\n",
            "\n",
            "Epoch 00152: val_loss improved from 0.82619 to 0.80768, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.7176 - acc: 0.7750 - val_loss: 0.8077 - val_acc: 0.6596\n",
            "Epoch 153/300\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 0.80768\n",
            "240/240 - 0s - loss: 0.6884 - acc: 0.7625 - val_loss: 0.8613 - val_acc: 0.6170\n",
            "Epoch 154/300\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.80768\n",
            "240/240 - 0s - loss: 0.6989 - acc: 0.7542 - val_loss: 0.8142 - val_acc: 0.6596\n",
            "Epoch 155/300\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.80768\n",
            "240/240 - 0s - loss: 0.6900 - acc: 0.7667 - val_loss: 0.8488 - val_acc: 0.6383\n",
            "Epoch 156/300\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.80768\n",
            "240/240 - 0s - loss: 0.6702 - acc: 0.7750 - val_loss: 0.8708 - val_acc: 0.5957\n",
            "Epoch 157/300\n",
            "\n",
            "Epoch 00157: val_loss improved from 0.80768 to 0.80619, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.7082 - acc: 0.7667 - val_loss: 0.8062 - val_acc: 0.6383\n",
            "Epoch 158/300\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.80619\n",
            "240/240 - 0s - loss: 0.7322 - acc: 0.7833 - val_loss: 0.8302 - val_acc: 0.6170\n",
            "Epoch 159/300\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.80619\n",
            "240/240 - 0s - loss: 0.6578 - acc: 0.8125 - val_loss: 0.8151 - val_acc: 0.6170\n",
            "Epoch 160/300\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.80619\n",
            "240/240 - 0s - loss: 0.6782 - acc: 0.7708 - val_loss: 0.8302 - val_acc: 0.6383\n",
            "Epoch 161/300\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.80619\n",
            "240/240 - 0s - loss: 0.6469 - acc: 0.8250 - val_loss: 0.8271 - val_acc: 0.6383\n",
            "Epoch 162/300\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.80619\n",
            "240/240 - 0s - loss: 0.6814 - acc: 0.7708 - val_loss: 0.8683 - val_acc: 0.5957\n",
            "Epoch 163/300\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.80619\n",
            "240/240 - 0s - loss: 0.6505 - acc: 0.7792 - val_loss: 0.8442 - val_acc: 0.5957\n",
            "Epoch 164/300\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.80619\n",
            "240/240 - 0s - loss: 0.7037 - acc: 0.7625 - val_loss: 0.8537 - val_acc: 0.6809\n",
            "Epoch 165/300\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.80619\n",
            "240/240 - 0s - loss: 0.6804 - acc: 0.7667 - val_loss: 0.8861 - val_acc: 0.5957\n",
            "Epoch 166/300\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 0.80619\n",
            "240/240 - 0s - loss: 0.7025 - acc: 0.7375 - val_loss: 0.8472 - val_acc: 0.6170\n",
            "Epoch 167/300\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.80619\n",
            "240/240 - 0s - loss: 0.6748 - acc: 0.7875 - val_loss: 0.8636 - val_acc: 0.5957\n",
            "Epoch 168/300\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.80619\n",
            "240/240 - 0s - loss: 0.7083 - acc: 0.7208 - val_loss: 0.8516 - val_acc: 0.6383\n",
            "Epoch 169/300\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 0.80619\n",
            "240/240 - 0s - loss: 0.7021 - acc: 0.7500 - val_loss: 0.8860 - val_acc: 0.6170\n",
            "Epoch 170/300\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.80619\n",
            "240/240 - 0s - loss: 0.6234 - acc: 0.8000 - val_loss: 0.8770 - val_acc: 0.6170\n",
            "Epoch 171/300\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.80619\n",
            "240/240 - 0s - loss: 0.6856 - acc: 0.7750 - val_loss: 0.8146 - val_acc: 0.6383\n",
            "Epoch 172/300\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.80619\n",
            "240/240 - 0s - loss: 0.6776 - acc: 0.8167 - val_loss: 0.8320 - val_acc: 0.5957\n",
            "Epoch 173/300\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.80619\n",
            "240/240 - 0s - loss: 0.7018 - acc: 0.7375 - val_loss: 0.8603 - val_acc: 0.5532\n",
            "Epoch 174/300\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.80619\n",
            "240/240 - 0s - loss: 0.6416 - acc: 0.8167 - val_loss: 0.8337 - val_acc: 0.5957\n",
            "Epoch 175/300\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.80619\n",
            "240/240 - 0s - loss: 0.6750 - acc: 0.7583 - val_loss: 0.8723 - val_acc: 0.5532\n",
            "Epoch 176/300\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 0.80619\n",
            "240/240 - 0s - loss: 0.6513 - acc: 0.7625 - val_loss: 0.8571 - val_acc: 0.6170\n",
            "Epoch 177/300\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.80619\n",
            "240/240 - 0s - loss: 0.6354 - acc: 0.8417 - val_loss: 0.8450 - val_acc: 0.6170\n",
            "Epoch 178/300\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.80619\n",
            "240/240 - 0s - loss: 0.6535 - acc: 0.7917 - val_loss: 0.8393 - val_acc: 0.6170\n",
            "Epoch 179/300\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.80619\n",
            "240/240 - 0s - loss: 0.6734 - acc: 0.7625 - val_loss: 0.8351 - val_acc: 0.5957\n",
            "Epoch 180/300\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.80619\n",
            "240/240 - 0s - loss: 0.6749 - acc: 0.7750 - val_loss: 0.8354 - val_acc: 0.5957\n",
            "Epoch 181/300\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.80619\n",
            "240/240 - 0s - loss: 0.6775 - acc: 0.7917 - val_loss: 0.8750 - val_acc: 0.6170\n",
            "Epoch 182/300\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.80619\n",
            "240/240 - 0s - loss: 0.7064 - acc: 0.7417 - val_loss: 0.8283 - val_acc: 0.6383\n",
            "Epoch 183/300\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.80619\n",
            "240/240 - 0s - loss: 0.6283 - acc: 0.7833 - val_loss: 0.8535 - val_acc: 0.6170\n",
            "Epoch 184/300\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 0.80619\n",
            "240/240 - 0s - loss: 0.6471 - acc: 0.7458 - val_loss: 0.8122 - val_acc: 0.6170\n",
            "Epoch 185/300\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.80619\n",
            "240/240 - 0s - loss: 0.6446 - acc: 0.7917 - val_loss: 0.8187 - val_acc: 0.5745\n",
            "Epoch 186/300\n",
            "\n",
            "Epoch 00186: val_loss improved from 0.80619 to 0.80233, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.7226 - acc: 0.7542 - val_loss: 0.8023 - val_acc: 0.5957\n",
            "Epoch 187/300\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 0.80233\n",
            "240/240 - 0s - loss: 0.6470 - acc: 0.7750 - val_loss: 0.8594 - val_acc: 0.5957\n",
            "Epoch 188/300\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.80233\n",
            "240/240 - 0s - loss: 0.6529 - acc: 0.8000 - val_loss: 0.8213 - val_acc: 0.5957\n",
            "Epoch 189/300\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.80233\n",
            "240/240 - 0s - loss: 0.6399 - acc: 0.8083 - val_loss: 0.8268 - val_acc: 0.5957\n",
            "Epoch 190/300\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.80233\n",
            "240/240 - 0s - loss: 0.6852 - acc: 0.7833 - val_loss: 0.8036 - val_acc: 0.6383\n",
            "Epoch 191/300\n",
            "\n",
            "Epoch 00191: val_loss improved from 0.80233 to 0.79705, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.6319 - acc: 0.7958 - val_loss: 0.7970 - val_acc: 0.6809\n",
            "Epoch 192/300\n",
            "\n",
            "Epoch 00192: val_loss improved from 0.79705 to 0.79342, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.6418 - acc: 0.8083 - val_loss: 0.7934 - val_acc: 0.6383\n",
            "Epoch 193/300\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.79342\n",
            "240/240 - 0s - loss: 0.6160 - acc: 0.8167 - val_loss: 0.8806 - val_acc: 0.5957\n",
            "Epoch 194/300\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.79342\n",
            "240/240 - 0s - loss: 0.6557 - acc: 0.7667 - val_loss: 0.8563 - val_acc: 0.6170\n",
            "Epoch 195/300\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.79342\n",
            "240/240 - 0s - loss: 0.6317 - acc: 0.8042 - val_loss: 0.8590 - val_acc: 0.6170\n",
            "Epoch 196/300\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.79342\n",
            "240/240 - 0s - loss: 0.6267 - acc: 0.7958 - val_loss: 0.8621 - val_acc: 0.5745\n",
            "Epoch 197/300\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.79342\n",
            "240/240 - 0s - loss: 0.6217 - acc: 0.8042 - val_loss: 0.8165 - val_acc: 0.6596\n",
            "Epoch 198/300\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.79342\n",
            "240/240 - 0s - loss: 0.6238 - acc: 0.7917 - val_loss: 0.8488 - val_acc: 0.5957\n",
            "Epoch 199/300\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.79342\n",
            "240/240 - 0s - loss: 0.6323 - acc: 0.7833 - val_loss: 0.8351 - val_acc: 0.5957\n",
            "Epoch 200/300\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.79342\n",
            "240/240 - 0s - loss: 0.6537 - acc: 0.7917 - val_loss: 0.8672 - val_acc: 0.5745\n",
            "Epoch 201/300\n",
            "\n",
            "Epoch 00201: val_loss did not improve from 0.79342\n",
            "240/240 - 0s - loss: 0.6306 - acc: 0.7833 - val_loss: 0.8988 - val_acc: 0.5532\n",
            "Epoch 202/300\n",
            "\n",
            "Epoch 00202: val_loss did not improve from 0.79342\n",
            "240/240 - 0s - loss: 0.6452 - acc: 0.7875 - val_loss: 0.8603 - val_acc: 0.5745\n",
            "Epoch 203/300\n",
            "\n",
            "Epoch 00203: val_loss did not improve from 0.79342\n",
            "240/240 - 0s - loss: 0.6451 - acc: 0.7917 - val_loss: 0.8949 - val_acc: 0.5745\n",
            "Epoch 204/300\n",
            "\n",
            "Epoch 00204: val_loss did not improve from 0.79342\n",
            "240/240 - 0s - loss: 0.6898 - acc: 0.7375 - val_loss: 0.8413 - val_acc: 0.6170\n",
            "Epoch 205/300\n",
            "\n",
            "Epoch 00205: val_loss did not improve from 0.79342\n",
            "240/240 - 0s - loss: 0.6597 - acc: 0.7667 - val_loss: 0.8163 - val_acc: 0.6596\n",
            "Epoch 206/300\n",
            "\n",
            "Epoch 00206: val_loss did not improve from 0.79342\n",
            "240/240 - 0s - loss: 0.6435 - acc: 0.8000 - val_loss: 0.8542 - val_acc: 0.5745\n",
            "Epoch 207/300\n",
            "\n",
            "Epoch 00207: val_loss did not improve from 0.79342\n",
            "240/240 - 0s - loss: 0.6522 - acc: 0.7708 - val_loss: 0.8359 - val_acc: 0.6170\n",
            "Epoch 208/300\n",
            "\n",
            "Epoch 00208: val_loss did not improve from 0.79342\n",
            "240/240 - 0s - loss: 0.6199 - acc: 0.8000 - val_loss: 0.8317 - val_acc: 0.6170\n",
            "Epoch 209/300\n",
            "\n",
            "Epoch 00209: val_loss did not improve from 0.79342\n",
            "240/240 - 0s - loss: 0.6168 - acc: 0.8000 - val_loss: 0.8081 - val_acc: 0.5957\n",
            "Epoch 210/300\n",
            "\n",
            "Epoch 00210: val_loss did not improve from 0.79342\n",
            "240/240 - 0s - loss: 0.6285 - acc: 0.7875 - val_loss: 0.8370 - val_acc: 0.6170\n",
            "Epoch 211/300\n",
            "\n",
            "Epoch 00211: val_loss did not improve from 0.79342\n",
            "240/240 - 0s - loss: 0.6508 - acc: 0.7667 - val_loss: 0.8251 - val_acc: 0.5957\n",
            "Epoch 212/300\n",
            "\n",
            "Epoch 00212: val_loss did not improve from 0.79342\n",
            "240/240 - 0s - loss: 0.6296 - acc: 0.7792 - val_loss: 0.8294 - val_acc: 0.5957\n",
            "Epoch 213/300\n",
            "\n",
            "Epoch 00213: val_loss did not improve from 0.79342\n",
            "240/240 - 0s - loss: 0.6587 - acc: 0.7625 - val_loss: 0.8163 - val_acc: 0.6170\n",
            "Epoch 214/300\n",
            "\n",
            "Epoch 00214: val_loss did not improve from 0.79342\n",
            "240/240 - 0s - loss: 0.6492 - acc: 0.7542 - val_loss: 0.8220 - val_acc: 0.6170\n",
            "Epoch 215/300\n",
            "\n",
            "Epoch 00215: val_loss did not improve from 0.79342\n",
            "240/240 - 0s - loss: 0.6234 - acc: 0.7375 - val_loss: 0.8523 - val_acc: 0.5745\n",
            "Epoch 216/300\n",
            "\n",
            "Epoch 00216: val_loss did not improve from 0.79342\n",
            "240/240 - 0s - loss: 0.6394 - acc: 0.8083 - val_loss: 0.8271 - val_acc: 0.6170\n",
            "Epoch 217/300\n",
            "\n",
            "Epoch 00217: val_loss did not improve from 0.79342\n",
            "240/240 - 0s - loss: 0.6128 - acc: 0.7958 - val_loss: 0.8193 - val_acc: 0.6809\n",
            "Epoch 218/300\n",
            "\n",
            "Epoch 00218: val_loss did not improve from 0.79342\n",
            "240/240 - 0s - loss: 0.6104 - acc: 0.8250 - val_loss: 0.8295 - val_acc: 0.5957\n",
            "Epoch 219/300\n",
            "\n",
            "Epoch 00219: val_loss did not improve from 0.79342\n",
            "240/240 - 0s - loss: 0.6451 - acc: 0.7708 - val_loss: 0.9108 - val_acc: 0.5532\n",
            "Epoch 220/300\n",
            "\n",
            "Epoch 00220: val_loss did not improve from 0.79342\n",
            "240/240 - 0s - loss: 0.6825 - acc: 0.7417 - val_loss: 0.8531 - val_acc: 0.6170\n",
            "Epoch 221/300\n",
            "\n",
            "Epoch 00221: val_loss did not improve from 0.79342\n",
            "240/240 - 0s - loss: 0.6307 - acc: 0.7875 - val_loss: 0.9107 - val_acc: 0.5745\n",
            "Epoch 222/300\n",
            "\n",
            "Epoch 00222: val_loss did not improve from 0.79342\n",
            "240/240 - 0s - loss: 0.6507 - acc: 0.7667 - val_loss: 0.8467 - val_acc: 0.6170\n",
            "Epoch 223/300\n",
            "\n",
            "Epoch 00223: val_loss did not improve from 0.79342\n",
            "240/240 - 0s - loss: 0.6278 - acc: 0.7625 - val_loss: 0.8628 - val_acc: 0.6596\n",
            "Epoch 224/300\n",
            "\n",
            "Epoch 00224: val_loss did not improve from 0.79342\n",
            "240/240 - 0s - loss: 0.6033 - acc: 0.8042 - val_loss: 0.8361 - val_acc: 0.6383\n",
            "Epoch 225/300\n",
            "\n",
            "Epoch 00225: val_loss did not improve from 0.79342\n",
            "240/240 - 0s - loss: 0.5754 - acc: 0.8083 - val_loss: 0.8000 - val_acc: 0.6596\n",
            "Epoch 226/300\n",
            "\n",
            "Epoch 00226: val_loss improved from 0.79342 to 0.79109, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.5599 - acc: 0.8417 - val_loss: 0.7911 - val_acc: 0.6596\n",
            "Epoch 227/300\n",
            "\n",
            "Epoch 00227: val_loss did not improve from 0.79109\n",
            "240/240 - 0s - loss: 0.6382 - acc: 0.7375 - val_loss: 0.8047 - val_acc: 0.6809\n",
            "Epoch 228/300\n",
            "\n",
            "Epoch 00228: val_loss did not improve from 0.79109\n",
            "240/240 - 0s - loss: 0.6002 - acc: 0.7958 - val_loss: 0.8150 - val_acc: 0.6596\n",
            "Epoch 229/300\n",
            "\n",
            "Epoch 00229: val_loss did not improve from 0.79109\n",
            "240/240 - 0s - loss: 0.6261 - acc: 0.7833 - val_loss: 0.8118 - val_acc: 0.7021\n",
            "Epoch 230/300\n",
            "\n",
            "Epoch 00230: val_loss did not improve from 0.79109\n",
            "240/240 - 0s - loss: 0.6432 - acc: 0.7500 - val_loss: 0.8181 - val_acc: 0.6809\n",
            "Epoch 231/300\n",
            "\n",
            "Epoch 00231: val_loss did not improve from 0.79109\n",
            "240/240 - 0s - loss: 0.6071 - acc: 0.7833 - val_loss: 0.8606 - val_acc: 0.6383\n",
            "Epoch 232/300\n",
            "\n",
            "Epoch 00232: val_loss did not improve from 0.79109\n",
            "240/240 - 0s - loss: 0.6029 - acc: 0.7875 - val_loss: 0.8714 - val_acc: 0.5957\n",
            "Epoch 233/300\n",
            "\n",
            "Epoch 00233: val_loss did not improve from 0.79109\n",
            "240/240 - 0s - loss: 0.5818 - acc: 0.8167 - val_loss: 0.8273 - val_acc: 0.6596\n",
            "Epoch 234/300\n",
            "\n",
            "Epoch 00234: val_loss did not improve from 0.79109\n",
            "240/240 - 0s - loss: 0.5906 - acc: 0.8042 - val_loss: 0.8376 - val_acc: 0.5957\n",
            "Epoch 235/300\n",
            "\n",
            "Epoch 00235: val_loss did not improve from 0.79109\n",
            "240/240 - 0s - loss: 0.6339 - acc: 0.7625 - val_loss: 0.8689 - val_acc: 0.6596\n",
            "Epoch 236/300\n",
            "\n",
            "Epoch 00236: val_loss did not improve from 0.79109\n",
            "240/240 - 0s - loss: 0.6059 - acc: 0.7750 - val_loss: 0.9070 - val_acc: 0.5745\n",
            "Epoch 237/300\n",
            "\n",
            "Epoch 00237: val_loss did not improve from 0.79109\n",
            "240/240 - 0s - loss: 0.5671 - acc: 0.8000 - val_loss: 0.8178 - val_acc: 0.6809\n",
            "Epoch 238/300\n",
            "\n",
            "Epoch 00238: val_loss did not improve from 0.79109\n",
            "240/240 - 0s - loss: 0.5579 - acc: 0.8250 - val_loss: 0.8573 - val_acc: 0.6170\n",
            "Epoch 239/300\n",
            "\n",
            "Epoch 00239: val_loss did not improve from 0.79109\n",
            "240/240 - 0s - loss: 0.5590 - acc: 0.8208 - val_loss: 0.8370 - val_acc: 0.6170\n",
            "Epoch 240/300\n",
            "\n",
            "Epoch 00240: val_loss did not improve from 0.79109\n",
            "240/240 - 0s - loss: 0.6218 - acc: 0.8167 - val_loss: 0.8393 - val_acc: 0.6383\n",
            "Epoch 241/300\n",
            "\n",
            "Epoch 00241: val_loss did not improve from 0.79109\n",
            "240/240 - 0s - loss: 0.5745 - acc: 0.8042 - val_loss: 0.8628 - val_acc: 0.6383\n",
            "Epoch 242/300\n",
            "\n",
            "Epoch 00242: val_loss did not improve from 0.79109\n",
            "240/240 - 0s - loss: 0.6967 - acc: 0.7375 - val_loss: 0.8484 - val_acc: 0.5957\n",
            "Epoch 243/300\n",
            "\n",
            "Epoch 00243: val_loss did not improve from 0.79109\n",
            "240/240 - 0s - loss: 0.6194 - acc: 0.7875 - val_loss: 0.8060 - val_acc: 0.6383\n",
            "Epoch 244/300\n",
            "\n",
            "Epoch 00244: val_loss did not improve from 0.79109\n",
            "240/240 - 0s - loss: 0.5817 - acc: 0.8250 - val_loss: 0.8260 - val_acc: 0.6170\n",
            "Epoch 245/300\n",
            "\n",
            "Epoch 00245: val_loss did not improve from 0.79109\n",
            "240/240 - 0s - loss: 0.6359 - acc: 0.7583 - val_loss: 0.8627 - val_acc: 0.6596\n",
            "Epoch 246/300\n",
            "\n",
            "Epoch 00246: val_loss did not improve from 0.79109\n",
            "240/240 - 0s - loss: 0.6107 - acc: 0.7833 - val_loss: 0.9027 - val_acc: 0.5957\n",
            "Epoch 247/300\n",
            "\n",
            "Epoch 00247: val_loss did not improve from 0.79109\n",
            "240/240 - 0s - loss: 0.6396 - acc: 0.7625 - val_loss: 0.8562 - val_acc: 0.6170\n",
            "Epoch 248/300\n",
            "\n",
            "Epoch 00248: val_loss did not improve from 0.79109\n",
            "240/240 - 0s - loss: 0.6008 - acc: 0.7875 - val_loss: 0.8934 - val_acc: 0.5745\n",
            "Epoch 249/300\n",
            "\n",
            "Epoch 00249: val_loss did not improve from 0.79109\n",
            "240/240 - 0s - loss: 0.6273 - acc: 0.7375 - val_loss: 0.8630 - val_acc: 0.5957\n",
            "Epoch 250/300\n",
            "\n",
            "Epoch 00250: val_loss did not improve from 0.79109\n",
            "240/240 - 0s - loss: 0.6126 - acc: 0.8042 - val_loss: 0.8405 - val_acc: 0.5745\n",
            "Epoch 251/300\n",
            "\n",
            "Epoch 00251: val_loss did not improve from 0.79109\n",
            "240/240 - 0s - loss: 0.5507 - acc: 0.8250 - val_loss: 0.8170 - val_acc: 0.5957\n",
            "Epoch 252/300\n",
            "\n",
            "Epoch 00252: val_loss did not improve from 0.79109\n",
            "240/240 - 0s - loss: 0.5592 - acc: 0.8208 - val_loss: 0.8154 - val_acc: 0.5957\n",
            "Epoch 253/300\n",
            "\n",
            "Epoch 00253: val_loss improved from 0.79109 to 0.78231, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.6148 - acc: 0.8083 - val_loss: 0.7823 - val_acc: 0.5957\n",
            "Epoch 254/300\n",
            "\n",
            "Epoch 00254: val_loss did not improve from 0.78231\n",
            "240/240 - 0s - loss: 0.5714 - acc: 0.7917 - val_loss: 0.7894 - val_acc: 0.5957\n",
            "Epoch 255/300\n",
            "\n",
            "Epoch 00255: val_loss improved from 0.78231 to 0.77262, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.5381 - acc: 0.8250 - val_loss: 0.7726 - val_acc: 0.6383\n",
            "Epoch 256/300\n",
            "\n",
            "Epoch 00256: val_loss did not improve from 0.77262\n",
            "240/240 - 0s - loss: 0.5824 - acc: 0.7917 - val_loss: 0.7900 - val_acc: 0.6383\n",
            "Epoch 257/300\n",
            "\n",
            "Epoch 00257: val_loss did not improve from 0.77262\n",
            "240/240 - 0s - loss: 0.5636 - acc: 0.8375 - val_loss: 0.8311 - val_acc: 0.5957\n",
            "Epoch 258/300\n",
            "\n",
            "Epoch 00258: val_loss did not improve from 0.77262\n",
            "240/240 - 0s - loss: 0.5552 - acc: 0.8125 - val_loss: 0.8086 - val_acc: 0.5957\n",
            "Epoch 259/300\n",
            "\n",
            "Epoch 00259: val_loss did not improve from 0.77262\n",
            "240/240 - 0s - loss: 0.5969 - acc: 0.7750 - val_loss: 0.8247 - val_acc: 0.6170\n",
            "Epoch 260/300\n",
            "\n",
            "Epoch 00260: val_loss did not improve from 0.77262\n",
            "240/240 - 0s - loss: 0.5715 - acc: 0.8000 - val_loss: 0.8683 - val_acc: 0.6170\n",
            "Epoch 261/300\n",
            "\n",
            "Epoch 00261: val_loss did not improve from 0.77262\n",
            "240/240 - 0s - loss: 0.5888 - acc: 0.8083 - val_loss: 0.8612 - val_acc: 0.5745\n",
            "Epoch 262/300\n",
            "\n",
            "Epoch 00262: val_loss did not improve from 0.77262\n",
            "240/240 - 0s - loss: 0.5802 - acc: 0.8167 - val_loss: 0.8817 - val_acc: 0.5745\n",
            "Epoch 263/300\n",
            "\n",
            "Epoch 00263: val_loss did not improve from 0.77262\n",
            "240/240 - 0s - loss: 0.5615 - acc: 0.8125 - val_loss: 0.8994 - val_acc: 0.5532\n",
            "Epoch 264/300\n",
            "\n",
            "Epoch 00264: val_loss did not improve from 0.77262\n",
            "240/240 - 0s - loss: 0.6108 - acc: 0.7833 - val_loss: 0.8670 - val_acc: 0.5957\n",
            "Epoch 265/300\n",
            "\n",
            "Epoch 00265: val_loss did not improve from 0.77262\n",
            "240/240 - 0s - loss: 0.5863 - acc: 0.7917 - val_loss: 0.8515 - val_acc: 0.5957\n",
            "Epoch 266/300\n",
            "\n",
            "Epoch 00266: val_loss did not improve from 0.77262\n",
            "240/240 - 0s - loss: 0.5411 - acc: 0.8375 - val_loss: 0.8458 - val_acc: 0.5745\n",
            "Epoch 267/300\n",
            "\n",
            "Epoch 00267: val_loss did not improve from 0.77262\n",
            "240/240 - 0s - loss: 0.6067 - acc: 0.7708 - val_loss: 0.8667 - val_acc: 0.5745\n",
            "Epoch 268/300\n",
            "\n",
            "Epoch 00268: val_loss did not improve from 0.77262\n",
            "240/240 - 0s - loss: 0.5383 - acc: 0.8250 - val_loss: 0.8567 - val_acc: 0.5957\n",
            "Epoch 269/300\n",
            "\n",
            "Epoch 00269: val_loss did not improve from 0.77262\n",
            "240/240 - 0s - loss: 0.5566 - acc: 0.8042 - val_loss: 0.8153 - val_acc: 0.5319\n",
            "Epoch 270/300\n",
            "\n",
            "Epoch 00270: val_loss did not improve from 0.77262\n",
            "240/240 - 0s - loss: 0.5341 - acc: 0.8458 - val_loss: 0.8096 - val_acc: 0.5745\n",
            "Epoch 271/300\n",
            "\n",
            "Epoch 00271: val_loss did not improve from 0.77262\n",
            "240/240 - 0s - loss: 0.5402 - acc: 0.8167 - val_loss: 0.8371 - val_acc: 0.5745\n",
            "Epoch 272/300\n",
            "\n",
            "Epoch 00272: val_loss did not improve from 0.77262\n",
            "240/240 - 0s - loss: 0.6106 - acc: 0.7667 - val_loss: 0.8187 - val_acc: 0.6170\n",
            "Epoch 273/300\n",
            "\n",
            "Epoch 00273: val_loss did not improve from 0.77262\n",
            "240/240 - 0s - loss: 0.5422 - acc: 0.8125 - val_loss: 0.8618 - val_acc: 0.6170\n",
            "Epoch 274/300\n",
            "\n",
            "Epoch 00274: val_loss did not improve from 0.77262\n",
            "240/240 - 0s - loss: 0.6068 - acc: 0.8083 - val_loss: 0.8301 - val_acc: 0.6170\n",
            "Epoch 275/300\n",
            "\n",
            "Epoch 00275: val_loss did not improve from 0.77262\n",
            "240/240 - 0s - loss: 0.5480 - acc: 0.8375 - val_loss: 0.8681 - val_acc: 0.6170\n",
            "Epoch 276/300\n",
            "\n",
            "Epoch 00276: val_loss did not improve from 0.77262\n",
            "240/240 - 0s - loss: 0.6076 - acc: 0.7667 - val_loss: 0.8635 - val_acc: 0.6809\n",
            "Epoch 277/300\n",
            "\n",
            "Epoch 00277: val_loss did not improve from 0.77262\n",
            "240/240 - 0s - loss: 0.5948 - acc: 0.8250 - val_loss: 0.8295 - val_acc: 0.5957\n",
            "Epoch 278/300\n",
            "\n",
            "Epoch 00278: val_loss did not improve from 0.77262\n",
            "240/240 - 0s - loss: 0.6229 - acc: 0.7333 - val_loss: 0.8052 - val_acc: 0.6170\n",
            "Epoch 279/300\n",
            "\n",
            "Epoch 00279: val_loss did not improve from 0.77262\n",
            "240/240 - 0s - loss: 0.5572 - acc: 0.8333 - val_loss: 0.7966 - val_acc: 0.6170\n",
            "Epoch 280/300\n",
            "\n",
            "Epoch 00280: val_loss did not improve from 0.77262\n",
            "240/240 - 0s - loss: 0.5721 - acc: 0.7958 - val_loss: 0.8124 - val_acc: 0.6383\n",
            "Epoch 281/300\n",
            "\n",
            "Epoch 00281: val_loss did not improve from 0.77262\n",
            "240/240 - 0s - loss: 0.6007 - acc: 0.7500 - val_loss: 0.8445 - val_acc: 0.6170\n",
            "Epoch 282/300\n",
            "\n",
            "Epoch 00282: val_loss did not improve from 0.77262\n",
            "240/240 - 0s - loss: 0.5981 - acc: 0.7750 - val_loss: 0.8661 - val_acc: 0.5957\n",
            "Epoch 283/300\n",
            "\n",
            "Epoch 00283: val_loss did not improve from 0.77262\n",
            "240/240 - 0s - loss: 0.5779 - acc: 0.8167 - val_loss: 0.8523 - val_acc: 0.5957\n",
            "Epoch 284/300\n",
            "\n",
            "Epoch 00284: val_loss did not improve from 0.77262\n",
            "240/240 - 0s - loss: 0.5953 - acc: 0.7958 - val_loss: 0.8437 - val_acc: 0.5957\n",
            "Epoch 285/300\n",
            "\n",
            "Epoch 00285: val_loss did not improve from 0.77262\n",
            "240/240 - 0s - loss: 0.5430 - acc: 0.8333 - val_loss: 0.8704 - val_acc: 0.5957\n",
            "Epoch 286/300\n",
            "\n",
            "Epoch 00286: val_loss did not improve from 0.77262\n",
            "240/240 - 0s - loss: 0.5913 - acc: 0.8083 - val_loss: 0.8918 - val_acc: 0.5532\n",
            "Epoch 287/300\n",
            "\n",
            "Epoch 00287: val_loss did not improve from 0.77262\n",
            "240/240 - 0s - loss: 0.5545 - acc: 0.8083 - val_loss: 0.8724 - val_acc: 0.5957\n",
            "Epoch 288/300\n",
            "\n",
            "Epoch 00288: val_loss did not improve from 0.77262\n",
            "240/240 - 0s - loss: 0.5575 - acc: 0.8083 - val_loss: 0.8754 - val_acc: 0.5319\n",
            "Epoch 289/300\n",
            "\n",
            "Epoch 00289: val_loss did not improve from 0.77262\n",
            "240/240 - 0s - loss: 0.5210 - acc: 0.8250 - val_loss: 0.8423 - val_acc: 0.6383\n",
            "Epoch 290/300\n",
            "\n",
            "Epoch 00290: val_loss did not improve from 0.77262\n",
            "240/240 - 0s - loss: 0.5446 - acc: 0.8125 - val_loss: 0.8156 - val_acc: 0.6383\n",
            "Epoch 291/300\n",
            "\n",
            "Epoch 00291: val_loss did not improve from 0.77262\n",
            "240/240 - 0s - loss: 0.5093 - acc: 0.8625 - val_loss: 0.8588 - val_acc: 0.5957\n",
            "Epoch 292/300\n",
            "\n",
            "Epoch 00292: val_loss did not improve from 0.77262\n",
            "240/240 - 0s - loss: 0.5577 - acc: 0.7917 - val_loss: 0.8431 - val_acc: 0.6170\n",
            "Epoch 293/300\n",
            "\n",
            "Epoch 00293: val_loss did not improve from 0.77262\n",
            "240/240 - 0s - loss: 0.5612 - acc: 0.7958 - val_loss: 0.8722 - val_acc: 0.6170\n",
            "Epoch 294/300\n",
            "\n",
            "Epoch 00294: val_loss did not improve from 0.77262\n",
            "240/240 - 0s - loss: 0.5467 - acc: 0.7792 - val_loss: 0.8045 - val_acc: 0.5957\n",
            "Epoch 295/300\n",
            "\n",
            "Epoch 00295: val_loss did not improve from 0.77262\n",
            "240/240 - 0s - loss: 0.5724 - acc: 0.8167 - val_loss: 0.8557 - val_acc: 0.6170\n",
            "Epoch 296/300\n",
            "\n",
            "Epoch 00296: val_loss did not improve from 0.77262\n",
            "240/240 - 0s - loss: 0.6242 - acc: 0.7625 - val_loss: 0.8422 - val_acc: 0.6596\n",
            "Epoch 297/300\n",
            "\n",
            "Epoch 00297: val_loss did not improve from 0.77262\n",
            "240/240 - 0s - loss: 0.5303 - acc: 0.8250 - val_loss: 0.8705 - val_acc: 0.5957\n",
            "Epoch 298/300\n",
            "\n",
            "Epoch 00298: val_loss did not improve from 0.77262\n",
            "240/240 - 0s - loss: 0.5874 - acc: 0.8208 - val_loss: 0.9129 - val_acc: 0.5957\n",
            "Epoch 299/300\n",
            "\n",
            "Epoch 00299: val_loss did not improve from 0.77262\n",
            "240/240 - 0s - loss: 0.5536 - acc: 0.8250 - val_loss: 0.9097 - val_acc: 0.6170\n",
            "Epoch 300/300\n",
            "\n",
            "Epoch 00300: val_loss did not improve from 0.77262\n",
            "240/240 - 0s - loss: 0.5527 - acc: 0.8083 - val_loss: 0.8516 - val_acc: 0.6596\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhmHEHBMEhmEYMed/AQI8i8YzNK5dAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "# Evaluate on test data\n",
            "288/288 [==============================] - 1s 2ms/sample - loss: 0.9166 - acc: 0.6285\n",
            "test loss, test acc: [0.9165536679987175, 0.6284722]\n",
            "EEG_Deep/Data2A/Data_A05T.mat\n",
            "(288, 22, 1875)\n",
            "(288, 1)\n",
            "(240, 22, 750)\n",
            "(240, 1)\n",
            "(47, 22, 750)\n",
            "(47, 1)\n",
            "X_train shape: (240, 1, 22, 750)\n",
            "240 train samples\n",
            "47 val samples\n",
            "EEG_Deep/Data2A/Data_A05E.mat\n",
            "(288, 22, 1875)\n",
            "(288, 1)\n",
            "(288, 22, 750)\n",
            "(288, 1)\n",
            "X_train shape: (288, 1, 22, 750)\n",
            "288 train samples\n",
            "Train on 240 samples, validate on 47 samples\n",
            "Epoch 1/300\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 1.38440, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 1s - loss: 1.4082 - acc: 0.2375 - val_loss: 1.3844 - val_acc: 0.2766\n",
            "Epoch 2/300\n",
            "\n",
            "Epoch 00002: val_loss improved from 1.38440 to 1.38326, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.3774 - acc: 0.2958 - val_loss: 1.3833 - val_acc: 0.3617\n",
            "Epoch 3/300\n",
            "\n",
            "Epoch 00003: val_loss improved from 1.38326 to 1.38251, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.3654 - acc: 0.3292 - val_loss: 1.3825 - val_acc: 0.3617\n",
            "Epoch 4/300\n",
            "\n",
            "Epoch 00004: val_loss improved from 1.38251 to 1.38093, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.3430 - acc: 0.4125 - val_loss: 1.3809 - val_acc: 0.3830\n",
            "Epoch 5/300\n",
            "\n",
            "Epoch 00005: val_loss improved from 1.38093 to 1.37813, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.3327 - acc: 0.4083 - val_loss: 1.3781 - val_acc: 0.3191\n",
            "Epoch 6/300\n",
            "\n",
            "Epoch 00006: val_loss improved from 1.37813 to 1.37441, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.3277 - acc: 0.3750 - val_loss: 1.3744 - val_acc: 0.2979\n",
            "Epoch 7/300\n",
            "\n",
            "Epoch 00007: val_loss improved from 1.37441 to 1.36974, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.3015 - acc: 0.3875 - val_loss: 1.3697 - val_acc: 0.3191\n",
            "Epoch 8/300\n",
            "\n",
            "Epoch 00008: val_loss improved from 1.36974 to 1.36735, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.2903 - acc: 0.4583 - val_loss: 1.3673 - val_acc: 0.2553\n",
            "Epoch 9/300\n",
            "\n",
            "Epoch 00009: val_loss improved from 1.36735 to 1.36206, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.2801 - acc: 0.5042 - val_loss: 1.3621 - val_acc: 0.2979\n",
            "Epoch 10/300\n",
            "\n",
            "Epoch 00010: val_loss improved from 1.36206 to 1.35819, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.2591 - acc: 0.5500 - val_loss: 1.3582 - val_acc: 0.3191\n",
            "Epoch 11/300\n",
            "\n",
            "Epoch 00011: val_loss improved from 1.35819 to 1.35236, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.2526 - acc: 0.5292 - val_loss: 1.3524 - val_acc: 0.3617\n",
            "Epoch 12/300\n",
            "\n",
            "Epoch 00012: val_loss improved from 1.35236 to 1.35113, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.2424 - acc: 0.5208 - val_loss: 1.3511 - val_acc: 0.3191\n",
            "Epoch 13/300\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 1.35113\n",
            "240/240 - 0s - loss: 1.2302 - acc: 0.5375 - val_loss: 1.3523 - val_acc: 0.2553\n",
            "Epoch 14/300\n",
            "\n",
            "Epoch 00014: val_loss improved from 1.35113 to 1.34952, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.2215 - acc: 0.5292 - val_loss: 1.3495 - val_acc: 0.2766\n",
            "Epoch 15/300\n",
            "\n",
            "Epoch 00015: val_loss improved from 1.34952 to 1.34561, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.2217 - acc: 0.5167 - val_loss: 1.3456 - val_acc: 0.2766\n",
            "Epoch 16/300\n",
            "\n",
            "Epoch 00016: val_loss improved from 1.34561 to 1.33991, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.1931 - acc: 0.5708 - val_loss: 1.3399 - val_acc: 0.3617\n",
            "Epoch 17/300\n",
            "\n",
            "Epoch 00017: val_loss improved from 1.33991 to 1.33018, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.1852 - acc: 0.5250 - val_loss: 1.3302 - val_acc: 0.3404\n",
            "Epoch 18/300\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 1.33018\n",
            "240/240 - 0s - loss: 1.1977 - acc: 0.5375 - val_loss: 1.3417 - val_acc: 0.2766\n",
            "Epoch 19/300\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 1.33018\n",
            "240/240 - 0s - loss: 1.1784 - acc: 0.5667 - val_loss: 1.3434 - val_acc: 0.3404\n",
            "Epoch 20/300\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 1.33018\n",
            "240/240 - 0s - loss: 1.1827 - acc: 0.5458 - val_loss: 1.3569 - val_acc: 0.3191\n",
            "Epoch 21/300\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 1.33018\n",
            "240/240 - 0s - loss: 1.1911 - acc: 0.5458 - val_loss: 1.3531 - val_acc: 0.2553\n",
            "Epoch 22/300\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 1.33018\n",
            "240/240 - 0s - loss: 1.1691 - acc: 0.5792 - val_loss: 1.3445 - val_acc: 0.3404\n",
            "Epoch 23/300\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 1.33018\n",
            "240/240 - 0s - loss: 1.1706 - acc: 0.5417 - val_loss: 1.3380 - val_acc: 0.3191\n",
            "Epoch 24/300\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 1.33018\n",
            "240/240 - 0s - loss: 1.1387 - acc: 0.5583 - val_loss: 1.3418 - val_acc: 0.3404\n",
            "Epoch 25/300\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 1.33018\n",
            "240/240 - 0s - loss: 1.1525 - acc: 0.5708 - val_loss: 1.3396 - val_acc: 0.3191\n",
            "Epoch 26/300\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 1.33018\n",
            "240/240 - 0s - loss: 1.1518 - acc: 0.5708 - val_loss: 1.3372 - val_acc: 0.2979\n",
            "Epoch 27/300\n",
            "\n",
            "Epoch 00027: val_loss improved from 1.33018 to 1.32797, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.1516 - acc: 0.5625 - val_loss: 1.3280 - val_acc: 0.3191\n",
            "Epoch 28/300\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 1.32797\n",
            "240/240 - 0s - loss: 1.1455 - acc: 0.5333 - val_loss: 1.3335 - val_acc: 0.2979\n",
            "Epoch 29/300\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 1.32797\n",
            "240/240 - 0s - loss: 1.1265 - acc: 0.5750 - val_loss: 1.3316 - val_acc: 0.3191\n",
            "Epoch 30/300\n",
            "\n",
            "Epoch 00030: val_loss improved from 1.32797 to 1.32518, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.1450 - acc: 0.5500 - val_loss: 1.3252 - val_acc: 0.2979\n",
            "Epoch 31/300\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 1.32518\n",
            "240/240 - 0s - loss: 1.1327 - acc: 0.5708 - val_loss: 1.3298 - val_acc: 0.2553\n",
            "Epoch 32/300\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 1.32518\n",
            "240/240 - 0s - loss: 1.1270 - acc: 0.5958 - val_loss: 1.3264 - val_acc: 0.2553\n",
            "Epoch 33/300\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 1.32518\n",
            "240/240 - 0s - loss: 1.1113 - acc: 0.5875 - val_loss: 1.3346 - val_acc: 0.2128\n",
            "Epoch 34/300\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 1.32518\n",
            "240/240 - 0s - loss: 1.1202 - acc: 0.5958 - val_loss: 1.3358 - val_acc: 0.3191\n",
            "Epoch 35/300\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 1.32518\n",
            "240/240 - 0s - loss: 1.1329 - acc: 0.6292 - val_loss: 1.3374 - val_acc: 0.2766\n",
            "Epoch 36/300\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 1.32518\n",
            "240/240 - 0s - loss: 1.1038 - acc: 0.5750 - val_loss: 1.3428 - val_acc: 0.2553\n",
            "Epoch 37/300\n",
            "\n",
            "Epoch 00037: val_loss improved from 1.32518 to 1.32442, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.1161 - acc: 0.6042 - val_loss: 1.3244 - val_acc: 0.2553\n",
            "Epoch 38/300\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 1.32442\n",
            "240/240 - 0s - loss: 1.1111 - acc: 0.6083 - val_loss: 1.3261 - val_acc: 0.2766\n",
            "Epoch 39/300\n",
            "\n",
            "Epoch 00039: val_loss improved from 1.32442 to 1.31331, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.1118 - acc: 0.5583 - val_loss: 1.3133 - val_acc: 0.2766\n",
            "Epoch 40/300\n",
            "\n",
            "Epoch 00040: val_loss improved from 1.31331 to 1.30528, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.0984 - acc: 0.5833 - val_loss: 1.3053 - val_acc: 0.2979\n",
            "Epoch 41/300\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 1.30528\n",
            "240/240 - 0s - loss: 1.0965 - acc: 0.6083 - val_loss: 1.3074 - val_acc: 0.2766\n",
            "Epoch 42/300\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 1.30528\n",
            "240/240 - 0s - loss: 1.0810 - acc: 0.5792 - val_loss: 1.3175 - val_acc: 0.2766\n",
            "Epoch 43/300\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 1.30528\n",
            "240/240 - 0s - loss: 1.1047 - acc: 0.5667 - val_loss: 1.3192 - val_acc: 0.2340\n",
            "Epoch 44/300\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 1.30528\n",
            "240/240 - 0s - loss: 1.0884 - acc: 0.6167 - val_loss: 1.3254 - val_acc: 0.2128\n",
            "Epoch 45/300\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 1.30528\n",
            "240/240 - 0s - loss: 1.1004 - acc: 0.5667 - val_loss: 1.3138 - val_acc: 0.2766\n",
            "Epoch 46/300\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 1.30528\n",
            "240/240 - 0s - loss: 1.0840 - acc: 0.6083 - val_loss: 1.3128 - val_acc: 0.2979\n",
            "Epoch 47/300\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 1.30528\n",
            "240/240 - 0s - loss: 1.1111 - acc: 0.5625 - val_loss: 1.3074 - val_acc: 0.3617\n",
            "Epoch 48/300\n",
            "\n",
            "Epoch 00048: val_loss improved from 1.30528 to 1.29531, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.0689 - acc: 0.6125 - val_loss: 1.2953 - val_acc: 0.3617\n",
            "Epoch 49/300\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 1.29531\n",
            "240/240 - 0s - loss: 1.0729 - acc: 0.5833 - val_loss: 1.3079 - val_acc: 0.3191\n",
            "Epoch 50/300\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 1.29531\n",
            "240/240 - 0s - loss: 1.0853 - acc: 0.6000 - val_loss: 1.3088 - val_acc: 0.2979\n",
            "Epoch 51/300\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 1.29531\n",
            "240/240 - 0s - loss: 1.0508 - acc: 0.6125 - val_loss: 1.3263 - val_acc: 0.2979\n",
            "Epoch 52/300\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 1.29531\n",
            "240/240 - 0s - loss: 1.0689 - acc: 0.5958 - val_loss: 1.3271 - val_acc: 0.3404\n",
            "Epoch 53/300\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 1.29531\n",
            "240/240 - 0s - loss: 1.0656 - acc: 0.6042 - val_loss: 1.3205 - val_acc: 0.3191\n",
            "Epoch 54/300\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 1.29531\n",
            "240/240 - 0s - loss: 1.0723 - acc: 0.5542 - val_loss: 1.3305 - val_acc: 0.2766\n",
            "Epoch 55/300\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 1.29531\n",
            "240/240 - 0s - loss: 1.0374 - acc: 0.6333 - val_loss: 1.3173 - val_acc: 0.2766\n",
            "Epoch 56/300\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 1.29531\n",
            "240/240 - 0s - loss: 1.0465 - acc: 0.6375 - val_loss: 1.3030 - val_acc: 0.2979\n",
            "Epoch 57/300\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 1.29531\n",
            "240/240 - 0s - loss: 1.0451 - acc: 0.6250 - val_loss: 1.3016 - val_acc: 0.3404\n",
            "Epoch 58/300\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 1.29531\n",
            "240/240 - 0s - loss: 1.0634 - acc: 0.6292 - val_loss: 1.3080 - val_acc: 0.3404\n",
            "Epoch 59/300\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 1.29531\n",
            "240/240 - 0s - loss: 1.0352 - acc: 0.6417 - val_loss: 1.3088 - val_acc: 0.3404\n",
            "Epoch 60/300\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 1.29531\n",
            "240/240 - 0s - loss: 1.0538 - acc: 0.5958 - val_loss: 1.3333 - val_acc: 0.3191\n",
            "Epoch 61/300\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 1.29531\n",
            "240/240 - 0s - loss: 1.0355 - acc: 0.6125 - val_loss: 1.3258 - val_acc: 0.3617\n",
            "Epoch 62/300\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 1.29531\n",
            "240/240 - 0s - loss: 1.0262 - acc: 0.6000 - val_loss: 1.3111 - val_acc: 0.3191\n",
            "Epoch 63/300\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 1.29531\n",
            "240/240 - 0s - loss: 1.0114 - acc: 0.6250 - val_loss: 1.3269 - val_acc: 0.3617\n",
            "Epoch 64/300\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 1.29531\n",
            "240/240 - 0s - loss: 1.0487 - acc: 0.6000 - val_loss: 1.3106 - val_acc: 0.3617\n",
            "Epoch 65/300\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 1.29531\n",
            "240/240 - 0s - loss: 1.0208 - acc: 0.6250 - val_loss: 1.3039 - val_acc: 0.3617\n",
            "Epoch 66/300\n",
            "\n",
            "Epoch 00066: val_loss improved from 1.29531 to 1.29304, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.0083 - acc: 0.6250 - val_loss: 1.2930 - val_acc: 0.3617\n",
            "Epoch 67/300\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 1.29304\n",
            "240/240 - 0s - loss: 0.9985 - acc: 0.6500 - val_loss: 1.2991 - val_acc: 0.3617\n",
            "Epoch 68/300\n",
            "\n",
            "Epoch 00068: val_loss improved from 1.29304 to 1.28881, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.0136 - acc: 0.6167 - val_loss: 1.2888 - val_acc: 0.2979\n",
            "Epoch 69/300\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 1.28881\n",
            "240/240 - 0s - loss: 1.0221 - acc: 0.6375 - val_loss: 1.3050 - val_acc: 0.3830\n",
            "Epoch 70/300\n",
            "\n",
            "Epoch 00070: val_loss improved from 1.28881 to 1.28529, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.0322 - acc: 0.6333 - val_loss: 1.2853 - val_acc: 0.3404\n",
            "Epoch 71/300\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 1.28529\n",
            "240/240 - 0s - loss: 0.9733 - acc: 0.7042 - val_loss: 1.2942 - val_acc: 0.3830\n",
            "Epoch 72/300\n",
            "\n",
            "Epoch 00072: val_loss improved from 1.28529 to 1.27092, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.0005 - acc: 0.6125 - val_loss: 1.2709 - val_acc: 0.4043\n",
            "Epoch 73/300\n",
            "\n",
            "Epoch 00073: val_loss improved from 1.27092 to 1.26440, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.9911 - acc: 0.6792 - val_loss: 1.2644 - val_acc: 0.4255\n",
            "Epoch 74/300\n",
            "\n",
            "Epoch 00074: val_loss improved from 1.26440 to 1.25246, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.9785 - acc: 0.6167 - val_loss: 1.2525 - val_acc: 0.4043\n",
            "Epoch 75/300\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 1.25246\n",
            "240/240 - 0s - loss: 1.0235 - acc: 0.6125 - val_loss: 1.2526 - val_acc: 0.4043\n",
            "Epoch 76/300\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 1.25246\n",
            "240/240 - 0s - loss: 1.0134 - acc: 0.6583 - val_loss: 1.2769 - val_acc: 0.4255\n",
            "Epoch 77/300\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 1.25246\n",
            "240/240 - 0s - loss: 0.9905 - acc: 0.6417 - val_loss: 1.2798 - val_acc: 0.3830\n",
            "Epoch 78/300\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 1.25246\n",
            "240/240 - 0s - loss: 1.0116 - acc: 0.6375 - val_loss: 1.2907 - val_acc: 0.3617\n",
            "Epoch 79/300\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 1.25246\n",
            "240/240 - 0s - loss: 0.9870 - acc: 0.6625 - val_loss: 1.2887 - val_acc: 0.3404\n",
            "Epoch 80/300\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 1.25246\n",
            "240/240 - 0s - loss: 0.9860 - acc: 0.6292 - val_loss: 1.2736 - val_acc: 0.4043\n",
            "Epoch 81/300\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 1.25246\n",
            "240/240 - 0s - loss: 0.9458 - acc: 0.7000 - val_loss: 1.2864 - val_acc: 0.3830\n",
            "Epoch 82/300\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 1.25246\n",
            "240/240 - 0s - loss: 0.9844 - acc: 0.6542 - val_loss: 1.2837 - val_acc: 0.3617\n",
            "Epoch 83/300\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 1.25246\n",
            "240/240 - 0s - loss: 0.9769 - acc: 0.6458 - val_loss: 1.2702 - val_acc: 0.3404\n",
            "Epoch 84/300\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 1.25246\n",
            "240/240 - 0s - loss: 0.9799 - acc: 0.6333 - val_loss: 1.2551 - val_acc: 0.4043\n",
            "Epoch 85/300\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 1.25246\n",
            "240/240 - 0s - loss: 1.0036 - acc: 0.6083 - val_loss: 1.2813 - val_acc: 0.3617\n",
            "Epoch 86/300\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 1.25246\n",
            "240/240 - 0s - loss: 0.9474 - acc: 0.6833 - val_loss: 1.2852 - val_acc: 0.3830\n",
            "Epoch 87/300\n",
            "\n",
            "Epoch 00087: val_loss improved from 1.25246 to 1.24911, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.9854 - acc: 0.6542 - val_loss: 1.2491 - val_acc: 0.4043\n",
            "Epoch 88/300\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 1.24911\n",
            "240/240 - 0s - loss: 0.9913 - acc: 0.6250 - val_loss: 1.2870 - val_acc: 0.4043\n",
            "Epoch 89/300\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 1.24911\n",
            "240/240 - 0s - loss: 0.9741 - acc: 0.6417 - val_loss: 1.2953 - val_acc: 0.3191\n",
            "Epoch 90/300\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 1.24911\n",
            "240/240 - 0s - loss: 0.9731 - acc: 0.6333 - val_loss: 1.2808 - val_acc: 0.4043\n",
            "Epoch 91/300\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 1.24911\n",
            "240/240 - 0s - loss: 0.9654 - acc: 0.6542 - val_loss: 1.2736 - val_acc: 0.4043\n",
            "Epoch 92/300\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 1.24911\n",
            "240/240 - 0s - loss: 0.9153 - acc: 0.7333 - val_loss: 1.3001 - val_acc: 0.3617\n",
            "Epoch 93/300\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 1.24911\n",
            "240/240 - 0s - loss: 0.9350 - acc: 0.6917 - val_loss: 1.2506 - val_acc: 0.4255\n",
            "Epoch 94/300\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 1.24911\n",
            "240/240 - 0s - loss: 0.9585 - acc: 0.6125 - val_loss: 1.2699 - val_acc: 0.3830\n",
            "Epoch 95/300\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 1.24911\n",
            "240/240 - 0s - loss: 0.9738 - acc: 0.6000 - val_loss: 1.2746 - val_acc: 0.4255\n",
            "Epoch 96/300\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 1.24911\n",
            "240/240 - 0s - loss: 0.9566 - acc: 0.6625 - val_loss: 1.2873 - val_acc: 0.4255\n",
            "Epoch 97/300\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 1.24911\n",
            "240/240 - 0s - loss: 0.9583 - acc: 0.6375 - val_loss: 1.2669 - val_acc: 0.4043\n",
            "Epoch 98/300\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 1.24911\n",
            "240/240 - 0s - loss: 0.9779 - acc: 0.6500 - val_loss: 1.2839 - val_acc: 0.4043\n",
            "Epoch 99/300\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 1.24911\n",
            "240/240 - 0s - loss: 0.9535 - acc: 0.6583 - val_loss: 1.2684 - val_acc: 0.4043\n",
            "Epoch 100/300\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 1.24911\n",
            "240/240 - 0s - loss: 0.9660 - acc: 0.6458 - val_loss: 1.2572 - val_acc: 0.4468\n",
            "Epoch 101/300\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 1.24911\n",
            "240/240 - 0s - loss: 0.8992 - acc: 0.6958 - val_loss: 1.3104 - val_acc: 0.4468\n",
            "Epoch 102/300\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 1.24911\n",
            "240/240 - 0s - loss: 0.9710 - acc: 0.6458 - val_loss: 1.2560 - val_acc: 0.4681\n",
            "Epoch 103/300\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 1.24911\n",
            "240/240 - 0s - loss: 0.9677 - acc: 0.6583 - val_loss: 1.2979 - val_acc: 0.4468\n",
            "Epoch 104/300\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 1.24911\n",
            "240/240 - 0s - loss: 0.9337 - acc: 0.6667 - val_loss: 1.2564 - val_acc: 0.4681\n",
            "Epoch 105/300\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 1.24911\n",
            "240/240 - 0s - loss: 0.9116 - acc: 0.7125 - val_loss: 1.2520 - val_acc: 0.4255\n",
            "Epoch 106/300\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 1.24911\n",
            "240/240 - 0s - loss: 0.9316 - acc: 0.6250 - val_loss: 1.2672 - val_acc: 0.4255\n",
            "Epoch 107/300\n",
            "\n",
            "Epoch 00107: val_loss improved from 1.24911 to 1.23597, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.9413 - acc: 0.6500 - val_loss: 1.2360 - val_acc: 0.4255\n",
            "Epoch 108/300\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 1.23597\n",
            "240/240 - 0s - loss: 0.9234 - acc: 0.6333 - val_loss: 1.2606 - val_acc: 0.4043\n",
            "Epoch 109/300\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 1.23597\n",
            "240/240 - 0s - loss: 0.9441 - acc: 0.6458 - val_loss: 1.2437 - val_acc: 0.3617\n",
            "Epoch 110/300\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 1.23597\n",
            "240/240 - 0s - loss: 0.9202 - acc: 0.6542 - val_loss: 1.2584 - val_acc: 0.3830\n",
            "Epoch 111/300\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 1.23597\n",
            "240/240 - 0s - loss: 0.9079 - acc: 0.6833 - val_loss: 1.2404 - val_acc: 0.4043\n",
            "Epoch 112/300\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 1.23597\n",
            "240/240 - 0s - loss: 0.9490 - acc: 0.6583 - val_loss: 1.2396 - val_acc: 0.4255\n",
            "Epoch 113/300\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 1.23597\n",
            "240/240 - 0s - loss: 0.9259 - acc: 0.6708 - val_loss: 1.2368 - val_acc: 0.4043\n",
            "Epoch 114/300\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 1.23597\n",
            "240/240 - 0s - loss: 0.9180 - acc: 0.6750 - val_loss: 1.2768 - val_acc: 0.4255\n",
            "Epoch 115/300\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 1.23597\n",
            "240/240 - 0s - loss: 0.8976 - acc: 0.6875 - val_loss: 1.2703 - val_acc: 0.4255\n",
            "Epoch 116/300\n",
            "\n",
            "Epoch 00116: val_loss improved from 1.23597 to 1.23450, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.9138 - acc: 0.6750 - val_loss: 1.2345 - val_acc: 0.4681\n",
            "Epoch 117/300\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 1.23450\n",
            "240/240 - 0s - loss: 0.9112 - acc: 0.6542 - val_loss: 1.2541 - val_acc: 0.4043\n",
            "Epoch 118/300\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 1.23450\n",
            "240/240 - 0s - loss: 0.9484 - acc: 0.6250 - val_loss: 1.2669 - val_acc: 0.4043\n",
            "Epoch 119/300\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 1.23450\n",
            "240/240 - 0s - loss: 0.9534 - acc: 0.6458 - val_loss: 1.2672 - val_acc: 0.4255\n",
            "Epoch 120/300\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 1.23450\n",
            "240/240 - 0s - loss: 0.9429 - acc: 0.6500 - val_loss: 1.2625 - val_acc: 0.4894\n",
            "Epoch 121/300\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 1.23450\n",
            "240/240 - 0s - loss: 0.8838 - acc: 0.6750 - val_loss: 1.2576 - val_acc: 0.4468\n",
            "Epoch 122/300\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 1.23450\n",
            "240/240 - 0s - loss: 0.9102 - acc: 0.6792 - val_loss: 1.2533 - val_acc: 0.3830\n",
            "Epoch 123/300\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 1.23450\n",
            "240/240 - 0s - loss: 0.9341 - acc: 0.6292 - val_loss: 1.2795 - val_acc: 0.3617\n",
            "Epoch 124/300\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 1.23450\n",
            "240/240 - 0s - loss: 0.9068 - acc: 0.6958 - val_loss: 1.2400 - val_acc: 0.4894\n",
            "Epoch 125/300\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 1.23450\n",
            "240/240 - 0s - loss: 0.8922 - acc: 0.6792 - val_loss: 1.2434 - val_acc: 0.4468\n",
            "Epoch 126/300\n",
            "\n",
            "Epoch 00126: val_loss improved from 1.23450 to 1.21902, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.8859 - acc: 0.6875 - val_loss: 1.2190 - val_acc: 0.4043\n",
            "Epoch 127/300\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 1.21902\n",
            "240/240 - 0s - loss: 0.8981 - acc: 0.7125 - val_loss: 1.2321 - val_acc: 0.4255\n",
            "Epoch 128/300\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 1.21902\n",
            "240/240 - 0s - loss: 0.9211 - acc: 0.6583 - val_loss: 1.2381 - val_acc: 0.4468\n",
            "Epoch 129/300\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 1.21902\n",
            "240/240 - 0s - loss: 0.8755 - acc: 0.7417 - val_loss: 1.2732 - val_acc: 0.4468\n",
            "Epoch 130/300\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 1.21902\n",
            "240/240 - 0s - loss: 0.9004 - acc: 0.6708 - val_loss: 1.2429 - val_acc: 0.5106\n",
            "Epoch 131/300\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 1.21902\n",
            "240/240 - 0s - loss: 0.9204 - acc: 0.6375 - val_loss: 1.2255 - val_acc: 0.4681\n",
            "Epoch 132/300\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 1.21902\n",
            "240/240 - 0s - loss: 0.9101 - acc: 0.7125 - val_loss: 1.2753 - val_acc: 0.4255\n",
            "Epoch 133/300\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 1.21902\n",
            "240/240 - 0s - loss: 0.8946 - acc: 0.6750 - val_loss: 1.2213 - val_acc: 0.5106\n",
            "Epoch 134/300\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 1.21902\n",
            "240/240 - 0s - loss: 0.8814 - acc: 0.6958 - val_loss: 1.2394 - val_acc: 0.4255\n",
            "Epoch 135/300\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 1.21902\n",
            "240/240 - 0s - loss: 0.9263 - acc: 0.6958 - val_loss: 1.2393 - val_acc: 0.4681\n",
            "Epoch 136/300\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 1.21902\n",
            "240/240 - 0s - loss: 0.8467 - acc: 0.7167 - val_loss: 1.2732 - val_acc: 0.3830\n",
            "Epoch 137/300\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 1.21902\n",
            "240/240 - 0s - loss: 0.8836 - acc: 0.7167 - val_loss: 1.2334 - val_acc: 0.4681\n",
            "Epoch 138/300\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 1.21902\n",
            "240/240 - 0s - loss: 0.8993 - acc: 0.6917 - val_loss: 1.2514 - val_acc: 0.4468\n",
            "Epoch 139/300\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 1.21902\n",
            "240/240 - 0s - loss: 0.8888 - acc: 0.7083 - val_loss: 1.2597 - val_acc: 0.4043\n",
            "Epoch 140/300\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 1.21902\n",
            "240/240 - 0s - loss: 0.8403 - acc: 0.7208 - val_loss: 1.2227 - val_acc: 0.4681\n",
            "Epoch 141/300\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 1.21902\n",
            "240/240 - 0s - loss: 0.8843 - acc: 0.6833 - val_loss: 1.2357 - val_acc: 0.4894\n",
            "Epoch 142/300\n",
            "\n",
            "Epoch 00142: val_loss improved from 1.21902 to 1.20869, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.8549 - acc: 0.7167 - val_loss: 1.2087 - val_acc: 0.5319\n",
            "Epoch 143/300\n",
            "\n",
            "Epoch 00143: val_loss improved from 1.20869 to 1.19510, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.8547 - acc: 0.6792 - val_loss: 1.1951 - val_acc: 0.5106\n",
            "Epoch 144/300\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.8573 - acc: 0.6958 - val_loss: 1.2217 - val_acc: 0.5106\n",
            "Epoch 145/300\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.8694 - acc: 0.6833 - val_loss: 1.2612 - val_acc: 0.4681\n",
            "Epoch 146/300\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.8174 - acc: 0.7417 - val_loss: 1.2477 - val_acc: 0.4681\n",
            "Epoch 147/300\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.8856 - acc: 0.6917 - val_loss: 1.2586 - val_acc: 0.3830\n",
            "Epoch 148/300\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.8772 - acc: 0.6708 - val_loss: 1.2472 - val_acc: 0.4468\n",
            "Epoch 149/300\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.8463 - acc: 0.7333 - val_loss: 1.2592 - val_acc: 0.4894\n",
            "Epoch 150/300\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.8753 - acc: 0.7000 - val_loss: 1.2579 - val_acc: 0.4681\n",
            "Epoch 151/300\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.8344 - acc: 0.7292 - val_loss: 1.2077 - val_acc: 0.4894\n",
            "Epoch 152/300\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.8747 - acc: 0.6708 - val_loss: 1.2503 - val_acc: 0.4681\n",
            "Epoch 153/300\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.8539 - acc: 0.7083 - val_loss: 1.2397 - val_acc: 0.4468\n",
            "Epoch 154/300\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.8802 - acc: 0.6792 - val_loss: 1.2408 - val_acc: 0.4894\n",
            "Epoch 155/300\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.8595 - acc: 0.6958 - val_loss: 1.2428 - val_acc: 0.4255\n",
            "Epoch 156/300\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.8556 - acc: 0.6917 - val_loss: 1.2417 - val_acc: 0.4255\n",
            "Epoch 157/300\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.8773 - acc: 0.6875 - val_loss: 1.2098 - val_acc: 0.4043\n",
            "Epoch 158/300\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.8104 - acc: 0.7458 - val_loss: 1.2859 - val_acc: 0.3830\n",
            "Epoch 159/300\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.8314 - acc: 0.7167 - val_loss: 1.2475 - val_acc: 0.4043\n",
            "Epoch 160/300\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.8837 - acc: 0.6958 - val_loss: 1.2633 - val_acc: 0.4255\n",
            "Epoch 161/300\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.8263 - acc: 0.7333 - val_loss: 1.2573 - val_acc: 0.3617\n",
            "Epoch 162/300\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.8877 - acc: 0.6708 - val_loss: 1.2722 - val_acc: 0.4681\n",
            "Epoch 163/300\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.8312 - acc: 0.7125 - val_loss: 1.2631 - val_acc: 0.4468\n",
            "Epoch 164/300\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.8233 - acc: 0.7292 - val_loss: 1.2216 - val_acc: 0.4255\n",
            "Epoch 165/300\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.8487 - acc: 0.7125 - val_loss: 1.2575 - val_acc: 0.4468\n",
            "Epoch 166/300\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.8121 - acc: 0.7333 - val_loss: 1.2349 - val_acc: 0.4255\n",
            "Epoch 167/300\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.8398 - acc: 0.6667 - val_loss: 1.2319 - val_acc: 0.4255\n",
            "Epoch 168/300\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.8479 - acc: 0.7417 - val_loss: 1.2810 - val_acc: 0.4043\n",
            "Epoch 169/300\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.8649 - acc: 0.6875 - val_loss: 1.2302 - val_acc: 0.4255\n",
            "Epoch 170/300\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.8696 - acc: 0.6708 - val_loss: 1.3247 - val_acc: 0.4043\n",
            "Epoch 171/300\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.8415 - acc: 0.6667 - val_loss: 1.2540 - val_acc: 0.4894\n",
            "Epoch 172/300\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.8031 - acc: 0.7083 - val_loss: 1.2278 - val_acc: 0.4894\n",
            "Epoch 173/300\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.8108 - acc: 0.7375 - val_loss: 1.2150 - val_acc: 0.5106\n",
            "Epoch 174/300\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.7910 - acc: 0.7375 - val_loss: 1.2836 - val_acc: 0.4681\n",
            "Epoch 175/300\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.8546 - acc: 0.7042 - val_loss: 1.2481 - val_acc: 0.4255\n",
            "Epoch 176/300\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.8222 - acc: 0.7333 - val_loss: 1.2648 - val_acc: 0.4894\n",
            "Epoch 177/300\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.8662 - acc: 0.6667 - val_loss: 1.2286 - val_acc: 0.4468\n",
            "Epoch 178/300\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.7842 - acc: 0.7208 - val_loss: 1.2290 - val_acc: 0.4681\n",
            "Epoch 179/300\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.8123 - acc: 0.7167 - val_loss: 1.2481 - val_acc: 0.4681\n",
            "Epoch 180/300\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.8151 - acc: 0.7167 - val_loss: 1.2547 - val_acc: 0.4681\n",
            "Epoch 181/300\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.8406 - acc: 0.7292 - val_loss: 1.3482 - val_acc: 0.4255\n",
            "Epoch 182/300\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.8230 - acc: 0.7208 - val_loss: 1.2580 - val_acc: 0.4681\n",
            "Epoch 183/300\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.8495 - acc: 0.6875 - val_loss: 1.2710 - val_acc: 0.4468\n",
            "Epoch 184/300\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.8081 - acc: 0.7333 - val_loss: 1.2664 - val_acc: 0.4255\n",
            "Epoch 185/300\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.8263 - acc: 0.7250 - val_loss: 1.2898 - val_acc: 0.3830\n",
            "Epoch 186/300\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.8429 - acc: 0.6958 - val_loss: 1.2333 - val_acc: 0.4468\n",
            "Epoch 187/300\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.8183 - acc: 0.7292 - val_loss: 1.2663 - val_acc: 0.4043\n",
            "Epoch 188/300\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.8149 - acc: 0.7250 - val_loss: 1.2635 - val_acc: 0.4043\n",
            "Epoch 189/300\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.8113 - acc: 0.7000 - val_loss: 1.2374 - val_acc: 0.4043\n",
            "Epoch 190/300\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.7872 - acc: 0.7708 - val_loss: 1.2353 - val_acc: 0.4255\n",
            "Epoch 191/300\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.7966 - acc: 0.7375 - val_loss: 1.2141 - val_acc: 0.4043\n",
            "Epoch 192/300\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.8156 - acc: 0.7333 - val_loss: 1.2249 - val_acc: 0.4043\n",
            "Epoch 193/300\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.7936 - acc: 0.7333 - val_loss: 1.1973 - val_acc: 0.4681\n",
            "Epoch 194/300\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.7991 - acc: 0.7167 - val_loss: 1.2478 - val_acc: 0.4043\n",
            "Epoch 195/300\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.8267 - acc: 0.6958 - val_loss: 1.2665 - val_acc: 0.3191\n",
            "Epoch 196/300\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.7720 - acc: 0.7375 - val_loss: 1.2757 - val_acc: 0.4255\n",
            "Epoch 197/300\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.8152 - acc: 0.7375 - val_loss: 1.2299 - val_acc: 0.4255\n",
            "Epoch 198/300\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.8171 - acc: 0.6958 - val_loss: 1.2617 - val_acc: 0.4043\n",
            "Epoch 199/300\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.8072 - acc: 0.7375 - val_loss: 1.2426 - val_acc: 0.4468\n",
            "Epoch 200/300\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.7751 - acc: 0.7458 - val_loss: 1.3071 - val_acc: 0.4255\n",
            "Epoch 201/300\n",
            "\n",
            "Epoch 00201: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.8125 - acc: 0.7042 - val_loss: 1.2340 - val_acc: 0.4468\n",
            "Epoch 202/300\n",
            "\n",
            "Epoch 00202: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.7925 - acc: 0.7208 - val_loss: 1.2560 - val_acc: 0.4043\n",
            "Epoch 203/300\n",
            "\n",
            "Epoch 00203: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.7625 - acc: 0.7750 - val_loss: 1.3152 - val_acc: 0.4043\n",
            "Epoch 204/300\n",
            "\n",
            "Epoch 00204: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.7966 - acc: 0.7375 - val_loss: 1.2378 - val_acc: 0.4468\n",
            "Epoch 205/300\n",
            "\n",
            "Epoch 00205: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.7800 - acc: 0.7250 - val_loss: 1.2335 - val_acc: 0.4043\n",
            "Epoch 206/300\n",
            "\n",
            "Epoch 00206: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.7644 - acc: 0.7542 - val_loss: 1.2354 - val_acc: 0.3617\n",
            "Epoch 207/300\n",
            "\n",
            "Epoch 00207: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.7317 - acc: 0.7792 - val_loss: 1.2472 - val_acc: 0.4255\n",
            "Epoch 208/300\n",
            "\n",
            "Epoch 00208: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.7555 - acc: 0.7625 - val_loss: 1.2471 - val_acc: 0.4043\n",
            "Epoch 209/300\n",
            "\n",
            "Epoch 00209: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.8094 - acc: 0.7042 - val_loss: 1.3174 - val_acc: 0.3830\n",
            "Epoch 210/300\n",
            "\n",
            "Epoch 00210: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.8321 - acc: 0.6750 - val_loss: 1.2660 - val_acc: 0.4043\n",
            "Epoch 211/300\n",
            "\n",
            "Epoch 00211: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.8022 - acc: 0.7083 - val_loss: 1.2828 - val_acc: 0.4043\n",
            "Epoch 212/300\n",
            "\n",
            "Epoch 00212: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.7873 - acc: 0.7292 - val_loss: 1.2457 - val_acc: 0.3830\n",
            "Epoch 213/300\n",
            "\n",
            "Epoch 00213: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.7849 - acc: 0.7458 - val_loss: 1.2697 - val_acc: 0.3830\n",
            "Epoch 214/300\n",
            "\n",
            "Epoch 00214: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.7681 - acc: 0.7583 - val_loss: 1.3095 - val_acc: 0.4043\n",
            "Epoch 215/300\n",
            "\n",
            "Epoch 00215: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.7692 - acc: 0.7292 - val_loss: 1.2433 - val_acc: 0.4255\n",
            "Epoch 216/300\n",
            "\n",
            "Epoch 00216: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.7516 - acc: 0.7667 - val_loss: 1.2509 - val_acc: 0.3830\n",
            "Epoch 217/300\n",
            "\n",
            "Epoch 00217: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.7497 - acc: 0.7625 - val_loss: 1.2433 - val_acc: 0.3830\n",
            "Epoch 218/300\n",
            "\n",
            "Epoch 00218: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.7446 - acc: 0.7583 - val_loss: 1.2245 - val_acc: 0.4255\n",
            "Epoch 219/300\n",
            "\n",
            "Epoch 00219: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.8219 - acc: 0.7250 - val_loss: 1.2338 - val_acc: 0.4255\n",
            "Epoch 220/300\n",
            "\n",
            "Epoch 00220: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.7775 - acc: 0.7333 - val_loss: 1.2721 - val_acc: 0.3617\n",
            "Epoch 221/300\n",
            "\n",
            "Epoch 00221: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.7527 - acc: 0.7292 - val_loss: 1.2005 - val_acc: 0.4468\n",
            "Epoch 222/300\n",
            "\n",
            "Epoch 00222: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.7497 - acc: 0.7708 - val_loss: 1.1978 - val_acc: 0.4468\n",
            "Epoch 223/300\n",
            "\n",
            "Epoch 00223: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.7787 - acc: 0.7583 - val_loss: 1.2081 - val_acc: 0.4468\n",
            "Epoch 224/300\n",
            "\n",
            "Epoch 00224: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.7481 - acc: 0.7583 - val_loss: 1.3025 - val_acc: 0.4468\n",
            "Epoch 225/300\n",
            "\n",
            "Epoch 00225: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.7705 - acc: 0.7292 - val_loss: 1.2571 - val_acc: 0.3830\n",
            "Epoch 226/300\n",
            "\n",
            "Epoch 00226: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.8065 - acc: 0.6750 - val_loss: 1.2543 - val_acc: 0.4043\n",
            "Epoch 227/300\n",
            "\n",
            "Epoch 00227: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.7553 - acc: 0.7458 - val_loss: 1.2375 - val_acc: 0.4894\n",
            "Epoch 228/300\n",
            "\n",
            "Epoch 00228: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.7392 - acc: 0.7167 - val_loss: 1.2250 - val_acc: 0.4043\n",
            "Epoch 229/300\n",
            "\n",
            "Epoch 00229: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.8086 - acc: 0.6625 - val_loss: 1.2476 - val_acc: 0.3404\n",
            "Epoch 230/300\n",
            "\n",
            "Epoch 00230: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.7636 - acc: 0.7542 - val_loss: 1.2446 - val_acc: 0.4255\n",
            "Epoch 231/300\n",
            "\n",
            "Epoch 00231: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.7213 - acc: 0.7500 - val_loss: 1.2211 - val_acc: 0.4681\n",
            "Epoch 232/300\n",
            "\n",
            "Epoch 00232: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.7658 - acc: 0.7417 - val_loss: 1.2526 - val_acc: 0.4255\n",
            "Epoch 233/300\n",
            "\n",
            "Epoch 00233: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.7255 - acc: 0.7500 - val_loss: 1.2590 - val_acc: 0.4255\n",
            "Epoch 234/300\n",
            "\n",
            "Epoch 00234: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.7826 - acc: 0.7042 - val_loss: 1.2204 - val_acc: 0.4681\n",
            "Epoch 235/300\n",
            "\n",
            "Epoch 00235: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.7746 - acc: 0.7125 - val_loss: 1.2384 - val_acc: 0.4255\n",
            "Epoch 236/300\n",
            "\n",
            "Epoch 00236: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.7526 - acc: 0.7250 - val_loss: 1.2364 - val_acc: 0.4043\n",
            "Epoch 237/300\n",
            "\n",
            "Epoch 00237: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.7383 - acc: 0.7375 - val_loss: 1.2700 - val_acc: 0.4043\n",
            "Epoch 238/300\n",
            "\n",
            "Epoch 00238: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.7670 - acc: 0.7208 - val_loss: 1.2502 - val_acc: 0.3617\n",
            "Epoch 239/300\n",
            "\n",
            "Epoch 00239: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.7213 - acc: 0.7625 - val_loss: 1.3044 - val_acc: 0.3830\n",
            "Epoch 240/300\n",
            "\n",
            "Epoch 00240: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.7595 - acc: 0.7458 - val_loss: 1.2775 - val_acc: 0.4043\n",
            "Epoch 241/300\n",
            "\n",
            "Epoch 00241: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.7716 - acc: 0.7417 - val_loss: 1.2911 - val_acc: 0.3617\n",
            "Epoch 242/300\n",
            "\n",
            "Epoch 00242: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.7437 - acc: 0.7500 - val_loss: 1.2854 - val_acc: 0.4255\n",
            "Epoch 243/300\n",
            "\n",
            "Epoch 00243: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.7366 - acc: 0.7458 - val_loss: 1.2937 - val_acc: 0.3830\n",
            "Epoch 244/300\n",
            "\n",
            "Epoch 00244: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.7135 - acc: 0.7667 - val_loss: 1.2997 - val_acc: 0.3830\n",
            "Epoch 245/300\n",
            "\n",
            "Epoch 00245: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.7004 - acc: 0.8167 - val_loss: 1.2656 - val_acc: 0.4043\n",
            "Epoch 246/300\n",
            "\n",
            "Epoch 00246: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.7290 - acc: 0.7417 - val_loss: 1.2907 - val_acc: 0.4255\n",
            "Epoch 247/300\n",
            "\n",
            "Epoch 00247: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.7862 - acc: 0.7500 - val_loss: 1.3086 - val_acc: 0.4043\n",
            "Epoch 248/300\n",
            "\n",
            "Epoch 00248: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.7424 - acc: 0.7458 - val_loss: 1.2628 - val_acc: 0.3830\n",
            "Epoch 249/300\n",
            "\n",
            "Epoch 00249: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.7700 - acc: 0.7292 - val_loss: 1.3070 - val_acc: 0.3617\n",
            "Epoch 250/300\n",
            "\n",
            "Epoch 00250: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.7200 - acc: 0.7625 - val_loss: 1.2876 - val_acc: 0.3830\n",
            "Epoch 251/300\n",
            "\n",
            "Epoch 00251: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.8138 - acc: 0.7000 - val_loss: 1.3172 - val_acc: 0.4468\n",
            "Epoch 252/300\n",
            "\n",
            "Epoch 00252: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.7982 - acc: 0.7042 - val_loss: 1.3383 - val_acc: 0.3617\n",
            "Epoch 253/300\n",
            "\n",
            "Epoch 00253: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.7445 - acc: 0.7583 - val_loss: 1.3640 - val_acc: 0.4255\n",
            "Epoch 254/300\n",
            "\n",
            "Epoch 00254: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.7244 - acc: 0.7417 - val_loss: 1.3011 - val_acc: 0.3830\n",
            "Epoch 255/300\n",
            "\n",
            "Epoch 00255: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.7314 - acc: 0.7333 - val_loss: 1.2543 - val_acc: 0.4468\n",
            "Epoch 256/300\n",
            "\n",
            "Epoch 00256: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.7838 - acc: 0.7125 - val_loss: 1.2940 - val_acc: 0.4043\n",
            "Epoch 257/300\n",
            "\n",
            "Epoch 00257: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.7495 - acc: 0.7292 - val_loss: 1.3066 - val_acc: 0.3830\n",
            "Epoch 258/300\n",
            "\n",
            "Epoch 00258: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.7360 - acc: 0.7500 - val_loss: 1.2971 - val_acc: 0.3830\n",
            "Epoch 259/300\n",
            "\n",
            "Epoch 00259: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.7293 - acc: 0.7667 - val_loss: 1.2704 - val_acc: 0.4894\n",
            "Epoch 260/300\n",
            "\n",
            "Epoch 00260: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.7608 - acc: 0.7250 - val_loss: 1.3181 - val_acc: 0.4255\n",
            "Epoch 261/300\n",
            "\n",
            "Epoch 00261: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.7049 - acc: 0.7542 - val_loss: 1.2582 - val_acc: 0.4468\n",
            "Epoch 262/300\n",
            "\n",
            "Epoch 00262: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.7732 - acc: 0.7417 - val_loss: 1.2458 - val_acc: 0.3830\n",
            "Epoch 263/300\n",
            "\n",
            "Epoch 00263: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.7102 - acc: 0.7542 - val_loss: 1.2884 - val_acc: 0.3404\n",
            "Epoch 264/300\n",
            "\n",
            "Epoch 00264: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.7485 - acc: 0.7542 - val_loss: 1.2607 - val_acc: 0.3830\n",
            "Epoch 265/300\n",
            "\n",
            "Epoch 00265: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.7127 - acc: 0.7750 - val_loss: 1.2572 - val_acc: 0.4255\n",
            "Epoch 266/300\n",
            "\n",
            "Epoch 00266: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.7222 - acc: 0.7583 - val_loss: 1.2060 - val_acc: 0.4255\n",
            "Epoch 267/300\n",
            "\n",
            "Epoch 00267: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.7594 - acc: 0.7208 - val_loss: 1.2764 - val_acc: 0.4468\n",
            "Epoch 268/300\n",
            "\n",
            "Epoch 00268: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.7284 - acc: 0.7792 - val_loss: 1.2185 - val_acc: 0.4681\n",
            "Epoch 269/300\n",
            "\n",
            "Epoch 00269: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.6868 - acc: 0.7917 - val_loss: 1.2517 - val_acc: 0.4043\n",
            "Epoch 270/300\n",
            "\n",
            "Epoch 00270: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.6693 - acc: 0.8125 - val_loss: 1.2094 - val_acc: 0.4255\n",
            "Epoch 271/300\n",
            "\n",
            "Epoch 00271: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.7291 - acc: 0.7875 - val_loss: 1.2839 - val_acc: 0.3404\n",
            "Epoch 272/300\n",
            "\n",
            "Epoch 00272: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.6606 - acc: 0.8042 - val_loss: 1.2339 - val_acc: 0.4681\n",
            "Epoch 273/300\n",
            "\n",
            "Epoch 00273: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.7074 - acc: 0.7583 - val_loss: 1.2532 - val_acc: 0.4043\n",
            "Epoch 274/300\n",
            "\n",
            "Epoch 00274: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.7114 - acc: 0.7417 - val_loss: 1.3047 - val_acc: 0.4255\n",
            "Epoch 275/300\n",
            "\n",
            "Epoch 00275: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.7041 - acc: 0.7833 - val_loss: 1.2820 - val_acc: 0.4255\n",
            "Epoch 276/300\n",
            "\n",
            "Epoch 00276: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.6874 - acc: 0.7750 - val_loss: 1.2622 - val_acc: 0.4043\n",
            "Epoch 277/300\n",
            "\n",
            "Epoch 00277: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.7237 - acc: 0.7542 - val_loss: 1.2380 - val_acc: 0.3830\n",
            "Epoch 278/300\n",
            "\n",
            "Epoch 00278: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.7165 - acc: 0.7333 - val_loss: 1.3512 - val_acc: 0.4043\n",
            "Epoch 279/300\n",
            "\n",
            "Epoch 00279: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.6435 - acc: 0.8000 - val_loss: 1.2507 - val_acc: 0.5106\n",
            "Epoch 280/300\n",
            "\n",
            "Epoch 00280: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.7354 - acc: 0.7125 - val_loss: 1.2726 - val_acc: 0.4255\n",
            "Epoch 281/300\n",
            "\n",
            "Epoch 00281: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.6976 - acc: 0.7542 - val_loss: 1.2695 - val_acc: 0.3191\n",
            "Epoch 282/300\n",
            "\n",
            "Epoch 00282: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.6668 - acc: 0.7792 - val_loss: 1.2260 - val_acc: 0.5106\n",
            "Epoch 283/300\n",
            "\n",
            "Epoch 00283: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.7465 - acc: 0.7333 - val_loss: 1.1988 - val_acc: 0.4681\n",
            "Epoch 284/300\n",
            "\n",
            "Epoch 00284: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.7238 - acc: 0.7458 - val_loss: 1.2637 - val_acc: 0.4043\n",
            "Epoch 285/300\n",
            "\n",
            "Epoch 00285: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.7378 - acc: 0.7458 - val_loss: 1.2650 - val_acc: 0.4681\n",
            "Epoch 286/300\n",
            "\n",
            "Epoch 00286: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.6717 - acc: 0.7458 - val_loss: 1.2538 - val_acc: 0.4255\n",
            "Epoch 287/300\n",
            "\n",
            "Epoch 00287: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.6786 - acc: 0.7958 - val_loss: 1.2721 - val_acc: 0.4894\n",
            "Epoch 288/300\n",
            "\n",
            "Epoch 00288: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.7061 - acc: 0.7542 - val_loss: 1.3010 - val_acc: 0.4468\n",
            "Epoch 289/300\n",
            "\n",
            "Epoch 00289: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.7127 - acc: 0.7458 - val_loss: 1.3015 - val_acc: 0.4043\n",
            "Epoch 290/300\n",
            "\n",
            "Epoch 00290: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.7091 - acc: 0.7917 - val_loss: 1.2609 - val_acc: 0.4255\n",
            "Epoch 291/300\n",
            "\n",
            "Epoch 00291: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.7041 - acc: 0.7583 - val_loss: 1.2741 - val_acc: 0.3830\n",
            "Epoch 292/300\n",
            "\n",
            "Epoch 00292: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.6963 - acc: 0.7542 - val_loss: 1.2583 - val_acc: 0.4681\n",
            "Epoch 293/300\n",
            "\n",
            "Epoch 00293: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.7125 - acc: 0.7625 - val_loss: 1.2914 - val_acc: 0.4043\n",
            "Epoch 294/300\n",
            "\n",
            "Epoch 00294: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.6637 - acc: 0.7833 - val_loss: 1.2507 - val_acc: 0.4468\n",
            "Epoch 295/300\n",
            "\n",
            "Epoch 00295: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.7233 - acc: 0.7167 - val_loss: 1.3065 - val_acc: 0.4043\n",
            "Epoch 296/300\n",
            "\n",
            "Epoch 00296: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.7070 - acc: 0.7458 - val_loss: 1.3654 - val_acc: 0.4043\n",
            "Epoch 297/300\n",
            "\n",
            "Epoch 00297: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.6954 - acc: 0.7917 - val_loss: 1.2495 - val_acc: 0.4681\n",
            "Epoch 298/300\n",
            "\n",
            "Epoch 00298: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.7190 - acc: 0.7625 - val_loss: 1.3269 - val_acc: 0.4255\n",
            "Epoch 299/300\n",
            "\n",
            "Epoch 00299: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.7150 - acc: 0.7292 - val_loss: 1.2530 - val_acc: 0.4043\n",
            "Epoch 300/300\n",
            "\n",
            "Epoch 00300: val_loss did not improve from 1.19510\n",
            "240/240 - 0s - loss: 0.6666 - acc: 0.7667 - val_loss: 1.2755 - val_acc: 0.4043\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "# Evaluate on test data\n",
            "288/288 [==============================] - 1s 2ms/sample - loss: 1.2605 - acc: 0.4653\n",
            "test loss, test acc: [1.2605026272746425, 0.4652778]\n",
            "EEG_Deep/Data2A/Data_A06T.mat\n",
            "(288, 22, 1875)\n",
            "(288, 1)\n",
            "(240, 22, 750)\n",
            "(240, 1)\n",
            "(47, 22, 750)\n",
            "(47, 1)\n",
            "X_train shape: (240, 1, 22, 750)\n",
            "240 train samples\n",
            "47 val samples\n",
            "EEG_Deep/Data2A/Data_A06E.mat\n",
            "(288, 22, 1875)\n",
            "(288, 1)\n",
            "(288, 22, 750)\n",
            "(288, 1)\n",
            "X_train shape: (288, 1, 22, 750)\n",
            "288 train samples\n",
            "Train on 240 samples, validate on 47 samples\n",
            "Epoch 1/300\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 1.38808, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 1s - loss: 1.4133 - acc: 0.2000 - val_loss: 1.3881 - val_acc: 0.1915\n",
            "Epoch 2/300\n",
            "\n",
            "Epoch 00002: val_loss improved from 1.38808 to 1.38640, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.3834 - acc: 0.2542 - val_loss: 1.3864 - val_acc: 0.2340\n",
            "Epoch 3/300\n",
            "\n",
            "Epoch 00003: val_loss improved from 1.38640 to 1.38526, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.3712 - acc: 0.3250 - val_loss: 1.3853 - val_acc: 0.2553\n",
            "Epoch 4/300\n",
            "\n",
            "Epoch 00004: val_loss improved from 1.38526 to 1.38501, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.3430 - acc: 0.3958 - val_loss: 1.3850 - val_acc: 0.2979\n",
            "Epoch 5/300\n",
            "\n",
            "Epoch 00005: val_loss improved from 1.38501 to 1.38434, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.3462 - acc: 0.3833 - val_loss: 1.3843 - val_acc: 0.2979\n",
            "Epoch 6/300\n",
            "\n",
            "Epoch 00006: val_loss improved from 1.38434 to 1.38251, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.3322 - acc: 0.4292 - val_loss: 1.3825 - val_acc: 0.4255\n",
            "Epoch 7/300\n",
            "\n",
            "Epoch 00007: val_loss improved from 1.38251 to 1.38097, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.3124 - acc: 0.4667 - val_loss: 1.3810 - val_acc: 0.4255\n",
            "Epoch 8/300\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 1.38097\n",
            "240/240 - 0s - loss: 1.3009 - acc: 0.4958 - val_loss: 1.3811 - val_acc: 0.4468\n",
            "Epoch 9/300\n",
            "\n",
            "Epoch 00009: val_loss improved from 1.38097 to 1.37880, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.2921 - acc: 0.4875 - val_loss: 1.3788 - val_acc: 0.4468\n",
            "Epoch 10/300\n",
            "\n",
            "Epoch 00010: val_loss improved from 1.37880 to 1.37703, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.2800 - acc: 0.5000 - val_loss: 1.3770 - val_acc: 0.4468\n",
            "Epoch 11/300\n",
            "\n",
            "Epoch 00011: val_loss improved from 1.37703 to 1.37678, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.2593 - acc: 0.5083 - val_loss: 1.3768 - val_acc: 0.4468\n",
            "Epoch 12/300\n",
            "\n",
            "Epoch 00012: val_loss improved from 1.37678 to 1.37410, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.2603 - acc: 0.4917 - val_loss: 1.3741 - val_acc: 0.4681\n",
            "Epoch 13/300\n",
            "\n",
            "Epoch 00013: val_loss improved from 1.37410 to 1.37129, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.2492 - acc: 0.4917 - val_loss: 1.3713 - val_acc: 0.4894\n",
            "Epoch 14/300\n",
            "\n",
            "Epoch 00014: val_loss improved from 1.37129 to 1.36637, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.2295 - acc: 0.5083 - val_loss: 1.3664 - val_acc: 0.4894\n",
            "Epoch 15/300\n",
            "\n",
            "Epoch 00015: val_loss improved from 1.36637 to 1.36611, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.2307 - acc: 0.4958 - val_loss: 1.3661 - val_acc: 0.4681\n",
            "Epoch 16/300\n",
            "\n",
            "Epoch 00016: val_loss improved from 1.36611 to 1.36170, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.2298 - acc: 0.4917 - val_loss: 1.3617 - val_acc: 0.4468\n",
            "Epoch 17/300\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 1.36170\n",
            "240/240 - 0s - loss: 1.2370 - acc: 0.4792 - val_loss: 1.3621 - val_acc: 0.4468\n",
            "Epoch 18/300\n",
            "\n",
            "Epoch 00018: val_loss improved from 1.36170 to 1.35757, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.2298 - acc: 0.4875 - val_loss: 1.3576 - val_acc: 0.4468\n",
            "Epoch 19/300\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 1.35757\n",
            "240/240 - 0s - loss: 1.2073 - acc: 0.5042 - val_loss: 1.3612 - val_acc: 0.4255\n",
            "Epoch 20/300\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 1.35757\n",
            "240/240 - 0s - loss: 1.1928 - acc: 0.5500 - val_loss: 1.3594 - val_acc: 0.4681\n",
            "Epoch 21/300\n",
            "\n",
            "Epoch 00021: val_loss improved from 1.35757 to 1.34680, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.2022 - acc: 0.5167 - val_loss: 1.3468 - val_acc: 0.5319\n",
            "Epoch 22/300\n",
            "\n",
            "Epoch 00022: val_loss improved from 1.34680 to 1.34483, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.1934 - acc: 0.5292 - val_loss: 1.3448 - val_acc: 0.4894\n",
            "Epoch 23/300\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 1.34483\n",
            "240/240 - 0s - loss: 1.1571 - acc: 0.5458 - val_loss: 1.3469 - val_acc: 0.5106\n",
            "Epoch 24/300\n",
            "\n",
            "Epoch 00024: val_loss improved from 1.34483 to 1.33877, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.1679 - acc: 0.5417 - val_loss: 1.3388 - val_acc: 0.5106\n",
            "Epoch 25/300\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 1.33877\n",
            "240/240 - 0s - loss: 1.1685 - acc: 0.5333 - val_loss: 1.3673 - val_acc: 0.4681\n",
            "Epoch 26/300\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 1.33877\n",
            "240/240 - 0s - loss: 1.1689 - acc: 0.5250 - val_loss: 1.3435 - val_acc: 0.4894\n",
            "Epoch 27/300\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 1.33877\n",
            "240/240 - 0s - loss: 1.1506 - acc: 0.5417 - val_loss: 1.3450 - val_acc: 0.4681\n",
            "Epoch 28/300\n",
            "\n",
            "Epoch 00028: val_loss improved from 1.33877 to 1.32356, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.1484 - acc: 0.5333 - val_loss: 1.3236 - val_acc: 0.4468\n",
            "Epoch 29/300\n",
            "\n",
            "Epoch 00029: val_loss improved from 1.32356 to 1.31453, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.1296 - acc: 0.5708 - val_loss: 1.3145 - val_acc: 0.4681\n",
            "Epoch 30/300\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 1.31453\n",
            "240/240 - 0s - loss: 1.1214 - acc: 0.5917 - val_loss: 1.3186 - val_acc: 0.4681\n",
            "Epoch 31/300\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 1.31453\n",
            "240/240 - 0s - loss: 1.1321 - acc: 0.5375 - val_loss: 1.3321 - val_acc: 0.3830\n",
            "Epoch 32/300\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 1.31453\n",
            "240/240 - 0s - loss: 1.0959 - acc: 0.6125 - val_loss: 1.3512 - val_acc: 0.4255\n",
            "Epoch 33/300\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 1.31453\n",
            "240/240 - 0s - loss: 1.1183 - acc: 0.5875 - val_loss: 1.3516 - val_acc: 0.4255\n",
            "Epoch 34/300\n",
            "\n",
            "Epoch 00034: val_loss improved from 1.31453 to 1.30924, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.1155 - acc: 0.6042 - val_loss: 1.3092 - val_acc: 0.4468\n",
            "Epoch 35/300\n",
            "\n",
            "Epoch 00035: val_loss improved from 1.30924 to 1.30397, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.0877 - acc: 0.5958 - val_loss: 1.3040 - val_acc: 0.4681\n",
            "Epoch 36/300\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 1.30397\n",
            "240/240 - 0s - loss: 1.1108 - acc: 0.5625 - val_loss: 1.3217 - val_acc: 0.4255\n",
            "Epoch 37/300\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 1.30397\n",
            "240/240 - 0s - loss: 1.0979 - acc: 0.5708 - val_loss: 1.3200 - val_acc: 0.4681\n",
            "Epoch 38/300\n",
            "\n",
            "Epoch 00038: val_loss improved from 1.30397 to 1.28897, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.0948 - acc: 0.6000 - val_loss: 1.2890 - val_acc: 0.4681\n",
            "Epoch 39/300\n",
            "\n",
            "Epoch 00039: val_loss improved from 1.28897 to 1.28646, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.0950 - acc: 0.5875 - val_loss: 1.2865 - val_acc: 0.4468\n",
            "Epoch 40/300\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 1.28646\n",
            "240/240 - 0s - loss: 1.0911 - acc: 0.5708 - val_loss: 1.3015 - val_acc: 0.4255\n",
            "Epoch 41/300\n",
            "\n",
            "Epoch 00041: val_loss improved from 1.28646 to 1.28424, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.0664 - acc: 0.5958 - val_loss: 1.2842 - val_acc: 0.4894\n",
            "Epoch 42/300\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 1.28424\n",
            "240/240 - 0s - loss: 1.0755 - acc: 0.6292 - val_loss: 1.3043 - val_acc: 0.4255\n",
            "Epoch 43/300\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 1.28424\n",
            "240/240 - 0s - loss: 1.0458 - acc: 0.6417 - val_loss: 1.2980 - val_acc: 0.4468\n",
            "Epoch 44/300\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 1.28424\n",
            "240/240 - 0s - loss: 1.0523 - acc: 0.6333 - val_loss: 1.3021 - val_acc: 0.4255\n",
            "Epoch 45/300\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 1.28424\n",
            "240/240 - 0s - loss: 1.0662 - acc: 0.6292 - val_loss: 1.3200 - val_acc: 0.4255\n",
            "Epoch 46/300\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 1.28424\n",
            "240/240 - 0s - loss: 1.0698 - acc: 0.5667 - val_loss: 1.2849 - val_acc: 0.4043\n",
            "Epoch 47/300\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 1.28424\n",
            "240/240 - 0s - loss: 1.0406 - acc: 0.6333 - val_loss: 1.2925 - val_acc: 0.4681\n",
            "Epoch 48/300\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 1.28424\n",
            "240/240 - 0s - loss: 1.0658 - acc: 0.6000 - val_loss: 1.2881 - val_acc: 0.4255\n",
            "Epoch 49/300\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 1.28424\n",
            "240/240 - 0s - loss: 1.0421 - acc: 0.6333 - val_loss: 1.2852 - val_acc: 0.4468\n",
            "Epoch 50/300\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 1.28424\n",
            "240/240 - 0s - loss: 1.0427 - acc: 0.6250 - val_loss: 1.2904 - val_acc: 0.4043\n",
            "Epoch 51/300\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 1.28424\n",
            "240/240 - 0s - loss: 1.0457 - acc: 0.6000 - val_loss: 1.3022 - val_acc: 0.4468\n",
            "Epoch 52/300\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 1.28424\n",
            "240/240 - 0s - loss: 1.0376 - acc: 0.6292 - val_loss: 1.2942 - val_acc: 0.4681\n",
            "Epoch 53/300\n",
            "\n",
            "Epoch 00053: val_loss improved from 1.28424 to 1.28230, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.0058 - acc: 0.6292 - val_loss: 1.2823 - val_acc: 0.4255\n",
            "Epoch 54/300\n",
            "\n",
            "Epoch 00054: val_loss improved from 1.28230 to 1.26740, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.0177 - acc: 0.6250 - val_loss: 1.2674 - val_acc: 0.4255\n",
            "Epoch 55/300\n",
            "\n",
            "Epoch 00055: val_loss improved from 1.26740 to 1.26392, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.0201 - acc: 0.5917 - val_loss: 1.2639 - val_acc: 0.4468\n",
            "Epoch 56/300\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 1.26392\n",
            "240/240 - 0s - loss: 0.9900 - acc: 0.6750 - val_loss: 1.3092 - val_acc: 0.4468\n",
            "Epoch 57/300\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 1.26392\n",
            "240/240 - 0s - loss: 1.0129 - acc: 0.6500 - val_loss: 1.3175 - val_acc: 0.3617\n",
            "Epoch 58/300\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 1.26392\n",
            "240/240 - 0s - loss: 1.0055 - acc: 0.6292 - val_loss: 1.2864 - val_acc: 0.4468\n",
            "Epoch 59/300\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 1.26392\n",
            "240/240 - 0s - loss: 1.0416 - acc: 0.5792 - val_loss: 1.2907 - val_acc: 0.4681\n",
            "Epoch 60/300\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 1.26392\n",
            "240/240 - 0s - loss: 1.0225 - acc: 0.6292 - val_loss: 1.3493 - val_acc: 0.4043\n",
            "Epoch 61/300\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 1.26392\n",
            "240/240 - 0s - loss: 0.9906 - acc: 0.6583 - val_loss: 1.3354 - val_acc: 0.4255\n",
            "Epoch 62/300\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 1.26392\n",
            "240/240 - 0s - loss: 0.9934 - acc: 0.6875 - val_loss: 1.3009 - val_acc: 0.4468\n",
            "Epoch 63/300\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 1.26392\n",
            "240/240 - 0s - loss: 0.9971 - acc: 0.6542 - val_loss: 1.2904 - val_acc: 0.4468\n",
            "Epoch 64/300\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 1.26392\n",
            "240/240 - 0s - loss: 0.9643 - acc: 0.6917 - val_loss: 1.2690 - val_acc: 0.4681\n",
            "Epoch 65/300\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 1.26392\n",
            "240/240 - 0s - loss: 1.0101 - acc: 0.6125 - val_loss: 1.2770 - val_acc: 0.4681\n",
            "Epoch 66/300\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 1.26392\n",
            "240/240 - 0s - loss: 0.9746 - acc: 0.6833 - val_loss: 1.2781 - val_acc: 0.4255\n",
            "Epoch 67/300\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 1.26392\n",
            "240/240 - 0s - loss: 0.9997 - acc: 0.6292 - val_loss: 1.2715 - val_acc: 0.4043\n",
            "Epoch 68/300\n",
            "\n",
            "Epoch 00068: val_loss improved from 1.26392 to 1.25542, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.9795 - acc: 0.6500 - val_loss: 1.2554 - val_acc: 0.4894\n",
            "Epoch 69/300\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 1.25542\n",
            "240/240 - 0s - loss: 0.9632 - acc: 0.6458 - val_loss: 1.2749 - val_acc: 0.4894\n",
            "Epoch 70/300\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 1.25542\n",
            "240/240 - 0s - loss: 0.9921 - acc: 0.6417 - val_loss: 1.2906 - val_acc: 0.5319\n",
            "Epoch 71/300\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 1.25542\n",
            "240/240 - 0s - loss: 0.9800 - acc: 0.6375 - val_loss: 1.2996 - val_acc: 0.4681\n",
            "Epoch 72/300\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 1.25542\n",
            "240/240 - 0s - loss: 0.9739 - acc: 0.6583 - val_loss: 1.2908 - val_acc: 0.4255\n",
            "Epoch 73/300\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 1.25542\n",
            "240/240 - 0s - loss: 0.9841 - acc: 0.6167 - val_loss: 1.2623 - val_acc: 0.4894\n",
            "Epoch 74/300\n",
            "\n",
            "Epoch 00074: val_loss improved from 1.25542 to 1.25388, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.9853 - acc: 0.6542 - val_loss: 1.2539 - val_acc: 0.4681\n",
            "Epoch 75/300\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 1.25388\n",
            "240/240 - 0s - loss: 0.9827 - acc: 0.6500 - val_loss: 1.2587 - val_acc: 0.4468\n",
            "Epoch 76/300\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 1.25388\n",
            "240/240 - 0s - loss: 0.9754 - acc: 0.6375 - val_loss: 1.2561 - val_acc: 0.4468\n",
            "Epoch 77/300\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 1.25388\n",
            "240/240 - 0s - loss: 0.9902 - acc: 0.6583 - val_loss: 1.3041 - val_acc: 0.4681\n",
            "Epoch 78/300\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 1.25388\n",
            "240/240 - 0s - loss: 0.9613 - acc: 0.6500 - val_loss: 1.3112 - val_acc: 0.4681\n",
            "Epoch 79/300\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 1.25388\n",
            "240/240 - 0s - loss: 0.9680 - acc: 0.6458 - val_loss: 1.3011 - val_acc: 0.4681\n",
            "Epoch 80/300\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 1.25388\n",
            "240/240 - 0s - loss: 1.0257 - acc: 0.5917 - val_loss: 1.2894 - val_acc: 0.4468\n",
            "Epoch 81/300\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 1.25388\n",
            "240/240 - 0s - loss: 0.9842 - acc: 0.6542 - val_loss: 1.2779 - val_acc: 0.4894\n",
            "Epoch 82/300\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 1.25388\n",
            "240/240 - 0s - loss: 0.9928 - acc: 0.6375 - val_loss: 1.2587 - val_acc: 0.5532\n",
            "Epoch 83/300\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 1.25388\n",
            "240/240 - 0s - loss: 0.9308 - acc: 0.7000 - val_loss: 1.2928 - val_acc: 0.4468\n",
            "Epoch 84/300\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 1.25388\n",
            "240/240 - 0s - loss: 0.9286 - acc: 0.6875 - val_loss: 1.2841 - val_acc: 0.4681\n",
            "Epoch 85/300\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 1.25388\n",
            "240/240 - 0s - loss: 0.9478 - acc: 0.6875 - val_loss: 1.2618 - val_acc: 0.4468\n",
            "Epoch 86/300\n",
            "\n",
            "Epoch 00086: val_loss improved from 1.25388 to 1.24990, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.9181 - acc: 0.6958 - val_loss: 1.2499 - val_acc: 0.4681\n",
            "Epoch 87/300\n",
            "\n",
            "Epoch 00087: val_loss improved from 1.24990 to 1.23876, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.9560 - acc: 0.6750 - val_loss: 1.2388 - val_acc: 0.4681\n",
            "Epoch 88/300\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 1.23876\n",
            "240/240 - 0s - loss: 0.9655 - acc: 0.6750 - val_loss: 1.2613 - val_acc: 0.4894\n",
            "Epoch 89/300\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 1.23876\n",
            "240/240 - 0s - loss: 0.9230 - acc: 0.7083 - val_loss: 1.2660 - val_acc: 0.4894\n",
            "Epoch 90/300\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 1.23876\n",
            "240/240 - 0s - loss: 0.9700 - acc: 0.6292 - val_loss: 1.2692 - val_acc: 0.4468\n",
            "Epoch 91/300\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 1.23876\n",
            "240/240 - 0s - loss: 0.9217 - acc: 0.7208 - val_loss: 1.2391 - val_acc: 0.4894\n",
            "Epoch 92/300\n",
            "\n",
            "Epoch 00092: val_loss improved from 1.23876 to 1.21729, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.9188 - acc: 0.7000 - val_loss: 1.2173 - val_acc: 0.4894\n",
            "Epoch 93/300\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 1.21729\n",
            "240/240 - 0s - loss: 0.9331 - acc: 0.6583 - val_loss: 1.2248 - val_acc: 0.4894\n",
            "Epoch 94/300\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 1.21729\n",
            "240/240 - 0s - loss: 0.9407 - acc: 0.6458 - val_loss: 1.2333 - val_acc: 0.4255\n",
            "Epoch 95/300\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 1.21729\n",
            "240/240 - 0s - loss: 0.9670 - acc: 0.6417 - val_loss: 1.2310 - val_acc: 0.4894\n",
            "Epoch 96/300\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 1.21729\n",
            "240/240 - 0s - loss: 0.9511 - acc: 0.6833 - val_loss: 1.2633 - val_acc: 0.4043\n",
            "Epoch 97/300\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 1.21729\n",
            "240/240 - 0s - loss: 0.9153 - acc: 0.6958 - val_loss: 1.2626 - val_acc: 0.5106\n",
            "Epoch 98/300\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 1.21729\n",
            "240/240 - 0s - loss: 0.9277 - acc: 0.6667 - val_loss: 1.2605 - val_acc: 0.4894\n",
            "Epoch 99/300\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 1.21729\n",
            "240/240 - 0s - loss: 0.9321 - acc: 0.6708 - val_loss: 1.2539 - val_acc: 0.5319\n",
            "Epoch 100/300\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 1.21729\n",
            "240/240 - 0s - loss: 0.9179 - acc: 0.6875 - val_loss: 1.2531 - val_acc: 0.4255\n",
            "Epoch 101/300\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 1.21729\n",
            "240/240 - 0s - loss: 0.8620 - acc: 0.7500 - val_loss: 1.2279 - val_acc: 0.5957\n",
            "Epoch 102/300\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 1.21729\n",
            "240/240 - 0s - loss: 0.8817 - acc: 0.7208 - val_loss: 1.2889 - val_acc: 0.4255\n",
            "Epoch 103/300\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 1.21729\n",
            "240/240 - 0s - loss: 0.8795 - acc: 0.6833 - val_loss: 1.2770 - val_acc: 0.4468\n",
            "Epoch 104/300\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 1.21729\n",
            "240/240 - 0s - loss: 0.8866 - acc: 0.7000 - val_loss: 1.2788 - val_acc: 0.4894\n",
            "Epoch 105/300\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 1.21729\n",
            "240/240 - 0s - loss: 0.8779 - acc: 0.7042 - val_loss: 1.2721 - val_acc: 0.4255\n",
            "Epoch 106/300\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 1.21729\n",
            "240/240 - 0s - loss: 0.9042 - acc: 0.6458 - val_loss: 1.2672 - val_acc: 0.4681\n",
            "Epoch 107/300\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 1.21729\n",
            "240/240 - 0s - loss: 0.8854 - acc: 0.7083 - val_loss: 1.2938 - val_acc: 0.3830\n",
            "Epoch 108/300\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 1.21729\n",
            "240/240 - 0s - loss: 0.8936 - acc: 0.6750 - val_loss: 1.2532 - val_acc: 0.4894\n",
            "Epoch 109/300\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 1.21729\n",
            "240/240 - 0s - loss: 0.9129 - acc: 0.6667 - val_loss: 1.2499 - val_acc: 0.4894\n",
            "Epoch 110/300\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 1.21729\n",
            "240/240 - 0s - loss: 0.9006 - acc: 0.6750 - val_loss: 1.2681 - val_acc: 0.4468\n",
            "Epoch 111/300\n",
            "\n",
            "Epoch 00111: val_loss improved from 1.21729 to 1.21456, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.9162 - acc: 0.6708 - val_loss: 1.2146 - val_acc: 0.4681\n",
            "Epoch 112/300\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 1.21456\n",
            "240/240 - 0s - loss: 0.8907 - acc: 0.7042 - val_loss: 1.2411 - val_acc: 0.4255\n",
            "Epoch 113/300\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 1.21456\n",
            "240/240 - 0s - loss: 0.8696 - acc: 0.7583 - val_loss: 1.2412 - val_acc: 0.5319\n",
            "Epoch 114/300\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 1.21456\n",
            "240/240 - 0s - loss: 0.9041 - acc: 0.6708 - val_loss: 1.2391 - val_acc: 0.4894\n",
            "Epoch 115/300\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 1.21456\n",
            "240/240 - 0s - loss: 0.8792 - acc: 0.7250 - val_loss: 1.2194 - val_acc: 0.4894\n",
            "Epoch 116/300\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 1.21456\n",
            "240/240 - 0s - loss: 0.8874 - acc: 0.6875 - val_loss: 1.2533 - val_acc: 0.5106\n",
            "Epoch 117/300\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 1.21456\n",
            "240/240 - 0s - loss: 0.8874 - acc: 0.6667 - val_loss: 1.2377 - val_acc: 0.5532\n",
            "Epoch 118/300\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 1.21456\n",
            "240/240 - 0s - loss: 0.8822 - acc: 0.6917 - val_loss: 1.2174 - val_acc: 0.5319\n",
            "Epoch 119/300\n",
            "\n",
            "Epoch 00119: val_loss improved from 1.21456 to 1.21288, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.8882 - acc: 0.6750 - val_loss: 1.2129 - val_acc: 0.5532\n",
            "Epoch 120/300\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 1.21288\n",
            "240/240 - 0s - loss: 0.9121 - acc: 0.6708 - val_loss: 1.2151 - val_acc: 0.5532\n",
            "Epoch 121/300\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 1.21288\n",
            "240/240 - 0s - loss: 0.8668 - acc: 0.7375 - val_loss: 1.2785 - val_acc: 0.4681\n",
            "Epoch 122/300\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 1.21288\n",
            "240/240 - 0s - loss: 0.8779 - acc: 0.6875 - val_loss: 1.2854 - val_acc: 0.4681\n",
            "Epoch 123/300\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 1.21288\n",
            "240/240 - 0s - loss: 0.8874 - acc: 0.6917 - val_loss: 1.2958 - val_acc: 0.4043\n",
            "Epoch 124/300\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 1.21288\n",
            "240/240 - 0s - loss: 0.8695 - acc: 0.7000 - val_loss: 1.2621 - val_acc: 0.4255\n",
            "Epoch 125/300\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 1.21288\n",
            "240/240 - 0s - loss: 0.8880 - acc: 0.6917 - val_loss: 1.2388 - val_acc: 0.5106\n",
            "Epoch 126/300\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 1.21288\n",
            "240/240 - 0s - loss: 0.8641 - acc: 0.7125 - val_loss: 1.2591 - val_acc: 0.5319\n",
            "Epoch 127/300\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 1.21288\n",
            "240/240 - 0s - loss: 0.8312 - acc: 0.7292 - val_loss: 1.2830 - val_acc: 0.5106\n",
            "Epoch 128/300\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 1.21288\n",
            "240/240 - 0s - loss: 0.8461 - acc: 0.7125 - val_loss: 1.3068 - val_acc: 0.4468\n",
            "Epoch 129/300\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 1.21288\n",
            "240/240 - 0s - loss: 0.8321 - acc: 0.7000 - val_loss: 1.3065 - val_acc: 0.4043\n",
            "Epoch 130/300\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 1.21288\n",
            "240/240 - 0s - loss: 0.8350 - acc: 0.7000 - val_loss: 1.2408 - val_acc: 0.5319\n",
            "Epoch 131/300\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 1.21288\n",
            "240/240 - 0s - loss: 0.8902 - acc: 0.6500 - val_loss: 1.2183 - val_acc: 0.5106\n",
            "Epoch 132/300\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 1.21288\n",
            "240/240 - 0s - loss: 0.8558 - acc: 0.7125 - val_loss: 1.2532 - val_acc: 0.4681\n",
            "Epoch 133/300\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 1.21288\n",
            "240/240 - 0s - loss: 0.8575 - acc: 0.7167 - val_loss: 1.2305 - val_acc: 0.5319\n",
            "Epoch 134/300\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 1.21288\n",
            "240/240 - 0s - loss: 0.8641 - acc: 0.7208 - val_loss: 1.2345 - val_acc: 0.5319\n",
            "Epoch 135/300\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 1.21288\n",
            "240/240 - 0s - loss: 0.8500 - acc: 0.7042 - val_loss: 1.2331 - val_acc: 0.5532\n",
            "Epoch 136/300\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 1.21288\n",
            "240/240 - 0s - loss: 0.8608 - acc: 0.7375 - val_loss: 1.2830 - val_acc: 0.5319\n",
            "Epoch 137/300\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 1.21288\n",
            "240/240 - 0s - loss: 0.8195 - acc: 0.7333 - val_loss: 1.2426 - val_acc: 0.4894\n",
            "Epoch 138/300\n",
            "\n",
            "Epoch 00138: val_loss improved from 1.21288 to 1.20678, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.8186 - acc: 0.7083 - val_loss: 1.2068 - val_acc: 0.4681\n",
            "Epoch 139/300\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.8191 - acc: 0.7333 - val_loss: 1.2135 - val_acc: 0.5319\n",
            "Epoch 140/300\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.8522 - acc: 0.6708 - val_loss: 1.2728 - val_acc: 0.4468\n",
            "Epoch 141/300\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.8445 - acc: 0.7083 - val_loss: 1.2635 - val_acc: 0.4894\n",
            "Epoch 142/300\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.8190 - acc: 0.7542 - val_loss: 1.2670 - val_acc: 0.5106\n",
            "Epoch 143/300\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.8139 - acc: 0.7292 - val_loss: 1.2472 - val_acc: 0.5532\n",
            "Epoch 144/300\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.8488 - acc: 0.6875 - val_loss: 1.2771 - val_acc: 0.4681\n",
            "Epoch 145/300\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.8434 - acc: 0.6917 - val_loss: 1.2784 - val_acc: 0.4255\n",
            "Epoch 146/300\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.8405 - acc: 0.7000 - val_loss: 1.2793 - val_acc: 0.4468\n",
            "Epoch 147/300\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.8294 - acc: 0.6792 - val_loss: 1.2940 - val_acc: 0.4043\n",
            "Epoch 148/300\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.8161 - acc: 0.7208 - val_loss: 1.3439 - val_acc: 0.4468\n",
            "Epoch 149/300\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.8218 - acc: 0.7208 - val_loss: 1.2726 - val_acc: 0.4255\n",
            "Epoch 150/300\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.8129 - acc: 0.7625 - val_loss: 1.2561 - val_acc: 0.4894\n",
            "Epoch 151/300\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.8102 - acc: 0.7250 - val_loss: 1.2742 - val_acc: 0.4468\n",
            "Epoch 152/300\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.8003 - acc: 0.7250 - val_loss: 1.2482 - val_acc: 0.5532\n",
            "Epoch 153/300\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.8432 - acc: 0.7042 - val_loss: 1.2664 - val_acc: 0.4468\n",
            "Epoch 154/300\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.8464 - acc: 0.7417 - val_loss: 1.2484 - val_acc: 0.4894\n",
            "Epoch 155/300\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.8211 - acc: 0.7167 - val_loss: 1.2707 - val_acc: 0.4894\n",
            "Epoch 156/300\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.7877 - acc: 0.7167 - val_loss: 1.3543 - val_acc: 0.4255\n",
            "Epoch 157/300\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.8146 - acc: 0.7042 - val_loss: 1.3050 - val_acc: 0.4681\n",
            "Epoch 158/300\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.8199 - acc: 0.6917 - val_loss: 1.2815 - val_acc: 0.5319\n",
            "Epoch 159/300\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.8462 - acc: 0.6917 - val_loss: 1.2639 - val_acc: 0.4894\n",
            "Epoch 160/300\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.7948 - acc: 0.7375 - val_loss: 1.2431 - val_acc: 0.5106\n",
            "Epoch 161/300\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.8119 - acc: 0.6917 - val_loss: 1.2942 - val_acc: 0.4681\n",
            "Epoch 162/300\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.8070 - acc: 0.7333 - val_loss: 1.2801 - val_acc: 0.4681\n",
            "Epoch 163/300\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.8260 - acc: 0.6917 - val_loss: 1.2961 - val_acc: 0.4894\n",
            "Epoch 164/300\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.8379 - acc: 0.7083 - val_loss: 1.2922 - val_acc: 0.5319\n",
            "Epoch 165/300\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.8145 - acc: 0.7708 - val_loss: 1.2961 - val_acc: 0.4681\n",
            "Epoch 166/300\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.7788 - acc: 0.7458 - val_loss: 1.3156 - val_acc: 0.4468\n",
            "Epoch 167/300\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.8333 - acc: 0.6750 - val_loss: 1.3118 - val_acc: 0.4468\n",
            "Epoch 168/300\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.8360 - acc: 0.6958 - val_loss: 1.2897 - val_acc: 0.4681\n",
            "Epoch 169/300\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.8262 - acc: 0.7208 - val_loss: 1.2926 - val_acc: 0.4255\n",
            "Epoch 170/300\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.8158 - acc: 0.7167 - val_loss: 1.3209 - val_acc: 0.4681\n",
            "Epoch 171/300\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.8129 - acc: 0.7167 - val_loss: 1.3066 - val_acc: 0.4468\n",
            "Epoch 172/300\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.7846 - acc: 0.7542 - val_loss: 1.2880 - val_acc: 0.4894\n",
            "Epoch 173/300\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.8420 - acc: 0.7125 - val_loss: 1.2524 - val_acc: 0.4468\n",
            "Epoch 174/300\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.8292 - acc: 0.7083 - val_loss: 1.2475 - val_acc: 0.5106\n",
            "Epoch 175/300\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.8050 - acc: 0.7083 - val_loss: 1.2770 - val_acc: 0.4681\n",
            "Epoch 176/300\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.8151 - acc: 0.6833 - val_loss: 1.2544 - val_acc: 0.5745\n",
            "Epoch 177/300\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.7419 - acc: 0.7792 - val_loss: 1.2959 - val_acc: 0.4681\n",
            "Epoch 178/300\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.7788 - acc: 0.7583 - val_loss: 1.2683 - val_acc: 0.4894\n",
            "Epoch 179/300\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.7619 - acc: 0.7917 - val_loss: 1.2805 - val_acc: 0.4681\n",
            "Epoch 180/300\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.8490 - acc: 0.7167 - val_loss: 1.2485 - val_acc: 0.5532\n",
            "Epoch 181/300\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.7931 - acc: 0.7292 - val_loss: 1.2467 - val_acc: 0.5106\n",
            "Epoch 182/300\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.7571 - acc: 0.7667 - val_loss: 1.3203 - val_acc: 0.4468\n",
            "Epoch 183/300\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.7658 - acc: 0.7417 - val_loss: 1.2696 - val_acc: 0.4681\n",
            "Epoch 184/300\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.7858 - acc: 0.7083 - val_loss: 1.2823 - val_acc: 0.4468\n",
            "Epoch 185/300\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.7684 - acc: 0.7583 - val_loss: 1.3241 - val_acc: 0.4681\n",
            "Epoch 186/300\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.8181 - acc: 0.7167 - val_loss: 1.2488 - val_acc: 0.4894\n",
            "Epoch 187/300\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.8201 - acc: 0.7000 - val_loss: 1.2127 - val_acc: 0.4894\n",
            "Epoch 188/300\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.8002 - acc: 0.6958 - val_loss: 1.2697 - val_acc: 0.4681\n",
            "Epoch 189/300\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.8012 - acc: 0.7083 - val_loss: 1.2978 - val_acc: 0.4681\n",
            "Epoch 190/300\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.7728 - acc: 0.7458 - val_loss: 1.2538 - val_acc: 0.5532\n",
            "Epoch 191/300\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.7995 - acc: 0.7208 - val_loss: 1.2656 - val_acc: 0.4468\n",
            "Epoch 192/300\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.7102 - acc: 0.7792 - val_loss: 1.2583 - val_acc: 0.5106\n",
            "Epoch 193/300\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.7737 - acc: 0.7542 - val_loss: 1.2949 - val_acc: 0.4468\n",
            "Epoch 194/300\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.8012 - acc: 0.7125 - val_loss: 1.3362 - val_acc: 0.4681\n",
            "Epoch 195/300\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.7371 - acc: 0.7792 - val_loss: 1.3239 - val_acc: 0.4468\n",
            "Epoch 196/300\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.7899 - acc: 0.7208 - val_loss: 1.3213 - val_acc: 0.4468\n",
            "Epoch 197/300\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.7582 - acc: 0.7417 - val_loss: 1.2807 - val_acc: 0.4681\n",
            "Epoch 198/300\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.7501 - acc: 0.7500 - val_loss: 1.2882 - val_acc: 0.4468\n",
            "Epoch 199/300\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.7171 - acc: 0.7833 - val_loss: 1.2771 - val_acc: 0.4043\n",
            "Epoch 200/300\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.7536 - acc: 0.7417 - val_loss: 1.2809 - val_acc: 0.4894\n",
            "Epoch 201/300\n",
            "\n",
            "Epoch 00201: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.7626 - acc: 0.7667 - val_loss: 1.2728 - val_acc: 0.4255\n",
            "Epoch 202/300\n",
            "\n",
            "Epoch 00202: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.7672 - acc: 0.7333 - val_loss: 1.2680 - val_acc: 0.3830\n",
            "Epoch 203/300\n",
            "\n",
            "Epoch 00203: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.7285 - acc: 0.7708 - val_loss: 1.2925 - val_acc: 0.4468\n",
            "Epoch 204/300\n",
            "\n",
            "Epoch 00204: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.7628 - acc: 0.7125 - val_loss: 1.2528 - val_acc: 0.4681\n",
            "Epoch 205/300\n",
            "\n",
            "Epoch 00205: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.7782 - acc: 0.7125 - val_loss: 1.2674 - val_acc: 0.4468\n",
            "Epoch 206/300\n",
            "\n",
            "Epoch 00206: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.7331 - acc: 0.7750 - val_loss: 1.2628 - val_acc: 0.4681\n",
            "Epoch 207/300\n",
            "\n",
            "Epoch 00207: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.7524 - acc: 0.7333 - val_loss: 1.2347 - val_acc: 0.5532\n",
            "Epoch 208/300\n",
            "\n",
            "Epoch 00208: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.7677 - acc: 0.7292 - val_loss: 1.2186 - val_acc: 0.5532\n",
            "Epoch 209/300\n",
            "\n",
            "Epoch 00209: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.7531 - acc: 0.7417 - val_loss: 1.2753 - val_acc: 0.3830\n",
            "Epoch 210/300\n",
            "\n",
            "Epoch 00210: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.7620 - acc: 0.7542 - val_loss: 1.2166 - val_acc: 0.4894\n",
            "Epoch 211/300\n",
            "\n",
            "Epoch 00211: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.7433 - acc: 0.7458 - val_loss: 1.2160 - val_acc: 0.5106\n",
            "Epoch 212/300\n",
            "\n",
            "Epoch 00212: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.7262 - acc: 0.7583 - val_loss: 1.2430 - val_acc: 0.4255\n",
            "Epoch 213/300\n",
            "\n",
            "Epoch 00213: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.7443 - acc: 0.7458 - val_loss: 1.2449 - val_acc: 0.5106\n",
            "Epoch 214/300\n",
            "\n",
            "Epoch 00214: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.7967 - acc: 0.7125 - val_loss: 1.2635 - val_acc: 0.4894\n",
            "Epoch 215/300\n",
            "\n",
            "Epoch 00215: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.7407 - acc: 0.7458 - val_loss: 1.2746 - val_acc: 0.4681\n",
            "Epoch 216/300\n",
            "\n",
            "Epoch 00216: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.7513 - acc: 0.7500 - val_loss: 1.2402 - val_acc: 0.5106\n",
            "Epoch 217/300\n",
            "\n",
            "Epoch 00217: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.7408 - acc: 0.7583 - val_loss: 1.2613 - val_acc: 0.4255\n",
            "Epoch 218/300\n",
            "\n",
            "Epoch 00218: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.7163 - acc: 0.7708 - val_loss: 1.2628 - val_acc: 0.5106\n",
            "Epoch 219/300\n",
            "\n",
            "Epoch 00219: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.8056 - acc: 0.6792 - val_loss: 1.2581 - val_acc: 0.4681\n",
            "Epoch 220/300\n",
            "\n",
            "Epoch 00220: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.7577 - acc: 0.7625 - val_loss: 1.3022 - val_acc: 0.4681\n",
            "Epoch 221/300\n",
            "\n",
            "Epoch 00221: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.7700 - acc: 0.7333 - val_loss: 1.3012 - val_acc: 0.5319\n",
            "Epoch 222/300\n",
            "\n",
            "Epoch 00222: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.7605 - acc: 0.7625 - val_loss: 1.3237 - val_acc: 0.4681\n",
            "Epoch 223/300\n",
            "\n",
            "Epoch 00223: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.7593 - acc: 0.7292 - val_loss: 1.3013 - val_acc: 0.5106\n",
            "Epoch 224/300\n",
            "\n",
            "Epoch 00224: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.7562 - acc: 0.7250 - val_loss: 1.2951 - val_acc: 0.4894\n",
            "Epoch 225/300\n",
            "\n",
            "Epoch 00225: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.7168 - acc: 0.7667 - val_loss: 1.3378 - val_acc: 0.4681\n",
            "Epoch 226/300\n",
            "\n",
            "Epoch 00226: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.7500 - acc: 0.7333 - val_loss: 1.3503 - val_acc: 0.4894\n",
            "Epoch 227/300\n",
            "\n",
            "Epoch 00227: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.7478 - acc: 0.7333 - val_loss: 1.3033 - val_acc: 0.5106\n",
            "Epoch 228/300\n",
            "\n",
            "Epoch 00228: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.7210 - acc: 0.7708 - val_loss: 1.3024 - val_acc: 0.4894\n",
            "Epoch 229/300\n",
            "\n",
            "Epoch 00229: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.7309 - acc: 0.7417 - val_loss: 1.3513 - val_acc: 0.5106\n",
            "Epoch 230/300\n",
            "\n",
            "Epoch 00230: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.7426 - acc: 0.7542 - val_loss: 1.2999 - val_acc: 0.4894\n",
            "Epoch 231/300\n",
            "\n",
            "Epoch 00231: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.6858 - acc: 0.7750 - val_loss: 1.3036 - val_acc: 0.5106\n",
            "Epoch 232/300\n",
            "\n",
            "Epoch 00232: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.7226 - acc: 0.7542 - val_loss: 1.3146 - val_acc: 0.5106\n",
            "Epoch 233/300\n",
            "\n",
            "Epoch 00233: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.7110 - acc: 0.7625 - val_loss: 1.2726 - val_acc: 0.5319\n",
            "Epoch 234/300\n",
            "\n",
            "Epoch 00234: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.7533 - acc: 0.7625 - val_loss: 1.3112 - val_acc: 0.4468\n",
            "Epoch 235/300\n",
            "\n",
            "Epoch 00235: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.7072 - acc: 0.7792 - val_loss: 1.3256 - val_acc: 0.4681\n",
            "Epoch 236/300\n",
            "\n",
            "Epoch 00236: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.7432 - acc: 0.7208 - val_loss: 1.2750 - val_acc: 0.4681\n",
            "Epoch 237/300\n",
            "\n",
            "Epoch 00237: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.7464 - acc: 0.7542 - val_loss: 1.2851 - val_acc: 0.4468\n",
            "Epoch 238/300\n",
            "\n",
            "Epoch 00238: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.7637 - acc: 0.7167 - val_loss: 1.2732 - val_acc: 0.5532\n",
            "Epoch 239/300\n",
            "\n",
            "Epoch 00239: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.7817 - acc: 0.7375 - val_loss: 1.2933 - val_acc: 0.4894\n",
            "Epoch 240/300\n",
            "\n",
            "Epoch 00240: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.7234 - acc: 0.7792 - val_loss: 1.3181 - val_acc: 0.4681\n",
            "Epoch 241/300\n",
            "\n",
            "Epoch 00241: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.7493 - acc: 0.7375 - val_loss: 1.2985 - val_acc: 0.4681\n",
            "Epoch 242/300\n",
            "\n",
            "Epoch 00242: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.7479 - acc: 0.7583 - val_loss: 1.2731 - val_acc: 0.4468\n",
            "Epoch 243/300\n",
            "\n",
            "Epoch 00243: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.7073 - acc: 0.7792 - val_loss: 1.3047 - val_acc: 0.4681\n",
            "Epoch 244/300\n",
            "\n",
            "Epoch 00244: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.7499 - acc: 0.7542 - val_loss: 1.2926 - val_acc: 0.4681\n",
            "Epoch 245/300\n",
            "\n",
            "Epoch 00245: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.7612 - acc: 0.7500 - val_loss: 1.3536 - val_acc: 0.4468\n",
            "Epoch 246/300\n",
            "\n",
            "Epoch 00246: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.7313 - acc: 0.7542 - val_loss: 1.2823 - val_acc: 0.4894\n",
            "Epoch 247/300\n",
            "\n",
            "Epoch 00247: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.7482 - acc: 0.7208 - val_loss: 1.3248 - val_acc: 0.4255\n",
            "Epoch 248/300\n",
            "\n",
            "Epoch 00248: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.7346 - acc: 0.7667 - val_loss: 1.3269 - val_acc: 0.4468\n",
            "Epoch 249/300\n",
            "\n",
            "Epoch 00249: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.7544 - acc: 0.7250 - val_loss: 1.3208 - val_acc: 0.4681\n",
            "Epoch 250/300\n",
            "\n",
            "Epoch 00250: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.7247 - acc: 0.7500 - val_loss: 1.2991 - val_acc: 0.4681\n",
            "Epoch 251/300\n",
            "\n",
            "Epoch 00251: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.7336 - acc: 0.7583 - val_loss: 1.2905 - val_acc: 0.4894\n",
            "Epoch 252/300\n",
            "\n",
            "Epoch 00252: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.7374 - acc: 0.7833 - val_loss: 1.3220 - val_acc: 0.4681\n",
            "Epoch 253/300\n",
            "\n",
            "Epoch 00253: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.7278 - acc: 0.7375 - val_loss: 1.3434 - val_acc: 0.5106\n",
            "Epoch 254/300\n",
            "\n",
            "Epoch 00254: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.7210 - acc: 0.7708 - val_loss: 1.3136 - val_acc: 0.4681\n",
            "Epoch 255/300\n",
            "\n",
            "Epoch 00255: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.7317 - acc: 0.7167 - val_loss: 1.3311 - val_acc: 0.5106\n",
            "Epoch 256/300\n",
            "\n",
            "Epoch 00256: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.7044 - acc: 0.7667 - val_loss: 1.2762 - val_acc: 0.5106\n",
            "Epoch 257/300\n",
            "\n",
            "Epoch 00257: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.6968 - acc: 0.7458 - val_loss: 1.2649 - val_acc: 0.4894\n",
            "Epoch 258/300\n",
            "\n",
            "Epoch 00258: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.7192 - acc: 0.7542 - val_loss: 1.2432 - val_acc: 0.5106\n",
            "Epoch 259/300\n",
            "\n",
            "Epoch 00259: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.7009 - acc: 0.7417 - val_loss: 1.2809 - val_acc: 0.4255\n",
            "Epoch 260/300\n",
            "\n",
            "Epoch 00260: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.6643 - acc: 0.8083 - val_loss: 1.2707 - val_acc: 0.5106\n",
            "Epoch 261/300\n",
            "\n",
            "Epoch 00261: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.7189 - acc: 0.7750 - val_loss: 1.3047 - val_acc: 0.4681\n",
            "Epoch 262/300\n",
            "\n",
            "Epoch 00262: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.7427 - acc: 0.7542 - val_loss: 1.3168 - val_acc: 0.4468\n",
            "Epoch 263/300\n",
            "\n",
            "Epoch 00263: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.6868 - acc: 0.7458 - val_loss: 1.3193 - val_acc: 0.3830\n",
            "Epoch 264/300\n",
            "\n",
            "Epoch 00264: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.7286 - acc: 0.7333 - val_loss: 1.3613 - val_acc: 0.4681\n",
            "Epoch 265/300\n",
            "\n",
            "Epoch 00265: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.7520 - acc: 0.7250 - val_loss: 1.3059 - val_acc: 0.4894\n",
            "Epoch 266/300\n",
            "\n",
            "Epoch 00266: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.7275 - acc: 0.7083 - val_loss: 1.3411 - val_acc: 0.4468\n",
            "Epoch 267/300\n",
            "\n",
            "Epoch 00267: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.6953 - acc: 0.7833 - val_loss: 1.3223 - val_acc: 0.4468\n",
            "Epoch 268/300\n",
            "\n",
            "Epoch 00268: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.7385 - acc: 0.7375 - val_loss: 1.2563 - val_acc: 0.4681\n",
            "Epoch 269/300\n",
            "\n",
            "Epoch 00269: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.7191 - acc: 0.7125 - val_loss: 1.3036 - val_acc: 0.4255\n",
            "Epoch 270/300\n",
            "\n",
            "Epoch 00270: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.7389 - acc: 0.7458 - val_loss: 1.3084 - val_acc: 0.4894\n",
            "Epoch 271/300\n",
            "\n",
            "Epoch 00271: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.6224 - acc: 0.7917 - val_loss: 1.3211 - val_acc: 0.4255\n",
            "Epoch 272/300\n",
            "\n",
            "Epoch 00272: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.7049 - acc: 0.7583 - val_loss: 1.3666 - val_acc: 0.4468\n",
            "Epoch 273/300\n",
            "\n",
            "Epoch 00273: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.6933 - acc: 0.7333 - val_loss: 1.3307 - val_acc: 0.4255\n",
            "Epoch 274/300\n",
            "\n",
            "Epoch 00274: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.7059 - acc: 0.7792 - val_loss: 1.3640 - val_acc: 0.4468\n",
            "Epoch 275/300\n",
            "\n",
            "Epoch 00275: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.6869 - acc: 0.7750 - val_loss: 1.3816 - val_acc: 0.3830\n",
            "Epoch 276/300\n",
            "\n",
            "Epoch 00276: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.7178 - acc: 0.7750 - val_loss: 1.3062 - val_acc: 0.4681\n",
            "Epoch 277/300\n",
            "\n",
            "Epoch 00277: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.7971 - acc: 0.6833 - val_loss: 1.2724 - val_acc: 0.4681\n",
            "Epoch 278/300\n",
            "\n",
            "Epoch 00278: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.7479 - acc: 0.7250 - val_loss: 1.3331 - val_acc: 0.4681\n",
            "Epoch 279/300\n",
            "\n",
            "Epoch 00279: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.7057 - acc: 0.7500 - val_loss: 1.3082 - val_acc: 0.5319\n",
            "Epoch 280/300\n",
            "\n",
            "Epoch 00280: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.7132 - acc: 0.7458 - val_loss: 1.2612 - val_acc: 0.4681\n",
            "Epoch 281/300\n",
            "\n",
            "Epoch 00281: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.7349 - acc: 0.7542 - val_loss: 1.3782 - val_acc: 0.4681\n",
            "Epoch 282/300\n",
            "\n",
            "Epoch 00282: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.6623 - acc: 0.7708 - val_loss: 1.2709 - val_acc: 0.5106\n",
            "Epoch 283/300\n",
            "\n",
            "Epoch 00283: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.6895 - acc: 0.7708 - val_loss: 1.2317 - val_acc: 0.5106\n",
            "Epoch 284/300\n",
            "\n",
            "Epoch 00284: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.7320 - acc: 0.7625 - val_loss: 1.2585 - val_acc: 0.4894\n",
            "Epoch 285/300\n",
            "\n",
            "Epoch 00285: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.7073 - acc: 0.7375 - val_loss: 1.2864 - val_acc: 0.4681\n",
            "Epoch 286/300\n",
            "\n",
            "Epoch 00286: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.7467 - acc: 0.7167 - val_loss: 1.3146 - val_acc: 0.4681\n",
            "Epoch 287/300\n",
            "\n",
            "Epoch 00287: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.6656 - acc: 0.8250 - val_loss: 1.3191 - val_acc: 0.4468\n",
            "Epoch 288/300\n",
            "\n",
            "Epoch 00288: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.7085 - acc: 0.7667 - val_loss: 1.2964 - val_acc: 0.4468\n",
            "Epoch 289/300\n",
            "\n",
            "Epoch 00289: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.6965 - acc: 0.8000 - val_loss: 1.2927 - val_acc: 0.5106\n",
            "Epoch 290/300\n",
            "\n",
            "Epoch 00290: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.7707 - acc: 0.7250 - val_loss: 1.3517 - val_acc: 0.4255\n",
            "Epoch 291/300\n",
            "\n",
            "Epoch 00291: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.7267 - acc: 0.7333 - val_loss: 1.2403 - val_acc: 0.4468\n",
            "Epoch 292/300\n",
            "\n",
            "Epoch 00292: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.7675 - acc: 0.7042 - val_loss: 1.2484 - val_acc: 0.4894\n",
            "Epoch 293/300\n",
            "\n",
            "Epoch 00293: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.6954 - acc: 0.7625 - val_loss: 1.2646 - val_acc: 0.4894\n",
            "Epoch 294/300\n",
            "\n",
            "Epoch 00294: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.6697 - acc: 0.7583 - val_loss: 1.3116 - val_acc: 0.4681\n",
            "Epoch 295/300\n",
            "\n",
            "Epoch 00295: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.6932 - acc: 0.7958 - val_loss: 1.2947 - val_acc: 0.4681\n",
            "Epoch 296/300\n",
            "\n",
            "Epoch 00296: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.6856 - acc: 0.7833 - val_loss: 1.3111 - val_acc: 0.5106\n",
            "Epoch 297/300\n",
            "\n",
            "Epoch 00297: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.7441 - acc: 0.7208 - val_loss: 1.2798 - val_acc: 0.5106\n",
            "Epoch 298/300\n",
            "\n",
            "Epoch 00298: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.6900 - acc: 0.7708 - val_loss: 1.2749 - val_acc: 0.4681\n",
            "Epoch 299/300\n",
            "\n",
            "Epoch 00299: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.6863 - acc: 0.7750 - val_loss: 1.2902 - val_acc: 0.5319\n",
            "Epoch 300/300\n",
            "\n",
            "Epoch 00300: val_loss did not improve from 1.20678\n",
            "240/240 - 0s - loss: 0.7024 - acc: 0.7458 - val_loss: 1.3681 - val_acc: 0.4681\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nw2LWk4lsDqwAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "# Evaluate on test data\n",
            "288/288 [==============================] - 1s 2ms/sample - loss: 1.2552 - acc: 0.4826\n",
            "test loss, test acc: [1.25522248130033, 0.4826389]\n",
            "EEG_Deep/Data2A/Data_A07T.mat\n",
            "(288, 22, 1875)\n",
            "(288, 1)\n",
            "(240, 22, 750)\n",
            "(240, 1)\n",
            "(47, 22, 750)\n",
            "(47, 1)\n",
            "X_train shape: (240, 1, 22, 750)\n",
            "240 train samples\n",
            "47 val samples\n",
            "EEG_Deep/Data2A/Data_A07E.mat\n",
            "(288, 22, 1875)\n",
            "(288, 1)\n",
            "(288, 22, 750)\n",
            "(288, 1)\n",
            "X_train shape: (288, 1, 22, 750)\n",
            "288 train samples\n",
            "Train on 240 samples, validate on 47 samples\n",
            "Epoch 1/300\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 1.38729, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 1s - loss: 1.4052 - acc: 0.2417 - val_loss: 1.3873 - val_acc: 0.2128\n",
            "Epoch 2/300\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 1.38729\n",
            "240/240 - 0s - loss: 1.3719 - acc: 0.3000 - val_loss: 1.3882 - val_acc: 0.2128\n",
            "Epoch 3/300\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 1.38729\n",
            "240/240 - 0s - loss: 1.3529 - acc: 0.3417 - val_loss: 1.3893 - val_acc: 0.2128\n",
            "Epoch 4/300\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 1.38729\n",
            "240/240 - 0s - loss: 1.3355 - acc: 0.3792 - val_loss: 1.3909 - val_acc: 0.2553\n",
            "Epoch 5/300\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 1.38729\n",
            "240/240 - 0s - loss: 1.3264 - acc: 0.3917 - val_loss: 1.3907 - val_acc: 0.2979\n",
            "Epoch 6/300\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 1.38729\n",
            "240/240 - 0s - loss: 1.3068 - acc: 0.4208 - val_loss: 1.3939 - val_acc: 0.3191\n",
            "Epoch 7/300\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 1.38729\n",
            "240/240 - 0s - loss: 1.2856 - acc: 0.4458 - val_loss: 1.3969 - val_acc: 0.2766\n",
            "Epoch 8/300\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 1.38729\n",
            "240/240 - 0s - loss: 1.2680 - acc: 0.4708 - val_loss: 1.3993 - val_acc: 0.3191\n",
            "Epoch 9/300\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 1.38729\n",
            "240/240 - 0s - loss: 1.2745 - acc: 0.4750 - val_loss: 1.3980 - val_acc: 0.2766\n",
            "Epoch 10/300\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 1.38729\n",
            "240/240 - 0s - loss: 1.2345 - acc: 0.4750 - val_loss: 1.3941 - val_acc: 0.2553\n",
            "Epoch 11/300\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 1.38729\n",
            "240/240 - 0s - loss: 1.2412 - acc: 0.4792 - val_loss: 1.3940 - val_acc: 0.2340\n",
            "Epoch 12/300\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 1.38729\n",
            "240/240 - 0s - loss: 1.2081 - acc: 0.5083 - val_loss: 1.3978 - val_acc: 0.2553\n",
            "Epoch 13/300\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 1.38729\n",
            "240/240 - 0s - loss: 1.1892 - acc: 0.4792 - val_loss: 1.4036 - val_acc: 0.2979\n",
            "Epoch 14/300\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 1.38729\n",
            "240/240 - 0s - loss: 1.1656 - acc: 0.4917 - val_loss: 1.4076 - val_acc: 0.2340\n",
            "Epoch 15/300\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 1.38729\n",
            "240/240 - 0s - loss: 1.1793 - acc: 0.5125 - val_loss: 1.4203 - val_acc: 0.2979\n",
            "Epoch 16/300\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 1.38729\n",
            "240/240 - 0s - loss: 1.1244 - acc: 0.5958 - val_loss: 1.4217 - val_acc: 0.2340\n",
            "Epoch 17/300\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 1.38729\n",
            "240/240 - 0s - loss: 1.1363 - acc: 0.5208 - val_loss: 1.4365 - val_acc: 0.2553\n",
            "Epoch 18/300\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 1.38729\n",
            "240/240 - 0s - loss: 1.1037 - acc: 0.5417 - val_loss: 1.4269 - val_acc: 0.2766\n",
            "Epoch 19/300\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 1.38729\n",
            "240/240 - 0s - loss: 1.0797 - acc: 0.5667 - val_loss: 1.3991 - val_acc: 0.3191\n",
            "Epoch 20/300\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 1.38729\n",
            "240/240 - 0s - loss: 1.0766 - acc: 0.5792 - val_loss: 1.4216 - val_acc: 0.3191\n",
            "Epoch 21/300\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 1.38729\n",
            "240/240 - 0s - loss: 1.1274 - acc: 0.5750 - val_loss: 1.4232 - val_acc: 0.2979\n",
            "Epoch 22/300\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 1.38729\n",
            "240/240 - 0s - loss: 1.0707 - acc: 0.5583 - val_loss: 1.4133 - val_acc: 0.2979\n",
            "Epoch 23/300\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 1.38729\n",
            "240/240 - 0s - loss: 1.0821 - acc: 0.5750 - val_loss: 1.4316 - val_acc: 0.3191\n",
            "Epoch 24/300\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 1.38729\n",
            "240/240 - 0s - loss: 1.0725 - acc: 0.6000 - val_loss: 1.4263 - val_acc: 0.3191\n",
            "Epoch 25/300\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 1.38729\n",
            "240/240 - 0s - loss: 1.0447 - acc: 0.5917 - val_loss: 1.4049 - val_acc: 0.3191\n",
            "Epoch 26/300\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 1.38729\n",
            "240/240 - 0s - loss: 1.0460 - acc: 0.5708 - val_loss: 1.4000 - val_acc: 0.3191\n",
            "Epoch 27/300\n",
            "\n",
            "Epoch 00027: val_loss improved from 1.38729 to 1.35700, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.0037 - acc: 0.6125 - val_loss: 1.3570 - val_acc: 0.2979\n",
            "Epoch 28/300\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 1.35700\n",
            "240/240 - 0s - loss: 0.9830 - acc: 0.6458 - val_loss: 1.3683 - val_acc: 0.3191\n",
            "Epoch 29/300\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 1.35700\n",
            "240/240 - 0s - loss: 1.0032 - acc: 0.5875 - val_loss: 1.3721 - val_acc: 0.2979\n",
            "Epoch 30/300\n",
            "\n",
            "Epoch 00030: val_loss improved from 1.35700 to 1.31655, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.0071 - acc: 0.5625 - val_loss: 1.3165 - val_acc: 0.3404\n",
            "Epoch 31/300\n",
            "\n",
            "Epoch 00031: val_loss improved from 1.31655 to 1.30947, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.9533 - acc: 0.6542 - val_loss: 1.3095 - val_acc: 0.3191\n",
            "Epoch 32/300\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 1.30947\n",
            "240/240 - 0s - loss: 0.9535 - acc: 0.6625 - val_loss: 1.3135 - val_acc: 0.3404\n",
            "Epoch 33/300\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 1.30947\n",
            "240/240 - 0s - loss: 0.9761 - acc: 0.6208 - val_loss: 1.3513 - val_acc: 0.3191\n",
            "Epoch 34/300\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 1.30947\n",
            "240/240 - 0s - loss: 0.9710 - acc: 0.6250 - val_loss: 1.3572 - val_acc: 0.2766\n",
            "Epoch 35/300\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 1.30947\n",
            "240/240 - 0s - loss: 0.9336 - acc: 0.6708 - val_loss: 1.3942 - val_acc: 0.2979\n",
            "Epoch 36/300\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 1.30947\n",
            "240/240 - 0s - loss: 0.9292 - acc: 0.6167 - val_loss: 1.3584 - val_acc: 0.2340\n",
            "Epoch 37/300\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 1.30947\n",
            "240/240 - 0s - loss: 0.9246 - acc: 0.6917 - val_loss: 1.4419 - val_acc: 0.3191\n",
            "Epoch 38/300\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 1.30947\n",
            "240/240 - 0s - loss: 0.9108 - acc: 0.6792 - val_loss: 1.3737 - val_acc: 0.2766\n",
            "Epoch 39/300\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 1.30947\n",
            "240/240 - 0s - loss: 0.8824 - acc: 0.6917 - val_loss: 1.4717 - val_acc: 0.2766\n",
            "Epoch 40/300\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 1.30947\n",
            "240/240 - 0s - loss: 0.8310 - acc: 0.7208 - val_loss: 1.5089 - val_acc: 0.2979\n",
            "Epoch 41/300\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 1.30947\n",
            "240/240 - 0s - loss: 0.8495 - acc: 0.6667 - val_loss: 1.5555 - val_acc: 0.2553\n",
            "Epoch 42/300\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 1.30947\n",
            "240/240 - 0s - loss: 0.8697 - acc: 0.6583 - val_loss: 1.7334 - val_acc: 0.3191\n",
            "Epoch 43/300\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 1.30947\n",
            "240/240 - 0s - loss: 0.8326 - acc: 0.6708 - val_loss: 1.6711 - val_acc: 0.2553\n",
            "Epoch 44/300\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 1.30947\n",
            "240/240 - 0s - loss: 0.7845 - acc: 0.7458 - val_loss: 1.6178 - val_acc: 0.2340\n",
            "Epoch 45/300\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 1.30947\n",
            "240/240 - 0s - loss: 0.7574 - acc: 0.7375 - val_loss: 1.5672 - val_acc: 0.3191\n",
            "Epoch 46/300\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 1.30947\n",
            "240/240 - 0s - loss: 0.7457 - acc: 0.7458 - val_loss: 1.4175 - val_acc: 0.3830\n",
            "Epoch 47/300\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 1.30947\n",
            "240/240 - 0s - loss: 0.7720 - acc: 0.7042 - val_loss: 1.4558 - val_acc: 0.3617\n",
            "Epoch 48/300\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 1.30947\n",
            "240/240 - 0s - loss: 0.7153 - acc: 0.7542 - val_loss: 1.3805 - val_acc: 0.3830\n",
            "Epoch 49/300\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 1.30947\n",
            "240/240 - 0s - loss: 0.7469 - acc: 0.6875 - val_loss: 1.3290 - val_acc: 0.4255\n",
            "Epoch 50/300\n",
            "\n",
            "Epoch 00050: val_loss improved from 1.30947 to 1.23949, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.6861 - acc: 0.7875 - val_loss: 1.2395 - val_acc: 0.4468\n",
            "Epoch 51/300\n",
            "\n",
            "Epoch 00051: val_loss improved from 1.23949 to 1.22975, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.7647 - acc: 0.7125 - val_loss: 1.2297 - val_acc: 0.4255\n",
            "Epoch 52/300\n",
            "\n",
            "Epoch 00052: val_loss improved from 1.22975 to 1.19956, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.7445 - acc: 0.7417 - val_loss: 1.1996 - val_acc: 0.4255\n",
            "Epoch 53/300\n",
            "\n",
            "Epoch 00053: val_loss improved from 1.19956 to 1.13425, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.7036 - acc: 0.7208 - val_loss: 1.1343 - val_acc: 0.4681\n",
            "Epoch 54/300\n",
            "\n",
            "Epoch 00054: val_loss improved from 1.13425 to 1.08130, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.6920 - acc: 0.7458 - val_loss: 1.0813 - val_acc: 0.4468\n",
            "Epoch 55/300\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 1.08130\n",
            "240/240 - 0s - loss: 0.7163 - acc: 0.7542 - val_loss: 1.1580 - val_acc: 0.4255\n",
            "Epoch 56/300\n",
            "\n",
            "Epoch 00056: val_loss improved from 1.08130 to 1.08098, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.6248 - acc: 0.8000 - val_loss: 1.0810 - val_acc: 0.4468\n",
            "Epoch 57/300\n",
            "\n",
            "Epoch 00057: val_loss improved from 1.08098 to 1.01076, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.6840 - acc: 0.7875 - val_loss: 1.0108 - val_acc: 0.4894\n",
            "Epoch 58/300\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 1.01076\n",
            "240/240 - 0s - loss: 0.6590 - acc: 0.7667 - val_loss: 1.0328 - val_acc: 0.5532\n",
            "Epoch 59/300\n",
            "\n",
            "Epoch 00059: val_loss improved from 1.01076 to 1.00106, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.6581 - acc: 0.7542 - val_loss: 1.0011 - val_acc: 0.5106\n",
            "Epoch 60/300\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 1.00106\n",
            "240/240 - 0s - loss: 0.6276 - acc: 0.7875 - val_loss: 1.0477 - val_acc: 0.4681\n",
            "Epoch 61/300\n",
            "\n",
            "Epoch 00061: val_loss improved from 1.00106 to 0.97300, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.7173 - acc: 0.6917 - val_loss: 0.9730 - val_acc: 0.4681\n",
            "Epoch 62/300\n",
            "\n",
            "Epoch 00062: val_loss improved from 0.97300 to 0.91296, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.7069 - acc: 0.7042 - val_loss: 0.9130 - val_acc: 0.5319\n",
            "Epoch 63/300\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.91296\n",
            "240/240 - 0s - loss: 0.6723 - acc: 0.7542 - val_loss: 0.9146 - val_acc: 0.5319\n",
            "Epoch 64/300\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.91296\n",
            "240/240 - 0s - loss: 0.6783 - acc: 0.7667 - val_loss: 0.9273 - val_acc: 0.5532\n",
            "Epoch 65/300\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.91296\n",
            "240/240 - 0s - loss: 0.6216 - acc: 0.7625 - val_loss: 0.9287 - val_acc: 0.5106\n",
            "Epoch 66/300\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.91296\n",
            "240/240 - 0s - loss: 0.6662 - acc: 0.7583 - val_loss: 0.9446 - val_acc: 0.4681\n",
            "Epoch 67/300\n",
            "\n",
            "Epoch 00067: val_loss improved from 0.91296 to 0.88950, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.6235 - acc: 0.7958 - val_loss: 0.8895 - val_acc: 0.5532\n",
            "Epoch 68/300\n",
            "\n",
            "Epoch 00068: val_loss improved from 0.88950 to 0.88268, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.6455 - acc: 0.7917 - val_loss: 0.8827 - val_acc: 0.5106\n",
            "Epoch 69/300\n",
            "\n",
            "Epoch 00069: val_loss improved from 0.88268 to 0.87895, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.6589 - acc: 0.7542 - val_loss: 0.8789 - val_acc: 0.5532\n",
            "Epoch 70/300\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.87895\n",
            "240/240 - 0s - loss: 0.6531 - acc: 0.7292 - val_loss: 0.8958 - val_acc: 0.5532\n",
            "Epoch 71/300\n",
            "\n",
            "Epoch 00071: val_loss improved from 0.87895 to 0.87244, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.6184 - acc: 0.7625 - val_loss: 0.8724 - val_acc: 0.5745\n",
            "Epoch 72/300\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.87244\n",
            "240/240 - 0s - loss: 0.6074 - acc: 0.8000 - val_loss: 0.8849 - val_acc: 0.5745\n",
            "Epoch 73/300\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.87244\n",
            "240/240 - 0s - loss: 0.6270 - acc: 0.7667 - val_loss: 0.8767 - val_acc: 0.6170\n",
            "Epoch 74/300\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.87244\n",
            "240/240 - 0s - loss: 0.6178 - acc: 0.7708 - val_loss: 0.8747 - val_acc: 0.5745\n",
            "Epoch 75/300\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.87244\n",
            "240/240 - 0s - loss: 0.6200 - acc: 0.7917 - val_loss: 0.8997 - val_acc: 0.5957\n",
            "Epoch 76/300\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.87244\n",
            "240/240 - 0s - loss: 0.5608 - acc: 0.8167 - val_loss: 0.9241 - val_acc: 0.5532\n",
            "Epoch 77/300\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.87244\n",
            "240/240 - 0s - loss: 0.6004 - acc: 0.7792 - val_loss: 0.8988 - val_acc: 0.6170\n",
            "Epoch 78/300\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.87244\n",
            "240/240 - 0s - loss: 0.6317 - acc: 0.7542 - val_loss: 0.9372 - val_acc: 0.5106\n",
            "Epoch 79/300\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.87244\n",
            "240/240 - 0s - loss: 0.6174 - acc: 0.7792 - val_loss: 0.9006 - val_acc: 0.5106\n",
            "Epoch 80/300\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.87244\n",
            "240/240 - 0s - loss: 0.6178 - acc: 0.7958 - val_loss: 0.8840 - val_acc: 0.5532\n",
            "Epoch 81/300\n",
            "\n",
            "Epoch 00081: val_loss improved from 0.87244 to 0.84845, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.5871 - acc: 0.8125 - val_loss: 0.8484 - val_acc: 0.5532\n",
            "Epoch 82/300\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.84845\n",
            "240/240 - 0s - loss: 0.5838 - acc: 0.8167 - val_loss: 0.8787 - val_acc: 0.5106\n",
            "Epoch 83/300\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.84845\n",
            "240/240 - 0s - loss: 0.5898 - acc: 0.8042 - val_loss: 0.8696 - val_acc: 0.5319\n",
            "Epoch 84/300\n",
            "\n",
            "Epoch 00084: val_loss improved from 0.84845 to 0.81633, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.5875 - acc: 0.7958 - val_loss: 0.8163 - val_acc: 0.5532\n",
            "Epoch 85/300\n",
            "\n",
            "Epoch 00085: val_loss improved from 0.81633 to 0.81542, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.5457 - acc: 0.8458 - val_loss: 0.8154 - val_acc: 0.5532\n",
            "Epoch 86/300\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.81542\n",
            "240/240 - 0s - loss: 0.5658 - acc: 0.7750 - val_loss: 0.8375 - val_acc: 0.5745\n",
            "Epoch 87/300\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.81542\n",
            "240/240 - 0s - loss: 0.5794 - acc: 0.7792 - val_loss: 0.8295 - val_acc: 0.6170\n",
            "Epoch 88/300\n",
            "\n",
            "Epoch 00088: val_loss improved from 0.81542 to 0.79515, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.5622 - acc: 0.8042 - val_loss: 0.7951 - val_acc: 0.6170\n",
            "Epoch 89/300\n",
            "\n",
            "Epoch 00089: val_loss improved from 0.79515 to 0.77717, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.5792 - acc: 0.7750 - val_loss: 0.7772 - val_acc: 0.6170\n",
            "Epoch 90/300\n",
            "\n",
            "Epoch 00090: val_loss improved from 0.77717 to 0.77352, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.5534 - acc: 0.8125 - val_loss: 0.7735 - val_acc: 0.6596\n",
            "Epoch 91/300\n",
            "\n",
            "Epoch 00091: val_loss improved from 0.77352 to 0.76981, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.5549 - acc: 0.8000 - val_loss: 0.7698 - val_acc: 0.6383\n",
            "Epoch 92/300\n",
            "\n",
            "Epoch 00092: val_loss improved from 0.76981 to 0.76800, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.5793 - acc: 0.7792 - val_loss: 0.7680 - val_acc: 0.7021\n",
            "Epoch 93/300\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.76800\n",
            "240/240 - 0s - loss: 0.5794 - acc: 0.8125 - val_loss: 0.8075 - val_acc: 0.5319\n",
            "Epoch 94/300\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.76800\n",
            "240/240 - 0s - loss: 0.5385 - acc: 0.8042 - val_loss: 0.7975 - val_acc: 0.6596\n",
            "Epoch 95/300\n",
            "\n",
            "Epoch 00095: val_loss improved from 0.76800 to 0.74859, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.5390 - acc: 0.8333 - val_loss: 0.7486 - val_acc: 0.6596\n",
            "Epoch 96/300\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.74859\n",
            "240/240 - 0s - loss: 0.5205 - acc: 0.8208 - val_loss: 0.7872 - val_acc: 0.5957\n",
            "Epoch 97/300\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.74859\n",
            "240/240 - 0s - loss: 0.5770 - acc: 0.7708 - val_loss: 0.7789 - val_acc: 0.5745\n",
            "Epoch 98/300\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.74859\n",
            "240/240 - 0s - loss: 0.5190 - acc: 0.8375 - val_loss: 0.7843 - val_acc: 0.5957\n",
            "Epoch 99/300\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.74859\n",
            "240/240 - 0s - loss: 0.5211 - acc: 0.8292 - val_loss: 0.7665 - val_acc: 0.6383\n",
            "Epoch 100/300\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.74859\n",
            "240/240 - 0s - loss: 0.5674 - acc: 0.7917 - val_loss: 0.7638 - val_acc: 0.5957\n",
            "Epoch 101/300\n",
            "\n",
            "Epoch 00101: val_loss improved from 0.74859 to 0.71562, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.5370 - acc: 0.8208 - val_loss: 0.7156 - val_acc: 0.6383\n",
            "Epoch 102/300\n",
            "\n",
            "Epoch 00102: val_loss improved from 0.71562 to 0.71248, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.5264 - acc: 0.8125 - val_loss: 0.7125 - val_acc: 0.6596\n",
            "Epoch 103/300\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.71248\n",
            "240/240 - 0s - loss: 0.5156 - acc: 0.8458 - val_loss: 0.7478 - val_acc: 0.6170\n",
            "Epoch 104/300\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.71248\n",
            "240/240 - 0s - loss: 0.4609 - acc: 0.8625 - val_loss: 0.7427 - val_acc: 0.6383\n",
            "Epoch 105/300\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 0.71248\n",
            "240/240 - 0s - loss: 0.5298 - acc: 0.8208 - val_loss: 0.7140 - val_acc: 0.6809\n",
            "Epoch 106/300\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 0.71248\n",
            "240/240 - 0s - loss: 0.4968 - acc: 0.8417 - val_loss: 0.7277 - val_acc: 0.6596\n",
            "Epoch 107/300\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.71248\n",
            "240/240 - 0s - loss: 0.5105 - acc: 0.8167 - val_loss: 0.7145 - val_acc: 0.6596\n",
            "Epoch 108/300\n",
            "\n",
            "Epoch 00108: val_loss improved from 0.71248 to 0.71186, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.5354 - acc: 0.8250 - val_loss: 0.7119 - val_acc: 0.6596\n",
            "Epoch 109/300\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.71186\n",
            "240/240 - 0s - loss: 0.4963 - acc: 0.8375 - val_loss: 0.7248 - val_acc: 0.6170\n",
            "Epoch 110/300\n",
            "\n",
            "Epoch 00110: val_loss improved from 0.71186 to 0.69452, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.4937 - acc: 0.8542 - val_loss: 0.6945 - val_acc: 0.7021\n",
            "Epoch 111/300\n",
            "\n",
            "Epoch 00111: val_loss improved from 0.69452 to 0.69076, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.4624 - acc: 0.8583 - val_loss: 0.6908 - val_acc: 0.7021\n",
            "Epoch 112/300\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 0.69076\n",
            "240/240 - 0s - loss: 0.5075 - acc: 0.8208 - val_loss: 0.6915 - val_acc: 0.7234\n",
            "Epoch 113/300\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.69076\n",
            "240/240 - 0s - loss: 0.4667 - acc: 0.8500 - val_loss: 0.6949 - val_acc: 0.7021\n",
            "Epoch 114/300\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.69076\n",
            "240/240 - 0s - loss: 0.5006 - acc: 0.8458 - val_loss: 0.7023 - val_acc: 0.6809\n",
            "Epoch 115/300\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.69076\n",
            "240/240 - 0s - loss: 0.4949 - acc: 0.8375 - val_loss: 0.6952 - val_acc: 0.6596\n",
            "Epoch 116/300\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.69076\n",
            "240/240 - 0s - loss: 0.4999 - acc: 0.7958 - val_loss: 0.7166 - val_acc: 0.6170\n",
            "Epoch 117/300\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.69076\n",
            "240/240 - 0s - loss: 0.4826 - acc: 0.8625 - val_loss: 0.6959 - val_acc: 0.7021\n",
            "Epoch 118/300\n",
            "\n",
            "Epoch 00118: val_loss improved from 0.69076 to 0.64537, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.5342 - acc: 0.7917 - val_loss: 0.6454 - val_acc: 0.7234\n",
            "Epoch 119/300\n",
            "\n",
            "Epoch 00119: val_loss improved from 0.64537 to 0.63415, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.4703 - acc: 0.8417 - val_loss: 0.6341 - val_acc: 0.7872\n",
            "Epoch 120/300\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.63415\n",
            "240/240 - 0s - loss: 0.4745 - acc: 0.8458 - val_loss: 0.6408 - val_acc: 0.7234\n",
            "Epoch 121/300\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.63415\n",
            "240/240 - 0s - loss: 0.4704 - acc: 0.8708 - val_loss: 0.6555 - val_acc: 0.7021\n",
            "Epoch 122/300\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.63415\n",
            "240/240 - 0s - loss: 0.4333 - acc: 0.8708 - val_loss: 0.6383 - val_acc: 0.7021\n",
            "Epoch 123/300\n",
            "\n",
            "Epoch 00123: val_loss improved from 0.63415 to 0.62620, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.4668 - acc: 0.8542 - val_loss: 0.6262 - val_acc: 0.7447\n",
            "Epoch 124/300\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 0.62620\n",
            "240/240 - 0s - loss: 0.4469 - acc: 0.8333 - val_loss: 0.6694 - val_acc: 0.6596\n",
            "Epoch 125/300\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 0.62620\n",
            "240/240 - 0s - loss: 0.4907 - acc: 0.8500 - val_loss: 0.6380 - val_acc: 0.7021\n",
            "Epoch 126/300\n",
            "\n",
            "Epoch 00126: val_loss improved from 0.62620 to 0.62229, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.4525 - acc: 0.8833 - val_loss: 0.6223 - val_acc: 0.7660\n",
            "Epoch 127/300\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 0.62229\n",
            "240/240 - 0s - loss: 0.4593 - acc: 0.8792 - val_loss: 0.6614 - val_acc: 0.7021\n",
            "Epoch 128/300\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 0.62229\n",
            "240/240 - 0s - loss: 0.4699 - acc: 0.8583 - val_loss: 0.6398 - val_acc: 0.6809\n",
            "Epoch 129/300\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.62229\n",
            "240/240 - 0s - loss: 0.4674 - acc: 0.8667 - val_loss: 0.6472 - val_acc: 0.7234\n",
            "Epoch 130/300\n",
            "\n",
            "Epoch 00130: val_loss improved from 0.62229 to 0.58394, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.4718 - acc: 0.8417 - val_loss: 0.5839 - val_acc: 0.7660\n",
            "Epoch 131/300\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 0.58394\n",
            "240/240 - 0s - loss: 0.4399 - acc: 0.8792 - val_loss: 0.6124 - val_acc: 0.7447\n",
            "Epoch 132/300\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 0.58394\n",
            "240/240 - 0s - loss: 0.4574 - acc: 0.8458 - val_loss: 0.6073 - val_acc: 0.7447\n",
            "Epoch 133/300\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.58394\n",
            "240/240 - 0s - loss: 0.5030 - acc: 0.8208 - val_loss: 0.5855 - val_acc: 0.7872\n",
            "Epoch 134/300\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.58394\n",
            "240/240 - 0s - loss: 0.4764 - acc: 0.8208 - val_loss: 0.6021 - val_acc: 0.7872\n",
            "Epoch 135/300\n",
            "\n",
            "Epoch 00135: val_loss improved from 0.58394 to 0.57474, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.4680 - acc: 0.8458 - val_loss: 0.5747 - val_acc: 0.7660\n",
            "Epoch 136/300\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.57474\n",
            "240/240 - 0s - loss: 0.4431 - acc: 0.8583 - val_loss: 0.5904 - val_acc: 0.7872\n",
            "Epoch 137/300\n",
            "\n",
            "Epoch 00137: val_loss improved from 0.57474 to 0.56430, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.4357 - acc: 0.8667 - val_loss: 0.5643 - val_acc: 0.8085\n",
            "Epoch 138/300\n",
            "\n",
            "Epoch 00138: val_loss improved from 0.56430 to 0.55548, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.4453 - acc: 0.8792 - val_loss: 0.5555 - val_acc: 0.8298\n",
            "Epoch 139/300\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.55548\n",
            "240/240 - 0s - loss: 0.4103 - acc: 0.9042 - val_loss: 0.5862 - val_acc: 0.8085\n",
            "Epoch 140/300\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 0.55548\n",
            "240/240 - 0s - loss: 0.4264 - acc: 0.8500 - val_loss: 0.5766 - val_acc: 0.8085\n",
            "Epoch 141/300\n",
            "\n",
            "Epoch 00141: val_loss improved from 0.55548 to 0.53509, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.4237 - acc: 0.8750 - val_loss: 0.5351 - val_acc: 0.7872\n",
            "Epoch 142/300\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.53509\n",
            "240/240 - 0s - loss: 0.4560 - acc: 0.8417 - val_loss: 0.5551 - val_acc: 0.7447\n",
            "Epoch 143/300\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 0.53509\n",
            "240/240 - 0s - loss: 0.4312 - acc: 0.8625 - val_loss: 0.5717 - val_acc: 0.7872\n",
            "Epoch 144/300\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 0.53509\n",
            "240/240 - 0s - loss: 0.3856 - acc: 0.9000 - val_loss: 0.5478 - val_acc: 0.8298\n",
            "Epoch 145/300\n",
            "\n",
            "Epoch 00145: val_loss improved from 0.53509 to 0.52432, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.4021 - acc: 0.8708 - val_loss: 0.5243 - val_acc: 0.8298\n",
            "Epoch 146/300\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.52432\n",
            "240/240 - 0s - loss: 0.4546 - acc: 0.8417 - val_loss: 0.5359 - val_acc: 0.7447\n",
            "Epoch 147/300\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.52432\n",
            "240/240 - 0s - loss: 0.4519 - acc: 0.8458 - val_loss: 0.5475 - val_acc: 0.8298\n",
            "Epoch 148/300\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 0.52432\n",
            "240/240 - 0s - loss: 0.4243 - acc: 0.8708 - val_loss: 0.5447 - val_acc: 0.8511\n",
            "Epoch 149/300\n",
            "\n",
            "Epoch 00149: val_loss improved from 0.52432 to 0.52159, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.4199 - acc: 0.8667 - val_loss: 0.5216 - val_acc: 0.8085\n",
            "Epoch 150/300\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.52159\n",
            "240/240 - 0s - loss: 0.3706 - acc: 0.9125 - val_loss: 0.5251 - val_acc: 0.8511\n",
            "Epoch 151/300\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.52159\n",
            "240/240 - 0s - loss: 0.3900 - acc: 0.8875 - val_loss: 0.5355 - val_acc: 0.7660\n",
            "Epoch 152/300\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.52159\n",
            "240/240 - 0s - loss: 0.3977 - acc: 0.8792 - val_loss: 0.5633 - val_acc: 0.7660\n",
            "Epoch 153/300\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 0.52159\n",
            "240/240 - 0s - loss: 0.4315 - acc: 0.8625 - val_loss: 0.5593 - val_acc: 0.8298\n",
            "Epoch 154/300\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.52159\n",
            "240/240 - 0s - loss: 0.4118 - acc: 0.8833 - val_loss: 0.5286 - val_acc: 0.7660\n",
            "Epoch 155/300\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.52159\n",
            "240/240 - 0s - loss: 0.3713 - acc: 0.9083 - val_loss: 0.5563 - val_acc: 0.7872\n",
            "Epoch 156/300\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.52159\n",
            "240/240 - 0s - loss: 0.3793 - acc: 0.8917 - val_loss: 0.5651 - val_acc: 0.7660\n",
            "Epoch 157/300\n",
            "\n",
            "Epoch 00157: val_loss improved from 0.52159 to 0.51060, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.3827 - acc: 0.8750 - val_loss: 0.5106 - val_acc: 0.8298\n",
            "Epoch 158/300\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.51060\n",
            "240/240 - 0s - loss: 0.3574 - acc: 0.9125 - val_loss: 0.5361 - val_acc: 0.8085\n",
            "Epoch 159/300\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.51060\n",
            "240/240 - 0s - loss: 0.3596 - acc: 0.8875 - val_loss: 0.5732 - val_acc: 0.8723\n",
            "Epoch 160/300\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.51060\n",
            "240/240 - 0s - loss: 0.4089 - acc: 0.8500 - val_loss: 0.5587 - val_acc: 0.8298\n",
            "Epoch 161/300\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.51060\n",
            "240/240 - 0s - loss: 0.4352 - acc: 0.8458 - val_loss: 0.5214 - val_acc: 0.8511\n",
            "Epoch 162/300\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.51060\n",
            "240/240 - 0s - loss: 0.3678 - acc: 0.9000 - val_loss: 0.5420 - val_acc: 0.8298\n",
            "Epoch 163/300\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.51060\n",
            "240/240 - 0s - loss: 0.3876 - acc: 0.8958 - val_loss: 0.5367 - val_acc: 0.8085\n",
            "Epoch 164/300\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.51060\n",
            "240/240 - 0s - loss: 0.4376 - acc: 0.8417 - val_loss: 0.5720 - val_acc: 0.7872\n",
            "Epoch 165/300\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.51060\n",
            "240/240 - 0s - loss: 0.3744 - acc: 0.8792 - val_loss: 0.5126 - val_acc: 0.7872\n",
            "Epoch 166/300\n",
            "\n",
            "Epoch 00166: val_loss improved from 0.51060 to 0.48537, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.3836 - acc: 0.8875 - val_loss: 0.4854 - val_acc: 0.8298\n",
            "Epoch 167/300\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.48537\n",
            "240/240 - 0s - loss: 0.3796 - acc: 0.8750 - val_loss: 0.5059 - val_acc: 0.8085\n",
            "Epoch 168/300\n",
            "\n",
            "Epoch 00168: val_loss improved from 0.48537 to 0.48051, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.3964 - acc: 0.8958 - val_loss: 0.4805 - val_acc: 0.8298\n",
            "Epoch 169/300\n",
            "\n",
            "Epoch 00169: val_loss improved from 0.48051 to 0.46636, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.3859 - acc: 0.8708 - val_loss: 0.4664 - val_acc: 0.8723\n",
            "Epoch 170/300\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.46636\n",
            "240/240 - 0s - loss: 0.3552 - acc: 0.9042 - val_loss: 0.5146 - val_acc: 0.8298\n",
            "Epoch 171/300\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.46636\n",
            "240/240 - 0s - loss: 0.3835 - acc: 0.8958 - val_loss: 0.4986 - val_acc: 0.8298\n",
            "Epoch 172/300\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.46636\n",
            "240/240 - 0s - loss: 0.3606 - acc: 0.8875 - val_loss: 0.4788 - val_acc: 0.8511\n",
            "Epoch 173/300\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.46636\n",
            "240/240 - 0s - loss: 0.3971 - acc: 0.8792 - val_loss: 0.4883 - val_acc: 0.8511\n",
            "Epoch 174/300\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.46636\n",
            "240/240 - 0s - loss: 0.3757 - acc: 0.8958 - val_loss: 0.5221 - val_acc: 0.8298\n",
            "Epoch 175/300\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.46636\n",
            "240/240 - 0s - loss: 0.3793 - acc: 0.8750 - val_loss: 0.5057 - val_acc: 0.8085\n",
            "Epoch 176/300\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 0.46636\n",
            "240/240 - 0s - loss: 0.3701 - acc: 0.8917 - val_loss: 0.4664 - val_acc: 0.8511\n",
            "Epoch 177/300\n",
            "\n",
            "Epoch 00177: val_loss improved from 0.46636 to 0.45855, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.3446 - acc: 0.9000 - val_loss: 0.4585 - val_acc: 0.8298\n",
            "Epoch 178/300\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.45855\n",
            "240/240 - 0s - loss: 0.3499 - acc: 0.9000 - val_loss: 0.4754 - val_acc: 0.8298\n",
            "Epoch 179/300\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.45855\n",
            "240/240 - 0s - loss: 0.3822 - acc: 0.9167 - val_loss: 0.4837 - val_acc: 0.7872\n",
            "Epoch 180/300\n",
            "\n",
            "Epoch 00180: val_loss improved from 0.45855 to 0.45580, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.3847 - acc: 0.8500 - val_loss: 0.4558 - val_acc: 0.8511\n",
            "Epoch 181/300\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.45580\n",
            "240/240 - 0s - loss: 0.3212 - acc: 0.9125 - val_loss: 0.4814 - val_acc: 0.7872\n",
            "Epoch 182/300\n",
            "\n",
            "Epoch 00182: val_loss improved from 0.45580 to 0.45266, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.3874 - acc: 0.8917 - val_loss: 0.4527 - val_acc: 0.8511\n",
            "Epoch 183/300\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.45266\n",
            "240/240 - 0s - loss: 0.3592 - acc: 0.8708 - val_loss: 0.4814 - val_acc: 0.8511\n",
            "Epoch 184/300\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 0.45266\n",
            "240/240 - 0s - loss: 0.3672 - acc: 0.8875 - val_loss: 0.4886 - val_acc: 0.8298\n",
            "Epoch 185/300\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.45266\n",
            "240/240 - 0s - loss: 0.3380 - acc: 0.9292 - val_loss: 0.4742 - val_acc: 0.8511\n",
            "Epoch 186/300\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.45266\n",
            "240/240 - 0s - loss: 0.3789 - acc: 0.8917 - val_loss: 0.4870 - val_acc: 0.7660\n",
            "Epoch 187/300\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 0.45266\n",
            "240/240 - 0s - loss: 0.3144 - acc: 0.9167 - val_loss: 0.4791 - val_acc: 0.8511\n",
            "Epoch 188/300\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.45266\n",
            "240/240 - 0s - loss: 0.3462 - acc: 0.8958 - val_loss: 0.4999 - val_acc: 0.8511\n",
            "Epoch 189/300\n",
            "\n",
            "Epoch 00189: val_loss improved from 0.45266 to 0.44542, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.3468 - acc: 0.8958 - val_loss: 0.4454 - val_acc: 0.8723\n",
            "Epoch 190/300\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.44542\n",
            "240/240 - 0s - loss: 0.3225 - acc: 0.9250 - val_loss: 0.4787 - val_acc: 0.8511\n",
            "Epoch 191/300\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.44542\n",
            "240/240 - 0s - loss: 0.3579 - acc: 0.8792 - val_loss: 0.4461 - val_acc: 0.8723\n",
            "Epoch 192/300\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 0.44542\n",
            "240/240 - 0s - loss: 0.3330 - acc: 0.9042 - val_loss: 0.4636 - val_acc: 0.8511\n",
            "Epoch 193/300\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.44542\n",
            "240/240 - 0s - loss: 0.3828 - acc: 0.8958 - val_loss: 0.4463 - val_acc: 0.8723\n",
            "Epoch 194/300\n",
            "\n",
            "Epoch 00194: val_loss improved from 0.44542 to 0.44495, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.3390 - acc: 0.9042 - val_loss: 0.4450 - val_acc: 0.8511\n",
            "Epoch 195/300\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.44495\n",
            "240/240 - 0s - loss: 0.3576 - acc: 0.8958 - val_loss: 0.5147 - val_acc: 0.8298\n",
            "Epoch 196/300\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.44495\n",
            "240/240 - 0s - loss: 0.3514 - acc: 0.8833 - val_loss: 0.4531 - val_acc: 0.8511\n",
            "Epoch 197/300\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.44495\n",
            "240/240 - 0s - loss: 0.3638 - acc: 0.8917 - val_loss: 0.5390 - val_acc: 0.7660\n",
            "Epoch 198/300\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.44495\n",
            "240/240 - 0s - loss: 0.4072 - acc: 0.8583 - val_loss: 0.4528 - val_acc: 0.8723\n",
            "Epoch 199/300\n",
            "\n",
            "Epoch 00199: val_loss improved from 0.44495 to 0.44267, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.3308 - acc: 0.9167 - val_loss: 0.4427 - val_acc: 0.8511\n",
            "Epoch 200/300\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.44267\n",
            "240/240 - 0s - loss: 0.3582 - acc: 0.9000 - val_loss: 0.4838 - val_acc: 0.8511\n",
            "Epoch 201/300\n",
            "\n",
            "Epoch 00201: val_loss did not improve from 0.44267\n",
            "240/240 - 0s - loss: 0.3378 - acc: 0.9167 - val_loss: 0.4451 - val_acc: 0.8723\n",
            "Epoch 202/300\n",
            "\n",
            "Epoch 00202: val_loss did not improve from 0.44267\n",
            "240/240 - 0s - loss: 0.3597 - acc: 0.8875 - val_loss: 0.4454 - val_acc: 0.8723\n",
            "Epoch 203/300\n",
            "\n",
            "Epoch 00203: val_loss did not improve from 0.44267\n",
            "240/240 - 0s - loss: 0.3144 - acc: 0.9208 - val_loss: 0.4708 - val_acc: 0.8085\n",
            "Epoch 204/300\n",
            "\n",
            "Epoch 00204: val_loss improved from 0.44267 to 0.43889, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.3634 - acc: 0.8833 - val_loss: 0.4389 - val_acc: 0.8511\n",
            "Epoch 205/300\n",
            "\n",
            "Epoch 00205: val_loss improved from 0.43889 to 0.41153, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.3521 - acc: 0.8833 - val_loss: 0.4115 - val_acc: 0.8936\n",
            "Epoch 206/300\n",
            "\n",
            "Epoch 00206: val_loss did not improve from 0.41153\n",
            "240/240 - 0s - loss: 0.3179 - acc: 0.9083 - val_loss: 0.4947 - val_acc: 0.8511\n",
            "Epoch 207/300\n",
            "\n",
            "Epoch 00207: val_loss did not improve from 0.41153\n",
            "240/240 - 0s - loss: 0.3358 - acc: 0.9000 - val_loss: 0.5041 - val_acc: 0.8511\n",
            "Epoch 208/300\n",
            "\n",
            "Epoch 00208: val_loss improved from 0.41153 to 0.40864, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.3148 - acc: 0.9292 - val_loss: 0.4086 - val_acc: 0.8936\n",
            "Epoch 209/300\n",
            "\n",
            "Epoch 00209: val_loss did not improve from 0.40864\n",
            "240/240 - 0s - loss: 0.3545 - acc: 0.8750 - val_loss: 0.4687 - val_acc: 0.8511\n",
            "Epoch 210/300\n",
            "\n",
            "Epoch 00210: val_loss did not improve from 0.40864\n",
            "240/240 - 0s - loss: 0.3125 - acc: 0.9208 - val_loss: 0.4676 - val_acc: 0.8511\n",
            "Epoch 211/300\n",
            "\n",
            "Epoch 00211: val_loss did not improve from 0.40864\n",
            "240/240 - 0s - loss: 0.3388 - acc: 0.9000 - val_loss: 0.4493 - val_acc: 0.8298\n",
            "Epoch 212/300\n",
            "\n",
            "Epoch 00212: val_loss did not improve from 0.40864\n",
            "240/240 - 0s - loss: 0.3494 - acc: 0.8917 - val_loss: 0.4557 - val_acc: 0.8723\n",
            "Epoch 213/300\n",
            "\n",
            "Epoch 00213: val_loss did not improve from 0.40864\n",
            "240/240 - 0s - loss: 0.3081 - acc: 0.8875 - val_loss: 0.6138 - val_acc: 0.7447\n",
            "Epoch 214/300\n",
            "\n",
            "Epoch 00214: val_loss did not improve from 0.40864\n",
            "240/240 - 0s - loss: 0.3682 - acc: 0.8708 - val_loss: 0.5629 - val_acc: 0.7872\n",
            "Epoch 215/300\n",
            "\n",
            "Epoch 00215: val_loss did not improve from 0.40864\n",
            "240/240 - 0s - loss: 0.3614 - acc: 0.8958 - val_loss: 0.4382 - val_acc: 0.8511\n",
            "Epoch 216/300\n",
            "\n",
            "Epoch 00216: val_loss improved from 0.40864 to 0.39188, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.3290 - acc: 0.9042 - val_loss: 0.3919 - val_acc: 0.8723\n",
            "Epoch 217/300\n",
            "\n",
            "Epoch 00217: val_loss did not improve from 0.39188\n",
            "240/240 - 0s - loss: 0.3091 - acc: 0.8958 - val_loss: 0.4427 - val_acc: 0.8511\n",
            "Epoch 218/300\n",
            "\n",
            "Epoch 00218: val_loss did not improve from 0.39188\n",
            "240/240 - 0s - loss: 0.3331 - acc: 0.9000 - val_loss: 0.4346 - val_acc: 0.8511\n",
            "Epoch 219/300\n",
            "\n",
            "Epoch 00219: val_loss did not improve from 0.39188\n",
            "240/240 - 0s - loss: 0.3260 - acc: 0.9000 - val_loss: 0.4140 - val_acc: 0.8936\n",
            "Epoch 220/300\n",
            "\n",
            "Epoch 00220: val_loss did not improve from 0.39188\n",
            "240/240 - 0s - loss: 0.3466 - acc: 0.8958 - val_loss: 0.4141 - val_acc: 0.8511\n",
            "Epoch 221/300\n",
            "\n",
            "Epoch 00221: val_loss did not improve from 0.39188\n",
            "240/240 - 0s - loss: 0.3587 - acc: 0.9000 - val_loss: 0.4349 - val_acc: 0.8723\n",
            "Epoch 222/300\n",
            "\n",
            "Epoch 00222: val_loss did not improve from 0.39188\n",
            "240/240 - 0s - loss: 0.2910 - acc: 0.9333 - val_loss: 0.4055 - val_acc: 0.8511\n",
            "Epoch 223/300\n",
            "\n",
            "Epoch 00223: val_loss did not improve from 0.39188\n",
            "240/240 - 0s - loss: 0.3165 - acc: 0.9167 - val_loss: 0.4213 - val_acc: 0.8723\n",
            "Epoch 224/300\n",
            "\n",
            "Epoch 00224: val_loss did not improve from 0.39188\n",
            "240/240 - 0s - loss: 0.3216 - acc: 0.9000 - val_loss: 0.4428 - val_acc: 0.8723\n",
            "Epoch 225/300\n",
            "\n",
            "Epoch 00225: val_loss did not improve from 0.39188\n",
            "240/240 - 0s - loss: 0.3043 - acc: 0.9125 - val_loss: 0.4143 - val_acc: 0.8936\n",
            "Epoch 226/300\n",
            "\n",
            "Epoch 00226: val_loss did not improve from 0.39188\n",
            "240/240 - 0s - loss: 0.2938 - acc: 0.9125 - val_loss: 0.4080 - val_acc: 0.8936\n",
            "Epoch 227/300\n",
            "\n",
            "Epoch 00227: val_loss did not improve from 0.39188\n",
            "240/240 - 0s - loss: 0.3004 - acc: 0.9042 - val_loss: 0.3932 - val_acc: 0.8511\n",
            "Epoch 228/300\n",
            "\n",
            "Epoch 00228: val_loss did not improve from 0.39188\n",
            "240/240 - 0s - loss: 0.3292 - acc: 0.8833 - val_loss: 0.3981 - val_acc: 0.8936\n",
            "Epoch 229/300\n",
            "\n",
            "Epoch 00229: val_loss did not improve from 0.39188\n",
            "240/240 - 0s - loss: 0.2985 - acc: 0.9125 - val_loss: 0.4266 - val_acc: 0.8298\n",
            "Epoch 230/300\n",
            "\n",
            "Epoch 00230: val_loss did not improve from 0.39188\n",
            "240/240 - 0s - loss: 0.3187 - acc: 0.9042 - val_loss: 0.4377 - val_acc: 0.8723\n",
            "Epoch 231/300\n",
            "\n",
            "Epoch 00231: val_loss did not improve from 0.39188\n",
            "240/240 - 0s - loss: 0.3532 - acc: 0.8750 - val_loss: 0.4085 - val_acc: 0.8723\n",
            "Epoch 232/300\n",
            "\n",
            "Epoch 00232: val_loss did not improve from 0.39188\n",
            "240/240 - 0s - loss: 0.3365 - acc: 0.8958 - val_loss: 0.4104 - val_acc: 0.8511\n",
            "Epoch 233/300\n",
            "\n",
            "Epoch 00233: val_loss did not improve from 0.39188\n",
            "240/240 - 0s - loss: 0.3074 - acc: 0.8833 - val_loss: 0.4388 - val_acc: 0.8298\n",
            "Epoch 234/300\n",
            "\n",
            "Epoch 00234: val_loss did not improve from 0.39188\n",
            "240/240 - 0s - loss: 0.2947 - acc: 0.9167 - val_loss: 0.4583 - val_acc: 0.8298\n",
            "Epoch 235/300\n",
            "\n",
            "Epoch 00235: val_loss did not improve from 0.39188\n",
            "240/240 - 0s - loss: 0.3283 - acc: 0.8833 - val_loss: 0.4002 - val_acc: 0.8298\n",
            "Epoch 236/300\n",
            "\n",
            "Epoch 00236: val_loss did not improve from 0.39188\n",
            "240/240 - 0s - loss: 0.3449 - acc: 0.8917 - val_loss: 0.4843 - val_acc: 0.7872\n",
            "Epoch 237/300\n",
            "\n",
            "Epoch 00237: val_loss did not improve from 0.39188\n",
            "240/240 - 0s - loss: 0.3008 - acc: 0.9125 - val_loss: 0.3981 - val_acc: 0.8511\n",
            "Epoch 238/300\n",
            "\n",
            "Epoch 00238: val_loss improved from 0.39188 to 0.38089, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.3501 - acc: 0.8875 - val_loss: 0.3809 - val_acc: 0.8511\n",
            "Epoch 239/300\n",
            "\n",
            "Epoch 00239: val_loss did not improve from 0.38089\n",
            "240/240 - 0s - loss: 0.3433 - acc: 0.8917 - val_loss: 0.3998 - val_acc: 0.8723\n",
            "Epoch 240/300\n",
            "\n",
            "Epoch 00240: val_loss did not improve from 0.38089\n",
            "240/240 - 0s - loss: 0.3049 - acc: 0.9000 - val_loss: 0.4359 - val_acc: 0.8298\n",
            "Epoch 241/300\n",
            "\n",
            "Epoch 00241: val_loss did not improve from 0.38089\n",
            "240/240 - 0s - loss: 0.3416 - acc: 0.8833 - val_loss: 0.3906 - val_acc: 0.8723\n",
            "Epoch 242/300\n",
            "\n",
            "Epoch 00242: val_loss did not improve from 0.38089\n",
            "240/240 - 0s - loss: 0.3156 - acc: 0.9167 - val_loss: 0.4364 - val_acc: 0.8298\n",
            "Epoch 243/300\n",
            "\n",
            "Epoch 00243: val_loss did not improve from 0.38089\n",
            "240/240 - 0s - loss: 0.3420 - acc: 0.8958 - val_loss: 0.4311 - val_acc: 0.8936\n",
            "Epoch 244/300\n",
            "\n",
            "Epoch 00244: val_loss did not improve from 0.38089\n",
            "240/240 - 0s - loss: 0.2836 - acc: 0.9250 - val_loss: 0.4099 - val_acc: 0.8723\n",
            "Epoch 245/300\n",
            "\n",
            "Epoch 00245: val_loss did not improve from 0.38089\n",
            "240/240 - 0s - loss: 0.3200 - acc: 0.8833 - val_loss: 0.4223 - val_acc: 0.8298\n",
            "Epoch 246/300\n",
            "\n",
            "Epoch 00246: val_loss did not improve from 0.38089\n",
            "240/240 - 0s - loss: 0.2937 - acc: 0.9125 - val_loss: 0.3944 - val_acc: 0.8511\n",
            "Epoch 247/300\n",
            "\n",
            "Epoch 00247: val_loss did not improve from 0.38089\n",
            "240/240 - 0s - loss: 0.2923 - acc: 0.9125 - val_loss: 0.4296 - val_acc: 0.8085\n",
            "Epoch 248/300\n",
            "\n",
            "Epoch 00248: val_loss did not improve from 0.38089\n",
            "240/240 - 0s - loss: 0.2809 - acc: 0.9042 - val_loss: 0.4572 - val_acc: 0.8085\n",
            "Epoch 249/300\n",
            "\n",
            "Epoch 00249: val_loss did not improve from 0.38089\n",
            "240/240 - 0s - loss: 0.2942 - acc: 0.9042 - val_loss: 0.4411 - val_acc: 0.8511\n",
            "Epoch 250/300\n",
            "\n",
            "Epoch 00250: val_loss did not improve from 0.38089\n",
            "240/240 - 0s - loss: 0.2915 - acc: 0.9083 - val_loss: 0.4560 - val_acc: 0.8511\n",
            "Epoch 251/300\n",
            "\n",
            "Epoch 00251: val_loss did not improve from 0.38089\n",
            "240/240 - 0s - loss: 0.2686 - acc: 0.9458 - val_loss: 0.4417 - val_acc: 0.8511\n",
            "Epoch 252/300\n",
            "\n",
            "Epoch 00252: val_loss did not improve from 0.38089\n",
            "240/240 - 0s - loss: 0.2873 - acc: 0.9042 - val_loss: 0.4216 - val_acc: 0.8723\n",
            "Epoch 253/300\n",
            "\n",
            "Epoch 00253: val_loss did not improve from 0.38089\n",
            "240/240 - 0s - loss: 0.2525 - acc: 0.9250 - val_loss: 0.4273 - val_acc: 0.8298\n",
            "Epoch 254/300\n",
            "\n",
            "Epoch 00254: val_loss did not improve from 0.38089\n",
            "240/240 - 0s - loss: 0.3066 - acc: 0.9000 - val_loss: 0.4086 - val_acc: 0.8511\n",
            "Epoch 255/300\n",
            "\n",
            "Epoch 00255: val_loss did not improve from 0.38089\n",
            "240/240 - 0s - loss: 0.2750 - acc: 0.9458 - val_loss: 0.3870 - val_acc: 0.8723\n",
            "Epoch 256/300\n",
            "\n",
            "Epoch 00256: val_loss did not improve from 0.38089\n",
            "240/240 - 0s - loss: 0.3466 - acc: 0.8625 - val_loss: 0.3877 - val_acc: 0.8723\n",
            "Epoch 257/300\n",
            "\n",
            "Epoch 00257: val_loss did not improve from 0.38089\n",
            "240/240 - 0s - loss: 0.2848 - acc: 0.9292 - val_loss: 0.3961 - val_acc: 0.8511\n",
            "Epoch 258/300\n",
            "\n",
            "Epoch 00258: val_loss did not improve from 0.38089\n",
            "240/240 - 0s - loss: 0.3351 - acc: 0.8792 - val_loss: 0.3979 - val_acc: 0.8723\n",
            "Epoch 259/300\n",
            "\n",
            "Epoch 00259: val_loss did not improve from 0.38089\n",
            "240/240 - 0s - loss: 0.3272 - acc: 0.9083 - val_loss: 0.4167 - val_acc: 0.8511\n",
            "Epoch 260/300\n",
            "\n",
            "Epoch 00260: val_loss did not improve from 0.38089\n",
            "240/240 - 0s - loss: 0.2470 - acc: 0.9458 - val_loss: 0.4286 - val_acc: 0.8723\n",
            "Epoch 261/300\n",
            "\n",
            "Epoch 00261: val_loss did not improve from 0.38089\n",
            "240/240 - 0s - loss: 0.2938 - acc: 0.8958 - val_loss: 0.4159 - val_acc: 0.8298\n",
            "Epoch 262/300\n",
            "\n",
            "Epoch 00262: val_loss did not improve from 0.38089\n",
            "240/240 - 0s - loss: 0.2881 - acc: 0.9167 - val_loss: 0.3885 - val_acc: 0.8298\n",
            "Epoch 263/300\n",
            "\n",
            "Epoch 00263: val_loss did not improve from 0.38089\n",
            "240/240 - 0s - loss: 0.3009 - acc: 0.9083 - val_loss: 0.4536 - val_acc: 0.8298\n",
            "Epoch 264/300\n",
            "\n",
            "Epoch 00264: val_loss did not improve from 0.38089\n",
            "240/240 - 0s - loss: 0.3034 - acc: 0.9000 - val_loss: 0.4315 - val_acc: 0.8298\n",
            "Epoch 265/300\n",
            "\n",
            "Epoch 00265: val_loss did not improve from 0.38089\n",
            "240/240 - 0s - loss: 0.3096 - acc: 0.8958 - val_loss: 0.4359 - val_acc: 0.8511\n",
            "Epoch 266/300\n",
            "\n",
            "Epoch 00266: val_loss did not improve from 0.38089\n",
            "240/240 - 0s - loss: 0.3064 - acc: 0.8792 - val_loss: 0.4023 - val_acc: 0.8723\n",
            "Epoch 267/300\n",
            "\n",
            "Epoch 00267: val_loss improved from 0.38089 to 0.36021, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.3021 - acc: 0.8958 - val_loss: 0.3602 - val_acc: 0.8723\n",
            "Epoch 268/300\n",
            "\n",
            "Epoch 00268: val_loss did not improve from 0.36021\n",
            "240/240 - 0s - loss: 0.3199 - acc: 0.9000 - val_loss: 0.4274 - val_acc: 0.8511\n",
            "Epoch 269/300\n",
            "\n",
            "Epoch 00269: val_loss did not improve from 0.36021\n",
            "240/240 - 0s - loss: 0.2893 - acc: 0.9042 - val_loss: 0.4117 - val_acc: 0.8511\n",
            "Epoch 270/300\n",
            "\n",
            "Epoch 00270: val_loss did not improve from 0.36021\n",
            "240/240 - 0s - loss: 0.2843 - acc: 0.9292 - val_loss: 0.4016 - val_acc: 0.8511\n",
            "Epoch 271/300\n",
            "\n",
            "Epoch 00271: val_loss did not improve from 0.36021\n",
            "240/240 - 0s - loss: 0.2986 - acc: 0.9083 - val_loss: 0.4851 - val_acc: 0.8298\n",
            "Epoch 272/300\n",
            "\n",
            "Epoch 00272: val_loss did not improve from 0.36021\n",
            "240/240 - 0s - loss: 0.2947 - acc: 0.9083 - val_loss: 0.4081 - val_acc: 0.8723\n",
            "Epoch 273/300\n",
            "\n",
            "Epoch 00273: val_loss did not improve from 0.36021\n",
            "240/240 - 0s - loss: 0.2931 - acc: 0.9083 - val_loss: 0.3800 - val_acc: 0.8723\n",
            "Epoch 274/300\n",
            "\n",
            "Epoch 00274: val_loss did not improve from 0.36021\n",
            "240/240 - 0s - loss: 0.2863 - acc: 0.9167 - val_loss: 0.4076 - val_acc: 0.8511\n",
            "Epoch 275/300\n",
            "\n",
            "Epoch 00275: val_loss did not improve from 0.36021\n",
            "240/240 - 0s - loss: 0.2321 - acc: 0.9500 - val_loss: 0.3921 - val_acc: 0.8723\n",
            "Epoch 276/300\n",
            "\n",
            "Epoch 00276: val_loss did not improve from 0.36021\n",
            "240/240 - 0s - loss: 0.2553 - acc: 0.9375 - val_loss: 0.4146 - val_acc: 0.8723\n",
            "Epoch 277/300\n",
            "\n",
            "Epoch 00277: val_loss did not improve from 0.36021\n",
            "240/240 - 0s - loss: 0.2952 - acc: 0.9125 - val_loss: 0.4141 - val_acc: 0.8936\n",
            "Epoch 278/300\n",
            "\n",
            "Epoch 00278: val_loss did not improve from 0.36021\n",
            "240/240 - 0s - loss: 0.2803 - acc: 0.9042 - val_loss: 0.4081 - val_acc: 0.8723\n",
            "Epoch 279/300\n",
            "\n",
            "Epoch 00279: val_loss did not improve from 0.36021\n",
            "240/240 - 0s - loss: 0.2929 - acc: 0.9250 - val_loss: 0.3889 - val_acc: 0.8511\n",
            "Epoch 280/300\n",
            "\n",
            "Epoch 00280: val_loss did not improve from 0.36021\n",
            "240/240 - 0s - loss: 0.3159 - acc: 0.9125 - val_loss: 0.4010 - val_acc: 0.8723\n",
            "Epoch 281/300\n",
            "\n",
            "Epoch 00281: val_loss did not improve from 0.36021\n",
            "240/240 - 0s - loss: 0.2838 - acc: 0.9208 - val_loss: 0.4199 - val_acc: 0.8298\n",
            "Epoch 282/300\n",
            "\n",
            "Epoch 00282: val_loss did not improve from 0.36021\n",
            "240/240 - 0s - loss: 0.2633 - acc: 0.9083 - val_loss: 0.3697 - val_acc: 0.8936\n",
            "Epoch 283/300\n",
            "\n",
            "Epoch 00283: val_loss did not improve from 0.36021\n",
            "240/240 - 0s - loss: 0.2733 - acc: 0.9167 - val_loss: 0.3611 - val_acc: 0.8511\n",
            "Epoch 284/300\n",
            "\n",
            "Epoch 00284: val_loss did not improve from 0.36021\n",
            "240/240 - 0s - loss: 0.2753 - acc: 0.9167 - val_loss: 0.3729 - val_acc: 0.8936\n",
            "Epoch 285/300\n",
            "\n",
            "Epoch 00285: val_loss improved from 0.36021 to 0.35253, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.2488 - acc: 0.9375 - val_loss: 0.3525 - val_acc: 0.8511\n",
            "Epoch 286/300\n",
            "\n",
            "Epoch 00286: val_loss did not improve from 0.35253\n",
            "240/240 - 0s - loss: 0.2677 - acc: 0.9250 - val_loss: 0.3892 - val_acc: 0.8511\n",
            "Epoch 287/300\n",
            "\n",
            "Epoch 00287: val_loss did not improve from 0.35253\n",
            "240/240 - 0s - loss: 0.2908 - acc: 0.9208 - val_loss: 0.3635 - val_acc: 0.8936\n",
            "Epoch 288/300\n",
            "\n",
            "Epoch 00288: val_loss did not improve from 0.35253\n",
            "240/240 - 0s - loss: 0.2687 - acc: 0.9250 - val_loss: 0.3828 - val_acc: 0.8936\n",
            "Epoch 289/300\n",
            "\n",
            "Epoch 00289: val_loss did not improve from 0.35253\n",
            "240/240 - 0s - loss: 0.2794 - acc: 0.9208 - val_loss: 0.3712 - val_acc: 0.8511\n",
            "Epoch 290/300\n",
            "\n",
            "Epoch 00290: val_loss did not improve from 0.35253\n",
            "240/240 - 0s - loss: 0.2767 - acc: 0.9042 - val_loss: 0.3823 - val_acc: 0.8511\n",
            "Epoch 291/300\n",
            "\n",
            "Epoch 00291: val_loss did not improve from 0.35253\n",
            "240/240 - 0s - loss: 0.2715 - acc: 0.9250 - val_loss: 0.4786 - val_acc: 0.8298\n",
            "Epoch 292/300\n",
            "\n",
            "Epoch 00292: val_loss did not improve from 0.35253\n",
            "240/240 - 0s - loss: 0.2782 - acc: 0.9167 - val_loss: 0.3917 - val_acc: 0.8511\n",
            "Epoch 293/300\n",
            "\n",
            "Epoch 00293: val_loss did not improve from 0.35253\n",
            "240/240 - 0s - loss: 0.3346 - acc: 0.8875 - val_loss: 0.4053 - val_acc: 0.8511\n",
            "Epoch 294/300\n",
            "\n",
            "Epoch 00294: val_loss did not improve from 0.35253\n",
            "240/240 - 0s - loss: 0.2454 - acc: 0.9333 - val_loss: 0.4389 - val_acc: 0.8723\n",
            "Epoch 295/300\n",
            "\n",
            "Epoch 00295: val_loss did not improve from 0.35253\n",
            "240/240 - 0s - loss: 0.3055 - acc: 0.9000 - val_loss: 0.3838 - val_acc: 0.8723\n",
            "Epoch 296/300\n",
            "\n",
            "Epoch 00296: val_loss did not improve from 0.35253\n",
            "240/240 - 0s - loss: 0.2884 - acc: 0.9292 - val_loss: 0.3812 - val_acc: 0.8723\n",
            "Epoch 297/300\n",
            "\n",
            "Epoch 00297: val_loss did not improve from 0.35253\n",
            "240/240 - 0s - loss: 0.2805 - acc: 0.9167 - val_loss: 0.3783 - val_acc: 0.8936\n",
            "Epoch 298/300\n",
            "\n",
            "Epoch 00298: val_loss did not improve from 0.35253\n",
            "240/240 - 0s - loss: 0.2768 - acc: 0.9208 - val_loss: 0.3752 - val_acc: 0.8936\n",
            "Epoch 299/300\n",
            "\n",
            "Epoch 00299: val_loss did not improve from 0.35253\n",
            "240/240 - 0s - loss: 0.2668 - acc: 0.9250 - val_loss: 0.3872 - val_acc: 0.8298\n",
            "Epoch 300/300\n",
            "\n",
            "Epoch 00300: val_loss did not improve from 0.35253\n",
            "240/240 - 0s - loss: 0.2702 - acc: 0.9250 - val_loss: 0.4022 - val_acc: 0.8723\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nxxSBYRhGgWOKwDAMo8D5/0aGKy00I8+FAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "# Evaluate on test data\n",
            "288/288 [==============================] - 1s 2ms/sample - loss: 0.4749 - acc: 0.8264\n",
            "test loss, test acc: [0.4749107967288487, 0.8263889]\n",
            "EEG_Deep/Data2A/Data_A08T.mat\n",
            "(288, 22, 1875)\n",
            "(288, 1)\n",
            "(240, 22, 750)\n",
            "(240, 1)\n",
            "(47, 22, 750)\n",
            "(47, 1)\n",
            "X_train shape: (240, 1, 22, 750)\n",
            "240 train samples\n",
            "47 val samples\n",
            "EEG_Deep/Data2A/Data_A08E.mat\n",
            "(288, 22, 1875)\n",
            "(288, 1)\n",
            "(288, 22, 750)\n",
            "(288, 1)\n",
            "X_train shape: (288, 1, 22, 750)\n",
            "288 train samples\n",
            "Train on 240 samples, validate on 47 samples\n",
            "Epoch 1/300\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 1.38204, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 1s - loss: 1.4066 - acc: 0.2667 - val_loss: 1.3820 - val_acc: 0.2553\n",
            "Epoch 2/300\n",
            "\n",
            "Epoch 00002: val_loss improved from 1.38204 to 1.38062, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.3776 - acc: 0.3042 - val_loss: 1.3806 - val_acc: 0.3191\n",
            "Epoch 3/300\n",
            "\n",
            "Epoch 00003: val_loss improved from 1.38062 to 1.37646, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.3381 - acc: 0.4167 - val_loss: 1.3765 - val_acc: 0.4255\n",
            "Epoch 4/300\n",
            "\n",
            "Epoch 00004: val_loss improved from 1.37646 to 1.36712, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.3001 - acc: 0.4125 - val_loss: 1.3671 - val_acc: 0.4043\n",
            "Epoch 5/300\n",
            "\n",
            "Epoch 00005: val_loss improved from 1.36712 to 1.35343, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.2519 - acc: 0.5042 - val_loss: 1.3534 - val_acc: 0.3830\n",
            "Epoch 6/300\n",
            "\n",
            "Epoch 00006: val_loss improved from 1.35343 to 1.34281, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.1847 - acc: 0.5708 - val_loss: 1.3428 - val_acc: 0.2979\n",
            "Epoch 7/300\n",
            "\n",
            "Epoch 00007: val_loss improved from 1.34281 to 1.33982, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.1320 - acc: 0.5500 - val_loss: 1.3398 - val_acc: 0.2553\n",
            "Epoch 8/300\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 1.33982\n",
            "240/240 - 0s - loss: 1.1105 - acc: 0.5000 - val_loss: 1.3421 - val_acc: 0.2766\n",
            "Epoch 9/300\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 1.33982\n",
            "240/240 - 0s - loss: 1.0812 - acc: 0.5917 - val_loss: 1.3435 - val_acc: 0.2766\n",
            "Epoch 10/300\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 1.33982\n",
            "240/240 - 0s - loss: 1.0325 - acc: 0.6083 - val_loss: 1.3621 - val_acc: 0.2766\n",
            "Epoch 11/300\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 1.33982\n",
            "240/240 - 0s - loss: 0.9964 - acc: 0.5875 - val_loss: 1.3835 - val_acc: 0.2979\n",
            "Epoch 12/300\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 1.33982\n",
            "240/240 - 0s - loss: 1.0039 - acc: 0.5958 - val_loss: 1.4108 - val_acc: 0.2979\n",
            "Epoch 13/300\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 1.33982\n",
            "240/240 - 0s - loss: 0.9733 - acc: 0.6167 - val_loss: 1.4663 - val_acc: 0.2766\n",
            "Epoch 14/300\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 1.33982\n",
            "240/240 - 0s - loss: 0.9721 - acc: 0.5917 - val_loss: 1.4701 - val_acc: 0.2766\n",
            "Epoch 15/300\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 1.33982\n",
            "240/240 - 0s - loss: 0.9538 - acc: 0.6292 - val_loss: 1.4992 - val_acc: 0.2766\n",
            "Epoch 16/300\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 1.33982\n",
            "240/240 - 0s - loss: 0.9148 - acc: 0.6375 - val_loss: 1.5401 - val_acc: 0.2766\n",
            "Epoch 17/300\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 1.33982\n",
            "240/240 - 0s - loss: 0.9362 - acc: 0.6250 - val_loss: 1.4990 - val_acc: 0.2766\n",
            "Epoch 18/300\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 1.33982\n",
            "240/240 - 0s - loss: 0.9250 - acc: 0.6125 - val_loss: 1.4346 - val_acc: 0.2979\n",
            "Epoch 19/300\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 1.33982\n",
            "240/240 - 0s - loss: 0.9110 - acc: 0.6083 - val_loss: 1.3399 - val_acc: 0.3191\n",
            "Epoch 20/300\n",
            "\n",
            "Epoch 00020: val_loss improved from 1.33982 to 1.33102, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.8980 - acc: 0.6250 - val_loss: 1.3310 - val_acc: 0.3617\n",
            "Epoch 21/300\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 1.33102\n",
            "240/240 - 0s - loss: 0.8946 - acc: 0.6375 - val_loss: 1.3441 - val_acc: 0.3617\n",
            "Epoch 22/300\n",
            "\n",
            "Epoch 00022: val_loss improved from 1.33102 to 1.27880, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.8764 - acc: 0.6667 - val_loss: 1.2788 - val_acc: 0.3830\n",
            "Epoch 23/300\n",
            "\n",
            "Epoch 00023: val_loss improved from 1.27880 to 1.22735, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.8505 - acc: 0.6333 - val_loss: 1.2274 - val_acc: 0.4255\n",
            "Epoch 24/300\n",
            "\n",
            "Epoch 00024: val_loss improved from 1.22735 to 1.19739, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.8639 - acc: 0.6417 - val_loss: 1.1974 - val_acc: 0.4255\n",
            "Epoch 25/300\n",
            "\n",
            "Epoch 00025: val_loss improved from 1.19739 to 1.18645, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.8762 - acc: 0.6500 - val_loss: 1.1864 - val_acc: 0.4255\n",
            "Epoch 26/300\n",
            "\n",
            "Epoch 00026: val_loss improved from 1.18645 to 1.13716, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.9029 - acc: 0.6208 - val_loss: 1.1372 - val_acc: 0.4255\n",
            "Epoch 27/300\n",
            "\n",
            "Epoch 00027: val_loss improved from 1.13716 to 1.13535, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.8727 - acc: 0.6708 - val_loss: 1.1354 - val_acc: 0.4468\n",
            "Epoch 28/300\n",
            "\n",
            "Epoch 00028: val_loss improved from 1.13535 to 1.07252, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.8368 - acc: 0.6583 - val_loss: 1.0725 - val_acc: 0.4468\n",
            "Epoch 29/300\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 1.07252\n",
            "240/240 - 0s - loss: 0.8501 - acc: 0.6500 - val_loss: 1.1185 - val_acc: 0.4255\n",
            "Epoch 30/300\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 1.07252\n",
            "240/240 - 0s - loss: 0.8391 - acc: 0.6583 - val_loss: 1.1229 - val_acc: 0.4043\n",
            "Epoch 31/300\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 1.07252\n",
            "240/240 - 0s - loss: 0.8550 - acc: 0.6708 - val_loss: 1.0888 - val_acc: 0.4255\n",
            "Epoch 32/300\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 1.07252\n",
            "240/240 - 0s - loss: 0.8064 - acc: 0.7000 - val_loss: 1.0964 - val_acc: 0.3830\n",
            "Epoch 33/300\n",
            "\n",
            "Epoch 00033: val_loss improved from 1.07252 to 1.03438, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.8038 - acc: 0.6667 - val_loss: 1.0344 - val_acc: 0.5319\n",
            "Epoch 34/300\n",
            "\n",
            "Epoch 00034: val_loss improved from 1.03438 to 1.01182, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.7775 - acc: 0.7250 - val_loss: 1.0118 - val_acc: 0.5532\n",
            "Epoch 35/300\n",
            "\n",
            "Epoch 00035: val_loss improved from 1.01182 to 0.99448, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.7912 - acc: 0.6958 - val_loss: 0.9945 - val_acc: 0.5532\n",
            "Epoch 36/300\n",
            "\n",
            "Epoch 00036: val_loss improved from 0.99448 to 0.96500, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.7899 - acc: 0.7167 - val_loss: 0.9650 - val_acc: 0.5106\n",
            "Epoch 37/300\n",
            "\n",
            "Epoch 00037: val_loss improved from 0.96500 to 0.93007, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.7642 - acc: 0.7167 - val_loss: 0.9301 - val_acc: 0.6170\n",
            "Epoch 38/300\n",
            "\n",
            "Epoch 00038: val_loss improved from 0.93007 to 0.89717, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.7174 - acc: 0.7750 - val_loss: 0.8972 - val_acc: 0.6809\n",
            "Epoch 39/300\n",
            "\n",
            "Epoch 00039: val_loss improved from 0.89717 to 0.87043, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.6812 - acc: 0.7917 - val_loss: 0.8704 - val_acc: 0.6170\n",
            "Epoch 40/300\n",
            "\n",
            "Epoch 00040: val_loss improved from 0.87043 to 0.85864, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.6990 - acc: 0.7667 - val_loss: 0.8586 - val_acc: 0.6383\n",
            "Epoch 41/300\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.85864\n",
            "240/240 - 0s - loss: 0.7228 - acc: 0.7542 - val_loss: 0.8976 - val_acc: 0.6596\n",
            "Epoch 42/300\n",
            "\n",
            "Epoch 00042: val_loss improved from 0.85864 to 0.82439, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.7223 - acc: 0.7500 - val_loss: 0.8244 - val_acc: 0.6596\n",
            "Epoch 43/300\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.82439\n",
            "240/240 - 0s - loss: 0.7045 - acc: 0.7750 - val_loss: 0.8729 - val_acc: 0.6596\n",
            "Epoch 44/300\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.82439\n",
            "240/240 - 0s - loss: 0.6862 - acc: 0.7750 - val_loss: 0.8667 - val_acc: 0.6809\n",
            "Epoch 45/300\n",
            "\n",
            "Epoch 00045: val_loss improved from 0.82439 to 0.80636, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.6656 - acc: 0.8083 - val_loss: 0.8064 - val_acc: 0.6170\n",
            "Epoch 46/300\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.80636\n",
            "240/240 - 0s - loss: 0.6536 - acc: 0.7708 - val_loss: 0.8210 - val_acc: 0.7021\n",
            "Epoch 47/300\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.80636\n",
            "240/240 - 0s - loss: 0.6684 - acc: 0.8083 - val_loss: 0.8103 - val_acc: 0.6809\n",
            "Epoch 48/300\n",
            "\n",
            "Epoch 00048: val_loss improved from 0.80636 to 0.80313, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.6539 - acc: 0.7667 - val_loss: 0.8031 - val_acc: 0.6809\n",
            "Epoch 49/300\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.80313\n",
            "240/240 - 0s - loss: 0.6696 - acc: 0.7625 - val_loss: 0.8395 - val_acc: 0.6383\n",
            "Epoch 50/300\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.80313\n",
            "240/240 - 0s - loss: 0.6484 - acc: 0.7625 - val_loss: 0.8174 - val_acc: 0.6809\n",
            "Epoch 51/300\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.80313\n",
            "240/240 - 0s - loss: 0.6384 - acc: 0.7917 - val_loss: 0.8034 - val_acc: 0.7021\n",
            "Epoch 52/300\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.80313\n",
            "240/240 - 0s - loss: 0.6364 - acc: 0.7917 - val_loss: 0.8076 - val_acc: 0.7447\n",
            "Epoch 53/300\n",
            "\n",
            "Epoch 00053: val_loss improved from 0.80313 to 0.79586, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.6276 - acc: 0.7708 - val_loss: 0.7959 - val_acc: 0.7234\n",
            "Epoch 54/300\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.79586\n",
            "240/240 - 0s - loss: 0.6206 - acc: 0.8000 - val_loss: 0.8093 - val_acc: 0.6809\n",
            "Epoch 55/300\n",
            "\n",
            "Epoch 00055: val_loss improved from 0.79586 to 0.79290, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.6931 - acc: 0.7542 - val_loss: 0.7929 - val_acc: 0.7021\n",
            "Epoch 56/300\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.79290\n",
            "240/240 - 0s - loss: 0.6483 - acc: 0.7708 - val_loss: 0.8586 - val_acc: 0.6383\n",
            "Epoch 57/300\n",
            "\n",
            "Epoch 00057: val_loss improved from 0.79290 to 0.75379, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.6217 - acc: 0.8042 - val_loss: 0.7538 - val_acc: 0.6596\n",
            "Epoch 58/300\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.75379\n",
            "240/240 - 0s - loss: 0.6648 - acc: 0.7917 - val_loss: 0.8085 - val_acc: 0.7021\n",
            "Epoch 59/300\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.75379\n",
            "240/240 - 0s - loss: 0.6512 - acc: 0.7542 - val_loss: 0.7955 - val_acc: 0.7234\n",
            "Epoch 60/300\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.75379\n",
            "240/240 - 0s - loss: 0.6082 - acc: 0.7917 - val_loss: 0.7704 - val_acc: 0.6809\n",
            "Epoch 61/300\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.75379\n",
            "240/240 - 0s - loss: 0.6430 - acc: 0.7958 - val_loss: 0.7884 - val_acc: 0.6809\n",
            "Epoch 62/300\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.75379\n",
            "240/240 - 0s - loss: 0.5719 - acc: 0.8333 - val_loss: 0.8722 - val_acc: 0.6383\n",
            "Epoch 63/300\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.75379\n",
            "240/240 - 0s - loss: 0.5850 - acc: 0.8208 - val_loss: 0.7911 - val_acc: 0.7021\n",
            "Epoch 64/300\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.75379\n",
            "240/240 - 0s - loss: 0.6101 - acc: 0.8000 - val_loss: 0.8010 - val_acc: 0.6809\n",
            "Epoch 65/300\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.75379\n",
            "240/240 - 0s - loss: 0.6222 - acc: 0.7917 - val_loss: 0.7859 - val_acc: 0.6596\n",
            "Epoch 66/300\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.75379\n",
            "240/240 - 0s - loss: 0.5437 - acc: 0.8292 - val_loss: 0.7827 - val_acc: 0.7021\n",
            "Epoch 67/300\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.75379\n",
            "240/240 - 0s - loss: 0.6116 - acc: 0.7875 - val_loss: 0.8095 - val_acc: 0.7021\n",
            "Epoch 68/300\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.75379\n",
            "240/240 - 0s - loss: 0.5724 - acc: 0.8042 - val_loss: 0.8286 - val_acc: 0.6383\n",
            "Epoch 69/300\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.75379\n",
            "240/240 - 0s - loss: 0.6039 - acc: 0.8000 - val_loss: 0.7935 - val_acc: 0.7021\n",
            "Epoch 70/300\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.75379\n",
            "240/240 - 0s - loss: 0.6090 - acc: 0.7833 - val_loss: 0.7596 - val_acc: 0.7447\n",
            "Epoch 71/300\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.75379\n",
            "240/240 - 0s - loss: 0.5858 - acc: 0.8333 - val_loss: 0.7915 - val_acc: 0.6809\n",
            "Epoch 72/300\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.75379\n",
            "240/240 - 0s - loss: 0.5458 - acc: 0.8292 - val_loss: 0.8025 - val_acc: 0.6596\n",
            "Epoch 73/300\n",
            "\n",
            "Epoch 00073: val_loss improved from 0.75379 to 0.72488, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.5669 - acc: 0.8250 - val_loss: 0.7249 - val_acc: 0.7234\n",
            "Epoch 74/300\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.72488\n",
            "240/240 - 0s - loss: 0.5789 - acc: 0.8125 - val_loss: 0.7554 - val_acc: 0.6596\n",
            "Epoch 75/300\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.72488\n",
            "240/240 - 0s - loss: 0.5785 - acc: 0.8208 - val_loss: 0.7727 - val_acc: 0.7234\n",
            "Epoch 76/300\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.72488\n",
            "240/240 - 0s - loss: 0.5079 - acc: 0.8583 - val_loss: 0.7706 - val_acc: 0.6596\n",
            "Epoch 77/300\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.72488\n",
            "240/240 - 0s - loss: 0.5700 - acc: 0.8167 - val_loss: 0.7626 - val_acc: 0.7021\n",
            "Epoch 78/300\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.72488\n",
            "240/240 - 0s - loss: 0.5569 - acc: 0.8042 - val_loss: 0.7712 - val_acc: 0.7021\n",
            "Epoch 79/300\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.72488\n",
            "240/240 - 0s - loss: 0.5695 - acc: 0.8167 - val_loss: 0.7683 - val_acc: 0.7234\n",
            "Epoch 80/300\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.72488\n",
            "240/240 - 0s - loss: 0.5595 - acc: 0.8042 - val_loss: 0.7773 - val_acc: 0.7021\n",
            "Epoch 81/300\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.72488\n",
            "240/240 - 0s - loss: 0.5597 - acc: 0.8333 - val_loss: 0.7766 - val_acc: 0.6170\n",
            "Epoch 82/300\n",
            "\n",
            "Epoch 00082: val_loss improved from 0.72488 to 0.70502, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.5628 - acc: 0.8458 - val_loss: 0.7050 - val_acc: 0.7021\n",
            "Epoch 83/300\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.70502\n",
            "240/240 - 0s - loss: 0.5470 - acc: 0.8000 - val_loss: 0.7204 - val_acc: 0.6809\n",
            "Epoch 84/300\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.70502\n",
            "240/240 - 0s - loss: 0.5686 - acc: 0.7958 - val_loss: 0.7298 - val_acc: 0.7234\n",
            "Epoch 85/300\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.70502\n",
            "240/240 - 0s - loss: 0.5429 - acc: 0.8375 - val_loss: 0.7662 - val_acc: 0.6809\n",
            "Epoch 86/300\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.70502\n",
            "240/240 - 0s - loss: 0.4936 - acc: 0.8583 - val_loss: 0.7399 - val_acc: 0.7021\n",
            "Epoch 87/300\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.70502\n",
            "240/240 - 0s - loss: 0.5014 - acc: 0.8375 - val_loss: 0.7781 - val_acc: 0.6809\n",
            "Epoch 88/300\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.70502\n",
            "240/240 - 0s - loss: 0.5713 - acc: 0.8000 - val_loss: 0.8196 - val_acc: 0.6596\n",
            "Epoch 89/300\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.70502\n",
            "240/240 - 0s - loss: 0.5959 - acc: 0.8167 - val_loss: 0.7275 - val_acc: 0.7021\n",
            "Epoch 90/300\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.70502\n",
            "240/240 - 0s - loss: 0.5191 - acc: 0.8458 - val_loss: 0.7421 - val_acc: 0.6809\n",
            "Epoch 91/300\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.70502\n",
            "240/240 - 0s - loss: 0.4842 - acc: 0.8625 - val_loss: 0.7729 - val_acc: 0.6170\n",
            "Epoch 92/300\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.70502\n",
            "240/240 - 0s - loss: 0.5290 - acc: 0.8583 - val_loss: 0.7535 - val_acc: 0.6383\n",
            "Epoch 93/300\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.70502\n",
            "240/240 - 0s - loss: 0.5544 - acc: 0.7958 - val_loss: 0.8218 - val_acc: 0.5957\n",
            "Epoch 94/300\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.70502\n",
            "240/240 - 0s - loss: 0.5002 - acc: 0.8417 - val_loss: 0.7909 - val_acc: 0.5957\n",
            "Epoch 95/300\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.70502\n",
            "240/240 - 0s - loss: 0.5496 - acc: 0.8417 - val_loss: 0.7841 - val_acc: 0.6383\n",
            "Epoch 96/300\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.70502\n",
            "240/240 - 0s - loss: 0.5659 - acc: 0.8292 - val_loss: 0.7770 - val_acc: 0.6596\n",
            "Epoch 97/300\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.70502\n",
            "240/240 - 0s - loss: 0.5260 - acc: 0.8042 - val_loss: 0.8153 - val_acc: 0.6170\n",
            "Epoch 98/300\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.70502\n",
            "240/240 - 0s - loss: 0.5560 - acc: 0.8125 - val_loss: 0.8235 - val_acc: 0.6383\n",
            "Epoch 99/300\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.70502\n",
            "240/240 - 0s - loss: 0.5348 - acc: 0.8375 - val_loss: 0.8056 - val_acc: 0.6596\n",
            "Epoch 100/300\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.70502\n",
            "240/240 - 0s - loss: 0.4947 - acc: 0.8708 - val_loss: 0.7753 - val_acc: 0.6383\n",
            "Epoch 101/300\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.70502\n",
            "240/240 - 0s - loss: 0.5051 - acc: 0.8333 - val_loss: 0.7192 - val_acc: 0.6809\n",
            "Epoch 102/300\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.70502\n",
            "240/240 - 0s - loss: 0.5478 - acc: 0.8292 - val_loss: 0.7567 - val_acc: 0.6596\n",
            "Epoch 103/300\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.70502\n",
            "240/240 - 0s - loss: 0.5062 - acc: 0.8458 - val_loss: 0.7222 - val_acc: 0.7021\n",
            "Epoch 104/300\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.70502\n",
            "240/240 - 0s - loss: 0.4763 - acc: 0.8667 - val_loss: 0.7463 - val_acc: 0.6596\n",
            "Epoch 105/300\n",
            "\n",
            "Epoch 00105: val_loss improved from 0.70502 to 0.69736, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.4637 - acc: 0.8542 - val_loss: 0.6974 - val_acc: 0.7021\n",
            "Epoch 106/300\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 0.69736\n",
            "240/240 - 0s - loss: 0.4933 - acc: 0.8500 - val_loss: 0.6990 - val_acc: 0.6809\n",
            "Epoch 107/300\n",
            "\n",
            "Epoch 00107: val_loss improved from 0.69736 to 0.69238, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.4754 - acc: 0.8583 - val_loss: 0.6924 - val_acc: 0.6809\n",
            "Epoch 108/300\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 0.69238\n",
            "240/240 - 0s - loss: 0.5539 - acc: 0.7875 - val_loss: 0.8235 - val_acc: 0.6809\n",
            "Epoch 109/300\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.69238\n",
            "240/240 - 0s - loss: 0.4891 - acc: 0.8208 - val_loss: 0.7187 - val_acc: 0.6809\n",
            "Epoch 110/300\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.69238\n",
            "240/240 - 0s - loss: 0.5036 - acc: 0.8417 - val_loss: 0.7679 - val_acc: 0.6170\n",
            "Epoch 111/300\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 0.69238\n",
            "240/240 - 0s - loss: 0.4855 - acc: 0.8667 - val_loss: 0.7106 - val_acc: 0.7021\n",
            "Epoch 112/300\n",
            "\n",
            "Epoch 00112: val_loss improved from 0.69238 to 0.68428, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.4984 - acc: 0.8292 - val_loss: 0.6843 - val_acc: 0.7234\n",
            "Epoch 113/300\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.68428\n",
            "240/240 - 0s - loss: 0.4888 - acc: 0.8500 - val_loss: 0.7604 - val_acc: 0.5957\n",
            "Epoch 114/300\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.68428\n",
            "240/240 - 0s - loss: 0.4938 - acc: 0.8542 - val_loss: 0.7245 - val_acc: 0.6596\n",
            "Epoch 115/300\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.68428\n",
            "240/240 - 0s - loss: 0.4695 - acc: 0.8625 - val_loss: 0.7530 - val_acc: 0.6383\n",
            "Epoch 116/300\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.68428\n",
            "240/240 - 0s - loss: 0.4685 - acc: 0.8708 - val_loss: 0.7549 - val_acc: 0.6170\n",
            "Epoch 117/300\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.68428\n",
            "240/240 - 0s - loss: 0.5073 - acc: 0.8208 - val_loss: 0.7694 - val_acc: 0.6383\n",
            "Epoch 118/300\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.68428\n",
            "240/240 - 0s - loss: 0.4457 - acc: 0.8708 - val_loss: 0.7598 - val_acc: 0.6809\n",
            "Epoch 119/300\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 0.68428\n",
            "240/240 - 0s - loss: 0.4795 - acc: 0.8583 - val_loss: 0.7515 - val_acc: 0.7021\n",
            "Epoch 120/300\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.68428\n",
            "240/240 - 0s - loss: 0.4832 - acc: 0.8417 - val_loss: 0.6961 - val_acc: 0.7234\n",
            "Epoch 121/300\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.68428\n",
            "240/240 - 0s - loss: 0.4774 - acc: 0.8542 - val_loss: 0.7135 - val_acc: 0.6809\n",
            "Epoch 122/300\n",
            "\n",
            "Epoch 00122: val_loss improved from 0.68428 to 0.68154, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.4997 - acc: 0.8292 - val_loss: 0.6815 - val_acc: 0.7872\n",
            "Epoch 123/300\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 0.68154\n",
            "240/240 - 0s - loss: 0.5172 - acc: 0.8292 - val_loss: 0.7254 - val_acc: 0.7021\n",
            "Epoch 124/300\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 0.68154\n",
            "240/240 - 0s - loss: 0.4826 - acc: 0.8000 - val_loss: 0.8090 - val_acc: 0.6809\n",
            "Epoch 125/300\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 0.68154\n",
            "240/240 - 0s - loss: 0.4477 - acc: 0.8583 - val_loss: 0.7538 - val_acc: 0.6383\n",
            "Epoch 126/300\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 0.68154\n",
            "240/240 - 0s - loss: 0.4760 - acc: 0.8458 - val_loss: 0.7237 - val_acc: 0.6596\n",
            "Epoch 127/300\n",
            "\n",
            "Epoch 00127: val_loss improved from 0.68154 to 0.68107, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.4580 - acc: 0.8625 - val_loss: 0.6811 - val_acc: 0.6809\n",
            "Epoch 128/300\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 0.68107\n",
            "240/240 - 0s - loss: 0.4541 - acc: 0.8917 - val_loss: 0.7055 - val_acc: 0.6596\n",
            "Epoch 129/300\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.68107\n",
            "240/240 - 0s - loss: 0.4893 - acc: 0.8458 - val_loss: 0.7171 - val_acc: 0.6809\n",
            "Epoch 130/300\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 0.68107\n",
            "240/240 - 0s - loss: 0.4608 - acc: 0.8875 - val_loss: 0.7460 - val_acc: 0.7234\n",
            "Epoch 131/300\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 0.68107\n",
            "240/240 - 0s - loss: 0.4507 - acc: 0.8708 - val_loss: 0.7209 - val_acc: 0.7660\n",
            "Epoch 132/300\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 0.68107\n",
            "240/240 - 0s - loss: 0.4258 - acc: 0.9083 - val_loss: 0.7192 - val_acc: 0.7447\n",
            "Epoch 133/300\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.68107\n",
            "240/240 - 0s - loss: 0.4423 - acc: 0.8833 - val_loss: 0.7641 - val_acc: 0.6809\n",
            "Epoch 134/300\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.68107\n",
            "240/240 - 0s - loss: 0.4681 - acc: 0.8667 - val_loss: 0.7831 - val_acc: 0.6383\n",
            "Epoch 135/300\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 0.68107\n",
            "240/240 - 0s - loss: 0.4286 - acc: 0.8833 - val_loss: 0.6917 - val_acc: 0.6596\n",
            "Epoch 136/300\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.68107\n",
            "240/240 - 0s - loss: 0.4984 - acc: 0.8417 - val_loss: 0.7127 - val_acc: 0.6596\n",
            "Epoch 137/300\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 0.68107\n",
            "240/240 - 0s - loss: 0.4750 - acc: 0.8458 - val_loss: 0.7430 - val_acc: 0.6809\n",
            "Epoch 138/300\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.68107\n",
            "240/240 - 0s - loss: 0.4521 - acc: 0.8833 - val_loss: 0.7215 - val_acc: 0.6596\n",
            "Epoch 139/300\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.68107\n",
            "240/240 - 0s - loss: 0.4890 - acc: 0.8167 - val_loss: 0.6902 - val_acc: 0.7021\n",
            "Epoch 140/300\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 0.68107\n",
            "240/240 - 0s - loss: 0.4310 - acc: 0.8750 - val_loss: 0.7155 - val_acc: 0.7234\n",
            "Epoch 141/300\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.68107\n",
            "240/240 - 0s - loss: 0.4825 - acc: 0.8333 - val_loss: 0.7350 - val_acc: 0.7021\n",
            "Epoch 142/300\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.68107\n",
            "240/240 - 0s - loss: 0.4592 - acc: 0.8708 - val_loss: 0.7473 - val_acc: 0.7234\n",
            "Epoch 143/300\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 0.68107\n",
            "240/240 - 0s - loss: 0.4828 - acc: 0.8167 - val_loss: 0.7033 - val_acc: 0.7660\n",
            "Epoch 144/300\n",
            "\n",
            "Epoch 00144: val_loss improved from 0.68107 to 0.68083, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.4721 - acc: 0.8583 - val_loss: 0.6808 - val_acc: 0.7234\n",
            "Epoch 145/300\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.68083\n",
            "240/240 - 0s - loss: 0.4656 - acc: 0.8667 - val_loss: 0.7014 - val_acc: 0.7021\n",
            "Epoch 146/300\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.68083\n",
            "240/240 - 0s - loss: 0.4406 - acc: 0.8750 - val_loss: 0.6892 - val_acc: 0.7234\n",
            "Epoch 147/300\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.68083\n",
            "240/240 - 0s - loss: 0.4296 - acc: 0.8667 - val_loss: 0.7039 - val_acc: 0.6809\n",
            "Epoch 148/300\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 0.68083\n",
            "240/240 - 0s - loss: 0.4165 - acc: 0.8792 - val_loss: 0.7123 - val_acc: 0.7234\n",
            "Epoch 149/300\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.68083\n",
            "240/240 - 0s - loss: 0.4200 - acc: 0.8583 - val_loss: 0.6899 - val_acc: 0.7021\n",
            "Epoch 150/300\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.68083\n",
            "240/240 - 0s - loss: 0.4350 - acc: 0.8583 - val_loss: 0.7090 - val_acc: 0.7021\n",
            "Epoch 151/300\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.68083\n",
            "240/240 - 0s - loss: 0.4449 - acc: 0.8500 - val_loss: 0.6947 - val_acc: 0.6809\n",
            "Epoch 152/300\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.68083\n",
            "240/240 - 0s - loss: 0.4416 - acc: 0.8750 - val_loss: 0.7081 - val_acc: 0.7447\n",
            "Epoch 153/300\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 0.68083\n",
            "240/240 - 0s - loss: 0.4378 - acc: 0.8667 - val_loss: 0.6882 - val_acc: 0.7660\n",
            "Epoch 154/300\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.68083\n",
            "240/240 - 0s - loss: 0.4234 - acc: 0.8833 - val_loss: 0.7462 - val_acc: 0.7021\n",
            "Epoch 155/300\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.68083\n",
            "240/240 - 0s - loss: 0.4405 - acc: 0.8375 - val_loss: 0.7881 - val_acc: 0.6383\n",
            "Epoch 156/300\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.68083\n",
            "240/240 - 0s - loss: 0.4385 - acc: 0.8583 - val_loss: 0.7061 - val_acc: 0.6809\n",
            "Epoch 157/300\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.68083\n",
            "240/240 - 0s - loss: 0.4127 - acc: 0.8708 - val_loss: 0.7092 - val_acc: 0.6596\n",
            "Epoch 158/300\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.68083\n",
            "240/240 - 0s - loss: 0.4434 - acc: 0.8750 - val_loss: 0.6923 - val_acc: 0.7021\n",
            "Epoch 159/300\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.68083\n",
            "240/240 - 0s - loss: 0.4326 - acc: 0.8792 - val_loss: 0.7268 - val_acc: 0.6596\n",
            "Epoch 160/300\n",
            "\n",
            "Epoch 00160: val_loss improved from 0.68083 to 0.65950, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.3910 - acc: 0.9083 - val_loss: 0.6595 - val_acc: 0.7234\n",
            "Epoch 161/300\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.65950\n",
            "240/240 - 0s - loss: 0.4048 - acc: 0.8792 - val_loss: 0.6641 - val_acc: 0.7021\n",
            "Epoch 162/300\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.65950\n",
            "240/240 - 0s - loss: 0.4165 - acc: 0.8667 - val_loss: 0.7742 - val_acc: 0.6596\n",
            "Epoch 163/300\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.65950\n",
            "240/240 - 0s - loss: 0.4124 - acc: 0.8667 - val_loss: 0.7484 - val_acc: 0.7021\n",
            "Epoch 164/300\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.65950\n",
            "240/240 - 0s - loss: 0.3900 - acc: 0.8708 - val_loss: 0.7282 - val_acc: 0.6596\n",
            "Epoch 165/300\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.65950\n",
            "240/240 - 0s - loss: 0.4055 - acc: 0.8792 - val_loss: 0.7097 - val_acc: 0.7234\n",
            "Epoch 166/300\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 0.65950\n",
            "240/240 - 0s - loss: 0.4285 - acc: 0.8792 - val_loss: 0.7067 - val_acc: 0.6383\n",
            "Epoch 167/300\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.65950\n",
            "240/240 - 0s - loss: 0.4289 - acc: 0.8625 - val_loss: 0.6839 - val_acc: 0.7021\n",
            "Epoch 168/300\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.65950\n",
            "240/240 - 0s - loss: 0.3900 - acc: 0.8958 - val_loss: 0.6897 - val_acc: 0.7234\n",
            "Epoch 169/300\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 0.65950\n",
            "240/240 - 0s - loss: 0.3940 - acc: 0.9000 - val_loss: 0.7197 - val_acc: 0.6809\n",
            "Epoch 170/300\n",
            "\n",
            "Epoch 00170: val_loss improved from 0.65950 to 0.64967, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.4060 - acc: 0.8833 - val_loss: 0.6497 - val_acc: 0.7234\n",
            "Epoch 171/300\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.64967\n",
            "240/240 - 0s - loss: 0.3545 - acc: 0.9250 - val_loss: 0.6595 - val_acc: 0.7021\n",
            "Epoch 172/300\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.64967\n",
            "240/240 - 0s - loss: 0.3832 - acc: 0.8958 - val_loss: 0.6830 - val_acc: 0.6596\n",
            "Epoch 173/300\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.64967\n",
            "240/240 - 0s - loss: 0.4233 - acc: 0.8500 - val_loss: 0.7235 - val_acc: 0.7021\n",
            "Epoch 174/300\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.64967\n",
            "240/240 - 0s - loss: 0.4463 - acc: 0.8417 - val_loss: 0.6742 - val_acc: 0.7021\n",
            "Epoch 175/300\n",
            "\n",
            "Epoch 00175: val_loss improved from 0.64967 to 0.64605, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.4373 - acc: 0.8375 - val_loss: 0.6461 - val_acc: 0.7021\n",
            "Epoch 176/300\n",
            "\n",
            "Epoch 00176: val_loss improved from 0.64605 to 0.64385, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.3720 - acc: 0.9042 - val_loss: 0.6439 - val_acc: 0.7234\n",
            "Epoch 177/300\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.64385\n",
            "240/240 - 0s - loss: 0.4232 - acc: 0.8458 - val_loss: 0.7193 - val_acc: 0.6383\n",
            "Epoch 178/300\n",
            "\n",
            "Epoch 00178: val_loss improved from 0.64385 to 0.62222, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.4261 - acc: 0.8667 - val_loss: 0.6222 - val_acc: 0.7234\n",
            "Epoch 179/300\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.62222\n",
            "240/240 - 0s - loss: 0.4372 - acc: 0.8667 - val_loss: 0.6602 - val_acc: 0.7234\n",
            "Epoch 180/300\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.62222\n",
            "240/240 - 0s - loss: 0.4192 - acc: 0.8708 - val_loss: 0.7062 - val_acc: 0.7021\n",
            "Epoch 181/300\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.62222\n",
            "240/240 - 0s - loss: 0.4043 - acc: 0.8625 - val_loss: 0.7818 - val_acc: 0.6809\n",
            "Epoch 182/300\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.62222\n",
            "240/240 - 0s - loss: 0.4054 - acc: 0.8708 - val_loss: 0.6756 - val_acc: 0.7021\n",
            "Epoch 183/300\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.62222\n",
            "240/240 - 0s - loss: 0.3930 - acc: 0.8917 - val_loss: 0.6459 - val_acc: 0.7447\n",
            "Epoch 184/300\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 0.62222\n",
            "240/240 - 0s - loss: 0.4236 - acc: 0.8417 - val_loss: 0.6558 - val_acc: 0.7021\n",
            "Epoch 185/300\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.62222\n",
            "240/240 - 0s - loss: 0.3957 - acc: 0.8833 - val_loss: 0.6243 - val_acc: 0.7447\n",
            "Epoch 186/300\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.62222\n",
            "240/240 - 0s - loss: 0.3536 - acc: 0.9000 - val_loss: 0.6669 - val_acc: 0.7021\n",
            "Epoch 187/300\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 0.62222\n",
            "240/240 - 0s - loss: 0.3711 - acc: 0.8875 - val_loss: 0.6432 - val_acc: 0.7021\n",
            "Epoch 188/300\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.62222\n",
            "240/240 - 0s - loss: 0.4008 - acc: 0.8792 - val_loss: 0.7320 - val_acc: 0.6596\n",
            "Epoch 189/300\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.62222\n",
            "240/240 - 0s - loss: 0.4070 - acc: 0.8542 - val_loss: 0.6833 - val_acc: 0.6596\n",
            "Epoch 190/300\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.62222\n",
            "240/240 - 0s - loss: 0.4005 - acc: 0.8875 - val_loss: 0.6555 - val_acc: 0.7447\n",
            "Epoch 191/300\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.62222\n",
            "240/240 - 0s - loss: 0.3725 - acc: 0.9125 - val_loss: 0.6977 - val_acc: 0.6809\n",
            "Epoch 192/300\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 0.62222\n",
            "240/240 - 0s - loss: 0.4002 - acc: 0.8500 - val_loss: 0.6522 - val_acc: 0.7021\n",
            "Epoch 193/300\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.62222\n",
            "240/240 - 0s - loss: 0.4091 - acc: 0.8833 - val_loss: 0.6357 - val_acc: 0.7234\n",
            "Epoch 194/300\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.62222\n",
            "240/240 - 0s - loss: 0.3737 - acc: 0.9083 - val_loss: 0.6627 - val_acc: 0.7234\n",
            "Epoch 195/300\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.62222\n",
            "240/240 - 0s - loss: 0.3899 - acc: 0.8833 - val_loss: 0.6939 - val_acc: 0.6809\n",
            "Epoch 196/300\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.62222\n",
            "240/240 - 0s - loss: 0.4011 - acc: 0.8542 - val_loss: 0.6573 - val_acc: 0.6809\n",
            "Epoch 197/300\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.62222\n",
            "240/240 - 0s - loss: 0.4257 - acc: 0.8417 - val_loss: 0.7258 - val_acc: 0.6596\n",
            "Epoch 198/300\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.62222\n",
            "240/240 - 0s - loss: 0.3912 - acc: 0.8917 - val_loss: 0.7085 - val_acc: 0.6596\n",
            "Epoch 199/300\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.62222\n",
            "240/240 - 0s - loss: 0.3632 - acc: 0.9000 - val_loss: 0.6327 - val_acc: 0.7021\n",
            "Epoch 200/300\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.62222\n",
            "240/240 - 0s - loss: 0.3564 - acc: 0.8958 - val_loss: 0.7449 - val_acc: 0.6809\n",
            "Epoch 201/300\n",
            "\n",
            "Epoch 00201: val_loss did not improve from 0.62222\n",
            "240/240 - 0s - loss: 0.3730 - acc: 0.9125 - val_loss: 0.6865 - val_acc: 0.6809\n",
            "Epoch 202/300\n",
            "\n",
            "Epoch 00202: val_loss did not improve from 0.62222\n",
            "240/240 - 0s - loss: 0.3790 - acc: 0.9125 - val_loss: 0.6700 - val_acc: 0.7021\n",
            "Epoch 203/300\n",
            "\n",
            "Epoch 00203: val_loss did not improve from 0.62222\n",
            "240/240 - 0s - loss: 0.3655 - acc: 0.8833 - val_loss: 0.7029 - val_acc: 0.6809\n",
            "Epoch 204/300\n",
            "\n",
            "Epoch 00204: val_loss did not improve from 0.62222\n",
            "240/240 - 0s - loss: 0.3776 - acc: 0.8667 - val_loss: 0.6945 - val_acc: 0.6809\n",
            "Epoch 205/300\n",
            "\n",
            "Epoch 00205: val_loss did not improve from 0.62222\n",
            "240/240 - 0s - loss: 0.3828 - acc: 0.8750 - val_loss: 0.7326 - val_acc: 0.7234\n",
            "Epoch 206/300\n",
            "\n",
            "Epoch 00206: val_loss did not improve from 0.62222\n",
            "240/240 - 0s - loss: 0.3942 - acc: 0.8667 - val_loss: 0.6745 - val_acc: 0.7660\n",
            "Epoch 207/300\n",
            "\n",
            "Epoch 00207: val_loss did not improve from 0.62222\n",
            "240/240 - 0s - loss: 0.3733 - acc: 0.9125 - val_loss: 0.7236 - val_acc: 0.7021\n",
            "Epoch 208/300\n",
            "\n",
            "Epoch 00208: val_loss did not improve from 0.62222\n",
            "240/240 - 0s - loss: 0.3781 - acc: 0.8917 - val_loss: 0.6933 - val_acc: 0.7021\n",
            "Epoch 209/300\n",
            "\n",
            "Epoch 00209: val_loss did not improve from 0.62222\n",
            "240/240 - 0s - loss: 0.3550 - acc: 0.9042 - val_loss: 0.6524 - val_acc: 0.7234\n",
            "Epoch 210/300\n",
            "\n",
            "Epoch 00210: val_loss did not improve from 0.62222\n",
            "240/240 - 0s - loss: 0.3429 - acc: 0.8958 - val_loss: 0.7236 - val_acc: 0.6596\n",
            "Epoch 211/300\n",
            "\n",
            "Epoch 00211: val_loss did not improve from 0.62222\n",
            "240/240 - 0s - loss: 0.3776 - acc: 0.8917 - val_loss: 0.7365 - val_acc: 0.7021\n",
            "Epoch 212/300\n",
            "\n",
            "Epoch 00212: val_loss did not improve from 0.62222\n",
            "240/240 - 0s - loss: 0.3531 - acc: 0.9208 - val_loss: 0.7038 - val_acc: 0.7021\n",
            "Epoch 213/300\n",
            "\n",
            "Epoch 00213: val_loss did not improve from 0.62222\n",
            "240/240 - 0s - loss: 0.3625 - acc: 0.8958 - val_loss: 0.6708 - val_acc: 0.7234\n",
            "Epoch 214/300\n",
            "\n",
            "Epoch 00214: val_loss did not improve from 0.62222\n",
            "240/240 - 0s - loss: 0.4001 - acc: 0.8583 - val_loss: 0.6893 - val_acc: 0.7447\n",
            "Epoch 215/300\n",
            "\n",
            "Epoch 00215: val_loss did not improve from 0.62222\n",
            "240/240 - 0s - loss: 0.3415 - acc: 0.9083 - val_loss: 0.7093 - val_acc: 0.7021\n",
            "Epoch 216/300\n",
            "\n",
            "Epoch 00216: val_loss did not improve from 0.62222\n",
            "240/240 - 0s - loss: 0.3558 - acc: 0.8875 - val_loss: 0.6998 - val_acc: 0.7234\n",
            "Epoch 217/300\n",
            "\n",
            "Epoch 00217: val_loss did not improve from 0.62222\n",
            "240/240 - 0s - loss: 0.3421 - acc: 0.8958 - val_loss: 0.6941 - val_acc: 0.6809\n",
            "Epoch 218/300\n",
            "\n",
            "Epoch 00218: val_loss did not improve from 0.62222\n",
            "240/240 - 0s - loss: 0.3472 - acc: 0.8917 - val_loss: 0.6779 - val_acc: 0.7021\n",
            "Epoch 219/300\n",
            "\n",
            "Epoch 00219: val_loss did not improve from 0.62222\n",
            "240/240 - 0s - loss: 0.3625 - acc: 0.8917 - val_loss: 0.6956 - val_acc: 0.7447\n",
            "Epoch 220/300\n",
            "\n",
            "Epoch 00220: val_loss did not improve from 0.62222\n",
            "240/240 - 0s - loss: 0.3969 - acc: 0.8792 - val_loss: 0.6476 - val_acc: 0.7872\n",
            "Epoch 221/300\n",
            "\n",
            "Epoch 00221: val_loss did not improve from 0.62222\n",
            "240/240 - 0s - loss: 0.3462 - acc: 0.8917 - val_loss: 0.6597 - val_acc: 0.7234\n",
            "Epoch 222/300\n",
            "\n",
            "Epoch 00222: val_loss did not improve from 0.62222\n",
            "240/240 - 0s - loss: 0.3769 - acc: 0.9000 - val_loss: 0.6410 - val_acc: 0.7872\n",
            "Epoch 223/300\n",
            "\n",
            "Epoch 00223: val_loss did not improve from 0.62222\n",
            "240/240 - 0s - loss: 0.3542 - acc: 0.9083 - val_loss: 0.6780 - val_acc: 0.7021\n",
            "Epoch 224/300\n",
            "\n",
            "Epoch 00224: val_loss did not improve from 0.62222\n",
            "240/240 - 0s - loss: 0.3688 - acc: 0.8792 - val_loss: 0.6674 - val_acc: 0.7234\n",
            "Epoch 225/300\n",
            "\n",
            "Epoch 00225: val_loss did not improve from 0.62222\n",
            "240/240 - 0s - loss: 0.3633 - acc: 0.8917 - val_loss: 0.7537 - val_acc: 0.6809\n",
            "Epoch 226/300\n",
            "\n",
            "Epoch 00226: val_loss did not improve from 0.62222\n",
            "240/240 - 0s - loss: 0.3635 - acc: 0.9083 - val_loss: 0.7703 - val_acc: 0.7021\n",
            "Epoch 227/300\n",
            "\n",
            "Epoch 00227: val_loss did not improve from 0.62222\n",
            "240/240 - 0s - loss: 0.3712 - acc: 0.8958 - val_loss: 0.6681 - val_acc: 0.7234\n",
            "Epoch 228/300\n",
            "\n",
            "Epoch 00228: val_loss did not improve from 0.62222\n",
            "240/240 - 0s - loss: 0.3921 - acc: 0.8458 - val_loss: 0.6834 - val_acc: 0.7021\n",
            "Epoch 229/300\n",
            "\n",
            "Epoch 00229: val_loss did not improve from 0.62222\n",
            "240/240 - 0s - loss: 0.3832 - acc: 0.8917 - val_loss: 0.7134 - val_acc: 0.7234\n",
            "Epoch 230/300\n",
            "\n",
            "Epoch 00230: val_loss did not improve from 0.62222\n",
            "240/240 - 0s - loss: 0.3685 - acc: 0.8958 - val_loss: 0.7201 - val_acc: 0.6809\n",
            "Epoch 231/300\n",
            "\n",
            "Epoch 00231: val_loss did not improve from 0.62222\n",
            "240/240 - 0s - loss: 0.3690 - acc: 0.9000 - val_loss: 0.6722 - val_acc: 0.7447\n",
            "Epoch 232/300\n",
            "\n",
            "Epoch 00232: val_loss improved from 0.62222 to 0.61866, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.3080 - acc: 0.9208 - val_loss: 0.6187 - val_acc: 0.7234\n",
            "Epoch 233/300\n",
            "\n",
            "Epoch 00233: val_loss did not improve from 0.61866\n",
            "240/240 - 0s - loss: 0.3770 - acc: 0.8625 - val_loss: 0.6453 - val_acc: 0.7234\n",
            "Epoch 234/300\n",
            "\n",
            "Epoch 00234: val_loss did not improve from 0.61866\n",
            "240/240 - 0s - loss: 0.3207 - acc: 0.8958 - val_loss: 0.6723 - val_acc: 0.7021\n",
            "Epoch 235/300\n",
            "\n",
            "Epoch 00235: val_loss did not improve from 0.61866\n",
            "240/240 - 0s - loss: 0.3308 - acc: 0.9000 - val_loss: 0.6671 - val_acc: 0.7021\n",
            "Epoch 236/300\n",
            "\n",
            "Epoch 00236: val_loss did not improve from 0.61866\n",
            "240/240 - 0s - loss: 0.3385 - acc: 0.8750 - val_loss: 0.7412 - val_acc: 0.6596\n",
            "Epoch 237/300\n",
            "\n",
            "Epoch 00237: val_loss did not improve from 0.61866\n",
            "240/240 - 0s - loss: 0.3848 - acc: 0.8833 - val_loss: 0.6946 - val_acc: 0.6809\n",
            "Epoch 238/300\n",
            "\n",
            "Epoch 00238: val_loss did not improve from 0.61866\n",
            "240/240 - 0s - loss: 0.3690 - acc: 0.8958 - val_loss: 0.7506 - val_acc: 0.6596\n",
            "Epoch 239/300\n",
            "\n",
            "Epoch 00239: val_loss improved from 0.61866 to 0.60545, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.3861 - acc: 0.8833 - val_loss: 0.6054 - val_acc: 0.7021\n",
            "Epoch 240/300\n",
            "\n",
            "Epoch 00240: val_loss did not improve from 0.60545\n",
            "240/240 - 0s - loss: 0.3555 - acc: 0.8833 - val_loss: 0.6344 - val_acc: 0.7234\n",
            "Epoch 241/300\n",
            "\n",
            "Epoch 00241: val_loss did not improve from 0.60545\n",
            "240/240 - 0s - loss: 0.3682 - acc: 0.8833 - val_loss: 0.7143 - val_acc: 0.6809\n",
            "Epoch 242/300\n",
            "\n",
            "Epoch 00242: val_loss did not improve from 0.60545\n",
            "240/240 - 0s - loss: 0.3560 - acc: 0.8917 - val_loss: 0.7003 - val_acc: 0.7447\n",
            "Epoch 243/300\n",
            "\n",
            "Epoch 00243: val_loss did not improve from 0.60545\n",
            "240/240 - 0s - loss: 0.3264 - acc: 0.9250 - val_loss: 0.7251 - val_acc: 0.7021\n",
            "Epoch 244/300\n",
            "\n",
            "Epoch 00244: val_loss did not improve from 0.60545\n",
            "240/240 - 0s - loss: 0.3374 - acc: 0.9042 - val_loss: 0.7190 - val_acc: 0.7234\n",
            "Epoch 245/300\n",
            "\n",
            "Epoch 00245: val_loss did not improve from 0.60545\n",
            "240/240 - 0s - loss: 0.3290 - acc: 0.9125 - val_loss: 0.6469 - val_acc: 0.7234\n",
            "Epoch 246/300\n",
            "\n",
            "Epoch 00246: val_loss did not improve from 0.60545\n",
            "240/240 - 0s - loss: 0.3422 - acc: 0.8833 - val_loss: 0.6608 - val_acc: 0.7021\n",
            "Epoch 247/300\n",
            "\n",
            "Epoch 00247: val_loss did not improve from 0.60545\n",
            "240/240 - 0s - loss: 0.3442 - acc: 0.8708 - val_loss: 0.7354 - val_acc: 0.6809\n",
            "Epoch 248/300\n",
            "\n",
            "Epoch 00248: val_loss did not improve from 0.60545\n",
            "240/240 - 0s - loss: 0.3227 - acc: 0.9083 - val_loss: 0.7108 - val_acc: 0.7021\n",
            "Epoch 249/300\n",
            "\n",
            "Epoch 00249: val_loss did not improve from 0.60545\n",
            "240/240 - 0s - loss: 0.3739 - acc: 0.8667 - val_loss: 0.6742 - val_acc: 0.7234\n",
            "Epoch 250/300\n",
            "\n",
            "Epoch 00250: val_loss did not improve from 0.60545\n",
            "240/240 - 0s - loss: 0.3752 - acc: 0.8708 - val_loss: 0.6745 - val_acc: 0.7234\n",
            "Epoch 251/300\n",
            "\n",
            "Epoch 00251: val_loss did not improve from 0.60545\n",
            "240/240 - 0s - loss: 0.3574 - acc: 0.8958 - val_loss: 0.6599 - val_acc: 0.7021\n",
            "Epoch 252/300\n",
            "\n",
            "Epoch 00252: val_loss did not improve from 0.60545\n",
            "240/240 - 0s - loss: 0.3645 - acc: 0.8708 - val_loss: 0.7373 - val_acc: 0.6809\n",
            "Epoch 253/300\n",
            "\n",
            "Epoch 00253: val_loss did not improve from 0.60545\n",
            "240/240 - 0s - loss: 0.3182 - acc: 0.9000 - val_loss: 0.6353 - val_acc: 0.7234\n",
            "Epoch 254/300\n",
            "\n",
            "Epoch 00254: val_loss did not improve from 0.60545\n",
            "240/240 - 0s - loss: 0.3493 - acc: 0.8708 - val_loss: 0.7395 - val_acc: 0.7021\n",
            "Epoch 255/300\n",
            "\n",
            "Epoch 00255: val_loss did not improve from 0.60545\n",
            "240/240 - 0s - loss: 0.3744 - acc: 0.8750 - val_loss: 0.6995 - val_acc: 0.7021\n",
            "Epoch 256/300\n",
            "\n",
            "Epoch 00256: val_loss did not improve from 0.60545\n",
            "240/240 - 0s - loss: 0.3696 - acc: 0.8958 - val_loss: 0.6312 - val_acc: 0.7872\n",
            "Epoch 257/300\n",
            "\n",
            "Epoch 00257: val_loss did not improve from 0.60545\n",
            "240/240 - 0s - loss: 0.3597 - acc: 0.8875 - val_loss: 0.6843 - val_acc: 0.7234\n",
            "Epoch 258/300\n",
            "\n",
            "Epoch 00258: val_loss did not improve from 0.60545\n",
            "240/240 - 0s - loss: 0.3218 - acc: 0.9167 - val_loss: 0.7311 - val_acc: 0.6596\n",
            "Epoch 259/300\n",
            "\n",
            "Epoch 00259: val_loss did not improve from 0.60545\n",
            "240/240 - 0s - loss: 0.3565 - acc: 0.8833 - val_loss: 0.7510 - val_acc: 0.7021\n",
            "Epoch 260/300\n",
            "\n",
            "Epoch 00260: val_loss did not improve from 0.60545\n",
            "240/240 - 0s - loss: 0.3459 - acc: 0.8917 - val_loss: 0.7596 - val_acc: 0.6809\n",
            "Epoch 261/300\n",
            "\n",
            "Epoch 00261: val_loss did not improve from 0.60545\n",
            "240/240 - 0s - loss: 0.3827 - acc: 0.8583 - val_loss: 0.8404 - val_acc: 0.6809\n",
            "Epoch 262/300\n",
            "\n",
            "Epoch 00262: val_loss did not improve from 0.60545\n",
            "240/240 - 0s - loss: 0.3617 - acc: 0.8917 - val_loss: 0.7590 - val_acc: 0.7234\n",
            "Epoch 263/300\n",
            "\n",
            "Epoch 00263: val_loss did not improve from 0.60545\n",
            "240/240 - 0s - loss: 0.3107 - acc: 0.9167 - val_loss: 0.6783 - val_acc: 0.7021\n",
            "Epoch 264/300\n",
            "\n",
            "Epoch 00264: val_loss did not improve from 0.60545\n",
            "240/240 - 0s - loss: 0.3272 - acc: 0.8792 - val_loss: 0.7096 - val_acc: 0.7234\n",
            "Epoch 265/300\n",
            "\n",
            "Epoch 00265: val_loss did not improve from 0.60545\n",
            "240/240 - 0s - loss: 0.3590 - acc: 0.8708 - val_loss: 0.7233 - val_acc: 0.6809\n",
            "Epoch 266/300\n",
            "\n",
            "Epoch 00266: val_loss did not improve from 0.60545\n",
            "240/240 - 0s - loss: 0.3016 - acc: 0.8917 - val_loss: 0.7296 - val_acc: 0.7234\n",
            "Epoch 267/300\n",
            "\n",
            "Epoch 00267: val_loss did not improve from 0.60545\n",
            "240/240 - 0s - loss: 0.3987 - acc: 0.8625 - val_loss: 0.7443 - val_acc: 0.7021\n",
            "Epoch 268/300\n",
            "\n",
            "Epoch 00268: val_loss did not improve from 0.60545\n",
            "240/240 - 0s - loss: 0.2675 - acc: 0.9250 - val_loss: 0.7719 - val_acc: 0.6809\n",
            "Epoch 269/300\n",
            "\n",
            "Epoch 00269: val_loss did not improve from 0.60545\n",
            "240/240 - 0s - loss: 0.3549 - acc: 0.8917 - val_loss: 0.7058 - val_acc: 0.7447\n",
            "Epoch 270/300\n",
            "\n",
            "Epoch 00270: val_loss did not improve from 0.60545\n",
            "240/240 - 0s - loss: 0.4026 - acc: 0.8542 - val_loss: 0.6951 - val_acc: 0.7447\n",
            "Epoch 271/300\n",
            "\n",
            "Epoch 00271: val_loss did not improve from 0.60545\n",
            "240/240 - 0s - loss: 0.3341 - acc: 0.8917 - val_loss: 0.7620 - val_acc: 0.6383\n",
            "Epoch 272/300\n",
            "\n",
            "Epoch 00272: val_loss did not improve from 0.60545\n",
            "240/240 - 0s - loss: 0.3194 - acc: 0.9000 - val_loss: 0.7744 - val_acc: 0.7234\n",
            "Epoch 273/300\n",
            "\n",
            "Epoch 00273: val_loss did not improve from 0.60545\n",
            "240/240 - 0s - loss: 0.3605 - acc: 0.8750 - val_loss: 0.8431 - val_acc: 0.6809\n",
            "Epoch 274/300\n",
            "\n",
            "Epoch 00274: val_loss did not improve from 0.60545\n",
            "240/240 - 0s - loss: 0.3654 - acc: 0.8833 - val_loss: 0.7130 - val_acc: 0.6809\n",
            "Epoch 275/300\n",
            "\n",
            "Epoch 00275: val_loss did not improve from 0.60545\n",
            "240/240 - 0s - loss: 0.2993 - acc: 0.9167 - val_loss: 0.6910 - val_acc: 0.7021\n",
            "Epoch 276/300\n",
            "\n",
            "Epoch 00276: val_loss did not improve from 0.60545\n",
            "240/240 - 0s - loss: 0.3348 - acc: 0.8958 - val_loss: 0.6679 - val_acc: 0.6809\n",
            "Epoch 277/300\n",
            "\n",
            "Epoch 00277: val_loss did not improve from 0.60545\n",
            "240/240 - 0s - loss: 0.3407 - acc: 0.8917 - val_loss: 0.6982 - val_acc: 0.7234\n",
            "Epoch 278/300\n",
            "\n",
            "Epoch 00278: val_loss did not improve from 0.60545\n",
            "240/240 - 0s - loss: 0.3127 - acc: 0.8917 - val_loss: 0.6975 - val_acc: 0.7660\n",
            "Epoch 279/300\n",
            "\n",
            "Epoch 00279: val_loss did not improve from 0.60545\n",
            "240/240 - 0s - loss: 0.2989 - acc: 0.9375 - val_loss: 0.7039 - val_acc: 0.7234\n",
            "Epoch 280/300\n",
            "\n",
            "Epoch 00280: val_loss did not improve from 0.60545\n",
            "240/240 - 0s - loss: 0.3465 - acc: 0.8958 - val_loss: 0.7731 - val_acc: 0.7021\n",
            "Epoch 281/300\n",
            "\n",
            "Epoch 00281: val_loss did not improve from 0.60545\n",
            "240/240 - 0s - loss: 0.2922 - acc: 0.9167 - val_loss: 0.7154 - val_acc: 0.7447\n",
            "Epoch 282/300\n",
            "\n",
            "Epoch 00282: val_loss did not improve from 0.60545\n",
            "240/240 - 0s - loss: 0.3375 - acc: 0.9042 - val_loss: 0.7757 - val_acc: 0.7021\n",
            "Epoch 283/300\n",
            "\n",
            "Epoch 00283: val_loss did not improve from 0.60545\n",
            "240/240 - 0s - loss: 0.2793 - acc: 0.9333 - val_loss: 0.7785 - val_acc: 0.6809\n",
            "Epoch 284/300\n",
            "\n",
            "Epoch 00284: val_loss did not improve from 0.60545\n",
            "240/240 - 0s - loss: 0.3657 - acc: 0.8875 - val_loss: 0.7333 - val_acc: 0.7447\n",
            "Epoch 285/300\n",
            "\n",
            "Epoch 00285: val_loss did not improve from 0.60545\n",
            "240/240 - 0s - loss: 0.3053 - acc: 0.8958 - val_loss: 0.6774 - val_acc: 0.7234\n",
            "Epoch 286/300\n",
            "\n",
            "Epoch 00286: val_loss did not improve from 0.60545\n",
            "240/240 - 0s - loss: 0.2961 - acc: 0.9333 - val_loss: 0.7055 - val_acc: 0.7234\n",
            "Epoch 287/300\n",
            "\n",
            "Epoch 00287: val_loss did not improve from 0.60545\n",
            "240/240 - 0s - loss: 0.3220 - acc: 0.9208 - val_loss: 0.7096 - val_acc: 0.7234\n",
            "Epoch 288/300\n",
            "\n",
            "Epoch 00288: val_loss did not improve from 0.60545\n",
            "240/240 - 0s - loss: 0.3615 - acc: 0.8625 - val_loss: 0.7290 - val_acc: 0.7021\n",
            "Epoch 289/300\n",
            "\n",
            "Epoch 00289: val_loss did not improve from 0.60545\n",
            "240/240 - 0s - loss: 0.3428 - acc: 0.8958 - val_loss: 0.8052 - val_acc: 0.7021\n",
            "Epoch 290/300\n",
            "\n",
            "Epoch 00290: val_loss did not improve from 0.60545\n",
            "240/240 - 0s - loss: 0.3519 - acc: 0.8833 - val_loss: 0.7508 - val_acc: 0.6809\n",
            "Epoch 291/300\n",
            "\n",
            "Epoch 00291: val_loss did not improve from 0.60545\n",
            "240/240 - 0s - loss: 0.3510 - acc: 0.9083 - val_loss: 0.7064 - val_acc: 0.7021\n",
            "Epoch 292/300\n",
            "\n",
            "Epoch 00292: val_loss did not improve from 0.60545\n",
            "240/240 - 0s - loss: 0.3347 - acc: 0.8917 - val_loss: 0.6802 - val_acc: 0.7447\n",
            "Epoch 293/300\n",
            "\n",
            "Epoch 00293: val_loss did not improve from 0.60545\n",
            "240/240 - 0s - loss: 0.2980 - acc: 0.9292 - val_loss: 0.7115 - val_acc: 0.7021\n",
            "Epoch 294/300\n",
            "\n",
            "Epoch 00294: val_loss did not improve from 0.60545\n",
            "240/240 - 0s - loss: 0.2996 - acc: 0.9125 - val_loss: 0.6375 - val_acc: 0.7660\n",
            "Epoch 295/300\n",
            "\n",
            "Epoch 00295: val_loss did not improve from 0.60545\n",
            "240/240 - 0s - loss: 0.2935 - acc: 0.9167 - val_loss: 0.6555 - val_acc: 0.7447\n",
            "Epoch 296/300\n",
            "\n",
            "Epoch 00296: val_loss did not improve from 0.60545\n",
            "240/240 - 0s - loss: 0.2966 - acc: 0.9083 - val_loss: 0.6729 - val_acc: 0.7660\n",
            "Epoch 297/300\n",
            "\n",
            "Epoch 00297: val_loss did not improve from 0.60545\n",
            "240/240 - 0s - loss: 0.3461 - acc: 0.8708 - val_loss: 0.7483 - val_acc: 0.6809\n",
            "Epoch 298/300\n",
            "\n",
            "Epoch 00298: val_loss did not improve from 0.60545\n",
            "240/240 - 0s - loss: 0.3154 - acc: 0.8875 - val_loss: 0.7802 - val_acc: 0.6596\n",
            "Epoch 299/300\n",
            "\n",
            "Epoch 00299: val_loss did not improve from 0.60545\n",
            "240/240 - 0s - loss: 0.2964 - acc: 0.9125 - val_loss: 0.6819 - val_acc: 0.7021\n",
            "Epoch 300/300\n",
            "\n",
            "Epoch 00300: val_loss did not improve from 0.60545\n",
            "240/240 - 0s - loss: 0.3365 - acc: 0.8875 - val_loss: 0.6900 - val_acc: 0.7021\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "# Evaluate on test data\n",
            "288/288 [==============================] - 1s 2ms/sample - loss: 0.6135 - acc: 0.7778\n",
            "test loss, test acc: [0.6135451559514169, 0.7777778]\n",
            "EEG_Deep/Data2A/Data_A09T.mat\n",
            "(288, 22, 1875)\n",
            "(288, 1)\n",
            "(240, 22, 750)\n",
            "(240, 1)\n",
            "(47, 22, 750)\n",
            "(47, 1)\n",
            "X_train shape: (240, 1, 22, 750)\n",
            "240 train samples\n",
            "47 val samples\n",
            "EEG_Deep/Data2A/Data_A09E.mat\n",
            "(288, 22, 1875)\n",
            "(288, 1)\n",
            "(288, 22, 750)\n",
            "(288, 1)\n",
            "X_train shape: (288, 1, 22, 750)\n",
            "288 train samples\n",
            "Train on 240 samples, validate on 47 samples\n",
            "Epoch 1/300\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 1.38843, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 1s - loss: 1.3858 - acc: 0.2625 - val_loss: 1.3884 - val_acc: 0.2979\n",
            "Epoch 2/300\n",
            "\n",
            "Epoch 00002: val_loss improved from 1.38843 to 1.38446, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.3503 - acc: 0.3833 - val_loss: 1.3845 - val_acc: 0.2340\n",
            "Epoch 3/300\n",
            "\n",
            "Epoch 00003: val_loss improved from 1.38446 to 1.38167, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.3345 - acc: 0.3750 - val_loss: 1.3817 - val_acc: 0.2553\n",
            "Epoch 4/300\n",
            "\n",
            "Epoch 00004: val_loss improved from 1.38167 to 1.37672, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.3045 - acc: 0.4792 - val_loss: 1.3767 - val_acc: 0.3404\n",
            "Epoch 5/300\n",
            "\n",
            "Epoch 00005: val_loss improved from 1.37672 to 1.36303, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.2648 - acc: 0.5542 - val_loss: 1.3630 - val_acc: 0.3191\n",
            "Epoch 6/300\n",
            "\n",
            "Epoch 00006: val_loss improved from 1.36303 to 1.34662, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.2286 - acc: 0.5375 - val_loss: 1.3466 - val_acc: 0.4043\n",
            "Epoch 7/300\n",
            "\n",
            "Epoch 00007: val_loss improved from 1.34662 to 1.32875, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.1731 - acc: 0.5875 - val_loss: 1.3287 - val_acc: 0.3830\n",
            "Epoch 8/300\n",
            "\n",
            "Epoch 00008: val_loss improved from 1.32875 to 1.28912, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.1103 - acc: 0.5792 - val_loss: 1.2891 - val_acc: 0.5106\n",
            "Epoch 9/300\n",
            "\n",
            "Epoch 00009: val_loss improved from 1.28912 to 1.26232, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.0576 - acc: 0.5875 - val_loss: 1.2623 - val_acc: 0.4255\n",
            "Epoch 10/300\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 1.26232\n",
            "240/240 - 0s - loss: 1.0364 - acc: 0.5875 - val_loss: 1.2633 - val_acc: 0.3404\n",
            "Epoch 11/300\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 1.26232\n",
            "240/240 - 0s - loss: 0.9671 - acc: 0.6375 - val_loss: 1.2735 - val_acc: 0.3191\n",
            "Epoch 12/300\n",
            "\n",
            "Epoch 00012: val_loss improved from 1.26232 to 1.25302, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.9660 - acc: 0.6083 - val_loss: 1.2530 - val_acc: 0.3404\n",
            "Epoch 13/300\n",
            "\n",
            "Epoch 00013: val_loss improved from 1.25302 to 1.25287, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.9103 - acc: 0.6875 - val_loss: 1.2529 - val_acc: 0.3830\n",
            "Epoch 14/300\n",
            "\n",
            "Epoch 00014: val_loss improved from 1.25287 to 1.18177, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.8955 - acc: 0.6792 - val_loss: 1.1818 - val_acc: 0.4681\n",
            "Epoch 15/300\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 1.18177\n",
            "240/240 - 0s - loss: 0.8480 - acc: 0.7083 - val_loss: 1.1842 - val_acc: 0.4468\n",
            "Epoch 16/300\n",
            "\n",
            "Epoch 00016: val_loss improved from 1.18177 to 1.10429, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.8488 - acc: 0.6875 - val_loss: 1.1043 - val_acc: 0.5106\n",
            "Epoch 17/300\n",
            "\n",
            "Epoch 00017: val_loss improved from 1.10429 to 1.03722, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.8145 - acc: 0.7125 - val_loss: 1.0372 - val_acc: 0.5319\n",
            "Epoch 18/300\n",
            "\n",
            "Epoch 00018: val_loss improved from 1.03722 to 1.03441, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.7907 - acc: 0.7042 - val_loss: 1.0344 - val_acc: 0.5745\n",
            "Epoch 19/300\n",
            "\n",
            "Epoch 00019: val_loss improved from 1.03441 to 0.97760, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.7470 - acc: 0.7417 - val_loss: 0.9776 - val_acc: 0.5957\n",
            "Epoch 20/300\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.97760\n",
            "240/240 - 0s - loss: 0.7610 - acc: 0.7542 - val_loss: 0.9883 - val_acc: 0.5745\n",
            "Epoch 21/300\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.97760\n",
            "240/240 - 0s - loss: 0.7419 - acc: 0.7375 - val_loss: 0.9896 - val_acc: 0.5319\n",
            "Epoch 22/300\n",
            "\n",
            "Epoch 00022: val_loss improved from 0.97760 to 0.95706, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.7335 - acc: 0.7708 - val_loss: 0.9571 - val_acc: 0.5745\n",
            "Epoch 23/300\n",
            "\n",
            "Epoch 00023: val_loss improved from 0.95706 to 0.86382, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.7411 - acc: 0.7125 - val_loss: 0.8638 - val_acc: 0.7447\n",
            "Epoch 24/300\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.86382\n",
            "240/240 - 0s - loss: 0.7094 - acc: 0.7833 - val_loss: 0.8696 - val_acc: 0.7234\n",
            "Epoch 25/300\n",
            "\n",
            "Epoch 00025: val_loss improved from 0.86382 to 0.82330, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.6859 - acc: 0.7792 - val_loss: 0.8233 - val_acc: 0.7660\n",
            "Epoch 26/300\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.82330\n",
            "240/240 - 0s - loss: 0.6754 - acc: 0.8042 - val_loss: 0.8653 - val_acc: 0.6383\n",
            "Epoch 27/300\n",
            "\n",
            "Epoch 00027: val_loss improved from 0.82330 to 0.79146, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.6258 - acc: 0.7792 - val_loss: 0.7915 - val_acc: 0.7660\n",
            "Epoch 28/300\n",
            "\n",
            "Epoch 00028: val_loss improved from 0.79146 to 0.77885, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.6387 - acc: 0.8000 - val_loss: 0.7789 - val_acc: 0.7660\n",
            "Epoch 29/300\n",
            "\n",
            "Epoch 00029: val_loss improved from 0.77885 to 0.75483, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.6442 - acc: 0.7917 - val_loss: 0.7548 - val_acc: 0.7872\n",
            "Epoch 30/300\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.75483\n",
            "240/240 - 0s - loss: 0.6204 - acc: 0.8083 - val_loss: 0.7869 - val_acc: 0.7447\n",
            "Epoch 31/300\n",
            "\n",
            "Epoch 00031: val_loss improved from 0.75483 to 0.72700, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.6415 - acc: 0.8042 - val_loss: 0.7270 - val_acc: 0.7660\n",
            "Epoch 32/300\n",
            "\n",
            "Epoch 00032: val_loss improved from 0.72700 to 0.72426, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.6420 - acc: 0.7750 - val_loss: 0.7243 - val_acc: 0.7660\n",
            "Epoch 33/300\n",
            "\n",
            "Epoch 00033: val_loss improved from 0.72426 to 0.70357, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.6070 - acc: 0.8000 - val_loss: 0.7036 - val_acc: 0.7234\n",
            "Epoch 34/300\n",
            "\n",
            "Epoch 00034: val_loss improved from 0.70357 to 0.67977, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.6034 - acc: 0.8042 - val_loss: 0.6798 - val_acc: 0.7447\n",
            "Epoch 35/300\n",
            "\n",
            "Epoch 00035: val_loss improved from 0.67977 to 0.66545, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.6085 - acc: 0.8167 - val_loss: 0.6655 - val_acc: 0.7447\n",
            "Epoch 36/300\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.66545\n",
            "240/240 - 0s - loss: 0.5821 - acc: 0.8250 - val_loss: 0.6665 - val_acc: 0.7660\n",
            "Epoch 37/300\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.66545\n",
            "240/240 - 0s - loss: 0.5787 - acc: 0.8458 - val_loss: 0.7099 - val_acc: 0.7234\n",
            "Epoch 38/300\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.66545\n",
            "240/240 - 0s - loss: 0.5533 - acc: 0.8417 - val_loss: 0.6672 - val_acc: 0.7234\n",
            "Epoch 39/300\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.66545\n",
            "240/240 - 0s - loss: 0.5670 - acc: 0.8042 - val_loss: 0.6680 - val_acc: 0.7447\n",
            "Epoch 40/300\n",
            "\n",
            "Epoch 00040: val_loss improved from 0.66545 to 0.64188, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.5165 - acc: 0.8208 - val_loss: 0.6419 - val_acc: 0.7660\n",
            "Epoch 41/300\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.64188\n",
            "240/240 - 0s - loss: 0.5823 - acc: 0.7875 - val_loss: 0.6426 - val_acc: 0.7660\n",
            "Epoch 42/300\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.64188\n",
            "240/240 - 0s - loss: 0.5685 - acc: 0.8292 - val_loss: 0.6564 - val_acc: 0.7660\n",
            "Epoch 43/300\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.64188\n",
            "240/240 - 0s - loss: 0.5645 - acc: 0.8250 - val_loss: 0.6620 - val_acc: 0.7660\n",
            "Epoch 44/300\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.64188\n",
            "240/240 - 0s - loss: 0.5407 - acc: 0.8500 - val_loss: 0.6968 - val_acc: 0.7447\n",
            "Epoch 45/300\n",
            "\n",
            "Epoch 00045: val_loss improved from 0.64188 to 0.64085, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.5271 - acc: 0.8292 - val_loss: 0.6409 - val_acc: 0.7660\n",
            "Epoch 46/300\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.64085\n",
            "240/240 - 0s - loss: 0.5391 - acc: 0.8375 - val_loss: 0.6663 - val_acc: 0.7447\n",
            "Epoch 47/300\n",
            "\n",
            "Epoch 00047: val_loss improved from 0.64085 to 0.61877, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.5390 - acc: 0.8417 - val_loss: 0.6188 - val_acc: 0.7872\n",
            "Epoch 48/300\n",
            "\n",
            "Epoch 00048: val_loss improved from 0.61877 to 0.60001, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.5288 - acc: 0.8500 - val_loss: 0.6000 - val_acc: 0.7660\n",
            "Epoch 49/300\n",
            "\n",
            "Epoch 00049: val_loss improved from 0.60001 to 0.59721, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.5056 - acc: 0.8542 - val_loss: 0.5972 - val_acc: 0.7872\n",
            "Epoch 50/300\n",
            "\n",
            "Epoch 00050: val_loss improved from 0.59721 to 0.57820, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.4937 - acc: 0.8583 - val_loss: 0.5782 - val_acc: 0.7872\n",
            "Epoch 51/300\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.57820\n",
            "240/240 - 0s - loss: 0.5166 - acc: 0.8500 - val_loss: 0.5984 - val_acc: 0.7660\n",
            "Epoch 52/300\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.57820\n",
            "240/240 - 0s - loss: 0.4989 - acc: 0.8458 - val_loss: 0.5835 - val_acc: 0.7660\n",
            "Epoch 53/300\n",
            "\n",
            "Epoch 00053: val_loss improved from 0.57820 to 0.56011, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.5077 - acc: 0.8458 - val_loss: 0.5601 - val_acc: 0.8085\n",
            "Epoch 54/300\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.56011\n",
            "240/240 - 0s - loss: 0.5039 - acc: 0.8583 - val_loss: 0.5739 - val_acc: 0.7872\n",
            "Epoch 55/300\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.56011\n",
            "240/240 - 0s - loss: 0.5246 - acc: 0.8417 - val_loss: 0.5641 - val_acc: 0.7872\n",
            "Epoch 56/300\n",
            "\n",
            "Epoch 00056: val_loss improved from 0.56011 to 0.55596, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.4845 - acc: 0.8417 - val_loss: 0.5560 - val_acc: 0.7872\n",
            "Epoch 57/300\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.55596\n",
            "240/240 - 0s - loss: 0.5091 - acc: 0.8375 - val_loss: 0.5738 - val_acc: 0.7872\n",
            "Epoch 58/300\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.55596\n",
            "240/240 - 0s - loss: 0.4873 - acc: 0.8625 - val_loss: 0.5847 - val_acc: 0.7872\n",
            "Epoch 59/300\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.55596\n",
            "240/240 - 0s - loss: 0.4679 - acc: 0.8750 - val_loss: 0.5722 - val_acc: 0.7872\n",
            "Epoch 60/300\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.55596\n",
            "240/240 - 0s - loss: 0.4650 - acc: 0.8500 - val_loss: 0.5811 - val_acc: 0.7872\n",
            "Epoch 61/300\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.55596\n",
            "240/240 - 0s - loss: 0.4854 - acc: 0.8375 - val_loss: 0.5726 - val_acc: 0.8085\n",
            "Epoch 62/300\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.55596\n",
            "240/240 - 0s - loss: 0.4903 - acc: 0.8458 - val_loss: 0.6008 - val_acc: 0.7447\n",
            "Epoch 63/300\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.55596\n",
            "240/240 - 0s - loss: 0.4765 - acc: 0.8375 - val_loss: 0.5855 - val_acc: 0.7872\n",
            "Epoch 64/300\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.55596\n",
            "240/240 - 0s - loss: 0.4479 - acc: 0.8667 - val_loss: 0.5957 - val_acc: 0.8085\n",
            "Epoch 65/300\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.55596\n",
            "240/240 - 0s - loss: 0.4779 - acc: 0.8583 - val_loss: 0.5973 - val_acc: 0.7872\n",
            "Epoch 66/300\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.55596\n",
            "240/240 - 0s - loss: 0.4666 - acc: 0.8500 - val_loss: 0.6036 - val_acc: 0.8085\n",
            "Epoch 67/300\n",
            "\n",
            "Epoch 00067: val_loss improved from 0.55596 to 0.55534, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.4548 - acc: 0.8750 - val_loss: 0.5553 - val_acc: 0.7660\n",
            "Epoch 68/300\n",
            "\n",
            "Epoch 00068: val_loss improved from 0.55534 to 0.53522, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.4446 - acc: 0.8708 - val_loss: 0.5352 - val_acc: 0.8085\n",
            "Epoch 69/300\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.53522\n",
            "240/240 - 0s - loss: 0.4323 - acc: 0.8750 - val_loss: 0.5473 - val_acc: 0.8085\n",
            "Epoch 70/300\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.53522\n",
            "240/240 - 0s - loss: 0.4797 - acc: 0.8208 - val_loss: 0.5569 - val_acc: 0.8085\n",
            "Epoch 71/300\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.53522\n",
            "240/240 - 0s - loss: 0.4489 - acc: 0.8667 - val_loss: 0.5776 - val_acc: 0.8298\n",
            "Epoch 72/300\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.53522\n",
            "240/240 - 0s - loss: 0.4334 - acc: 0.8750 - val_loss: 0.5508 - val_acc: 0.8085\n",
            "Epoch 73/300\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.53522\n",
            "240/240 - 0s - loss: 0.4225 - acc: 0.8667 - val_loss: 0.5441 - val_acc: 0.8298\n",
            "Epoch 74/300\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.53522\n",
            "240/240 - 0s - loss: 0.4398 - acc: 0.8833 - val_loss: 0.5444 - val_acc: 0.8085\n",
            "Epoch 75/300\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.53522\n",
            "240/240 - 0s - loss: 0.4178 - acc: 0.8792 - val_loss: 0.5356 - val_acc: 0.7872\n",
            "Epoch 76/300\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.53522\n",
            "240/240 - 0s - loss: 0.4253 - acc: 0.8625 - val_loss: 0.5490 - val_acc: 0.8298\n",
            "Epoch 77/300\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.53522\n",
            "240/240 - 0s - loss: 0.4320 - acc: 0.8875 - val_loss: 0.5675 - val_acc: 0.7872\n",
            "Epoch 78/300\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.53522\n",
            "240/240 - 0s - loss: 0.4324 - acc: 0.8708 - val_loss: 0.5672 - val_acc: 0.8085\n",
            "Epoch 79/300\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.53522\n",
            "240/240 - 0s - loss: 0.4035 - acc: 0.8833 - val_loss: 0.5461 - val_acc: 0.8298\n",
            "Epoch 80/300\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.53522\n",
            "240/240 - 0s - loss: 0.4239 - acc: 0.8583 - val_loss: 0.5800 - val_acc: 0.7872\n",
            "Epoch 81/300\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.53522\n",
            "240/240 - 0s - loss: 0.4346 - acc: 0.8792 - val_loss: 0.5811 - val_acc: 0.7660\n",
            "Epoch 82/300\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.53522\n",
            "240/240 - 0s - loss: 0.4156 - acc: 0.9042 - val_loss: 0.5584 - val_acc: 0.8085\n",
            "Epoch 83/300\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.53522\n",
            "240/240 - 0s - loss: 0.4160 - acc: 0.8542 - val_loss: 0.5427 - val_acc: 0.8085\n",
            "Epoch 84/300\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.53522\n",
            "240/240 - 0s - loss: 0.3975 - acc: 0.8792 - val_loss: 0.5419 - val_acc: 0.8298\n",
            "Epoch 85/300\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.53522\n",
            "240/240 - 0s - loss: 0.4488 - acc: 0.8417 - val_loss: 0.5822 - val_acc: 0.7447\n",
            "Epoch 86/300\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.53522\n",
            "240/240 - 0s - loss: 0.4084 - acc: 0.8750 - val_loss: 0.5499 - val_acc: 0.7872\n",
            "Epoch 87/300\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.53522\n",
            "240/240 - 0s - loss: 0.4328 - acc: 0.8417 - val_loss: 0.5536 - val_acc: 0.7660\n",
            "Epoch 88/300\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.53522\n",
            "240/240 - 0s - loss: 0.3890 - acc: 0.9000 - val_loss: 0.5519 - val_acc: 0.7872\n",
            "Epoch 89/300\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.53522\n",
            "240/240 - 0s - loss: 0.4203 - acc: 0.8833 - val_loss: 0.5423 - val_acc: 0.7872\n",
            "Epoch 90/300\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.53522\n",
            "240/240 - 0s - loss: 0.3982 - acc: 0.8875 - val_loss: 0.5372 - val_acc: 0.7872\n",
            "Epoch 91/300\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.53522\n",
            "240/240 - 0s - loss: 0.4040 - acc: 0.9125 - val_loss: 0.5597 - val_acc: 0.7234\n",
            "Epoch 92/300\n",
            "\n",
            "Epoch 00092: val_loss improved from 0.53522 to 0.52601, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.4213 - acc: 0.8667 - val_loss: 0.5260 - val_acc: 0.8085\n",
            "Epoch 93/300\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.52601\n",
            "240/240 - 0s - loss: 0.4083 - acc: 0.8833 - val_loss: 0.5512 - val_acc: 0.8298\n",
            "Epoch 94/300\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.52601\n",
            "240/240 - 0s - loss: 0.3961 - acc: 0.8875 - val_loss: 0.5347 - val_acc: 0.8085\n",
            "Epoch 95/300\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.52601\n",
            "240/240 - 0s - loss: 0.3827 - acc: 0.8708 - val_loss: 0.5578 - val_acc: 0.8298\n",
            "Epoch 96/300\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.52601\n",
            "240/240 - 0s - loss: 0.4244 - acc: 0.8542 - val_loss: 0.5680 - val_acc: 0.8085\n",
            "Epoch 97/300\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.52601\n",
            "240/240 - 0s - loss: 0.4114 - acc: 0.8917 - val_loss: 0.5442 - val_acc: 0.8298\n",
            "Epoch 98/300\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.52601\n",
            "240/240 - 0s - loss: 0.4342 - acc: 0.8542 - val_loss: 0.5333 - val_acc: 0.8085\n",
            "Epoch 99/300\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.52601\n",
            "240/240 - 0s - loss: 0.3961 - acc: 0.8625 - val_loss: 0.5307 - val_acc: 0.8085\n",
            "Epoch 100/300\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.52601\n",
            "240/240 - 0s - loss: 0.4272 - acc: 0.8542 - val_loss: 0.5466 - val_acc: 0.8298\n",
            "Epoch 101/300\n",
            "\n",
            "Epoch 00101: val_loss improved from 0.52601 to 0.52390, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.4084 - acc: 0.8917 - val_loss: 0.5239 - val_acc: 0.8085\n",
            "Epoch 102/300\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.52390\n",
            "240/240 - 0s - loss: 0.3613 - acc: 0.9042 - val_loss: 0.5546 - val_acc: 0.8298\n",
            "Epoch 103/300\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.52390\n",
            "240/240 - 0s - loss: 0.4256 - acc: 0.8667 - val_loss: 0.5444 - val_acc: 0.8085\n",
            "Epoch 104/300\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.52390\n",
            "240/240 - 0s - loss: 0.3556 - acc: 0.9083 - val_loss: 0.5768 - val_acc: 0.8085\n",
            "Epoch 105/300\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 0.52390\n",
            "240/240 - 0s - loss: 0.3719 - acc: 0.8875 - val_loss: 0.5259 - val_acc: 0.8298\n",
            "Epoch 106/300\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 0.52390\n",
            "240/240 - 0s - loss: 0.3863 - acc: 0.8750 - val_loss: 0.5421 - val_acc: 0.8085\n",
            "Epoch 107/300\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.52390\n",
            "240/240 - 0s - loss: 0.3728 - acc: 0.8708 - val_loss: 0.5375 - val_acc: 0.8085\n",
            "Epoch 108/300\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 0.52390\n",
            "240/240 - 0s - loss: 0.3916 - acc: 0.8792 - val_loss: 0.5384 - val_acc: 0.8085\n",
            "Epoch 109/300\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.52390\n",
            "240/240 - 0s - loss: 0.3386 - acc: 0.9125 - val_loss: 0.5385 - val_acc: 0.8085\n",
            "Epoch 110/300\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.52390\n",
            "240/240 - 0s - loss: 0.3256 - acc: 0.9333 - val_loss: 0.5287 - val_acc: 0.8298\n",
            "Epoch 111/300\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 0.52390\n",
            "240/240 - 0s - loss: 0.3825 - acc: 0.8792 - val_loss: 0.5269 - val_acc: 0.7872\n",
            "Epoch 112/300\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 0.52390\n",
            "240/240 - 0s - loss: 0.3455 - acc: 0.9167 - val_loss: 0.5534 - val_acc: 0.7447\n",
            "Epoch 113/300\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.52390\n",
            "240/240 - 0s - loss: 0.3921 - acc: 0.8917 - val_loss: 0.5410 - val_acc: 0.8298\n",
            "Epoch 114/300\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.52390\n",
            "240/240 - 0s - loss: 0.3833 - acc: 0.8667 - val_loss: 0.5620 - val_acc: 0.8085\n",
            "Epoch 115/300\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.52390\n",
            "240/240 - 0s - loss: 0.3400 - acc: 0.9125 - val_loss: 0.5787 - val_acc: 0.8298\n",
            "Epoch 116/300\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.52390\n",
            "240/240 - 0s - loss: 0.3701 - acc: 0.8917 - val_loss: 0.5541 - val_acc: 0.8085\n",
            "Epoch 117/300\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.52390\n",
            "240/240 - 0s - loss: 0.3392 - acc: 0.9250 - val_loss: 0.5641 - val_acc: 0.7872\n",
            "Epoch 118/300\n",
            "\n",
            "Epoch 00118: val_loss improved from 0.52390 to 0.51581, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.3792 - acc: 0.9042 - val_loss: 0.5158 - val_acc: 0.8298\n",
            "Epoch 119/300\n",
            "\n",
            "Epoch 00119: val_loss improved from 0.51581 to 0.50297, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.3451 - acc: 0.9000 - val_loss: 0.5030 - val_acc: 0.8085\n",
            "Epoch 120/300\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.50297\n",
            "240/240 - 0s - loss: 0.3761 - acc: 0.8750 - val_loss: 0.5458 - val_acc: 0.7872\n",
            "Epoch 121/300\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.50297\n",
            "240/240 - 0s - loss: 0.3356 - acc: 0.9125 - val_loss: 0.5622 - val_acc: 0.7872\n",
            "Epoch 122/300\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.50297\n",
            "240/240 - 0s - loss: 0.3255 - acc: 0.9083 - val_loss: 0.5060 - val_acc: 0.7872\n",
            "Epoch 123/300\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 0.50297\n",
            "240/240 - 0s - loss: 0.4012 - acc: 0.8958 - val_loss: 0.5110 - val_acc: 0.8085\n",
            "Epoch 124/300\n",
            "\n",
            "Epoch 00124: val_loss improved from 0.50297 to 0.50118, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.3892 - acc: 0.8792 - val_loss: 0.5012 - val_acc: 0.8085\n",
            "Epoch 125/300\n",
            "\n",
            "Epoch 00125: val_loss improved from 0.50118 to 0.49157, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.3476 - acc: 0.9000 - val_loss: 0.4916 - val_acc: 0.8298\n",
            "Epoch 126/300\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 0.49157\n",
            "240/240 - 0s - loss: 0.3455 - acc: 0.9167 - val_loss: 0.5574 - val_acc: 0.7872\n",
            "Epoch 127/300\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 0.49157\n",
            "240/240 - 0s - loss: 0.3286 - acc: 0.9042 - val_loss: 0.5845 - val_acc: 0.7447\n",
            "Epoch 128/300\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 0.49157\n",
            "240/240 - 0s - loss: 0.3379 - acc: 0.9000 - val_loss: 0.5219 - val_acc: 0.8085\n",
            "Epoch 129/300\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.49157\n",
            "240/240 - 0s - loss: 0.3489 - acc: 0.8875 - val_loss: 0.5447 - val_acc: 0.7660\n",
            "Epoch 130/300\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 0.49157\n",
            "240/240 - 0s - loss: 0.3474 - acc: 0.8958 - val_loss: 0.5648 - val_acc: 0.7660\n",
            "Epoch 131/300\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 0.49157\n",
            "240/240 - 0s - loss: 0.3093 - acc: 0.9125 - val_loss: 0.5165 - val_acc: 0.7660\n",
            "Epoch 132/300\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 0.49157\n",
            "240/240 - 0s - loss: 0.3113 - acc: 0.9208 - val_loss: 0.5073 - val_acc: 0.8298\n",
            "Epoch 133/300\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.49157\n",
            "240/240 - 0s - loss: 0.3632 - acc: 0.8917 - val_loss: 0.5346 - val_acc: 0.7660\n",
            "Epoch 134/300\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.49157\n",
            "240/240 - 0s - loss: 0.3785 - acc: 0.8792 - val_loss: 0.5176 - val_acc: 0.8085\n",
            "Epoch 135/300\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 0.49157\n",
            "240/240 - 0s - loss: 0.3384 - acc: 0.9000 - val_loss: 0.5181 - val_acc: 0.8298\n",
            "Epoch 136/300\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.49157\n",
            "240/240 - 0s - loss: 0.3526 - acc: 0.8875 - val_loss: 0.5084 - val_acc: 0.8298\n",
            "Epoch 137/300\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 0.49157\n",
            "240/240 - 0s - loss: 0.3152 - acc: 0.9042 - val_loss: 0.5267 - val_acc: 0.8085\n",
            "Epoch 138/300\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.49157\n",
            "240/240 - 0s - loss: 0.3375 - acc: 0.8875 - val_loss: 0.5374 - val_acc: 0.7660\n",
            "Epoch 139/300\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.49157\n",
            "240/240 - 0s - loss: 0.3223 - acc: 0.9208 - val_loss: 0.5130 - val_acc: 0.8298\n",
            "Epoch 140/300\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 0.49157\n",
            "240/240 - 0s - loss: 0.3568 - acc: 0.9042 - val_loss: 0.5070 - val_acc: 0.8298\n",
            "Epoch 141/300\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.49157\n",
            "240/240 - 0s - loss: 0.3280 - acc: 0.8958 - val_loss: 0.5470 - val_acc: 0.8085\n",
            "Epoch 142/300\n",
            "\n",
            "Epoch 00142: val_loss improved from 0.49157 to 0.48634, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.2683 - acc: 0.9250 - val_loss: 0.4863 - val_acc: 0.8298\n",
            "Epoch 143/300\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 0.48634\n",
            "240/240 - 0s - loss: 0.3473 - acc: 0.8958 - val_loss: 0.5209 - val_acc: 0.7660\n",
            "Epoch 144/300\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 0.48634\n",
            "240/240 - 0s - loss: 0.3473 - acc: 0.8792 - val_loss: 0.5583 - val_acc: 0.7872\n",
            "Epoch 145/300\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.48634\n",
            "240/240 - 0s - loss: 0.3300 - acc: 0.8917 - val_loss: 0.5896 - val_acc: 0.7660\n",
            "Epoch 146/300\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.48634\n",
            "240/240 - 0s - loss: 0.3445 - acc: 0.8958 - val_loss: 0.5517 - val_acc: 0.7660\n",
            "Epoch 147/300\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.48634\n",
            "240/240 - 0s - loss: 0.3216 - acc: 0.9167 - val_loss: 0.5059 - val_acc: 0.7660\n",
            "Epoch 148/300\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 0.48634\n",
            "240/240 - 0s - loss: 0.3344 - acc: 0.8792 - val_loss: 0.5205 - val_acc: 0.7872\n",
            "Epoch 149/300\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.48634\n",
            "240/240 - 0s - loss: 0.2912 - acc: 0.9500 - val_loss: 0.5204 - val_acc: 0.8298\n",
            "Epoch 150/300\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.48634\n",
            "240/240 - 0s - loss: 0.3489 - acc: 0.8750 - val_loss: 0.5418 - val_acc: 0.7660\n",
            "Epoch 151/300\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.48634\n",
            "240/240 - 0s - loss: 0.3042 - acc: 0.8958 - val_loss: 0.5328 - val_acc: 0.8085\n",
            "Epoch 152/300\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.48634\n",
            "240/240 - 0s - loss: 0.3093 - acc: 0.9000 - val_loss: 0.5249 - val_acc: 0.8085\n",
            "Epoch 153/300\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 0.48634\n",
            "240/240 - 0s - loss: 0.2851 - acc: 0.9125 - val_loss: 0.5261 - val_acc: 0.8298\n",
            "Epoch 154/300\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.48634\n",
            "240/240 - 0s - loss: 0.3097 - acc: 0.9042 - val_loss: 0.5121 - val_acc: 0.8298\n",
            "Epoch 155/300\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.48634\n",
            "240/240 - 0s - loss: 0.2856 - acc: 0.9417 - val_loss: 0.5791 - val_acc: 0.7660\n",
            "Epoch 156/300\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.48634\n",
            "240/240 - 0s - loss: 0.3216 - acc: 0.9083 - val_loss: 0.5208 - val_acc: 0.7872\n",
            "Epoch 157/300\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.48634\n",
            "240/240 - 0s - loss: 0.3295 - acc: 0.8875 - val_loss: 0.5288 - val_acc: 0.8085\n",
            "Epoch 158/300\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.48634\n",
            "240/240 - 0s - loss: 0.3165 - acc: 0.9042 - val_loss: 0.5443 - val_acc: 0.8298\n",
            "Epoch 159/300\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.48634\n",
            "240/240 - 0s - loss: 0.3229 - acc: 0.8958 - val_loss: 0.5044 - val_acc: 0.7872\n",
            "Epoch 160/300\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.48634\n",
            "240/240 - 0s - loss: 0.3613 - acc: 0.8792 - val_loss: 0.5196 - val_acc: 0.8085\n",
            "Epoch 161/300\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.48634\n",
            "240/240 - 0s - loss: 0.3401 - acc: 0.8958 - val_loss: 0.4953 - val_acc: 0.8298\n",
            "Epoch 162/300\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.48634\n",
            "240/240 - 0s - loss: 0.3147 - acc: 0.8958 - val_loss: 0.4953 - val_acc: 0.8298\n",
            "Epoch 163/300\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.48634\n",
            "240/240 - 0s - loss: 0.3104 - acc: 0.9125 - val_loss: 0.5379 - val_acc: 0.8085\n",
            "Epoch 164/300\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.48634\n",
            "240/240 - 0s - loss: 0.3333 - acc: 0.9000 - val_loss: 0.5120 - val_acc: 0.7660\n",
            "Epoch 165/300\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.48634\n",
            "240/240 - 0s - loss: 0.3237 - acc: 0.9042 - val_loss: 0.4985 - val_acc: 0.8085\n",
            "Epoch 166/300\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 0.48634\n",
            "240/240 - 0s - loss: 0.3624 - acc: 0.8708 - val_loss: 0.5025 - val_acc: 0.7872\n",
            "Epoch 167/300\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.48634\n",
            "240/240 - 0s - loss: 0.3435 - acc: 0.8875 - val_loss: 0.5141 - val_acc: 0.8298\n",
            "Epoch 168/300\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.48634\n",
            "240/240 - 0s - loss: 0.3347 - acc: 0.8875 - val_loss: 0.5121 - val_acc: 0.8298\n",
            "Epoch 169/300\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 0.48634\n",
            "240/240 - 0s - loss: 0.3344 - acc: 0.8917 - val_loss: 0.5333 - val_acc: 0.8298\n",
            "Epoch 170/300\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.48634\n",
            "240/240 - 0s - loss: 0.3130 - acc: 0.9042 - val_loss: 0.5450 - val_acc: 0.8298\n",
            "Epoch 171/300\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.48634\n",
            "240/240 - 0s - loss: 0.3151 - acc: 0.9208 - val_loss: 0.5573 - val_acc: 0.7872\n",
            "Epoch 172/300\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.48634\n",
            "240/240 - 0s - loss: 0.3301 - acc: 0.8792 - val_loss: 0.5287 - val_acc: 0.7872\n",
            "Epoch 173/300\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.48634\n",
            "240/240 - 0s - loss: 0.3021 - acc: 0.9167 - val_loss: 0.5274 - val_acc: 0.8085\n",
            "Epoch 174/300\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.48634\n",
            "240/240 - 0s - loss: 0.3000 - acc: 0.9083 - val_loss: 0.5191 - val_acc: 0.8085\n",
            "Epoch 175/300\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.48634\n",
            "240/240 - 0s - loss: 0.3272 - acc: 0.8958 - val_loss: 0.5364 - val_acc: 0.7660\n",
            "Epoch 176/300\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 0.48634\n",
            "240/240 - 0s - loss: 0.3023 - acc: 0.9000 - val_loss: 0.5387 - val_acc: 0.7872\n",
            "Epoch 177/300\n",
            "\n",
            "Epoch 00177: val_loss improved from 0.48634 to 0.47993, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.2776 - acc: 0.9250 - val_loss: 0.4799 - val_acc: 0.8085\n",
            "Epoch 178/300\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.47993\n",
            "240/240 - 0s - loss: 0.2846 - acc: 0.9458 - val_loss: 0.5242 - val_acc: 0.7447\n",
            "Epoch 179/300\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.47993\n",
            "240/240 - 0s - loss: 0.2767 - acc: 0.9208 - val_loss: 0.5581 - val_acc: 0.7660\n",
            "Epoch 180/300\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.47993\n",
            "240/240 - 0s - loss: 0.2861 - acc: 0.9125 - val_loss: 0.5302 - val_acc: 0.8085\n",
            "Epoch 181/300\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.47993\n",
            "240/240 - 0s - loss: 0.2941 - acc: 0.9125 - val_loss: 0.5135 - val_acc: 0.8085\n",
            "Epoch 182/300\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.47993\n",
            "240/240 - 0s - loss: 0.3031 - acc: 0.9000 - val_loss: 0.5608 - val_acc: 0.7660\n",
            "Epoch 183/300\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.47993\n",
            "240/240 - 0s - loss: 0.2783 - acc: 0.9333 - val_loss: 0.5269 - val_acc: 0.7872\n",
            "Epoch 184/300\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 0.47993\n",
            "240/240 - 0s - loss: 0.3166 - acc: 0.9042 - val_loss: 0.5498 - val_acc: 0.7872\n",
            "Epoch 185/300\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.47993\n",
            "240/240 - 0s - loss: 0.3222 - acc: 0.8917 - val_loss: 0.5228 - val_acc: 0.8085\n",
            "Epoch 186/300\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.47993\n",
            "240/240 - 0s - loss: 0.2863 - acc: 0.9125 - val_loss: 0.5401 - val_acc: 0.7872\n",
            "Epoch 187/300\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 0.47993\n",
            "240/240 - 0s - loss: 0.2919 - acc: 0.8958 - val_loss: 0.5110 - val_acc: 0.8298\n",
            "Epoch 188/300\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.47993\n",
            "240/240 - 0s - loss: 0.3273 - acc: 0.9083 - val_loss: 0.5095 - val_acc: 0.8085\n",
            "Epoch 189/300\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.47993\n",
            "240/240 - 0s - loss: 0.2963 - acc: 0.9167 - val_loss: 0.5483 - val_acc: 0.7872\n",
            "Epoch 190/300\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.47993\n",
            "240/240 - 0s - loss: 0.3091 - acc: 0.9042 - val_loss: 0.5025 - val_acc: 0.7660\n",
            "Epoch 191/300\n",
            "\n",
            "Epoch 00191: val_loss improved from 0.47993 to 0.47567, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.2752 - acc: 0.9208 - val_loss: 0.4757 - val_acc: 0.8298\n",
            "Epoch 192/300\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 0.47567\n",
            "240/240 - 0s - loss: 0.2886 - acc: 0.9125 - val_loss: 0.4851 - val_acc: 0.7872\n",
            "Epoch 193/300\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.47567\n",
            "240/240 - 0s - loss: 0.3037 - acc: 0.9042 - val_loss: 0.5386 - val_acc: 0.7872\n",
            "Epoch 194/300\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.47567\n",
            "240/240 - 0s - loss: 0.2672 - acc: 0.9167 - val_loss: 0.5813 - val_acc: 0.7872\n",
            "Epoch 195/300\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.47567\n",
            "240/240 - 0s - loss: 0.3122 - acc: 0.9167 - val_loss: 0.5401 - val_acc: 0.8298\n",
            "Epoch 196/300\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.47567\n",
            "240/240 - 0s - loss: 0.2978 - acc: 0.8833 - val_loss: 0.5220 - val_acc: 0.7660\n",
            "Epoch 197/300\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.47567\n",
            "240/240 - 0s - loss: 0.2744 - acc: 0.9292 - val_loss: 0.5116 - val_acc: 0.8085\n",
            "Epoch 198/300\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.47567\n",
            "240/240 - 0s - loss: 0.2706 - acc: 0.9208 - val_loss: 0.5174 - val_acc: 0.8298\n",
            "Epoch 199/300\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.47567\n",
            "240/240 - 0s - loss: 0.2573 - acc: 0.9250 - val_loss: 0.5326 - val_acc: 0.8085\n",
            "Epoch 200/300\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.47567\n",
            "240/240 - 0s - loss: 0.2995 - acc: 0.9125 - val_loss: 0.5162 - val_acc: 0.7660\n",
            "Epoch 201/300\n",
            "\n",
            "Epoch 00201: val_loss did not improve from 0.47567\n",
            "240/240 - 0s - loss: 0.2388 - acc: 0.9333 - val_loss: 0.5274 - val_acc: 0.7660\n",
            "Epoch 202/300\n",
            "\n",
            "Epoch 00202: val_loss did not improve from 0.47567\n",
            "240/240 - 0s - loss: 0.3234 - acc: 0.8875 - val_loss: 0.5606 - val_acc: 0.7660\n",
            "Epoch 203/300\n",
            "\n",
            "Epoch 00203: val_loss did not improve from 0.47567\n",
            "240/240 - 0s - loss: 0.2660 - acc: 0.9167 - val_loss: 0.5406 - val_acc: 0.7872\n",
            "Epoch 204/300\n",
            "\n",
            "Epoch 00204: val_loss did not improve from 0.47567\n",
            "240/240 - 0s - loss: 0.2858 - acc: 0.9208 - val_loss: 0.5149 - val_acc: 0.7872\n",
            "Epoch 205/300\n",
            "\n",
            "Epoch 00205: val_loss did not improve from 0.47567\n",
            "240/240 - 0s - loss: 0.2716 - acc: 0.9208 - val_loss: 0.5439 - val_acc: 0.7872\n",
            "Epoch 206/300\n",
            "\n",
            "Epoch 00206: val_loss did not improve from 0.47567\n",
            "240/240 - 0s - loss: 0.2546 - acc: 0.9375 - val_loss: 0.5237 - val_acc: 0.8085\n",
            "Epoch 207/300\n",
            "\n",
            "Epoch 00207: val_loss did not improve from 0.47567\n",
            "240/240 - 0s - loss: 0.2585 - acc: 0.9208 - val_loss: 0.5217 - val_acc: 0.7872\n",
            "Epoch 208/300\n",
            "\n",
            "Epoch 00208: val_loss did not improve from 0.47567\n",
            "240/240 - 0s - loss: 0.2851 - acc: 0.9250 - val_loss: 0.5605 - val_acc: 0.8085\n",
            "Epoch 209/300\n",
            "\n",
            "Epoch 00209: val_loss did not improve from 0.47567\n",
            "240/240 - 0s - loss: 0.2366 - acc: 0.9333 - val_loss: 0.5650 - val_acc: 0.8085\n",
            "Epoch 210/300\n",
            "\n",
            "Epoch 00210: val_loss did not improve from 0.47567\n",
            "240/240 - 0s - loss: 0.2654 - acc: 0.9250 - val_loss: 0.5678 - val_acc: 0.8085\n",
            "Epoch 211/300\n",
            "\n",
            "Epoch 00211: val_loss did not improve from 0.47567\n",
            "240/240 - 0s - loss: 0.2846 - acc: 0.9208 - val_loss: 0.6110 - val_acc: 0.7872\n",
            "Epoch 212/300\n",
            "\n",
            "Epoch 00212: val_loss did not improve from 0.47567\n",
            "240/240 - 0s - loss: 0.2405 - acc: 0.9333 - val_loss: 0.6084 - val_acc: 0.7872\n",
            "Epoch 213/300\n",
            "\n",
            "Epoch 00213: val_loss did not improve from 0.47567\n",
            "240/240 - 0s - loss: 0.2677 - acc: 0.9167 - val_loss: 0.5614 - val_acc: 0.7872\n",
            "Epoch 214/300\n",
            "\n",
            "Epoch 00214: val_loss did not improve from 0.47567\n",
            "240/240 - 0s - loss: 0.2825 - acc: 0.8917 - val_loss: 0.5441 - val_acc: 0.8085\n",
            "Epoch 215/300\n",
            "\n",
            "Epoch 00215: val_loss did not improve from 0.47567\n",
            "240/240 - 0s - loss: 0.2960 - acc: 0.9042 - val_loss: 0.5644 - val_acc: 0.7872\n",
            "Epoch 216/300\n",
            "\n",
            "Epoch 00216: val_loss did not improve from 0.47567\n",
            "240/240 - 0s - loss: 0.2600 - acc: 0.9292 - val_loss: 0.5522 - val_acc: 0.8085\n",
            "Epoch 217/300\n",
            "\n",
            "Epoch 00217: val_loss did not improve from 0.47567\n",
            "240/240 - 0s - loss: 0.2478 - acc: 0.9458 - val_loss: 0.5125 - val_acc: 0.7447\n",
            "Epoch 218/300\n",
            "\n",
            "Epoch 00218: val_loss did not improve from 0.47567\n",
            "240/240 - 0s - loss: 0.2112 - acc: 0.9625 - val_loss: 0.5174 - val_acc: 0.8085\n",
            "Epoch 219/300\n",
            "\n",
            "Epoch 00219: val_loss did not improve from 0.47567\n",
            "240/240 - 0s - loss: 0.2840 - acc: 0.8917 - val_loss: 0.5411 - val_acc: 0.7872\n",
            "Epoch 220/300\n",
            "\n",
            "Epoch 00220: val_loss did not improve from 0.47567\n",
            "240/240 - 0s - loss: 0.2350 - acc: 0.9500 - val_loss: 0.5118 - val_acc: 0.8298\n",
            "Epoch 221/300\n",
            "\n",
            "Epoch 00221: val_loss did not improve from 0.47567\n",
            "240/240 - 0s - loss: 0.2929 - acc: 0.8917 - val_loss: 0.5223 - val_acc: 0.8085\n",
            "Epoch 222/300\n",
            "\n",
            "Epoch 00222: val_loss did not improve from 0.47567\n",
            "240/240 - 0s - loss: 0.2730 - acc: 0.9208 - val_loss: 0.5160 - val_acc: 0.7872\n",
            "Epoch 223/300\n",
            "\n",
            "Epoch 00223: val_loss did not improve from 0.47567\n",
            "240/240 - 0s - loss: 0.2685 - acc: 0.8958 - val_loss: 0.5477 - val_acc: 0.8298\n",
            "Epoch 224/300\n",
            "\n",
            "Epoch 00224: val_loss did not improve from 0.47567\n",
            "240/240 - 0s - loss: 0.2769 - acc: 0.9208 - val_loss: 0.5679 - val_acc: 0.7234\n",
            "Epoch 225/300\n",
            "\n",
            "Epoch 00225: val_loss did not improve from 0.47567\n",
            "240/240 - 0s - loss: 0.2673 - acc: 0.9167 - val_loss: 0.5875 - val_acc: 0.7660\n",
            "Epoch 226/300\n",
            "\n",
            "Epoch 00226: val_loss did not improve from 0.47567\n",
            "240/240 - 0s - loss: 0.2535 - acc: 0.9417 - val_loss: 0.5656 - val_acc: 0.7660\n",
            "Epoch 227/300\n",
            "\n",
            "Epoch 00227: val_loss did not improve from 0.47567\n",
            "240/240 - 0s - loss: 0.2786 - acc: 0.9375 - val_loss: 0.5214 - val_acc: 0.8298\n",
            "Epoch 228/300\n",
            "\n",
            "Epoch 00228: val_loss did not improve from 0.47567\n",
            "240/240 - 0s - loss: 0.2729 - acc: 0.9042 - val_loss: 0.4947 - val_acc: 0.7660\n",
            "Epoch 229/300\n",
            "\n",
            "Epoch 00229: val_loss did not improve from 0.47567\n",
            "240/240 - 0s - loss: 0.2511 - acc: 0.9250 - val_loss: 0.5387 - val_acc: 0.7660\n",
            "Epoch 230/300\n",
            "\n",
            "Epoch 00230: val_loss did not improve from 0.47567\n",
            "240/240 - 0s - loss: 0.2344 - acc: 0.9208 - val_loss: 0.5140 - val_acc: 0.8085\n",
            "Epoch 231/300\n",
            "\n",
            "Epoch 00231: val_loss did not improve from 0.47567\n",
            "240/240 - 0s - loss: 0.2454 - acc: 0.9208 - val_loss: 0.4900 - val_acc: 0.8298\n",
            "Epoch 232/300\n",
            "\n",
            "Epoch 00232: val_loss did not improve from 0.47567\n",
            "240/240 - 0s - loss: 0.2217 - acc: 0.9333 - val_loss: 0.5248 - val_acc: 0.8298\n",
            "Epoch 233/300\n",
            "\n",
            "Epoch 00233: val_loss did not improve from 0.47567\n",
            "240/240 - 0s - loss: 0.2791 - acc: 0.9083 - val_loss: 0.5349 - val_acc: 0.7872\n",
            "Epoch 234/300\n",
            "\n",
            "Epoch 00234: val_loss did not improve from 0.47567\n",
            "240/240 - 0s - loss: 0.2567 - acc: 0.9250 - val_loss: 0.5183 - val_acc: 0.7872\n",
            "Epoch 235/300\n",
            "\n",
            "Epoch 00235: val_loss did not improve from 0.47567\n",
            "240/240 - 0s - loss: 0.2775 - acc: 0.9000 - val_loss: 0.5024 - val_acc: 0.7872\n",
            "Epoch 236/300\n",
            "\n",
            "Epoch 00236: val_loss did not improve from 0.47567\n",
            "240/240 - 0s - loss: 0.2397 - acc: 0.9458 - val_loss: 0.5231 - val_acc: 0.8085\n",
            "Epoch 237/300\n",
            "\n",
            "Epoch 00237: val_loss did not improve from 0.47567\n",
            "240/240 - 0s - loss: 0.2640 - acc: 0.9125 - val_loss: 0.5756 - val_acc: 0.7872\n",
            "Epoch 238/300\n",
            "\n",
            "Epoch 00238: val_loss did not improve from 0.47567\n",
            "240/240 - 0s - loss: 0.2749 - acc: 0.9083 - val_loss: 0.5363 - val_acc: 0.8085\n",
            "Epoch 239/300\n",
            "\n",
            "Epoch 00239: val_loss did not improve from 0.47567\n",
            "240/240 - 0s - loss: 0.2902 - acc: 0.9208 - val_loss: 0.5218 - val_acc: 0.8085\n",
            "Epoch 240/300\n",
            "\n",
            "Epoch 00240: val_loss did not improve from 0.47567\n",
            "240/240 - 0s - loss: 0.3115 - acc: 0.8917 - val_loss: 0.5937 - val_acc: 0.7872\n",
            "Epoch 241/300\n",
            "\n",
            "Epoch 00241: val_loss did not improve from 0.47567\n",
            "240/240 - 0s - loss: 0.2616 - acc: 0.9208 - val_loss: 0.5238 - val_acc: 0.8085\n",
            "Epoch 242/300\n",
            "\n",
            "Epoch 00242: val_loss did not improve from 0.47567\n",
            "240/240 - 0s - loss: 0.2530 - acc: 0.9333 - val_loss: 0.5249 - val_acc: 0.7872\n",
            "Epoch 243/300\n",
            "\n",
            "Epoch 00243: val_loss improved from 0.47567 to 0.44818, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.2632 - acc: 0.9375 - val_loss: 0.4482 - val_acc: 0.7872\n",
            "Epoch 244/300\n",
            "\n",
            "Epoch 00244: val_loss did not improve from 0.44818\n",
            "240/240 - 0s - loss: 0.2700 - acc: 0.9125 - val_loss: 0.5021 - val_acc: 0.7660\n",
            "Epoch 245/300\n",
            "\n",
            "Epoch 00245: val_loss did not improve from 0.44818\n",
            "240/240 - 0s - loss: 0.2531 - acc: 0.9250 - val_loss: 0.5249 - val_acc: 0.7660\n",
            "Epoch 246/300\n",
            "\n",
            "Epoch 00246: val_loss did not improve from 0.44818\n",
            "240/240 - 0s - loss: 0.2374 - acc: 0.9250 - val_loss: 0.5061 - val_acc: 0.7660\n",
            "Epoch 247/300\n",
            "\n",
            "Epoch 00247: val_loss did not improve from 0.44818\n",
            "240/240 - 0s - loss: 0.2209 - acc: 0.9417 - val_loss: 0.5103 - val_acc: 0.8298\n",
            "Epoch 248/300\n",
            "\n",
            "Epoch 00248: val_loss did not improve from 0.44818\n",
            "240/240 - 0s - loss: 0.2353 - acc: 0.9375 - val_loss: 0.5202 - val_acc: 0.7872\n",
            "Epoch 249/300\n",
            "\n",
            "Epoch 00249: val_loss did not improve from 0.44818\n",
            "240/240 - 0s - loss: 0.2974 - acc: 0.8958 - val_loss: 0.5612 - val_acc: 0.7872\n",
            "Epoch 250/300\n",
            "\n",
            "Epoch 00250: val_loss did not improve from 0.44818\n",
            "240/240 - 0s - loss: 0.2319 - acc: 0.9250 - val_loss: 0.5774 - val_acc: 0.7660\n",
            "Epoch 251/300\n",
            "\n",
            "Epoch 00251: val_loss did not improve from 0.44818\n",
            "240/240 - 0s - loss: 0.2387 - acc: 0.9292 - val_loss: 0.5155 - val_acc: 0.8298\n",
            "Epoch 252/300\n",
            "\n",
            "Epoch 00252: val_loss did not improve from 0.44818\n",
            "240/240 - 0s - loss: 0.2368 - acc: 0.9208 - val_loss: 0.4939 - val_acc: 0.7234\n",
            "Epoch 253/300\n",
            "\n",
            "Epoch 00253: val_loss did not improve from 0.44818\n",
            "240/240 - 0s - loss: 0.2217 - acc: 0.9417 - val_loss: 0.5162 - val_acc: 0.7872\n",
            "Epoch 254/300\n",
            "\n",
            "Epoch 00254: val_loss did not improve from 0.44818\n",
            "240/240 - 0s - loss: 0.2778 - acc: 0.9208 - val_loss: 0.5027 - val_acc: 0.7660\n",
            "Epoch 255/300\n",
            "\n",
            "Epoch 00255: val_loss did not improve from 0.44818\n",
            "240/240 - 0s - loss: 0.2320 - acc: 0.9292 - val_loss: 0.5843 - val_acc: 0.7660\n",
            "Epoch 256/300\n",
            "\n",
            "Epoch 00256: val_loss did not improve from 0.44818\n",
            "240/240 - 0s - loss: 0.2442 - acc: 0.9250 - val_loss: 0.5273 - val_acc: 0.7872\n",
            "Epoch 257/300\n",
            "\n",
            "Epoch 00257: val_loss did not improve from 0.44818\n",
            "240/240 - 0s - loss: 0.2380 - acc: 0.9375 - val_loss: 0.5155 - val_acc: 0.8298\n",
            "Epoch 258/300\n",
            "\n",
            "Epoch 00258: val_loss did not improve from 0.44818\n",
            "240/240 - 0s - loss: 0.2581 - acc: 0.9042 - val_loss: 0.5339 - val_acc: 0.7660\n",
            "Epoch 259/300\n",
            "\n",
            "Epoch 00259: val_loss did not improve from 0.44818\n",
            "240/240 - 0s - loss: 0.2424 - acc: 0.9250 - val_loss: 0.5263 - val_acc: 0.8085\n",
            "Epoch 260/300\n",
            "\n",
            "Epoch 00260: val_loss did not improve from 0.44818\n",
            "240/240 - 0s - loss: 0.2301 - acc: 0.9375 - val_loss: 0.4768 - val_acc: 0.8085\n",
            "Epoch 261/300\n",
            "\n",
            "Epoch 00261: val_loss did not improve from 0.44818\n",
            "240/240 - 0s - loss: 0.2053 - acc: 0.9583 - val_loss: 0.5607 - val_acc: 0.8085\n",
            "Epoch 262/300\n",
            "\n",
            "Epoch 00262: val_loss did not improve from 0.44818\n",
            "240/240 - 0s - loss: 0.2433 - acc: 0.9333 - val_loss: 0.5271 - val_acc: 0.8085\n",
            "Epoch 263/300\n",
            "\n",
            "Epoch 00263: val_loss did not improve from 0.44818\n",
            "240/240 - 0s - loss: 0.2624 - acc: 0.9125 - val_loss: 0.5132 - val_acc: 0.8085\n",
            "Epoch 264/300\n",
            "\n",
            "Epoch 00264: val_loss did not improve from 0.44818\n",
            "240/240 - 0s - loss: 0.2193 - acc: 0.9417 - val_loss: 0.5295 - val_acc: 0.8085\n",
            "Epoch 265/300\n",
            "\n",
            "Epoch 00265: val_loss did not improve from 0.44818\n",
            "240/240 - 0s - loss: 0.2487 - acc: 0.9292 - val_loss: 0.5709 - val_acc: 0.7872\n",
            "Epoch 266/300\n",
            "\n",
            "Epoch 00266: val_loss did not improve from 0.44818\n",
            "240/240 - 0s - loss: 0.2717 - acc: 0.9292 - val_loss: 0.4918 - val_acc: 0.7872\n",
            "Epoch 267/300\n",
            "\n",
            "Epoch 00267: val_loss did not improve from 0.44818\n",
            "240/240 - 0s - loss: 0.2644 - acc: 0.9208 - val_loss: 0.4950 - val_acc: 0.8085\n",
            "Epoch 268/300\n",
            "\n",
            "Epoch 00268: val_loss did not improve from 0.44818\n",
            "240/240 - 0s - loss: 0.2484 - acc: 0.9125 - val_loss: 0.4725 - val_acc: 0.7872\n",
            "Epoch 269/300\n",
            "\n",
            "Epoch 00269: val_loss did not improve from 0.44818\n",
            "240/240 - 0s - loss: 0.2379 - acc: 0.9292 - val_loss: 0.5116 - val_acc: 0.7872\n",
            "Epoch 270/300\n",
            "\n",
            "Epoch 00270: val_loss did not improve from 0.44818\n",
            "240/240 - 0s - loss: 0.2179 - acc: 0.9417 - val_loss: 0.5404 - val_acc: 0.7872\n",
            "Epoch 271/300\n",
            "\n",
            "Epoch 00271: val_loss did not improve from 0.44818\n",
            "240/240 - 0s - loss: 0.2476 - acc: 0.9333 - val_loss: 0.5569 - val_acc: 0.7872\n",
            "Epoch 272/300\n",
            "\n",
            "Epoch 00272: val_loss did not improve from 0.44818\n",
            "240/240 - 0s - loss: 0.2206 - acc: 0.9458 - val_loss: 0.5594 - val_acc: 0.7660\n",
            "Epoch 273/300\n",
            "\n",
            "Epoch 00273: val_loss did not improve from 0.44818\n",
            "240/240 - 0s - loss: 0.2528 - acc: 0.9250 - val_loss: 0.5758 - val_acc: 0.7660\n",
            "Epoch 274/300\n",
            "\n",
            "Epoch 00274: val_loss did not improve from 0.44818\n",
            "240/240 - 0s - loss: 0.2029 - acc: 0.9500 - val_loss: 0.6347 - val_acc: 0.7872\n",
            "Epoch 275/300\n",
            "\n",
            "Epoch 00275: val_loss did not improve from 0.44818\n",
            "240/240 - 0s - loss: 0.2482 - acc: 0.9208 - val_loss: 0.5338 - val_acc: 0.7872\n",
            "Epoch 276/300\n",
            "\n",
            "Epoch 00276: val_loss did not improve from 0.44818\n",
            "240/240 - 0s - loss: 0.2321 - acc: 0.9250 - val_loss: 0.5374 - val_acc: 0.7660\n",
            "Epoch 277/300\n",
            "\n",
            "Epoch 00277: val_loss did not improve from 0.44818\n",
            "240/240 - 0s - loss: 0.2093 - acc: 0.9417 - val_loss: 0.5946 - val_acc: 0.7872\n",
            "Epoch 278/300\n",
            "\n",
            "Epoch 00278: val_loss did not improve from 0.44818\n",
            "240/240 - 0s - loss: 0.2410 - acc: 0.9458 - val_loss: 0.5875 - val_acc: 0.7660\n",
            "Epoch 279/300\n",
            "\n",
            "Epoch 00279: val_loss did not improve from 0.44818\n",
            "240/240 - 0s - loss: 0.2298 - acc: 0.9333 - val_loss: 0.5636 - val_acc: 0.7872\n",
            "Epoch 280/300\n",
            "\n",
            "Epoch 00280: val_loss did not improve from 0.44818\n",
            "240/240 - 0s - loss: 0.2241 - acc: 0.9417 - val_loss: 0.5405 - val_acc: 0.7872\n",
            "Epoch 281/300\n",
            "\n",
            "Epoch 00281: val_loss did not improve from 0.44818\n",
            "240/240 - 0s - loss: 0.2312 - acc: 0.9417 - val_loss: 0.5560 - val_acc: 0.8085\n",
            "Epoch 282/300\n",
            "\n",
            "Epoch 00282: val_loss did not improve from 0.44818\n",
            "240/240 - 0s - loss: 0.2313 - acc: 0.9375 - val_loss: 0.5482 - val_acc: 0.7660\n",
            "Epoch 283/300\n",
            "\n",
            "Epoch 00283: val_loss did not improve from 0.44818\n",
            "240/240 - 0s - loss: 0.2791 - acc: 0.8958 - val_loss: 0.5859 - val_acc: 0.8298\n",
            "Epoch 284/300\n",
            "\n",
            "Epoch 00284: val_loss did not improve from 0.44818\n",
            "240/240 - 0s - loss: 0.2445 - acc: 0.9292 - val_loss: 0.5359 - val_acc: 0.8085\n",
            "Epoch 285/300\n",
            "\n",
            "Epoch 00285: val_loss did not improve from 0.44818\n",
            "240/240 - 0s - loss: 0.2722 - acc: 0.9125 - val_loss: 0.4956 - val_acc: 0.8085\n",
            "Epoch 286/300\n",
            "\n",
            "Epoch 00286: val_loss did not improve from 0.44818\n",
            "240/240 - 0s - loss: 0.2446 - acc: 0.9167 - val_loss: 0.5232 - val_acc: 0.8085\n",
            "Epoch 287/300\n",
            "\n",
            "Epoch 00287: val_loss did not improve from 0.44818\n",
            "240/240 - 0s - loss: 0.2045 - acc: 0.9583 - val_loss: 0.4767 - val_acc: 0.8298\n",
            "Epoch 288/300\n",
            "\n",
            "Epoch 00288: val_loss did not improve from 0.44818\n",
            "240/240 - 0s - loss: 0.2574 - acc: 0.9250 - val_loss: 0.5087 - val_acc: 0.8298\n",
            "Epoch 289/300\n",
            "\n",
            "Epoch 00289: val_loss did not improve from 0.44818\n",
            "240/240 - 0s - loss: 0.2076 - acc: 0.9250 - val_loss: 0.5070 - val_acc: 0.7872\n",
            "Epoch 290/300\n",
            "\n",
            "Epoch 00290: val_loss did not improve from 0.44818\n",
            "240/240 - 0s - loss: 0.2723 - acc: 0.9000 - val_loss: 0.5127 - val_acc: 0.8085\n",
            "Epoch 291/300\n",
            "\n",
            "Epoch 00291: val_loss improved from 0.44818 to 0.44747, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.2026 - acc: 0.9583 - val_loss: 0.4475 - val_acc: 0.7872\n",
            "Epoch 292/300\n",
            "\n",
            "Epoch 00292: val_loss did not improve from 0.44747\n",
            "240/240 - 0s - loss: 0.2480 - acc: 0.9250 - val_loss: 0.4893 - val_acc: 0.8085\n",
            "Epoch 293/300\n",
            "\n",
            "Epoch 00293: val_loss did not improve from 0.44747\n",
            "240/240 - 0s - loss: 0.2316 - acc: 0.9375 - val_loss: 0.4930 - val_acc: 0.7872\n",
            "Epoch 294/300\n",
            "\n",
            "Epoch 00294: val_loss did not improve from 0.44747\n",
            "240/240 - 0s - loss: 0.2272 - acc: 0.9250 - val_loss: 0.4810 - val_acc: 0.8085\n",
            "Epoch 295/300\n",
            "\n",
            "Epoch 00295: val_loss did not improve from 0.44747\n",
            "240/240 - 0s - loss: 0.2524 - acc: 0.9333 - val_loss: 0.5160 - val_acc: 0.8298\n",
            "Epoch 296/300\n",
            "\n",
            "Epoch 00296: val_loss did not improve from 0.44747\n",
            "240/240 - 0s - loss: 0.2134 - acc: 0.9292 - val_loss: 0.5023 - val_acc: 0.8085\n",
            "Epoch 297/300\n",
            "\n",
            "Epoch 00297: val_loss did not improve from 0.44747\n",
            "240/240 - 0s - loss: 0.1903 - acc: 0.9542 - val_loss: 0.4929 - val_acc: 0.8298\n",
            "Epoch 298/300\n",
            "\n",
            "Epoch 00298: val_loss did not improve from 0.44747\n",
            "240/240 - 0s - loss: 0.2103 - acc: 0.9417 - val_loss: 0.5223 - val_acc: 0.8085\n",
            "Epoch 299/300\n",
            "\n",
            "Epoch 00299: val_loss did not improve from 0.44747\n",
            "240/240 - 0s - loss: 0.2216 - acc: 0.9417 - val_loss: 0.5213 - val_acc: 0.8085\n",
            "Epoch 300/300\n",
            "\n",
            "Epoch 00300: val_loss did not improve from 0.44747\n",
            "240/240 - 0s - loss: 0.2122 - acc: 0.9417 - val_loss: 0.4876 - val_acc: 0.7660\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\np0QgIpJxSgQiIhmnRCAiknFKBCIiGff/xetUH8pHIhgAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "# Evaluate on test data\n",
            "288/288 [==============================] - 1s 2ms/sample - loss: 0.4885 - acc: 0.8056\n",
            "test loss, test acc: [0.48848120658289595, 0.8055556]\n",
            "[[0.59980498 0.75347221]\n",
            " [1.17749401 0.47916666]\n",
            " [0.49895159 0.78472221]\n",
            " [0.91655367 0.62847221]\n",
            " [1.26050263 0.46527779]\n",
            " [1.25522248 0.4826389 ]\n",
            " [0.4749108  0.8263889 ]\n",
            " [0.61354516 0.77777779]\n",
            " [0.48848121 0.80555558]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AST-KRZ7WXEX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "df_accl_all = pd.DataFrame({'Loss ': 100*acc_all[:, 1], 'Eval Acc': 100*acc_all[:, 1]})\n",
        "df_accl_all.to_csv (r'df_accl_8_12_Hz4_Class.csv', index = None, header=True) #Don't forget to add '.csv' at the end of the path"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}