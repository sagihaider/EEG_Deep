{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "main_Single_Trial_Classification_Accuracy_all_subjects_FilteredData",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sagihaider/EEG_Deep/blob/master/main_Single_Trial_Classification_Accuracy_all_subjects_FilteredData.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X110PE9MjrlE",
        "colab_type": "code",
        "outputId": "1949e066-6836-40bd-ba10-b474188a10fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "!git clone https://github.com/sagihaider/EEG_Deep.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'EEG_Deep'...\n",
            "remote: Enumerating objects: 19, done.\u001b[K\n",
            "remote: Counting objects: 100% (19/19), done.\u001b[K\n",
            "remote: Compressing objects: 100% (19/19), done.\u001b[K\n",
            "remote: Total 266 (delta 9), reused 0 (delta 0), pack-reused 247\u001b[K\n",
            "Receiving objects: 100% (266/266), 1.07 GiB | 39.07 MiB/s, done.\n",
            "Resolving deltas: 100% (113/113), done.\n",
            "Checking out files: 100% (72/72), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ioLzMoiS60Qf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 63
        },
        "outputId": "8f65c462-3e82-42a0-b68e-a635564aa25e"
      },
      "source": [
        "import scipy.io as spio\n",
        "import numpy as np\n",
        "from importlib.machinery import SourceFileLoader\n",
        "\n",
        "# EEGNet-specific imports\n",
        "from EEG_Deep.EEGModels import EEGNet, ShallowConvNet, DeepConvNet\n",
        "from tensorflow.keras import utils as np_utils\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras import backend as K\n",
        "K.set_image_data_format('channels_first')\n",
        "\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# tools for plotting confusion matrices\n",
        "from matplotlib import pyplot as plt\n",
        "from scipy.signal import butter, lfilter"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mo80Jg_Pn5lP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Band-pass Filter\n",
        "def butter_bandpass_filter(data, lowcut, highcut, fs, order=4):\n",
        "    nyq = 0.5 * fs\n",
        "    low = lowcut / nyq\n",
        "    high = highcut / nyq\n",
        "    b, a = butter(order, [low, high], btype='band')\n",
        "    y = lfilter(b, a, data)\n",
        "    return y\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZjtnE_6RSpum",
        "colab_type": "code",
        "outputId": "5ef5af48-3824-4428-d815-b35a391d75b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from numpy import zeros\n",
        "K.clear_session()\n",
        "cols = 2\n",
        "rows = 9\n",
        "acc_all = zeros([rows, cols])\n",
        "X_tr = np.empty([288, 22, 1875])\n",
        "X_ts = np.empty([288, 22, 1875])\n",
        "\n",
        "\n",
        "for x in range(1,10):\n",
        "  fName = 'EEG_Deep/Data2A/Data_A0' + str(x) + 'T.mat'  # Load Data\n",
        "  print(fName)\n",
        "  mat = spio.loadmat(fName)\n",
        "  r_X_tr = mat['cleanRawEEGData']\n",
        "  y_tr = mat['cleanClassLabels']\n",
        "  \n",
        "  print(np.shape(r_X_tr))\n",
        "  print(np.shape(y_tr))\n",
        "\n",
        "  for t in range(r_X_tr.shape[0]):\n",
        "    tril = r_X_tr[t,:,:]\n",
        "    # tril = tril.transpose()\n",
        "    tril_filtered = butter_bandpass_filter(tril, \n",
        "                                              lowcut=4, \n",
        "                                              highcut=40, \n",
        "                                              fs=250,\n",
        "                                              order=4)\n",
        "    # tril_filtered = tril_filtered.transpose()\n",
        "    X_tr[t,:,:] = tril_filtered \n",
        "    \n",
        "  # split data of each subject in training and validation\n",
        "  X_train      = X_tr[0:240,:,500:1250]\n",
        "  Y_train      = y_tr[0:240]\n",
        "  X_val       = X_tr[241:,:,500:1250]\n",
        "  Y_val       = y_tr[241:]\n",
        "\n",
        "  print(np.shape(X_train))\n",
        "  print(np.shape(Y_train))\n",
        "  print(np.shape(X_val))\n",
        "  print(np.shape(Y_val))\n",
        "  \n",
        "  # convert labels to one-hot encodings.\n",
        "  Y_train      = np_utils.to_categorical(Y_train-1)\n",
        "  Y_val       = np_utils.to_categorical(Y_val-1)\n",
        "\n",
        "  kernels, chans, samples = 1, 22, 750\n",
        "  # convert data to NCHW (trials, kernels, channels, samples) format. Data \n",
        "  # contains 22 channels and 500 time-points. Set the number of kernels to 1.\n",
        "  X_train      = X_train.reshape(X_train.shape[0], kernels, chans, samples)\n",
        "  X_val       = X_val.reshape(X_val.shape[0], kernels, chans, samples)\n",
        "   \n",
        "  print('X_train shape:', X_train.shape)\n",
        "  print(X_train.shape[0], 'train samples')\n",
        "  print(X_val.shape[0], 'val samples')\n",
        "\n",
        "  # Load test data         \n",
        "  fName = 'EEG_Deep/Data2A/Data_A0' + str(x) + 'E.mat'  # Load Data\n",
        "  print(fName)\n",
        "  mat = spio.loadmat(fName)\n",
        "  r_X_ts = mat['cleanRawEEGData']\n",
        "  y_ts = mat['cleanClassLabels']\n",
        "\n",
        "  print(np.shape(r_X_ts))\n",
        "  print(np.shape(y_ts))\n",
        "\n",
        "  for t in range(r_X_ts.shape[0]):\n",
        "    tril = r_X_ts[t,:,:]\n",
        "    # tril = tril.transpose()\n",
        "    tril_filtered = butter_bandpass_filter(tril, \n",
        "                                              lowcut=4, \n",
        "                                              highcut=40, \n",
        "                                              fs=250,\n",
        "                                              order=4)\n",
        "    # tril_filtered = tril_filtered.transpose()\n",
        "    X_ts[t,:,:] = tril_filtered \n",
        "\n",
        "  X_test      = X_ts[:,:,500:1250]\n",
        "  Y_test      = y_ts[:]\n",
        "  print(np.shape(X_test))\n",
        "  print(np.shape(Y_test))\n",
        "\n",
        "  #convert labels to one-hot encodings.\n",
        "  Y_test      = np_utils.to_categorical(Y_test-1)\n",
        "\n",
        "  # convert data to NCHW (trials, kernels, channels, samples) format. Data \n",
        "  # contains 22 channels and 500 time-points. Set the number of kernels to 1.\n",
        "  X_test      = X_test.reshape(X_test.shape[0], kernels, chans, samples)\n",
        "\n",
        "  print('X_train shape:', X_test.shape)\n",
        "  print(X_test.shape[0], 'train samples')\n",
        "\n",
        "  # configure the EEGNet-8,2,16 model with kernel length of 32 samples (other \n",
        "  # model configurations may do better, but this is a good starting point)\n",
        "  model = EEGNet(nb_classes = 4, Chans = 22, Samples = 750, \n",
        "                 dropoutRate = 0.25, kernLength = 25, F1 = 8, \n",
        "                 D = 2, F2 = 16, norm_rate = 0.25, dropoutType = 'Dropout')\n",
        "\n",
        "  # compile the model and set the optimizers\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='adam', \n",
        "                metrics = ['accuracy'])\n",
        "\n",
        "  # count number of parameters in the model\n",
        "  numParams    = model.count_params() \n",
        "\n",
        "  # set a valid path for your system to record model checkpoints\n",
        "  checkpointer = ModelCheckpoint(filepath='/tmp/checkpoint.h5', verbose=1,\n",
        "                                 save_best_only=True)\n",
        "  \n",
        "  # the syntax is {class_1:weight_1, class_2:weight_2,...}. Here just setting\n",
        "  # the weights all to be 1\n",
        "  class_weights = {0:1, 1:1, 2:1, 3:1}\n",
        "\n",
        "################################################################################\n",
        "# fit the model. Due to very small sample sizes this can get\n",
        "# pretty noisy run-to-run, but most runs should be comparable to xDAWN + \n",
        "# Riemannian geometry classification (below)\n",
        "################################################################################\n",
        "  history = model.fit(X_train, Y_train, batch_size = 16, epochs = 300, \n",
        "                      verbose = 2, validation_data=(X_val, Y_val),\n",
        "                      callbacks=[checkpointer], class_weight = class_weights)\n",
        "  \n",
        "  # Plot training & validation accuracy values\n",
        "  plt.plot(history.history['acc'])\n",
        "  plt.plot(history.history['val_acc'])\n",
        "  plt.title('Model accuracy')\n",
        "  plt.ylabel('Accuracy')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.legend(['Train', 'Test'], loc='upper left')\n",
        "  plt.show()\n",
        "  figName = 'Accuracy_A0' + str(x) + '.png'  \n",
        "  plt.savefig(figName)\n",
        "\n",
        "  print('\\n# Evaluate on test data')\n",
        "  results = model.evaluate(X_test, Y_test, batch_size=1)\n",
        "  print('test loss, test acc:', results)\n",
        "\n",
        "  acc_all[x - 1, 0] = results[0]\n",
        "  acc_all[x - 1, 1] = results[1]\n",
        "\n",
        "  from keras import backend as K \n",
        "  # Do some code, e.g. train and save model\n",
        "  K.clear_session()\n",
        "\n",
        "\n",
        "print(acc_all)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "EEG_Deep/Data2A/Data_A01T.mat\n",
            "(288, 22, 1875)\n",
            "(288, 1)\n",
            "(240, 22, 750)\n",
            "(240, 1)\n",
            "(47, 22, 750)\n",
            "(47, 1)\n",
            "X_train shape: (240, 1, 22, 750)\n",
            "240 train samples\n",
            "47 val samples\n",
            "EEG_Deep/Data2A/Data_A01E.mat\n",
            "(288, 22, 1875)\n",
            "(288, 1)\n",
            "(288, 22, 750)\n",
            "(288, 1)\n",
            "X_train shape: (288, 1, 22, 750)\n",
            "288 train samples\n",
            "Train on 240 samples, validate on 47 samples\n",
            "Epoch 1/300\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 1.38222, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 1s - loss: 1.4062 - acc: 0.2458 - val_loss: 1.3822 - val_acc: 0.3191\n",
            "Epoch 2/300\n",
            "\n",
            "Epoch 00002: val_loss improved from 1.38222 to 1.37921, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.3428 - acc: 0.3833 - val_loss: 1.3792 - val_acc: 0.4043\n",
            "Epoch 3/300\n",
            "\n",
            "Epoch 00003: val_loss improved from 1.37921 to 1.36866, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.2676 - acc: 0.5625 - val_loss: 1.3687 - val_acc: 0.4043\n",
            "Epoch 4/300\n",
            "\n",
            "Epoch 00004: val_loss improved from 1.36866 to 1.34289, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.1509 - acc: 0.6208 - val_loss: 1.3429 - val_acc: 0.3404\n",
            "Epoch 5/300\n",
            "\n",
            "Epoch 00005: val_loss improved from 1.34289 to 1.31355, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.0116 - acc: 0.6000 - val_loss: 1.3136 - val_acc: 0.2766\n",
            "Epoch 6/300\n",
            "\n",
            "Epoch 00006: val_loss improved from 1.31355 to 1.29439, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.9418 - acc: 0.6458 - val_loss: 1.2944 - val_acc: 0.2766\n",
            "Epoch 7/300\n",
            "\n",
            "Epoch 00007: val_loss improved from 1.29439 to 1.28836, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.8784 - acc: 0.6458 - val_loss: 1.2884 - val_acc: 0.2766\n",
            "Epoch 8/300\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 1.28836\n",
            "240/240 - 0s - loss: 0.8639 - acc: 0.6458 - val_loss: 1.2887 - val_acc: 0.2766\n",
            "Epoch 9/300\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 1.28836\n",
            "240/240 - 0s - loss: 0.8481 - acc: 0.6875 - val_loss: 1.3001 - val_acc: 0.2766\n",
            "Epoch 10/300\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 1.28836\n",
            "240/240 - 0s - loss: 0.8155 - acc: 0.6708 - val_loss: 1.3178 - val_acc: 0.2766\n",
            "Epoch 11/300\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 1.28836\n",
            "240/240 - 0s - loss: 0.7988 - acc: 0.6542 - val_loss: 1.3022 - val_acc: 0.2766\n",
            "Epoch 12/300\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 1.28836\n",
            "240/240 - 0s - loss: 0.7984 - acc: 0.6667 - val_loss: 1.3126 - val_acc: 0.2766\n",
            "Epoch 13/300\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 1.28836\n",
            "240/240 - 0s - loss: 0.7914 - acc: 0.6958 - val_loss: 1.3226 - val_acc: 0.2766\n",
            "Epoch 14/300\n",
            "\n",
            "Epoch 00014: val_loss improved from 1.28836 to 1.26245, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.7515 - acc: 0.7167 - val_loss: 1.2624 - val_acc: 0.2979\n",
            "Epoch 15/300\n",
            "\n",
            "Epoch 00015: val_loss improved from 1.26245 to 1.25004, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.7316 - acc: 0.7458 - val_loss: 1.2500 - val_acc: 0.3191\n",
            "Epoch 16/300\n",
            "\n",
            "Epoch 00016: val_loss improved from 1.25004 to 1.20782, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.7445 - acc: 0.7083 - val_loss: 1.2078 - val_acc: 0.3830\n",
            "Epoch 17/300\n",
            "\n",
            "Epoch 00017: val_loss improved from 1.20782 to 1.20183, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.7149 - acc: 0.7417 - val_loss: 1.2018 - val_acc: 0.3191\n",
            "Epoch 18/300\n",
            "\n",
            "Epoch 00018: val_loss improved from 1.20183 to 1.15832, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.7114 - acc: 0.7333 - val_loss: 1.1583 - val_acc: 0.3830\n",
            "Epoch 19/300\n",
            "\n",
            "Epoch 00019: val_loss improved from 1.15832 to 1.13171, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.6781 - acc: 0.7833 - val_loss: 1.1317 - val_acc: 0.5106\n",
            "Epoch 20/300\n",
            "\n",
            "Epoch 00020: val_loss improved from 1.13171 to 1.13079, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.6907 - acc: 0.7417 - val_loss: 1.1308 - val_acc: 0.4255\n",
            "Epoch 21/300\n",
            "\n",
            "Epoch 00021: val_loss improved from 1.13079 to 1.08296, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.6718 - acc: 0.7833 - val_loss: 1.0830 - val_acc: 0.4894\n",
            "Epoch 22/300\n",
            "\n",
            "Epoch 00022: val_loss improved from 1.08296 to 1.03263, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.6807 - acc: 0.7583 - val_loss: 1.0326 - val_acc: 0.4681\n",
            "Epoch 23/300\n",
            "\n",
            "Epoch 00023: val_loss improved from 1.03263 to 0.98227, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.6604 - acc: 0.7625 - val_loss: 0.9823 - val_acc: 0.5106\n",
            "Epoch 24/300\n",
            "\n",
            "Epoch 00024: val_loss improved from 0.98227 to 0.94193, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.6756 - acc: 0.7667 - val_loss: 0.9419 - val_acc: 0.5745\n",
            "Epoch 25/300\n",
            "\n",
            "Epoch 00025: val_loss improved from 0.94193 to 0.88752, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.6482 - acc: 0.7917 - val_loss: 0.8875 - val_acc: 0.6170\n",
            "Epoch 26/300\n",
            "\n",
            "Epoch 00026: val_loss improved from 0.88752 to 0.87854, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.6313 - acc: 0.7875 - val_loss: 0.8785 - val_acc: 0.5957\n",
            "Epoch 27/300\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.87854\n",
            "240/240 - 0s - loss: 0.6230 - acc: 0.7792 - val_loss: 0.9323 - val_acc: 0.5319\n",
            "Epoch 28/300\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.87854\n",
            "240/240 - 0s - loss: 0.6442 - acc: 0.7500 - val_loss: 0.9275 - val_acc: 0.5745\n",
            "Epoch 29/300\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.87854\n",
            "240/240 - 0s - loss: 0.6274 - acc: 0.7792 - val_loss: 0.9084 - val_acc: 0.5532\n",
            "Epoch 30/300\n",
            "\n",
            "Epoch 00030: val_loss improved from 0.87854 to 0.85696, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.6009 - acc: 0.8125 - val_loss: 0.8570 - val_acc: 0.5745\n",
            "Epoch 31/300\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.85696\n",
            "240/240 - 0s - loss: 0.6363 - acc: 0.7500 - val_loss: 0.8654 - val_acc: 0.6170\n",
            "Epoch 32/300\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.85696\n",
            "240/240 - 0s - loss: 0.6448 - acc: 0.7792 - val_loss: 0.8610 - val_acc: 0.6383\n",
            "Epoch 33/300\n",
            "\n",
            "Epoch 00033: val_loss improved from 0.85696 to 0.83355, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.6191 - acc: 0.8042 - val_loss: 0.8335 - val_acc: 0.6596\n",
            "Epoch 34/300\n",
            "\n",
            "Epoch 00034: val_loss improved from 0.83355 to 0.81533, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.5873 - acc: 0.8375 - val_loss: 0.8153 - val_acc: 0.7021\n",
            "Epoch 35/300\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.81533\n",
            "240/240 - 0s - loss: 0.5923 - acc: 0.8083 - val_loss: 0.8352 - val_acc: 0.6383\n",
            "Epoch 36/300\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.81533\n",
            "240/240 - 0s - loss: 0.5988 - acc: 0.7917 - val_loss: 0.8253 - val_acc: 0.6170\n",
            "Epoch 37/300\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.81533\n",
            "240/240 - 0s - loss: 0.5898 - acc: 0.8083 - val_loss: 0.8407 - val_acc: 0.7021\n",
            "Epoch 38/300\n",
            "\n",
            "Epoch 00038: val_loss improved from 0.81533 to 0.80613, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.6018 - acc: 0.7917 - val_loss: 0.8061 - val_acc: 0.7447\n",
            "Epoch 39/300\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.80613\n",
            "240/240 - 0s - loss: 0.5601 - acc: 0.8625 - val_loss: 0.8322 - val_acc: 0.6809\n",
            "Epoch 40/300\n",
            "\n",
            "Epoch 00040: val_loss improved from 0.80613 to 0.80172, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.5778 - acc: 0.8375 - val_loss: 0.8017 - val_acc: 0.7234\n",
            "Epoch 41/300\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.80172\n",
            "240/240 - 0s - loss: 0.5647 - acc: 0.8125 - val_loss: 0.8107 - val_acc: 0.7660\n",
            "Epoch 42/300\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.80172\n",
            "240/240 - 0s - loss: 0.5940 - acc: 0.7958 - val_loss: 0.8042 - val_acc: 0.7021\n",
            "Epoch 43/300\n",
            "\n",
            "Epoch 00043: val_loss improved from 0.80172 to 0.76506, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.5632 - acc: 0.8250 - val_loss: 0.7651 - val_acc: 0.7234\n",
            "Epoch 44/300\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.76506\n",
            "240/240 - 0s - loss: 0.5645 - acc: 0.8042 - val_loss: 0.7969 - val_acc: 0.7447\n",
            "Epoch 45/300\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.76506\n",
            "240/240 - 0s - loss: 0.5264 - acc: 0.8417 - val_loss: 0.7825 - val_acc: 0.7234\n",
            "Epoch 46/300\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.76506\n",
            "240/240 - 0s - loss: 0.5714 - acc: 0.8167 - val_loss: 0.8036 - val_acc: 0.6809\n",
            "Epoch 47/300\n",
            "\n",
            "Epoch 00047: val_loss improved from 0.76506 to 0.76353, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.5682 - acc: 0.8208 - val_loss: 0.7635 - val_acc: 0.7234\n",
            "Epoch 48/300\n",
            "\n",
            "Epoch 00048: val_loss improved from 0.76353 to 0.74373, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.5479 - acc: 0.8292 - val_loss: 0.7437 - val_acc: 0.7234\n",
            "Epoch 49/300\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.74373\n",
            "240/240 - 0s - loss: 0.5609 - acc: 0.7875 - val_loss: 0.7630 - val_acc: 0.7021\n",
            "Epoch 50/300\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.74373\n",
            "240/240 - 0s - loss: 0.5498 - acc: 0.8125 - val_loss: 0.7665 - val_acc: 0.7234\n",
            "Epoch 51/300\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.74373\n",
            "240/240 - 0s - loss: 0.5456 - acc: 0.8042 - val_loss: 0.7704 - val_acc: 0.7660\n",
            "Epoch 52/300\n",
            "\n",
            "Epoch 00052: val_loss improved from 0.74373 to 0.73880, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.5520 - acc: 0.8417 - val_loss: 0.7388 - val_acc: 0.7872\n",
            "Epoch 53/300\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.73880\n",
            "240/240 - 0s - loss: 0.5387 - acc: 0.8042 - val_loss: 0.7493 - val_acc: 0.7660\n",
            "Epoch 54/300\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.73880\n",
            "240/240 - 0s - loss: 0.5600 - acc: 0.8167 - val_loss: 0.7560 - val_acc: 0.7447\n",
            "Epoch 55/300\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.73880\n",
            "240/240 - 0s - loss: 0.5316 - acc: 0.8208 - val_loss: 0.7438 - val_acc: 0.7234\n",
            "Epoch 56/300\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.73880\n",
            "240/240 - 0s - loss: 0.5617 - acc: 0.7917 - val_loss: 0.7577 - val_acc: 0.7447\n",
            "Epoch 57/300\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.73880\n",
            "240/240 - 0s - loss: 0.5150 - acc: 0.8792 - val_loss: 0.7872 - val_acc: 0.7234\n",
            "Epoch 58/300\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.73880\n",
            "240/240 - 0s - loss: 0.5410 - acc: 0.8333 - val_loss: 0.7929 - val_acc: 0.7234\n",
            "Epoch 59/300\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.73880\n",
            "240/240 - 0s - loss: 0.5511 - acc: 0.8375 - val_loss: 0.7989 - val_acc: 0.7021\n",
            "Epoch 60/300\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.73880\n",
            "240/240 - 0s - loss: 0.5225 - acc: 0.8417 - val_loss: 0.7733 - val_acc: 0.7234\n",
            "Epoch 61/300\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.73880\n",
            "240/240 - 0s - loss: 0.5320 - acc: 0.8458 - val_loss: 0.7930 - val_acc: 0.6596\n",
            "Epoch 62/300\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.73880\n",
            "240/240 - 0s - loss: 0.5250 - acc: 0.8542 - val_loss: 0.7408 - val_acc: 0.7021\n",
            "Epoch 63/300\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.73880\n",
            "240/240 - 0s - loss: 0.5175 - acc: 0.8583 - val_loss: 0.7769 - val_acc: 0.7021\n",
            "Epoch 64/300\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.73880\n",
            "240/240 - 0s - loss: 0.4955 - acc: 0.8542 - val_loss: 0.7875 - val_acc: 0.7660\n",
            "Epoch 65/300\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.73880\n",
            "240/240 - 0s - loss: 0.4553 - acc: 0.9125 - val_loss: 0.7651 - val_acc: 0.7021\n",
            "Epoch 66/300\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.73880\n",
            "240/240 - 0s - loss: 0.4844 - acc: 0.8583 - val_loss: 0.7783 - val_acc: 0.6809\n",
            "Epoch 67/300\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.73880\n",
            "240/240 - 0s - loss: 0.4739 - acc: 0.8583 - val_loss: 0.7876 - val_acc: 0.6596\n",
            "Epoch 68/300\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.73880\n",
            "240/240 - 0s - loss: 0.5157 - acc: 0.8208 - val_loss: 0.7893 - val_acc: 0.7234\n",
            "Epoch 69/300\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.73880\n",
            "240/240 - 0s - loss: 0.4762 - acc: 0.8625 - val_loss: 0.7952 - val_acc: 0.6596\n",
            "Epoch 70/300\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.73880\n",
            "240/240 - 0s - loss: 0.4737 - acc: 0.8750 - val_loss: 0.7945 - val_acc: 0.6809\n",
            "Epoch 71/300\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.73880\n",
            "240/240 - 0s - loss: 0.4897 - acc: 0.8458 - val_loss: 0.8226 - val_acc: 0.6809\n",
            "Epoch 72/300\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.73880\n",
            "240/240 - 0s - loss: 0.4675 - acc: 0.8833 - val_loss: 0.8250 - val_acc: 0.7234\n",
            "Epoch 73/300\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.73880\n",
            "240/240 - 0s - loss: 0.4629 - acc: 0.8708 - val_loss: 0.8398 - val_acc: 0.6596\n",
            "Epoch 74/300\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.73880\n",
            "240/240 - 0s - loss: 0.4559 - acc: 0.8625 - val_loss: 0.8010 - val_acc: 0.7021\n",
            "Epoch 75/300\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.73880\n",
            "240/240 - 0s - loss: 0.4957 - acc: 0.8208 - val_loss: 0.8192 - val_acc: 0.6596\n",
            "Epoch 76/300\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.73880\n",
            "240/240 - 0s - loss: 0.4848 - acc: 0.8708 - val_loss: 0.8494 - val_acc: 0.6809\n",
            "Epoch 77/300\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.73880\n",
            "240/240 - 0s - loss: 0.4734 - acc: 0.8833 - val_loss: 0.8589 - val_acc: 0.5319\n",
            "Epoch 78/300\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.73880\n",
            "240/240 - 0s - loss: 0.4865 - acc: 0.8833 - val_loss: 0.8322 - val_acc: 0.7234\n",
            "Epoch 79/300\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.73880\n",
            "240/240 - 0s - loss: 0.4628 - acc: 0.8667 - val_loss: 0.7941 - val_acc: 0.7021\n",
            "Epoch 80/300\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.73880\n",
            "240/240 - 0s - loss: 0.4618 - acc: 0.8667 - val_loss: 0.7932 - val_acc: 0.6809\n",
            "Epoch 81/300\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.73880\n",
            "240/240 - 0s - loss: 0.4770 - acc: 0.8667 - val_loss: 0.8112 - val_acc: 0.6809\n",
            "Epoch 82/300\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.73880\n",
            "240/240 - 0s - loss: 0.4613 - acc: 0.8667 - val_loss: 0.7994 - val_acc: 0.7234\n",
            "Epoch 83/300\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.73880\n",
            "240/240 - 0s - loss: 0.4536 - acc: 0.8708 - val_loss: 0.7946 - val_acc: 0.7447\n",
            "Epoch 84/300\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.73880\n",
            "240/240 - 0s - loss: 0.4548 - acc: 0.8792 - val_loss: 0.7895 - val_acc: 0.7021\n",
            "Epoch 85/300\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.73880\n",
            "240/240 - 0s - loss: 0.4625 - acc: 0.8542 - val_loss: 0.7928 - val_acc: 0.6596\n",
            "Epoch 86/300\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.73880\n",
            "240/240 - 0s - loss: 0.4655 - acc: 0.8458 - val_loss: 0.7747 - val_acc: 0.6809\n",
            "Epoch 87/300\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.73880\n",
            "240/240 - 0s - loss: 0.4688 - acc: 0.8417 - val_loss: 0.8204 - val_acc: 0.6809\n",
            "Epoch 88/300\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.73880\n",
            "240/240 - 0s - loss: 0.4322 - acc: 0.8833 - val_loss: 0.8058 - val_acc: 0.6383\n",
            "Epoch 89/300\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.73880\n",
            "240/240 - 0s - loss: 0.4242 - acc: 0.8792 - val_loss: 0.8264 - val_acc: 0.6596\n",
            "Epoch 90/300\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.73880\n",
            "240/240 - 0s - loss: 0.4625 - acc: 0.8583 - val_loss: 0.8050 - val_acc: 0.6596\n",
            "Epoch 91/300\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.73880\n",
            "240/240 - 0s - loss: 0.4394 - acc: 0.8833 - val_loss: 0.8164 - val_acc: 0.6809\n",
            "Epoch 92/300\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.73880\n",
            "240/240 - 0s - loss: 0.4267 - acc: 0.8667 - val_loss: 0.8074 - val_acc: 0.6596\n",
            "Epoch 93/300\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.73880\n",
            "240/240 - 0s - loss: 0.4191 - acc: 0.8708 - val_loss: 0.8011 - val_acc: 0.7021\n",
            "Epoch 94/300\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.73880\n",
            "240/240 - 0s - loss: 0.4158 - acc: 0.8792 - val_loss: 0.8118 - val_acc: 0.7234\n",
            "Epoch 95/300\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.73880\n",
            "240/240 - 0s - loss: 0.4690 - acc: 0.8667 - val_loss: 0.8061 - val_acc: 0.6596\n",
            "Epoch 96/300\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.73880\n",
            "240/240 - 0s - loss: 0.4185 - acc: 0.8875 - val_loss: 0.8137 - val_acc: 0.6809\n",
            "Epoch 97/300\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.73880\n",
            "240/240 - 0s - loss: 0.3810 - acc: 0.9125 - val_loss: 0.8081 - val_acc: 0.7021\n",
            "Epoch 98/300\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.73880\n",
            "240/240 - 0s - loss: 0.4116 - acc: 0.9083 - val_loss: 0.8063 - val_acc: 0.7021\n",
            "Epoch 99/300\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.73880\n",
            "240/240 - 0s - loss: 0.4139 - acc: 0.8750 - val_loss: 0.8535 - val_acc: 0.7021\n",
            "Epoch 100/300\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.73880\n",
            "240/240 - 0s - loss: 0.4528 - acc: 0.8375 - val_loss: 0.9120 - val_acc: 0.5745\n",
            "Epoch 101/300\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.73880\n",
            "240/240 - 0s - loss: 0.4150 - acc: 0.8667 - val_loss: 0.8339 - val_acc: 0.6809\n",
            "Epoch 102/300\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.73880\n",
            "240/240 - 0s - loss: 0.4909 - acc: 0.8250 - val_loss: 0.8266 - val_acc: 0.7447\n",
            "Epoch 103/300\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.73880\n",
            "240/240 - 0s - loss: 0.4426 - acc: 0.8458 - val_loss: 0.9212 - val_acc: 0.5319\n",
            "Epoch 104/300\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.73880\n",
            "240/240 - 0s - loss: 0.4211 - acc: 0.8750 - val_loss: 0.9270 - val_acc: 0.6170\n",
            "Epoch 105/300\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 0.73880\n",
            "240/240 - 0s - loss: 0.4037 - acc: 0.8625 - val_loss: 0.8805 - val_acc: 0.7234\n",
            "Epoch 106/300\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 0.73880\n",
            "240/240 - 0s - loss: 0.4144 - acc: 0.8875 - val_loss: 0.8678 - val_acc: 0.7021\n",
            "Epoch 107/300\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.73880\n",
            "240/240 - 0s - loss: 0.3920 - acc: 0.9083 - val_loss: 0.8126 - val_acc: 0.6809\n",
            "Epoch 108/300\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 0.73880\n",
            "240/240 - 0s - loss: 0.3834 - acc: 0.8917 - val_loss: 0.8401 - val_acc: 0.5745\n",
            "Epoch 109/300\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.73880\n",
            "240/240 - 0s - loss: 0.4030 - acc: 0.8875 - val_loss: 0.8765 - val_acc: 0.6809\n",
            "Epoch 110/300\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.73880\n",
            "240/240 - 0s - loss: 0.3518 - acc: 0.9417 - val_loss: 0.8198 - val_acc: 0.7021\n",
            "Epoch 111/300\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 0.73880\n",
            "240/240 - 0s - loss: 0.3797 - acc: 0.8917 - val_loss: 0.8410 - val_acc: 0.6383\n",
            "Epoch 112/300\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 0.73880\n",
            "240/240 - 0s - loss: 0.4090 - acc: 0.8750 - val_loss: 0.8586 - val_acc: 0.6596\n",
            "Epoch 113/300\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.73880\n",
            "240/240 - 0s - loss: 0.3652 - acc: 0.9083 - val_loss: 0.8455 - val_acc: 0.6383\n",
            "Epoch 114/300\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.73880\n",
            "240/240 - 0s - loss: 0.3655 - acc: 0.9042 - val_loss: 0.8267 - val_acc: 0.6383\n",
            "Epoch 115/300\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.73880\n",
            "240/240 - 0s - loss: 0.4043 - acc: 0.8917 - val_loss: 0.8120 - val_acc: 0.6809\n",
            "Epoch 116/300\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.73880\n",
            "240/240 - 0s - loss: 0.3566 - acc: 0.9125 - val_loss: 0.8386 - val_acc: 0.6170\n",
            "Epoch 117/300\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.73880\n",
            "240/240 - 0s - loss: 0.3675 - acc: 0.9083 - val_loss: 0.9378 - val_acc: 0.6596\n",
            "Epoch 118/300\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.73880\n",
            "240/240 - 0s - loss: 0.3792 - acc: 0.9042 - val_loss: 0.8322 - val_acc: 0.6170\n",
            "Epoch 119/300\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 0.73880\n",
            "240/240 - 0s - loss: 0.3739 - acc: 0.9000 - val_loss: 0.8712 - val_acc: 0.6596\n",
            "Epoch 120/300\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.73880\n",
            "240/240 - 0s - loss: 0.3603 - acc: 0.8958 - val_loss: 0.8581 - val_acc: 0.6383\n",
            "Epoch 121/300\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.73880\n",
            "240/240 - 0s - loss: 0.3786 - acc: 0.9167 - val_loss: 0.8093 - val_acc: 0.6809\n",
            "Epoch 122/300\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.73880\n",
            "240/240 - 0s - loss: 0.3616 - acc: 0.9208 - val_loss: 0.8164 - val_acc: 0.7021\n",
            "Epoch 123/300\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 0.73880\n",
            "240/240 - 0s - loss: 0.3477 - acc: 0.9208 - val_loss: 0.8077 - val_acc: 0.6809\n",
            "Epoch 124/300\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 0.73880\n",
            "240/240 - 0s - loss: 0.3365 - acc: 0.9292 - val_loss: 0.8598 - val_acc: 0.6596\n",
            "Epoch 125/300\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 0.73880\n",
            "240/240 - 0s - loss: 0.3774 - acc: 0.8875 - val_loss: 0.8474 - val_acc: 0.5957\n",
            "Epoch 126/300\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 0.73880\n",
            "240/240 - 0s - loss: 0.3745 - acc: 0.8917 - val_loss: 0.8131 - val_acc: 0.6809\n",
            "Epoch 127/300\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 0.73880\n",
            "240/240 - 0s - loss: 0.3380 - acc: 0.9250 - val_loss: 0.8968 - val_acc: 0.6596\n",
            "Epoch 128/300\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 0.73880\n",
            "240/240 - 0s - loss: 0.3569 - acc: 0.9125 - val_loss: 0.8471 - val_acc: 0.6596\n",
            "Epoch 129/300\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.73880\n",
            "240/240 - 0s - loss: 0.3311 - acc: 0.9167 - val_loss: 0.8375 - val_acc: 0.6596\n",
            "Epoch 130/300\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 0.73880\n",
            "240/240 - 0s - loss: 0.3383 - acc: 0.9292 - val_loss: 0.8674 - val_acc: 0.6383\n",
            "Epoch 131/300\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 0.73880\n",
            "240/240 - 0s - loss: 0.3133 - acc: 0.9333 - val_loss: 0.8491 - val_acc: 0.6809\n",
            "Epoch 132/300\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 0.73880\n",
            "240/240 - 0s - loss: 0.3266 - acc: 0.8958 - val_loss: 0.8846 - val_acc: 0.6596\n",
            "Epoch 133/300\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.73880\n",
            "240/240 - 0s - loss: 0.3381 - acc: 0.9333 - val_loss: 0.8989 - val_acc: 0.5319\n",
            "Epoch 134/300\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.73880\n",
            "240/240 - 0s - loss: 0.3649 - acc: 0.8875 - val_loss: 0.8532 - val_acc: 0.7021\n",
            "Epoch 135/300\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 0.73880\n",
            "240/240 - 0s - loss: 0.3794 - acc: 0.9000 - val_loss: 0.8794 - val_acc: 0.7021\n",
            "Epoch 136/300\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.73880\n",
            "240/240 - 0s - loss: 0.3147 - acc: 0.9167 - val_loss: 0.8032 - val_acc: 0.6809\n",
            "Epoch 137/300\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 0.73880\n",
            "240/240 - 0s - loss: 0.3641 - acc: 0.9167 - val_loss: 0.8277 - val_acc: 0.6383\n",
            "Epoch 138/300\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.73880\n",
            "240/240 - 0s - loss: 0.2922 - acc: 0.9500 - val_loss: 0.8115 - val_acc: 0.6383\n",
            "Epoch 139/300\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.73880\n",
            "240/240 - 0s - loss: 0.3143 - acc: 0.9333 - val_loss: 0.8050 - val_acc: 0.6809\n",
            "Epoch 140/300\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 0.73880\n",
            "240/240 - 0s - loss: 0.3032 - acc: 0.9333 - val_loss: 0.8543 - val_acc: 0.6596\n",
            "Epoch 141/300\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.73880\n",
            "240/240 - 0s - loss: 0.3536 - acc: 0.8958 - val_loss: 0.7932 - val_acc: 0.6809\n",
            "Epoch 142/300\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.73880\n",
            "240/240 - 0s - loss: 0.3222 - acc: 0.9125 - val_loss: 0.7999 - val_acc: 0.6383\n",
            "Epoch 143/300\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 0.73880\n",
            "240/240 - 0s - loss: 0.3034 - acc: 0.9292 - val_loss: 0.8490 - val_acc: 0.6170\n",
            "Epoch 144/300\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 0.73880\n",
            "240/240 - 0s - loss: 0.3018 - acc: 0.9292 - val_loss: 0.8029 - val_acc: 0.7234\n",
            "Epoch 145/300\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.73880\n",
            "240/240 - 0s - loss: 0.2887 - acc: 0.9250 - val_loss: 0.8002 - val_acc: 0.7021\n",
            "Epoch 146/300\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.73880\n",
            "240/240 - 0s - loss: 0.3126 - acc: 0.9250 - val_loss: 0.8440 - val_acc: 0.6809\n",
            "Epoch 147/300\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.73880\n",
            "240/240 - 0s - loss: 0.3165 - acc: 0.9250 - val_loss: 0.8412 - val_acc: 0.6596\n",
            "Epoch 148/300\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 0.73880\n",
            "240/240 - 0s - loss: 0.2974 - acc: 0.9333 - val_loss: 0.8742 - val_acc: 0.7021\n",
            "Epoch 149/300\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.73880\n",
            "240/240 - 0s - loss: 0.2761 - acc: 0.9333 - val_loss: 0.8365 - val_acc: 0.6596\n",
            "Epoch 150/300\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.73880\n",
            "240/240 - 0s - loss: 0.3491 - acc: 0.9083 - val_loss: 0.8130 - val_acc: 0.6809\n",
            "Epoch 151/300\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.73880\n",
            "240/240 - 0s - loss: 0.3277 - acc: 0.9083 - val_loss: 0.8435 - val_acc: 0.6596\n",
            "Epoch 152/300\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.73880\n",
            "240/240 - 0s - loss: 0.3354 - acc: 0.8958 - val_loss: 0.8437 - val_acc: 0.7234\n",
            "Epoch 153/300\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 0.73880\n",
            "240/240 - 0s - loss: 0.3166 - acc: 0.9125 - val_loss: 0.8594 - val_acc: 0.6596\n",
            "Epoch 154/300\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.73880\n",
            "240/240 - 0s - loss: 0.2917 - acc: 0.9375 - val_loss: 0.8057 - val_acc: 0.6596\n",
            "Epoch 155/300\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.73880\n",
            "240/240 - 0s - loss: 0.3143 - acc: 0.9125 - val_loss: 0.8395 - val_acc: 0.6596\n",
            "Epoch 156/300\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.73880\n",
            "240/240 - 0s - loss: 0.2716 - acc: 0.9542 - val_loss: 0.7747 - val_acc: 0.6809\n",
            "Epoch 157/300\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.73880\n",
            "240/240 - 0s - loss: 0.2785 - acc: 0.9458 - val_loss: 0.8044 - val_acc: 0.6170\n",
            "Epoch 158/300\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.73880\n",
            "240/240 - 0s - loss: 0.2467 - acc: 0.9625 - val_loss: 0.8115 - val_acc: 0.7021\n",
            "Epoch 159/300\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.73880\n",
            "240/240 - 0s - loss: 0.2888 - acc: 0.9458 - val_loss: 0.8042 - val_acc: 0.7021\n",
            "Epoch 160/300\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.73880\n",
            "240/240 - 0s - loss: 0.2694 - acc: 0.9500 - val_loss: 0.7969 - val_acc: 0.7021\n",
            "Epoch 161/300\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.73880\n",
            "240/240 - 0s - loss: 0.2868 - acc: 0.9375 - val_loss: 0.8328 - val_acc: 0.6809\n",
            "Epoch 162/300\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.73880\n",
            "240/240 - 0s - loss: 0.2886 - acc: 0.9250 - val_loss: 0.8319 - val_acc: 0.7021\n",
            "Epoch 163/300\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.73880\n",
            "240/240 - 0s - loss: 0.2908 - acc: 0.9375 - val_loss: 0.8559 - val_acc: 0.7234\n",
            "Epoch 164/300\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.73880\n",
            "240/240 - 0s - loss: 0.2980 - acc: 0.9417 - val_loss: 0.8300 - val_acc: 0.7021\n",
            "Epoch 165/300\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.73880\n",
            "240/240 - 0s - loss: 0.2791 - acc: 0.9542 - val_loss: 0.7471 - val_acc: 0.7021\n",
            "Epoch 166/300\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 0.73880\n",
            "240/240 - 0s - loss: 0.2723 - acc: 0.9250 - val_loss: 0.8422 - val_acc: 0.7021\n",
            "Epoch 167/300\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.73880\n",
            "240/240 - 0s - loss: 0.2808 - acc: 0.9292 - val_loss: 0.7934 - val_acc: 0.6383\n",
            "Epoch 168/300\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.73880\n",
            "240/240 - 0s - loss: 0.2708 - acc: 0.9500 - val_loss: 0.8176 - val_acc: 0.7234\n",
            "Epoch 169/300\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 0.73880\n",
            "240/240 - 0s - loss: 0.3070 - acc: 0.9125 - val_loss: 0.8309 - val_acc: 0.7234\n",
            "Epoch 170/300\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.73880\n",
            "240/240 - 0s - loss: 0.3035 - acc: 0.9167 - val_loss: 0.7575 - val_acc: 0.7447\n",
            "Epoch 171/300\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.73880\n",
            "240/240 - 0s - loss: 0.2775 - acc: 0.9292 - val_loss: 0.7670 - val_acc: 0.7021\n",
            "Epoch 172/300\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.73880\n",
            "240/240 - 0s - loss: 0.2847 - acc: 0.9417 - val_loss: 0.8544 - val_acc: 0.7447\n",
            "Epoch 173/300\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.73880\n",
            "240/240 - 0s - loss: 0.2868 - acc: 0.9167 - val_loss: 0.8312 - val_acc: 0.6809\n",
            "Epoch 174/300\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.73880\n",
            "240/240 - 0s - loss: 0.2898 - acc: 0.9083 - val_loss: 0.7734 - val_acc: 0.6809\n",
            "Epoch 175/300\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.73880\n",
            "240/240 - 0s - loss: 0.2943 - acc: 0.9250 - val_loss: 0.7807 - val_acc: 0.6809\n",
            "Epoch 176/300\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 0.73880\n",
            "240/240 - 0s - loss: 0.2789 - acc: 0.9250 - val_loss: 0.8183 - val_acc: 0.7234\n",
            "Epoch 177/300\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.73880\n",
            "240/240 - 0s - loss: 0.2783 - acc: 0.9333 - val_loss: 0.7636 - val_acc: 0.7234\n",
            "Epoch 178/300\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.73880\n",
            "240/240 - 0s - loss: 0.2705 - acc: 0.9500 - val_loss: 0.7910 - val_acc: 0.6596\n",
            "Epoch 179/300\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.73880\n",
            "240/240 - 0s - loss: 0.2820 - acc: 0.9333 - val_loss: 0.7975 - val_acc: 0.7447\n",
            "Epoch 180/300\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.73880\n",
            "240/240 - 0s - loss: 0.3097 - acc: 0.9167 - val_loss: 0.7798 - val_acc: 0.7234\n",
            "Epoch 181/300\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.73880\n",
            "240/240 - 0s - loss: 0.2829 - acc: 0.9583 - val_loss: 0.8720 - val_acc: 0.6809\n",
            "Epoch 182/300\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.73880\n",
            "240/240 - 0s - loss: 0.2344 - acc: 0.9542 - val_loss: 0.7712 - val_acc: 0.7021\n",
            "Epoch 183/300\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.73880\n",
            "240/240 - 0s - loss: 0.2238 - acc: 0.9667 - val_loss: 0.8287 - val_acc: 0.6596\n",
            "Epoch 184/300\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 0.73880\n",
            "240/240 - 0s - loss: 0.2181 - acc: 0.9583 - val_loss: 0.7726 - val_acc: 0.6809\n",
            "Epoch 185/300\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.73880\n",
            "240/240 - 0s - loss: 0.2510 - acc: 0.9417 - val_loss: 0.8464 - val_acc: 0.6809\n",
            "Epoch 186/300\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.73880\n",
            "240/240 - 0s - loss: 0.2730 - acc: 0.9167 - val_loss: 0.8317 - val_acc: 0.6809\n",
            "Epoch 187/300\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 0.73880\n",
            "240/240 - 0s - loss: 0.2431 - acc: 0.9333 - val_loss: 0.7493 - val_acc: 0.6809\n",
            "Epoch 188/300\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.73880\n",
            "240/240 - 0s - loss: 0.2462 - acc: 0.9292 - val_loss: 0.7989 - val_acc: 0.6809\n",
            "Epoch 189/300\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.73880\n",
            "240/240 - 0s - loss: 0.2462 - acc: 0.9583 - val_loss: 0.7629 - val_acc: 0.7234\n",
            "Epoch 190/300\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.73880\n",
            "240/240 - 0s - loss: 0.2634 - acc: 0.9375 - val_loss: 0.7491 - val_acc: 0.7021\n",
            "Epoch 191/300\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.73880\n",
            "240/240 - 0s - loss: 0.2250 - acc: 0.9583 - val_loss: 0.7934 - val_acc: 0.7234\n",
            "Epoch 192/300\n",
            "\n",
            "Epoch 00192: val_loss improved from 0.73880 to 0.73512, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.2494 - acc: 0.9542 - val_loss: 0.7351 - val_acc: 0.7234\n",
            "Epoch 193/300\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.73512\n",
            "240/240 - 0s - loss: 0.2786 - acc: 0.9250 - val_loss: 0.7551 - val_acc: 0.7021\n",
            "Epoch 194/300\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.73512\n",
            "240/240 - 0s - loss: 0.2339 - acc: 0.9708 - val_loss: 0.7580 - val_acc: 0.6596\n",
            "Epoch 195/300\n",
            "\n",
            "Epoch 00195: val_loss improved from 0.73512 to 0.71820, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.2316 - acc: 0.9417 - val_loss: 0.7182 - val_acc: 0.7234\n",
            "Epoch 196/300\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.71820\n",
            "240/240 - 0s - loss: 0.2521 - acc: 0.9542 - val_loss: 0.7942 - val_acc: 0.6596\n",
            "Epoch 197/300\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.71820\n",
            "240/240 - 0s - loss: 0.2509 - acc: 0.9292 - val_loss: 0.7419 - val_acc: 0.7234\n",
            "Epoch 198/300\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.71820\n",
            "240/240 - 0s - loss: 0.2549 - acc: 0.9417 - val_loss: 0.7371 - val_acc: 0.7021\n",
            "Epoch 199/300\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.71820\n",
            "240/240 - 0s - loss: 0.2404 - acc: 0.9375 - val_loss: 0.8936 - val_acc: 0.7021\n",
            "Epoch 200/300\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.71820\n",
            "240/240 - 0s - loss: 0.2611 - acc: 0.9333 - val_loss: 0.8226 - val_acc: 0.6809\n",
            "Epoch 201/300\n",
            "\n",
            "Epoch 00201: val_loss did not improve from 0.71820\n",
            "240/240 - 0s - loss: 0.2432 - acc: 0.9458 - val_loss: 0.8621 - val_acc: 0.6809\n",
            "Epoch 202/300\n",
            "\n",
            "Epoch 00202: val_loss did not improve from 0.71820\n",
            "240/240 - 0s - loss: 0.2414 - acc: 0.9583 - val_loss: 0.7998 - val_acc: 0.6809\n",
            "Epoch 203/300\n",
            "\n",
            "Epoch 00203: val_loss did not improve from 0.71820\n",
            "240/240 - 0s - loss: 0.2510 - acc: 0.9583 - val_loss: 0.7904 - val_acc: 0.7021\n",
            "Epoch 204/300\n",
            "\n",
            "Epoch 00204: val_loss did not improve from 0.71820\n",
            "240/240 - 0s - loss: 0.2596 - acc: 0.9333 - val_loss: 0.7738 - val_acc: 0.7021\n",
            "Epoch 205/300\n",
            "\n",
            "Epoch 00205: val_loss did not improve from 0.71820\n",
            "240/240 - 0s - loss: 0.2549 - acc: 0.9333 - val_loss: 0.7324 - val_acc: 0.7021\n",
            "Epoch 206/300\n",
            "\n",
            "Epoch 00206: val_loss did not improve from 0.71820\n",
            "240/240 - 0s - loss: 0.2436 - acc: 0.9583 - val_loss: 0.7670 - val_acc: 0.7021\n",
            "Epoch 207/300\n",
            "\n",
            "Epoch 00207: val_loss did not improve from 0.71820\n",
            "240/240 - 0s - loss: 0.2187 - acc: 0.9417 - val_loss: 0.7798 - val_acc: 0.7234\n",
            "Epoch 208/300\n",
            "\n",
            "Epoch 00208: val_loss did not improve from 0.71820\n",
            "240/240 - 0s - loss: 0.2449 - acc: 0.9333 - val_loss: 0.7297 - val_acc: 0.7234\n",
            "Epoch 209/300\n",
            "\n",
            "Epoch 00209: val_loss did not improve from 0.71820\n",
            "240/240 - 0s - loss: 0.2495 - acc: 0.9458 - val_loss: 0.7910 - val_acc: 0.7660\n",
            "Epoch 210/300\n",
            "\n",
            "Epoch 00210: val_loss did not improve from 0.71820\n",
            "240/240 - 0s - loss: 0.2437 - acc: 0.9458 - val_loss: 0.8471 - val_acc: 0.7447\n",
            "Epoch 211/300\n",
            "\n",
            "Epoch 00211: val_loss did not improve from 0.71820\n",
            "240/240 - 0s - loss: 0.2339 - acc: 0.9625 - val_loss: 0.8364 - val_acc: 0.7234\n",
            "Epoch 212/300\n",
            "\n",
            "Epoch 00212: val_loss did not improve from 0.71820\n",
            "240/240 - 0s - loss: 0.2243 - acc: 0.9583 - val_loss: 0.7781 - val_acc: 0.6809\n",
            "Epoch 213/300\n",
            "\n",
            "Epoch 00213: val_loss did not improve from 0.71820\n",
            "240/240 - 0s - loss: 0.2086 - acc: 0.9625 - val_loss: 0.7745 - val_acc: 0.7234\n",
            "Epoch 214/300\n",
            "\n",
            "Epoch 00214: val_loss improved from 0.71820 to 0.70147, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.2440 - acc: 0.9292 - val_loss: 0.7015 - val_acc: 0.7234\n",
            "Epoch 215/300\n",
            "\n",
            "Epoch 00215: val_loss did not improve from 0.70147\n",
            "240/240 - 0s - loss: 0.2339 - acc: 0.9458 - val_loss: 0.7302 - val_acc: 0.7021\n",
            "Epoch 216/300\n",
            "\n",
            "Epoch 00216: val_loss did not improve from 0.70147\n",
            "240/240 - 0s - loss: 0.2153 - acc: 0.9625 - val_loss: 0.9021 - val_acc: 0.7021\n",
            "Epoch 217/300\n",
            "\n",
            "Epoch 00217: val_loss did not improve from 0.70147\n",
            "240/240 - 0s - loss: 0.2410 - acc: 0.9375 - val_loss: 0.7792 - val_acc: 0.6596\n",
            "Epoch 218/300\n",
            "\n",
            "Epoch 00218: val_loss did not improve from 0.70147\n",
            "240/240 - 0s - loss: 0.2330 - acc: 0.9417 - val_loss: 0.7408 - val_acc: 0.7021\n",
            "Epoch 219/300\n",
            "\n",
            "Epoch 00219: val_loss did not improve from 0.70147\n",
            "240/240 - 0s - loss: 0.2284 - acc: 0.9458 - val_loss: 0.7413 - val_acc: 0.7447\n",
            "Epoch 220/300\n",
            "\n",
            "Epoch 00220: val_loss did not improve from 0.70147\n",
            "240/240 - 0s - loss: 0.2284 - acc: 0.9333 - val_loss: 0.7966 - val_acc: 0.6596\n",
            "Epoch 221/300\n",
            "\n",
            "Epoch 00221: val_loss did not improve from 0.70147\n",
            "240/240 - 0s - loss: 0.2382 - acc: 0.9375 - val_loss: 0.7879 - val_acc: 0.7234\n",
            "Epoch 222/300\n",
            "\n",
            "Epoch 00222: val_loss did not improve from 0.70147\n",
            "240/240 - 0s - loss: 0.2538 - acc: 0.9333 - val_loss: 0.7995 - val_acc: 0.7234\n",
            "Epoch 223/300\n",
            "\n",
            "Epoch 00223: val_loss did not improve from 0.70147\n",
            "240/240 - 0s - loss: 0.2337 - acc: 0.9542 - val_loss: 0.7486 - val_acc: 0.7021\n",
            "Epoch 224/300\n",
            "\n",
            "Epoch 00224: val_loss did not improve from 0.70147\n",
            "240/240 - 0s - loss: 0.2345 - acc: 0.9500 - val_loss: 0.8118 - val_acc: 0.7234\n",
            "Epoch 225/300\n",
            "\n",
            "Epoch 00225: val_loss did not improve from 0.70147\n",
            "240/240 - 0s - loss: 0.2124 - acc: 0.9750 - val_loss: 0.7495 - val_acc: 0.6809\n",
            "Epoch 226/300\n",
            "\n",
            "Epoch 00226: val_loss did not improve from 0.70147\n",
            "240/240 - 0s - loss: 0.2465 - acc: 0.9250 - val_loss: 0.7954 - val_acc: 0.7660\n",
            "Epoch 227/300\n",
            "\n",
            "Epoch 00227: val_loss did not improve from 0.70147\n",
            "240/240 - 0s - loss: 0.2117 - acc: 0.9542 - val_loss: 0.7817 - val_acc: 0.7021\n",
            "Epoch 228/300\n",
            "\n",
            "Epoch 00228: val_loss did not improve from 0.70147\n",
            "240/240 - 0s - loss: 0.2502 - acc: 0.9375 - val_loss: 0.7393 - val_acc: 0.7234\n",
            "Epoch 229/300\n",
            "\n",
            "Epoch 00229: val_loss did not improve from 0.70147\n",
            "240/240 - 0s - loss: 0.2569 - acc: 0.9250 - val_loss: 0.8101 - val_acc: 0.7021\n",
            "Epoch 230/300\n",
            "\n",
            "Epoch 00230: val_loss did not improve from 0.70147\n",
            "240/240 - 0s - loss: 0.2640 - acc: 0.9250 - val_loss: 0.7701 - val_acc: 0.7021\n",
            "Epoch 231/300\n",
            "\n",
            "Epoch 00231: val_loss did not improve from 0.70147\n",
            "240/240 - 0s - loss: 0.2272 - acc: 0.9625 - val_loss: 0.7531 - val_acc: 0.7021\n",
            "Epoch 232/300\n",
            "\n",
            "Epoch 00232: val_loss did not improve from 0.70147\n",
            "240/240 - 0s - loss: 0.1966 - acc: 0.9625 - val_loss: 0.8021 - val_acc: 0.7021\n",
            "Epoch 233/300\n",
            "\n",
            "Epoch 00233: val_loss did not improve from 0.70147\n",
            "240/240 - 0s - loss: 0.2146 - acc: 0.9583 - val_loss: 0.7537 - val_acc: 0.7234\n",
            "Epoch 234/300\n",
            "\n",
            "Epoch 00234: val_loss did not improve from 0.70147\n",
            "240/240 - 0s - loss: 0.2302 - acc: 0.9417 - val_loss: 0.7588 - val_acc: 0.7234\n",
            "Epoch 235/300\n",
            "\n",
            "Epoch 00235: val_loss did not improve from 0.70147\n",
            "240/240 - 0s - loss: 0.2186 - acc: 0.9500 - val_loss: 0.7585 - val_acc: 0.7234\n",
            "Epoch 236/300\n",
            "\n",
            "Epoch 00236: val_loss did not improve from 0.70147\n",
            "240/240 - 0s - loss: 0.1982 - acc: 0.9583 - val_loss: 0.7319 - val_acc: 0.7234\n",
            "Epoch 237/300\n",
            "\n",
            "Epoch 00237: val_loss did not improve from 0.70147\n",
            "240/240 - 0s - loss: 0.2458 - acc: 0.9375 - val_loss: 0.8421 - val_acc: 0.7234\n",
            "Epoch 238/300\n",
            "\n",
            "Epoch 00238: val_loss improved from 0.70147 to 0.65992, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.2412 - acc: 0.9583 - val_loss: 0.6599 - val_acc: 0.7234\n",
            "Epoch 239/300\n",
            "\n",
            "Epoch 00239: val_loss did not improve from 0.65992\n",
            "240/240 - 0s - loss: 0.2041 - acc: 0.9542 - val_loss: 0.7918 - val_acc: 0.7660\n",
            "Epoch 240/300\n",
            "\n",
            "Epoch 00240: val_loss did not improve from 0.65992\n",
            "240/240 - 0s - loss: 0.2485 - acc: 0.9333 - val_loss: 0.7334 - val_acc: 0.7234\n",
            "Epoch 241/300\n",
            "\n",
            "Epoch 00241: val_loss did not improve from 0.65992\n",
            "240/240 - 0s - loss: 0.1987 - acc: 0.9750 - val_loss: 0.7254 - val_acc: 0.7447\n",
            "Epoch 242/300\n",
            "\n",
            "Epoch 00242: val_loss did not improve from 0.65992\n",
            "240/240 - 0s - loss: 0.1970 - acc: 0.9500 - val_loss: 0.7703 - val_acc: 0.7447\n",
            "Epoch 243/300\n",
            "\n",
            "Epoch 00243: val_loss did not improve from 0.65992\n",
            "240/240 - 0s - loss: 0.2008 - acc: 0.9542 - val_loss: 0.7745 - val_acc: 0.7234\n",
            "Epoch 244/300\n",
            "\n",
            "Epoch 00244: val_loss did not improve from 0.65992\n",
            "240/240 - 0s - loss: 0.2140 - acc: 0.9583 - val_loss: 0.7947 - val_acc: 0.7447\n",
            "Epoch 245/300\n",
            "\n",
            "Epoch 00245: val_loss did not improve from 0.65992\n",
            "240/240 - 0s - loss: 0.2272 - acc: 0.9542 - val_loss: 0.7302 - val_acc: 0.7234\n",
            "Epoch 246/300\n",
            "\n",
            "Epoch 00246: val_loss did not improve from 0.65992\n",
            "240/240 - 0s - loss: 0.1809 - acc: 0.9750 - val_loss: 0.8482 - val_acc: 0.7660\n",
            "Epoch 247/300\n",
            "\n",
            "Epoch 00247: val_loss did not improve from 0.65992\n",
            "240/240 - 0s - loss: 0.2071 - acc: 0.9667 - val_loss: 0.7561 - val_acc: 0.7660\n",
            "Epoch 248/300\n",
            "\n",
            "Epoch 00248: val_loss did not improve from 0.65992\n",
            "240/240 - 0s - loss: 0.1810 - acc: 0.9667 - val_loss: 0.7270 - val_acc: 0.7234\n",
            "Epoch 249/300\n",
            "\n",
            "Epoch 00249: val_loss did not improve from 0.65992\n",
            "240/240 - 0s - loss: 0.1823 - acc: 0.9708 - val_loss: 0.6789 - val_acc: 0.7234\n",
            "Epoch 250/300\n",
            "\n",
            "Epoch 00250: val_loss did not improve from 0.65992\n",
            "240/240 - 0s - loss: 0.2045 - acc: 0.9417 - val_loss: 0.6741 - val_acc: 0.7447\n",
            "Epoch 251/300\n",
            "\n",
            "Epoch 00251: val_loss did not improve from 0.65992\n",
            "240/240 - 0s - loss: 0.2356 - acc: 0.9375 - val_loss: 0.7411 - val_acc: 0.7021\n",
            "Epoch 252/300\n",
            "\n",
            "Epoch 00252: val_loss did not improve from 0.65992\n",
            "240/240 - 0s - loss: 0.1907 - acc: 0.9750 - val_loss: 0.7587 - val_acc: 0.7021\n",
            "Epoch 253/300\n",
            "\n",
            "Epoch 00253: val_loss did not improve from 0.65992\n",
            "240/240 - 0s - loss: 0.2153 - acc: 0.9625 - val_loss: 0.7660 - val_acc: 0.7234\n",
            "Epoch 254/300\n",
            "\n",
            "Epoch 00254: val_loss did not improve from 0.65992\n",
            "240/240 - 0s - loss: 0.2069 - acc: 0.9583 - val_loss: 0.7102 - val_acc: 0.7234\n",
            "Epoch 255/300\n",
            "\n",
            "Epoch 00255: val_loss did not improve from 0.65992\n",
            "240/240 - 0s - loss: 0.2116 - acc: 0.9542 - val_loss: 0.7489 - val_acc: 0.7021\n",
            "Epoch 256/300\n",
            "\n",
            "Epoch 00256: val_loss did not improve from 0.65992\n",
            "240/240 - 0s - loss: 0.2019 - acc: 0.9583 - val_loss: 0.7661 - val_acc: 0.6809\n",
            "Epoch 257/300\n",
            "\n",
            "Epoch 00257: val_loss did not improve from 0.65992\n",
            "240/240 - 0s - loss: 0.1710 - acc: 0.9875 - val_loss: 0.7487 - val_acc: 0.7234\n",
            "Epoch 258/300\n",
            "\n",
            "Epoch 00258: val_loss did not improve from 0.65992\n",
            "240/240 - 0s - loss: 0.2276 - acc: 0.9333 - val_loss: 0.7348 - val_acc: 0.7660\n",
            "Epoch 259/300\n",
            "\n",
            "Epoch 00259: val_loss did not improve from 0.65992\n",
            "240/240 - 0s - loss: 0.2224 - acc: 0.9667 - val_loss: 0.7724 - val_acc: 0.6809\n",
            "Epoch 260/300\n",
            "\n",
            "Epoch 00260: val_loss did not improve from 0.65992\n",
            "240/240 - 0s - loss: 0.2071 - acc: 0.9375 - val_loss: 0.8003 - val_acc: 0.7234\n",
            "Epoch 261/300\n",
            "\n",
            "Epoch 00261: val_loss did not improve from 0.65992\n",
            "240/240 - 0s - loss: 0.2167 - acc: 0.9333 - val_loss: 0.7634 - val_acc: 0.7021\n",
            "Epoch 262/300\n",
            "\n",
            "Epoch 00262: val_loss did not improve from 0.65992\n",
            "240/240 - 0s - loss: 0.2282 - acc: 0.9542 - val_loss: 0.7423 - val_acc: 0.7021\n",
            "Epoch 263/300\n",
            "\n",
            "Epoch 00263: val_loss did not improve from 0.65992\n",
            "240/240 - 0s - loss: 0.2002 - acc: 0.9500 - val_loss: 0.7465 - val_acc: 0.7872\n",
            "Epoch 264/300\n",
            "\n",
            "Epoch 00264: val_loss did not improve from 0.65992\n",
            "240/240 - 0s - loss: 0.2290 - acc: 0.9500 - val_loss: 0.7038 - val_acc: 0.7021\n",
            "Epoch 265/300\n",
            "\n",
            "Epoch 00265: val_loss did not improve from 0.65992\n",
            "240/240 - 0s - loss: 0.1989 - acc: 0.9542 - val_loss: 0.7012 - val_acc: 0.7021\n",
            "Epoch 266/300\n",
            "\n",
            "Epoch 00266: val_loss did not improve from 0.65992\n",
            "240/240 - 0s - loss: 0.2026 - acc: 0.9542 - val_loss: 0.6972 - val_acc: 0.6809\n",
            "Epoch 267/300\n",
            "\n",
            "Epoch 00267: val_loss did not improve from 0.65992\n",
            "240/240 - 0s - loss: 0.1986 - acc: 0.9542 - val_loss: 0.7039 - val_acc: 0.6809\n",
            "Epoch 268/300\n",
            "\n",
            "Epoch 00268: val_loss did not improve from 0.65992\n",
            "240/240 - 0s - loss: 0.2003 - acc: 0.9625 - val_loss: 0.7335 - val_acc: 0.7447\n",
            "Epoch 269/300\n",
            "\n",
            "Epoch 00269: val_loss did not improve from 0.65992\n",
            "240/240 - 0s - loss: 0.1738 - acc: 0.9792 - val_loss: 0.7293 - val_acc: 0.7234\n",
            "Epoch 270/300\n",
            "\n",
            "Epoch 00270: val_loss did not improve from 0.65992\n",
            "240/240 - 0s - loss: 0.1856 - acc: 0.9625 - val_loss: 0.7800 - val_acc: 0.7234\n",
            "Epoch 271/300\n",
            "\n",
            "Epoch 00271: val_loss did not improve from 0.65992\n",
            "240/240 - 0s - loss: 0.1938 - acc: 0.9625 - val_loss: 0.8056 - val_acc: 0.7021\n",
            "Epoch 272/300\n",
            "\n",
            "Epoch 00272: val_loss did not improve from 0.65992\n",
            "240/240 - 0s - loss: 0.1952 - acc: 0.9667 - val_loss: 0.7791 - val_acc: 0.7021\n",
            "Epoch 273/300\n",
            "\n",
            "Epoch 00273: val_loss did not improve from 0.65992\n",
            "240/240 - 0s - loss: 0.1941 - acc: 0.9625 - val_loss: 0.7013 - val_acc: 0.7447\n",
            "Epoch 274/300\n",
            "\n",
            "Epoch 00274: val_loss did not improve from 0.65992\n",
            "240/240 - 0s - loss: 0.1888 - acc: 0.9625 - val_loss: 0.7305 - val_acc: 0.7234\n",
            "Epoch 275/300\n",
            "\n",
            "Epoch 00275: val_loss did not improve from 0.65992\n",
            "240/240 - 0s - loss: 0.1691 - acc: 0.9750 - val_loss: 0.6975 - val_acc: 0.7234\n",
            "Epoch 276/300\n",
            "\n",
            "Epoch 00276: val_loss did not improve from 0.65992\n",
            "240/240 - 0s - loss: 0.1768 - acc: 0.9708 - val_loss: 0.7017 - val_acc: 0.7234\n",
            "Epoch 277/300\n",
            "\n",
            "Epoch 00277: val_loss did not improve from 0.65992\n",
            "240/240 - 0s - loss: 0.1591 - acc: 0.9708 - val_loss: 0.7364 - val_acc: 0.7234\n",
            "Epoch 278/300\n",
            "\n",
            "Epoch 00278: val_loss did not improve from 0.65992\n",
            "240/240 - 0s - loss: 0.1892 - acc: 0.9625 - val_loss: 0.7457 - val_acc: 0.7447\n",
            "Epoch 279/300\n",
            "\n",
            "Epoch 00279: val_loss did not improve from 0.65992\n",
            "240/240 - 0s - loss: 0.1674 - acc: 0.9708 - val_loss: 0.7394 - val_acc: 0.7021\n",
            "Epoch 280/300\n",
            "\n",
            "Epoch 00280: val_loss did not improve from 0.65992\n",
            "240/240 - 0s - loss: 0.1699 - acc: 0.9792 - val_loss: 0.8024 - val_acc: 0.7021\n",
            "Epoch 281/300\n",
            "\n",
            "Epoch 00281: val_loss did not improve from 0.65992\n",
            "240/240 - 0s - loss: 0.1900 - acc: 0.9792 - val_loss: 0.7961 - val_acc: 0.7234\n",
            "Epoch 282/300\n",
            "\n",
            "Epoch 00282: val_loss did not improve from 0.65992\n",
            "240/240 - 0s - loss: 0.1688 - acc: 0.9542 - val_loss: 0.7754 - val_acc: 0.7447\n",
            "Epoch 283/300\n",
            "\n",
            "Epoch 00283: val_loss did not improve from 0.65992\n",
            "240/240 - 0s - loss: 0.1876 - acc: 0.9458 - val_loss: 0.7481 - val_acc: 0.7234\n",
            "Epoch 284/300\n",
            "\n",
            "Epoch 00284: val_loss did not improve from 0.65992\n",
            "240/240 - 0s - loss: 0.1831 - acc: 0.9583 - val_loss: 0.6783 - val_acc: 0.7447\n",
            "Epoch 285/300\n",
            "\n",
            "Epoch 00285: val_loss did not improve from 0.65992\n",
            "240/240 - 0s - loss: 0.2064 - acc: 0.9583 - val_loss: 0.7020 - val_acc: 0.7447\n",
            "Epoch 286/300\n",
            "\n",
            "Epoch 00286: val_loss did not improve from 0.65992\n",
            "240/240 - 0s - loss: 0.1667 - acc: 0.9708 - val_loss: 0.8691 - val_acc: 0.7447\n",
            "Epoch 287/300\n",
            "\n",
            "Epoch 00287: val_loss did not improve from 0.65992\n",
            "240/240 - 0s - loss: 0.1840 - acc: 0.9500 - val_loss: 0.8027 - val_acc: 0.7447\n",
            "Epoch 288/300\n",
            "\n",
            "Epoch 00288: val_loss did not improve from 0.65992\n",
            "240/240 - 0s - loss: 0.1666 - acc: 0.9792 - val_loss: 0.7858 - val_acc: 0.7234\n",
            "Epoch 289/300\n",
            "\n",
            "Epoch 00289: val_loss did not improve from 0.65992\n",
            "240/240 - 0s - loss: 0.1930 - acc: 0.9708 - val_loss: 0.7317 - val_acc: 0.7021\n",
            "Epoch 290/300\n",
            "\n",
            "Epoch 00290: val_loss did not improve from 0.65992\n",
            "240/240 - 0s - loss: 0.1801 - acc: 0.9583 - val_loss: 0.8308 - val_acc: 0.7234\n",
            "Epoch 291/300\n",
            "\n",
            "Epoch 00291: val_loss did not improve from 0.65992\n",
            "240/240 - 0s - loss: 0.1748 - acc: 0.9667 - val_loss: 0.7320 - val_acc: 0.7234\n",
            "Epoch 292/300\n",
            "\n",
            "Epoch 00292: val_loss did not improve from 0.65992\n",
            "240/240 - 0s - loss: 0.1651 - acc: 0.9667 - val_loss: 0.8108 - val_acc: 0.7234\n",
            "Epoch 293/300\n",
            "\n",
            "Epoch 00293: val_loss did not improve from 0.65992\n",
            "240/240 - 0s - loss: 0.1657 - acc: 0.9667 - val_loss: 0.7848 - val_acc: 0.7447\n",
            "Epoch 294/300\n",
            "\n",
            "Epoch 00294: val_loss did not improve from 0.65992\n",
            "240/240 - 0s - loss: 0.1669 - acc: 0.9833 - val_loss: 0.7255 - val_acc: 0.7021\n",
            "Epoch 295/300\n",
            "\n",
            "Epoch 00295: val_loss did not improve from 0.65992\n",
            "240/240 - 0s - loss: 0.1654 - acc: 0.9708 - val_loss: 0.7494 - val_acc: 0.7447\n",
            "Epoch 296/300\n",
            "\n",
            "Epoch 00296: val_loss did not improve from 0.65992\n",
            "240/240 - 0s - loss: 0.1419 - acc: 0.9833 - val_loss: 0.7595 - val_acc: 0.7234\n",
            "Epoch 297/300\n",
            "\n",
            "Epoch 00297: val_loss did not improve from 0.65992\n",
            "240/240 - 0s - loss: 0.2040 - acc: 0.9500 - val_loss: 0.7597 - val_acc: 0.7234\n",
            "Epoch 298/300\n",
            "\n",
            "Epoch 00298: val_loss did not improve from 0.65992\n",
            "240/240 - 0s - loss: 0.1687 - acc: 0.9792 - val_loss: 0.7565 - val_acc: 0.7021\n",
            "Epoch 299/300\n",
            "\n",
            "Epoch 00299: val_loss did not improve from 0.65992\n",
            "240/240 - 0s - loss: 0.1767 - acc: 0.9625 - val_loss: 0.7203 - val_acc: 0.7234\n",
            "Epoch 300/300\n",
            "\n",
            "Epoch 00300: val_loss did not improve from 0.65992\n",
            "240/240 - 0s - loss: 0.1669 - acc: 0.9708 - val_loss: 0.7958 - val_acc: 0.7021\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\npggMwzASzv8Hi7Uy2tHJd7AAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "# Evaluate on test data\n",
            "288/288 [==============================] - 1s 2ms/sample - loss: 0.5094 - acc: 0.7882\n",
            "test loss, test acc: [0.5093975289180688, 0.7881944]\n",
            "EEG_Deep/Data2A/Data_A02T.mat\n",
            "(288, 22, 1875)\n",
            "(288, 1)\n",
            "(240, 22, 750)\n",
            "(240, 1)\n",
            "(47, 22, 750)\n",
            "(47, 1)\n",
            "X_train shape: (240, 1, 22, 750)\n",
            "240 train samples\n",
            "47 val samples\n",
            "EEG_Deep/Data2A/Data_A02E.mat\n",
            "(288, 22, 1875)\n",
            "(288, 1)\n",
            "(288, 22, 750)\n",
            "(288, 1)\n",
            "X_train shape: (288, 1, 22, 750)\n",
            "288 train samples\n",
            "Train on 240 samples, validate on 47 samples\n",
            "Epoch 1/300\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 1.38547, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 1s - loss: 1.3864 - acc: 0.2667 - val_loss: 1.3855 - val_acc: 0.2340\n",
            "Epoch 2/300\n",
            "\n",
            "Epoch 00002: val_loss improved from 1.38547 to 1.38493, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.3572 - acc: 0.4000 - val_loss: 1.3849 - val_acc: 0.2766\n",
            "Epoch 3/300\n",
            "\n",
            "Epoch 00003: val_loss improved from 1.38493 to 1.38397, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.3398 - acc: 0.4333 - val_loss: 1.3840 - val_acc: 0.2553\n",
            "Epoch 4/300\n",
            "\n",
            "Epoch 00004: val_loss improved from 1.38397 to 1.38339, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.3037 - acc: 0.5917 - val_loss: 1.3834 - val_acc: 0.1915\n",
            "Epoch 5/300\n",
            "\n",
            "Epoch 00005: val_loss improved from 1.38339 to 1.38267, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.2904 - acc: 0.5500 - val_loss: 1.3827 - val_acc: 0.2128\n",
            "Epoch 6/300\n",
            "\n",
            "Epoch 00006: val_loss improved from 1.38267 to 1.38035, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.2839 - acc: 0.4875 - val_loss: 1.3804 - val_acc: 0.2128\n",
            "Epoch 7/300\n",
            "\n",
            "Epoch 00007: val_loss improved from 1.38035 to 1.37967, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.2537 - acc: 0.5792 - val_loss: 1.3797 - val_acc: 0.2128\n",
            "Epoch 8/300\n",
            "\n",
            "Epoch 00008: val_loss improved from 1.37967 to 1.37715, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.2348 - acc: 0.5625 - val_loss: 1.3772 - val_acc: 0.2766\n",
            "Epoch 9/300\n",
            "\n",
            "Epoch 00009: val_loss improved from 1.37715 to 1.37500, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.2297 - acc: 0.5625 - val_loss: 1.3750 - val_acc: 0.3617\n",
            "Epoch 10/300\n",
            "\n",
            "Epoch 00010: val_loss improved from 1.37500 to 1.37302, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.2065 - acc: 0.5750 - val_loss: 1.3730 - val_acc: 0.3617\n",
            "Epoch 11/300\n",
            "\n",
            "Epoch 00011: val_loss improved from 1.37302 to 1.36755, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.1935 - acc: 0.6125 - val_loss: 1.3675 - val_acc: 0.3830\n",
            "Epoch 12/300\n",
            "\n",
            "Epoch 00012: val_loss improved from 1.36755 to 1.36398, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.1718 - acc: 0.5792 - val_loss: 1.3640 - val_acc: 0.3830\n",
            "Epoch 13/300\n",
            "\n",
            "Epoch 00013: val_loss improved from 1.36398 to 1.35927, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.1640 - acc: 0.6167 - val_loss: 1.3593 - val_acc: 0.3830\n",
            "Epoch 14/300\n",
            "\n",
            "Epoch 00014: val_loss improved from 1.35927 to 1.35727, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.1388 - acc: 0.6125 - val_loss: 1.3573 - val_acc: 0.4468\n",
            "Epoch 15/300\n",
            "\n",
            "Epoch 00015: val_loss improved from 1.35727 to 1.35200, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.1358 - acc: 0.6208 - val_loss: 1.3520 - val_acc: 0.3830\n",
            "Epoch 16/300\n",
            "\n",
            "Epoch 00016: val_loss improved from 1.35200 to 1.34759, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.1116 - acc: 0.6458 - val_loss: 1.3476 - val_acc: 0.3617\n",
            "Epoch 17/300\n",
            "\n",
            "Epoch 00017: val_loss improved from 1.34759 to 1.33880, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.0941 - acc: 0.6583 - val_loss: 1.3388 - val_acc: 0.3404\n",
            "Epoch 18/300\n",
            "\n",
            "Epoch 00018: val_loss improved from 1.33880 to 1.33552, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.0879 - acc: 0.6542 - val_loss: 1.3355 - val_acc: 0.2979\n",
            "Epoch 19/300\n",
            "\n",
            "Epoch 00019: val_loss improved from 1.33552 to 1.32211, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.0858 - acc: 0.6250 - val_loss: 1.3221 - val_acc: 0.3404\n",
            "Epoch 20/300\n",
            "\n",
            "Epoch 00020: val_loss improved from 1.32211 to 1.31725, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.0672 - acc: 0.6708 - val_loss: 1.3172 - val_acc: 0.3617\n",
            "Epoch 21/300\n",
            "\n",
            "Epoch 00021: val_loss improved from 1.31725 to 1.29911, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.0675 - acc: 0.6375 - val_loss: 1.2991 - val_acc: 0.3830\n",
            "Epoch 22/300\n",
            "\n",
            "Epoch 00022: val_loss improved from 1.29911 to 1.28991, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.0369 - acc: 0.6875 - val_loss: 1.2899 - val_acc: 0.3830\n",
            "Epoch 23/300\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 1.28991\n",
            "240/240 - 0s - loss: 1.0272 - acc: 0.7167 - val_loss: 1.2961 - val_acc: 0.3830\n",
            "Epoch 24/300\n",
            "\n",
            "Epoch 00024: val_loss improved from 1.28991 to 1.28162, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.0342 - acc: 0.6958 - val_loss: 1.2816 - val_acc: 0.4255\n",
            "Epoch 25/300\n",
            "\n",
            "Epoch 00025: val_loss improved from 1.28162 to 1.26007, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.0228 - acc: 0.7250 - val_loss: 1.2601 - val_acc: 0.4681\n",
            "Epoch 26/300\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 1.26007\n",
            "240/240 - 0s - loss: 0.9956 - acc: 0.7250 - val_loss: 1.2631 - val_acc: 0.4681\n",
            "Epoch 27/300\n",
            "\n",
            "Epoch 00027: val_loss improved from 1.26007 to 1.25712, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.0026 - acc: 0.6875 - val_loss: 1.2571 - val_acc: 0.5319\n",
            "Epoch 28/300\n",
            "\n",
            "Epoch 00028: val_loss improved from 1.25712 to 1.24914, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.0016 - acc: 0.7042 - val_loss: 1.2491 - val_acc: 0.4468\n",
            "Epoch 29/300\n",
            "\n",
            "Epoch 00029: val_loss improved from 1.24914 to 1.23883, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.9798 - acc: 0.6917 - val_loss: 1.2388 - val_acc: 0.4681\n",
            "Epoch 30/300\n",
            "\n",
            "Epoch 00030: val_loss improved from 1.23883 to 1.21748, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.9628 - acc: 0.7583 - val_loss: 1.2175 - val_acc: 0.5319\n",
            "Epoch 31/300\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 1.21748\n",
            "240/240 - 0s - loss: 0.9690 - acc: 0.6833 - val_loss: 1.2288 - val_acc: 0.4681\n",
            "Epoch 32/300\n",
            "\n",
            "Epoch 00032: val_loss improved from 1.21748 to 1.20910, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.9552 - acc: 0.7458 - val_loss: 1.2091 - val_acc: 0.5106\n",
            "Epoch 33/300\n",
            "\n",
            "Epoch 00033: val_loss improved from 1.20910 to 1.19192, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.9344 - acc: 0.7333 - val_loss: 1.1919 - val_acc: 0.5106\n",
            "Epoch 34/300\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 1.19192\n",
            "240/240 - 0s - loss: 0.9403 - acc: 0.7458 - val_loss: 1.1961 - val_acc: 0.4894\n",
            "Epoch 35/300\n",
            "\n",
            "Epoch 00035: val_loss improved from 1.19192 to 1.18264, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.9294 - acc: 0.7667 - val_loss: 1.1826 - val_acc: 0.4681\n",
            "Epoch 36/300\n",
            "\n",
            "Epoch 00036: val_loss improved from 1.18264 to 1.17620, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.9100 - acc: 0.7833 - val_loss: 1.1762 - val_acc: 0.5106\n",
            "Epoch 37/300\n",
            "\n",
            "Epoch 00037: val_loss improved from 1.17620 to 1.15526, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.9202 - acc: 0.7667 - val_loss: 1.1553 - val_acc: 0.5532\n",
            "Epoch 38/300\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 1.15526\n",
            "240/240 - 0s - loss: 0.9231 - acc: 0.7500 - val_loss: 1.1640 - val_acc: 0.5106\n",
            "Epoch 39/300\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 1.15526\n",
            "240/240 - 0s - loss: 0.8846 - acc: 0.7667 - val_loss: 1.1629 - val_acc: 0.5532\n",
            "Epoch 40/300\n",
            "\n",
            "Epoch 00040: val_loss improved from 1.15526 to 1.15140, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.8803 - acc: 0.7667 - val_loss: 1.1514 - val_acc: 0.5532\n",
            "Epoch 41/300\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 1.15140\n",
            "240/240 - 0s - loss: 0.8865 - acc: 0.7708 - val_loss: 1.1542 - val_acc: 0.5106\n",
            "Epoch 42/300\n",
            "\n",
            "Epoch 00042: val_loss improved from 1.15140 to 1.13387, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.8903 - acc: 0.7375 - val_loss: 1.1339 - val_acc: 0.5745\n",
            "Epoch 43/300\n",
            "\n",
            "Epoch 00043: val_loss improved from 1.13387 to 1.13245, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.8746 - acc: 0.7583 - val_loss: 1.1324 - val_acc: 0.5532\n",
            "Epoch 44/300\n",
            "\n",
            "Epoch 00044: val_loss improved from 1.13245 to 1.12797, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.8511 - acc: 0.7917 - val_loss: 1.1280 - val_acc: 0.5532\n",
            "Epoch 45/300\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 1.12797\n",
            "240/240 - 0s - loss: 0.8748 - acc: 0.7667 - val_loss: 1.1439 - val_acc: 0.5532\n",
            "Epoch 46/300\n",
            "\n",
            "Epoch 00046: val_loss improved from 1.12797 to 1.10323, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.8770 - acc: 0.7500 - val_loss: 1.1032 - val_acc: 0.5319\n",
            "Epoch 47/300\n",
            "\n",
            "Epoch 00047: val_loss improved from 1.10323 to 1.10073, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.8519 - acc: 0.7750 - val_loss: 1.1007 - val_acc: 0.5957\n",
            "Epoch 48/300\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 1.10073\n",
            "240/240 - 0s - loss: 0.8574 - acc: 0.7667 - val_loss: 1.1137 - val_acc: 0.5532\n",
            "Epoch 49/300\n",
            "\n",
            "Epoch 00049: val_loss improved from 1.10073 to 1.09711, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.8567 - acc: 0.7667 - val_loss: 1.0971 - val_acc: 0.5319\n",
            "Epoch 50/300\n",
            "\n",
            "Epoch 00050: val_loss improved from 1.09711 to 1.09291, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.8501 - acc: 0.7583 - val_loss: 1.0929 - val_acc: 0.5319\n",
            "Epoch 51/300\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 1.09291\n",
            "240/240 - 0s - loss: 0.8552 - acc: 0.7708 - val_loss: 1.0958 - val_acc: 0.5532\n",
            "Epoch 52/300\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 1.09291\n",
            "240/240 - 0s - loss: 0.8345 - acc: 0.7583 - val_loss: 1.1112 - val_acc: 0.5532\n",
            "Epoch 53/300\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 1.09291\n",
            "240/240 - 0s - loss: 0.8466 - acc: 0.8125 - val_loss: 1.1109 - val_acc: 0.5319\n",
            "Epoch 54/300\n",
            "\n",
            "Epoch 00054: val_loss improved from 1.09291 to 1.09051, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.8221 - acc: 0.7500 - val_loss: 1.0905 - val_acc: 0.5957\n",
            "Epoch 55/300\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 1.09051\n",
            "240/240 - 0s - loss: 0.8146 - acc: 0.7708 - val_loss: 1.1051 - val_acc: 0.5745\n",
            "Epoch 56/300\n",
            "\n",
            "Epoch 00056: val_loss improved from 1.09051 to 1.08802, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.8126 - acc: 0.7583 - val_loss: 1.0880 - val_acc: 0.5957\n",
            "Epoch 57/300\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 1.08802\n",
            "240/240 - 0s - loss: 0.8070 - acc: 0.8167 - val_loss: 1.1087 - val_acc: 0.5532\n",
            "Epoch 58/300\n",
            "\n",
            "Epoch 00058: val_loss improved from 1.08802 to 1.07961, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.8202 - acc: 0.7917 - val_loss: 1.0796 - val_acc: 0.5745\n",
            "Epoch 59/300\n",
            "\n",
            "Epoch 00059: val_loss improved from 1.07961 to 1.07403, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.8014 - acc: 0.7750 - val_loss: 1.0740 - val_acc: 0.6383\n",
            "Epoch 60/300\n",
            "\n",
            "Epoch 00060: val_loss improved from 1.07403 to 1.05327, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.8062 - acc: 0.7792 - val_loss: 1.0533 - val_acc: 0.5957\n",
            "Epoch 61/300\n",
            "\n",
            "Epoch 00061: val_loss improved from 1.05327 to 1.04699, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.7634 - acc: 0.8208 - val_loss: 1.0470 - val_acc: 0.6170\n",
            "Epoch 62/300\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 1.04699\n",
            "240/240 - 0s - loss: 0.7764 - acc: 0.8125 - val_loss: 1.0516 - val_acc: 0.5957\n",
            "Epoch 63/300\n",
            "\n",
            "Epoch 00063: val_loss improved from 1.04699 to 1.04359, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.7790 - acc: 0.7875 - val_loss: 1.0436 - val_acc: 0.6383\n",
            "Epoch 64/300\n",
            "\n",
            "Epoch 00064: val_loss improved from 1.04359 to 1.03298, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.7828 - acc: 0.7833 - val_loss: 1.0330 - val_acc: 0.6383\n",
            "Epoch 65/300\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 1.03298\n",
            "240/240 - 0s - loss: 0.7614 - acc: 0.8125 - val_loss: 1.0331 - val_acc: 0.6383\n",
            "Epoch 66/300\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 1.03298\n",
            "240/240 - 0s - loss: 0.7851 - acc: 0.8000 - val_loss: 1.0586 - val_acc: 0.5957\n",
            "Epoch 67/300\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 1.03298\n",
            "240/240 - 0s - loss: 0.8044 - acc: 0.7708 - val_loss: 1.0413 - val_acc: 0.6170\n",
            "Epoch 68/300\n",
            "\n",
            "Epoch 00068: val_loss improved from 1.03298 to 1.02667, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.7446 - acc: 0.8125 - val_loss: 1.0267 - val_acc: 0.6383\n",
            "Epoch 69/300\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 1.02667\n",
            "240/240 - 0s - loss: 0.7616 - acc: 0.8000 - val_loss: 1.0382 - val_acc: 0.6383\n",
            "Epoch 70/300\n",
            "\n",
            "Epoch 00070: val_loss improved from 1.02667 to 1.02454, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.7685 - acc: 0.7875 - val_loss: 1.0245 - val_acc: 0.5957\n",
            "Epoch 71/300\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 1.02454\n",
            "240/240 - 0s - loss: 0.7286 - acc: 0.8125 - val_loss: 1.0370 - val_acc: 0.6383\n",
            "Epoch 72/300\n",
            "\n",
            "Epoch 00072: val_loss improved from 1.02454 to 1.01426, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.7513 - acc: 0.8250 - val_loss: 1.0143 - val_acc: 0.5745\n",
            "Epoch 73/300\n",
            "\n",
            "Epoch 00073: val_loss improved from 1.01426 to 1.00796, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.7623 - acc: 0.7792 - val_loss: 1.0080 - val_acc: 0.6596\n",
            "Epoch 74/300\n",
            "\n",
            "Epoch 00074: val_loss improved from 1.00796 to 0.98969, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.7399 - acc: 0.7875 - val_loss: 0.9897 - val_acc: 0.6170\n",
            "Epoch 75/300\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.98969\n",
            "240/240 - 0s - loss: 0.7322 - acc: 0.8042 - val_loss: 0.9993 - val_acc: 0.6383\n",
            "Epoch 76/300\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.98969\n",
            "240/240 - 0s - loss: 0.7213 - acc: 0.8292 - val_loss: 0.9985 - val_acc: 0.6170\n",
            "Epoch 77/300\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.98969\n",
            "240/240 - 0s - loss: 0.7596 - acc: 0.7958 - val_loss: 1.0072 - val_acc: 0.6170\n",
            "Epoch 78/300\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.98969\n",
            "240/240 - 0s - loss: 0.7286 - acc: 0.8167 - val_loss: 1.0248 - val_acc: 0.6383\n",
            "Epoch 79/300\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.98969\n",
            "240/240 - 0s - loss: 0.7398 - acc: 0.7875 - val_loss: 1.0141 - val_acc: 0.6383\n",
            "Epoch 80/300\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.98969\n",
            "240/240 - 0s - loss: 0.7471 - acc: 0.8000 - val_loss: 0.9950 - val_acc: 0.6809\n",
            "Epoch 81/300\n",
            "\n",
            "Epoch 00081: val_loss improved from 0.98969 to 0.98326, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.6989 - acc: 0.8208 - val_loss: 0.9833 - val_acc: 0.6383\n",
            "Epoch 82/300\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.98326\n",
            "240/240 - 0s - loss: 0.6882 - acc: 0.8417 - val_loss: 1.0044 - val_acc: 0.6383\n",
            "Epoch 83/300\n",
            "\n",
            "Epoch 00083: val_loss improved from 0.98326 to 0.97030, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.6992 - acc: 0.8292 - val_loss: 0.9703 - val_acc: 0.6596\n",
            "Epoch 84/300\n",
            "\n",
            "Epoch 00084: val_loss improved from 0.97030 to 0.94825, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.7278 - acc: 0.8042 - val_loss: 0.9482 - val_acc: 0.6596\n",
            "Epoch 85/300\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.94825\n",
            "240/240 - 0s - loss: 0.7158 - acc: 0.8000 - val_loss: 0.9547 - val_acc: 0.7234\n",
            "Epoch 86/300\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.94825\n",
            "240/240 - 0s - loss: 0.6909 - acc: 0.8292 - val_loss: 0.9694 - val_acc: 0.6383\n",
            "Epoch 87/300\n",
            "\n",
            "Epoch 00087: val_loss improved from 0.94825 to 0.91144, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.6903 - acc: 0.8083 - val_loss: 0.9114 - val_acc: 0.7660\n",
            "Epoch 88/300\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.91144\n",
            "240/240 - 0s - loss: 0.6917 - acc: 0.8292 - val_loss: 0.9615 - val_acc: 0.5745\n",
            "Epoch 89/300\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.91144\n",
            "240/240 - 0s - loss: 0.7153 - acc: 0.7917 - val_loss: 1.0002 - val_acc: 0.6383\n",
            "Epoch 90/300\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.91144\n",
            "240/240 - 0s - loss: 0.7031 - acc: 0.8333 - val_loss: 0.9671 - val_acc: 0.6596\n",
            "Epoch 91/300\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.91144\n",
            "240/240 - 0s - loss: 0.6991 - acc: 0.7917 - val_loss: 0.9394 - val_acc: 0.6596\n",
            "Epoch 92/300\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.91144\n",
            "240/240 - 0s - loss: 0.6679 - acc: 0.8167 - val_loss: 0.9637 - val_acc: 0.7021\n",
            "Epoch 93/300\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.91144\n",
            "240/240 - 0s - loss: 0.6676 - acc: 0.8250 - val_loss: 0.9585 - val_acc: 0.5957\n",
            "Epoch 94/300\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.91144\n",
            "240/240 - 0s - loss: 0.6503 - acc: 0.8500 - val_loss: 0.9541 - val_acc: 0.7021\n",
            "Epoch 95/300\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.91144\n",
            "240/240 - 0s - loss: 0.6575 - acc: 0.8167 - val_loss: 0.9397 - val_acc: 0.6170\n",
            "Epoch 96/300\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.91144\n",
            "240/240 - 0s - loss: 0.6433 - acc: 0.8375 - val_loss: 0.9320 - val_acc: 0.6383\n",
            "Epoch 97/300\n",
            "\n",
            "Epoch 00097: val_loss improved from 0.91144 to 0.90518, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.6507 - acc: 0.8375 - val_loss: 0.9052 - val_acc: 0.7234\n",
            "Epoch 98/300\n",
            "\n",
            "Epoch 00098: val_loss improved from 0.90518 to 0.88292, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.6405 - acc: 0.8417 - val_loss: 0.8829 - val_acc: 0.7021\n",
            "Epoch 99/300\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.88292\n",
            "240/240 - 0s - loss: 0.6694 - acc: 0.8250 - val_loss: 0.8847 - val_acc: 0.7660\n",
            "Epoch 100/300\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.88292\n",
            "240/240 - 0s - loss: 0.6198 - acc: 0.8375 - val_loss: 0.8924 - val_acc: 0.7021\n",
            "Epoch 101/300\n",
            "\n",
            "Epoch 00101: val_loss improved from 0.88292 to 0.87408, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.6059 - acc: 0.8375 - val_loss: 0.8741 - val_acc: 0.7872\n",
            "Epoch 102/300\n",
            "\n",
            "Epoch 00102: val_loss improved from 0.87408 to 0.86572, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.6039 - acc: 0.8625 - val_loss: 0.8657 - val_acc: 0.6809\n",
            "Epoch 103/300\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.86572\n",
            "240/240 - 0s - loss: 0.5879 - acc: 0.8625 - val_loss: 0.8752 - val_acc: 0.7447\n",
            "Epoch 104/300\n",
            "\n",
            "Epoch 00104: val_loss improved from 0.86572 to 0.85819, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.6042 - acc: 0.8583 - val_loss: 0.8582 - val_acc: 0.8085\n",
            "Epoch 105/300\n",
            "\n",
            "Epoch 00105: val_loss improved from 0.85819 to 0.85651, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.6162 - acc: 0.8250 - val_loss: 0.8565 - val_acc: 0.7447\n",
            "Epoch 106/300\n",
            "\n",
            "Epoch 00106: val_loss improved from 0.85651 to 0.85129, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.5814 - acc: 0.8750 - val_loss: 0.8513 - val_acc: 0.7021\n",
            "Epoch 107/300\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.85129\n",
            "240/240 - 0s - loss: 0.5785 - acc: 0.8583 - val_loss: 0.8673 - val_acc: 0.7447\n",
            "Epoch 108/300\n",
            "\n",
            "Epoch 00108: val_loss improved from 0.85129 to 0.84468, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.5757 - acc: 0.8708 - val_loss: 0.8447 - val_acc: 0.7660\n",
            "Epoch 109/300\n",
            "\n",
            "Epoch 00109: val_loss improved from 0.84468 to 0.83657, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.5537 - acc: 0.8958 - val_loss: 0.8366 - val_acc: 0.7660\n",
            "Epoch 110/300\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.83657\n",
            "240/240 - 0s - loss: 0.5607 - acc: 0.8542 - val_loss: 0.8993 - val_acc: 0.7447\n",
            "Epoch 111/300\n",
            "\n",
            "Epoch 00111: val_loss improved from 0.83657 to 0.82812, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.5695 - acc: 0.8458 - val_loss: 0.8281 - val_acc: 0.7872\n",
            "Epoch 112/300\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 0.82812\n",
            "240/240 - 0s - loss: 0.5713 - acc: 0.8500 - val_loss: 0.8483 - val_acc: 0.7660\n",
            "Epoch 113/300\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.82812\n",
            "240/240 - 0s - loss: 0.5427 - acc: 0.8792 - val_loss: 0.8663 - val_acc: 0.7872\n",
            "Epoch 114/300\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.82812\n",
            "240/240 - 0s - loss: 0.5511 - acc: 0.8875 - val_loss: 0.8635 - val_acc: 0.7660\n",
            "Epoch 115/300\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.82812\n",
            "240/240 - 0s - loss: 0.5523 - acc: 0.8667 - val_loss: 0.8558 - val_acc: 0.6596\n",
            "Epoch 116/300\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.82812\n",
            "240/240 - 0s - loss: 0.5522 - acc: 0.8792 - val_loss: 0.8628 - val_acc: 0.8298\n",
            "Epoch 117/300\n",
            "\n",
            "Epoch 00117: val_loss improved from 0.82812 to 0.81532, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.5503 - acc: 0.8958 - val_loss: 0.8153 - val_acc: 0.7872\n",
            "Epoch 118/300\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.81532\n",
            "240/240 - 0s - loss: 0.5591 - acc: 0.8667 - val_loss: 0.8718 - val_acc: 0.7021\n",
            "Epoch 119/300\n",
            "\n",
            "Epoch 00119: val_loss improved from 0.81532 to 0.81482, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.5518 - acc: 0.8333 - val_loss: 0.8148 - val_acc: 0.8511\n",
            "Epoch 120/300\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.81482\n",
            "240/240 - 0s - loss: 0.5304 - acc: 0.8917 - val_loss: 0.8564 - val_acc: 0.7447\n",
            "Epoch 121/300\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.81482\n",
            "240/240 - 0s - loss: 0.5289 - acc: 0.8833 - val_loss: 0.8368 - val_acc: 0.7660\n",
            "Epoch 122/300\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.81482\n",
            "240/240 - 0s - loss: 0.5319 - acc: 0.8583 - val_loss: 0.8420 - val_acc: 0.7660\n",
            "Epoch 123/300\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 0.81482\n",
            "240/240 - 0s - loss: 0.5154 - acc: 0.8917 - val_loss: 0.8539 - val_acc: 0.6596\n",
            "Epoch 124/300\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 0.81482\n",
            "240/240 - 0s - loss: 0.5184 - acc: 0.8792 - val_loss: 0.8489 - val_acc: 0.7234\n",
            "Epoch 125/300\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 0.81482\n",
            "240/240 - 0s - loss: 0.4821 - acc: 0.8917 - val_loss: 0.8422 - val_acc: 0.7021\n",
            "Epoch 126/300\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 0.81482\n",
            "240/240 - 0s - loss: 0.5302 - acc: 0.8542 - val_loss: 0.8550 - val_acc: 0.7660\n",
            "Epoch 127/300\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 0.81482\n",
            "240/240 - 0s - loss: 0.5071 - acc: 0.8875 - val_loss: 0.8866 - val_acc: 0.6596\n",
            "Epoch 128/300\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 0.81482\n",
            "240/240 - 0s - loss: 0.5146 - acc: 0.8750 - val_loss: 0.8398 - val_acc: 0.7660\n",
            "Epoch 129/300\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.81482\n",
            "240/240 - 0s - loss: 0.5129 - acc: 0.8667 - val_loss: 0.8539 - val_acc: 0.7447\n",
            "Epoch 130/300\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 0.81482\n",
            "240/240 - 0s - loss: 0.5164 - acc: 0.8750 - val_loss: 0.8418 - val_acc: 0.8298\n",
            "Epoch 131/300\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 0.81482\n",
            "240/240 - 0s - loss: 0.4967 - acc: 0.8792 - val_loss: 0.8307 - val_acc: 0.7021\n",
            "Epoch 132/300\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 0.81482\n",
            "240/240 - 0s - loss: 0.5157 - acc: 0.8667 - val_loss: 0.8535 - val_acc: 0.7021\n",
            "Epoch 133/300\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.81482\n",
            "240/240 - 0s - loss: 0.5021 - acc: 0.8875 - val_loss: 0.8397 - val_acc: 0.7234\n",
            "Epoch 134/300\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.81482\n",
            "240/240 - 0s - loss: 0.5235 - acc: 0.8750 - val_loss: 0.8325 - val_acc: 0.8298\n",
            "Epoch 135/300\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 0.81482\n",
            "240/240 - 0s - loss: 0.5025 - acc: 0.8792 - val_loss: 0.8226 - val_acc: 0.7447\n",
            "Epoch 136/300\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.81482\n",
            "240/240 - 0s - loss: 0.4816 - acc: 0.9042 - val_loss: 0.8404 - val_acc: 0.7660\n",
            "Epoch 137/300\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 0.81482\n",
            "240/240 - 0s - loss: 0.5271 - acc: 0.8667 - val_loss: 0.8389 - val_acc: 0.8085\n",
            "Epoch 138/300\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.81482\n",
            "240/240 - 0s - loss: 0.4837 - acc: 0.8958 - val_loss: 0.8431 - val_acc: 0.7021\n",
            "Epoch 139/300\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.81482\n",
            "240/240 - 0s - loss: 0.4810 - acc: 0.8708 - val_loss: 0.8167 - val_acc: 0.8298\n",
            "Epoch 140/300\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 0.81482\n",
            "240/240 - 0s - loss: 0.4815 - acc: 0.9000 - val_loss: 0.8433 - val_acc: 0.7021\n",
            "Epoch 141/300\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.81482\n",
            "240/240 - 0s - loss: 0.5164 - acc: 0.8458 - val_loss: 0.8206 - val_acc: 0.7872\n",
            "Epoch 142/300\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.81482\n",
            "240/240 - 0s - loss: 0.4835 - acc: 0.8583 - val_loss: 0.8438 - val_acc: 0.7021\n",
            "Epoch 143/300\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 0.81482\n",
            "240/240 - 0s - loss: 0.4782 - acc: 0.8792 - val_loss: 0.8374 - val_acc: 0.7234\n",
            "Epoch 144/300\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 0.81482\n",
            "240/240 - 0s - loss: 0.4501 - acc: 0.9167 - val_loss: 0.8340 - val_acc: 0.7872\n",
            "Epoch 145/300\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.81482\n",
            "240/240 - 0s - loss: 0.4534 - acc: 0.9125 - val_loss: 0.8516 - val_acc: 0.7660\n",
            "Epoch 146/300\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.81482\n",
            "240/240 - 0s - loss: 0.4703 - acc: 0.8708 - val_loss: 0.8172 - val_acc: 0.8085\n",
            "Epoch 147/300\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.81482\n",
            "240/240 - 0s - loss: 0.4793 - acc: 0.8750 - val_loss: 0.8213 - val_acc: 0.8511\n",
            "Epoch 148/300\n",
            "\n",
            "Epoch 00148: val_loss improved from 0.81482 to 0.81395, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.4679 - acc: 0.9000 - val_loss: 0.8140 - val_acc: 0.7021\n",
            "Epoch 149/300\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.81395\n",
            "240/240 - 0s - loss: 0.4636 - acc: 0.8875 - val_loss: 0.8229 - val_acc: 0.8085\n",
            "Epoch 150/300\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.81395\n",
            "240/240 - 0s - loss: 0.4505 - acc: 0.9125 - val_loss: 0.8786 - val_acc: 0.7021\n",
            "Epoch 151/300\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.81395\n",
            "240/240 - 0s - loss: 0.4341 - acc: 0.9125 - val_loss: 0.8196 - val_acc: 0.6809\n",
            "Epoch 152/300\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.81395\n",
            "240/240 - 0s - loss: 0.4313 - acc: 0.9458 - val_loss: 0.8371 - val_acc: 0.7021\n",
            "Epoch 153/300\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 0.81395\n",
            "240/240 - 0s - loss: 0.4497 - acc: 0.9000 - val_loss: 0.8325 - val_acc: 0.7447\n",
            "Epoch 154/300\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.81395\n",
            "240/240 - 0s - loss: 0.4408 - acc: 0.9208 - val_loss: 0.8371 - val_acc: 0.7447\n",
            "Epoch 155/300\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.81395\n",
            "240/240 - 0s - loss: 0.4396 - acc: 0.9208 - val_loss: 0.8214 - val_acc: 0.7447\n",
            "Epoch 156/300\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.81395\n",
            "240/240 - 0s - loss: 0.4441 - acc: 0.8875 - val_loss: 0.8364 - val_acc: 0.7660\n",
            "Epoch 157/300\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.81395\n",
            "240/240 - 0s - loss: 0.4480 - acc: 0.8958 - val_loss: 0.8682 - val_acc: 0.7021\n",
            "Epoch 158/300\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.81395\n",
            "240/240 - 0s - loss: 0.4417 - acc: 0.8875 - val_loss: 0.9198 - val_acc: 0.6809\n",
            "Epoch 159/300\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.81395\n",
            "240/240 - 0s - loss: 0.4544 - acc: 0.8708 - val_loss: 0.8813 - val_acc: 0.7021\n",
            "Epoch 160/300\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.81395\n",
            "240/240 - 0s - loss: 0.4507 - acc: 0.9125 - val_loss: 0.8630 - val_acc: 0.7021\n",
            "Epoch 161/300\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.81395\n",
            "240/240 - 0s - loss: 0.4291 - acc: 0.8875 - val_loss: 0.8672 - val_acc: 0.7021\n",
            "Epoch 162/300\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.81395\n",
            "240/240 - 0s - loss: 0.4297 - acc: 0.9083 - val_loss: 0.8365 - val_acc: 0.6809\n",
            "Epoch 163/300\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.81395\n",
            "240/240 - 0s - loss: 0.4501 - acc: 0.8792 - val_loss: 0.8408 - val_acc: 0.6809\n",
            "Epoch 164/300\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.81395\n",
            "240/240 - 0s - loss: 0.4379 - acc: 0.9167 - val_loss: 0.8247 - val_acc: 0.7447\n",
            "Epoch 165/300\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.81395\n",
            "240/240 - 0s - loss: 0.4515 - acc: 0.8958 - val_loss: 0.8562 - val_acc: 0.7021\n",
            "Epoch 166/300\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 0.81395\n",
            "240/240 - 0s - loss: 0.4037 - acc: 0.9125 - val_loss: 0.8195 - val_acc: 0.7660\n",
            "Epoch 167/300\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.81395\n",
            "240/240 - 0s - loss: 0.4184 - acc: 0.9125 - val_loss: 0.8151 - val_acc: 0.7234\n",
            "Epoch 168/300\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.81395\n",
            "240/240 - 0s - loss: 0.4050 - acc: 0.9208 - val_loss: 0.8502 - val_acc: 0.7021\n",
            "Epoch 169/300\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 0.81395\n",
            "240/240 - 0s - loss: 0.4364 - acc: 0.8833 - val_loss: 0.8528 - val_acc: 0.7234\n",
            "Epoch 170/300\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.81395\n",
            "240/240 - 0s - loss: 0.4674 - acc: 0.8625 - val_loss: 0.8304 - val_acc: 0.7660\n",
            "Epoch 171/300\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.81395\n",
            "240/240 - 0s - loss: 0.4355 - acc: 0.9000 - val_loss: 0.8236 - val_acc: 0.7447\n",
            "Epoch 172/300\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.81395\n",
            "240/240 - 0s - loss: 0.4671 - acc: 0.8875 - val_loss: 0.8640 - val_acc: 0.6809\n",
            "Epoch 173/300\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.81395\n",
            "240/240 - 0s - loss: 0.4085 - acc: 0.8958 - val_loss: 0.8295 - val_acc: 0.7660\n",
            "Epoch 174/300\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.81395\n",
            "240/240 - 0s - loss: 0.4228 - acc: 0.9083 - val_loss: 0.8359 - val_acc: 0.7872\n",
            "Epoch 175/300\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.81395\n",
            "240/240 - 0s - loss: 0.4134 - acc: 0.8958 - val_loss: 0.8473 - val_acc: 0.7660\n",
            "Epoch 176/300\n",
            "\n",
            "Epoch 00176: val_loss improved from 0.81395 to 0.78880, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.4226 - acc: 0.9083 - val_loss: 0.7888 - val_acc: 0.7660\n",
            "Epoch 177/300\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.78880\n",
            "240/240 - 0s - loss: 0.4082 - acc: 0.9167 - val_loss: 0.7894 - val_acc: 0.7234\n",
            "Epoch 178/300\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.78880\n",
            "240/240 - 0s - loss: 0.3971 - acc: 0.9208 - val_loss: 0.8202 - val_acc: 0.7660\n",
            "Epoch 179/300\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.78880\n",
            "240/240 - 0s - loss: 0.4283 - acc: 0.9167 - val_loss: 0.8305 - val_acc: 0.7234\n",
            "Epoch 180/300\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.78880\n",
            "240/240 - 0s - loss: 0.3798 - acc: 0.9333 - val_loss: 0.8575 - val_acc: 0.7021\n",
            "Epoch 181/300\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.78880\n",
            "240/240 - 0s - loss: 0.3945 - acc: 0.8958 - val_loss: 0.8209 - val_acc: 0.7021\n",
            "Epoch 182/300\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.78880\n",
            "240/240 - 0s - loss: 0.3923 - acc: 0.9417 - val_loss: 0.8772 - val_acc: 0.6596\n",
            "Epoch 183/300\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.78880\n",
            "240/240 - 0s - loss: 0.3864 - acc: 0.9333 - val_loss: 0.8374 - val_acc: 0.7234\n",
            "Epoch 184/300\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 0.78880\n",
            "240/240 - 0s - loss: 0.4140 - acc: 0.9167 - val_loss: 0.8593 - val_acc: 0.7021\n",
            "Epoch 185/300\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.78880\n",
            "240/240 - 0s - loss: 0.4248 - acc: 0.8708 - val_loss: 0.8756 - val_acc: 0.6809\n",
            "Epoch 186/300\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.78880\n",
            "240/240 - 0s - loss: 0.4075 - acc: 0.9250 - val_loss: 0.8822 - val_acc: 0.6809\n",
            "Epoch 187/300\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 0.78880\n",
            "240/240 - 0s - loss: 0.3953 - acc: 0.9250 - val_loss: 0.8300 - val_acc: 0.7021\n",
            "Epoch 188/300\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.78880\n",
            "240/240 - 0s - loss: 0.3791 - acc: 0.9125 - val_loss: 0.8244 - val_acc: 0.7234\n",
            "Epoch 189/300\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.78880\n",
            "240/240 - 0s - loss: 0.3928 - acc: 0.9333 - val_loss: 0.8296 - val_acc: 0.6809\n",
            "Epoch 190/300\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.78880\n",
            "240/240 - 0s - loss: 0.4199 - acc: 0.9083 - val_loss: 0.8450 - val_acc: 0.7021\n",
            "Epoch 191/300\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.78880\n",
            "240/240 - 0s - loss: 0.4373 - acc: 0.8750 - val_loss: 0.8475 - val_acc: 0.6809\n",
            "Epoch 192/300\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 0.78880\n",
            "240/240 - 0s - loss: 0.3789 - acc: 0.9208 - val_loss: 0.8274 - val_acc: 0.7234\n",
            "Epoch 193/300\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.78880\n",
            "240/240 - 0s - loss: 0.3872 - acc: 0.9292 - val_loss: 0.8491 - val_acc: 0.7234\n",
            "Epoch 194/300\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.78880\n",
            "240/240 - 0s - loss: 0.3839 - acc: 0.9125 - val_loss: 0.8469 - val_acc: 0.7021\n",
            "Epoch 195/300\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.78880\n",
            "240/240 - 0s - loss: 0.4003 - acc: 0.9083 - val_loss: 0.8356 - val_acc: 0.6596\n",
            "Epoch 196/300\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.78880\n",
            "240/240 - 0s - loss: 0.3536 - acc: 0.9375 - val_loss: 0.8304 - val_acc: 0.7234\n",
            "Epoch 197/300\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.78880\n",
            "240/240 - 0s - loss: 0.3959 - acc: 0.9000 - val_loss: 0.8463 - val_acc: 0.6170\n",
            "Epoch 198/300\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.78880\n",
            "240/240 - 0s - loss: 0.3985 - acc: 0.9125 - val_loss: 0.8932 - val_acc: 0.6809\n",
            "Epoch 199/300\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.78880\n",
            "240/240 - 0s - loss: 0.3949 - acc: 0.9083 - val_loss: 0.8725 - val_acc: 0.7234\n",
            "Epoch 200/300\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.78880\n",
            "240/240 - 0s - loss: 0.3949 - acc: 0.9083 - val_loss: 0.8322 - val_acc: 0.7234\n",
            "Epoch 201/300\n",
            "\n",
            "Epoch 00201: val_loss did not improve from 0.78880\n",
            "240/240 - 0s - loss: 0.3893 - acc: 0.9000 - val_loss: 0.8451 - val_acc: 0.6809\n",
            "Epoch 202/300\n",
            "\n",
            "Epoch 00202: val_loss did not improve from 0.78880\n",
            "240/240 - 0s - loss: 0.3760 - acc: 0.9000 - val_loss: 0.9185 - val_acc: 0.6170\n",
            "Epoch 203/300\n",
            "\n",
            "Epoch 00203: val_loss did not improve from 0.78880\n",
            "240/240 - 0s - loss: 0.4187 - acc: 0.8792 - val_loss: 0.8932 - val_acc: 0.6596\n",
            "Epoch 204/300\n",
            "\n",
            "Epoch 00204: val_loss did not improve from 0.78880\n",
            "240/240 - 0s - loss: 0.4189 - acc: 0.9000 - val_loss: 0.8784 - val_acc: 0.6596\n",
            "Epoch 205/300\n",
            "\n",
            "Epoch 00205: val_loss did not improve from 0.78880\n",
            "240/240 - 0s - loss: 0.3776 - acc: 0.9167 - val_loss: 0.8603 - val_acc: 0.6809\n",
            "Epoch 206/300\n",
            "\n",
            "Epoch 00206: val_loss did not improve from 0.78880\n",
            "240/240 - 0s - loss: 0.3898 - acc: 0.9125 - val_loss: 0.8449 - val_acc: 0.7447\n",
            "Epoch 207/300\n",
            "\n",
            "Epoch 00207: val_loss did not improve from 0.78880\n",
            "240/240 - 0s - loss: 0.3859 - acc: 0.9167 - val_loss: 0.8785 - val_acc: 0.6170\n",
            "Epoch 208/300\n",
            "\n",
            "Epoch 00208: val_loss did not improve from 0.78880\n",
            "240/240 - 0s - loss: 0.3347 - acc: 0.9333 - val_loss: 0.8343 - val_acc: 0.6809\n",
            "Epoch 209/300\n",
            "\n",
            "Epoch 00209: val_loss did not improve from 0.78880\n",
            "240/240 - 0s - loss: 0.3787 - acc: 0.9125 - val_loss: 0.8179 - val_acc: 0.6809\n",
            "Epoch 210/300\n",
            "\n",
            "Epoch 00210: val_loss did not improve from 0.78880\n",
            "240/240 - 0s - loss: 0.3765 - acc: 0.9083 - val_loss: 0.8520 - val_acc: 0.6596\n",
            "Epoch 211/300\n",
            "\n",
            "Epoch 00211: val_loss did not improve from 0.78880\n",
            "240/240 - 0s - loss: 0.3547 - acc: 0.9417 - val_loss: 0.8433 - val_acc: 0.6596\n",
            "Epoch 212/300\n",
            "\n",
            "Epoch 00212: val_loss did not improve from 0.78880\n",
            "240/240 - 0s - loss: 0.3965 - acc: 0.9083 - val_loss: 0.8597 - val_acc: 0.6809\n",
            "Epoch 213/300\n",
            "\n",
            "Epoch 00213: val_loss did not improve from 0.78880\n",
            "240/240 - 0s - loss: 0.3440 - acc: 0.9458 - val_loss: 0.9146 - val_acc: 0.6170\n",
            "Epoch 214/300\n",
            "\n",
            "Epoch 00214: val_loss did not improve from 0.78880\n",
            "240/240 - 0s - loss: 0.3745 - acc: 0.9167 - val_loss: 0.8733 - val_acc: 0.7660\n",
            "Epoch 215/300\n",
            "\n",
            "Epoch 00215: val_loss did not improve from 0.78880\n",
            "240/240 - 0s - loss: 0.3793 - acc: 0.8917 - val_loss: 0.8881 - val_acc: 0.5957\n",
            "Epoch 216/300\n",
            "\n",
            "Epoch 00216: val_loss did not improve from 0.78880\n",
            "240/240 - 0s - loss: 0.3562 - acc: 0.9208 - val_loss: 0.8402 - val_acc: 0.7021\n",
            "Epoch 217/300\n",
            "\n",
            "Epoch 00217: val_loss did not improve from 0.78880\n",
            "240/240 - 0s - loss: 0.3254 - acc: 0.9417 - val_loss: 0.8557 - val_acc: 0.6809\n",
            "Epoch 218/300\n",
            "\n",
            "Epoch 00218: val_loss did not improve from 0.78880\n",
            "240/240 - 0s - loss: 0.3302 - acc: 0.9458 - val_loss: 0.8460 - val_acc: 0.7021\n",
            "Epoch 219/300\n",
            "\n",
            "Epoch 00219: val_loss did not improve from 0.78880\n",
            "240/240 - 0s - loss: 0.3372 - acc: 0.9292 - val_loss: 0.8569 - val_acc: 0.6809\n",
            "Epoch 220/300\n",
            "\n",
            "Epoch 00220: val_loss did not improve from 0.78880\n",
            "240/240 - 0s - loss: 0.3240 - acc: 0.9542 - val_loss: 0.8702 - val_acc: 0.6809\n",
            "Epoch 221/300\n",
            "\n",
            "Epoch 00221: val_loss did not improve from 0.78880\n",
            "240/240 - 0s - loss: 0.3302 - acc: 0.9458 - val_loss: 0.8739 - val_acc: 0.7021\n",
            "Epoch 222/300\n",
            "\n",
            "Epoch 00222: val_loss did not improve from 0.78880\n",
            "240/240 - 0s - loss: 0.3274 - acc: 0.9458 - val_loss: 0.8503 - val_acc: 0.7021\n",
            "Epoch 223/300\n",
            "\n",
            "Epoch 00223: val_loss did not improve from 0.78880\n",
            "240/240 - 0s - loss: 0.3508 - acc: 0.9458 - val_loss: 0.8670 - val_acc: 0.7234\n",
            "Epoch 224/300\n",
            "\n",
            "Epoch 00224: val_loss did not improve from 0.78880\n",
            "240/240 - 0s - loss: 0.3349 - acc: 0.9417 - val_loss: 0.8747 - val_acc: 0.7021\n",
            "Epoch 225/300\n",
            "\n",
            "Epoch 00225: val_loss did not improve from 0.78880\n",
            "240/240 - 0s - loss: 0.3580 - acc: 0.9208 - val_loss: 0.8425 - val_acc: 0.6596\n",
            "Epoch 226/300\n",
            "\n",
            "Epoch 00226: val_loss did not improve from 0.78880\n",
            "240/240 - 0s - loss: 0.3748 - acc: 0.9083 - val_loss: 0.8980 - val_acc: 0.6596\n",
            "Epoch 227/300\n",
            "\n",
            "Epoch 00227: val_loss did not improve from 0.78880\n",
            "240/240 - 0s - loss: 0.3790 - acc: 0.9167 - val_loss: 0.9104 - val_acc: 0.7021\n",
            "Epoch 228/300\n",
            "\n",
            "Epoch 00228: val_loss did not improve from 0.78880\n",
            "240/240 - 0s - loss: 0.3658 - acc: 0.9083 - val_loss: 0.8675 - val_acc: 0.6809\n",
            "Epoch 229/300\n",
            "\n",
            "Epoch 00229: val_loss did not improve from 0.78880\n",
            "240/240 - 0s - loss: 0.3376 - acc: 0.9292 - val_loss: 0.8902 - val_acc: 0.7021\n",
            "Epoch 230/300\n",
            "\n",
            "Epoch 00230: val_loss did not improve from 0.78880\n",
            "240/240 - 0s - loss: 0.3317 - acc: 0.9417 - val_loss: 0.8632 - val_acc: 0.6809\n",
            "Epoch 231/300\n",
            "\n",
            "Epoch 00231: val_loss did not improve from 0.78880\n",
            "240/240 - 0s - loss: 0.3338 - acc: 0.9167 - val_loss: 0.8642 - val_acc: 0.6809\n",
            "Epoch 232/300\n",
            "\n",
            "Epoch 00232: val_loss did not improve from 0.78880\n",
            "240/240 - 0s - loss: 0.3454 - acc: 0.9417 - val_loss: 0.8564 - val_acc: 0.6809\n",
            "Epoch 233/300\n",
            "\n",
            "Epoch 00233: val_loss did not improve from 0.78880\n",
            "240/240 - 0s - loss: 0.3364 - acc: 0.9167 - val_loss: 0.8557 - val_acc: 0.6596\n",
            "Epoch 234/300\n",
            "\n",
            "Epoch 00234: val_loss did not improve from 0.78880\n",
            "240/240 - 0s - loss: 0.3017 - acc: 0.9625 - val_loss: 0.8647 - val_acc: 0.7234\n",
            "Epoch 235/300\n",
            "\n",
            "Epoch 00235: val_loss did not improve from 0.78880\n",
            "240/240 - 0s - loss: 0.3485 - acc: 0.9125 - val_loss: 0.8239 - val_acc: 0.7021\n",
            "Epoch 236/300\n",
            "\n",
            "Epoch 00236: val_loss did not improve from 0.78880\n",
            "240/240 - 0s - loss: 0.3204 - acc: 0.9333 - val_loss: 0.8666 - val_acc: 0.5957\n",
            "Epoch 237/300\n",
            "\n",
            "Epoch 00237: val_loss did not improve from 0.78880\n",
            "240/240 - 0s - loss: 0.3712 - acc: 0.9083 - val_loss: 0.9500 - val_acc: 0.6596\n",
            "Epoch 238/300\n",
            "\n",
            "Epoch 00238: val_loss did not improve from 0.78880\n",
            "240/240 - 0s - loss: 0.3396 - acc: 0.9125 - val_loss: 0.9283 - val_acc: 0.6383\n",
            "Epoch 239/300\n",
            "\n",
            "Epoch 00239: val_loss did not improve from 0.78880\n",
            "240/240 - 0s - loss: 0.3486 - acc: 0.9333 - val_loss: 0.8801 - val_acc: 0.6383\n",
            "Epoch 240/300\n",
            "\n",
            "Epoch 00240: val_loss did not improve from 0.78880\n",
            "240/240 - 0s - loss: 0.3583 - acc: 0.9250 - val_loss: 0.8698 - val_acc: 0.6809\n",
            "Epoch 241/300\n",
            "\n",
            "Epoch 00241: val_loss did not improve from 0.78880\n",
            "240/240 - 0s - loss: 0.3335 - acc: 0.9292 - val_loss: 0.9091 - val_acc: 0.6596\n",
            "Epoch 242/300\n",
            "\n",
            "Epoch 00242: val_loss did not improve from 0.78880\n",
            "240/240 - 0s - loss: 0.3369 - acc: 0.9083 - val_loss: 0.8684 - val_acc: 0.6383\n",
            "Epoch 243/300\n",
            "\n",
            "Epoch 00243: val_loss did not improve from 0.78880\n",
            "240/240 - 0s - loss: 0.3401 - acc: 0.9375 - val_loss: 0.9432 - val_acc: 0.6383\n",
            "Epoch 244/300\n",
            "\n",
            "Epoch 00244: val_loss did not improve from 0.78880\n",
            "240/240 - 0s - loss: 0.3254 - acc: 0.9167 - val_loss: 0.9042 - val_acc: 0.6596\n",
            "Epoch 245/300\n",
            "\n",
            "Epoch 00245: val_loss did not improve from 0.78880\n",
            "240/240 - 0s - loss: 0.3278 - acc: 0.9417 - val_loss: 0.8456 - val_acc: 0.6596\n",
            "Epoch 246/300\n",
            "\n",
            "Epoch 00246: val_loss did not improve from 0.78880\n",
            "240/240 - 0s - loss: 0.3569 - acc: 0.9125 - val_loss: 0.9592 - val_acc: 0.6383\n",
            "Epoch 247/300\n",
            "\n",
            "Epoch 00247: val_loss did not improve from 0.78880\n",
            "240/240 - 0s - loss: 0.3564 - acc: 0.9417 - val_loss: 0.8301 - val_acc: 0.7021\n",
            "Epoch 248/300\n",
            "\n",
            "Epoch 00248: val_loss did not improve from 0.78880\n",
            "240/240 - 0s - loss: 0.3193 - acc: 0.9292 - val_loss: 0.9491 - val_acc: 0.6383\n",
            "Epoch 249/300\n",
            "\n",
            "Epoch 00249: val_loss did not improve from 0.78880\n",
            "240/240 - 0s - loss: 0.3114 - acc: 0.9583 - val_loss: 0.8772 - val_acc: 0.7021\n",
            "Epoch 250/300\n",
            "\n",
            "Epoch 00250: val_loss did not improve from 0.78880\n",
            "240/240 - 0s - loss: 0.2862 - acc: 0.9667 - val_loss: 0.9071 - val_acc: 0.6383\n",
            "Epoch 251/300\n",
            "\n",
            "Epoch 00251: val_loss did not improve from 0.78880\n",
            "240/240 - 0s - loss: 0.3140 - acc: 0.9333 - val_loss: 0.8947 - val_acc: 0.6383\n",
            "Epoch 252/300\n",
            "\n",
            "Epoch 00252: val_loss did not improve from 0.78880\n",
            "240/240 - 0s - loss: 0.3399 - acc: 0.9250 - val_loss: 0.8652 - val_acc: 0.6596\n",
            "Epoch 253/300\n",
            "\n",
            "Epoch 00253: val_loss did not improve from 0.78880\n",
            "240/240 - 0s - loss: 0.3360 - acc: 0.9292 - val_loss: 0.8628 - val_acc: 0.6596\n",
            "Epoch 254/300\n",
            "\n",
            "Epoch 00254: val_loss did not improve from 0.78880\n",
            "240/240 - 0s - loss: 0.3330 - acc: 0.9375 - val_loss: 0.9290 - val_acc: 0.6809\n",
            "Epoch 255/300\n",
            "\n",
            "Epoch 00255: val_loss did not improve from 0.78880\n",
            "240/240 - 0s - loss: 0.3062 - acc: 0.9417 - val_loss: 0.8647 - val_acc: 0.6596\n",
            "Epoch 256/300\n",
            "\n",
            "Epoch 00256: val_loss did not improve from 0.78880\n",
            "240/240 - 0s - loss: 0.3331 - acc: 0.9208 - val_loss: 0.9069 - val_acc: 0.7021\n",
            "Epoch 257/300\n",
            "\n",
            "Epoch 00257: val_loss did not improve from 0.78880\n",
            "240/240 - 0s - loss: 0.3432 - acc: 0.9250 - val_loss: 0.9427 - val_acc: 0.5957\n",
            "Epoch 258/300\n",
            "\n",
            "Epoch 00258: val_loss did not improve from 0.78880\n",
            "240/240 - 0s - loss: 0.3250 - acc: 0.9500 - val_loss: 0.8782 - val_acc: 0.6383\n",
            "Epoch 259/300\n",
            "\n",
            "Epoch 00259: val_loss did not improve from 0.78880\n",
            "240/240 - 0s - loss: 0.2897 - acc: 0.9667 - val_loss: 0.9403 - val_acc: 0.6170\n",
            "Epoch 260/300\n",
            "\n",
            "Epoch 00260: val_loss did not improve from 0.78880\n",
            "240/240 - 0s - loss: 0.3332 - acc: 0.9375 - val_loss: 0.9338 - val_acc: 0.6383\n",
            "Epoch 261/300\n",
            "\n",
            "Epoch 00261: val_loss did not improve from 0.78880\n",
            "240/240 - 0s - loss: 0.2931 - acc: 0.9500 - val_loss: 0.9451 - val_acc: 0.6383\n",
            "Epoch 262/300\n",
            "\n",
            "Epoch 00262: val_loss did not improve from 0.78880\n",
            "240/240 - 0s - loss: 0.2927 - acc: 0.9375 - val_loss: 0.9048 - val_acc: 0.6170\n",
            "Epoch 263/300\n",
            "\n",
            "Epoch 00263: val_loss did not improve from 0.78880\n",
            "240/240 - 0s - loss: 0.3132 - acc: 0.9083 - val_loss: 0.8866 - val_acc: 0.6596\n",
            "Epoch 264/300\n",
            "\n",
            "Epoch 00264: val_loss did not improve from 0.78880\n",
            "240/240 - 0s - loss: 0.2771 - acc: 0.9583 - val_loss: 0.9045 - val_acc: 0.6596\n",
            "Epoch 265/300\n",
            "\n",
            "Epoch 00265: val_loss did not improve from 0.78880\n",
            "240/240 - 0s - loss: 0.2900 - acc: 0.9417 - val_loss: 0.9173 - val_acc: 0.6809\n",
            "Epoch 266/300\n",
            "\n",
            "Epoch 00266: val_loss did not improve from 0.78880\n",
            "240/240 - 0s - loss: 0.3312 - acc: 0.9375 - val_loss: 0.8716 - val_acc: 0.6383\n",
            "Epoch 267/300\n",
            "\n",
            "Epoch 00267: val_loss did not improve from 0.78880\n",
            "240/240 - 0s - loss: 0.3029 - acc: 0.9292 - val_loss: 0.9129 - val_acc: 0.6383\n",
            "Epoch 268/300\n",
            "\n",
            "Epoch 00268: val_loss did not improve from 0.78880\n",
            "240/240 - 0s - loss: 0.2565 - acc: 0.9542 - val_loss: 0.8502 - val_acc: 0.6809\n",
            "Epoch 269/300\n",
            "\n",
            "Epoch 00269: val_loss did not improve from 0.78880\n",
            "240/240 - 0s - loss: 0.2808 - acc: 0.9667 - val_loss: 0.8960 - val_acc: 0.6383\n",
            "Epoch 270/300\n",
            "\n",
            "Epoch 00270: val_loss did not improve from 0.78880\n",
            "240/240 - 0s - loss: 0.2941 - acc: 0.9333 - val_loss: 0.9007 - val_acc: 0.6383\n",
            "Epoch 271/300\n",
            "\n",
            "Epoch 00271: val_loss did not improve from 0.78880\n",
            "240/240 - 0s - loss: 0.3011 - acc: 0.9417 - val_loss: 0.9305 - val_acc: 0.5957\n",
            "Epoch 272/300\n",
            "\n",
            "Epoch 00272: val_loss did not improve from 0.78880\n",
            "240/240 - 0s - loss: 0.2727 - acc: 0.9542 - val_loss: 0.9198 - val_acc: 0.5957\n",
            "Epoch 273/300\n",
            "\n",
            "Epoch 00273: val_loss did not improve from 0.78880\n",
            "240/240 - 0s - loss: 0.3178 - acc: 0.9333 - val_loss: 0.9500 - val_acc: 0.6383\n",
            "Epoch 274/300\n",
            "\n",
            "Epoch 00274: val_loss did not improve from 0.78880\n",
            "240/240 - 0s - loss: 0.3078 - acc: 0.9375 - val_loss: 0.9499 - val_acc: 0.5957\n",
            "Epoch 275/300\n",
            "\n",
            "Epoch 00275: val_loss did not improve from 0.78880\n",
            "240/240 - 0s - loss: 0.3405 - acc: 0.9167 - val_loss: 0.9108 - val_acc: 0.5957\n",
            "Epoch 276/300\n",
            "\n",
            "Epoch 00276: val_loss did not improve from 0.78880\n",
            "240/240 - 0s - loss: 0.3152 - acc: 0.9250 - val_loss: 0.9222 - val_acc: 0.7021\n",
            "Epoch 277/300\n",
            "\n",
            "Epoch 00277: val_loss did not improve from 0.78880\n",
            "240/240 - 0s - loss: 0.3518 - acc: 0.9250 - val_loss: 0.9078 - val_acc: 0.5957\n",
            "Epoch 278/300\n",
            "\n",
            "Epoch 00278: val_loss did not improve from 0.78880\n",
            "240/240 - 0s - loss: 0.3047 - acc: 0.9500 - val_loss: 0.8670 - val_acc: 0.6596\n",
            "Epoch 279/300\n",
            "\n",
            "Epoch 00279: val_loss did not improve from 0.78880\n",
            "240/240 - 0s - loss: 0.3021 - acc: 0.9333 - val_loss: 0.9374 - val_acc: 0.6383\n",
            "Epoch 280/300\n",
            "\n",
            "Epoch 00280: val_loss did not improve from 0.78880\n",
            "240/240 - 0s - loss: 0.3170 - acc: 0.9208 - val_loss: 0.8961 - val_acc: 0.6170\n",
            "Epoch 281/300\n",
            "\n",
            "Epoch 00281: val_loss did not improve from 0.78880\n",
            "240/240 - 0s - loss: 0.2560 - acc: 0.9542 - val_loss: 0.9018 - val_acc: 0.6383\n",
            "Epoch 282/300\n",
            "\n",
            "Epoch 00282: val_loss did not improve from 0.78880\n",
            "240/240 - 0s - loss: 0.2775 - acc: 0.9500 - val_loss: 0.9134 - val_acc: 0.5957\n",
            "Epoch 283/300\n",
            "\n",
            "Epoch 00283: val_loss did not improve from 0.78880\n",
            "240/240 - 0s - loss: 0.2905 - acc: 0.9375 - val_loss: 0.9198 - val_acc: 0.6596\n",
            "Epoch 284/300\n",
            "\n",
            "Epoch 00284: val_loss did not improve from 0.78880\n",
            "240/240 - 0s - loss: 0.2747 - acc: 0.9542 - val_loss: 0.8818 - val_acc: 0.6170\n",
            "Epoch 285/300\n",
            "\n",
            "Epoch 00285: val_loss did not improve from 0.78880\n",
            "240/240 - 0s - loss: 0.2732 - acc: 0.9625 - val_loss: 0.8764 - val_acc: 0.6383\n",
            "Epoch 286/300\n",
            "\n",
            "Epoch 00286: val_loss did not improve from 0.78880\n",
            "240/240 - 0s - loss: 0.3091 - acc: 0.9250 - val_loss: 0.9345 - val_acc: 0.6809\n",
            "Epoch 287/300\n",
            "\n",
            "Epoch 00287: val_loss did not improve from 0.78880\n",
            "240/240 - 0s - loss: 0.3015 - acc: 0.9208 - val_loss: 0.9687 - val_acc: 0.6383\n",
            "Epoch 288/300\n",
            "\n",
            "Epoch 00288: val_loss did not improve from 0.78880\n",
            "240/240 - 0s - loss: 0.2870 - acc: 0.9458 - val_loss: 0.9542 - val_acc: 0.6170\n",
            "Epoch 289/300\n",
            "\n",
            "Epoch 00289: val_loss did not improve from 0.78880\n",
            "240/240 - 0s - loss: 0.2623 - acc: 0.9667 - val_loss: 0.9191 - val_acc: 0.6596\n",
            "Epoch 290/300\n",
            "\n",
            "Epoch 00290: val_loss did not improve from 0.78880\n",
            "240/240 - 0s - loss: 0.2782 - acc: 0.9583 - val_loss: 0.9067 - val_acc: 0.6596\n",
            "Epoch 291/300\n",
            "\n",
            "Epoch 00291: val_loss did not improve from 0.78880\n",
            "240/240 - 0s - loss: 0.2678 - acc: 0.9542 - val_loss: 0.8549 - val_acc: 0.6809\n",
            "Epoch 292/300\n",
            "\n",
            "Epoch 00292: val_loss did not improve from 0.78880\n",
            "240/240 - 0s - loss: 0.3145 - acc: 0.9292 - val_loss: 0.9322 - val_acc: 0.6170\n",
            "Epoch 293/300\n",
            "\n",
            "Epoch 00293: val_loss did not improve from 0.78880\n",
            "240/240 - 0s - loss: 0.2791 - acc: 0.9375 - val_loss: 0.9708 - val_acc: 0.6809\n",
            "Epoch 294/300\n",
            "\n",
            "Epoch 00294: val_loss did not improve from 0.78880\n",
            "240/240 - 0s - loss: 0.2910 - acc: 0.9375 - val_loss: 0.9346 - val_acc: 0.6383\n",
            "Epoch 295/300\n",
            "\n",
            "Epoch 00295: val_loss did not improve from 0.78880\n",
            "240/240 - 0s - loss: 0.3251 - acc: 0.9167 - val_loss: 0.8827 - val_acc: 0.6383\n",
            "Epoch 296/300\n",
            "\n",
            "Epoch 00296: val_loss did not improve from 0.78880\n",
            "240/240 - 0s - loss: 0.2537 - acc: 0.9500 - val_loss: 0.9347 - val_acc: 0.6170\n",
            "Epoch 297/300\n",
            "\n",
            "Epoch 00297: val_loss did not improve from 0.78880\n",
            "240/240 - 0s - loss: 0.2609 - acc: 0.9458 - val_loss: 0.9281 - val_acc: 0.6170\n",
            "Epoch 298/300\n",
            "\n",
            "Epoch 00298: val_loss did not improve from 0.78880\n",
            "240/240 - 0s - loss: 0.2470 - acc: 0.9708 - val_loss: 0.9009 - val_acc: 0.6170\n",
            "Epoch 299/300\n",
            "\n",
            "Epoch 00299: val_loss did not improve from 0.78880\n",
            "240/240 - 0s - loss: 0.3002 - acc: 0.9375 - val_loss: 0.8849 - val_acc: 0.6596\n",
            "Epoch 300/300\n",
            "\n",
            "Epoch 00300: val_loss did not improve from 0.78880\n",
            "240/240 - 0s - loss: 0.3046 - acc: 0.9333 - val_loss: 0.8921 - val_acc: 0.6596\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "# Evaluate on test data\n",
            "288/288 [==============================] - 1s 2ms/sample - loss: 1.1173 - acc: 0.5972\n",
            "test loss, test acc: [1.1172564191191567, 0.5972222]\n",
            "EEG_Deep/Data2A/Data_A03T.mat\n",
            "(288, 22, 1875)\n",
            "(288, 1)\n",
            "(240, 22, 750)\n",
            "(240, 1)\n",
            "(47, 22, 750)\n",
            "(47, 1)\n",
            "X_train shape: (240, 1, 22, 750)\n",
            "240 train samples\n",
            "47 val samples\n",
            "EEG_Deep/Data2A/Data_A03E.mat\n",
            "(288, 22, 1875)\n",
            "(288, 1)\n",
            "(288, 22, 750)\n",
            "(288, 1)\n",
            "X_train shape: (288, 1, 22, 750)\n",
            "288 train samples\n",
            "Train on 240 samples, validate on 47 samples\n",
            "Epoch 1/300\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 1.38285, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 1s - loss: 1.3960 - acc: 0.2083 - val_loss: 1.3829 - val_acc: 0.3404\n",
            "Epoch 2/300\n",
            "\n",
            "Epoch 00002: val_loss improved from 1.38285 to 1.38239, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.3663 - acc: 0.3500 - val_loss: 1.3824 - val_acc: 0.3191\n",
            "Epoch 3/300\n",
            "\n",
            "Epoch 00003: val_loss improved from 1.38239 to 1.38013, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.3294 - acc: 0.4250 - val_loss: 1.3801 - val_acc: 0.3191\n",
            "Epoch 4/300\n",
            "\n",
            "Epoch 00004: val_loss improved from 1.38013 to 1.37484, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.2977 - acc: 0.4500 - val_loss: 1.3748 - val_acc: 0.3617\n",
            "Epoch 5/300\n",
            "\n",
            "Epoch 00005: val_loss improved from 1.37484 to 1.35882, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.2411 - acc: 0.5042 - val_loss: 1.3588 - val_acc: 0.4255\n",
            "Epoch 6/300\n",
            "\n",
            "Epoch 00006: val_loss improved from 1.35882 to 1.33790, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.1688 - acc: 0.5708 - val_loss: 1.3379 - val_acc: 0.3830\n",
            "Epoch 7/300\n",
            "\n",
            "Epoch 00007: val_loss improved from 1.33790 to 1.31017, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.1249 - acc: 0.5875 - val_loss: 1.3102 - val_acc: 0.4255\n",
            "Epoch 8/300\n",
            "\n",
            "Epoch 00008: val_loss improved from 1.31017 to 1.27894, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.0728 - acc: 0.6583 - val_loss: 1.2789 - val_acc: 0.5106\n",
            "Epoch 9/300\n",
            "\n",
            "Epoch 00009: val_loss improved from 1.27894 to 1.25523, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.0368 - acc: 0.5833 - val_loss: 1.2552 - val_acc: 0.4468\n",
            "Epoch 10/300\n",
            "\n",
            "Epoch 00010: val_loss improved from 1.25523 to 1.21557, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.9763 - acc: 0.6667 - val_loss: 1.2156 - val_acc: 0.5319\n",
            "Epoch 11/300\n",
            "\n",
            "Epoch 00011: val_loss improved from 1.21557 to 1.18416, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.9428 - acc: 0.6708 - val_loss: 1.1842 - val_acc: 0.5957\n",
            "Epoch 12/300\n",
            "\n",
            "Epoch 00012: val_loss improved from 1.18416 to 1.15170, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.9184 - acc: 0.6958 - val_loss: 1.1517 - val_acc: 0.6596\n",
            "Epoch 13/300\n",
            "\n",
            "Epoch 00013: val_loss improved from 1.15170 to 1.14031, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.8708 - acc: 0.7333 - val_loss: 1.1403 - val_acc: 0.5745\n",
            "Epoch 14/300\n",
            "\n",
            "Epoch 00014: val_loss improved from 1.14031 to 1.08847, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.8197 - acc: 0.7458 - val_loss: 1.0885 - val_acc: 0.5106\n",
            "Epoch 15/300\n",
            "\n",
            "Epoch 00015: val_loss improved from 1.08847 to 1.02606, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.8037 - acc: 0.7542 - val_loss: 1.0261 - val_acc: 0.5957\n",
            "Epoch 16/300\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 1.02606\n",
            "240/240 - 0s - loss: 0.7879 - acc: 0.7750 - val_loss: 1.0278 - val_acc: 0.5532\n",
            "Epoch 17/300\n",
            "\n",
            "Epoch 00017: val_loss improved from 1.02606 to 1.00970, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.7577 - acc: 0.7750 - val_loss: 1.0097 - val_acc: 0.5532\n",
            "Epoch 18/300\n",
            "\n",
            "Epoch 00018: val_loss improved from 1.00970 to 0.96320, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.7406 - acc: 0.8083 - val_loss: 0.9632 - val_acc: 0.5745\n",
            "Epoch 19/300\n",
            "\n",
            "Epoch 00019: val_loss improved from 0.96320 to 0.91483, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.7265 - acc: 0.7667 - val_loss: 0.9148 - val_acc: 0.6170\n",
            "Epoch 20/300\n",
            "\n",
            "Epoch 00020: val_loss improved from 0.91483 to 0.88704, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.7131 - acc: 0.7750 - val_loss: 0.8870 - val_acc: 0.6809\n",
            "Epoch 21/300\n",
            "\n",
            "Epoch 00021: val_loss improved from 0.88704 to 0.87203, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.6825 - acc: 0.7833 - val_loss: 0.8720 - val_acc: 0.6596\n",
            "Epoch 22/300\n",
            "\n",
            "Epoch 00022: val_loss improved from 0.87203 to 0.83910, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.7213 - acc: 0.7875 - val_loss: 0.8391 - val_acc: 0.6596\n",
            "Epoch 23/300\n",
            "\n",
            "Epoch 00023: val_loss improved from 0.83910 to 0.81489, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.6501 - acc: 0.8000 - val_loss: 0.8149 - val_acc: 0.6809\n",
            "Epoch 24/300\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.81489\n",
            "240/240 - 0s - loss: 0.6618 - acc: 0.8125 - val_loss: 0.8421 - val_acc: 0.6809\n",
            "Epoch 25/300\n",
            "\n",
            "Epoch 00025: val_loss improved from 0.81489 to 0.80520, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.6281 - acc: 0.8125 - val_loss: 0.8052 - val_acc: 0.6809\n",
            "Epoch 26/300\n",
            "\n",
            "Epoch 00026: val_loss improved from 0.80520 to 0.79746, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.6472 - acc: 0.8167 - val_loss: 0.7975 - val_acc: 0.6809\n",
            "Epoch 27/300\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.79746\n",
            "240/240 - 0s - loss: 0.6688 - acc: 0.7750 - val_loss: 0.8240 - val_acc: 0.7021\n",
            "Epoch 28/300\n",
            "\n",
            "Epoch 00028: val_loss improved from 0.79746 to 0.76877, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.6358 - acc: 0.8250 - val_loss: 0.7688 - val_acc: 0.7021\n",
            "Epoch 29/300\n",
            "\n",
            "Epoch 00029: val_loss improved from 0.76877 to 0.75346, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.6080 - acc: 0.8208 - val_loss: 0.7535 - val_acc: 0.7021\n",
            "Epoch 30/300\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.75346\n",
            "240/240 - 0s - loss: 0.6222 - acc: 0.8333 - val_loss: 0.7578 - val_acc: 0.7234\n",
            "Epoch 31/300\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.75346\n",
            "240/240 - 0s - loss: 0.5908 - acc: 0.8375 - val_loss: 0.7542 - val_acc: 0.6809\n",
            "Epoch 32/300\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.75346\n",
            "240/240 - 0s - loss: 0.6232 - acc: 0.8042 - val_loss: 0.7671 - val_acc: 0.6809\n",
            "Epoch 33/300\n",
            "\n",
            "Epoch 00033: val_loss improved from 0.75346 to 0.74855, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.5835 - acc: 0.8333 - val_loss: 0.7486 - val_acc: 0.7021\n",
            "Epoch 34/300\n",
            "\n",
            "Epoch 00034: val_loss improved from 0.74855 to 0.70684, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.5680 - acc: 0.8417 - val_loss: 0.7068 - val_acc: 0.6809\n",
            "Epoch 35/300\n",
            "\n",
            "Epoch 00035: val_loss improved from 0.70684 to 0.69695, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.5855 - acc: 0.8208 - val_loss: 0.6970 - val_acc: 0.7021\n",
            "Epoch 36/300\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.69695\n",
            "240/240 - 0s - loss: 0.5622 - acc: 0.8500 - val_loss: 0.7774 - val_acc: 0.7021\n",
            "Epoch 37/300\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.69695\n",
            "240/240 - 0s - loss: 0.5553 - acc: 0.8750 - val_loss: 0.7201 - val_acc: 0.7021\n",
            "Epoch 38/300\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.69695\n",
            "240/240 - 0s - loss: 0.5637 - acc: 0.8417 - val_loss: 0.7172 - val_acc: 0.7234\n",
            "Epoch 39/300\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.69695\n",
            "240/240 - 0s - loss: 0.5537 - acc: 0.8458 - val_loss: 0.7016 - val_acc: 0.6809\n",
            "Epoch 40/300\n",
            "\n",
            "Epoch 00040: val_loss improved from 0.69695 to 0.69551, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.5561 - acc: 0.8500 - val_loss: 0.6955 - val_acc: 0.7234\n",
            "Epoch 41/300\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.69551\n",
            "240/240 - 0s - loss: 0.5223 - acc: 0.8500 - val_loss: 0.6998 - val_acc: 0.6596\n",
            "Epoch 42/300\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.69551\n",
            "240/240 - 0s - loss: 0.5421 - acc: 0.8375 - val_loss: 0.7355 - val_acc: 0.6596\n",
            "Epoch 43/300\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.69551\n",
            "240/240 - 0s - loss: 0.5424 - acc: 0.8375 - val_loss: 0.7089 - val_acc: 0.7234\n",
            "Epoch 44/300\n",
            "\n",
            "Epoch 00044: val_loss improved from 0.69551 to 0.67904, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.5352 - acc: 0.8458 - val_loss: 0.6790 - val_acc: 0.7234\n",
            "Epoch 45/300\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.67904\n",
            "240/240 - 0s - loss: 0.5259 - acc: 0.8625 - val_loss: 0.6892 - val_acc: 0.7660\n",
            "Epoch 46/300\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.67904\n",
            "240/240 - 0s - loss: 0.5292 - acc: 0.8292 - val_loss: 0.6895 - val_acc: 0.7872\n",
            "Epoch 47/300\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.67904\n",
            "240/240 - 0s - loss: 0.5036 - acc: 0.8667 - val_loss: 0.6880 - val_acc: 0.7872\n",
            "Epoch 48/300\n",
            "\n",
            "Epoch 00048: val_loss improved from 0.67904 to 0.67637, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.5125 - acc: 0.8583 - val_loss: 0.6764 - val_acc: 0.7447\n",
            "Epoch 49/300\n",
            "\n",
            "Epoch 00049: val_loss improved from 0.67637 to 0.67511, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.5079 - acc: 0.8625 - val_loss: 0.6751 - val_acc: 0.7660\n",
            "Epoch 50/300\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.67511\n",
            "240/240 - 0s - loss: 0.4716 - acc: 0.9042 - val_loss: 0.7464 - val_acc: 0.7021\n",
            "Epoch 51/300\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.67511\n",
            "240/240 - 0s - loss: 0.5183 - acc: 0.8583 - val_loss: 0.6850 - val_acc: 0.7021\n",
            "Epoch 52/300\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.67511\n",
            "240/240 - 0s - loss: 0.4882 - acc: 0.8667 - val_loss: 0.7440 - val_acc: 0.7447\n",
            "Epoch 53/300\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.67511\n",
            "240/240 - 0s - loss: 0.4865 - acc: 0.8667 - val_loss: 0.6752 - val_acc: 0.7447\n",
            "Epoch 54/300\n",
            "\n",
            "Epoch 00054: val_loss improved from 0.67511 to 0.66561, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.4537 - acc: 0.8875 - val_loss: 0.6656 - val_acc: 0.7660\n",
            "Epoch 55/300\n",
            "\n",
            "Epoch 00055: val_loss improved from 0.66561 to 0.66427, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.4568 - acc: 0.8792 - val_loss: 0.6643 - val_acc: 0.7447\n",
            "Epoch 56/300\n",
            "\n",
            "Epoch 00056: val_loss improved from 0.66427 to 0.61801, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.4596 - acc: 0.9000 - val_loss: 0.6180 - val_acc: 0.8085\n",
            "Epoch 57/300\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.61801\n",
            "240/240 - 0s - loss: 0.4424 - acc: 0.9083 - val_loss: 0.6382 - val_acc: 0.7660\n",
            "Epoch 58/300\n",
            "\n",
            "Epoch 00058: val_loss improved from 0.61801 to 0.61031, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.4775 - acc: 0.8833 - val_loss: 0.6103 - val_acc: 0.7660\n",
            "Epoch 59/300\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.61031\n",
            "240/240 - 0s - loss: 0.4202 - acc: 0.9000 - val_loss: 0.7112 - val_acc: 0.7660\n",
            "Epoch 60/300\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.61031\n",
            "240/240 - 0s - loss: 0.4279 - acc: 0.9125 - val_loss: 0.6587 - val_acc: 0.7447\n",
            "Epoch 61/300\n",
            "\n",
            "Epoch 00061: val_loss improved from 0.61031 to 0.59401, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.4199 - acc: 0.9000 - val_loss: 0.5940 - val_acc: 0.7872\n",
            "Epoch 62/300\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.59401\n",
            "240/240 - 0s - loss: 0.4256 - acc: 0.9083 - val_loss: 0.6300 - val_acc: 0.7660\n",
            "Epoch 63/300\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.59401\n",
            "240/240 - 0s - loss: 0.4257 - acc: 0.8792 - val_loss: 0.6129 - val_acc: 0.7234\n",
            "Epoch 64/300\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.59401\n",
            "240/240 - 0s - loss: 0.4541 - acc: 0.9000 - val_loss: 0.7039 - val_acc: 0.6809\n",
            "Epoch 65/300\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.59401\n",
            "240/240 - 0s - loss: 0.4095 - acc: 0.8917 - val_loss: 0.6003 - val_acc: 0.8085\n",
            "Epoch 66/300\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.59401\n",
            "240/240 - 0s - loss: 0.4060 - acc: 0.9083 - val_loss: 0.6021 - val_acc: 0.7660\n",
            "Epoch 67/300\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.59401\n",
            "240/240 - 0s - loss: 0.3627 - acc: 0.9542 - val_loss: 0.6213 - val_acc: 0.7872\n",
            "Epoch 68/300\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.59401\n",
            "240/240 - 0s - loss: 0.3710 - acc: 0.9125 - val_loss: 0.6394 - val_acc: 0.7660\n",
            "Epoch 69/300\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.59401\n",
            "240/240 - 0s - loss: 0.4300 - acc: 0.9083 - val_loss: 0.6440 - val_acc: 0.7660\n",
            "Epoch 70/300\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.59401\n",
            "240/240 - 0s - loss: 0.3860 - acc: 0.9208 - val_loss: 0.6220 - val_acc: 0.7660\n",
            "Epoch 71/300\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.59401\n",
            "240/240 - 0s - loss: 0.3750 - acc: 0.9208 - val_loss: 0.6064 - val_acc: 0.7872\n",
            "Epoch 72/300\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.59401\n",
            "240/240 - 0s - loss: 0.3534 - acc: 0.9458 - val_loss: 0.6011 - val_acc: 0.8085\n",
            "Epoch 73/300\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.59401\n",
            "240/240 - 0s - loss: 0.4081 - acc: 0.8958 - val_loss: 0.6067 - val_acc: 0.7872\n",
            "Epoch 74/300\n",
            "\n",
            "Epoch 00074: val_loss improved from 0.59401 to 0.55285, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.3568 - acc: 0.9250 - val_loss: 0.5529 - val_acc: 0.7872\n",
            "Epoch 75/300\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.55285\n",
            "240/240 - 0s - loss: 0.3825 - acc: 0.9292 - val_loss: 0.6125 - val_acc: 0.7872\n",
            "Epoch 76/300\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.55285\n",
            "240/240 - 0s - loss: 0.3640 - acc: 0.9083 - val_loss: 0.6524 - val_acc: 0.7660\n",
            "Epoch 77/300\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.55285\n",
            "240/240 - 0s - loss: 0.3476 - acc: 0.9333 - val_loss: 0.5841 - val_acc: 0.8085\n",
            "Epoch 78/300\n",
            "\n",
            "Epoch 00078: val_loss improved from 0.55285 to 0.54878, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.3666 - acc: 0.9167 - val_loss: 0.5488 - val_acc: 0.8085\n",
            "Epoch 79/300\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.54878\n",
            "240/240 - 0s - loss: 0.3649 - acc: 0.9083 - val_loss: 0.5610 - val_acc: 0.7872\n",
            "Epoch 80/300\n",
            "\n",
            "Epoch 00080: val_loss improved from 0.54878 to 0.49933, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.3583 - acc: 0.9000 - val_loss: 0.4993 - val_acc: 0.8085\n",
            "Epoch 81/300\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.49933\n",
            "240/240 - 0s - loss: 0.3723 - acc: 0.8958 - val_loss: 0.5586 - val_acc: 0.7660\n",
            "Epoch 82/300\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.49933\n",
            "240/240 - 0s - loss: 0.3435 - acc: 0.9375 - val_loss: 0.5608 - val_acc: 0.8085\n",
            "Epoch 83/300\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.49933\n",
            "240/240 - 0s - loss: 0.3411 - acc: 0.9250 - val_loss: 0.5882 - val_acc: 0.7660\n",
            "Epoch 84/300\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.49933\n",
            "240/240 - 0s - loss: 0.3287 - acc: 0.9417 - val_loss: 0.5326 - val_acc: 0.7872\n",
            "Epoch 85/300\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.49933\n",
            "240/240 - 0s - loss: 0.3292 - acc: 0.9458 - val_loss: 0.6165 - val_acc: 0.8085\n",
            "Epoch 86/300\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.49933\n",
            "240/240 - 0s - loss: 0.3276 - acc: 0.9292 - val_loss: 0.5107 - val_acc: 0.8085\n",
            "Epoch 87/300\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.49933\n",
            "240/240 - 0s - loss: 0.3624 - acc: 0.9125 - val_loss: 0.5598 - val_acc: 0.8085\n",
            "Epoch 88/300\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.49933\n",
            "240/240 - 0s - loss: 0.3425 - acc: 0.9292 - val_loss: 0.5769 - val_acc: 0.7872\n",
            "Epoch 89/300\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.49933\n",
            "240/240 - 0s - loss: 0.3195 - acc: 0.9375 - val_loss: 0.5786 - val_acc: 0.7872\n",
            "Epoch 90/300\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.49933\n",
            "240/240 - 0s - loss: 0.3236 - acc: 0.9417 - val_loss: 0.5597 - val_acc: 0.8085\n",
            "Epoch 91/300\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.49933\n",
            "240/240 - 0s - loss: 0.3232 - acc: 0.9333 - val_loss: 0.5245 - val_acc: 0.7872\n",
            "Epoch 92/300\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.49933\n",
            "240/240 - 0s - loss: 0.3485 - acc: 0.9042 - val_loss: 0.5262 - val_acc: 0.8085\n",
            "Epoch 93/300\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.49933\n",
            "240/240 - 0s - loss: 0.3138 - acc: 0.9292 - val_loss: 0.5379 - val_acc: 0.8085\n",
            "Epoch 94/300\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.49933\n",
            "240/240 - 0s - loss: 0.3338 - acc: 0.9083 - val_loss: 0.5008 - val_acc: 0.8298\n",
            "Epoch 95/300\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.49933\n",
            "240/240 - 0s - loss: 0.2957 - acc: 0.9542 - val_loss: 0.5095 - val_acc: 0.8298\n",
            "Epoch 96/300\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.49933\n",
            "240/240 - 0s - loss: 0.2704 - acc: 0.9667 - val_loss: 0.5166 - val_acc: 0.8298\n",
            "Epoch 97/300\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.49933\n",
            "240/240 - 0s - loss: 0.3116 - acc: 0.9125 - val_loss: 0.5161 - val_acc: 0.8298\n",
            "Epoch 98/300\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.49933\n",
            "240/240 - 0s - loss: 0.2948 - acc: 0.9375 - val_loss: 0.6569 - val_acc: 0.7872\n",
            "Epoch 99/300\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.49933\n",
            "240/240 - 0s - loss: 0.3042 - acc: 0.9458 - val_loss: 0.5755 - val_acc: 0.7234\n",
            "Epoch 100/300\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.49933\n",
            "240/240 - 0s - loss: 0.2934 - acc: 0.9625 - val_loss: 0.6052 - val_acc: 0.7660\n",
            "Epoch 101/300\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.49933\n",
            "240/240 - 0s - loss: 0.3222 - acc: 0.9333 - val_loss: 0.5765 - val_acc: 0.7872\n",
            "Epoch 102/300\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.49933\n",
            "240/240 - 0s - loss: 0.3118 - acc: 0.9250 - val_loss: 0.5342 - val_acc: 0.8298\n",
            "Epoch 103/300\n",
            "\n",
            "Epoch 00103: val_loss improved from 0.49933 to 0.46782, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.2607 - acc: 0.9667 - val_loss: 0.4678 - val_acc: 0.8723\n",
            "Epoch 104/300\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.46782\n",
            "240/240 - 0s - loss: 0.2796 - acc: 0.9375 - val_loss: 0.4908 - val_acc: 0.8085\n",
            "Epoch 105/300\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 0.46782\n",
            "240/240 - 0s - loss: 0.3209 - acc: 0.9417 - val_loss: 0.4976 - val_acc: 0.8085\n",
            "Epoch 106/300\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 0.46782\n",
            "240/240 - 0s - loss: 0.2693 - acc: 0.9417 - val_loss: 0.4695 - val_acc: 0.8511\n",
            "Epoch 107/300\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.46782\n",
            "240/240 - 0s - loss: 0.3016 - acc: 0.9125 - val_loss: 0.5178 - val_acc: 0.8298\n",
            "Epoch 108/300\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 0.46782\n",
            "240/240 - 0s - loss: 0.3010 - acc: 0.9458 - val_loss: 0.4998 - val_acc: 0.8511\n",
            "Epoch 109/300\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.46782\n",
            "240/240 - 0s - loss: 0.2566 - acc: 0.9583 - val_loss: 0.4704 - val_acc: 0.8085\n",
            "Epoch 110/300\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.46782\n",
            "240/240 - 0s - loss: 0.2813 - acc: 0.9417 - val_loss: 0.4696 - val_acc: 0.8085\n",
            "Epoch 111/300\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 0.46782\n",
            "240/240 - 0s - loss: 0.3027 - acc: 0.9042 - val_loss: 0.4690 - val_acc: 0.8298\n",
            "Epoch 112/300\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 0.46782\n",
            "240/240 - 0s - loss: 0.2930 - acc: 0.9208 - val_loss: 0.4814 - val_acc: 0.8085\n",
            "Epoch 113/300\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.46782\n",
            "240/240 - 0s - loss: 0.2759 - acc: 0.9500 - val_loss: 0.4777 - val_acc: 0.8298\n",
            "Epoch 114/300\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.46782\n",
            "240/240 - 0s - loss: 0.2550 - acc: 0.9750 - val_loss: 0.5279 - val_acc: 0.8298\n",
            "Epoch 115/300\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.46782\n",
            "240/240 - 0s - loss: 0.2596 - acc: 0.9458 - val_loss: 0.5222 - val_acc: 0.8298\n",
            "Epoch 116/300\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.46782\n",
            "240/240 - 0s - loss: 0.2537 - acc: 0.9500 - val_loss: 0.4966 - val_acc: 0.8085\n",
            "Epoch 117/300\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.46782\n",
            "240/240 - 0s - loss: 0.2441 - acc: 0.9792 - val_loss: 0.4878 - val_acc: 0.8511\n",
            "Epoch 118/300\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.46782\n",
            "240/240 - 0s - loss: 0.2693 - acc: 0.9500 - val_loss: 0.4698 - val_acc: 0.8723\n",
            "Epoch 119/300\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 0.46782\n",
            "240/240 - 0s - loss: 0.2837 - acc: 0.9375 - val_loss: 0.4696 - val_acc: 0.8511\n",
            "Epoch 120/300\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.46782\n",
            "240/240 - 0s - loss: 0.2670 - acc: 0.9458 - val_loss: 0.4755 - val_acc: 0.8511\n",
            "Epoch 121/300\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.46782\n",
            "240/240 - 0s - loss: 0.2804 - acc: 0.9417 - val_loss: 0.5446 - val_acc: 0.7447\n",
            "Epoch 122/300\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.46782\n",
            "240/240 - 0s - loss: 0.2674 - acc: 0.9417 - val_loss: 0.4801 - val_acc: 0.8511\n",
            "Epoch 123/300\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 0.46782\n",
            "240/240 - 0s - loss: 0.2599 - acc: 0.9625 - val_loss: 0.4712 - val_acc: 0.8298\n",
            "Epoch 124/300\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 0.46782\n",
            "240/240 - 0s - loss: 0.2679 - acc: 0.9542 - val_loss: 0.5024 - val_acc: 0.8511\n",
            "Epoch 125/300\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 0.46782\n",
            "240/240 - 0s - loss: 0.2285 - acc: 0.9625 - val_loss: 0.4864 - val_acc: 0.8723\n",
            "Epoch 126/300\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 0.46782\n",
            "240/240 - 0s - loss: 0.2410 - acc: 0.9542 - val_loss: 0.4860 - val_acc: 0.8511\n",
            "Epoch 127/300\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 0.46782\n",
            "240/240 - 0s - loss: 0.2614 - acc: 0.9458 - val_loss: 0.4800 - val_acc: 0.8298\n",
            "Epoch 128/300\n",
            "\n",
            "Epoch 00128: val_loss improved from 0.46782 to 0.45029, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.2206 - acc: 0.9625 - val_loss: 0.4503 - val_acc: 0.8511\n",
            "Epoch 129/300\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.45029\n",
            "240/240 - 0s - loss: 0.2620 - acc: 0.9500 - val_loss: 0.4678 - val_acc: 0.8511\n",
            "Epoch 130/300\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 0.45029\n",
            "240/240 - 0s - loss: 0.2333 - acc: 0.9667 - val_loss: 0.4962 - val_acc: 0.8298\n",
            "Epoch 131/300\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 0.45029\n",
            "240/240 - 0s - loss: 0.2442 - acc: 0.9417 - val_loss: 0.5040 - val_acc: 0.8723\n",
            "Epoch 132/300\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 0.45029\n",
            "240/240 - 0s - loss: 0.2419 - acc: 0.9708 - val_loss: 0.4982 - val_acc: 0.8298\n",
            "Epoch 133/300\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.45029\n",
            "240/240 - 0s - loss: 0.2444 - acc: 0.9458 - val_loss: 0.5262 - val_acc: 0.7872\n",
            "Epoch 134/300\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.45029\n",
            "240/240 - 0s - loss: 0.2308 - acc: 0.9625 - val_loss: 0.5707 - val_acc: 0.8085\n",
            "Epoch 135/300\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 0.45029\n",
            "240/240 - 0s - loss: 0.2590 - acc: 0.9417 - val_loss: 0.5192 - val_acc: 0.8298\n",
            "Epoch 136/300\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.45029\n",
            "240/240 - 0s - loss: 0.2438 - acc: 0.9500 - val_loss: 0.4572 - val_acc: 0.8511\n",
            "Epoch 137/300\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 0.45029\n",
            "240/240 - 0s - loss: 0.2530 - acc: 0.9458 - val_loss: 0.5059 - val_acc: 0.8723\n",
            "Epoch 138/300\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.45029\n",
            "240/240 - 0s - loss: 0.2361 - acc: 0.9417 - val_loss: 0.4743 - val_acc: 0.8298\n",
            "Epoch 139/300\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.45029\n",
            "240/240 - 0s - loss: 0.2227 - acc: 0.9667 - val_loss: 0.4732 - val_acc: 0.8298\n",
            "Epoch 140/300\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 0.45029\n",
            "240/240 - 0s - loss: 0.2149 - acc: 0.9792 - val_loss: 0.4805 - val_acc: 0.8085\n",
            "Epoch 141/300\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.45029\n",
            "240/240 - 0s - loss: 0.2191 - acc: 0.9583 - val_loss: 0.4932 - val_acc: 0.8298\n",
            "Epoch 142/300\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.45029\n",
            "240/240 - 0s - loss: 0.2246 - acc: 0.9583 - val_loss: 0.4896 - val_acc: 0.7872\n",
            "Epoch 143/300\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 0.45029\n",
            "240/240 - 0s - loss: 0.2382 - acc: 0.9542 - val_loss: 0.5011 - val_acc: 0.8085\n",
            "Epoch 144/300\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 0.45029\n",
            "240/240 - 0s - loss: 0.2419 - acc: 0.9417 - val_loss: 0.4826 - val_acc: 0.8511\n",
            "Epoch 145/300\n",
            "\n",
            "Epoch 00145: val_loss improved from 0.45029 to 0.44425, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.2246 - acc: 0.9583 - val_loss: 0.4443 - val_acc: 0.8723\n",
            "Epoch 146/300\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.44425\n",
            "240/240 - 0s - loss: 0.2817 - acc: 0.9375 - val_loss: 0.5782 - val_acc: 0.8085\n",
            "Epoch 147/300\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.44425\n",
            "240/240 - 0s - loss: 0.2360 - acc: 0.9625 - val_loss: 0.4958 - val_acc: 0.8511\n",
            "Epoch 148/300\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 0.44425\n",
            "240/240 - 0s - loss: 0.2104 - acc: 0.9625 - val_loss: 0.4999 - val_acc: 0.8511\n",
            "Epoch 149/300\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.44425\n",
            "240/240 - 0s - loss: 0.2094 - acc: 0.9667 - val_loss: 0.4842 - val_acc: 0.8723\n",
            "Epoch 150/300\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.44425\n",
            "240/240 - 0s - loss: 0.2135 - acc: 0.9542 - val_loss: 0.4650 - val_acc: 0.8723\n",
            "Epoch 151/300\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.44425\n",
            "240/240 - 0s - loss: 0.2487 - acc: 0.9417 - val_loss: 0.5114 - val_acc: 0.8298\n",
            "Epoch 152/300\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.44425\n",
            "240/240 - 0s - loss: 0.2127 - acc: 0.9708 - val_loss: 0.4607 - val_acc: 0.8723\n",
            "Epoch 153/300\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 0.44425\n",
            "240/240 - 0s - loss: 0.2249 - acc: 0.9458 - val_loss: 0.5163 - val_acc: 0.8298\n",
            "Epoch 154/300\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.44425\n",
            "240/240 - 0s - loss: 0.1898 - acc: 0.9917 - val_loss: 0.4662 - val_acc: 0.8723\n",
            "Epoch 155/300\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.44425\n",
            "240/240 - 0s - loss: 0.2047 - acc: 0.9625 - val_loss: 0.4836 - val_acc: 0.8511\n",
            "Epoch 156/300\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.44425\n",
            "240/240 - 0s - loss: 0.2224 - acc: 0.9667 - val_loss: 0.4824 - val_acc: 0.8511\n",
            "Epoch 157/300\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.44425\n",
            "240/240 - 0s - loss: 0.2213 - acc: 0.9625 - val_loss: 0.4588 - val_acc: 0.8511\n",
            "Epoch 158/300\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.44425\n",
            "240/240 - 0s - loss: 0.2241 - acc: 0.9625 - val_loss: 0.5088 - val_acc: 0.8298\n",
            "Epoch 159/300\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.44425\n",
            "240/240 - 0s - loss: 0.1977 - acc: 0.9792 - val_loss: 0.4937 - val_acc: 0.8511\n",
            "Epoch 160/300\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.44425\n",
            "240/240 - 0s - loss: 0.1944 - acc: 0.9667 - val_loss: 0.5146 - val_acc: 0.8085\n",
            "Epoch 161/300\n",
            "\n",
            "Epoch 00161: val_loss improved from 0.44425 to 0.44166, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.2340 - acc: 0.9292 - val_loss: 0.4417 - val_acc: 0.8723\n",
            "Epoch 162/300\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.44166\n",
            "240/240 - 0s - loss: 0.2693 - acc: 0.9250 - val_loss: 0.5834 - val_acc: 0.7872\n",
            "Epoch 163/300\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.44166\n",
            "240/240 - 0s - loss: 0.2163 - acc: 0.9667 - val_loss: 0.4759 - val_acc: 0.8085\n",
            "Epoch 164/300\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.44166\n",
            "240/240 - 0s - loss: 0.2134 - acc: 0.9542 - val_loss: 0.5517 - val_acc: 0.8298\n",
            "Epoch 165/300\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.44166\n",
            "240/240 - 0s - loss: 0.2208 - acc: 0.9583 - val_loss: 0.4702 - val_acc: 0.8723\n",
            "Epoch 166/300\n",
            "\n",
            "Epoch 00166: val_loss improved from 0.44166 to 0.42482, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.1929 - acc: 0.9625 - val_loss: 0.4248 - val_acc: 0.9149\n",
            "Epoch 167/300\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.42482\n",
            "240/240 - 0s - loss: 0.2083 - acc: 0.9625 - val_loss: 0.4715 - val_acc: 0.8511\n",
            "Epoch 168/300\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.42482\n",
            "240/240 - 0s - loss: 0.1904 - acc: 0.9542 - val_loss: 0.4786 - val_acc: 0.8511\n",
            "Epoch 169/300\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 0.42482\n",
            "240/240 - 0s - loss: 0.2109 - acc: 0.9708 - val_loss: 0.4973 - val_acc: 0.8085\n",
            "Epoch 170/300\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.42482\n",
            "240/240 - 0s - loss: 0.2070 - acc: 0.9667 - val_loss: 0.5126 - val_acc: 0.8298\n",
            "Epoch 171/300\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.42482\n",
            "240/240 - 0s - loss: 0.1913 - acc: 0.9708 - val_loss: 0.5330 - val_acc: 0.8085\n",
            "Epoch 172/300\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.42482\n",
            "240/240 - 0s - loss: 0.2355 - acc: 0.9375 - val_loss: 0.5096 - val_acc: 0.8085\n",
            "Epoch 173/300\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.42482\n",
            "240/240 - 0s - loss: 0.2114 - acc: 0.9667 - val_loss: 0.6690 - val_acc: 0.7660\n",
            "Epoch 174/300\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.42482\n",
            "240/240 - 0s - loss: 0.2478 - acc: 0.9500 - val_loss: 0.4915 - val_acc: 0.7872\n",
            "Epoch 175/300\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.42482\n",
            "240/240 - 0s - loss: 0.2087 - acc: 0.9667 - val_loss: 0.4421 - val_acc: 0.8723\n",
            "Epoch 176/300\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 0.42482\n",
            "240/240 - 0s - loss: 0.1969 - acc: 0.9792 - val_loss: 0.4669 - val_acc: 0.8936\n",
            "Epoch 177/300\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.42482\n",
            "240/240 - 0s - loss: 0.2036 - acc: 0.9583 - val_loss: 0.4301 - val_acc: 0.8936\n",
            "Epoch 178/300\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.42482\n",
            "240/240 - 0s - loss: 0.1820 - acc: 0.9750 - val_loss: 0.4262 - val_acc: 0.8936\n",
            "Epoch 179/300\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.42482\n",
            "240/240 - 0s - loss: 0.2311 - acc: 0.9458 - val_loss: 0.5801 - val_acc: 0.8085\n",
            "Epoch 180/300\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.42482\n",
            "240/240 - 0s - loss: 0.1728 - acc: 0.9708 - val_loss: 0.5322 - val_acc: 0.8298\n",
            "Epoch 181/300\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.42482\n",
            "240/240 - 0s - loss: 0.2140 - acc: 0.9500 - val_loss: 0.5027 - val_acc: 0.8511\n",
            "Epoch 182/300\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.42482\n",
            "240/240 - 0s - loss: 0.1966 - acc: 0.9708 - val_loss: 0.4476 - val_acc: 0.8723\n",
            "Epoch 183/300\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.42482\n",
            "240/240 - 0s - loss: 0.2222 - acc: 0.9542 - val_loss: 0.5223 - val_acc: 0.8511\n",
            "Epoch 184/300\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 0.42482\n",
            "240/240 - 0s - loss: 0.2055 - acc: 0.9708 - val_loss: 0.5301 - val_acc: 0.8511\n",
            "Epoch 185/300\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.42482\n",
            "240/240 - 0s - loss: 0.2130 - acc: 0.9583 - val_loss: 0.4856 - val_acc: 0.8723\n",
            "Epoch 186/300\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.42482\n",
            "240/240 - 0s - loss: 0.1795 - acc: 0.9750 - val_loss: 0.4300 - val_acc: 0.8723\n",
            "Epoch 187/300\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 0.42482\n",
            "240/240 - 0s - loss: 0.2098 - acc: 0.9500 - val_loss: 0.4681 - val_acc: 0.8511\n",
            "Epoch 188/300\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.42482\n",
            "240/240 - 0s - loss: 0.2083 - acc: 0.9583 - val_loss: 0.4916 - val_acc: 0.8298\n",
            "Epoch 189/300\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.42482\n",
            "240/240 - 0s - loss: 0.2010 - acc: 0.9667 - val_loss: 0.4461 - val_acc: 0.8085\n",
            "Epoch 190/300\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.42482\n",
            "240/240 - 0s - loss: 0.1935 - acc: 0.9625 - val_loss: 0.4803 - val_acc: 0.8085\n",
            "Epoch 191/300\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.42482\n",
            "240/240 - 0s - loss: 0.1712 - acc: 0.9875 - val_loss: 0.4707 - val_acc: 0.8723\n",
            "Epoch 192/300\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 0.42482\n",
            "240/240 - 0s - loss: 0.1867 - acc: 0.9667 - val_loss: 0.4699 - val_acc: 0.8298\n",
            "Epoch 193/300\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.42482\n",
            "240/240 - 0s - loss: 0.1698 - acc: 0.9833 - val_loss: 0.4779 - val_acc: 0.8511\n",
            "Epoch 194/300\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.42482\n",
            "240/240 - 0s - loss: 0.1839 - acc: 0.9792 - val_loss: 0.4366 - val_acc: 0.9149\n",
            "Epoch 195/300\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.42482\n",
            "240/240 - 0s - loss: 0.1784 - acc: 0.9792 - val_loss: 0.4542 - val_acc: 0.8723\n",
            "Epoch 196/300\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.42482\n",
            "240/240 - 0s - loss: 0.1442 - acc: 0.9875 - val_loss: 0.4899 - val_acc: 0.8298\n",
            "Epoch 197/300\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.42482\n",
            "240/240 - 0s - loss: 0.1854 - acc: 0.9667 - val_loss: 0.5469 - val_acc: 0.8298\n",
            "Epoch 198/300\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.42482\n",
            "240/240 - 0s - loss: 0.1932 - acc: 0.9500 - val_loss: 0.5728 - val_acc: 0.8298\n",
            "Epoch 199/300\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.42482\n",
            "240/240 - 0s - loss: 0.1788 - acc: 0.9708 - val_loss: 0.4632 - val_acc: 0.8298\n",
            "Epoch 200/300\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.42482\n",
            "240/240 - 0s - loss: 0.2034 - acc: 0.9542 - val_loss: 0.5067 - val_acc: 0.7872\n",
            "Epoch 201/300\n",
            "\n",
            "Epoch 00201: val_loss did not improve from 0.42482\n",
            "240/240 - 0s - loss: 0.1728 - acc: 0.9792 - val_loss: 0.4347 - val_acc: 0.8511\n",
            "Epoch 202/300\n",
            "\n",
            "Epoch 00202: val_loss did not improve from 0.42482\n",
            "240/240 - 0s - loss: 0.2077 - acc: 0.9500 - val_loss: 0.4925 - val_acc: 0.8511\n",
            "Epoch 203/300\n",
            "\n",
            "Epoch 00203: val_loss did not improve from 0.42482\n",
            "240/240 - 0s - loss: 0.1835 - acc: 0.9583 - val_loss: 0.4364 - val_acc: 0.8723\n",
            "Epoch 204/300\n",
            "\n",
            "Epoch 00204: val_loss did not improve from 0.42482\n",
            "240/240 - 0s - loss: 0.1954 - acc: 0.9667 - val_loss: 0.5002 - val_acc: 0.7872\n",
            "Epoch 205/300\n",
            "\n",
            "Epoch 00205: val_loss did not improve from 0.42482\n",
            "240/240 - 0s - loss: 0.1769 - acc: 0.9708 - val_loss: 0.4564 - val_acc: 0.8085\n",
            "Epoch 206/300\n",
            "\n",
            "Epoch 00206: val_loss improved from 0.42482 to 0.40926, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.1725 - acc: 0.9708 - val_loss: 0.4093 - val_acc: 0.8723\n",
            "Epoch 207/300\n",
            "\n",
            "Epoch 00207: val_loss did not improve from 0.40926\n",
            "240/240 - 0s - loss: 0.1969 - acc: 0.9417 - val_loss: 0.4685 - val_acc: 0.8511\n",
            "Epoch 208/300\n",
            "\n",
            "Epoch 00208: val_loss did not improve from 0.40926\n",
            "240/240 - 0s - loss: 0.2013 - acc: 0.9500 - val_loss: 0.4442 - val_acc: 0.8511\n",
            "Epoch 209/300\n",
            "\n",
            "Epoch 00209: val_loss did not improve from 0.40926\n",
            "240/240 - 0s - loss: 0.1733 - acc: 0.9625 - val_loss: 0.4756 - val_acc: 0.8723\n",
            "Epoch 210/300\n",
            "\n",
            "Epoch 00210: val_loss did not improve from 0.40926\n",
            "240/240 - 0s - loss: 0.1677 - acc: 0.9750 - val_loss: 0.4934 - val_acc: 0.8298\n",
            "Epoch 211/300\n",
            "\n",
            "Epoch 00211: val_loss did not improve from 0.40926\n",
            "240/240 - 0s - loss: 0.1741 - acc: 0.9792 - val_loss: 0.4544 - val_acc: 0.8936\n",
            "Epoch 212/300\n",
            "\n",
            "Epoch 00212: val_loss did not improve from 0.40926\n",
            "240/240 - 0s - loss: 0.1580 - acc: 0.9708 - val_loss: 0.4430 - val_acc: 0.8511\n",
            "Epoch 213/300\n",
            "\n",
            "Epoch 00213: val_loss did not improve from 0.40926\n",
            "240/240 - 0s - loss: 0.1686 - acc: 0.9583 - val_loss: 0.4182 - val_acc: 0.8723\n",
            "Epoch 214/300\n",
            "\n",
            "Epoch 00214: val_loss did not improve from 0.40926\n",
            "240/240 - 0s - loss: 0.1613 - acc: 0.9625 - val_loss: 0.4572 - val_acc: 0.8723\n",
            "Epoch 215/300\n",
            "\n",
            "Epoch 00215: val_loss did not improve from 0.40926\n",
            "240/240 - 0s - loss: 0.1642 - acc: 0.9708 - val_loss: 0.4940 - val_acc: 0.8511\n",
            "Epoch 216/300\n",
            "\n",
            "Epoch 00216: val_loss did not improve from 0.40926\n",
            "240/240 - 0s - loss: 0.1716 - acc: 0.9708 - val_loss: 0.6287 - val_acc: 0.7234\n",
            "Epoch 217/300\n",
            "\n",
            "Epoch 00217: val_loss improved from 0.40926 to 0.40292, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.1639 - acc: 0.9667 - val_loss: 0.4029 - val_acc: 0.8723\n",
            "Epoch 218/300\n",
            "\n",
            "Epoch 00218: val_loss did not improve from 0.40292\n",
            "240/240 - 0s - loss: 0.1731 - acc: 0.9667 - val_loss: 0.4403 - val_acc: 0.8511\n",
            "Epoch 219/300\n",
            "\n",
            "Epoch 00219: val_loss did not improve from 0.40292\n",
            "240/240 - 0s - loss: 0.1512 - acc: 0.9875 - val_loss: 0.4215 - val_acc: 0.8511\n",
            "Epoch 220/300\n",
            "\n",
            "Epoch 00220: val_loss improved from 0.40292 to 0.38906, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.1528 - acc: 0.9792 - val_loss: 0.3891 - val_acc: 0.8723\n",
            "Epoch 221/300\n",
            "\n",
            "Epoch 00221: val_loss did not improve from 0.38906\n",
            "240/240 - 0s - loss: 0.1655 - acc: 0.9833 - val_loss: 0.4599 - val_acc: 0.8511\n",
            "Epoch 222/300\n",
            "\n",
            "Epoch 00222: val_loss did not improve from 0.38906\n",
            "240/240 - 0s - loss: 0.1675 - acc: 0.9542 - val_loss: 0.5092 - val_acc: 0.8085\n",
            "Epoch 223/300\n",
            "\n",
            "Epoch 00223: val_loss did not improve from 0.38906\n",
            "240/240 - 0s - loss: 0.1683 - acc: 0.9708 - val_loss: 0.4500 - val_acc: 0.8298\n",
            "Epoch 224/300\n",
            "\n",
            "Epoch 00224: val_loss did not improve from 0.38906\n",
            "240/240 - 0s - loss: 0.1665 - acc: 0.9833 - val_loss: 0.4182 - val_acc: 0.8936\n",
            "Epoch 225/300\n",
            "\n",
            "Epoch 00225: val_loss did not improve from 0.38906\n",
            "240/240 - 0s - loss: 0.1393 - acc: 0.9958 - val_loss: 0.4437 - val_acc: 0.8723\n",
            "Epoch 226/300\n",
            "\n",
            "Epoch 00226: val_loss did not improve from 0.38906\n",
            "240/240 - 0s - loss: 0.1561 - acc: 0.9625 - val_loss: 0.4226 - val_acc: 0.8723\n",
            "Epoch 227/300\n",
            "\n",
            "Epoch 00227: val_loss did not improve from 0.38906\n",
            "240/240 - 0s - loss: 0.1734 - acc: 0.9583 - val_loss: 0.4506 - val_acc: 0.8723\n",
            "Epoch 228/300\n",
            "\n",
            "Epoch 00228: val_loss did not improve from 0.38906\n",
            "240/240 - 0s - loss: 0.1619 - acc: 0.9833 - val_loss: 0.4364 - val_acc: 0.8511\n",
            "Epoch 229/300\n",
            "\n",
            "Epoch 00229: val_loss did not improve from 0.38906\n",
            "240/240 - 0s - loss: 0.1844 - acc: 0.9667 - val_loss: 0.4343 - val_acc: 0.8936\n",
            "Epoch 230/300\n",
            "\n",
            "Epoch 00230: val_loss did not improve from 0.38906\n",
            "240/240 - 0s - loss: 0.1972 - acc: 0.9542 - val_loss: 0.5189 - val_acc: 0.7872\n",
            "Epoch 231/300\n",
            "\n",
            "Epoch 00231: val_loss did not improve from 0.38906\n",
            "240/240 - 0s - loss: 0.1684 - acc: 0.9708 - val_loss: 0.4925 - val_acc: 0.8511\n",
            "Epoch 232/300\n",
            "\n",
            "Epoch 00232: val_loss did not improve from 0.38906\n",
            "240/240 - 0s - loss: 0.1738 - acc: 0.9750 - val_loss: 0.4635 - val_acc: 0.8085\n",
            "Epoch 233/300\n",
            "\n",
            "Epoch 00233: val_loss did not improve from 0.38906\n",
            "240/240 - 0s - loss: 0.1804 - acc: 0.9708 - val_loss: 0.4658 - val_acc: 0.8723\n",
            "Epoch 234/300\n",
            "\n",
            "Epoch 00234: val_loss did not improve from 0.38906\n",
            "240/240 - 0s - loss: 0.1705 - acc: 0.9625 - val_loss: 0.4605 - val_acc: 0.8723\n",
            "Epoch 235/300\n",
            "\n",
            "Epoch 00235: val_loss did not improve from 0.38906\n",
            "240/240 - 0s - loss: 0.1689 - acc: 0.9583 - val_loss: 0.4776 - val_acc: 0.8511\n",
            "Epoch 236/300\n",
            "\n",
            "Epoch 00236: val_loss did not improve from 0.38906\n",
            "240/240 - 0s - loss: 0.1804 - acc: 0.9708 - val_loss: 0.4330 - val_acc: 0.8936\n",
            "Epoch 237/300\n",
            "\n",
            "Epoch 00237: val_loss did not improve from 0.38906\n",
            "240/240 - 0s - loss: 0.1472 - acc: 0.9833 - val_loss: 0.4626 - val_acc: 0.8298\n",
            "Epoch 238/300\n",
            "\n",
            "Epoch 00238: val_loss did not improve from 0.38906\n",
            "240/240 - 0s - loss: 0.1475 - acc: 0.9875 - val_loss: 0.4190 - val_acc: 0.8936\n",
            "Epoch 239/300\n",
            "\n",
            "Epoch 00239: val_loss did not improve from 0.38906\n",
            "240/240 - 0s - loss: 0.1535 - acc: 0.9708 - val_loss: 0.4993 - val_acc: 0.8723\n",
            "Epoch 240/300\n",
            "\n",
            "Epoch 00240: val_loss did not improve from 0.38906\n",
            "240/240 - 0s - loss: 0.1381 - acc: 0.9792 - val_loss: 0.4332 - val_acc: 0.8298\n",
            "Epoch 241/300\n",
            "\n",
            "Epoch 00241: val_loss did not improve from 0.38906\n",
            "240/240 - 0s - loss: 0.2009 - acc: 0.9542 - val_loss: 0.4424 - val_acc: 0.8298\n",
            "Epoch 242/300\n",
            "\n",
            "Epoch 00242: val_loss did not improve from 0.38906\n",
            "240/240 - 0s - loss: 0.1515 - acc: 0.9750 - val_loss: 0.3996 - val_acc: 0.8511\n",
            "Epoch 243/300\n",
            "\n",
            "Epoch 00243: val_loss did not improve from 0.38906\n",
            "240/240 - 0s - loss: 0.1674 - acc: 0.9583 - val_loss: 0.4491 - val_acc: 0.8936\n",
            "Epoch 244/300\n",
            "\n",
            "Epoch 00244: val_loss did not improve from 0.38906\n",
            "240/240 - 0s - loss: 0.1601 - acc: 0.9625 - val_loss: 0.4611 - val_acc: 0.8298\n",
            "Epoch 245/300\n",
            "\n",
            "Epoch 00245: val_loss did not improve from 0.38906\n",
            "240/240 - 0s - loss: 0.1466 - acc: 0.9625 - val_loss: 0.4283 - val_acc: 0.8511\n",
            "Epoch 246/300\n",
            "\n",
            "Epoch 00246: val_loss did not improve from 0.38906\n",
            "240/240 - 0s - loss: 0.1689 - acc: 0.9625 - val_loss: 0.4977 - val_acc: 0.8298\n",
            "Epoch 247/300\n",
            "\n",
            "Epoch 00247: val_loss did not improve from 0.38906\n",
            "240/240 - 0s - loss: 0.1467 - acc: 0.9750 - val_loss: 0.5138 - val_acc: 0.8298\n",
            "Epoch 248/300\n",
            "\n",
            "Epoch 00248: val_loss did not improve from 0.38906\n",
            "240/240 - 0s - loss: 0.1235 - acc: 0.9917 - val_loss: 0.5703 - val_acc: 0.7872\n",
            "Epoch 249/300\n",
            "\n",
            "Epoch 00249: val_loss did not improve from 0.38906\n",
            "240/240 - 0s - loss: 0.1667 - acc: 0.9625 - val_loss: 0.5661 - val_acc: 0.7660\n",
            "Epoch 250/300\n",
            "\n",
            "Epoch 00250: val_loss did not improve from 0.38906\n",
            "240/240 - 0s - loss: 0.1779 - acc: 0.9667 - val_loss: 0.5138 - val_acc: 0.8085\n",
            "Epoch 251/300\n",
            "\n",
            "Epoch 00251: val_loss did not improve from 0.38906\n",
            "240/240 - 0s - loss: 0.1482 - acc: 0.9708 - val_loss: 0.5427 - val_acc: 0.7872\n",
            "Epoch 252/300\n",
            "\n",
            "Epoch 00252: val_loss did not improve from 0.38906\n",
            "240/240 - 0s - loss: 0.1631 - acc: 0.9750 - val_loss: 0.4754 - val_acc: 0.8298\n",
            "Epoch 253/300\n",
            "\n",
            "Epoch 00253: val_loss did not improve from 0.38906\n",
            "240/240 - 0s - loss: 0.1690 - acc: 0.9667 - val_loss: 0.4509 - val_acc: 0.8511\n",
            "Epoch 254/300\n",
            "\n",
            "Epoch 00254: val_loss did not improve from 0.38906\n",
            "240/240 - 0s - loss: 0.1316 - acc: 0.9792 - val_loss: 0.4602 - val_acc: 0.8936\n",
            "Epoch 255/300\n",
            "\n",
            "Epoch 00255: val_loss did not improve from 0.38906\n",
            "240/240 - 0s - loss: 0.1496 - acc: 0.9792 - val_loss: 0.4156 - val_acc: 0.8511\n",
            "Epoch 256/300\n",
            "\n",
            "Epoch 00256: val_loss did not improve from 0.38906\n",
            "240/240 - 0s - loss: 0.1368 - acc: 0.9792 - val_loss: 0.4303 - val_acc: 0.8723\n",
            "Epoch 257/300\n",
            "\n",
            "Epoch 00257: val_loss did not improve from 0.38906\n",
            "240/240 - 0s - loss: 0.1365 - acc: 0.9792 - val_loss: 0.3989 - val_acc: 0.8723\n",
            "Epoch 258/300\n",
            "\n",
            "Epoch 00258: val_loss did not improve from 0.38906\n",
            "240/240 - 0s - loss: 0.1465 - acc: 0.9750 - val_loss: 0.4717 - val_acc: 0.8511\n",
            "Epoch 259/300\n",
            "\n",
            "Epoch 00259: val_loss did not improve from 0.38906\n",
            "240/240 - 0s - loss: 0.1478 - acc: 0.9708 - val_loss: 0.4622 - val_acc: 0.8723\n",
            "Epoch 260/300\n",
            "\n",
            "Epoch 00260: val_loss did not improve from 0.38906\n",
            "240/240 - 0s - loss: 0.1407 - acc: 0.9750 - val_loss: 0.4685 - val_acc: 0.8298\n",
            "Epoch 261/300\n",
            "\n",
            "Epoch 00261: val_loss did not improve from 0.38906\n",
            "240/240 - 0s - loss: 0.1304 - acc: 0.9917 - val_loss: 0.5158 - val_acc: 0.8298\n",
            "Epoch 262/300\n",
            "\n",
            "Epoch 00262: val_loss did not improve from 0.38906\n",
            "240/240 - 0s - loss: 0.1407 - acc: 0.9708 - val_loss: 0.4575 - val_acc: 0.8298\n",
            "Epoch 263/300\n",
            "\n",
            "Epoch 00263: val_loss did not improve from 0.38906\n",
            "240/240 - 0s - loss: 0.1391 - acc: 0.9833 - val_loss: 0.4040 - val_acc: 0.8936\n",
            "Epoch 264/300\n",
            "\n",
            "Epoch 00264: val_loss did not improve from 0.38906\n",
            "240/240 - 0s - loss: 0.1516 - acc: 0.9750 - val_loss: 0.4755 - val_acc: 0.8511\n",
            "Epoch 265/300\n",
            "\n",
            "Epoch 00265: val_loss did not improve from 0.38906\n",
            "240/240 - 0s - loss: 0.1508 - acc: 0.9792 - val_loss: 0.4230 - val_acc: 0.8723\n",
            "Epoch 266/300\n",
            "\n",
            "Epoch 00266: val_loss did not improve from 0.38906\n",
            "240/240 - 0s - loss: 0.1286 - acc: 0.9833 - val_loss: 0.4454 - val_acc: 0.8723\n",
            "Epoch 267/300\n",
            "\n",
            "Epoch 00267: val_loss did not improve from 0.38906\n",
            "240/240 - 0s - loss: 0.1573 - acc: 0.9833 - val_loss: 0.4139 - val_acc: 0.8936\n",
            "Epoch 268/300\n",
            "\n",
            "Epoch 00268: val_loss did not improve from 0.38906\n",
            "240/240 - 0s - loss: 0.1370 - acc: 0.9792 - val_loss: 0.4057 - val_acc: 0.8723\n",
            "Epoch 269/300\n",
            "\n",
            "Epoch 00269: val_loss did not improve from 0.38906\n",
            "240/240 - 0s - loss: 0.1218 - acc: 0.9708 - val_loss: 0.4164 - val_acc: 0.8723\n",
            "Epoch 270/300\n",
            "\n",
            "Epoch 00270: val_loss did not improve from 0.38906\n",
            "240/240 - 0s - loss: 0.1414 - acc: 0.9708 - val_loss: 0.4959 - val_acc: 0.8511\n",
            "Epoch 271/300\n",
            "\n",
            "Epoch 00271: val_loss did not improve from 0.38906\n",
            "240/240 - 0s - loss: 0.1360 - acc: 0.9792 - val_loss: 0.4936 - val_acc: 0.7872\n",
            "Epoch 272/300\n",
            "\n",
            "Epoch 00272: val_loss did not improve from 0.38906\n",
            "240/240 - 0s - loss: 0.1410 - acc: 0.9792 - val_loss: 0.4317 - val_acc: 0.8936\n",
            "Epoch 273/300\n",
            "\n",
            "Epoch 00273: val_loss did not improve from 0.38906\n",
            "240/240 - 0s - loss: 0.1409 - acc: 0.9833 - val_loss: 0.4260 - val_acc: 0.8723\n",
            "Epoch 274/300\n",
            "\n",
            "Epoch 00274: val_loss did not improve from 0.38906\n",
            "240/240 - 0s - loss: 0.1685 - acc: 0.9500 - val_loss: 0.4186 - val_acc: 0.8723\n",
            "Epoch 275/300\n",
            "\n",
            "Epoch 00275: val_loss did not improve from 0.38906\n",
            "240/240 - 0s - loss: 0.1216 - acc: 0.9792 - val_loss: 0.4256 - val_acc: 0.8723\n",
            "Epoch 276/300\n",
            "\n",
            "Epoch 00276: val_loss did not improve from 0.38906\n",
            "240/240 - 0s - loss: 0.1172 - acc: 0.9917 - val_loss: 0.5248 - val_acc: 0.8511\n",
            "Epoch 277/300\n",
            "\n",
            "Epoch 00277: val_loss did not improve from 0.38906\n",
            "240/240 - 0s - loss: 0.1246 - acc: 0.9833 - val_loss: 0.4350 - val_acc: 0.8723\n",
            "Epoch 278/300\n",
            "\n",
            "Epoch 00278: val_loss did not improve from 0.38906\n",
            "240/240 - 0s - loss: 0.1335 - acc: 0.9833 - val_loss: 0.4532 - val_acc: 0.8723\n",
            "Epoch 279/300\n",
            "\n",
            "Epoch 00279: val_loss did not improve from 0.38906\n",
            "240/240 - 0s - loss: 0.1297 - acc: 0.9792 - val_loss: 0.4690 - val_acc: 0.7872\n",
            "Epoch 280/300\n",
            "\n",
            "Epoch 00280: val_loss did not improve from 0.38906\n",
            "240/240 - 0s - loss: 0.1323 - acc: 0.9792 - val_loss: 0.4030 - val_acc: 0.8936\n",
            "Epoch 281/300\n",
            "\n",
            "Epoch 00281: val_loss did not improve from 0.38906\n",
            "240/240 - 0s - loss: 0.1324 - acc: 0.9833 - val_loss: 0.4174 - val_acc: 0.8511\n",
            "Epoch 282/300\n",
            "\n",
            "Epoch 00282: val_loss did not improve from 0.38906\n",
            "240/240 - 0s - loss: 0.1039 - acc: 0.9875 - val_loss: 0.4591 - val_acc: 0.8298\n",
            "Epoch 283/300\n",
            "\n",
            "Epoch 00283: val_loss did not improve from 0.38906\n",
            "240/240 - 0s - loss: 0.1168 - acc: 0.9875 - val_loss: 0.4053 - val_acc: 0.8511\n",
            "Epoch 284/300\n",
            "\n",
            "Epoch 00284: val_loss did not improve from 0.38906\n",
            "240/240 - 0s - loss: 0.1382 - acc: 0.9708 - val_loss: 0.4468 - val_acc: 0.8298\n",
            "Epoch 285/300\n",
            "\n",
            "Epoch 00285: val_loss did not improve from 0.38906\n",
            "240/240 - 0s - loss: 0.1416 - acc: 0.9750 - val_loss: 0.4156 - val_acc: 0.8723\n",
            "Epoch 286/300\n",
            "\n",
            "Epoch 00286: val_loss did not improve from 0.38906\n",
            "240/240 - 0s - loss: 0.1275 - acc: 0.9792 - val_loss: 0.5475 - val_acc: 0.8723\n",
            "Epoch 287/300\n",
            "\n",
            "Epoch 00287: val_loss did not improve from 0.38906\n",
            "240/240 - 0s - loss: 0.1574 - acc: 0.9625 - val_loss: 0.4675 - val_acc: 0.8085\n",
            "Epoch 288/300\n",
            "\n",
            "Epoch 00288: val_loss did not improve from 0.38906\n",
            "240/240 - 0s - loss: 0.1537 - acc: 0.9667 - val_loss: 0.4481 - val_acc: 0.8936\n",
            "Epoch 289/300\n",
            "\n",
            "Epoch 00289: val_loss did not improve from 0.38906\n",
            "240/240 - 0s - loss: 0.1529 - acc: 0.9708 - val_loss: 0.4423 - val_acc: 0.8511\n",
            "Epoch 290/300\n",
            "\n",
            "Epoch 00290: val_loss did not improve from 0.38906\n",
            "240/240 - 0s - loss: 0.1450 - acc: 0.9667 - val_loss: 0.4742 - val_acc: 0.7872\n",
            "Epoch 291/300\n",
            "\n",
            "Epoch 00291: val_loss did not improve from 0.38906\n",
            "240/240 - 0s - loss: 0.1367 - acc: 0.9750 - val_loss: 0.4464 - val_acc: 0.8723\n",
            "Epoch 292/300\n",
            "\n",
            "Epoch 00292: val_loss did not improve from 0.38906\n",
            "240/240 - 0s - loss: 0.1205 - acc: 0.9917 - val_loss: 0.4946 - val_acc: 0.8298\n",
            "Epoch 293/300\n",
            "\n",
            "Epoch 00293: val_loss did not improve from 0.38906\n",
            "240/240 - 0s - loss: 0.1074 - acc: 0.9833 - val_loss: 0.4708 - val_acc: 0.8723\n",
            "Epoch 294/300\n",
            "\n",
            "Epoch 00294: val_loss improved from 0.38906 to 0.38657, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.1381 - acc: 0.9792 - val_loss: 0.3866 - val_acc: 0.8723\n",
            "Epoch 295/300\n",
            "\n",
            "Epoch 00295: val_loss did not improve from 0.38657\n",
            "240/240 - 0s - loss: 0.1033 - acc: 0.9917 - val_loss: 0.3902 - val_acc: 0.8723\n",
            "Epoch 296/300\n",
            "\n",
            "Epoch 00296: val_loss did not improve from 0.38657\n",
            "240/240 - 0s - loss: 0.1158 - acc: 0.9792 - val_loss: 0.4737 - val_acc: 0.8298\n",
            "Epoch 297/300\n",
            "\n",
            "Epoch 00297: val_loss did not improve from 0.38657\n",
            "240/240 - 0s - loss: 0.1117 - acc: 0.9833 - val_loss: 0.4389 - val_acc: 0.8723\n",
            "Epoch 298/300\n",
            "\n",
            "Epoch 00298: val_loss did not improve from 0.38657\n",
            "240/240 - 0s - loss: 0.1388 - acc: 0.9583 - val_loss: 0.4715 - val_acc: 0.7872\n",
            "Epoch 299/300\n",
            "\n",
            "Epoch 00299: val_loss did not improve from 0.38657\n",
            "240/240 - 0s - loss: 0.1209 - acc: 0.9875 - val_loss: 0.4559 - val_acc: 0.8298\n",
            "Epoch 300/300\n",
            "\n",
            "Epoch 00300: val_loss did not improve from 0.38657\n",
            "240/240 - 0s - loss: 0.1303 - acc: 0.9833 - val_loss: 0.4589 - val_acc: 0.8511\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "# Evaluate on test data\n",
            "288/288 [==============================] - 1s 2ms/sample - loss: 0.5119 - acc: 0.8368\n",
            "test loss, test acc: [0.5119487557689253, 0.8368056]\n",
            "EEG_Deep/Data2A/Data_A04T.mat\n",
            "(288, 22, 1875)\n",
            "(288, 1)\n",
            "(240, 22, 750)\n",
            "(240, 1)\n",
            "(47, 22, 750)\n",
            "(47, 1)\n",
            "X_train shape: (240, 1, 22, 750)\n",
            "240 train samples\n",
            "47 val samples\n",
            "EEG_Deep/Data2A/Data_A04E.mat\n",
            "(288, 22, 1875)\n",
            "(288, 1)\n",
            "(288, 22, 750)\n",
            "(288, 1)\n",
            "X_train shape: (288, 1, 22, 750)\n",
            "288 train samples\n",
            "Train on 240 samples, validate on 47 samples\n",
            "Epoch 1/300\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 1.38350, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 1s - loss: 1.4041 - acc: 0.2125 - val_loss: 1.3835 - val_acc: 0.3191\n",
            "Epoch 2/300\n",
            "\n",
            "Epoch 00002: val_loss improved from 1.38350 to 1.38044, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.3640 - acc: 0.3458 - val_loss: 1.3804 - val_acc: 0.2979\n",
            "Epoch 3/300\n",
            "\n",
            "Epoch 00003: val_loss improved from 1.38044 to 1.37440, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.3494 - acc: 0.3917 - val_loss: 1.3744 - val_acc: 0.3191\n",
            "Epoch 4/300\n",
            "\n",
            "Epoch 00004: val_loss improved from 1.37440 to 1.36784, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.3147 - acc: 0.4375 - val_loss: 1.3678 - val_acc: 0.3191\n",
            "Epoch 5/300\n",
            "\n",
            "Epoch 00005: val_loss improved from 1.36784 to 1.35964, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.2858 - acc: 0.5583 - val_loss: 1.3596 - val_acc: 0.3830\n",
            "Epoch 6/300\n",
            "\n",
            "Epoch 00006: val_loss improved from 1.35964 to 1.35183, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.2668 - acc: 0.5458 - val_loss: 1.3518 - val_acc: 0.3830\n",
            "Epoch 7/300\n",
            "\n",
            "Epoch 00007: val_loss improved from 1.35183 to 1.33650, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.2411 - acc: 0.5625 - val_loss: 1.3365 - val_acc: 0.4043\n",
            "Epoch 8/300\n",
            "\n",
            "Epoch 00008: val_loss improved from 1.33650 to 1.32477, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.2054 - acc: 0.5708 - val_loss: 1.3248 - val_acc: 0.4043\n",
            "Epoch 9/300\n",
            "\n",
            "Epoch 00009: val_loss improved from 1.32477 to 1.30075, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.1749 - acc: 0.5667 - val_loss: 1.3008 - val_acc: 0.4894\n",
            "Epoch 10/300\n",
            "\n",
            "Epoch 00010: val_loss improved from 1.30075 to 1.28309, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.1300 - acc: 0.6333 - val_loss: 1.2831 - val_acc: 0.4681\n",
            "Epoch 11/300\n",
            "\n",
            "Epoch 00011: val_loss improved from 1.28309 to 1.27562, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.0960 - acc: 0.5875 - val_loss: 1.2756 - val_acc: 0.4468\n",
            "Epoch 12/300\n",
            "\n",
            "Epoch 00012: val_loss improved from 1.27562 to 1.23695, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.0709 - acc: 0.6375 - val_loss: 1.2369 - val_acc: 0.4255\n",
            "Epoch 13/300\n",
            "\n",
            "Epoch 00013: val_loss improved from 1.23695 to 1.20460, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.0510 - acc: 0.6333 - val_loss: 1.2046 - val_acc: 0.5745\n",
            "Epoch 14/300\n",
            "\n",
            "Epoch 00014: val_loss improved from 1.20460 to 1.19952, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.0547 - acc: 0.5917 - val_loss: 1.1995 - val_acc: 0.4894\n",
            "Epoch 15/300\n",
            "\n",
            "Epoch 00015: val_loss improved from 1.19952 to 1.17478, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.0365 - acc: 0.6417 - val_loss: 1.1748 - val_acc: 0.5319\n",
            "Epoch 16/300\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 1.17478\n",
            "240/240 - 0s - loss: 1.0108 - acc: 0.6208 - val_loss: 1.1862 - val_acc: 0.4681\n",
            "Epoch 17/300\n",
            "\n",
            "Epoch 00017: val_loss improved from 1.17478 to 1.15868, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.0066 - acc: 0.6292 - val_loss: 1.1587 - val_acc: 0.5319\n",
            "Epoch 18/300\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 1.15868\n",
            "240/240 - 0s - loss: 1.0176 - acc: 0.6500 - val_loss: 1.1629 - val_acc: 0.5106\n",
            "Epoch 19/300\n",
            "\n",
            "Epoch 00019: val_loss improved from 1.15868 to 1.13344, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.9907 - acc: 0.6250 - val_loss: 1.1334 - val_acc: 0.5319\n",
            "Epoch 20/300\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 1.13344\n",
            "240/240 - 0s - loss: 0.9988 - acc: 0.6333 - val_loss: 1.1391 - val_acc: 0.4894\n",
            "Epoch 21/300\n",
            "\n",
            "Epoch 00021: val_loss improved from 1.13344 to 1.11075, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.9393 - acc: 0.6833 - val_loss: 1.1108 - val_acc: 0.5532\n",
            "Epoch 22/300\n",
            "\n",
            "Epoch 00022: val_loss improved from 1.11075 to 1.09828, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.9488 - acc: 0.6750 - val_loss: 1.0983 - val_acc: 0.5319\n",
            "Epoch 23/300\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 1.09828\n",
            "240/240 - 0s - loss: 0.9488 - acc: 0.7000 - val_loss: 1.1039 - val_acc: 0.5319\n",
            "Epoch 24/300\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 1.09828\n",
            "240/240 - 0s - loss: 0.9338 - acc: 0.6792 - val_loss: 1.1152 - val_acc: 0.5319\n",
            "Epoch 25/300\n",
            "\n",
            "Epoch 00025: val_loss improved from 1.09828 to 1.09776, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.9134 - acc: 0.6625 - val_loss: 1.0978 - val_acc: 0.5532\n",
            "Epoch 26/300\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 1.09776\n",
            "240/240 - 0s - loss: 0.9419 - acc: 0.6875 - val_loss: 1.1148 - val_acc: 0.5106\n",
            "Epoch 27/300\n",
            "\n",
            "Epoch 00027: val_loss improved from 1.09776 to 1.06395, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.9199 - acc: 0.6958 - val_loss: 1.0640 - val_acc: 0.5319\n",
            "Epoch 28/300\n",
            "\n",
            "Epoch 00028: val_loss improved from 1.06395 to 1.06013, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.9166 - acc: 0.6750 - val_loss: 1.0601 - val_acc: 0.5532\n",
            "Epoch 29/300\n",
            "\n",
            "Epoch 00029: val_loss improved from 1.06013 to 1.05383, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.9147 - acc: 0.6667 - val_loss: 1.0538 - val_acc: 0.5319\n",
            "Epoch 30/300\n",
            "\n",
            "Epoch 00030: val_loss improved from 1.05383 to 1.05332, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.9070 - acc: 0.7042 - val_loss: 1.0533 - val_acc: 0.5745\n",
            "Epoch 31/300\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 1.05332\n",
            "240/240 - 0s - loss: 0.8910 - acc: 0.6833 - val_loss: 1.0814 - val_acc: 0.5532\n",
            "Epoch 32/300\n",
            "\n",
            "Epoch 00032: val_loss improved from 1.05332 to 1.04296, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.8898 - acc: 0.7292 - val_loss: 1.0430 - val_acc: 0.5957\n",
            "Epoch 33/300\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 1.04296\n",
            "240/240 - 0s - loss: 0.8556 - acc: 0.7375 - val_loss: 1.0630 - val_acc: 0.5957\n",
            "Epoch 34/300\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 1.04296\n",
            "240/240 - 0s - loss: 0.8591 - acc: 0.7458 - val_loss: 1.0900 - val_acc: 0.5957\n",
            "Epoch 35/300\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 1.04296\n",
            "240/240 - 0s - loss: 0.8647 - acc: 0.7375 - val_loss: 1.0531 - val_acc: 0.5957\n",
            "Epoch 36/300\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 1.04296\n",
            "240/240 - 0s - loss: 0.8736 - acc: 0.7125 - val_loss: 1.0605 - val_acc: 0.5745\n",
            "Epoch 37/300\n",
            "\n",
            "Epoch 00037: val_loss improved from 1.04296 to 1.02059, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.8508 - acc: 0.7542 - val_loss: 1.0206 - val_acc: 0.5745\n",
            "Epoch 38/300\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 1.02059\n",
            "240/240 - 0s - loss: 0.8576 - acc: 0.7167 - val_loss: 1.0685 - val_acc: 0.5319\n",
            "Epoch 39/300\n",
            "\n",
            "Epoch 00039: val_loss improved from 1.02059 to 1.00440, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.8473 - acc: 0.7292 - val_loss: 1.0044 - val_acc: 0.5957\n",
            "Epoch 40/300\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 1.00440\n",
            "240/240 - 0s - loss: 0.8464 - acc: 0.7417 - val_loss: 1.0471 - val_acc: 0.6170\n",
            "Epoch 41/300\n",
            "\n",
            "Epoch 00041: val_loss improved from 1.00440 to 1.00023, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.8533 - acc: 0.7375 - val_loss: 1.0002 - val_acc: 0.5745\n",
            "Epoch 42/300\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 1.00023\n",
            "240/240 - 0s - loss: 0.8538 - acc: 0.7583 - val_loss: 1.0125 - val_acc: 0.6170\n",
            "Epoch 43/300\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 1.00023\n",
            "240/240 - 0s - loss: 0.8249 - acc: 0.7417 - val_loss: 1.0609 - val_acc: 0.5106\n",
            "Epoch 44/300\n",
            "\n",
            "Epoch 00044: val_loss improved from 1.00023 to 0.98146, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.8308 - acc: 0.7708 - val_loss: 0.9815 - val_acc: 0.6170\n",
            "Epoch 45/300\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.98146\n",
            "240/240 - 0s - loss: 0.8075 - acc: 0.7625 - val_loss: 0.9983 - val_acc: 0.6170\n",
            "Epoch 46/300\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.98146\n",
            "240/240 - 0s - loss: 0.8436 - acc: 0.7583 - val_loss: 1.0224 - val_acc: 0.5957\n",
            "Epoch 47/300\n",
            "\n",
            "Epoch 00047: val_loss improved from 0.98146 to 0.96735, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.8171 - acc: 0.7250 - val_loss: 0.9673 - val_acc: 0.5957\n",
            "Epoch 48/300\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.96735\n",
            "240/240 - 0s - loss: 0.7992 - acc: 0.7667 - val_loss: 1.0233 - val_acc: 0.5957\n",
            "Epoch 49/300\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.96735\n",
            "240/240 - 0s - loss: 0.8215 - acc: 0.7125 - val_loss: 1.0134 - val_acc: 0.5532\n",
            "Epoch 50/300\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.96735\n",
            "240/240 - 0s - loss: 0.8315 - acc: 0.7333 - val_loss: 1.0373 - val_acc: 0.4894\n",
            "Epoch 51/300\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.96735\n",
            "240/240 - 0s - loss: 0.7916 - acc: 0.7583 - val_loss: 1.0036 - val_acc: 0.6170\n",
            "Epoch 52/300\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.96735\n",
            "240/240 - 0s - loss: 0.7938 - acc: 0.7708 - val_loss: 1.0234 - val_acc: 0.5745\n",
            "Epoch 53/300\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.96735\n",
            "240/240 - 0s - loss: 0.7680 - acc: 0.7625 - val_loss: 0.9798 - val_acc: 0.5745\n",
            "Epoch 54/300\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.96735\n",
            "240/240 - 0s - loss: 0.7747 - acc: 0.7542 - val_loss: 0.9905 - val_acc: 0.6170\n",
            "Epoch 55/300\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.96735\n",
            "240/240 - 0s - loss: 0.7605 - acc: 0.8000 - val_loss: 0.9889 - val_acc: 0.6383\n",
            "Epoch 56/300\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.96735\n",
            "240/240 - 0s - loss: 0.7551 - acc: 0.8292 - val_loss: 0.9814 - val_acc: 0.6383\n",
            "Epoch 57/300\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.96735\n",
            "240/240 - 0s - loss: 0.7580 - acc: 0.7625 - val_loss: 0.9860 - val_acc: 0.5745\n",
            "Epoch 58/300\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.96735\n",
            "240/240 - 0s - loss: 0.7575 - acc: 0.7792 - val_loss: 0.9827 - val_acc: 0.6170\n",
            "Epoch 59/300\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.96735\n",
            "240/240 - 0s - loss: 0.7474 - acc: 0.7958 - val_loss: 0.9956 - val_acc: 0.5957\n",
            "Epoch 60/300\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.96735\n",
            "240/240 - 0s - loss: 0.7447 - acc: 0.7625 - val_loss: 1.0331 - val_acc: 0.5106\n",
            "Epoch 61/300\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.96735\n",
            "240/240 - 0s - loss: 0.7781 - acc: 0.7417 - val_loss: 0.9897 - val_acc: 0.6170\n",
            "Epoch 62/300\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.96735\n",
            "240/240 - 0s - loss: 0.7519 - acc: 0.7708 - val_loss: 0.9823 - val_acc: 0.6596\n",
            "Epoch 63/300\n",
            "\n",
            "Epoch 00063: val_loss improved from 0.96735 to 0.96595, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.7300 - acc: 0.8083 - val_loss: 0.9660 - val_acc: 0.6383\n",
            "Epoch 64/300\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.96595\n",
            "240/240 - 0s - loss: 0.7117 - acc: 0.8250 - val_loss: 0.9958 - val_acc: 0.5745\n",
            "Epoch 65/300\n",
            "\n",
            "Epoch 00065: val_loss improved from 0.96595 to 0.96013, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.7384 - acc: 0.8167 - val_loss: 0.9601 - val_acc: 0.5957\n",
            "Epoch 66/300\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.96013\n",
            "240/240 - 0s - loss: 0.7284 - acc: 0.8167 - val_loss: 0.9614 - val_acc: 0.5532\n",
            "Epoch 67/300\n",
            "\n",
            "Epoch 00067: val_loss improved from 0.96013 to 0.95997, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.7489 - acc: 0.7792 - val_loss: 0.9600 - val_acc: 0.6170\n",
            "Epoch 68/300\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.95997\n",
            "240/240 - 0s - loss: 0.7209 - acc: 0.7750 - val_loss: 1.0510 - val_acc: 0.5106\n",
            "Epoch 69/300\n",
            "\n",
            "Epoch 00069: val_loss improved from 0.95997 to 0.92506, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.7226 - acc: 0.7708 - val_loss: 0.9251 - val_acc: 0.6596\n",
            "Epoch 70/300\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.92506\n",
            "240/240 - 0s - loss: 0.7465 - acc: 0.7750 - val_loss: 0.9451 - val_acc: 0.6383\n",
            "Epoch 71/300\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.92506\n",
            "240/240 - 0s - loss: 0.7146 - acc: 0.7958 - val_loss: 0.9909 - val_acc: 0.5957\n",
            "Epoch 72/300\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.92506\n",
            "240/240 - 0s - loss: 0.6934 - acc: 0.8125 - val_loss: 0.9674 - val_acc: 0.5745\n",
            "Epoch 73/300\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.92506\n",
            "240/240 - 0s - loss: 0.6903 - acc: 0.8083 - val_loss: 0.9379 - val_acc: 0.6170\n",
            "Epoch 74/300\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.92506\n",
            "240/240 - 0s - loss: 0.6893 - acc: 0.8167 - val_loss: 0.9450 - val_acc: 0.6809\n",
            "Epoch 75/300\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.92506\n",
            "240/240 - 0s - loss: 0.7008 - acc: 0.8000 - val_loss: 0.9651 - val_acc: 0.6170\n",
            "Epoch 76/300\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.92506\n",
            "240/240 - 0s - loss: 0.7217 - acc: 0.7458 - val_loss: 0.9566 - val_acc: 0.5745\n",
            "Epoch 77/300\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.92506\n",
            "240/240 - 0s - loss: 0.7150 - acc: 0.8208 - val_loss: 1.0201 - val_acc: 0.5745\n",
            "Epoch 78/300\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.92506\n",
            "240/240 - 0s - loss: 0.7011 - acc: 0.7792 - val_loss: 0.9314 - val_acc: 0.6809\n",
            "Epoch 79/300\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.92506\n",
            "240/240 - 0s - loss: 0.6809 - acc: 0.8083 - val_loss: 0.9625 - val_acc: 0.5957\n",
            "Epoch 80/300\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.92506\n",
            "240/240 - 0s - loss: 0.6413 - acc: 0.8500 - val_loss: 0.9517 - val_acc: 0.6383\n",
            "Epoch 81/300\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.92506\n",
            "240/240 - 0s - loss: 0.7047 - acc: 0.8042 - val_loss: 0.9273 - val_acc: 0.6809\n",
            "Epoch 82/300\n",
            "\n",
            "Epoch 00082: val_loss improved from 0.92506 to 0.91460, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.6770 - acc: 0.8208 - val_loss: 0.9146 - val_acc: 0.6596\n",
            "Epoch 83/300\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.91460\n",
            "240/240 - 0s - loss: 0.6412 - acc: 0.8250 - val_loss: 0.9367 - val_acc: 0.6383\n",
            "Epoch 84/300\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.91460\n",
            "240/240 - 0s - loss: 0.6556 - acc: 0.8333 - val_loss: 0.9254 - val_acc: 0.6383\n",
            "Epoch 85/300\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.91460\n",
            "240/240 - 0s - loss: 0.6660 - acc: 0.8250 - val_loss: 0.9409 - val_acc: 0.5957\n",
            "Epoch 86/300\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.91460\n",
            "240/240 - 0s - loss: 0.7095 - acc: 0.7542 - val_loss: 0.9289 - val_acc: 0.6596\n",
            "Epoch 87/300\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.91460\n",
            "240/240 - 0s - loss: 0.6650 - acc: 0.7917 - val_loss: 0.9606 - val_acc: 0.6383\n",
            "Epoch 88/300\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.91460\n",
            "240/240 - 0s - loss: 0.6385 - acc: 0.8292 - val_loss: 0.9152 - val_acc: 0.6596\n",
            "Epoch 89/300\n",
            "\n",
            "Epoch 00089: val_loss improved from 0.91460 to 0.90560, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.6528 - acc: 0.8250 - val_loss: 0.9056 - val_acc: 0.6596\n",
            "Epoch 90/300\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.90560\n",
            "240/240 - 0s - loss: 0.6820 - acc: 0.7958 - val_loss: 0.9244 - val_acc: 0.6809\n",
            "Epoch 91/300\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.90560\n",
            "240/240 - 0s - loss: 0.6844 - acc: 0.7917 - val_loss: 0.9233 - val_acc: 0.6170\n",
            "Epoch 92/300\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.90560\n",
            "240/240 - 0s - loss: 0.6114 - acc: 0.8625 - val_loss: 0.9216 - val_acc: 0.6809\n",
            "Epoch 93/300\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.90560\n",
            "240/240 - 0s - loss: 0.6699 - acc: 0.8083 - val_loss: 0.9439 - val_acc: 0.5957\n",
            "Epoch 94/300\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.90560\n",
            "240/240 - 0s - loss: 0.6271 - acc: 0.8208 - val_loss: 0.9131 - val_acc: 0.6809\n",
            "Epoch 95/300\n",
            "\n",
            "Epoch 00095: val_loss improved from 0.90560 to 0.89765, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.6144 - acc: 0.8625 - val_loss: 0.8977 - val_acc: 0.6809\n",
            "Epoch 96/300\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.89765\n",
            "240/240 - 0s - loss: 0.6154 - acc: 0.8458 - val_loss: 0.9322 - val_acc: 0.6596\n",
            "Epoch 97/300\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.89765\n",
            "240/240 - 0s - loss: 0.6027 - acc: 0.8792 - val_loss: 0.9324 - val_acc: 0.6383\n",
            "Epoch 98/300\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.89765\n",
            "240/240 - 0s - loss: 0.6252 - acc: 0.8000 - val_loss: 0.8991 - val_acc: 0.6809\n",
            "Epoch 99/300\n",
            "\n",
            "Epoch 00099: val_loss improved from 0.89765 to 0.87997, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.6057 - acc: 0.8500 - val_loss: 0.8800 - val_acc: 0.6809\n",
            "Epoch 100/300\n",
            "\n",
            "Epoch 00100: val_loss improved from 0.87997 to 0.87712, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.6245 - acc: 0.8333 - val_loss: 0.8771 - val_acc: 0.7021\n",
            "Epoch 101/300\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.87712\n",
            "240/240 - 0s - loss: 0.6124 - acc: 0.8083 - val_loss: 0.9511 - val_acc: 0.6170\n",
            "Epoch 102/300\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.87712\n",
            "240/240 - 0s - loss: 0.6412 - acc: 0.8000 - val_loss: 0.9167 - val_acc: 0.6170\n",
            "Epoch 103/300\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.87712\n",
            "240/240 - 0s - loss: 0.6285 - acc: 0.8292 - val_loss: 0.9110 - val_acc: 0.7447\n",
            "Epoch 104/300\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.87712\n",
            "240/240 - 0s - loss: 0.6081 - acc: 0.8458 - val_loss: 0.8936 - val_acc: 0.7021\n",
            "Epoch 105/300\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 0.87712\n",
            "240/240 - 0s - loss: 0.5912 - acc: 0.8792 - val_loss: 0.8865 - val_acc: 0.6596\n",
            "Epoch 106/300\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 0.87712\n",
            "240/240 - 0s - loss: 0.5898 - acc: 0.8458 - val_loss: 0.9031 - val_acc: 0.6596\n",
            "Epoch 107/300\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.87712\n",
            "240/240 - 0s - loss: 0.6248 - acc: 0.8167 - val_loss: 0.9416 - val_acc: 0.6596\n",
            "Epoch 108/300\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 0.87712\n",
            "240/240 - 0s - loss: 0.5963 - acc: 0.8375 - val_loss: 0.9367 - val_acc: 0.6596\n",
            "Epoch 109/300\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.87712\n",
            "240/240 - 0s - loss: 0.6297 - acc: 0.8083 - val_loss: 0.9066 - val_acc: 0.6596\n",
            "Epoch 110/300\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.87712\n",
            "240/240 - 0s - loss: 0.5753 - acc: 0.8625 - val_loss: 0.9177 - val_acc: 0.6383\n",
            "Epoch 111/300\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 0.87712\n",
            "240/240 - 0s - loss: 0.5750 - acc: 0.8542 - val_loss: 0.9464 - val_acc: 0.5957\n",
            "Epoch 112/300\n",
            "\n",
            "Epoch 00112: val_loss improved from 0.87712 to 0.86279, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.5867 - acc: 0.8625 - val_loss: 0.8628 - val_acc: 0.7021\n",
            "Epoch 113/300\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.86279\n",
            "240/240 - 0s - loss: 0.5868 - acc: 0.8250 - val_loss: 0.8732 - val_acc: 0.6809\n",
            "Epoch 114/300\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.86279\n",
            "240/240 - 0s - loss: 0.5801 - acc: 0.8375 - val_loss: 0.8799 - val_acc: 0.6809\n",
            "Epoch 115/300\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.86279\n",
            "240/240 - 0s - loss: 0.5761 - acc: 0.8708 - val_loss: 0.9193 - val_acc: 0.6170\n",
            "Epoch 116/300\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.86279\n",
            "240/240 - 0s - loss: 0.5343 - acc: 0.8792 - val_loss: 0.8628 - val_acc: 0.6596\n",
            "Epoch 117/300\n",
            "\n",
            "Epoch 00117: val_loss improved from 0.86279 to 0.85741, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.5583 - acc: 0.8708 - val_loss: 0.8574 - val_acc: 0.6383\n",
            "Epoch 118/300\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.85741\n",
            "240/240 - 0s - loss: 0.5759 - acc: 0.8292 - val_loss: 0.9305 - val_acc: 0.6596\n",
            "Epoch 119/300\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 0.85741\n",
            "240/240 - 0s - loss: 0.5887 - acc: 0.8375 - val_loss: 0.9305 - val_acc: 0.6383\n",
            "Epoch 120/300\n",
            "\n",
            "Epoch 00120: val_loss improved from 0.85741 to 0.85348, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.5864 - acc: 0.8417 - val_loss: 0.8535 - val_acc: 0.7021\n",
            "Epoch 121/300\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.85348\n",
            "240/240 - 0s - loss: 0.5883 - acc: 0.8500 - val_loss: 0.8599 - val_acc: 0.6809\n",
            "Epoch 122/300\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.85348\n",
            "240/240 - 0s - loss: 0.5476 - acc: 0.8875 - val_loss: 0.8616 - val_acc: 0.7021\n",
            "Epoch 123/300\n",
            "\n",
            "Epoch 00123: val_loss improved from 0.85348 to 0.84260, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.5537 - acc: 0.8792 - val_loss: 0.8426 - val_acc: 0.6809\n",
            "Epoch 124/300\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 0.84260\n",
            "240/240 - 0s - loss: 0.5106 - acc: 0.8792 - val_loss: 0.8454 - val_acc: 0.6596\n",
            "Epoch 125/300\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 0.84260\n",
            "240/240 - 0s - loss: 0.5502 - acc: 0.8792 - val_loss: 0.8891 - val_acc: 0.5745\n",
            "Epoch 126/300\n",
            "\n",
            "Epoch 00126: val_loss improved from 0.84260 to 0.83233, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.5291 - acc: 0.8792 - val_loss: 0.8323 - val_acc: 0.6596\n",
            "Epoch 127/300\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 0.83233\n",
            "240/240 - 0s - loss: 0.5232 - acc: 0.8833 - val_loss: 0.8627 - val_acc: 0.7021\n",
            "Epoch 128/300\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 0.83233\n",
            "240/240 - 0s - loss: 0.5317 - acc: 0.8667 - val_loss: 0.8707 - val_acc: 0.6170\n",
            "Epoch 129/300\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.83233\n",
            "240/240 - 0s - loss: 0.5395 - acc: 0.8917 - val_loss: 0.9027 - val_acc: 0.6170\n",
            "Epoch 130/300\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 0.83233\n",
            "240/240 - 0s - loss: 0.5868 - acc: 0.8375 - val_loss: 0.8971 - val_acc: 0.5957\n",
            "Epoch 131/300\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 0.83233\n",
            "240/240 - 0s - loss: 0.5263 - acc: 0.8750 - val_loss: 0.9351 - val_acc: 0.6170\n",
            "Epoch 132/300\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 0.83233\n",
            "240/240 - 0s - loss: 0.5477 - acc: 0.8708 - val_loss: 0.8634 - val_acc: 0.6596\n",
            "Epoch 133/300\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.83233\n",
            "240/240 - 0s - loss: 0.5378 - acc: 0.8458 - val_loss: 0.8609 - val_acc: 0.6596\n",
            "Epoch 134/300\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.83233\n",
            "240/240 - 0s - loss: 0.5170 - acc: 0.8750 - val_loss: 0.8404 - val_acc: 0.6809\n",
            "Epoch 135/300\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 0.83233\n",
            "240/240 - 0s - loss: 0.5235 - acc: 0.8750 - val_loss: 0.8597 - val_acc: 0.6383\n",
            "Epoch 136/300\n",
            "\n",
            "Epoch 00136: val_loss improved from 0.83233 to 0.83117, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.4893 - acc: 0.9083 - val_loss: 0.8312 - val_acc: 0.6596\n",
            "Epoch 137/300\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 0.83117\n",
            "240/240 - 0s - loss: 0.5138 - acc: 0.8667 - val_loss: 0.8527 - val_acc: 0.5957\n",
            "Epoch 138/300\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.83117\n",
            "240/240 - 0s - loss: 0.5160 - acc: 0.8667 - val_loss: 0.9014 - val_acc: 0.6170\n",
            "Epoch 139/300\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.83117\n",
            "240/240 - 0s - loss: 0.4947 - acc: 0.9125 - val_loss: 0.8549 - val_acc: 0.6383\n",
            "Epoch 140/300\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 0.83117\n",
            "240/240 - 0s - loss: 0.5134 - acc: 0.9000 - val_loss: 0.8802 - val_acc: 0.6383\n",
            "Epoch 141/300\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.83117\n",
            "240/240 - 0s - loss: 0.5325 - acc: 0.9042 - val_loss: 0.8798 - val_acc: 0.6596\n",
            "Epoch 142/300\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.83117\n",
            "240/240 - 0s - loss: 0.4688 - acc: 0.9125 - val_loss: 0.8677 - val_acc: 0.6596\n",
            "Epoch 143/300\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 0.83117\n",
            "240/240 - 0s - loss: 0.4850 - acc: 0.8833 - val_loss: 0.8856 - val_acc: 0.6170\n",
            "Epoch 144/300\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 0.83117\n",
            "240/240 - 0s - loss: 0.5120 - acc: 0.8833 - val_loss: 0.8953 - val_acc: 0.6170\n",
            "Epoch 145/300\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.83117\n",
            "240/240 - 0s - loss: 0.5099 - acc: 0.8917 - val_loss: 0.8822 - val_acc: 0.6809\n",
            "Epoch 146/300\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.83117\n",
            "240/240 - 0s - loss: 0.4821 - acc: 0.8833 - val_loss: 0.8592 - val_acc: 0.6596\n",
            "Epoch 147/300\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.83117\n",
            "240/240 - 0s - loss: 0.5012 - acc: 0.8542 - val_loss: 0.8709 - val_acc: 0.6383\n",
            "Epoch 148/300\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 0.83117\n",
            "240/240 - 0s - loss: 0.4913 - acc: 0.8792 - val_loss: 0.8605 - val_acc: 0.6596\n",
            "Epoch 149/300\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.83117\n",
            "240/240 - 0s - loss: 0.4932 - acc: 0.9000 - val_loss: 0.8361 - val_acc: 0.6383\n",
            "Epoch 150/300\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.83117\n",
            "240/240 - 0s - loss: 0.4558 - acc: 0.9292 - val_loss: 0.8531 - val_acc: 0.7021\n",
            "Epoch 151/300\n",
            "\n",
            "Epoch 00151: val_loss improved from 0.83117 to 0.82853, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.4678 - acc: 0.8917 - val_loss: 0.8285 - val_acc: 0.7021\n",
            "Epoch 152/300\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.82853\n",
            "240/240 - 0s - loss: 0.4797 - acc: 0.8875 - val_loss: 0.8870 - val_acc: 0.7021\n",
            "Epoch 153/300\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 0.82853\n",
            "240/240 - 0s - loss: 0.4912 - acc: 0.8708 - val_loss: 0.8775 - val_acc: 0.6596\n",
            "Epoch 154/300\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.82853\n",
            "240/240 - 0s - loss: 0.4918 - acc: 0.8875 - val_loss: 0.8352 - val_acc: 0.6170\n",
            "Epoch 155/300\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.82853\n",
            "240/240 - 0s - loss: 0.4858 - acc: 0.9042 - val_loss: 0.8561 - val_acc: 0.6596\n",
            "Epoch 156/300\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.82853\n",
            "240/240 - 0s - loss: 0.4724 - acc: 0.8958 - val_loss: 0.8439 - val_acc: 0.6383\n",
            "Epoch 157/300\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.82853\n",
            "240/240 - 0s - loss: 0.4542 - acc: 0.9000 - val_loss: 0.8538 - val_acc: 0.6596\n",
            "Epoch 158/300\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.82853\n",
            "240/240 - 0s - loss: 0.4728 - acc: 0.9208 - val_loss: 0.8943 - val_acc: 0.6170\n",
            "Epoch 159/300\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.82853\n",
            "240/240 - 0s - loss: 0.4779 - acc: 0.9042 - val_loss: 0.8689 - val_acc: 0.6170\n",
            "Epoch 160/300\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.82853\n",
            "240/240 - 0s - loss: 0.4655 - acc: 0.9000 - val_loss: 0.8566 - val_acc: 0.6596\n",
            "Epoch 161/300\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.82853\n",
            "240/240 - 0s - loss: 0.4360 - acc: 0.9083 - val_loss: 0.8564 - val_acc: 0.6383\n",
            "Epoch 162/300\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.82853\n",
            "240/240 - 0s - loss: 0.4850 - acc: 0.8708 - val_loss: 0.8529 - val_acc: 0.6383\n",
            "Epoch 163/300\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.82853\n",
            "240/240 - 0s - loss: 0.4637 - acc: 0.9042 - val_loss: 0.8573 - val_acc: 0.6383\n",
            "Epoch 164/300\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.82853\n",
            "240/240 - 0s - loss: 0.4559 - acc: 0.8917 - val_loss: 0.8481 - val_acc: 0.7021\n",
            "Epoch 165/300\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.82853\n",
            "240/240 - 0s - loss: 0.4375 - acc: 0.9208 - val_loss: 0.8396 - val_acc: 0.6383\n",
            "Epoch 166/300\n",
            "\n",
            "Epoch 00166: val_loss improved from 0.82853 to 0.80677, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.4431 - acc: 0.8917 - val_loss: 0.8068 - val_acc: 0.7021\n",
            "Epoch 167/300\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.80677\n",
            "240/240 - 0s - loss: 0.4444 - acc: 0.8917 - val_loss: 0.8388 - val_acc: 0.6596\n",
            "Epoch 168/300\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.80677\n",
            "240/240 - 0s - loss: 0.4491 - acc: 0.8917 - val_loss: 0.8311 - val_acc: 0.6383\n",
            "Epoch 169/300\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 0.80677\n",
            "240/240 - 0s - loss: 0.4484 - acc: 0.8875 - val_loss: 0.8308 - val_acc: 0.6596\n",
            "Epoch 170/300\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.80677\n",
            "240/240 - 0s - loss: 0.4214 - acc: 0.9167 - val_loss: 0.8766 - val_acc: 0.6596\n",
            "Epoch 171/300\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.80677\n",
            "240/240 - 0s - loss: 0.4498 - acc: 0.8875 - val_loss: 0.8691 - val_acc: 0.6170\n",
            "Epoch 172/300\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.80677\n",
            "240/240 - 0s - loss: 0.4389 - acc: 0.9000 - val_loss: 0.8981 - val_acc: 0.6170\n",
            "Epoch 173/300\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.80677\n",
            "240/240 - 0s - loss: 0.4136 - acc: 0.9250 - val_loss: 0.8436 - val_acc: 0.6383\n",
            "Epoch 174/300\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.80677\n",
            "240/240 - 0s - loss: 0.4471 - acc: 0.8875 - val_loss: 0.8599 - val_acc: 0.6170\n",
            "Epoch 175/300\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.80677\n",
            "240/240 - 0s - loss: 0.4249 - acc: 0.8958 - val_loss: 0.9380 - val_acc: 0.5106\n",
            "Epoch 176/300\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 0.80677\n",
            "240/240 - 0s - loss: 0.4475 - acc: 0.8750 - val_loss: 0.8491 - val_acc: 0.6383\n",
            "Epoch 177/300\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.80677\n",
            "240/240 - 0s - loss: 0.4424 - acc: 0.9000 - val_loss: 0.8635 - val_acc: 0.5745\n",
            "Epoch 178/300\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.80677\n",
            "240/240 - 0s - loss: 0.4339 - acc: 0.9125 - val_loss: 0.8825 - val_acc: 0.6809\n",
            "Epoch 179/300\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.80677\n",
            "240/240 - 0s - loss: 0.4213 - acc: 0.9083 - val_loss: 0.8630 - val_acc: 0.6383\n",
            "Epoch 180/300\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.80677\n",
            "240/240 - 0s - loss: 0.4347 - acc: 0.9042 - val_loss: 0.8948 - val_acc: 0.5745\n",
            "Epoch 181/300\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.80677\n",
            "240/240 - 0s - loss: 0.4427 - acc: 0.9042 - val_loss: 0.8505 - val_acc: 0.6809\n",
            "Epoch 182/300\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.80677\n",
            "240/240 - 0s - loss: 0.4548 - acc: 0.8917 - val_loss: 0.8760 - val_acc: 0.5745\n",
            "Epoch 183/300\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.80677\n",
            "240/240 - 0s - loss: 0.4130 - acc: 0.9250 - val_loss: 0.8425 - val_acc: 0.6809\n",
            "Epoch 184/300\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 0.80677\n",
            "240/240 - 0s - loss: 0.4208 - acc: 0.9292 - val_loss: 0.8367 - val_acc: 0.6596\n",
            "Epoch 185/300\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.80677\n",
            "240/240 - 0s - loss: 0.4524 - acc: 0.8750 - val_loss: 0.8258 - val_acc: 0.6809\n",
            "Epoch 186/300\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.80677\n",
            "240/240 - 0s - loss: 0.4387 - acc: 0.8833 - val_loss: 0.9170 - val_acc: 0.5957\n",
            "Epoch 187/300\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 0.80677\n",
            "240/240 - 0s - loss: 0.4339 - acc: 0.8958 - val_loss: 0.8570 - val_acc: 0.6596\n",
            "Epoch 188/300\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.80677\n",
            "240/240 - 0s - loss: 0.4222 - acc: 0.9167 - val_loss: 0.8380 - val_acc: 0.6596\n",
            "Epoch 189/300\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.80677\n",
            "240/240 - 0s - loss: 0.4224 - acc: 0.9375 - val_loss: 0.8772 - val_acc: 0.5957\n",
            "Epoch 190/300\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.80677\n",
            "240/240 - 0s - loss: 0.4236 - acc: 0.9083 - val_loss: 0.8499 - val_acc: 0.6383\n",
            "Epoch 191/300\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.80677\n",
            "240/240 - 0s - loss: 0.3956 - acc: 0.9208 - val_loss: 0.8462 - val_acc: 0.6596\n",
            "Epoch 192/300\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 0.80677\n",
            "240/240 - 0s - loss: 0.4397 - acc: 0.8917 - val_loss: 0.9197 - val_acc: 0.5957\n",
            "Epoch 193/300\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.80677\n",
            "240/240 - 0s - loss: 0.4030 - acc: 0.9042 - val_loss: 0.9081 - val_acc: 0.6170\n",
            "Epoch 194/300\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.80677\n",
            "240/240 - 0s - loss: 0.4177 - acc: 0.9250 - val_loss: 0.8604 - val_acc: 0.6383\n",
            "Epoch 195/300\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.80677\n",
            "240/240 - 0s - loss: 0.4173 - acc: 0.9125 - val_loss: 0.8660 - val_acc: 0.6170\n",
            "Epoch 196/300\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.80677\n",
            "240/240 - 0s - loss: 0.3669 - acc: 0.9417 - val_loss: 0.8498 - val_acc: 0.6170\n",
            "Epoch 197/300\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.80677\n",
            "240/240 - 0s - loss: 0.4173 - acc: 0.8750 - val_loss: 0.8733 - val_acc: 0.5957\n",
            "Epoch 198/300\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.80677\n",
            "240/240 - 0s - loss: 0.4171 - acc: 0.8958 - val_loss: 0.8112 - val_acc: 0.6383\n",
            "Epoch 199/300\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.80677\n",
            "240/240 - 0s - loss: 0.4073 - acc: 0.9208 - val_loss: 0.8274 - val_acc: 0.6383\n",
            "Epoch 200/300\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.80677\n",
            "240/240 - 0s - loss: 0.3720 - acc: 0.9333 - val_loss: 0.8402 - val_acc: 0.7021\n",
            "Epoch 201/300\n",
            "\n",
            "Epoch 00201: val_loss did not improve from 0.80677\n",
            "240/240 - 0s - loss: 0.4050 - acc: 0.9125 - val_loss: 0.8362 - val_acc: 0.6383\n",
            "Epoch 202/300\n",
            "\n",
            "Epoch 00202: val_loss did not improve from 0.80677\n",
            "240/240 - 0s - loss: 0.4055 - acc: 0.9250 - val_loss: 0.8866 - val_acc: 0.5957\n",
            "Epoch 203/300\n",
            "\n",
            "Epoch 00203: val_loss did not improve from 0.80677\n",
            "240/240 - 0s - loss: 0.4054 - acc: 0.9042 - val_loss: 0.8499 - val_acc: 0.6596\n",
            "Epoch 204/300\n",
            "\n",
            "Epoch 00204: val_loss did not improve from 0.80677\n",
            "240/240 - 0s - loss: 0.3679 - acc: 0.9292 - val_loss: 0.8465 - val_acc: 0.6170\n",
            "Epoch 205/300\n",
            "\n",
            "Epoch 00205: val_loss did not improve from 0.80677\n",
            "240/240 - 0s - loss: 0.3921 - acc: 0.9000 - val_loss: 0.8705 - val_acc: 0.6170\n",
            "Epoch 206/300\n",
            "\n",
            "Epoch 00206: val_loss did not improve from 0.80677\n",
            "240/240 - 0s - loss: 0.3428 - acc: 0.9417 - val_loss: 0.8479 - val_acc: 0.5745\n",
            "Epoch 207/300\n",
            "\n",
            "Epoch 00207: val_loss did not improve from 0.80677\n",
            "240/240 - 0s - loss: 0.4056 - acc: 0.8958 - val_loss: 0.8256 - val_acc: 0.6596\n",
            "Epoch 208/300\n",
            "\n",
            "Epoch 00208: val_loss did not improve from 0.80677\n",
            "240/240 - 0s - loss: 0.3585 - acc: 0.9375 - val_loss: 0.8280 - val_acc: 0.6383\n",
            "Epoch 209/300\n",
            "\n",
            "Epoch 00209: val_loss did not improve from 0.80677\n",
            "240/240 - 0s - loss: 0.3860 - acc: 0.9083 - val_loss: 0.8550 - val_acc: 0.6170\n",
            "Epoch 210/300\n",
            "\n",
            "Epoch 00210: val_loss did not improve from 0.80677\n",
            "240/240 - 0s - loss: 0.3968 - acc: 0.9125 - val_loss: 0.8198 - val_acc: 0.6596\n",
            "Epoch 211/300\n",
            "\n",
            "Epoch 00211: val_loss did not improve from 0.80677\n",
            "240/240 - 0s - loss: 0.3666 - acc: 0.9292 - val_loss: 0.8379 - val_acc: 0.6170\n",
            "Epoch 212/300\n",
            "\n",
            "Epoch 00212: val_loss did not improve from 0.80677\n",
            "240/240 - 0s - loss: 0.3803 - acc: 0.9250 - val_loss: 0.8327 - val_acc: 0.6383\n",
            "Epoch 213/300\n",
            "\n",
            "Epoch 00213: val_loss did not improve from 0.80677\n",
            "240/240 - 0s - loss: 0.3423 - acc: 0.9458 - val_loss: 0.8631 - val_acc: 0.6596\n",
            "Epoch 214/300\n",
            "\n",
            "Epoch 00214: val_loss did not improve from 0.80677\n",
            "240/240 - 0s - loss: 0.4090 - acc: 0.8792 - val_loss: 0.8777 - val_acc: 0.6170\n",
            "Epoch 215/300\n",
            "\n",
            "Epoch 00215: val_loss did not improve from 0.80677\n",
            "240/240 - 0s - loss: 0.3755 - acc: 0.9250 - val_loss: 0.8366 - val_acc: 0.6383\n",
            "Epoch 216/300\n",
            "\n",
            "Epoch 00216: val_loss did not improve from 0.80677\n",
            "240/240 - 0s - loss: 0.3640 - acc: 0.9333 - val_loss: 0.8474 - val_acc: 0.6170\n",
            "Epoch 217/300\n",
            "\n",
            "Epoch 00217: val_loss did not improve from 0.80677\n",
            "240/240 - 0s - loss: 0.3684 - acc: 0.9208 - val_loss: 0.8450 - val_acc: 0.6596\n",
            "Epoch 218/300\n",
            "\n",
            "Epoch 00218: val_loss did not improve from 0.80677\n",
            "240/240 - 0s - loss: 0.3683 - acc: 0.9167 - val_loss: 0.8676 - val_acc: 0.6170\n",
            "Epoch 219/300\n",
            "\n",
            "Epoch 00219: val_loss did not improve from 0.80677\n",
            "240/240 - 0s - loss: 0.3500 - acc: 0.9292 - val_loss: 0.8951 - val_acc: 0.5745\n",
            "Epoch 220/300\n",
            "\n",
            "Epoch 00220: val_loss did not improve from 0.80677\n",
            "240/240 - 0s - loss: 0.3883 - acc: 0.9125 - val_loss: 0.8247 - val_acc: 0.6383\n",
            "Epoch 221/300\n",
            "\n",
            "Epoch 00221: val_loss did not improve from 0.80677\n",
            "240/240 - 0s - loss: 0.3517 - acc: 0.9042 - val_loss: 0.8543 - val_acc: 0.6170\n",
            "Epoch 222/300\n",
            "\n",
            "Epoch 00222: val_loss did not improve from 0.80677\n",
            "240/240 - 0s - loss: 0.3471 - acc: 0.9125 - val_loss: 0.8450 - val_acc: 0.6596\n",
            "Epoch 223/300\n",
            "\n",
            "Epoch 00223: val_loss did not improve from 0.80677\n",
            "240/240 - 0s - loss: 0.3649 - acc: 0.9250 - val_loss: 0.8086 - val_acc: 0.6809\n",
            "Epoch 224/300\n",
            "\n",
            "Epoch 00224: val_loss did not improve from 0.80677\n",
            "240/240 - 0s - loss: 0.3702 - acc: 0.9292 - val_loss: 0.8295 - val_acc: 0.6596\n",
            "Epoch 225/300\n",
            "\n",
            "Epoch 00225: val_loss did not improve from 0.80677\n",
            "240/240 - 0s - loss: 0.3606 - acc: 0.9292 - val_loss: 0.8687 - val_acc: 0.6596\n",
            "Epoch 226/300\n",
            "\n",
            "Epoch 00226: val_loss did not improve from 0.80677\n",
            "240/240 - 0s - loss: 0.3841 - acc: 0.9125 - val_loss: 0.8553 - val_acc: 0.5957\n",
            "Epoch 227/300\n",
            "\n",
            "Epoch 00227: val_loss improved from 0.80677 to 0.76864, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.3858 - acc: 0.8875 - val_loss: 0.7686 - val_acc: 0.6383\n",
            "Epoch 228/300\n",
            "\n",
            "Epoch 00228: val_loss did not improve from 0.76864\n",
            "240/240 - 0s - loss: 0.3650 - acc: 0.9250 - val_loss: 0.8641 - val_acc: 0.5957\n",
            "Epoch 229/300\n",
            "\n",
            "Epoch 00229: val_loss did not improve from 0.76864\n",
            "240/240 - 0s - loss: 0.4189 - acc: 0.8833 - val_loss: 0.9833 - val_acc: 0.5745\n",
            "Epoch 230/300\n",
            "\n",
            "Epoch 00230: val_loss did not improve from 0.76864\n",
            "240/240 - 0s - loss: 0.4367 - acc: 0.8750 - val_loss: 0.8925 - val_acc: 0.5745\n",
            "Epoch 231/300\n",
            "\n",
            "Epoch 00231: val_loss did not improve from 0.76864\n",
            "240/240 - 0s - loss: 0.3626 - acc: 0.9000 - val_loss: 0.8472 - val_acc: 0.6596\n",
            "Epoch 232/300\n",
            "\n",
            "Epoch 00232: val_loss did not improve from 0.76864\n",
            "240/240 - 0s - loss: 0.3539 - acc: 0.9167 - val_loss: 0.8982 - val_acc: 0.5957\n",
            "Epoch 233/300\n",
            "\n",
            "Epoch 00233: val_loss did not improve from 0.76864\n",
            "240/240 - 0s - loss: 0.3747 - acc: 0.9125 - val_loss: 0.8589 - val_acc: 0.5957\n",
            "Epoch 234/300\n",
            "\n",
            "Epoch 00234: val_loss did not improve from 0.76864\n",
            "240/240 - 0s - loss: 0.4214 - acc: 0.8792 - val_loss: 0.8582 - val_acc: 0.5745\n",
            "Epoch 235/300\n",
            "\n",
            "Epoch 00235: val_loss did not improve from 0.76864\n",
            "240/240 - 0s - loss: 0.3405 - acc: 0.9542 - val_loss: 0.8211 - val_acc: 0.6383\n",
            "Epoch 236/300\n",
            "\n",
            "Epoch 00236: val_loss did not improve from 0.76864\n",
            "240/240 - 0s - loss: 0.3331 - acc: 0.9583 - val_loss: 0.8187 - val_acc: 0.5957\n",
            "Epoch 237/300\n",
            "\n",
            "Epoch 00237: val_loss did not improve from 0.76864\n",
            "240/240 - 0s - loss: 0.3592 - acc: 0.9292 - val_loss: 0.8379 - val_acc: 0.5532\n",
            "Epoch 238/300\n",
            "\n",
            "Epoch 00238: val_loss did not improve from 0.76864\n",
            "240/240 - 0s - loss: 0.3238 - acc: 0.9458 - val_loss: 0.8051 - val_acc: 0.6383\n",
            "Epoch 239/300\n",
            "\n",
            "Epoch 00239: val_loss did not improve from 0.76864\n",
            "240/240 - 0s - loss: 0.3387 - acc: 0.9417 - val_loss: 0.8698 - val_acc: 0.5106\n",
            "Epoch 240/300\n",
            "\n",
            "Epoch 00240: val_loss did not improve from 0.76864\n",
            "240/240 - 0s - loss: 0.3855 - acc: 0.9000 - val_loss: 0.9168 - val_acc: 0.6596\n",
            "Epoch 241/300\n",
            "\n",
            "Epoch 00241: val_loss did not improve from 0.76864\n",
            "240/240 - 0s - loss: 0.3860 - acc: 0.9083 - val_loss: 0.8529 - val_acc: 0.6170\n",
            "Epoch 242/300\n",
            "\n",
            "Epoch 00242: val_loss did not improve from 0.76864\n",
            "240/240 - 0s - loss: 0.3482 - acc: 0.9333 - val_loss: 0.8811 - val_acc: 0.6809\n",
            "Epoch 243/300\n",
            "\n",
            "Epoch 00243: val_loss did not improve from 0.76864\n",
            "240/240 - 0s - loss: 0.3071 - acc: 0.9292 - val_loss: 0.8933 - val_acc: 0.5745\n",
            "Epoch 244/300\n",
            "\n",
            "Epoch 00244: val_loss did not improve from 0.76864\n",
            "240/240 - 0s - loss: 0.3442 - acc: 0.9167 - val_loss: 0.8716 - val_acc: 0.6383\n",
            "Epoch 245/300\n",
            "\n",
            "Epoch 00245: val_loss did not improve from 0.76864\n",
            "240/240 - 0s - loss: 0.3352 - acc: 0.9417 - val_loss: 0.8473 - val_acc: 0.5745\n",
            "Epoch 246/300\n",
            "\n",
            "Epoch 00246: val_loss did not improve from 0.76864\n",
            "240/240 - 0s - loss: 0.3393 - acc: 0.9458 - val_loss: 0.8908 - val_acc: 0.5532\n",
            "Epoch 247/300\n",
            "\n",
            "Epoch 00247: val_loss did not improve from 0.76864\n",
            "240/240 - 0s - loss: 0.3683 - acc: 0.9083 - val_loss: 0.8189 - val_acc: 0.6596\n",
            "Epoch 248/300\n",
            "\n",
            "Epoch 00248: val_loss did not improve from 0.76864\n",
            "240/240 - 0s - loss: 0.3289 - acc: 0.9542 - val_loss: 0.8986 - val_acc: 0.6170\n",
            "Epoch 249/300\n",
            "\n",
            "Epoch 00249: val_loss did not improve from 0.76864\n",
            "240/240 - 0s - loss: 0.3260 - acc: 0.9375 - val_loss: 0.8271 - val_acc: 0.6383\n",
            "Epoch 250/300\n",
            "\n",
            "Epoch 00250: val_loss did not improve from 0.76864\n",
            "240/240 - 0s - loss: 0.3109 - acc: 0.9333 - val_loss: 0.8609 - val_acc: 0.5957\n",
            "Epoch 251/300\n",
            "\n",
            "Epoch 00251: val_loss did not improve from 0.76864\n",
            "240/240 - 0s - loss: 0.3301 - acc: 0.9292 - val_loss: 0.8133 - val_acc: 0.6596\n",
            "Epoch 252/300\n",
            "\n",
            "Epoch 00252: val_loss did not improve from 0.76864\n",
            "240/240 - 0s - loss: 0.3233 - acc: 0.9458 - val_loss: 0.8794 - val_acc: 0.6170\n",
            "Epoch 253/300\n",
            "\n",
            "Epoch 00253: val_loss did not improve from 0.76864\n",
            "240/240 - 0s - loss: 0.3134 - acc: 0.9375 - val_loss: 0.8535 - val_acc: 0.5745\n",
            "Epoch 254/300\n",
            "\n",
            "Epoch 00254: val_loss did not improve from 0.76864\n",
            "240/240 - 0s - loss: 0.3359 - acc: 0.9167 - val_loss: 0.8607 - val_acc: 0.5957\n",
            "Epoch 255/300\n",
            "\n",
            "Epoch 00255: val_loss did not improve from 0.76864\n",
            "240/240 - 0s - loss: 0.3274 - acc: 0.9292 - val_loss: 0.9111 - val_acc: 0.5745\n",
            "Epoch 256/300\n",
            "\n",
            "Epoch 00256: val_loss did not improve from 0.76864\n",
            "240/240 - 0s - loss: 0.3447 - acc: 0.9292 - val_loss: 0.8766 - val_acc: 0.6809\n",
            "Epoch 257/300\n",
            "\n",
            "Epoch 00257: val_loss did not improve from 0.76864\n",
            "240/240 - 0s - loss: 0.3468 - acc: 0.9292 - val_loss: 0.8557 - val_acc: 0.6170\n",
            "Epoch 258/300\n",
            "\n",
            "Epoch 00258: val_loss did not improve from 0.76864\n",
            "240/240 - 0s - loss: 0.3033 - acc: 0.9625 - val_loss: 0.8875 - val_acc: 0.6170\n",
            "Epoch 259/300\n",
            "\n",
            "Epoch 00259: val_loss did not improve from 0.76864\n",
            "240/240 - 0s - loss: 0.2943 - acc: 0.9375 - val_loss: 0.9124 - val_acc: 0.5957\n",
            "Epoch 260/300\n",
            "\n",
            "Epoch 00260: val_loss did not improve from 0.76864\n",
            "240/240 - 0s - loss: 0.2960 - acc: 0.9458 - val_loss: 0.8388 - val_acc: 0.6383\n",
            "Epoch 261/300\n",
            "\n",
            "Epoch 00261: val_loss did not improve from 0.76864\n",
            "240/240 - 0s - loss: 0.3096 - acc: 0.9458 - val_loss: 0.8968 - val_acc: 0.5532\n",
            "Epoch 262/300\n",
            "\n",
            "Epoch 00262: val_loss did not improve from 0.76864\n",
            "240/240 - 0s - loss: 0.2771 - acc: 0.9500 - val_loss: 0.8382 - val_acc: 0.5957\n",
            "Epoch 263/300\n",
            "\n",
            "Epoch 00263: val_loss did not improve from 0.76864\n",
            "240/240 - 0s - loss: 0.3119 - acc: 0.9292 - val_loss: 0.8771 - val_acc: 0.5745\n",
            "Epoch 264/300\n",
            "\n",
            "Epoch 00264: val_loss did not improve from 0.76864\n",
            "240/240 - 0s - loss: 0.3531 - acc: 0.9250 - val_loss: 0.8371 - val_acc: 0.6383\n",
            "Epoch 265/300\n",
            "\n",
            "Epoch 00265: val_loss did not improve from 0.76864\n",
            "240/240 - 0s - loss: 0.3182 - acc: 0.9333 - val_loss: 0.8550 - val_acc: 0.5957\n",
            "Epoch 266/300\n",
            "\n",
            "Epoch 00266: val_loss did not improve from 0.76864\n",
            "240/240 - 0s - loss: 0.3405 - acc: 0.9167 - val_loss: 0.9066 - val_acc: 0.5745\n",
            "Epoch 267/300\n",
            "\n",
            "Epoch 00267: val_loss did not improve from 0.76864\n",
            "240/240 - 0s - loss: 0.3185 - acc: 0.9417 - val_loss: 0.8822 - val_acc: 0.5957\n",
            "Epoch 268/300\n",
            "\n",
            "Epoch 00268: val_loss did not improve from 0.76864\n",
            "240/240 - 0s - loss: 0.3207 - acc: 0.9333 - val_loss: 0.8773 - val_acc: 0.6170\n",
            "Epoch 269/300\n",
            "\n",
            "Epoch 00269: val_loss did not improve from 0.76864\n",
            "240/240 - 0s - loss: 0.3143 - acc: 0.9333 - val_loss: 0.8485 - val_acc: 0.5957\n",
            "Epoch 270/300\n",
            "\n",
            "Epoch 00270: val_loss did not improve from 0.76864\n",
            "240/240 - 0s - loss: 0.2917 - acc: 0.9625 - val_loss: 0.7970 - val_acc: 0.7234\n",
            "Epoch 271/300\n",
            "\n",
            "Epoch 00271: val_loss did not improve from 0.76864\n",
            "240/240 - 0s - loss: 0.3239 - acc: 0.9208 - val_loss: 0.8477 - val_acc: 0.5745\n",
            "Epoch 272/300\n",
            "\n",
            "Epoch 00272: val_loss did not improve from 0.76864\n",
            "240/240 - 0s - loss: 0.3355 - acc: 0.9292 - val_loss: 0.8164 - val_acc: 0.6170\n",
            "Epoch 273/300\n",
            "\n",
            "Epoch 00273: val_loss did not improve from 0.76864\n",
            "240/240 - 0s - loss: 0.3057 - acc: 0.9292 - val_loss: 0.8744 - val_acc: 0.5957\n",
            "Epoch 274/300\n",
            "\n",
            "Epoch 00274: val_loss did not improve from 0.76864\n",
            "240/240 - 0s - loss: 0.3124 - acc: 0.9375 - val_loss: 0.8336 - val_acc: 0.6170\n",
            "Epoch 275/300\n",
            "\n",
            "Epoch 00275: val_loss did not improve from 0.76864\n",
            "240/240 - 0s - loss: 0.3113 - acc: 0.9542 - val_loss: 0.8466 - val_acc: 0.5532\n",
            "Epoch 276/300\n",
            "\n",
            "Epoch 00276: val_loss did not improve from 0.76864\n",
            "240/240 - 0s - loss: 0.2869 - acc: 0.9333 - val_loss: 0.8916 - val_acc: 0.5957\n",
            "Epoch 277/300\n",
            "\n",
            "Epoch 00277: val_loss did not improve from 0.76864\n",
            "240/240 - 0s - loss: 0.3098 - acc: 0.9375 - val_loss: 0.8340 - val_acc: 0.6170\n",
            "Epoch 278/300\n",
            "\n",
            "Epoch 00278: val_loss did not improve from 0.76864\n",
            "240/240 - 0s - loss: 0.3321 - acc: 0.9083 - val_loss: 0.8218 - val_acc: 0.6170\n",
            "Epoch 279/300\n",
            "\n",
            "Epoch 00279: val_loss did not improve from 0.76864\n",
            "240/240 - 0s - loss: 0.2863 - acc: 0.9542 - val_loss: 0.8359 - val_acc: 0.6383\n",
            "Epoch 280/300\n",
            "\n",
            "Epoch 00280: val_loss did not improve from 0.76864\n",
            "240/240 - 0s - loss: 0.2869 - acc: 0.9542 - val_loss: 0.8670 - val_acc: 0.6170\n",
            "Epoch 281/300\n",
            "\n",
            "Epoch 00281: val_loss did not improve from 0.76864\n",
            "240/240 - 0s - loss: 0.3618 - acc: 0.8958 - val_loss: 0.8663 - val_acc: 0.6170\n",
            "Epoch 282/300\n",
            "\n",
            "Epoch 00282: val_loss did not improve from 0.76864\n",
            "240/240 - 0s - loss: 0.3340 - acc: 0.9250 - val_loss: 0.9359 - val_acc: 0.5319\n",
            "Epoch 283/300\n",
            "\n",
            "Epoch 00283: val_loss did not improve from 0.76864\n",
            "240/240 - 0s - loss: 0.3012 - acc: 0.9583 - val_loss: 0.8232 - val_acc: 0.6383\n",
            "Epoch 284/300\n",
            "\n",
            "Epoch 00284: val_loss did not improve from 0.76864\n",
            "240/240 - 0s - loss: 0.3497 - acc: 0.9250 - val_loss: 0.8844 - val_acc: 0.5745\n",
            "Epoch 285/300\n",
            "\n",
            "Epoch 00285: val_loss did not improve from 0.76864\n",
            "240/240 - 0s - loss: 0.2984 - acc: 0.9542 - val_loss: 0.8505 - val_acc: 0.5532\n",
            "Epoch 286/300\n",
            "\n",
            "Epoch 00286: val_loss did not improve from 0.76864\n",
            "240/240 - 0s - loss: 0.3027 - acc: 0.9250 - val_loss: 0.8962 - val_acc: 0.5745\n",
            "Epoch 287/300\n",
            "\n",
            "Epoch 00287: val_loss did not improve from 0.76864\n",
            "240/240 - 0s - loss: 0.2761 - acc: 0.9375 - val_loss: 0.8110 - val_acc: 0.6809\n",
            "Epoch 288/300\n",
            "\n",
            "Epoch 00288: val_loss did not improve from 0.76864\n",
            "240/240 - 0s - loss: 0.2547 - acc: 0.9583 - val_loss: 0.8475 - val_acc: 0.5745\n",
            "Epoch 289/300\n",
            "\n",
            "Epoch 00289: val_loss did not improve from 0.76864\n",
            "240/240 - 0s - loss: 0.2634 - acc: 0.9417 - val_loss: 0.8730 - val_acc: 0.6170\n",
            "Epoch 290/300\n",
            "\n",
            "Epoch 00290: val_loss did not improve from 0.76864\n",
            "240/240 - 0s - loss: 0.2640 - acc: 0.9458 - val_loss: 0.8610 - val_acc: 0.5957\n",
            "Epoch 291/300\n",
            "\n",
            "Epoch 00291: val_loss did not improve from 0.76864\n",
            "240/240 - 0s - loss: 0.2702 - acc: 0.9583 - val_loss: 0.8390 - val_acc: 0.6383\n",
            "Epoch 292/300\n",
            "\n",
            "Epoch 00292: val_loss did not improve from 0.76864\n",
            "240/240 - 0s - loss: 0.3027 - acc: 0.9375 - val_loss: 0.8590 - val_acc: 0.5957\n",
            "Epoch 293/300\n",
            "\n",
            "Epoch 00293: val_loss did not improve from 0.76864\n",
            "240/240 - 0s - loss: 0.3018 - acc: 0.9375 - val_loss: 0.9451 - val_acc: 0.6170\n",
            "Epoch 294/300\n",
            "\n",
            "Epoch 00294: val_loss did not improve from 0.76864\n",
            "240/240 - 0s - loss: 0.2793 - acc: 0.9333 - val_loss: 0.8920 - val_acc: 0.5745\n",
            "Epoch 295/300\n",
            "\n",
            "Epoch 00295: val_loss did not improve from 0.76864\n",
            "240/240 - 0s - loss: 0.2853 - acc: 0.9500 - val_loss: 0.8392 - val_acc: 0.6170\n",
            "Epoch 296/300\n",
            "\n",
            "Epoch 00296: val_loss did not improve from 0.76864\n",
            "240/240 - 0s - loss: 0.2641 - acc: 0.9583 - val_loss: 0.7893 - val_acc: 0.6383\n",
            "Epoch 297/300\n",
            "\n",
            "Epoch 00297: val_loss did not improve from 0.76864\n",
            "240/240 - 0s - loss: 0.3114 - acc: 0.9167 - val_loss: 0.8599 - val_acc: 0.5532\n",
            "Epoch 298/300\n",
            "\n",
            "Epoch 00298: val_loss did not improve from 0.76864\n",
            "240/240 - 0s - loss: 0.2812 - acc: 0.9375 - val_loss: 0.8451 - val_acc: 0.6383\n",
            "Epoch 299/300\n",
            "\n",
            "Epoch 00299: val_loss did not improve from 0.76864\n",
            "240/240 - 0s - loss: 0.2778 - acc: 0.9458 - val_loss: 0.8494 - val_acc: 0.5957\n",
            "Epoch 300/300\n",
            "\n",
            "Epoch 00300: val_loss did not improve from 0.76864\n",
            "240/240 - 0s - loss: 0.3047 - acc: 0.9333 - val_loss: 0.8251 - val_acc: 0.6170\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nWqz7wIQ4AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "# Evaluate on test data\n",
            "288/288 [==============================] - 1s 2ms/sample - loss: 1.0079 - acc: 0.5590\n",
            "test loss, test acc: [1.0079053095485304, 0.5590278]\n",
            "EEG_Deep/Data2A/Data_A05T.mat\n",
            "(288, 22, 1875)\n",
            "(288, 1)\n",
            "(240, 22, 750)\n",
            "(240, 1)\n",
            "(47, 22, 750)\n",
            "(47, 1)\n",
            "X_train shape: (240, 1, 22, 750)\n",
            "240 train samples\n",
            "47 val samples\n",
            "EEG_Deep/Data2A/Data_A05E.mat\n",
            "(288, 22, 1875)\n",
            "(288, 1)\n",
            "(288, 22, 750)\n",
            "(288, 1)\n",
            "X_train shape: (288, 1, 22, 750)\n",
            "288 train samples\n",
            "Train on 240 samples, validate on 47 samples\n",
            "Epoch 1/300\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 1.38172, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 1s - loss: 1.4026 - acc: 0.2333 - val_loss: 1.3817 - val_acc: 0.2766\n",
            "Epoch 2/300\n",
            "\n",
            "Epoch 00002: val_loss improved from 1.38172 to 1.37987, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.3513 - acc: 0.3958 - val_loss: 1.3799 - val_acc: 0.3830\n",
            "Epoch 3/300\n",
            "\n",
            "Epoch 00003: val_loss improved from 1.37987 to 1.37722, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.3400 - acc: 0.3667 - val_loss: 1.3772 - val_acc: 0.4043\n",
            "Epoch 4/300\n",
            "\n",
            "Epoch 00004: val_loss improved from 1.37722 to 1.37330, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.3054 - acc: 0.4750 - val_loss: 1.3733 - val_acc: 0.4255\n",
            "Epoch 5/300\n",
            "\n",
            "Epoch 00005: val_loss improved from 1.37330 to 1.37104, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.2739 - acc: 0.5292 - val_loss: 1.3710 - val_acc: 0.3830\n",
            "Epoch 6/300\n",
            "\n",
            "Epoch 00006: val_loss improved from 1.37104 to 1.36708, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.2550 - acc: 0.5625 - val_loss: 1.3671 - val_acc: 0.4043\n",
            "Epoch 7/300\n",
            "\n",
            "Epoch 00007: val_loss improved from 1.36708 to 1.36138, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.2381 - acc: 0.5583 - val_loss: 1.3614 - val_acc: 0.3830\n",
            "Epoch 8/300\n",
            "\n",
            "Epoch 00008: val_loss improved from 1.36138 to 1.35561, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.2117 - acc: 0.5833 - val_loss: 1.3556 - val_acc: 0.3830\n",
            "Epoch 9/300\n",
            "\n",
            "Epoch 00009: val_loss improved from 1.35561 to 1.35409, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.1891 - acc: 0.6042 - val_loss: 1.3541 - val_acc: 0.3404\n",
            "Epoch 10/300\n",
            "\n",
            "Epoch 00010: val_loss improved from 1.35409 to 1.34867, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.1667 - acc: 0.6000 - val_loss: 1.3487 - val_acc: 0.3191\n",
            "Epoch 11/300\n",
            "\n",
            "Epoch 00011: val_loss improved from 1.34867 to 1.34264, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.1498 - acc: 0.6042 - val_loss: 1.3426 - val_acc: 0.3404\n",
            "Epoch 12/300\n",
            "\n",
            "Epoch 00012: val_loss improved from 1.34264 to 1.33925, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.1377 - acc: 0.6292 - val_loss: 1.3392 - val_acc: 0.3191\n",
            "Epoch 13/300\n",
            "\n",
            "Epoch 00013: val_loss improved from 1.33925 to 1.33721, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.1360 - acc: 0.5833 - val_loss: 1.3372 - val_acc: 0.2766\n",
            "Epoch 14/300\n",
            "\n",
            "Epoch 00014: val_loss improved from 1.33721 to 1.33661, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.1128 - acc: 0.6500 - val_loss: 1.3366 - val_acc: 0.2979\n",
            "Epoch 15/300\n",
            "\n",
            "Epoch 00015: val_loss improved from 1.33661 to 1.33527, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.1019 - acc: 0.6417 - val_loss: 1.3353 - val_acc: 0.3404\n",
            "Epoch 16/300\n",
            "\n",
            "Epoch 00016: val_loss improved from 1.33527 to 1.32463, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.0962 - acc: 0.6292 - val_loss: 1.3246 - val_acc: 0.3191\n",
            "Epoch 17/300\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 1.32463\n",
            "240/240 - 0s - loss: 1.0806 - acc: 0.6500 - val_loss: 1.3268 - val_acc: 0.2979\n",
            "Epoch 18/300\n",
            "\n",
            "Epoch 00018: val_loss improved from 1.32463 to 1.31957, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.0912 - acc: 0.6500 - val_loss: 1.3196 - val_acc: 0.2979\n",
            "Epoch 19/300\n",
            "\n",
            "Epoch 00019: val_loss improved from 1.31957 to 1.31073, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.0719 - acc: 0.6417 - val_loss: 1.3107 - val_acc: 0.2766\n",
            "Epoch 20/300\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 1.31073\n",
            "240/240 - 0s - loss: 1.0730 - acc: 0.6167 - val_loss: 1.3160 - val_acc: 0.3404\n",
            "Epoch 21/300\n",
            "\n",
            "Epoch 00021: val_loss improved from 1.31073 to 1.30989, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.0432 - acc: 0.6792 - val_loss: 1.3099 - val_acc: 0.3830\n",
            "Epoch 22/300\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 1.30989\n",
            "240/240 - 0s - loss: 1.0302 - acc: 0.6875 - val_loss: 1.3113 - val_acc: 0.2979\n",
            "Epoch 23/300\n",
            "\n",
            "Epoch 00023: val_loss improved from 1.30989 to 1.30156, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.0483 - acc: 0.6750 - val_loss: 1.3016 - val_acc: 0.3830\n",
            "Epoch 24/300\n",
            "\n",
            "Epoch 00024: val_loss improved from 1.30156 to 1.29667, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.0437 - acc: 0.6542 - val_loss: 1.2967 - val_acc: 0.3617\n",
            "Epoch 25/300\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 1.29667\n",
            "240/240 - 0s - loss: 1.0438 - acc: 0.6833 - val_loss: 1.2999 - val_acc: 0.3830\n",
            "Epoch 26/300\n",
            "\n",
            "Epoch 00026: val_loss improved from 1.29667 to 1.29155, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.0035 - acc: 0.6958 - val_loss: 1.2915 - val_acc: 0.4043\n",
            "Epoch 27/300\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 1.29155\n",
            "240/240 - 0s - loss: 1.0146 - acc: 0.6583 - val_loss: 1.2975 - val_acc: 0.3830\n",
            "Epoch 28/300\n",
            "\n",
            "Epoch 00028: val_loss improved from 1.29155 to 1.28929, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.9866 - acc: 0.7417 - val_loss: 1.2893 - val_acc: 0.4255\n",
            "Epoch 29/300\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 1.28929\n",
            "240/240 - 0s - loss: 0.9990 - acc: 0.7375 - val_loss: 1.2946 - val_acc: 0.3830\n",
            "Epoch 30/300\n",
            "\n",
            "Epoch 00030: val_loss improved from 1.28929 to 1.28378, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.9752 - acc: 0.7000 - val_loss: 1.2838 - val_acc: 0.4043\n",
            "Epoch 31/300\n",
            "\n",
            "Epoch 00031: val_loss improved from 1.28378 to 1.27753, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.9639 - acc: 0.7458 - val_loss: 1.2775 - val_acc: 0.3617\n",
            "Epoch 32/300\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 1.27753\n",
            "240/240 - 0s - loss: 0.9862 - acc: 0.7000 - val_loss: 1.2838 - val_acc: 0.3617\n",
            "Epoch 33/300\n",
            "\n",
            "Epoch 00033: val_loss improved from 1.27753 to 1.27618, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.9697 - acc: 0.7250 - val_loss: 1.2762 - val_acc: 0.3617\n",
            "Epoch 34/300\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 1.27618\n",
            "240/240 - 0s - loss: 0.9776 - acc: 0.7000 - val_loss: 1.2869 - val_acc: 0.3617\n",
            "Epoch 35/300\n",
            "\n",
            "Epoch 00035: val_loss improved from 1.27618 to 1.27092, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.9557 - acc: 0.6958 - val_loss: 1.2709 - val_acc: 0.4468\n",
            "Epoch 36/300\n",
            "\n",
            "Epoch 00036: val_loss improved from 1.27092 to 1.26946, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.9624 - acc: 0.7083 - val_loss: 1.2695 - val_acc: 0.4255\n",
            "Epoch 37/300\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 1.26946\n",
            "240/240 - 0s - loss: 0.9506 - acc: 0.7333 - val_loss: 1.2796 - val_acc: 0.4255\n",
            "Epoch 38/300\n",
            "\n",
            "Epoch 00038: val_loss improved from 1.26946 to 1.25293, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.9402 - acc: 0.7083 - val_loss: 1.2529 - val_acc: 0.4255\n",
            "Epoch 39/300\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 1.25293\n",
            "240/240 - 0s - loss: 0.9440 - acc: 0.7292 - val_loss: 1.2754 - val_acc: 0.4043\n",
            "Epoch 40/300\n",
            "\n",
            "Epoch 00040: val_loss improved from 1.25293 to 1.24878, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.9341 - acc: 0.7292 - val_loss: 1.2488 - val_acc: 0.4681\n",
            "Epoch 41/300\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 1.24878\n",
            "240/240 - 0s - loss: 0.9277 - acc: 0.7333 - val_loss: 1.2681 - val_acc: 0.3617\n",
            "Epoch 42/300\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 1.24878\n",
            "240/240 - 0s - loss: 0.9260 - acc: 0.7250 - val_loss: 1.2742 - val_acc: 0.3404\n",
            "Epoch 43/300\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 1.24878\n",
            "240/240 - 0s - loss: 0.9149 - acc: 0.7500 - val_loss: 1.2612 - val_acc: 0.4255\n",
            "Epoch 44/300\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 1.24878\n",
            "240/240 - 0s - loss: 0.9293 - acc: 0.7417 - val_loss: 1.2612 - val_acc: 0.4255\n",
            "Epoch 45/300\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 1.24878\n",
            "240/240 - 0s - loss: 0.9312 - acc: 0.7417 - val_loss: 1.2790 - val_acc: 0.4255\n",
            "Epoch 46/300\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 1.24878\n",
            "240/240 - 0s - loss: 0.8992 - acc: 0.7250 - val_loss: 1.2663 - val_acc: 0.4255\n",
            "Epoch 47/300\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 1.24878\n",
            "240/240 - 0s - loss: 0.9097 - acc: 0.7417 - val_loss: 1.2694 - val_acc: 0.3830\n",
            "Epoch 48/300\n",
            "\n",
            "Epoch 00048: val_loss improved from 1.24878 to 1.24710, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.9019 - acc: 0.7500 - val_loss: 1.2471 - val_acc: 0.4894\n",
            "Epoch 49/300\n",
            "\n",
            "Epoch 00049: val_loss improved from 1.24710 to 1.24332, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.8821 - acc: 0.7792 - val_loss: 1.2433 - val_acc: 0.4468\n",
            "Epoch 50/300\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 1.24332\n",
            "240/240 - 0s - loss: 0.8805 - acc: 0.7417 - val_loss: 1.2771 - val_acc: 0.4681\n",
            "Epoch 51/300\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 1.24332\n",
            "240/240 - 0s - loss: 0.8868 - acc: 0.7750 - val_loss: 1.2602 - val_acc: 0.4468\n",
            "Epoch 52/300\n",
            "\n",
            "Epoch 00052: val_loss improved from 1.24332 to 1.23350, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.8746 - acc: 0.7500 - val_loss: 1.2335 - val_acc: 0.4681\n",
            "Epoch 53/300\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 1.23350\n",
            "240/240 - 0s - loss: 0.8617 - acc: 0.7583 - val_loss: 1.2434 - val_acc: 0.5106\n",
            "Epoch 54/300\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 1.23350\n",
            "240/240 - 0s - loss: 0.8543 - acc: 0.7500 - val_loss: 1.2656 - val_acc: 0.4681\n",
            "Epoch 55/300\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 1.23350\n",
            "240/240 - 0s - loss: 0.8285 - acc: 0.7833 - val_loss: 1.2375 - val_acc: 0.4894\n",
            "Epoch 56/300\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 1.23350\n",
            "240/240 - 0s - loss: 0.8716 - acc: 0.7250 - val_loss: 1.2964 - val_acc: 0.5106\n",
            "Epoch 57/300\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 1.23350\n",
            "240/240 - 0s - loss: 0.8504 - acc: 0.7375 - val_loss: 1.2364 - val_acc: 0.4255\n",
            "Epoch 58/300\n",
            "\n",
            "Epoch 00058: val_loss improved from 1.23350 to 1.21299, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.8253 - acc: 0.7625 - val_loss: 1.2130 - val_acc: 0.4681\n",
            "Epoch 59/300\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 1.21299\n",
            "240/240 - 0s - loss: 0.8288 - acc: 0.7292 - val_loss: 1.2582 - val_acc: 0.4468\n",
            "Epoch 60/300\n",
            "\n",
            "Epoch 00060: val_loss improved from 1.21299 to 1.21259, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.8045 - acc: 0.7625 - val_loss: 1.2126 - val_acc: 0.5106\n",
            "Epoch 61/300\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 1.21259\n",
            "240/240 - 0s - loss: 0.7991 - acc: 0.7458 - val_loss: 1.2723 - val_acc: 0.4255\n",
            "Epoch 62/300\n",
            "\n",
            "Epoch 00062: val_loss improved from 1.21259 to 1.20565, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.7727 - acc: 0.8125 - val_loss: 1.2057 - val_acc: 0.5106\n",
            "Epoch 63/300\n",
            "\n",
            "Epoch 00063: val_loss improved from 1.20565 to 1.18872, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.8045 - acc: 0.7792 - val_loss: 1.1887 - val_acc: 0.4681\n",
            "Epoch 64/300\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 1.18872\n",
            "240/240 - 0s - loss: 0.7867 - acc: 0.8208 - val_loss: 1.2410 - val_acc: 0.4255\n",
            "Epoch 65/300\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 1.18872\n",
            "240/240 - 0s - loss: 0.7921 - acc: 0.7792 - val_loss: 1.2036 - val_acc: 0.4468\n",
            "Epoch 66/300\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 1.18872\n",
            "240/240 - 0s - loss: 0.8218 - acc: 0.7458 - val_loss: 1.2210 - val_acc: 0.5106\n",
            "Epoch 67/300\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 1.18872\n",
            "240/240 - 0s - loss: 0.7716 - acc: 0.8083 - val_loss: 1.2482 - val_acc: 0.4468\n",
            "Epoch 68/300\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 1.18872\n",
            "240/240 - 0s - loss: 0.7559 - acc: 0.7917 - val_loss: 1.2169 - val_acc: 0.4894\n",
            "Epoch 69/300\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 1.18872\n",
            "240/240 - 0s - loss: 0.7680 - acc: 0.7875 - val_loss: 1.2128 - val_acc: 0.5106\n",
            "Epoch 70/300\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 1.18872\n",
            "240/240 - 0s - loss: 0.7744 - acc: 0.7792 - val_loss: 1.1912 - val_acc: 0.5745\n",
            "Epoch 71/300\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 1.18872\n",
            "240/240 - 0s - loss: 0.7609 - acc: 0.8250 - val_loss: 1.2070 - val_acc: 0.5319\n",
            "Epoch 72/300\n",
            "\n",
            "Epoch 00072: val_loss improved from 1.18872 to 1.15783, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.7893 - acc: 0.7750 - val_loss: 1.1578 - val_acc: 0.4894\n",
            "Epoch 73/300\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 1.15783\n",
            "240/240 - 0s - loss: 0.7719 - acc: 0.7583 - val_loss: 1.1769 - val_acc: 0.5106\n",
            "Epoch 74/300\n",
            "\n",
            "Epoch 00074: val_loss improved from 1.15783 to 1.13313, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.7456 - acc: 0.7958 - val_loss: 1.1331 - val_acc: 0.4894\n",
            "Epoch 75/300\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 1.13313\n",
            "240/240 - 0s - loss: 0.7567 - acc: 0.8000 - val_loss: 1.1748 - val_acc: 0.4894\n",
            "Epoch 76/300\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 1.13313\n",
            "240/240 - 0s - loss: 0.7420 - acc: 0.7750 - val_loss: 1.1789 - val_acc: 0.5106\n",
            "Epoch 77/300\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 1.13313\n",
            "240/240 - 0s - loss: 0.7528 - acc: 0.7958 - val_loss: 1.1687 - val_acc: 0.4468\n",
            "Epoch 78/300\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 1.13313\n",
            "240/240 - 0s - loss: 0.7490 - acc: 0.7708 - val_loss: 1.1346 - val_acc: 0.4894\n",
            "Epoch 79/300\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 1.13313\n",
            "240/240 - 0s - loss: 0.7507 - acc: 0.7792 - val_loss: 1.1441 - val_acc: 0.4681\n",
            "Epoch 80/300\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 1.13313\n",
            "240/240 - 0s - loss: 0.7630 - acc: 0.7792 - val_loss: 1.1580 - val_acc: 0.5745\n",
            "Epoch 81/300\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 1.13313\n",
            "240/240 - 0s - loss: 0.7354 - acc: 0.7917 - val_loss: 1.1586 - val_acc: 0.5106\n",
            "Epoch 82/300\n",
            "\n",
            "Epoch 00082: val_loss improved from 1.13313 to 1.11940, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.7136 - acc: 0.8208 - val_loss: 1.1194 - val_acc: 0.5106\n",
            "Epoch 83/300\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 1.11940\n",
            "240/240 - 0s - loss: 0.7015 - acc: 0.7958 - val_loss: 1.1541 - val_acc: 0.5532\n",
            "Epoch 84/300\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 1.11940\n",
            "240/240 - 0s - loss: 0.7039 - acc: 0.8125 - val_loss: 1.1409 - val_acc: 0.5745\n",
            "Epoch 85/300\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 1.11940\n",
            "240/240 - 0s - loss: 0.6993 - acc: 0.7917 - val_loss: 1.1458 - val_acc: 0.5319\n",
            "Epoch 86/300\n",
            "\n",
            "Epoch 00086: val_loss improved from 1.11940 to 1.11794, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.7280 - acc: 0.7625 - val_loss: 1.1179 - val_acc: 0.5106\n",
            "Epoch 87/300\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 1.11794\n",
            "240/240 - 0s - loss: 0.7280 - acc: 0.7833 - val_loss: 1.1560 - val_acc: 0.5106\n",
            "Epoch 88/300\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 1.11794\n",
            "240/240 - 0s - loss: 0.7258 - acc: 0.7958 - val_loss: 1.1281 - val_acc: 0.5532\n",
            "Epoch 89/300\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 1.11794\n",
            "240/240 - 0s - loss: 0.7356 - acc: 0.7917 - val_loss: 1.1331 - val_acc: 0.4894\n",
            "Epoch 90/300\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 1.11794\n",
            "240/240 - 0s - loss: 0.7011 - acc: 0.8000 - val_loss: 1.1242 - val_acc: 0.4681\n",
            "Epoch 91/300\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 1.11794\n",
            "240/240 - 0s - loss: 0.7205 - acc: 0.7875 - val_loss: 1.1726 - val_acc: 0.5106\n",
            "Epoch 92/300\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 1.11794\n",
            "240/240 - 0s - loss: 0.7167 - acc: 0.7833 - val_loss: 1.1307 - val_acc: 0.5106\n",
            "Epoch 93/300\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 1.11794\n",
            "240/240 - 0s - loss: 0.7309 - acc: 0.7958 - val_loss: 1.1544 - val_acc: 0.5532\n",
            "Epoch 94/300\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 1.11794\n",
            "240/240 - 0s - loss: 0.6979 - acc: 0.7750 - val_loss: 1.1310 - val_acc: 0.5319\n",
            "Epoch 95/300\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 1.11794\n",
            "240/240 - 0s - loss: 0.7011 - acc: 0.8375 - val_loss: 1.1767 - val_acc: 0.5745\n",
            "Epoch 96/300\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 1.11794\n",
            "240/240 - 0s - loss: 0.6671 - acc: 0.8333 - val_loss: 1.1350 - val_acc: 0.5319\n",
            "Epoch 97/300\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 1.11794\n",
            "240/240 - 0s - loss: 0.6633 - acc: 0.8375 - val_loss: 1.1658 - val_acc: 0.5319\n",
            "Epoch 98/300\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 1.11794\n",
            "240/240 - 0s - loss: 0.7141 - acc: 0.7875 - val_loss: 1.1656 - val_acc: 0.5319\n",
            "Epoch 99/300\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 1.11794\n",
            "240/240 - 0s - loss: 0.6865 - acc: 0.8250 - val_loss: 1.1411 - val_acc: 0.5319\n",
            "Epoch 100/300\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 1.11794\n",
            "240/240 - 0s - loss: 0.6560 - acc: 0.8167 - val_loss: 1.1277 - val_acc: 0.5532\n",
            "Epoch 101/300\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 1.11794\n",
            "240/240 - 0s - loss: 0.6971 - acc: 0.8083 - val_loss: 1.1237 - val_acc: 0.4681\n",
            "Epoch 102/300\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 1.11794\n",
            "240/240 - 0s - loss: 0.6879 - acc: 0.8125 - val_loss: 1.1605 - val_acc: 0.5106\n",
            "Epoch 103/300\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 1.11794\n",
            "240/240 - 0s - loss: 0.6964 - acc: 0.7958 - val_loss: 1.1200 - val_acc: 0.5106\n",
            "Epoch 104/300\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 1.11794\n",
            "240/240 - 0s - loss: 0.6469 - acc: 0.8292 - val_loss: 1.1706 - val_acc: 0.5532\n",
            "Epoch 105/300\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 1.11794\n",
            "240/240 - 0s - loss: 0.6821 - acc: 0.8208 - val_loss: 1.1200 - val_acc: 0.4468\n",
            "Epoch 106/300\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 1.11794\n",
            "240/240 - 0s - loss: 0.6585 - acc: 0.8000 - val_loss: 1.2261 - val_acc: 0.5319\n",
            "Epoch 107/300\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 1.11794\n",
            "240/240 - 0s - loss: 0.6722 - acc: 0.8000 - val_loss: 1.1263 - val_acc: 0.4468\n",
            "Epoch 108/300\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 1.11794\n",
            "240/240 - 0s - loss: 0.6800 - acc: 0.8042 - val_loss: 1.1827 - val_acc: 0.5319\n",
            "Epoch 109/300\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 1.11794\n",
            "240/240 - 0s - loss: 0.6678 - acc: 0.8000 - val_loss: 1.1357 - val_acc: 0.5532\n",
            "Epoch 110/300\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 1.11794\n",
            "240/240 - 0s - loss: 0.6399 - acc: 0.8417 - val_loss: 1.1358 - val_acc: 0.4681\n",
            "Epoch 111/300\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 1.11794\n",
            "240/240 - 0s - loss: 0.6567 - acc: 0.8042 - val_loss: 1.1291 - val_acc: 0.5319\n",
            "Epoch 112/300\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 1.11794\n",
            "240/240 - 0s - loss: 0.6685 - acc: 0.7958 - val_loss: 1.1246 - val_acc: 0.4894\n",
            "Epoch 113/300\n",
            "\n",
            "Epoch 00113: val_loss improved from 1.11794 to 1.05641, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.6583 - acc: 0.8167 - val_loss: 1.0564 - val_acc: 0.5106\n",
            "Epoch 114/300\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 1.05641\n",
            "240/240 - 0s - loss: 0.6552 - acc: 0.7875 - val_loss: 1.0908 - val_acc: 0.5106\n",
            "Epoch 115/300\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 1.05641\n",
            "240/240 - 0s - loss: 0.6577 - acc: 0.8000 - val_loss: 1.1046 - val_acc: 0.5532\n",
            "Epoch 116/300\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 1.05641\n",
            "240/240 - 0s - loss: 0.6003 - acc: 0.8292 - val_loss: 1.0968 - val_acc: 0.6170\n",
            "Epoch 117/300\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 1.05641\n",
            "240/240 - 0s - loss: 0.6308 - acc: 0.8500 - val_loss: 1.1166 - val_acc: 0.5319\n",
            "Epoch 118/300\n",
            "\n",
            "Epoch 00118: val_loss improved from 1.05641 to 1.05072, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.6403 - acc: 0.8208 - val_loss: 1.0507 - val_acc: 0.4894\n",
            "Epoch 119/300\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 1.05072\n",
            "240/240 - 0s - loss: 0.5961 - acc: 0.8417 - val_loss: 1.0866 - val_acc: 0.5106\n",
            "Epoch 120/300\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 1.05072\n",
            "240/240 - 0s - loss: 0.6205 - acc: 0.8250 - val_loss: 1.1068 - val_acc: 0.5106\n",
            "Epoch 121/300\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 1.05072\n",
            "240/240 - 0s - loss: 0.6731 - acc: 0.7917 - val_loss: 1.0697 - val_acc: 0.4043\n",
            "Epoch 122/300\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 1.05072\n",
            "240/240 - 0s - loss: 0.6375 - acc: 0.7792 - val_loss: 1.0802 - val_acc: 0.5319\n",
            "Epoch 123/300\n",
            "\n",
            "Epoch 00123: val_loss improved from 1.05072 to 1.00794, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.6323 - acc: 0.8167 - val_loss: 1.0079 - val_acc: 0.5319\n",
            "Epoch 124/300\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 1.00794\n",
            "240/240 - 0s - loss: 0.6362 - acc: 0.8000 - val_loss: 1.0493 - val_acc: 0.4468\n",
            "Epoch 125/300\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 1.00794\n",
            "240/240 - 0s - loss: 0.5992 - acc: 0.8542 - val_loss: 1.0531 - val_acc: 0.4468\n",
            "Epoch 126/300\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 1.00794\n",
            "240/240 - 0s - loss: 0.6339 - acc: 0.8292 - val_loss: 1.0165 - val_acc: 0.5319\n",
            "Epoch 127/300\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 1.00794\n",
            "240/240 - 0s - loss: 0.5655 - acc: 0.8625 - val_loss: 1.0538 - val_acc: 0.5319\n",
            "Epoch 128/300\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 1.00794\n",
            "240/240 - 0s - loss: 0.6068 - acc: 0.7958 - val_loss: 1.0122 - val_acc: 0.5532\n",
            "Epoch 129/300\n",
            "\n",
            "Epoch 00129: val_loss improved from 1.00794 to 0.97070, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.5577 - acc: 0.8375 - val_loss: 0.9707 - val_acc: 0.5106\n",
            "Epoch 130/300\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 0.97070\n",
            "240/240 - 0s - loss: 0.5833 - acc: 0.8375 - val_loss: 0.9971 - val_acc: 0.5532\n",
            "Epoch 131/300\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 0.97070\n",
            "240/240 - 0s - loss: 0.5821 - acc: 0.8292 - val_loss: 1.0089 - val_acc: 0.4894\n",
            "Epoch 132/300\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 0.97070\n",
            "240/240 - 0s - loss: 0.5829 - acc: 0.8417 - val_loss: 0.9862 - val_acc: 0.5532\n",
            "Epoch 133/300\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.97070\n",
            "240/240 - 0s - loss: 0.6041 - acc: 0.8417 - val_loss: 0.9750 - val_acc: 0.5106\n",
            "Epoch 134/300\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.97070\n",
            "240/240 - 0s - loss: 0.5823 - acc: 0.8375 - val_loss: 1.0102 - val_acc: 0.4681\n",
            "Epoch 135/300\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 0.97070\n",
            "240/240 - 0s - loss: 0.6116 - acc: 0.8292 - val_loss: 1.0461 - val_acc: 0.4681\n",
            "Epoch 136/300\n",
            "\n",
            "Epoch 00136: val_loss improved from 0.97070 to 0.96803, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.5610 - acc: 0.8667 - val_loss: 0.9680 - val_acc: 0.4681\n",
            "Epoch 137/300\n",
            "\n",
            "Epoch 00137: val_loss improved from 0.96803 to 0.95350, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.5868 - acc: 0.8167 - val_loss: 0.9535 - val_acc: 0.5106\n",
            "Epoch 138/300\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.95350\n",
            "240/240 - 0s - loss: 0.5656 - acc: 0.8292 - val_loss: 0.9640 - val_acc: 0.5319\n",
            "Epoch 139/300\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.95350\n",
            "240/240 - 0s - loss: 0.5810 - acc: 0.8375 - val_loss: 0.9810 - val_acc: 0.5532\n",
            "Epoch 140/300\n",
            "\n",
            "Epoch 00140: val_loss improved from 0.95350 to 0.93538, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.5738 - acc: 0.8208 - val_loss: 0.9354 - val_acc: 0.5532\n",
            "Epoch 141/300\n",
            "\n",
            "Epoch 00141: val_loss improved from 0.93538 to 0.93168, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.5713 - acc: 0.8417 - val_loss: 0.9317 - val_acc: 0.5106\n",
            "Epoch 142/300\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.93168\n",
            "240/240 - 0s - loss: 0.5695 - acc: 0.8333 - val_loss: 0.9793 - val_acc: 0.5957\n",
            "Epoch 143/300\n",
            "\n",
            "Epoch 00143: val_loss improved from 0.93168 to 0.87779, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.5333 - acc: 0.8417 - val_loss: 0.8778 - val_acc: 0.5957\n",
            "Epoch 144/300\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 0.87779\n",
            "240/240 - 0s - loss: 0.5369 - acc: 0.8583 - val_loss: 0.8903 - val_acc: 0.5957\n",
            "Epoch 145/300\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.87779\n",
            "240/240 - 0s - loss: 0.5277 - acc: 0.8792 - val_loss: 0.9204 - val_acc: 0.5745\n",
            "Epoch 146/300\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.87779\n",
            "240/240 - 0s - loss: 0.5467 - acc: 0.8708 - val_loss: 0.9154 - val_acc: 0.5957\n",
            "Epoch 147/300\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.87779\n",
            "240/240 - 0s - loss: 0.5367 - acc: 0.8500 - val_loss: 0.9027 - val_acc: 0.5745\n",
            "Epoch 148/300\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 0.87779\n",
            "240/240 - 0s - loss: 0.5760 - acc: 0.8625 - val_loss: 0.8789 - val_acc: 0.6170\n",
            "Epoch 149/300\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.87779\n",
            "240/240 - 0s - loss: 0.5710 - acc: 0.8417 - val_loss: 1.0142 - val_acc: 0.5532\n",
            "Epoch 150/300\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.87779\n",
            "240/240 - 0s - loss: 0.5383 - acc: 0.8667 - val_loss: 0.9146 - val_acc: 0.6170\n",
            "Epoch 151/300\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.87779\n",
            "240/240 - 0s - loss: 0.5414 - acc: 0.8583 - val_loss: 0.9074 - val_acc: 0.5532\n",
            "Epoch 152/300\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.87779\n",
            "240/240 - 0s - loss: 0.5235 - acc: 0.8625 - val_loss: 0.9377 - val_acc: 0.5745\n",
            "Epoch 153/300\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 0.87779\n",
            "240/240 - 0s - loss: 0.5605 - acc: 0.8458 - val_loss: 0.8951 - val_acc: 0.6383\n",
            "Epoch 154/300\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.87779\n",
            "240/240 - 0s - loss: 0.5538 - acc: 0.8375 - val_loss: 0.9079 - val_acc: 0.6809\n",
            "Epoch 155/300\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.87779\n",
            "240/240 - 0s - loss: 0.5217 - acc: 0.8625 - val_loss: 0.8879 - val_acc: 0.6596\n",
            "Epoch 156/300\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.87779\n",
            "240/240 - 0s - loss: 0.5318 - acc: 0.8333 - val_loss: 0.8990 - val_acc: 0.6170\n",
            "Epoch 157/300\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.87779\n",
            "240/240 - 0s - loss: 0.5603 - acc: 0.8208 - val_loss: 0.9284 - val_acc: 0.6170\n",
            "Epoch 158/300\n",
            "\n",
            "Epoch 00158: val_loss improved from 0.87779 to 0.85142, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.5443 - acc: 0.8500 - val_loss: 0.8514 - val_acc: 0.6596\n",
            "Epoch 159/300\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.85142\n",
            "240/240 - 0s - loss: 0.5184 - acc: 0.8833 - val_loss: 0.8623 - val_acc: 0.6170\n",
            "Epoch 160/300\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.85142\n",
            "240/240 - 0s - loss: 0.5213 - acc: 0.8750 - val_loss: 0.8578 - val_acc: 0.5319\n",
            "Epoch 161/300\n",
            "\n",
            "Epoch 00161: val_loss improved from 0.85142 to 0.84340, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.5124 - acc: 0.8500 - val_loss: 0.8434 - val_acc: 0.6170\n",
            "Epoch 162/300\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.84340\n",
            "240/240 - 0s - loss: 0.5410 - acc: 0.8875 - val_loss: 0.8495 - val_acc: 0.5957\n",
            "Epoch 163/300\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.84340\n",
            "240/240 - 0s - loss: 0.5191 - acc: 0.8542 - val_loss: 0.8877 - val_acc: 0.5745\n",
            "Epoch 164/300\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.84340\n",
            "240/240 - 0s - loss: 0.4940 - acc: 0.8667 - val_loss: 0.8968 - val_acc: 0.5532\n",
            "Epoch 165/300\n",
            "\n",
            "Epoch 00165: val_loss improved from 0.84340 to 0.81676, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.4816 - acc: 0.8583 - val_loss: 0.8168 - val_acc: 0.6809\n",
            "Epoch 166/300\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 0.81676\n",
            "240/240 - 0s - loss: 0.4865 - acc: 0.8917 - val_loss: 0.8501 - val_acc: 0.6596\n",
            "Epoch 167/300\n",
            "\n",
            "Epoch 00167: val_loss improved from 0.81676 to 0.80808, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.4939 - acc: 0.8667 - val_loss: 0.8081 - val_acc: 0.6383\n",
            "Epoch 168/300\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.80808\n",
            "240/240 - 0s - loss: 0.4812 - acc: 0.8875 - val_loss: 0.8160 - val_acc: 0.7234\n",
            "Epoch 169/300\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 0.80808\n",
            "240/240 - 0s - loss: 0.4749 - acc: 0.8917 - val_loss: 0.8373 - val_acc: 0.6170\n",
            "Epoch 170/300\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.80808\n",
            "240/240 - 0s - loss: 0.4847 - acc: 0.8667 - val_loss: 0.8320 - val_acc: 0.6170\n",
            "Epoch 171/300\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.80808\n",
            "240/240 - 0s - loss: 0.4663 - acc: 0.8917 - val_loss: 0.8339 - val_acc: 0.5957\n",
            "Epoch 172/300\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.80808\n",
            "240/240 - 0s - loss: 0.4844 - acc: 0.8667 - val_loss: 0.8788 - val_acc: 0.6809\n",
            "Epoch 173/300\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.80808\n",
            "240/240 - 0s - loss: 0.5056 - acc: 0.8500 - val_loss: 0.8138 - val_acc: 0.7021\n",
            "Epoch 174/300\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.80808\n",
            "240/240 - 0s - loss: 0.4853 - acc: 0.8792 - val_loss: 0.8378 - val_acc: 0.6809\n",
            "Epoch 175/300\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.80808\n",
            "240/240 - 0s - loss: 0.4895 - acc: 0.8667 - val_loss: 0.8282 - val_acc: 0.6383\n",
            "Epoch 176/300\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 0.80808\n",
            "240/240 - 0s - loss: 0.4742 - acc: 0.8542 - val_loss: 0.8207 - val_acc: 0.5745\n",
            "Epoch 177/300\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.80808\n",
            "240/240 - 0s - loss: 0.4482 - acc: 0.9083 - val_loss: 0.8312 - val_acc: 0.5957\n",
            "Epoch 178/300\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.80808\n",
            "240/240 - 0s - loss: 0.5069 - acc: 0.8750 - val_loss: 0.8807 - val_acc: 0.5957\n",
            "Epoch 179/300\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.80808\n",
            "240/240 - 0s - loss: 0.4979 - acc: 0.8917 - val_loss: 0.9190 - val_acc: 0.6170\n",
            "Epoch 180/300\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.80808\n",
            "240/240 - 0s - loss: 0.4742 - acc: 0.8625 - val_loss: 0.8417 - val_acc: 0.6170\n",
            "Epoch 181/300\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.80808\n",
            "240/240 - 0s - loss: 0.4719 - acc: 0.8667 - val_loss: 0.8242 - val_acc: 0.5745\n",
            "Epoch 182/300\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.80808\n",
            "240/240 - 0s - loss: 0.4978 - acc: 0.8667 - val_loss: 0.8403 - val_acc: 0.6809\n",
            "Epoch 183/300\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.80808\n",
            "240/240 - 0s - loss: 0.4786 - acc: 0.8583 - val_loss: 0.8443 - val_acc: 0.7021\n",
            "Epoch 184/300\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 0.80808\n",
            "240/240 - 0s - loss: 0.4356 - acc: 0.9292 - val_loss: 0.8356 - val_acc: 0.6809\n",
            "Epoch 185/300\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.80808\n",
            "240/240 - 0s - loss: 0.4567 - acc: 0.8958 - val_loss: 0.8508 - val_acc: 0.6170\n",
            "Epoch 186/300\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.80808\n",
            "240/240 - 0s - loss: 0.5205 - acc: 0.8458 - val_loss: 0.8327 - val_acc: 0.6596\n",
            "Epoch 187/300\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 0.80808\n",
            "240/240 - 0s - loss: 0.4302 - acc: 0.9250 - val_loss: 0.8484 - val_acc: 0.6809\n",
            "Epoch 188/300\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.80808\n",
            "240/240 - 0s - loss: 0.4498 - acc: 0.9000 - val_loss: 0.8176 - val_acc: 0.6809\n",
            "Epoch 189/300\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.80808\n",
            "240/240 - 0s - loss: 0.4677 - acc: 0.8875 - val_loss: 0.8430 - val_acc: 0.6383\n",
            "Epoch 190/300\n",
            "\n",
            "Epoch 00190: val_loss improved from 0.80808 to 0.80691, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.4633 - acc: 0.8625 - val_loss: 0.8069 - val_acc: 0.7021\n",
            "Epoch 191/300\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.80691\n",
            "240/240 - 0s - loss: 0.4370 - acc: 0.8917 - val_loss: 0.8111 - val_acc: 0.6809\n",
            "Epoch 192/300\n",
            "\n",
            "Epoch 00192: val_loss improved from 0.80691 to 0.78006, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.4701 - acc: 0.8792 - val_loss: 0.7801 - val_acc: 0.6809\n",
            "Epoch 193/300\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.78006\n",
            "240/240 - 0s - loss: 0.4526 - acc: 0.8667 - val_loss: 0.8356 - val_acc: 0.6809\n",
            "Epoch 194/300\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.78006\n",
            "240/240 - 0s - loss: 0.4391 - acc: 0.9208 - val_loss: 0.7970 - val_acc: 0.6383\n",
            "Epoch 195/300\n",
            "\n",
            "Epoch 00195: val_loss improved from 0.78006 to 0.77724, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.4130 - acc: 0.9042 - val_loss: 0.7772 - val_acc: 0.6809\n",
            "Epoch 196/300\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.77724\n",
            "240/240 - 0s - loss: 0.4483 - acc: 0.9042 - val_loss: 0.7792 - val_acc: 0.7021\n",
            "Epoch 197/300\n",
            "\n",
            "Epoch 00197: val_loss improved from 0.77724 to 0.77697, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.4607 - acc: 0.8917 - val_loss: 0.7770 - val_acc: 0.7021\n",
            "Epoch 198/300\n",
            "\n",
            "Epoch 00198: val_loss improved from 0.77697 to 0.76322, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.4689 - acc: 0.8833 - val_loss: 0.7632 - val_acc: 0.6809\n",
            "Epoch 199/300\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.76322\n",
            "240/240 - 0s - loss: 0.4388 - acc: 0.8708 - val_loss: 0.7652 - val_acc: 0.6383\n",
            "Epoch 200/300\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.76322\n",
            "240/240 - 0s - loss: 0.4454 - acc: 0.8708 - val_loss: 0.7987 - val_acc: 0.6596\n",
            "Epoch 201/300\n",
            "\n",
            "Epoch 00201: val_loss did not improve from 0.76322\n",
            "240/240 - 0s - loss: 0.4595 - acc: 0.8792 - val_loss: 0.7753 - val_acc: 0.7234\n",
            "Epoch 202/300\n",
            "\n",
            "Epoch 00202: val_loss did not improve from 0.76322\n",
            "240/240 - 0s - loss: 0.4221 - acc: 0.9000 - val_loss: 0.8294 - val_acc: 0.6596\n",
            "Epoch 203/300\n",
            "\n",
            "Epoch 00203: val_loss improved from 0.76322 to 0.76131, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.4323 - acc: 0.9292 - val_loss: 0.7613 - val_acc: 0.7447\n",
            "Epoch 204/300\n",
            "\n",
            "Epoch 00204: val_loss did not improve from 0.76131\n",
            "240/240 - 0s - loss: 0.4200 - acc: 0.9208 - val_loss: 0.8120 - val_acc: 0.6809\n",
            "Epoch 205/300\n",
            "\n",
            "Epoch 00205: val_loss improved from 0.76131 to 0.72832, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.4208 - acc: 0.9000 - val_loss: 0.7283 - val_acc: 0.7021\n",
            "Epoch 206/300\n",
            "\n",
            "Epoch 00206: val_loss did not improve from 0.72832\n",
            "240/240 - 0s - loss: 0.4637 - acc: 0.9000 - val_loss: 0.9271 - val_acc: 0.6170\n",
            "Epoch 207/300\n",
            "\n",
            "Epoch 00207: val_loss did not improve from 0.72832\n",
            "240/240 - 0s - loss: 0.4674 - acc: 0.8542 - val_loss: 0.7716 - val_acc: 0.6809\n",
            "Epoch 208/300\n",
            "\n",
            "Epoch 00208: val_loss did not improve from 0.72832\n",
            "240/240 - 0s - loss: 0.4061 - acc: 0.9083 - val_loss: 0.9232 - val_acc: 0.5957\n",
            "Epoch 209/300\n",
            "\n",
            "Epoch 00209: val_loss did not improve from 0.72832\n",
            "240/240 - 0s - loss: 0.4119 - acc: 0.9000 - val_loss: 0.8342 - val_acc: 0.6596\n",
            "Epoch 210/300\n",
            "\n",
            "Epoch 00210: val_loss did not improve from 0.72832\n",
            "240/240 - 0s - loss: 0.4170 - acc: 0.8917 - val_loss: 0.8630 - val_acc: 0.6383\n",
            "Epoch 211/300\n",
            "\n",
            "Epoch 00211: val_loss did not improve from 0.72832\n",
            "240/240 - 0s - loss: 0.4394 - acc: 0.8667 - val_loss: 0.7767 - val_acc: 0.6383\n",
            "Epoch 212/300\n",
            "\n",
            "Epoch 00212: val_loss did not improve from 0.72832\n",
            "240/240 - 0s - loss: 0.4455 - acc: 0.9167 - val_loss: 0.7493 - val_acc: 0.6809\n",
            "Epoch 213/300\n",
            "\n",
            "Epoch 00213: val_loss did not improve from 0.72832\n",
            "240/240 - 0s - loss: 0.4184 - acc: 0.9083 - val_loss: 0.7910 - val_acc: 0.6809\n",
            "Epoch 214/300\n",
            "\n",
            "Epoch 00214: val_loss did not improve from 0.72832\n",
            "240/240 - 0s - loss: 0.4509 - acc: 0.8792 - val_loss: 0.7621 - val_acc: 0.6809\n",
            "Epoch 215/300\n",
            "\n",
            "Epoch 00215: val_loss did not improve from 0.72832\n",
            "240/240 - 0s - loss: 0.3851 - acc: 0.9208 - val_loss: 0.7967 - val_acc: 0.6809\n",
            "Epoch 216/300\n",
            "\n",
            "Epoch 00216: val_loss improved from 0.72832 to 0.71805, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.3811 - acc: 0.9250 - val_loss: 0.7181 - val_acc: 0.7234\n",
            "Epoch 217/300\n",
            "\n",
            "Epoch 00217: val_loss did not improve from 0.71805\n",
            "240/240 - 0s - loss: 0.4472 - acc: 0.8667 - val_loss: 0.8035 - val_acc: 0.6596\n",
            "Epoch 218/300\n",
            "\n",
            "Epoch 00218: val_loss did not improve from 0.71805\n",
            "240/240 - 0s - loss: 0.4099 - acc: 0.9125 - val_loss: 0.7913 - val_acc: 0.6596\n",
            "Epoch 219/300\n",
            "\n",
            "Epoch 00219: val_loss did not improve from 0.71805\n",
            "240/240 - 0s - loss: 0.3960 - acc: 0.9333 - val_loss: 0.8716 - val_acc: 0.6383\n",
            "Epoch 220/300\n",
            "\n",
            "Epoch 00220: val_loss did not improve from 0.71805\n",
            "240/240 - 0s - loss: 0.4137 - acc: 0.8958 - val_loss: 0.8019 - val_acc: 0.6809\n",
            "Epoch 221/300\n",
            "\n",
            "Epoch 00221: val_loss did not improve from 0.71805\n",
            "240/240 - 0s - loss: 0.4448 - acc: 0.9042 - val_loss: 0.7756 - val_acc: 0.6809\n",
            "Epoch 222/300\n",
            "\n",
            "Epoch 00222: val_loss did not improve from 0.71805\n",
            "240/240 - 0s - loss: 0.3946 - acc: 0.9125 - val_loss: 0.7328 - val_acc: 0.7660\n",
            "Epoch 223/300\n",
            "\n",
            "Epoch 00223: val_loss did not improve from 0.71805\n",
            "240/240 - 0s - loss: 0.3844 - acc: 0.9458 - val_loss: 0.9249 - val_acc: 0.6596\n",
            "Epoch 224/300\n",
            "\n",
            "Epoch 00224: val_loss did not improve from 0.71805\n",
            "240/240 - 0s - loss: 0.4068 - acc: 0.8875 - val_loss: 0.7784 - val_acc: 0.6596\n",
            "Epoch 225/300\n",
            "\n",
            "Epoch 00225: val_loss did not improve from 0.71805\n",
            "240/240 - 0s - loss: 0.4051 - acc: 0.9125 - val_loss: 0.7675 - val_acc: 0.6596\n",
            "Epoch 226/300\n",
            "\n",
            "Epoch 00226: val_loss did not improve from 0.71805\n",
            "240/240 - 0s - loss: 0.3942 - acc: 0.9083 - val_loss: 0.7957 - val_acc: 0.7021\n",
            "Epoch 227/300\n",
            "\n",
            "Epoch 00227: val_loss did not improve from 0.71805\n",
            "240/240 - 0s - loss: 0.4190 - acc: 0.8917 - val_loss: 0.8034 - val_acc: 0.6383\n",
            "Epoch 228/300\n",
            "\n",
            "Epoch 00228: val_loss did not improve from 0.71805\n",
            "240/240 - 0s - loss: 0.3769 - acc: 0.9250 - val_loss: 0.7400 - val_acc: 0.7447\n",
            "Epoch 229/300\n",
            "\n",
            "Epoch 00229: val_loss did not improve from 0.71805\n",
            "240/240 - 0s - loss: 0.4248 - acc: 0.8792 - val_loss: 0.9669 - val_acc: 0.5532\n",
            "Epoch 230/300\n",
            "\n",
            "Epoch 00230: val_loss did not improve from 0.71805\n",
            "240/240 - 0s - loss: 0.3940 - acc: 0.9167 - val_loss: 0.8195 - val_acc: 0.6596\n",
            "Epoch 231/300\n",
            "\n",
            "Epoch 00231: val_loss did not improve from 0.71805\n",
            "240/240 - 0s - loss: 0.4025 - acc: 0.8875 - val_loss: 0.7407 - val_acc: 0.7234\n",
            "Epoch 232/300\n",
            "\n",
            "Epoch 00232: val_loss did not improve from 0.71805\n",
            "240/240 - 0s - loss: 0.3986 - acc: 0.8958 - val_loss: 0.8037 - val_acc: 0.6383\n",
            "Epoch 233/300\n",
            "\n",
            "Epoch 00233: val_loss did not improve from 0.71805\n",
            "240/240 - 0s - loss: 0.3844 - acc: 0.9292 - val_loss: 0.7971 - val_acc: 0.6809\n",
            "Epoch 234/300\n",
            "\n",
            "Epoch 00234: val_loss did not improve from 0.71805\n",
            "240/240 - 0s - loss: 0.3964 - acc: 0.9042 - val_loss: 0.8520 - val_acc: 0.5957\n",
            "Epoch 235/300\n",
            "\n",
            "Epoch 00235: val_loss did not improve from 0.71805\n",
            "240/240 - 0s - loss: 0.3961 - acc: 0.8833 - val_loss: 0.8465 - val_acc: 0.6809\n",
            "Epoch 236/300\n",
            "\n",
            "Epoch 00236: val_loss did not improve from 0.71805\n",
            "240/240 - 0s - loss: 0.3600 - acc: 0.9250 - val_loss: 0.7898 - val_acc: 0.6596\n",
            "Epoch 237/300\n",
            "\n",
            "Epoch 00237: val_loss did not improve from 0.71805\n",
            "240/240 - 0s - loss: 0.3586 - acc: 0.9250 - val_loss: 0.7692 - val_acc: 0.6809\n",
            "Epoch 238/300\n",
            "\n",
            "Epoch 00238: val_loss did not improve from 0.71805\n",
            "240/240 - 0s - loss: 0.3550 - acc: 0.9250 - val_loss: 0.8154 - val_acc: 0.6596\n",
            "Epoch 239/300\n",
            "\n",
            "Epoch 00239: val_loss did not improve from 0.71805\n",
            "240/240 - 0s - loss: 0.4190 - acc: 0.9000 - val_loss: 0.7767 - val_acc: 0.6596\n",
            "Epoch 240/300\n",
            "\n",
            "Epoch 00240: val_loss did not improve from 0.71805\n",
            "240/240 - 0s - loss: 0.3845 - acc: 0.9125 - val_loss: 0.8561 - val_acc: 0.6170\n",
            "Epoch 241/300\n",
            "\n",
            "Epoch 00241: val_loss did not improve from 0.71805\n",
            "240/240 - 0s - loss: 0.3562 - acc: 0.9250 - val_loss: 0.7683 - val_acc: 0.7234\n",
            "Epoch 242/300\n",
            "\n",
            "Epoch 00242: val_loss did not improve from 0.71805\n",
            "240/240 - 0s - loss: 0.3895 - acc: 0.8917 - val_loss: 0.7843 - val_acc: 0.6596\n",
            "Epoch 243/300\n",
            "\n",
            "Epoch 00243: val_loss did not improve from 0.71805\n",
            "240/240 - 0s - loss: 0.3856 - acc: 0.8958 - val_loss: 0.7654 - val_acc: 0.6596\n",
            "Epoch 244/300\n",
            "\n",
            "Epoch 00244: val_loss did not improve from 0.71805\n",
            "240/240 - 0s - loss: 0.3594 - acc: 0.9458 - val_loss: 0.7694 - val_acc: 0.7021\n",
            "Epoch 245/300\n",
            "\n",
            "Epoch 00245: val_loss did not improve from 0.71805\n",
            "240/240 - 0s - loss: 0.4268 - acc: 0.8875 - val_loss: 0.7560 - val_acc: 0.7660\n",
            "Epoch 246/300\n",
            "\n",
            "Epoch 00246: val_loss did not improve from 0.71805\n",
            "240/240 - 0s - loss: 0.4485 - acc: 0.8833 - val_loss: 0.9508 - val_acc: 0.5745\n",
            "Epoch 247/300\n",
            "\n",
            "Epoch 00247: val_loss did not improve from 0.71805\n",
            "240/240 - 0s - loss: 0.3543 - acc: 0.9417 - val_loss: 0.7707 - val_acc: 0.6383\n",
            "Epoch 248/300\n",
            "\n",
            "Epoch 00248: val_loss did not improve from 0.71805\n",
            "240/240 - 0s - loss: 0.3650 - acc: 0.9375 - val_loss: 0.9387 - val_acc: 0.6170\n",
            "Epoch 249/300\n",
            "\n",
            "Epoch 00249: val_loss did not improve from 0.71805\n",
            "240/240 - 0s - loss: 0.3841 - acc: 0.9000 - val_loss: 0.7832 - val_acc: 0.6809\n",
            "Epoch 250/300\n",
            "\n",
            "Epoch 00250: val_loss did not improve from 0.71805\n",
            "240/240 - 0s - loss: 0.3675 - acc: 0.9208 - val_loss: 0.8801 - val_acc: 0.5957\n",
            "Epoch 251/300\n",
            "\n",
            "Epoch 00251: val_loss did not improve from 0.71805\n",
            "240/240 - 0s - loss: 0.3759 - acc: 0.9208 - val_loss: 0.8101 - val_acc: 0.6596\n",
            "Epoch 252/300\n",
            "\n",
            "Epoch 00252: val_loss did not improve from 0.71805\n",
            "240/240 - 0s - loss: 0.3418 - acc: 0.9250 - val_loss: 0.8011 - val_acc: 0.6383\n",
            "Epoch 253/300\n",
            "\n",
            "Epoch 00253: val_loss did not improve from 0.71805\n",
            "240/240 - 0s - loss: 0.3620 - acc: 0.9083 - val_loss: 0.8111 - val_acc: 0.6383\n",
            "Epoch 254/300\n",
            "\n",
            "Epoch 00254: val_loss did not improve from 0.71805\n",
            "240/240 - 0s - loss: 0.3695 - acc: 0.9000 - val_loss: 0.8381 - val_acc: 0.6383\n",
            "Epoch 255/300\n",
            "\n",
            "Epoch 00255: val_loss did not improve from 0.71805\n",
            "240/240 - 0s - loss: 0.3863 - acc: 0.9042 - val_loss: 0.7922 - val_acc: 0.6809\n",
            "Epoch 256/300\n",
            "\n",
            "Epoch 00256: val_loss did not improve from 0.71805\n",
            "240/240 - 0s - loss: 0.3386 - acc: 0.9333 - val_loss: 0.8857 - val_acc: 0.6383\n",
            "Epoch 257/300\n",
            "\n",
            "Epoch 00257: val_loss did not improve from 0.71805\n",
            "240/240 - 0s - loss: 0.3900 - acc: 0.8958 - val_loss: 0.8802 - val_acc: 0.6170\n",
            "Epoch 258/300\n",
            "\n",
            "Epoch 00258: val_loss did not improve from 0.71805\n",
            "240/240 - 0s - loss: 0.3830 - acc: 0.9083 - val_loss: 0.8353 - val_acc: 0.6170\n",
            "Epoch 259/300\n",
            "\n",
            "Epoch 00259: val_loss did not improve from 0.71805\n",
            "240/240 - 0s - loss: 0.3711 - acc: 0.9125 - val_loss: 0.8596 - val_acc: 0.6170\n",
            "Epoch 260/300\n",
            "\n",
            "Epoch 00260: val_loss did not improve from 0.71805\n",
            "240/240 - 0s - loss: 0.3306 - acc: 0.9417 - val_loss: 0.7348 - val_acc: 0.7021\n",
            "Epoch 261/300\n",
            "\n",
            "Epoch 00261: val_loss did not improve from 0.71805\n",
            "240/240 - 0s - loss: 0.3751 - acc: 0.9083 - val_loss: 0.8229 - val_acc: 0.6596\n",
            "Epoch 262/300\n",
            "\n",
            "Epoch 00262: val_loss did not improve from 0.71805\n",
            "240/240 - 0s - loss: 0.3487 - acc: 0.9208 - val_loss: 0.8005 - val_acc: 0.6596\n",
            "Epoch 263/300\n",
            "\n",
            "Epoch 00263: val_loss did not improve from 0.71805\n",
            "240/240 - 0s - loss: 0.3338 - acc: 0.9292 - val_loss: 0.7516 - val_acc: 0.7234\n",
            "Epoch 264/300\n",
            "\n",
            "Epoch 00264: val_loss did not improve from 0.71805\n",
            "240/240 - 0s - loss: 0.3441 - acc: 0.9208 - val_loss: 0.7565 - val_acc: 0.7021\n",
            "Epoch 265/300\n",
            "\n",
            "Epoch 00265: val_loss did not improve from 0.71805\n",
            "240/240 - 0s - loss: 0.3194 - acc: 0.9375 - val_loss: 0.7198 - val_acc: 0.7234\n",
            "Epoch 266/300\n",
            "\n",
            "Epoch 00266: val_loss did not improve from 0.71805\n",
            "240/240 - 0s - loss: 0.3480 - acc: 0.9208 - val_loss: 0.7835 - val_acc: 0.6809\n",
            "Epoch 267/300\n",
            "\n",
            "Epoch 00267: val_loss did not improve from 0.71805\n",
            "240/240 - 0s - loss: 0.3439 - acc: 0.9458 - val_loss: 0.8764 - val_acc: 0.6170\n",
            "Epoch 268/300\n",
            "\n",
            "Epoch 00268: val_loss did not improve from 0.71805\n",
            "240/240 - 0s - loss: 0.3596 - acc: 0.9125 - val_loss: 0.8002 - val_acc: 0.6383\n",
            "Epoch 269/300\n",
            "\n",
            "Epoch 00269: val_loss did not improve from 0.71805\n",
            "240/240 - 0s - loss: 0.3835 - acc: 0.8875 - val_loss: 0.7920 - val_acc: 0.6809\n",
            "Epoch 270/300\n",
            "\n",
            "Epoch 00270: val_loss did not improve from 0.71805\n",
            "240/240 - 0s - loss: 0.4062 - acc: 0.8708 - val_loss: 0.8238 - val_acc: 0.7021\n",
            "Epoch 271/300\n",
            "\n",
            "Epoch 00271: val_loss did not improve from 0.71805\n",
            "240/240 - 0s - loss: 0.3685 - acc: 0.9250 - val_loss: 0.9589 - val_acc: 0.5957\n",
            "Epoch 272/300\n",
            "\n",
            "Epoch 00272: val_loss did not improve from 0.71805\n",
            "240/240 - 0s - loss: 0.3509 - acc: 0.9417 - val_loss: 0.7734 - val_acc: 0.6809\n",
            "Epoch 273/300\n",
            "\n",
            "Epoch 00273: val_loss did not improve from 0.71805\n",
            "240/240 - 0s - loss: 0.3225 - acc: 0.9417 - val_loss: 0.8276 - val_acc: 0.6809\n",
            "Epoch 274/300\n",
            "\n",
            "Epoch 00274: val_loss did not improve from 0.71805\n",
            "240/240 - 0s - loss: 0.3340 - acc: 0.9375 - val_loss: 0.8709 - val_acc: 0.6383\n",
            "Epoch 275/300\n",
            "\n",
            "Epoch 00275: val_loss did not improve from 0.71805\n",
            "240/240 - 0s - loss: 0.3378 - acc: 0.9250 - val_loss: 0.8753 - val_acc: 0.6383\n",
            "Epoch 276/300\n",
            "\n",
            "Epoch 00276: val_loss did not improve from 0.71805\n",
            "240/240 - 0s - loss: 0.3121 - acc: 0.9333 - val_loss: 0.8056 - val_acc: 0.6383\n",
            "Epoch 277/300\n",
            "\n",
            "Epoch 00277: val_loss did not improve from 0.71805\n",
            "240/240 - 0s - loss: 0.3519 - acc: 0.9208 - val_loss: 0.8217 - val_acc: 0.6170\n",
            "Epoch 278/300\n",
            "\n",
            "Epoch 00278: val_loss did not improve from 0.71805\n",
            "240/240 - 0s - loss: 0.3501 - acc: 0.9333 - val_loss: 0.8538 - val_acc: 0.6170\n",
            "Epoch 279/300\n",
            "\n",
            "Epoch 00279: val_loss did not improve from 0.71805\n",
            "240/240 - 0s - loss: 0.3196 - acc: 0.9417 - val_loss: 0.8306 - val_acc: 0.7234\n",
            "Epoch 280/300\n",
            "\n",
            "Epoch 00280: val_loss did not improve from 0.71805\n",
            "240/240 - 0s - loss: 0.3654 - acc: 0.9167 - val_loss: 0.8397 - val_acc: 0.6596\n",
            "Epoch 281/300\n",
            "\n",
            "Epoch 00281: val_loss did not improve from 0.71805\n",
            "240/240 - 0s - loss: 0.3381 - acc: 0.9250 - val_loss: 0.7473 - val_acc: 0.7234\n",
            "Epoch 282/300\n",
            "\n",
            "Epoch 00282: val_loss did not improve from 0.71805\n",
            "240/240 - 0s - loss: 0.2983 - acc: 0.9375 - val_loss: 0.7706 - val_acc: 0.7021\n",
            "Epoch 283/300\n",
            "\n",
            "Epoch 00283: val_loss did not improve from 0.71805\n",
            "240/240 - 0s - loss: 0.3113 - acc: 0.9542 - val_loss: 0.8253 - val_acc: 0.6596\n",
            "Epoch 284/300\n",
            "\n",
            "Epoch 00284: val_loss did not improve from 0.71805\n",
            "240/240 - 0s - loss: 0.3232 - acc: 0.9292 - val_loss: 0.8175 - val_acc: 0.5957\n",
            "Epoch 285/300\n",
            "\n",
            "Epoch 00285: val_loss did not improve from 0.71805\n",
            "240/240 - 0s - loss: 0.3382 - acc: 0.9292 - val_loss: 0.8093 - val_acc: 0.6596\n",
            "Epoch 286/300\n",
            "\n",
            "Epoch 00286: val_loss did not improve from 0.71805\n",
            "240/240 - 0s - loss: 0.3273 - acc: 0.9458 - val_loss: 0.8385 - val_acc: 0.6383\n",
            "Epoch 287/300\n",
            "\n",
            "Epoch 00287: val_loss did not improve from 0.71805\n",
            "240/240 - 0s - loss: 0.3443 - acc: 0.9167 - val_loss: 0.7575 - val_acc: 0.7021\n",
            "Epoch 288/300\n",
            "\n",
            "Epoch 00288: val_loss did not improve from 0.71805\n",
            "240/240 - 0s - loss: 0.3309 - acc: 0.9167 - val_loss: 0.7940 - val_acc: 0.6383\n",
            "Epoch 289/300\n",
            "\n",
            "Epoch 00289: val_loss did not improve from 0.71805\n",
            "240/240 - 0s - loss: 0.3467 - acc: 0.9375 - val_loss: 0.8771 - val_acc: 0.6383\n",
            "Epoch 290/300\n",
            "\n",
            "Epoch 00290: val_loss did not improve from 0.71805\n",
            "240/240 - 0s - loss: 0.3320 - acc: 0.9208 - val_loss: 0.7814 - val_acc: 0.6809\n",
            "Epoch 291/300\n",
            "\n",
            "Epoch 00291: val_loss did not improve from 0.71805\n",
            "240/240 - 0s - loss: 0.3415 - acc: 0.9292 - val_loss: 0.8344 - val_acc: 0.5957\n",
            "Epoch 292/300\n",
            "\n",
            "Epoch 00292: val_loss did not improve from 0.71805\n",
            "240/240 - 0s - loss: 0.2925 - acc: 0.9500 - val_loss: 0.8181 - val_acc: 0.6383\n",
            "Epoch 293/300\n",
            "\n",
            "Epoch 00293: val_loss did not improve from 0.71805\n",
            "240/240 - 0s - loss: 0.3161 - acc: 0.9208 - val_loss: 0.8313 - val_acc: 0.6596\n",
            "Epoch 294/300\n",
            "\n",
            "Epoch 00294: val_loss did not improve from 0.71805\n",
            "240/240 - 0s - loss: 0.3107 - acc: 0.9417 - val_loss: 0.7769 - val_acc: 0.6809\n",
            "Epoch 295/300\n",
            "\n",
            "Epoch 00295: val_loss did not improve from 0.71805\n",
            "240/240 - 0s - loss: 0.3120 - acc: 0.9292 - val_loss: 0.8764 - val_acc: 0.6383\n",
            "Epoch 296/300\n",
            "\n",
            "Epoch 00296: val_loss did not improve from 0.71805\n",
            "240/240 - 0s - loss: 0.3466 - acc: 0.9208 - val_loss: 0.8556 - val_acc: 0.6383\n",
            "Epoch 297/300\n",
            "\n",
            "Epoch 00297: val_loss did not improve from 0.71805\n",
            "240/240 - 0s - loss: 0.3301 - acc: 0.9417 - val_loss: 0.7987 - val_acc: 0.6809\n",
            "Epoch 298/300\n",
            "\n",
            "Epoch 00298: val_loss did not improve from 0.71805\n",
            "240/240 - 0s - loss: 0.3165 - acc: 0.9292 - val_loss: 0.7256 - val_acc: 0.6383\n",
            "Epoch 299/300\n",
            "\n",
            "Epoch 00299: val_loss did not improve from 0.71805\n",
            "240/240 - 0s - loss: 0.3108 - acc: 0.9292 - val_loss: 0.8858 - val_acc: 0.5957\n",
            "Epoch 300/300\n",
            "\n",
            "Epoch 00300: val_loss did not improve from 0.71805\n",
            "240/240 - 0s - loss: 0.3067 - acc: 0.9375 - val_loss: 0.7355 - val_acc: 0.7021\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n2AAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "# Evaluate on test data\n",
            "288/288 [==============================] - 1s 2ms/sample - loss: 2.1327 - acc: 0.4132\n",
            "test loss, test acc: [2.13269824896391, 0.41319445]\n",
            "EEG_Deep/Data2A/Data_A06T.mat\n",
            "(288, 22, 1875)\n",
            "(288, 1)\n",
            "(240, 22, 750)\n",
            "(240, 1)\n",
            "(47, 22, 750)\n",
            "(47, 1)\n",
            "X_train shape: (240, 1, 22, 750)\n",
            "240 train samples\n",
            "47 val samples\n",
            "EEG_Deep/Data2A/Data_A06E.mat\n",
            "(288, 22, 1875)\n",
            "(288, 1)\n",
            "(288, 22, 750)\n",
            "(288, 1)\n",
            "X_train shape: (288, 1, 22, 750)\n",
            "288 train samples\n",
            "Train on 240 samples, validate on 47 samples\n",
            "Epoch 1/300\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 1.38405, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 1s - loss: 1.3860 - acc: 0.2542 - val_loss: 1.3841 - val_acc: 0.3191\n",
            "Epoch 2/300\n",
            "\n",
            "Epoch 00002: val_loss improved from 1.38405 to 1.38315, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.3584 - acc: 0.3458 - val_loss: 1.3832 - val_acc: 0.3830\n",
            "Epoch 3/300\n",
            "\n",
            "Epoch 00003: val_loss improved from 1.38315 to 1.38232, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.3282 - acc: 0.4750 - val_loss: 1.3823 - val_acc: 0.4255\n",
            "Epoch 4/300\n",
            "\n",
            "Epoch 00004: val_loss improved from 1.38232 to 1.38003, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.3004 - acc: 0.4917 - val_loss: 1.3800 - val_acc: 0.4468\n",
            "Epoch 5/300\n",
            "\n",
            "Epoch 00005: val_loss improved from 1.38003 to 1.37717, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.2841 - acc: 0.5208 - val_loss: 1.3772 - val_acc: 0.4468\n",
            "Epoch 6/300\n",
            "\n",
            "Epoch 00006: val_loss improved from 1.37717 to 1.37383, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.2613 - acc: 0.5375 - val_loss: 1.3738 - val_acc: 0.4894\n",
            "Epoch 7/300\n",
            "\n",
            "Epoch 00007: val_loss improved from 1.37383 to 1.37087, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.2394 - acc: 0.5417 - val_loss: 1.3709 - val_acc: 0.4681\n",
            "Epoch 8/300\n",
            "\n",
            "Epoch 00008: val_loss improved from 1.37087 to 1.36814, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.2056 - acc: 0.5875 - val_loss: 1.3681 - val_acc: 0.4043\n",
            "Epoch 9/300\n",
            "\n",
            "Epoch 00009: val_loss improved from 1.36814 to 1.36125, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.1791 - acc: 0.5667 - val_loss: 1.3612 - val_acc: 0.4255\n",
            "Epoch 10/300\n",
            "\n",
            "Epoch 00010: val_loss improved from 1.36125 to 1.35418, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.1670 - acc: 0.6000 - val_loss: 1.3542 - val_acc: 0.4894\n",
            "Epoch 11/300\n",
            "\n",
            "Epoch 00011: val_loss improved from 1.35418 to 1.34693, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.1424 - acc: 0.6083 - val_loss: 1.3469 - val_acc: 0.4255\n",
            "Epoch 12/300\n",
            "\n",
            "Epoch 00012: val_loss improved from 1.34693 to 1.34334, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.1394 - acc: 0.5958 - val_loss: 1.3433 - val_acc: 0.3617\n",
            "Epoch 13/300\n",
            "\n",
            "Epoch 00013: val_loss improved from 1.34334 to 1.33349, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.1170 - acc: 0.6125 - val_loss: 1.3335 - val_acc: 0.4255\n",
            "Epoch 14/300\n",
            "\n",
            "Epoch 00014: val_loss improved from 1.33349 to 1.32685, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.0985 - acc: 0.6250 - val_loss: 1.3268 - val_acc: 0.4043\n",
            "Epoch 15/300\n",
            "\n",
            "Epoch 00015: val_loss improved from 1.32685 to 1.31943, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.0857 - acc: 0.6417 - val_loss: 1.3194 - val_acc: 0.4681\n",
            "Epoch 16/300\n",
            "\n",
            "Epoch 00016: val_loss improved from 1.31943 to 1.30843, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.0955 - acc: 0.6208 - val_loss: 1.3084 - val_acc: 0.4894\n",
            "Epoch 17/300\n",
            "\n",
            "Epoch 00017: val_loss improved from 1.30843 to 1.30172, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.0987 - acc: 0.6458 - val_loss: 1.3017 - val_acc: 0.5106\n",
            "Epoch 18/300\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 1.30172\n",
            "240/240 - 0s - loss: 1.0767 - acc: 0.6208 - val_loss: 1.3052 - val_acc: 0.4255\n",
            "Epoch 19/300\n",
            "\n",
            "Epoch 00019: val_loss improved from 1.30172 to 1.29237, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.0643 - acc: 0.6625 - val_loss: 1.2924 - val_acc: 0.4468\n",
            "Epoch 20/300\n",
            "\n",
            "Epoch 00020: val_loss improved from 1.29237 to 1.29046, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.0367 - acc: 0.6625 - val_loss: 1.2905 - val_acc: 0.4894\n",
            "Epoch 21/300\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 1.29046\n",
            "240/240 - 0s - loss: 1.0463 - acc: 0.6708 - val_loss: 1.2915 - val_acc: 0.4681\n",
            "Epoch 22/300\n",
            "\n",
            "Epoch 00022: val_loss improved from 1.29046 to 1.26777, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.0456 - acc: 0.6542 - val_loss: 1.2678 - val_acc: 0.4681\n",
            "Epoch 23/300\n",
            "\n",
            "Epoch 00023: val_loss improved from 1.26777 to 1.25165, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.0360 - acc: 0.6458 - val_loss: 1.2516 - val_acc: 0.5319\n",
            "Epoch 24/300\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 1.25165\n",
            "240/240 - 0s - loss: 0.9950 - acc: 0.7125 - val_loss: 1.2536 - val_acc: 0.4894\n",
            "Epoch 25/300\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 1.25165\n",
            "240/240 - 0s - loss: 1.0183 - acc: 0.6458 - val_loss: 1.2589 - val_acc: 0.4894\n",
            "Epoch 26/300\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 1.25165\n",
            "240/240 - 0s - loss: 1.0028 - acc: 0.7000 - val_loss: 1.2681 - val_acc: 0.5106\n",
            "Epoch 27/300\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 1.25165\n",
            "240/240 - 0s - loss: 0.9912 - acc: 0.6833 - val_loss: 1.2700 - val_acc: 0.5106\n",
            "Epoch 28/300\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 1.25165\n",
            "240/240 - 0s - loss: 0.9869 - acc: 0.6875 - val_loss: 1.2713 - val_acc: 0.4894\n",
            "Epoch 29/300\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 1.25165\n",
            "240/240 - 0s - loss: 0.9733 - acc: 0.6792 - val_loss: 1.2774 - val_acc: 0.4894\n",
            "Epoch 30/300\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 1.25165\n",
            "240/240 - 0s - loss: 0.9768 - acc: 0.6833 - val_loss: 1.2775 - val_acc: 0.4681\n",
            "Epoch 31/300\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 1.25165\n",
            "240/240 - 0s - loss: 0.9754 - acc: 0.7000 - val_loss: 1.2774 - val_acc: 0.4894\n",
            "Epoch 32/300\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 1.25165\n",
            "240/240 - 0s - loss: 0.9574 - acc: 0.7042 - val_loss: 1.3065 - val_acc: 0.4468\n",
            "Epoch 33/300\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 1.25165\n",
            "240/240 - 0s - loss: 0.9745 - acc: 0.6667 - val_loss: 1.2644 - val_acc: 0.5106\n",
            "Epoch 34/300\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 1.25165\n",
            "240/240 - 0s - loss: 0.9507 - acc: 0.7458 - val_loss: 1.2631 - val_acc: 0.5319\n",
            "Epoch 35/300\n",
            "\n",
            "Epoch 00035: val_loss improved from 1.25165 to 1.24721, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.9553 - acc: 0.7208 - val_loss: 1.2472 - val_acc: 0.5319\n",
            "Epoch 36/300\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 1.24721\n",
            "240/240 - 0s - loss: 0.9269 - acc: 0.6875 - val_loss: 1.2633 - val_acc: 0.5106\n",
            "Epoch 37/300\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 1.24721\n",
            "240/240 - 0s - loss: 0.9322 - acc: 0.6750 - val_loss: 1.2696 - val_acc: 0.5106\n",
            "Epoch 38/300\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 1.24721\n",
            "240/240 - 0s - loss: 0.9327 - acc: 0.7042 - val_loss: 1.2592 - val_acc: 0.4894\n",
            "Epoch 39/300\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 1.24721\n",
            "240/240 - 0s - loss: 0.9084 - acc: 0.7417 - val_loss: 1.2601 - val_acc: 0.5319\n",
            "Epoch 40/300\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 1.24721\n",
            "240/240 - 0s - loss: 0.9501 - acc: 0.6917 - val_loss: 1.3031 - val_acc: 0.4894\n",
            "Epoch 41/300\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 1.24721\n",
            "240/240 - 0s - loss: 0.9302 - acc: 0.7083 - val_loss: 1.2634 - val_acc: 0.4894\n",
            "Epoch 42/300\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 1.24721\n",
            "240/240 - 0s - loss: 0.9221 - acc: 0.7042 - val_loss: 1.2649 - val_acc: 0.5319\n",
            "Epoch 43/300\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 1.24721\n",
            "240/240 - 0s - loss: 0.8954 - acc: 0.7375 - val_loss: 1.2674 - val_acc: 0.5106\n",
            "Epoch 44/300\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 1.24721\n",
            "240/240 - 0s - loss: 0.8850 - acc: 0.7708 - val_loss: 1.2576 - val_acc: 0.5319\n",
            "Epoch 45/300\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 1.24721\n",
            "240/240 - 0s - loss: 0.8806 - acc: 0.7500 - val_loss: 1.2481 - val_acc: 0.4894\n",
            "Epoch 46/300\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 1.24721\n",
            "240/240 - 0s - loss: 0.8690 - acc: 0.7500 - val_loss: 1.2534 - val_acc: 0.4894\n",
            "Epoch 47/300\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 1.24721\n",
            "240/240 - 0s - loss: 0.8779 - acc: 0.7208 - val_loss: 1.2553 - val_acc: 0.5106\n",
            "Epoch 48/300\n",
            "\n",
            "Epoch 00048: val_loss improved from 1.24721 to 1.24495, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.8511 - acc: 0.7625 - val_loss: 1.2450 - val_acc: 0.5106\n",
            "Epoch 49/300\n",
            "\n",
            "Epoch 00049: val_loss improved from 1.24495 to 1.21954, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.8506 - acc: 0.7667 - val_loss: 1.2195 - val_acc: 0.4894\n",
            "Epoch 50/300\n",
            "\n",
            "Epoch 00050: val_loss improved from 1.21954 to 1.21073, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.8464 - acc: 0.7500 - val_loss: 1.2107 - val_acc: 0.5319\n",
            "Epoch 51/300\n",
            "\n",
            "Epoch 00051: val_loss improved from 1.21073 to 1.20506, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.8105 - acc: 0.8000 - val_loss: 1.2051 - val_acc: 0.5106\n",
            "Epoch 52/300\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 1.20506\n",
            "240/240 - 0s - loss: 0.8407 - acc: 0.7625 - val_loss: 1.2270 - val_acc: 0.4468\n",
            "Epoch 53/300\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 1.20506\n",
            "240/240 - 0s - loss: 0.8128 - acc: 0.7500 - val_loss: 1.2379 - val_acc: 0.4468\n",
            "Epoch 54/300\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 1.20506\n",
            "240/240 - 0s - loss: 0.8148 - acc: 0.7875 - val_loss: 1.2190 - val_acc: 0.4681\n",
            "Epoch 55/300\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 1.20506\n",
            "240/240 - 0s - loss: 0.7870 - acc: 0.7958 - val_loss: 1.2364 - val_acc: 0.5106\n",
            "Epoch 56/300\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 1.20506\n",
            "240/240 - 0s - loss: 0.8021 - acc: 0.7792 - val_loss: 1.2559 - val_acc: 0.4894\n",
            "Epoch 57/300\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 1.20506\n",
            "240/240 - 0s - loss: 0.8232 - acc: 0.7708 - val_loss: 1.2238 - val_acc: 0.5319\n",
            "Epoch 58/300\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 1.20506\n",
            "240/240 - 0s - loss: 0.7669 - acc: 0.7917 - val_loss: 1.2160 - val_acc: 0.4894\n",
            "Epoch 59/300\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 1.20506\n",
            "240/240 - 0s - loss: 0.7524 - acc: 0.8250 - val_loss: 1.2453 - val_acc: 0.5106\n",
            "Epoch 60/300\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 1.20506\n",
            "240/240 - 0s - loss: 0.7938 - acc: 0.8042 - val_loss: 1.2219 - val_acc: 0.5106\n",
            "Epoch 61/300\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 1.20506\n",
            "240/240 - 0s - loss: 0.7825 - acc: 0.7750 - val_loss: 1.2276 - val_acc: 0.5106\n",
            "Epoch 62/300\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 1.20506\n",
            "240/240 - 0s - loss: 0.7821 - acc: 0.7833 - val_loss: 1.2533 - val_acc: 0.5319\n",
            "Epoch 63/300\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 1.20506\n",
            "240/240 - 0s - loss: 0.7874 - acc: 0.7833 - val_loss: 1.2676 - val_acc: 0.4894\n",
            "Epoch 64/300\n",
            "\n",
            "Epoch 00064: val_loss improved from 1.20506 to 1.20218, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.7679 - acc: 0.7708 - val_loss: 1.2022 - val_acc: 0.5532\n",
            "Epoch 65/300\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 1.20218\n",
            "240/240 - 0s - loss: 0.7466 - acc: 0.8042 - val_loss: 1.2091 - val_acc: 0.5532\n",
            "Epoch 66/300\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 1.20218\n",
            "240/240 - 0s - loss: 0.7468 - acc: 0.8250 - val_loss: 1.2226 - val_acc: 0.4681\n",
            "Epoch 67/300\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 1.20218\n",
            "240/240 - 0s - loss: 0.7585 - acc: 0.7792 - val_loss: 1.2330 - val_acc: 0.5106\n",
            "Epoch 68/300\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 1.20218\n",
            "240/240 - 0s - loss: 0.7445 - acc: 0.7958 - val_loss: 1.2232 - val_acc: 0.5106\n",
            "Epoch 69/300\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 1.20218\n",
            "240/240 - 0s - loss: 0.7176 - acc: 0.8208 - val_loss: 1.2432 - val_acc: 0.4468\n",
            "Epoch 70/300\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 1.20218\n",
            "240/240 - 0s - loss: 0.7366 - acc: 0.8042 - val_loss: 1.2365 - val_acc: 0.5106\n",
            "Epoch 71/300\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 1.20218\n",
            "240/240 - 0s - loss: 0.7151 - acc: 0.8250 - val_loss: 1.2027 - val_acc: 0.5532\n",
            "Epoch 72/300\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 1.20218\n",
            "240/240 - 0s - loss: 0.7237 - acc: 0.8208 - val_loss: 1.2286 - val_acc: 0.5106\n",
            "Epoch 73/300\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 1.20218\n",
            "240/240 - 0s - loss: 0.7263 - acc: 0.8083 - val_loss: 1.2577 - val_acc: 0.5319\n",
            "Epoch 74/300\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 1.20218\n",
            "240/240 - 0s - loss: 0.7088 - acc: 0.8042 - val_loss: 1.2152 - val_acc: 0.5957\n",
            "Epoch 75/300\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 1.20218\n",
            "240/240 - 0s - loss: 0.7093 - acc: 0.8125 - val_loss: 1.2064 - val_acc: 0.5745\n",
            "Epoch 76/300\n",
            "\n",
            "Epoch 00076: val_loss improved from 1.20218 to 1.20025, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.7321 - acc: 0.7667 - val_loss: 1.2002 - val_acc: 0.4894\n",
            "Epoch 77/300\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 1.20025\n",
            "240/240 - 0s - loss: 0.7131 - acc: 0.8000 - val_loss: 1.2169 - val_acc: 0.6170\n",
            "Epoch 78/300\n",
            "\n",
            "Epoch 00078: val_loss improved from 1.20025 to 1.19984, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.6993 - acc: 0.8125 - val_loss: 1.1998 - val_acc: 0.5745\n",
            "Epoch 79/300\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 1.19984\n",
            "240/240 - 0s - loss: 0.7034 - acc: 0.8292 - val_loss: 1.2279 - val_acc: 0.5745\n",
            "Epoch 80/300\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 1.19984\n",
            "240/240 - 0s - loss: 0.7115 - acc: 0.8000 - val_loss: 1.2564 - val_acc: 0.5319\n",
            "Epoch 81/300\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 1.19984\n",
            "240/240 - 0s - loss: 0.7039 - acc: 0.8042 - val_loss: 1.2520 - val_acc: 0.4894\n",
            "Epoch 82/300\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 1.19984\n",
            "240/240 - 0s - loss: 0.6936 - acc: 0.8042 - val_loss: 1.2703 - val_acc: 0.5106\n",
            "Epoch 83/300\n",
            "\n",
            "Epoch 00083: val_loss improved from 1.19984 to 1.19020, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.6859 - acc: 0.8042 - val_loss: 1.1902 - val_acc: 0.5532\n",
            "Epoch 84/300\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.6771 - acc: 0.8083 - val_loss: 1.2233 - val_acc: 0.5745\n",
            "Epoch 85/300\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.6860 - acc: 0.7958 - val_loss: 1.2012 - val_acc: 0.5532\n",
            "Epoch 86/300\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.6589 - acc: 0.8333 - val_loss: 1.1946 - val_acc: 0.5106\n",
            "Epoch 87/300\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.6590 - acc: 0.8417 - val_loss: 1.2133 - val_acc: 0.5532\n",
            "Epoch 88/300\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.6581 - acc: 0.8500 - val_loss: 1.2464 - val_acc: 0.5106\n",
            "Epoch 89/300\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.6587 - acc: 0.8417 - val_loss: 1.1967 - val_acc: 0.5319\n",
            "Epoch 90/300\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.6571 - acc: 0.8083 - val_loss: 1.2294 - val_acc: 0.5532\n",
            "Epoch 91/300\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.6824 - acc: 0.8250 - val_loss: 1.2628 - val_acc: 0.5319\n",
            "Epoch 92/300\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.6433 - acc: 0.8292 - val_loss: 1.2237 - val_acc: 0.5319\n",
            "Epoch 93/300\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.6703 - acc: 0.8292 - val_loss: 1.2587 - val_acc: 0.5106\n",
            "Epoch 94/300\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.6566 - acc: 0.8417 - val_loss: 1.2172 - val_acc: 0.5532\n",
            "Epoch 95/300\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.6450 - acc: 0.8667 - val_loss: 1.2117 - val_acc: 0.5319\n",
            "Epoch 96/300\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.6603 - acc: 0.8500 - val_loss: 1.2382 - val_acc: 0.5319\n",
            "Epoch 97/300\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.6258 - acc: 0.8583 - val_loss: 1.2499 - val_acc: 0.5532\n",
            "Epoch 98/300\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.6368 - acc: 0.8375 - val_loss: 1.1995 - val_acc: 0.6170\n",
            "Epoch 99/300\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.6384 - acc: 0.8250 - val_loss: 1.2095 - val_acc: 0.5957\n",
            "Epoch 100/300\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.6471 - acc: 0.8417 - val_loss: 1.2438 - val_acc: 0.5319\n",
            "Epoch 101/300\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.6325 - acc: 0.8375 - val_loss: 1.2323 - val_acc: 0.5106\n",
            "Epoch 102/300\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.6393 - acc: 0.8625 - val_loss: 1.2459 - val_acc: 0.5532\n",
            "Epoch 103/300\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.6192 - acc: 0.8542 - val_loss: 1.2479 - val_acc: 0.5745\n",
            "Epoch 104/300\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.6341 - acc: 0.8458 - val_loss: 1.2889 - val_acc: 0.4894\n",
            "Epoch 105/300\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.6262 - acc: 0.8333 - val_loss: 1.3116 - val_acc: 0.4681\n",
            "Epoch 106/300\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.6174 - acc: 0.8417 - val_loss: 1.2659 - val_acc: 0.5319\n",
            "Epoch 107/300\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.6159 - acc: 0.8542 - val_loss: 1.3767 - val_acc: 0.4681\n",
            "Epoch 108/300\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.6227 - acc: 0.8542 - val_loss: 1.3232 - val_acc: 0.4468\n",
            "Epoch 109/300\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.5729 - acc: 0.8792 - val_loss: 1.2556 - val_acc: 0.4894\n",
            "Epoch 110/300\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.6484 - acc: 0.8708 - val_loss: 1.3665 - val_acc: 0.4681\n",
            "Epoch 111/300\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.6120 - acc: 0.8708 - val_loss: 1.2425 - val_acc: 0.5106\n",
            "Epoch 112/300\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.6270 - acc: 0.8333 - val_loss: 1.3139 - val_acc: 0.4681\n",
            "Epoch 113/300\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.6037 - acc: 0.8542 - val_loss: 1.3511 - val_acc: 0.4681\n",
            "Epoch 114/300\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.5699 - acc: 0.8958 - val_loss: 1.2519 - val_acc: 0.4468\n",
            "Epoch 115/300\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.6178 - acc: 0.8167 - val_loss: 1.3175 - val_acc: 0.5106\n",
            "Epoch 116/300\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.6117 - acc: 0.8333 - val_loss: 1.2161 - val_acc: 0.5319\n",
            "Epoch 117/300\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.5644 - acc: 0.8750 - val_loss: 1.2835 - val_acc: 0.4894\n",
            "Epoch 118/300\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.6069 - acc: 0.8667 - val_loss: 1.2539 - val_acc: 0.5532\n",
            "Epoch 119/300\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.5854 - acc: 0.8583 - val_loss: 1.2622 - val_acc: 0.5745\n",
            "Epoch 120/300\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.5950 - acc: 0.8750 - val_loss: 1.2555 - val_acc: 0.5745\n",
            "Epoch 121/300\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.5518 - acc: 0.8792 - val_loss: 1.2386 - val_acc: 0.5319\n",
            "Epoch 122/300\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.5675 - acc: 0.8917 - val_loss: 1.2542 - val_acc: 0.5745\n",
            "Epoch 123/300\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.5748 - acc: 0.8583 - val_loss: 1.2628 - val_acc: 0.5745\n",
            "Epoch 124/300\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.5837 - acc: 0.8375 - val_loss: 1.2330 - val_acc: 0.5319\n",
            "Epoch 125/300\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.5698 - acc: 0.8750 - val_loss: 1.2775 - val_acc: 0.4681\n",
            "Epoch 126/300\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.5739 - acc: 0.8208 - val_loss: 1.2823 - val_acc: 0.5319\n",
            "Epoch 127/300\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.5889 - acc: 0.8500 - val_loss: 1.2165 - val_acc: 0.5106\n",
            "Epoch 128/300\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.5960 - acc: 0.8417 - val_loss: 1.2735 - val_acc: 0.5319\n",
            "Epoch 129/300\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.5572 - acc: 0.8625 - val_loss: 1.2730 - val_acc: 0.4894\n",
            "Epoch 130/300\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.5682 - acc: 0.8458 - val_loss: 1.2587 - val_acc: 0.5532\n",
            "Epoch 131/300\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.5373 - acc: 0.8750 - val_loss: 1.2966 - val_acc: 0.5532\n",
            "Epoch 132/300\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.5432 - acc: 0.8792 - val_loss: 1.3232 - val_acc: 0.4894\n",
            "Epoch 133/300\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.5334 - acc: 0.9000 - val_loss: 1.3058 - val_acc: 0.5319\n",
            "Epoch 134/300\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.5536 - acc: 0.8458 - val_loss: 1.2616 - val_acc: 0.5319\n",
            "Epoch 135/300\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.5456 - acc: 0.8583 - val_loss: 1.2903 - val_acc: 0.4894\n",
            "Epoch 136/300\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.5551 - acc: 0.8667 - val_loss: 1.2588 - val_acc: 0.5745\n",
            "Epoch 137/300\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.6036 - acc: 0.8542 - val_loss: 1.3295 - val_acc: 0.4681\n",
            "Epoch 138/300\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.5443 - acc: 0.8833 - val_loss: 1.2202 - val_acc: 0.4681\n",
            "Epoch 139/300\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.5627 - acc: 0.8500 - val_loss: 1.3065 - val_acc: 0.4894\n",
            "Epoch 140/300\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.5272 - acc: 0.9000 - val_loss: 1.2342 - val_acc: 0.5532\n",
            "Epoch 141/300\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.5401 - acc: 0.8917 - val_loss: 1.2185 - val_acc: 0.5319\n",
            "Epoch 142/300\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.5209 - acc: 0.8750 - val_loss: 1.2407 - val_acc: 0.5319\n",
            "Epoch 143/300\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.5330 - acc: 0.9083 - val_loss: 1.2426 - val_acc: 0.4681\n",
            "Epoch 144/300\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.5471 - acc: 0.8667 - val_loss: 1.2620 - val_acc: 0.5106\n",
            "Epoch 145/300\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.5151 - acc: 0.9000 - val_loss: 1.2779 - val_acc: 0.4894\n",
            "Epoch 146/300\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.5443 - acc: 0.8667 - val_loss: 1.2304 - val_acc: 0.5532\n",
            "Epoch 147/300\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.5343 - acc: 0.8917 - val_loss: 1.2794 - val_acc: 0.5106\n",
            "Epoch 148/300\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.5204 - acc: 0.9042 - val_loss: 1.2297 - val_acc: 0.5106\n",
            "Epoch 149/300\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.5661 - acc: 0.8625 - val_loss: 1.2721 - val_acc: 0.5106\n",
            "Epoch 150/300\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.5345 - acc: 0.8875 - val_loss: 1.2206 - val_acc: 0.4681\n",
            "Epoch 151/300\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.5140 - acc: 0.8833 - val_loss: 1.2315 - val_acc: 0.5532\n",
            "Epoch 152/300\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.5093 - acc: 0.9083 - val_loss: 1.2316 - val_acc: 0.5106\n",
            "Epoch 153/300\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.5029 - acc: 0.8958 - val_loss: 1.2694 - val_acc: 0.4894\n",
            "Epoch 154/300\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.5035 - acc: 0.8917 - val_loss: 1.2289 - val_acc: 0.5319\n",
            "Epoch 155/300\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.5113 - acc: 0.8833 - val_loss: 1.2758 - val_acc: 0.5319\n",
            "Epoch 156/300\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.4812 - acc: 0.9042 - val_loss: 1.2544 - val_acc: 0.4681\n",
            "Epoch 157/300\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.4921 - acc: 0.8917 - val_loss: 1.3139 - val_acc: 0.5106\n",
            "Epoch 158/300\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.5065 - acc: 0.8708 - val_loss: 1.2605 - val_acc: 0.5319\n",
            "Epoch 159/300\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.4966 - acc: 0.8833 - val_loss: 1.2687 - val_acc: 0.5319\n",
            "Epoch 160/300\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.5034 - acc: 0.9083 - val_loss: 1.2238 - val_acc: 0.4894\n",
            "Epoch 161/300\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.4881 - acc: 0.8875 - val_loss: 1.2492 - val_acc: 0.5532\n",
            "Epoch 162/300\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.5174 - acc: 0.8833 - val_loss: 1.2252 - val_acc: 0.5106\n",
            "Epoch 163/300\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.5146 - acc: 0.8708 - val_loss: 1.2355 - val_acc: 0.4681\n",
            "Epoch 164/300\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.4753 - acc: 0.9083 - val_loss: 1.2972 - val_acc: 0.4894\n",
            "Epoch 165/300\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.4733 - acc: 0.9208 - val_loss: 1.2825 - val_acc: 0.4894\n",
            "Epoch 166/300\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.4572 - acc: 0.9042 - val_loss: 1.2933 - val_acc: 0.4894\n",
            "Epoch 167/300\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.4921 - acc: 0.8833 - val_loss: 1.2267 - val_acc: 0.5532\n",
            "Epoch 168/300\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.4875 - acc: 0.8708 - val_loss: 1.2832 - val_acc: 0.4468\n",
            "Epoch 169/300\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.4641 - acc: 0.9083 - val_loss: 1.2664 - val_acc: 0.5532\n",
            "Epoch 170/300\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.4996 - acc: 0.9000 - val_loss: 1.2933 - val_acc: 0.5106\n",
            "Epoch 171/300\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.4797 - acc: 0.8917 - val_loss: 1.3348 - val_acc: 0.4681\n",
            "Epoch 172/300\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.4507 - acc: 0.9167 - val_loss: 1.2484 - val_acc: 0.5532\n",
            "Epoch 173/300\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.4987 - acc: 0.9000 - val_loss: 1.2405 - val_acc: 0.5106\n",
            "Epoch 174/300\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.4818 - acc: 0.8833 - val_loss: 1.2346 - val_acc: 0.5957\n",
            "Epoch 175/300\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.4849 - acc: 0.8792 - val_loss: 1.2806 - val_acc: 0.4468\n",
            "Epoch 176/300\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.4724 - acc: 0.9125 - val_loss: 1.2806 - val_acc: 0.5532\n",
            "Epoch 177/300\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.4631 - acc: 0.9042 - val_loss: 1.3513 - val_acc: 0.5319\n",
            "Epoch 178/300\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.4862 - acc: 0.8958 - val_loss: 1.2926 - val_acc: 0.5745\n",
            "Epoch 179/300\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.5047 - acc: 0.8750 - val_loss: 1.3907 - val_acc: 0.4894\n",
            "Epoch 180/300\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.5069 - acc: 0.8500 - val_loss: 1.3053 - val_acc: 0.4468\n",
            "Epoch 181/300\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.4675 - acc: 0.9208 - val_loss: 1.3167 - val_acc: 0.4681\n",
            "Epoch 182/300\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.4765 - acc: 0.8750 - val_loss: 1.3350 - val_acc: 0.5319\n",
            "Epoch 183/300\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.4697 - acc: 0.8875 - val_loss: 1.3251 - val_acc: 0.4255\n",
            "Epoch 184/300\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.4633 - acc: 0.8708 - val_loss: 1.3111 - val_acc: 0.5106\n",
            "Epoch 185/300\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.4577 - acc: 0.8917 - val_loss: 1.2893 - val_acc: 0.4894\n",
            "Epoch 186/300\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.4623 - acc: 0.9167 - val_loss: 1.2533 - val_acc: 0.5532\n",
            "Epoch 187/300\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.4267 - acc: 0.9208 - val_loss: 1.3392 - val_acc: 0.5106\n",
            "Epoch 188/300\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.4279 - acc: 0.8875 - val_loss: 1.3505 - val_acc: 0.4468\n",
            "Epoch 189/300\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.4432 - acc: 0.9250 - val_loss: 1.3372 - val_acc: 0.4894\n",
            "Epoch 190/300\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.4186 - acc: 0.9292 - val_loss: 1.3604 - val_acc: 0.4894\n",
            "Epoch 191/300\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.4572 - acc: 0.9250 - val_loss: 1.5268 - val_acc: 0.4043\n",
            "Epoch 192/300\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.4636 - acc: 0.8833 - val_loss: 1.3419 - val_acc: 0.5106\n",
            "Epoch 193/300\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.4371 - acc: 0.9042 - val_loss: 1.3825 - val_acc: 0.4468\n",
            "Epoch 194/300\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.4279 - acc: 0.9167 - val_loss: 1.3318 - val_acc: 0.5106\n",
            "Epoch 195/300\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.4585 - acc: 0.8917 - val_loss: 1.2967 - val_acc: 0.4681\n",
            "Epoch 196/300\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.4071 - acc: 0.9250 - val_loss: 1.3921 - val_acc: 0.4468\n",
            "Epoch 197/300\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.4264 - acc: 0.9167 - val_loss: 1.3325 - val_acc: 0.4468\n",
            "Epoch 198/300\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.3986 - acc: 0.9125 - val_loss: 1.3603 - val_acc: 0.4468\n",
            "Epoch 199/300\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.4327 - acc: 0.8917 - val_loss: 1.3786 - val_acc: 0.4681\n",
            "Epoch 200/300\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.4323 - acc: 0.9000 - val_loss: 1.3514 - val_acc: 0.4255\n",
            "Epoch 201/300\n",
            "\n",
            "Epoch 00201: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.4343 - acc: 0.9000 - val_loss: 1.3638 - val_acc: 0.5745\n",
            "Epoch 202/300\n",
            "\n",
            "Epoch 00202: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.4271 - acc: 0.9167 - val_loss: 1.3717 - val_acc: 0.4894\n",
            "Epoch 203/300\n",
            "\n",
            "Epoch 00203: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.4262 - acc: 0.9292 - val_loss: 1.3056 - val_acc: 0.5106\n",
            "Epoch 204/300\n",
            "\n",
            "Epoch 00204: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.4040 - acc: 0.9333 - val_loss: 1.2981 - val_acc: 0.5106\n",
            "Epoch 205/300\n",
            "\n",
            "Epoch 00205: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.3784 - acc: 0.9333 - val_loss: 1.3129 - val_acc: 0.4681\n",
            "Epoch 206/300\n",
            "\n",
            "Epoch 00206: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.3884 - acc: 0.9250 - val_loss: 1.4308 - val_acc: 0.4894\n",
            "Epoch 207/300\n",
            "\n",
            "Epoch 00207: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.3985 - acc: 0.9292 - val_loss: 1.4198 - val_acc: 0.4894\n",
            "Epoch 208/300\n",
            "\n",
            "Epoch 00208: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.3952 - acc: 0.9333 - val_loss: 1.3917 - val_acc: 0.4255\n",
            "Epoch 209/300\n",
            "\n",
            "Epoch 00209: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.4611 - acc: 0.9042 - val_loss: 1.3649 - val_acc: 0.5319\n",
            "Epoch 210/300\n",
            "\n",
            "Epoch 00210: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.4275 - acc: 0.9125 - val_loss: 1.4430 - val_acc: 0.4255\n",
            "Epoch 211/300\n",
            "\n",
            "Epoch 00211: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.3854 - acc: 0.9250 - val_loss: 1.4112 - val_acc: 0.4681\n",
            "Epoch 212/300\n",
            "\n",
            "Epoch 00212: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.4148 - acc: 0.9292 - val_loss: 1.4031 - val_acc: 0.5745\n",
            "Epoch 213/300\n",
            "\n",
            "Epoch 00213: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.4248 - acc: 0.9333 - val_loss: 1.4388 - val_acc: 0.4894\n",
            "Epoch 214/300\n",
            "\n",
            "Epoch 00214: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.3764 - acc: 0.9083 - val_loss: 1.3581 - val_acc: 0.4894\n",
            "Epoch 215/300\n",
            "\n",
            "Epoch 00215: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.3992 - acc: 0.9458 - val_loss: 1.3421 - val_acc: 0.4894\n",
            "Epoch 216/300\n",
            "\n",
            "Epoch 00216: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.4209 - acc: 0.9042 - val_loss: 1.3362 - val_acc: 0.5957\n",
            "Epoch 217/300\n",
            "\n",
            "Epoch 00217: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.4094 - acc: 0.9125 - val_loss: 1.3491 - val_acc: 0.4894\n",
            "Epoch 218/300\n",
            "\n",
            "Epoch 00218: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.4137 - acc: 0.8917 - val_loss: 1.3456 - val_acc: 0.4894\n",
            "Epoch 219/300\n",
            "\n",
            "Epoch 00219: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.3934 - acc: 0.9167 - val_loss: 1.3899 - val_acc: 0.4681\n",
            "Epoch 220/300\n",
            "\n",
            "Epoch 00220: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.3636 - acc: 0.9375 - val_loss: 1.3709 - val_acc: 0.4894\n",
            "Epoch 221/300\n",
            "\n",
            "Epoch 00221: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.3991 - acc: 0.9125 - val_loss: 1.3940 - val_acc: 0.4681\n",
            "Epoch 222/300\n",
            "\n",
            "Epoch 00222: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.4109 - acc: 0.9333 - val_loss: 1.3398 - val_acc: 0.5106\n",
            "Epoch 223/300\n",
            "\n",
            "Epoch 00223: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.4020 - acc: 0.9125 - val_loss: 1.3272 - val_acc: 0.4681\n",
            "Epoch 224/300\n",
            "\n",
            "Epoch 00224: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.3638 - acc: 0.9458 - val_loss: 1.4001 - val_acc: 0.4681\n",
            "Epoch 225/300\n",
            "\n",
            "Epoch 00225: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.3763 - acc: 0.9208 - val_loss: 1.3734 - val_acc: 0.5532\n",
            "Epoch 226/300\n",
            "\n",
            "Epoch 00226: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.3801 - acc: 0.9167 - val_loss: 1.3511 - val_acc: 0.5106\n",
            "Epoch 227/300\n",
            "\n",
            "Epoch 00227: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.4041 - acc: 0.9125 - val_loss: 1.4216 - val_acc: 0.4894\n",
            "Epoch 228/300\n",
            "\n",
            "Epoch 00228: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.4119 - acc: 0.8958 - val_loss: 1.3770 - val_acc: 0.4681\n",
            "Epoch 229/300\n",
            "\n",
            "Epoch 00229: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.4096 - acc: 0.9042 - val_loss: 1.4203 - val_acc: 0.4681\n",
            "Epoch 230/300\n",
            "\n",
            "Epoch 00230: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.3650 - acc: 0.9292 - val_loss: 1.3511 - val_acc: 0.5319\n",
            "Epoch 231/300\n",
            "\n",
            "Epoch 00231: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.4039 - acc: 0.9292 - val_loss: 1.4356 - val_acc: 0.4043\n",
            "Epoch 232/300\n",
            "\n",
            "Epoch 00232: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.4025 - acc: 0.9167 - val_loss: 1.4056 - val_acc: 0.5319\n",
            "Epoch 233/300\n",
            "\n",
            "Epoch 00233: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.3814 - acc: 0.9250 - val_loss: 1.4027 - val_acc: 0.4681\n",
            "Epoch 234/300\n",
            "\n",
            "Epoch 00234: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.3900 - acc: 0.9125 - val_loss: 1.3719 - val_acc: 0.4468\n",
            "Epoch 235/300\n",
            "\n",
            "Epoch 00235: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.3795 - acc: 0.9417 - val_loss: 1.3557 - val_acc: 0.5532\n",
            "Epoch 236/300\n",
            "\n",
            "Epoch 00236: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.3960 - acc: 0.9208 - val_loss: 1.3880 - val_acc: 0.4681\n",
            "Epoch 237/300\n",
            "\n",
            "Epoch 00237: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.3628 - acc: 0.9417 - val_loss: 1.3652 - val_acc: 0.4894\n",
            "Epoch 238/300\n",
            "\n",
            "Epoch 00238: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.3554 - acc: 0.9500 - val_loss: 1.3026 - val_acc: 0.5745\n",
            "Epoch 239/300\n",
            "\n",
            "Epoch 00239: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.3823 - acc: 0.9083 - val_loss: 1.3667 - val_acc: 0.5106\n",
            "Epoch 240/300\n",
            "\n",
            "Epoch 00240: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.3394 - acc: 0.9333 - val_loss: 1.3835 - val_acc: 0.5319\n",
            "Epoch 241/300\n",
            "\n",
            "Epoch 00241: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.3486 - acc: 0.9208 - val_loss: 1.4122 - val_acc: 0.4468\n",
            "Epoch 242/300\n",
            "\n",
            "Epoch 00242: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.3591 - acc: 0.9208 - val_loss: 1.4492 - val_acc: 0.4468\n",
            "Epoch 243/300\n",
            "\n",
            "Epoch 00243: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.3363 - acc: 0.9417 - val_loss: 1.4433 - val_acc: 0.3830\n",
            "Epoch 244/300\n",
            "\n",
            "Epoch 00244: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.3756 - acc: 0.9333 - val_loss: 1.4747 - val_acc: 0.4255\n",
            "Epoch 245/300\n",
            "\n",
            "Epoch 00245: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.3661 - acc: 0.9292 - val_loss: 1.3835 - val_acc: 0.4894\n",
            "Epoch 246/300\n",
            "\n",
            "Epoch 00246: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.3809 - acc: 0.9250 - val_loss: 1.3907 - val_acc: 0.4255\n",
            "Epoch 247/300\n",
            "\n",
            "Epoch 00247: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.3848 - acc: 0.9042 - val_loss: 1.4353 - val_acc: 0.4681\n",
            "Epoch 248/300\n",
            "\n",
            "Epoch 00248: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.3584 - acc: 0.9208 - val_loss: 1.4863 - val_acc: 0.4681\n",
            "Epoch 249/300\n",
            "\n",
            "Epoch 00249: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.3619 - acc: 0.9167 - val_loss: 1.5076 - val_acc: 0.4681\n",
            "Epoch 250/300\n",
            "\n",
            "Epoch 00250: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.3627 - acc: 0.9333 - val_loss: 1.4280 - val_acc: 0.4681\n",
            "Epoch 251/300\n",
            "\n",
            "Epoch 00251: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.3807 - acc: 0.9000 - val_loss: 1.4460 - val_acc: 0.4681\n",
            "Epoch 252/300\n",
            "\n",
            "Epoch 00252: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.3537 - acc: 0.9333 - val_loss: 1.4283 - val_acc: 0.4894\n",
            "Epoch 253/300\n",
            "\n",
            "Epoch 00253: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.3831 - acc: 0.9250 - val_loss: 1.4666 - val_acc: 0.5106\n",
            "Epoch 254/300\n",
            "\n",
            "Epoch 00254: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.3543 - acc: 0.9292 - val_loss: 1.3964 - val_acc: 0.4894\n",
            "Epoch 255/300\n",
            "\n",
            "Epoch 00255: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.4073 - acc: 0.8958 - val_loss: 1.4687 - val_acc: 0.5106\n",
            "Epoch 256/300\n",
            "\n",
            "Epoch 00256: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.3486 - acc: 0.9417 - val_loss: 1.4651 - val_acc: 0.5106\n",
            "Epoch 257/300\n",
            "\n",
            "Epoch 00257: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.3325 - acc: 0.9250 - val_loss: 1.4234 - val_acc: 0.5106\n",
            "Epoch 258/300\n",
            "\n",
            "Epoch 00258: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.3660 - acc: 0.9292 - val_loss: 1.3976 - val_acc: 0.4894\n",
            "Epoch 259/300\n",
            "\n",
            "Epoch 00259: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.3759 - acc: 0.9375 - val_loss: 1.3617 - val_acc: 0.5319\n",
            "Epoch 260/300\n",
            "\n",
            "Epoch 00260: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.3785 - acc: 0.9167 - val_loss: 1.4290 - val_acc: 0.4894\n",
            "Epoch 261/300\n",
            "\n",
            "Epoch 00261: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.3646 - acc: 0.9250 - val_loss: 1.4008 - val_acc: 0.5319\n",
            "Epoch 262/300\n",
            "\n",
            "Epoch 00262: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.3335 - acc: 0.9583 - val_loss: 1.3615 - val_acc: 0.5106\n",
            "Epoch 263/300\n",
            "\n",
            "Epoch 00263: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.3497 - acc: 0.9250 - val_loss: 1.3659 - val_acc: 0.5106\n",
            "Epoch 264/300\n",
            "\n",
            "Epoch 00264: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.3471 - acc: 0.9333 - val_loss: 1.4437 - val_acc: 0.4681\n",
            "Epoch 265/300\n",
            "\n",
            "Epoch 00265: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.3804 - acc: 0.9125 - val_loss: 1.4423 - val_acc: 0.4255\n",
            "Epoch 266/300\n",
            "\n",
            "Epoch 00266: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.3345 - acc: 0.9333 - val_loss: 1.4178 - val_acc: 0.4681\n",
            "Epoch 267/300\n",
            "\n",
            "Epoch 00267: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.3257 - acc: 0.9500 - val_loss: 1.4231 - val_acc: 0.4681\n",
            "Epoch 268/300\n",
            "\n",
            "Epoch 00268: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.3604 - acc: 0.9167 - val_loss: 1.4584 - val_acc: 0.5532\n",
            "Epoch 269/300\n",
            "\n",
            "Epoch 00269: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.3263 - acc: 0.9417 - val_loss: 1.5102 - val_acc: 0.4255\n",
            "Epoch 270/300\n",
            "\n",
            "Epoch 00270: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.3551 - acc: 0.9208 - val_loss: 1.4684 - val_acc: 0.4894\n",
            "Epoch 271/300\n",
            "\n",
            "Epoch 00271: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.3377 - acc: 0.9375 - val_loss: 1.3635 - val_acc: 0.4894\n",
            "Epoch 272/300\n",
            "\n",
            "Epoch 00272: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.3224 - acc: 0.9500 - val_loss: 1.3587 - val_acc: 0.5319\n",
            "Epoch 273/300\n",
            "\n",
            "Epoch 00273: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.3434 - acc: 0.8875 - val_loss: 1.3733 - val_acc: 0.5745\n",
            "Epoch 274/300\n",
            "\n",
            "Epoch 00274: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.3353 - acc: 0.9333 - val_loss: 1.4135 - val_acc: 0.4894\n",
            "Epoch 275/300\n",
            "\n",
            "Epoch 00275: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.3394 - acc: 0.9458 - val_loss: 1.3572 - val_acc: 0.5745\n",
            "Epoch 276/300\n",
            "\n",
            "Epoch 00276: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.3341 - acc: 0.9125 - val_loss: 1.3914 - val_acc: 0.4681\n",
            "Epoch 277/300\n",
            "\n",
            "Epoch 00277: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.3306 - acc: 0.9250 - val_loss: 1.5183 - val_acc: 0.4681\n",
            "Epoch 278/300\n",
            "\n",
            "Epoch 00278: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.3504 - acc: 0.9208 - val_loss: 1.4587 - val_acc: 0.4468\n",
            "Epoch 279/300\n",
            "\n",
            "Epoch 00279: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.3344 - acc: 0.9333 - val_loss: 1.3913 - val_acc: 0.4468\n",
            "Epoch 280/300\n",
            "\n",
            "Epoch 00280: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.3315 - acc: 0.9375 - val_loss: 1.4173 - val_acc: 0.5106\n",
            "Epoch 281/300\n",
            "\n",
            "Epoch 00281: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.3487 - acc: 0.9292 - val_loss: 1.4478 - val_acc: 0.4681\n",
            "Epoch 282/300\n",
            "\n",
            "Epoch 00282: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.3151 - acc: 0.9500 - val_loss: 1.4737 - val_acc: 0.5532\n",
            "Epoch 283/300\n",
            "\n",
            "Epoch 00283: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.2976 - acc: 0.9458 - val_loss: 1.4321 - val_acc: 0.4681\n",
            "Epoch 284/300\n",
            "\n",
            "Epoch 00284: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.3297 - acc: 0.9333 - val_loss: 1.4550 - val_acc: 0.5319\n",
            "Epoch 285/300\n",
            "\n",
            "Epoch 00285: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.3444 - acc: 0.9125 - val_loss: 1.3965 - val_acc: 0.4894\n",
            "Epoch 286/300\n",
            "\n",
            "Epoch 00286: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.3457 - acc: 0.9125 - val_loss: 1.3898 - val_acc: 0.5957\n",
            "Epoch 287/300\n",
            "\n",
            "Epoch 00287: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.3274 - acc: 0.9167 - val_loss: 1.5016 - val_acc: 0.4681\n",
            "Epoch 288/300\n",
            "\n",
            "Epoch 00288: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.3383 - acc: 0.9375 - val_loss: 1.4426 - val_acc: 0.5106\n",
            "Epoch 289/300\n",
            "\n",
            "Epoch 00289: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.3171 - acc: 0.9250 - val_loss: 1.6408 - val_acc: 0.4043\n",
            "Epoch 290/300\n",
            "\n",
            "Epoch 00290: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.3506 - acc: 0.9208 - val_loss: 1.5661 - val_acc: 0.3830\n",
            "Epoch 291/300\n",
            "\n",
            "Epoch 00291: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.3192 - acc: 0.9167 - val_loss: 1.6856 - val_acc: 0.4043\n",
            "Epoch 292/300\n",
            "\n",
            "Epoch 00292: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.2929 - acc: 0.9458 - val_loss: 1.5830 - val_acc: 0.5319\n",
            "Epoch 293/300\n",
            "\n",
            "Epoch 00293: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.3093 - acc: 0.9417 - val_loss: 1.5167 - val_acc: 0.5319\n",
            "Epoch 294/300\n",
            "\n",
            "Epoch 00294: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.3168 - acc: 0.9375 - val_loss: 1.4991 - val_acc: 0.4681\n",
            "Epoch 295/300\n",
            "\n",
            "Epoch 00295: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.3236 - acc: 0.9250 - val_loss: 1.5547 - val_acc: 0.5106\n",
            "Epoch 296/300\n",
            "\n",
            "Epoch 00296: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.3080 - acc: 0.9375 - val_loss: 1.5319 - val_acc: 0.4894\n",
            "Epoch 297/300\n",
            "\n",
            "Epoch 00297: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.3241 - acc: 0.9375 - val_loss: 1.4690 - val_acc: 0.4468\n",
            "Epoch 298/300\n",
            "\n",
            "Epoch 00298: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.2946 - acc: 0.9583 - val_loss: 1.4657 - val_acc: 0.4681\n",
            "Epoch 299/300\n",
            "\n",
            "Epoch 00299: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.2893 - acc: 0.9292 - val_loss: 1.4367 - val_acc: 0.4255\n",
            "Epoch 300/300\n",
            "\n",
            "Epoch 00300: val_loss did not improve from 1.19020\n",
            "240/240 - 0s - loss: 0.3379 - acc: 0.9292 - val_loss: 1.4446 - val_acc: 0.5319\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nogVTrMG0DQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "# Evaluate on test data\n",
            "288/288 [==============================] - 1s 2ms/sample - loss: 1.1197 - acc: 0.5903\n",
            "test loss, test acc: [1.1196955970788924, 0.5902778]\n",
            "EEG_Deep/Data2A/Data_A07T.mat\n",
            "(288, 22, 1875)\n",
            "(288, 1)\n",
            "(240, 22, 750)\n",
            "(240, 1)\n",
            "(47, 22, 750)\n",
            "(47, 1)\n",
            "X_train shape: (240, 1, 22, 750)\n",
            "240 train samples\n",
            "47 val samples\n",
            "EEG_Deep/Data2A/Data_A07E.mat\n",
            "(288, 22, 1875)\n",
            "(288, 1)\n",
            "(288, 22, 750)\n",
            "(288, 1)\n",
            "X_train shape: (288, 1, 22, 750)\n",
            "288 train samples\n",
            "Train on 240 samples, validate on 47 samples\n",
            "Epoch 1/300\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 1.38947, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 1s - loss: 1.4014 - acc: 0.2625 - val_loss: 1.3895 - val_acc: 0.1915\n",
            "Epoch 2/300\n",
            "\n",
            "Epoch 00002: val_loss improved from 1.38947 to 1.38709, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.3489 - acc: 0.3917 - val_loss: 1.3871 - val_acc: 0.2128\n",
            "Epoch 3/300\n",
            "\n",
            "Epoch 00003: val_loss improved from 1.38709 to 1.38530, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.3191 - acc: 0.4750 - val_loss: 1.3853 - val_acc: 0.2340\n",
            "Epoch 4/300\n",
            "\n",
            "Epoch 00004: val_loss improved from 1.38530 to 1.38236, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.2961 - acc: 0.5375 - val_loss: 1.3824 - val_acc: 0.2766\n",
            "Epoch 5/300\n",
            "\n",
            "Epoch 00005: val_loss improved from 1.38236 to 1.37630, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.2590 - acc: 0.5458 - val_loss: 1.3763 - val_acc: 0.2553\n",
            "Epoch 6/300\n",
            "\n",
            "Epoch 00006: val_loss improved from 1.37630 to 1.37586, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.2371 - acc: 0.5458 - val_loss: 1.3759 - val_acc: 0.2340\n",
            "Epoch 7/300\n",
            "\n",
            "Epoch 00007: val_loss improved from 1.37586 to 1.37372, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.2039 - acc: 0.5500 - val_loss: 1.3737 - val_acc: 0.2340\n",
            "Epoch 8/300\n",
            "\n",
            "Epoch 00008: val_loss improved from 1.37372 to 1.36796, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.1821 - acc: 0.6083 - val_loss: 1.3680 - val_acc: 0.3404\n",
            "Epoch 9/300\n",
            "\n",
            "Epoch 00009: val_loss improved from 1.36796 to 1.36405, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.1631 - acc: 0.5958 - val_loss: 1.3641 - val_acc: 0.4043\n",
            "Epoch 10/300\n",
            "\n",
            "Epoch 00010: val_loss improved from 1.36405 to 1.36224, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.1549 - acc: 0.5917 - val_loss: 1.3622 - val_acc: 0.3404\n",
            "Epoch 11/300\n",
            "\n",
            "Epoch 00011: val_loss improved from 1.36224 to 1.35855, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.1286 - acc: 0.6250 - val_loss: 1.3586 - val_acc: 0.3191\n",
            "Epoch 12/300\n",
            "\n",
            "Epoch 00012: val_loss improved from 1.35855 to 1.35742, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.1104 - acc: 0.6375 - val_loss: 1.3574 - val_acc: 0.3404\n",
            "Epoch 13/300\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 1.35742\n",
            "240/240 - 0s - loss: 1.0902 - acc: 0.6375 - val_loss: 1.3655 - val_acc: 0.2766\n",
            "Epoch 14/300\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 1.35742\n",
            "240/240 - 0s - loss: 1.0866 - acc: 0.6625 - val_loss: 1.3616 - val_acc: 0.3404\n",
            "Epoch 15/300\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 1.35742\n",
            "240/240 - 0s - loss: 1.0700 - acc: 0.6625 - val_loss: 1.3690 - val_acc: 0.2553\n",
            "Epoch 16/300\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 1.35742\n",
            "240/240 - 0s - loss: 1.0323 - acc: 0.6875 - val_loss: 1.3687 - val_acc: 0.2766\n",
            "Epoch 17/300\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 1.35742\n",
            "240/240 - 0s - loss: 1.0386 - acc: 0.6792 - val_loss: 1.3815 - val_acc: 0.2766\n",
            "Epoch 18/300\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 1.35742\n",
            "240/240 - 0s - loss: 1.0000 - acc: 0.6750 - val_loss: 1.3766 - val_acc: 0.2979\n",
            "Epoch 19/300\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 1.35742\n",
            "240/240 - 0s - loss: 1.0277 - acc: 0.6458 - val_loss: 1.3968 - val_acc: 0.2340\n",
            "Epoch 20/300\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 1.35742\n",
            "240/240 - 0s - loss: 0.9866 - acc: 0.7292 - val_loss: 1.4284 - val_acc: 0.2553\n",
            "Epoch 21/300\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 1.35742\n",
            "240/240 - 0s - loss: 0.9563 - acc: 0.7083 - val_loss: 1.4096 - val_acc: 0.3191\n",
            "Epoch 22/300\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 1.35742\n",
            "240/240 - 0s - loss: 0.9540 - acc: 0.7125 - val_loss: 1.3886 - val_acc: 0.2766\n",
            "Epoch 23/300\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 1.35742\n",
            "240/240 - 0s - loss: 0.9405 - acc: 0.7125 - val_loss: 1.4088 - val_acc: 0.2766\n",
            "Epoch 24/300\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 1.35742\n",
            "240/240 - 0s - loss: 0.9156 - acc: 0.7333 - val_loss: 1.3653 - val_acc: 0.3404\n",
            "Epoch 25/300\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 1.35742\n",
            "240/240 - 0s - loss: 0.8866 - acc: 0.7125 - val_loss: 1.3791 - val_acc: 0.2979\n",
            "Epoch 26/300\n",
            "\n",
            "Epoch 00026: val_loss improved from 1.35742 to 1.29624, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.8787 - acc: 0.7708 - val_loss: 1.2962 - val_acc: 0.3191\n",
            "Epoch 27/300\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 1.29624\n",
            "240/240 - 0s - loss: 0.8678 - acc: 0.7333 - val_loss: 1.3825 - val_acc: 0.3191\n",
            "Epoch 28/300\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 1.29624\n",
            "240/240 - 0s - loss: 0.8208 - acc: 0.7792 - val_loss: 1.4002 - val_acc: 0.2979\n",
            "Epoch 29/300\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 1.29624\n",
            "240/240 - 0s - loss: 0.8484 - acc: 0.7375 - val_loss: 1.3622 - val_acc: 0.3404\n",
            "Epoch 30/300\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 1.29624\n",
            "240/240 - 0s - loss: 0.8017 - acc: 0.7500 - val_loss: 1.3900 - val_acc: 0.3404\n",
            "Epoch 31/300\n",
            "\n",
            "Epoch 00031: val_loss improved from 1.29624 to 1.25172, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.8112 - acc: 0.7500 - val_loss: 1.2517 - val_acc: 0.3617\n",
            "Epoch 32/300\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 1.25172\n",
            "240/240 - 0s - loss: 0.7499 - acc: 0.8125 - val_loss: 1.2800 - val_acc: 0.3404\n",
            "Epoch 33/300\n",
            "\n",
            "Epoch 00033: val_loss improved from 1.25172 to 1.24564, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.7440 - acc: 0.7917 - val_loss: 1.2456 - val_acc: 0.4255\n",
            "Epoch 34/300\n",
            "\n",
            "Epoch 00034: val_loss improved from 1.24564 to 1.24157, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.7122 - acc: 0.8167 - val_loss: 1.2416 - val_acc: 0.4255\n",
            "Epoch 35/300\n",
            "\n",
            "Epoch 00035: val_loss improved from 1.24157 to 1.21531, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.7044 - acc: 0.8208 - val_loss: 1.2153 - val_acc: 0.4255\n",
            "Epoch 36/300\n",
            "\n",
            "Epoch 00036: val_loss improved from 1.21531 to 1.11863, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.6515 - acc: 0.8417 - val_loss: 1.1186 - val_acc: 0.4894\n",
            "Epoch 37/300\n",
            "\n",
            "Epoch 00037: val_loss improved from 1.11863 to 1.05991, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.6465 - acc: 0.8667 - val_loss: 1.0599 - val_acc: 0.4894\n",
            "Epoch 38/300\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 1.05991\n",
            "240/240 - 0s - loss: 0.6320 - acc: 0.8708 - val_loss: 1.1954 - val_acc: 0.4894\n",
            "Epoch 39/300\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 1.05991\n",
            "240/240 - 0s - loss: 0.6130 - acc: 0.8625 - val_loss: 1.3019 - val_acc: 0.4681\n",
            "Epoch 40/300\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 1.05991\n",
            "240/240 - 0s - loss: 0.5993 - acc: 0.8542 - val_loss: 1.1091 - val_acc: 0.5319\n",
            "Epoch 41/300\n",
            "\n",
            "Epoch 00041: val_loss improved from 1.05991 to 1.03916, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.5653 - acc: 0.8875 - val_loss: 1.0392 - val_acc: 0.5319\n",
            "Epoch 42/300\n",
            "\n",
            "Epoch 00042: val_loss improved from 1.03916 to 1.03328, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.5487 - acc: 0.8583 - val_loss: 1.0333 - val_acc: 0.5532\n",
            "Epoch 43/300\n",
            "\n",
            "Epoch 00043: val_loss improved from 1.03328 to 1.01883, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.5637 - acc: 0.8750 - val_loss: 1.0188 - val_acc: 0.5106\n",
            "Epoch 44/300\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 1.01883\n",
            "240/240 - 0s - loss: 0.5415 - acc: 0.9000 - val_loss: 1.0501 - val_acc: 0.5532\n",
            "Epoch 45/300\n",
            "\n",
            "Epoch 00045: val_loss improved from 1.01883 to 0.84845, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.5280 - acc: 0.8833 - val_loss: 0.8485 - val_acc: 0.5957\n",
            "Epoch 46/300\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.84845\n",
            "240/240 - 0s - loss: 0.5282 - acc: 0.8917 - val_loss: 0.9356 - val_acc: 0.6170\n",
            "Epoch 47/300\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.84845\n",
            "240/240 - 0s - loss: 0.5005 - acc: 0.9125 - val_loss: 0.8851 - val_acc: 0.6383\n",
            "Epoch 48/300\n",
            "\n",
            "Epoch 00048: val_loss improved from 0.84845 to 0.83506, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.5012 - acc: 0.8917 - val_loss: 0.8351 - val_acc: 0.5957\n",
            "Epoch 49/300\n",
            "\n",
            "Epoch 00049: val_loss improved from 0.83506 to 0.73193, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.4780 - acc: 0.9208 - val_loss: 0.7319 - val_acc: 0.6383\n",
            "Epoch 50/300\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.73193\n",
            "240/240 - 0s - loss: 0.5085 - acc: 0.8583 - val_loss: 0.8030 - val_acc: 0.6596\n",
            "Epoch 51/300\n",
            "\n",
            "Epoch 00051: val_loss improved from 0.73193 to 0.72794, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.4480 - acc: 0.9125 - val_loss: 0.7279 - val_acc: 0.7021\n",
            "Epoch 52/300\n",
            "\n",
            "Epoch 00052: val_loss improved from 0.72794 to 0.71703, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.4626 - acc: 0.9125 - val_loss: 0.7170 - val_acc: 0.7021\n",
            "Epoch 53/300\n",
            "\n",
            "Epoch 00053: val_loss improved from 0.71703 to 0.69556, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.4716 - acc: 0.8833 - val_loss: 0.6956 - val_acc: 0.7447\n",
            "Epoch 54/300\n",
            "\n",
            "Epoch 00054: val_loss improved from 0.69556 to 0.67182, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.4748 - acc: 0.8875 - val_loss: 0.6718 - val_acc: 0.7660\n",
            "Epoch 55/300\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.67182\n",
            "240/240 - 0s - loss: 0.4675 - acc: 0.8875 - val_loss: 0.7052 - val_acc: 0.7234\n",
            "Epoch 56/300\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.67182\n",
            "240/240 - 0s - loss: 0.4508 - acc: 0.9083 - val_loss: 0.7306 - val_acc: 0.6809\n",
            "Epoch 57/300\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.67182\n",
            "240/240 - 0s - loss: 0.4306 - acc: 0.9125 - val_loss: 0.6751 - val_acc: 0.7234\n",
            "Epoch 58/300\n",
            "\n",
            "Epoch 00058: val_loss improved from 0.67182 to 0.66722, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.4344 - acc: 0.9000 - val_loss: 0.6672 - val_acc: 0.6809\n",
            "Epoch 59/300\n",
            "\n",
            "Epoch 00059: val_loss improved from 0.66722 to 0.66376, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.4040 - acc: 0.9083 - val_loss: 0.6638 - val_acc: 0.7660\n",
            "Epoch 60/300\n",
            "\n",
            "Epoch 00060: val_loss improved from 0.66376 to 0.65146, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.4045 - acc: 0.9125 - val_loss: 0.6515 - val_acc: 0.7021\n",
            "Epoch 61/300\n",
            "\n",
            "Epoch 00061: val_loss improved from 0.65146 to 0.65048, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.4130 - acc: 0.9417 - val_loss: 0.6505 - val_acc: 0.7021\n",
            "Epoch 62/300\n",
            "\n",
            "Epoch 00062: val_loss improved from 0.65048 to 0.64228, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.4169 - acc: 0.9083 - val_loss: 0.6423 - val_acc: 0.7660\n",
            "Epoch 63/300\n",
            "\n",
            "Epoch 00063: val_loss improved from 0.64228 to 0.63699, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.4168 - acc: 0.9083 - val_loss: 0.6370 - val_acc: 0.7021\n",
            "Epoch 64/300\n",
            "\n",
            "Epoch 00064: val_loss improved from 0.63699 to 0.62789, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.3840 - acc: 0.9250 - val_loss: 0.6279 - val_acc: 0.6809\n",
            "Epoch 65/300\n",
            "\n",
            "Epoch 00065: val_loss improved from 0.62789 to 0.61990, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.3881 - acc: 0.9083 - val_loss: 0.6199 - val_acc: 0.7872\n",
            "Epoch 66/300\n",
            "\n",
            "Epoch 00066: val_loss improved from 0.61990 to 0.60512, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.4051 - acc: 0.9250 - val_loss: 0.6051 - val_acc: 0.7872\n",
            "Epoch 67/300\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.60512\n",
            "240/240 - 0s - loss: 0.3846 - acc: 0.9375 - val_loss: 0.6178 - val_acc: 0.8085\n",
            "Epoch 68/300\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.60512\n",
            "240/240 - 0s - loss: 0.3969 - acc: 0.9125 - val_loss: 0.6054 - val_acc: 0.7872\n",
            "Epoch 69/300\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.60512\n",
            "240/240 - 0s - loss: 0.3701 - acc: 0.9208 - val_loss: 0.6125 - val_acc: 0.7234\n",
            "Epoch 70/300\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.60512\n",
            "240/240 - 0s - loss: 0.3743 - acc: 0.9167 - val_loss: 0.6056 - val_acc: 0.7872\n",
            "Epoch 71/300\n",
            "\n",
            "Epoch 00071: val_loss improved from 0.60512 to 0.58919, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.3785 - acc: 0.9417 - val_loss: 0.5892 - val_acc: 0.8085\n",
            "Epoch 72/300\n",
            "\n",
            "Epoch 00072: val_loss improved from 0.58919 to 0.58033, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.3614 - acc: 0.9125 - val_loss: 0.5803 - val_acc: 0.8511\n",
            "Epoch 73/300\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.58033\n",
            "240/240 - 0s - loss: 0.3650 - acc: 0.9292 - val_loss: 0.5857 - val_acc: 0.7447\n",
            "Epoch 74/300\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.58033\n",
            "240/240 - 0s - loss: 0.3973 - acc: 0.8875 - val_loss: 0.5855 - val_acc: 0.8085\n",
            "Epoch 75/300\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.58033\n",
            "240/240 - 0s - loss: 0.3543 - acc: 0.9417 - val_loss: 0.6403 - val_acc: 0.7021\n",
            "Epoch 76/300\n",
            "\n",
            "Epoch 00076: val_loss improved from 0.58033 to 0.55970, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.3658 - acc: 0.9375 - val_loss: 0.5597 - val_acc: 0.7872\n",
            "Epoch 77/300\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.55970\n",
            "240/240 - 0s - loss: 0.3450 - acc: 0.9417 - val_loss: 0.6026 - val_acc: 0.7447\n",
            "Epoch 78/300\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.55970\n",
            "240/240 - 0s - loss: 0.3470 - acc: 0.9208 - val_loss: 0.5705 - val_acc: 0.7872\n",
            "Epoch 79/300\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.55970\n",
            "240/240 - 0s - loss: 0.3437 - acc: 0.9458 - val_loss: 0.5634 - val_acc: 0.8298\n",
            "Epoch 80/300\n",
            "\n",
            "Epoch 00080: val_loss improved from 0.55970 to 0.53633, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.3414 - acc: 0.9417 - val_loss: 0.5363 - val_acc: 0.8298\n",
            "Epoch 81/300\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.53633\n",
            "240/240 - 0s - loss: 0.3472 - acc: 0.9333 - val_loss: 0.5439 - val_acc: 0.8298\n",
            "Epoch 82/300\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.53633\n",
            "240/240 - 0s - loss: 0.3678 - acc: 0.9250 - val_loss: 0.5422 - val_acc: 0.8085\n",
            "Epoch 83/300\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.53633\n",
            "240/240 - 0s - loss: 0.3809 - acc: 0.8958 - val_loss: 0.5438 - val_acc: 0.8085\n",
            "Epoch 84/300\n",
            "\n",
            "Epoch 00084: val_loss improved from 0.53633 to 0.52736, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.3629 - acc: 0.9333 - val_loss: 0.5274 - val_acc: 0.8298\n",
            "Epoch 85/300\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.52736\n",
            "240/240 - 0s - loss: 0.3229 - acc: 0.9542 - val_loss: 0.5310 - val_acc: 0.8085\n",
            "Epoch 86/300\n",
            "\n",
            "Epoch 00086: val_loss improved from 0.52736 to 0.51905, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.3394 - acc: 0.9167 - val_loss: 0.5191 - val_acc: 0.8085\n",
            "Epoch 87/300\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.51905\n",
            "240/240 - 0s - loss: 0.3385 - acc: 0.9292 - val_loss: 0.5248 - val_acc: 0.8085\n",
            "Epoch 88/300\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.51905\n",
            "240/240 - 0s - loss: 0.3356 - acc: 0.9458 - val_loss: 0.5618 - val_acc: 0.7447\n",
            "Epoch 89/300\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.51905\n",
            "240/240 - 0s - loss: 0.3332 - acc: 0.9583 - val_loss: 0.6068 - val_acc: 0.7447\n",
            "Epoch 90/300\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.51905\n",
            "240/240 - 0s - loss: 0.3585 - acc: 0.9208 - val_loss: 0.5213 - val_acc: 0.7660\n",
            "Epoch 91/300\n",
            "\n",
            "Epoch 00091: val_loss improved from 0.51905 to 0.50815, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.3229 - acc: 0.9500 - val_loss: 0.5082 - val_acc: 0.8298\n",
            "Epoch 92/300\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.50815\n",
            "240/240 - 0s - loss: 0.3365 - acc: 0.9333 - val_loss: 0.5397 - val_acc: 0.8085\n",
            "Epoch 93/300\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.50815\n",
            "240/240 - 0s - loss: 0.3197 - acc: 0.9417 - val_loss: 0.5167 - val_acc: 0.8085\n",
            "Epoch 94/300\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.50815\n",
            "240/240 - 0s - loss: 0.3064 - acc: 0.9625 - val_loss: 0.5699 - val_acc: 0.7660\n",
            "Epoch 95/300\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.50815\n",
            "240/240 - 0s - loss: 0.3148 - acc: 0.9417 - val_loss: 0.5337 - val_acc: 0.7872\n",
            "Epoch 96/300\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.50815\n",
            "240/240 - 0s - loss: 0.2847 - acc: 0.9500 - val_loss: 0.5167 - val_acc: 0.8298\n",
            "Epoch 97/300\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.50815\n",
            "240/240 - 0s - loss: 0.2915 - acc: 0.9667 - val_loss: 0.5107 - val_acc: 0.8511\n",
            "Epoch 98/300\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.50815\n",
            "240/240 - 0s - loss: 0.3135 - acc: 0.9458 - val_loss: 0.5436 - val_acc: 0.8085\n",
            "Epoch 99/300\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.50815\n",
            "240/240 - 0s - loss: 0.3141 - acc: 0.9417 - val_loss: 0.5211 - val_acc: 0.7872\n",
            "Epoch 100/300\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.50815\n",
            "240/240 - 0s - loss: 0.3051 - acc: 0.9583 - val_loss: 0.5168 - val_acc: 0.7660\n",
            "Epoch 101/300\n",
            "\n",
            "Epoch 00101: val_loss improved from 0.50815 to 0.49755, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.2838 - acc: 0.9458 - val_loss: 0.4976 - val_acc: 0.8298\n",
            "Epoch 102/300\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.49755\n",
            "240/240 - 0s - loss: 0.2802 - acc: 0.9500 - val_loss: 0.5094 - val_acc: 0.7660\n",
            "Epoch 103/300\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.49755\n",
            "240/240 - 0s - loss: 0.2751 - acc: 0.9625 - val_loss: 0.5133 - val_acc: 0.7872\n",
            "Epoch 104/300\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.49755\n",
            "240/240 - 0s - loss: 0.3046 - acc: 0.9458 - val_loss: 0.5026 - val_acc: 0.7447\n",
            "Epoch 105/300\n",
            "\n",
            "Epoch 00105: val_loss improved from 0.49755 to 0.49063, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.3004 - acc: 0.9667 - val_loss: 0.4906 - val_acc: 0.7872\n",
            "Epoch 106/300\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 0.49063\n",
            "240/240 - 0s - loss: 0.2927 - acc: 0.9542 - val_loss: 0.4935 - val_acc: 0.8298\n",
            "Epoch 107/300\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.49063\n",
            "240/240 - 0s - loss: 0.2677 - acc: 0.9667 - val_loss: 0.4917 - val_acc: 0.8298\n",
            "Epoch 108/300\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 0.49063\n",
            "240/240 - 0s - loss: 0.2810 - acc: 0.9542 - val_loss: 0.5316 - val_acc: 0.8085\n",
            "Epoch 109/300\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.49063\n",
            "240/240 - 0s - loss: 0.2850 - acc: 0.9417 - val_loss: 0.5263 - val_acc: 0.7872\n",
            "Epoch 110/300\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.49063\n",
            "240/240 - 0s - loss: 0.2590 - acc: 0.9417 - val_loss: 0.5213 - val_acc: 0.7872\n",
            "Epoch 111/300\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 0.49063\n",
            "240/240 - 0s - loss: 0.2891 - acc: 0.9458 - val_loss: 0.5267 - val_acc: 0.7872\n",
            "Epoch 112/300\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 0.49063\n",
            "240/240 - 0s - loss: 0.2695 - acc: 0.9417 - val_loss: 0.5200 - val_acc: 0.8085\n",
            "Epoch 113/300\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.49063\n",
            "240/240 - 0s - loss: 0.2407 - acc: 0.9750 - val_loss: 0.6007 - val_acc: 0.7660\n",
            "Epoch 114/300\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.49063\n",
            "240/240 - 0s - loss: 0.2734 - acc: 0.9542 - val_loss: 0.5243 - val_acc: 0.7660\n",
            "Epoch 115/300\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.49063\n",
            "240/240 - 0s - loss: 0.2805 - acc: 0.9625 - val_loss: 0.5016 - val_acc: 0.8298\n",
            "Epoch 116/300\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.49063\n",
            "240/240 - 0s - loss: 0.2740 - acc: 0.9625 - val_loss: 0.5610 - val_acc: 0.7872\n",
            "Epoch 117/300\n",
            "\n",
            "Epoch 00117: val_loss improved from 0.49063 to 0.48674, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.2546 - acc: 0.9583 - val_loss: 0.4867 - val_acc: 0.8298\n",
            "Epoch 118/300\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.48674\n",
            "240/240 - 0s - loss: 0.2570 - acc: 0.9583 - val_loss: 0.5056 - val_acc: 0.8298\n",
            "Epoch 119/300\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 0.48674\n",
            "240/240 - 0s - loss: 0.2317 - acc: 0.9583 - val_loss: 0.5371 - val_acc: 0.8085\n",
            "Epoch 120/300\n",
            "\n",
            "Epoch 00120: val_loss improved from 0.48674 to 0.48150, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.2501 - acc: 0.9708 - val_loss: 0.4815 - val_acc: 0.8511\n",
            "Epoch 121/300\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.48150\n",
            "240/240 - 0s - loss: 0.2424 - acc: 0.9708 - val_loss: 0.5500 - val_acc: 0.8298\n",
            "Epoch 122/300\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.48150\n",
            "240/240 - 0s - loss: 0.2677 - acc: 0.9542 - val_loss: 0.5019 - val_acc: 0.8085\n",
            "Epoch 123/300\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 0.48150\n",
            "240/240 - 0s - loss: 0.2290 - acc: 0.9542 - val_loss: 0.5705 - val_acc: 0.7660\n",
            "Epoch 124/300\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 0.48150\n",
            "240/240 - 0s - loss: 0.2607 - acc: 0.9750 - val_loss: 0.5299 - val_acc: 0.7872\n",
            "Epoch 125/300\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 0.48150\n",
            "240/240 - 0s - loss: 0.2611 - acc: 0.9542 - val_loss: 0.4930 - val_acc: 0.8085\n",
            "Epoch 126/300\n",
            "\n",
            "Epoch 00126: val_loss improved from 0.48150 to 0.45135, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.2370 - acc: 0.9750 - val_loss: 0.4514 - val_acc: 0.8511\n",
            "Epoch 127/300\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 0.45135\n",
            "240/240 - 0s - loss: 0.2287 - acc: 0.9750 - val_loss: 0.4664 - val_acc: 0.8298\n",
            "Epoch 128/300\n",
            "\n",
            "Epoch 00128: val_loss improved from 0.45135 to 0.45080, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.2335 - acc: 0.9625 - val_loss: 0.4508 - val_acc: 0.8298\n",
            "Epoch 129/300\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.45080\n",
            "240/240 - 0s - loss: 0.2582 - acc: 0.9583 - val_loss: 0.4828 - val_acc: 0.8085\n",
            "Epoch 130/300\n",
            "\n",
            "Epoch 00130: val_loss improved from 0.45080 to 0.44200, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.2143 - acc: 0.9833 - val_loss: 0.4420 - val_acc: 0.8723\n",
            "Epoch 131/300\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 0.44200\n",
            "240/240 - 0s - loss: 0.2715 - acc: 0.9458 - val_loss: 0.4569 - val_acc: 0.8511\n",
            "Epoch 132/300\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 0.44200\n",
            "240/240 - 0s - loss: 0.2485 - acc: 0.9542 - val_loss: 0.6065 - val_acc: 0.7447\n",
            "Epoch 133/300\n",
            "\n",
            "Epoch 00133: val_loss improved from 0.44200 to 0.43912, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.2422 - acc: 0.9583 - val_loss: 0.4391 - val_acc: 0.8723\n",
            "Epoch 134/300\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.43912\n",
            "240/240 - 0s - loss: 0.2268 - acc: 0.9583 - val_loss: 0.4815 - val_acc: 0.8085\n",
            "Epoch 135/300\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 0.43912\n",
            "240/240 - 0s - loss: 0.2204 - acc: 0.9750 - val_loss: 0.5330 - val_acc: 0.6809\n",
            "Epoch 136/300\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.43912\n",
            "240/240 - 0s - loss: 0.2127 - acc: 0.9708 - val_loss: 0.5056 - val_acc: 0.7660\n",
            "Epoch 137/300\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 0.43912\n",
            "240/240 - 0s - loss: 0.2191 - acc: 0.9583 - val_loss: 0.4630 - val_acc: 0.8511\n",
            "Epoch 138/300\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.43912\n",
            "240/240 - 0s - loss: 0.2245 - acc: 0.9667 - val_loss: 0.4872 - val_acc: 0.8085\n",
            "Epoch 139/300\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.43912\n",
            "240/240 - 0s - loss: 0.2222 - acc: 0.9750 - val_loss: 0.5032 - val_acc: 0.8298\n",
            "Epoch 140/300\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 0.43912\n",
            "240/240 - 0s - loss: 0.1870 - acc: 0.9792 - val_loss: 0.4491 - val_acc: 0.8723\n",
            "Epoch 141/300\n",
            "\n",
            "Epoch 00141: val_loss improved from 0.43912 to 0.40537, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.2244 - acc: 0.9708 - val_loss: 0.4054 - val_acc: 0.8723\n",
            "Epoch 142/300\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.40537\n",
            "240/240 - 0s - loss: 0.2173 - acc: 0.9625 - val_loss: 0.5027 - val_acc: 0.7660\n",
            "Epoch 143/300\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 0.40537\n",
            "240/240 - 0s - loss: 0.1953 - acc: 0.9792 - val_loss: 0.4181 - val_acc: 0.8085\n",
            "Epoch 144/300\n",
            "\n",
            "Epoch 00144: val_loss improved from 0.40537 to 0.39567, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.1994 - acc: 0.9750 - val_loss: 0.3957 - val_acc: 0.8511\n",
            "Epoch 145/300\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.39567\n",
            "240/240 - 0s - loss: 0.2113 - acc: 0.9750 - val_loss: 0.4749 - val_acc: 0.7660\n",
            "Epoch 146/300\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.39567\n",
            "240/240 - 0s - loss: 0.2079 - acc: 0.9583 - val_loss: 0.4284 - val_acc: 0.8085\n",
            "Epoch 147/300\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.39567\n",
            "240/240 - 0s - loss: 0.2192 - acc: 0.9750 - val_loss: 0.4955 - val_acc: 0.7447\n",
            "Epoch 148/300\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 0.39567\n",
            "240/240 - 0s - loss: 0.1891 - acc: 0.9750 - val_loss: 0.4266 - val_acc: 0.8511\n",
            "Epoch 149/300\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.39567\n",
            "240/240 - 0s - loss: 0.2403 - acc: 0.9542 - val_loss: 0.4104 - val_acc: 0.8723\n",
            "Epoch 150/300\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.39567\n",
            "240/240 - 0s - loss: 0.2156 - acc: 0.9625 - val_loss: 0.4092 - val_acc: 0.8298\n",
            "Epoch 151/300\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.39567\n",
            "240/240 - 0s - loss: 0.2147 - acc: 0.9625 - val_loss: 0.3990 - val_acc: 0.8723\n",
            "Epoch 152/300\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.39567\n",
            "240/240 - 0s - loss: 0.2292 - acc: 0.9542 - val_loss: 0.4308 - val_acc: 0.7660\n",
            "Epoch 153/300\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 0.39567\n",
            "240/240 - 0s - loss: 0.1882 - acc: 0.9833 - val_loss: 0.4609 - val_acc: 0.7872\n",
            "Epoch 154/300\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.39567\n",
            "240/240 - 0s - loss: 0.2063 - acc: 0.9625 - val_loss: 0.4034 - val_acc: 0.8298\n",
            "Epoch 155/300\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.39567\n",
            "240/240 - 0s - loss: 0.1947 - acc: 0.9667 - val_loss: 0.5405 - val_acc: 0.7447\n",
            "Epoch 156/300\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.39567\n",
            "240/240 - 0s - loss: 0.1964 - acc: 0.9708 - val_loss: 0.4204 - val_acc: 0.8298\n",
            "Epoch 157/300\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.39567\n",
            "240/240 - 0s - loss: 0.2017 - acc: 0.9708 - val_loss: 0.5224 - val_acc: 0.7234\n",
            "Epoch 158/300\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.39567\n",
            "240/240 - 0s - loss: 0.1654 - acc: 0.9875 - val_loss: 0.4470 - val_acc: 0.8085\n",
            "Epoch 159/300\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.39567\n",
            "240/240 - 0s - loss: 0.2070 - acc: 0.9708 - val_loss: 0.3993 - val_acc: 0.8723\n",
            "Epoch 160/300\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.39567\n",
            "240/240 - 0s - loss: 0.1932 - acc: 0.9792 - val_loss: 0.4154 - val_acc: 0.8511\n",
            "Epoch 161/300\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.39567\n",
            "240/240 - 0s - loss: 0.2156 - acc: 0.9667 - val_loss: 0.6770 - val_acc: 0.7447\n",
            "Epoch 162/300\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.39567\n",
            "240/240 - 0s - loss: 0.2067 - acc: 0.9750 - val_loss: 0.4230 - val_acc: 0.8085\n",
            "Epoch 163/300\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.39567\n",
            "240/240 - 0s - loss: 0.2108 - acc: 0.9583 - val_loss: 0.4534 - val_acc: 0.7872\n",
            "Epoch 164/300\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.39567\n",
            "240/240 - 0s - loss: 0.1867 - acc: 0.9708 - val_loss: 0.4210 - val_acc: 0.8298\n",
            "Epoch 165/300\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.39567\n",
            "240/240 - 0s - loss: 0.2148 - acc: 0.9667 - val_loss: 0.4242 - val_acc: 0.8511\n",
            "Epoch 166/300\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 0.39567\n",
            "240/240 - 0s - loss: 0.1925 - acc: 0.9625 - val_loss: 0.4185 - val_acc: 0.8085\n",
            "Epoch 167/300\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.39567\n",
            "240/240 - 0s - loss: 0.1804 - acc: 0.9750 - val_loss: 0.4044 - val_acc: 0.8298\n",
            "Epoch 168/300\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.39567\n",
            "240/240 - 0s - loss: 0.1770 - acc: 0.9750 - val_loss: 0.4538 - val_acc: 0.7872\n",
            "Epoch 169/300\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 0.39567\n",
            "240/240 - 0s - loss: 0.1506 - acc: 0.9792 - val_loss: 0.4474 - val_acc: 0.7660\n",
            "Epoch 170/300\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.39567\n",
            "240/240 - 0s - loss: 0.1797 - acc: 0.9625 - val_loss: 0.4151 - val_acc: 0.8723\n",
            "Epoch 171/300\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.39567\n",
            "240/240 - 0s - loss: 0.1813 - acc: 0.9875 - val_loss: 0.3985 - val_acc: 0.8511\n",
            "Epoch 172/300\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.39567\n",
            "240/240 - 0s - loss: 0.1747 - acc: 0.9833 - val_loss: 0.4547 - val_acc: 0.8298\n",
            "Epoch 173/300\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.39567\n",
            "240/240 - 0s - loss: 0.1918 - acc: 0.9750 - val_loss: 0.4939 - val_acc: 0.7872\n",
            "Epoch 174/300\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.39567\n",
            "240/240 - 0s - loss: 0.2031 - acc: 0.9417 - val_loss: 0.4008 - val_acc: 0.9149\n",
            "Epoch 175/300\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.39567\n",
            "240/240 - 0s - loss: 0.1889 - acc: 0.9625 - val_loss: 0.4283 - val_acc: 0.8085\n",
            "Epoch 176/300\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 0.39567\n",
            "240/240 - 0s - loss: 0.1870 - acc: 0.9667 - val_loss: 0.4250 - val_acc: 0.8936\n",
            "Epoch 177/300\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.39567\n",
            "240/240 - 0s - loss: 0.1758 - acc: 0.9792 - val_loss: 0.4610 - val_acc: 0.8085\n",
            "Epoch 178/300\n",
            "\n",
            "Epoch 00178: val_loss improved from 0.39567 to 0.36885, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.1652 - acc: 0.9917 - val_loss: 0.3688 - val_acc: 0.8936\n",
            "Epoch 179/300\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.36885\n",
            "240/240 - 0s - loss: 0.1691 - acc: 0.9792 - val_loss: 0.4472 - val_acc: 0.8085\n",
            "Epoch 180/300\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.36885\n",
            "240/240 - 0s - loss: 0.1598 - acc: 0.9625 - val_loss: 0.4073 - val_acc: 0.8298\n",
            "Epoch 181/300\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.36885\n",
            "240/240 - 0s - loss: 0.1628 - acc: 0.9792 - val_loss: 0.4560 - val_acc: 0.8723\n",
            "Epoch 182/300\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.36885\n",
            "240/240 - 0s - loss: 0.1721 - acc: 0.9750 - val_loss: 0.4150 - val_acc: 0.8085\n",
            "Epoch 183/300\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.36885\n",
            "240/240 - 0s - loss: 0.1631 - acc: 0.9708 - val_loss: 0.4491 - val_acc: 0.8085\n",
            "Epoch 184/300\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 0.36885\n",
            "240/240 - 0s - loss: 0.1691 - acc: 0.9792 - val_loss: 0.3944 - val_acc: 0.8085\n",
            "Epoch 185/300\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.36885\n",
            "240/240 - 0s - loss: 0.1700 - acc: 0.9625 - val_loss: 0.4335 - val_acc: 0.8723\n",
            "Epoch 186/300\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.36885\n",
            "240/240 - 0s - loss: 0.1756 - acc: 0.9542 - val_loss: 0.4386 - val_acc: 0.7660\n",
            "Epoch 187/300\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 0.36885\n",
            "240/240 - 0s - loss: 0.1613 - acc: 0.9750 - val_loss: 0.3950 - val_acc: 0.8511\n",
            "Epoch 188/300\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.36885\n",
            "240/240 - 0s - loss: 0.1897 - acc: 0.9750 - val_loss: 0.4199 - val_acc: 0.8085\n",
            "Epoch 189/300\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.36885\n",
            "240/240 - 0s - loss: 0.1608 - acc: 0.9792 - val_loss: 0.3963 - val_acc: 0.8723\n",
            "Epoch 190/300\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.36885\n",
            "240/240 - 0s - loss: 0.2125 - acc: 0.9542 - val_loss: 0.4875 - val_acc: 0.7660\n",
            "Epoch 191/300\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.36885\n",
            "240/240 - 0s - loss: 0.2001 - acc: 0.9542 - val_loss: 0.5772 - val_acc: 0.7021\n",
            "Epoch 192/300\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 0.36885\n",
            "240/240 - 0s - loss: 0.1639 - acc: 0.9708 - val_loss: 0.3978 - val_acc: 0.8298\n",
            "Epoch 193/300\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.36885\n",
            "240/240 - 0s - loss: 0.1834 - acc: 0.9708 - val_loss: 0.4145 - val_acc: 0.8936\n",
            "Epoch 194/300\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.36885\n",
            "240/240 - 0s - loss: 0.1619 - acc: 0.9667 - val_loss: 0.4016 - val_acc: 0.8298\n",
            "Epoch 195/300\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.36885\n",
            "240/240 - 0s - loss: 0.1638 - acc: 0.9792 - val_loss: 0.4050 - val_acc: 0.8936\n",
            "Epoch 196/300\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.36885\n",
            "240/240 - 0s - loss: 0.1589 - acc: 0.9667 - val_loss: 0.4150 - val_acc: 0.8085\n",
            "Epoch 197/300\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.36885\n",
            "240/240 - 0s - loss: 0.1504 - acc: 0.9875 - val_loss: 0.3903 - val_acc: 0.8511\n",
            "Epoch 198/300\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.36885\n",
            "240/240 - 0s - loss: 0.1668 - acc: 0.9667 - val_loss: 0.3882 - val_acc: 0.8085\n",
            "Epoch 199/300\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.36885\n",
            "240/240 - 0s - loss: 0.1703 - acc: 0.9625 - val_loss: 0.4099 - val_acc: 0.8298\n",
            "Epoch 200/300\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.36885\n",
            "240/240 - 0s - loss: 0.1473 - acc: 0.9750 - val_loss: 0.4565 - val_acc: 0.8298\n",
            "Epoch 201/300\n",
            "\n",
            "Epoch 00201: val_loss improved from 0.36885 to 0.35471, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.1543 - acc: 0.9833 - val_loss: 0.3547 - val_acc: 0.8298\n",
            "Epoch 202/300\n",
            "\n",
            "Epoch 00202: val_loss did not improve from 0.35471\n",
            "240/240 - 0s - loss: 0.1416 - acc: 0.9875 - val_loss: 0.3716 - val_acc: 0.8723\n",
            "Epoch 203/300\n",
            "\n",
            "Epoch 00203: val_loss did not improve from 0.35471\n",
            "240/240 - 0s - loss: 0.1490 - acc: 0.9917 - val_loss: 0.3762 - val_acc: 0.8511\n",
            "Epoch 204/300\n",
            "\n",
            "Epoch 00204: val_loss did not improve from 0.35471\n",
            "240/240 - 0s - loss: 0.1255 - acc: 0.9750 - val_loss: 0.3778 - val_acc: 0.8936\n",
            "Epoch 205/300\n",
            "\n",
            "Epoch 00205: val_loss did not improve from 0.35471\n",
            "240/240 - 0s - loss: 0.1335 - acc: 0.9750 - val_loss: 0.4457 - val_acc: 0.7234\n",
            "Epoch 206/300\n",
            "\n",
            "Epoch 00206: val_loss did not improve from 0.35471\n",
            "240/240 - 0s - loss: 0.1378 - acc: 0.9833 - val_loss: 0.4000 - val_acc: 0.8723\n",
            "Epoch 207/300\n",
            "\n",
            "Epoch 00207: val_loss did not improve from 0.35471\n",
            "240/240 - 0s - loss: 0.1371 - acc: 0.9875 - val_loss: 0.3841 - val_acc: 0.8723\n",
            "Epoch 208/300\n",
            "\n",
            "Epoch 00208: val_loss did not improve from 0.35471\n",
            "240/240 - 0s - loss: 0.1290 - acc: 0.9917 - val_loss: 0.4207 - val_acc: 0.8085\n",
            "Epoch 209/300\n",
            "\n",
            "Epoch 00209: val_loss did not improve from 0.35471\n",
            "240/240 - 0s - loss: 0.1382 - acc: 0.9833 - val_loss: 0.3792 - val_acc: 0.8298\n",
            "Epoch 210/300\n",
            "\n",
            "Epoch 00210: val_loss did not improve from 0.35471\n",
            "240/240 - 0s - loss: 0.1394 - acc: 0.9917 - val_loss: 0.4269 - val_acc: 0.8723\n",
            "Epoch 211/300\n",
            "\n",
            "Epoch 00211: val_loss did not improve from 0.35471\n",
            "240/240 - 0s - loss: 0.1240 - acc: 0.9917 - val_loss: 0.3776 - val_acc: 0.8723\n",
            "Epoch 212/300\n",
            "\n",
            "Epoch 00212: val_loss improved from 0.35471 to 0.35387, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.1372 - acc: 0.9917 - val_loss: 0.3539 - val_acc: 0.8723\n",
            "Epoch 213/300\n",
            "\n",
            "Epoch 00213: val_loss did not improve from 0.35387\n",
            "240/240 - 0s - loss: 0.1523 - acc: 0.9792 - val_loss: 0.4610 - val_acc: 0.7234\n",
            "Epoch 214/300\n",
            "\n",
            "Epoch 00214: val_loss did not improve from 0.35387\n",
            "240/240 - 0s - loss: 0.1651 - acc: 0.9708 - val_loss: 0.3585 - val_acc: 0.8936\n",
            "Epoch 215/300\n",
            "\n",
            "Epoch 00215: val_loss did not improve from 0.35387\n",
            "240/240 - 0s - loss: 0.1551 - acc: 0.9750 - val_loss: 0.3710 - val_acc: 0.8723\n",
            "Epoch 216/300\n",
            "\n",
            "Epoch 00216: val_loss did not improve from 0.35387\n",
            "240/240 - 0s - loss: 0.1450 - acc: 0.9875 - val_loss: 0.4069 - val_acc: 0.8298\n",
            "Epoch 217/300\n",
            "\n",
            "Epoch 00217: val_loss did not improve from 0.35387\n",
            "240/240 - 0s - loss: 0.1676 - acc: 0.9667 - val_loss: 0.4376 - val_acc: 0.8085\n",
            "Epoch 218/300\n",
            "\n",
            "Epoch 00218: val_loss did not improve from 0.35387\n",
            "240/240 - 0s - loss: 0.1447 - acc: 0.9792 - val_loss: 0.3829 - val_acc: 0.8936\n",
            "Epoch 219/300\n",
            "\n",
            "Epoch 00219: val_loss did not improve from 0.35387\n",
            "240/240 - 0s - loss: 0.1597 - acc: 0.9833 - val_loss: 0.4337 - val_acc: 0.8511\n",
            "Epoch 220/300\n",
            "\n",
            "Epoch 00220: val_loss did not improve from 0.35387\n",
            "240/240 - 0s - loss: 0.1387 - acc: 0.9917 - val_loss: 0.3597 - val_acc: 0.8723\n",
            "Epoch 221/300\n",
            "\n",
            "Epoch 00221: val_loss did not improve from 0.35387\n",
            "240/240 - 0s - loss: 0.1132 - acc: 0.9917 - val_loss: 0.3794 - val_acc: 0.8298\n",
            "Epoch 222/300\n",
            "\n",
            "Epoch 00222: val_loss did not improve from 0.35387\n",
            "240/240 - 0s - loss: 0.1266 - acc: 0.9917 - val_loss: 0.4221 - val_acc: 0.8723\n",
            "Epoch 223/300\n",
            "\n",
            "Epoch 00223: val_loss did not improve from 0.35387\n",
            "240/240 - 0s - loss: 0.1479 - acc: 0.9792 - val_loss: 0.3657 - val_acc: 0.8723\n",
            "Epoch 224/300\n",
            "\n",
            "Epoch 00224: val_loss did not improve from 0.35387\n",
            "240/240 - 0s - loss: 0.1397 - acc: 0.9708 - val_loss: 0.4785 - val_acc: 0.7234\n",
            "Epoch 225/300\n",
            "\n",
            "Epoch 00225: val_loss did not improve from 0.35387\n",
            "240/240 - 0s - loss: 0.1437 - acc: 0.9875 - val_loss: 0.3864 - val_acc: 0.8298\n",
            "Epoch 226/300\n",
            "\n",
            "Epoch 00226: val_loss did not improve from 0.35387\n",
            "240/240 - 0s - loss: 0.1560 - acc: 0.9750 - val_loss: 0.3686 - val_acc: 0.8723\n",
            "Epoch 227/300\n",
            "\n",
            "Epoch 00227: val_loss improved from 0.35387 to 0.34805, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.1339 - acc: 0.9875 - val_loss: 0.3481 - val_acc: 0.8511\n",
            "Epoch 228/300\n",
            "\n",
            "Epoch 00228: val_loss did not improve from 0.34805\n",
            "240/240 - 0s - loss: 0.1462 - acc: 0.9792 - val_loss: 0.3628 - val_acc: 0.8511\n",
            "Epoch 229/300\n",
            "\n",
            "Epoch 00229: val_loss did not improve from 0.34805\n",
            "240/240 - 0s - loss: 0.1481 - acc: 0.9542 - val_loss: 0.3676 - val_acc: 0.8511\n",
            "Epoch 230/300\n",
            "\n",
            "Epoch 00230: val_loss did not improve from 0.34805\n",
            "240/240 - 0s - loss: 0.1245 - acc: 0.9875 - val_loss: 0.4698 - val_acc: 0.7447\n",
            "Epoch 231/300\n",
            "\n",
            "Epoch 00231: val_loss did not improve from 0.34805\n",
            "240/240 - 0s - loss: 0.1167 - acc: 0.9833 - val_loss: 0.3649 - val_acc: 0.8723\n",
            "Epoch 232/300\n",
            "\n",
            "Epoch 00232: val_loss did not improve from 0.34805\n",
            "240/240 - 0s - loss: 0.1235 - acc: 0.9875 - val_loss: 0.4289 - val_acc: 0.7660\n",
            "Epoch 233/300\n",
            "\n",
            "Epoch 00233: val_loss did not improve from 0.34805\n",
            "240/240 - 0s - loss: 0.1251 - acc: 0.9917 - val_loss: 0.3722 - val_acc: 0.8511\n",
            "Epoch 234/300\n",
            "\n",
            "Epoch 00234: val_loss improved from 0.34805 to 0.34282, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.1201 - acc: 0.9875 - val_loss: 0.3428 - val_acc: 0.8723\n",
            "Epoch 235/300\n",
            "\n",
            "Epoch 00235: val_loss did not improve from 0.34282\n",
            "240/240 - 0s - loss: 0.1229 - acc: 0.9875 - val_loss: 0.4644 - val_acc: 0.7447\n",
            "Epoch 236/300\n",
            "\n",
            "Epoch 00236: val_loss did not improve from 0.34282\n",
            "240/240 - 0s - loss: 0.1194 - acc: 0.9875 - val_loss: 0.3889 - val_acc: 0.8511\n",
            "Epoch 237/300\n",
            "\n",
            "Epoch 00237: val_loss did not improve from 0.34282\n",
            "240/240 - 0s - loss: 0.1086 - acc: 0.9958 - val_loss: 0.4017 - val_acc: 0.8511\n",
            "Epoch 238/300\n",
            "\n",
            "Epoch 00238: val_loss did not improve from 0.34282\n",
            "240/240 - 0s - loss: 0.1193 - acc: 0.9792 - val_loss: 0.3485 - val_acc: 0.9149\n",
            "Epoch 239/300\n",
            "\n",
            "Epoch 00239: val_loss improved from 0.34282 to 0.33474, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.1199 - acc: 0.9792 - val_loss: 0.3347 - val_acc: 0.8936\n",
            "Epoch 240/300\n",
            "\n",
            "Epoch 00240: val_loss improved from 0.33474 to 0.33147, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.1141 - acc: 0.9792 - val_loss: 0.3315 - val_acc: 0.9149\n",
            "Epoch 241/300\n",
            "\n",
            "Epoch 00241: val_loss improved from 0.33147 to 0.31517, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.1159 - acc: 0.9833 - val_loss: 0.3152 - val_acc: 0.9149\n",
            "Epoch 242/300\n",
            "\n",
            "Epoch 00242: val_loss did not improve from 0.31517\n",
            "240/240 - 0s - loss: 0.1063 - acc: 0.9917 - val_loss: 0.3964 - val_acc: 0.7660\n",
            "Epoch 243/300\n",
            "\n",
            "Epoch 00243: val_loss did not improve from 0.31517\n",
            "240/240 - 0s - loss: 0.1285 - acc: 0.9750 - val_loss: 0.3851 - val_acc: 0.8511\n",
            "Epoch 244/300\n",
            "\n",
            "Epoch 00244: val_loss did not improve from 0.31517\n",
            "240/240 - 0s - loss: 0.1123 - acc: 0.9917 - val_loss: 0.3628 - val_acc: 0.8511\n",
            "Epoch 245/300\n",
            "\n",
            "Epoch 00245: val_loss did not improve from 0.31517\n",
            "240/240 - 0s - loss: 0.1204 - acc: 0.9875 - val_loss: 0.3208 - val_acc: 0.8936\n",
            "Epoch 246/300\n",
            "\n",
            "Epoch 00246: val_loss did not improve from 0.31517\n",
            "240/240 - 0s - loss: 0.0943 - acc: 0.9958 - val_loss: 0.3185 - val_acc: 0.9149\n",
            "Epoch 247/300\n",
            "\n",
            "Epoch 00247: val_loss did not improve from 0.31517\n",
            "240/240 - 0s - loss: 0.0964 - acc: 0.9958 - val_loss: 0.3724 - val_acc: 0.8936\n",
            "Epoch 248/300\n",
            "\n",
            "Epoch 00248: val_loss did not improve from 0.31517\n",
            "240/240 - 0s - loss: 0.1229 - acc: 0.9875 - val_loss: 0.3280 - val_acc: 0.9149\n",
            "Epoch 249/300\n",
            "\n",
            "Epoch 00249: val_loss did not improve from 0.31517\n",
            "240/240 - 0s - loss: 0.1263 - acc: 0.9833 - val_loss: 0.3855 - val_acc: 0.8723\n",
            "Epoch 250/300\n",
            "\n",
            "Epoch 00250: val_loss did not improve from 0.31517\n",
            "240/240 - 0s - loss: 0.1262 - acc: 0.9750 - val_loss: 0.3661 - val_acc: 0.8298\n",
            "Epoch 251/300\n",
            "\n",
            "Epoch 00251: val_loss did not improve from 0.31517\n",
            "240/240 - 0s - loss: 0.1030 - acc: 0.9875 - val_loss: 0.3645 - val_acc: 0.8085\n",
            "Epoch 252/300\n",
            "\n",
            "Epoch 00252: val_loss did not improve from 0.31517\n",
            "240/240 - 0s - loss: 0.1356 - acc: 0.9792 - val_loss: 0.3322 - val_acc: 0.8936\n",
            "Epoch 253/300\n",
            "\n",
            "Epoch 00253: val_loss did not improve from 0.31517\n",
            "240/240 - 0s - loss: 0.1026 - acc: 0.9917 - val_loss: 0.3926 - val_acc: 0.8085\n",
            "Epoch 254/300\n",
            "\n",
            "Epoch 00254: val_loss did not improve from 0.31517\n",
            "240/240 - 0s - loss: 0.1280 - acc: 0.9792 - val_loss: 0.3778 - val_acc: 0.8511\n",
            "Epoch 255/300\n",
            "\n",
            "Epoch 00255: val_loss did not improve from 0.31517\n",
            "240/240 - 0s - loss: 0.1317 - acc: 0.9625 - val_loss: 0.3227 - val_acc: 0.8936\n",
            "Epoch 256/300\n",
            "\n",
            "Epoch 00256: val_loss did not improve from 0.31517\n",
            "240/240 - 0s - loss: 0.1112 - acc: 0.9875 - val_loss: 0.4525 - val_acc: 0.7872\n",
            "Epoch 257/300\n",
            "\n",
            "Epoch 00257: val_loss did not improve from 0.31517\n",
            "240/240 - 0s - loss: 0.1235 - acc: 0.9875 - val_loss: 0.3202 - val_acc: 0.9362\n",
            "Epoch 258/300\n",
            "\n",
            "Epoch 00258: val_loss did not improve from 0.31517\n",
            "240/240 - 0s - loss: 0.1188 - acc: 0.9917 - val_loss: 0.3505 - val_acc: 0.8511\n",
            "Epoch 259/300\n",
            "\n",
            "Epoch 00259: val_loss did not improve from 0.31517\n",
            "240/240 - 0s - loss: 0.1264 - acc: 0.9833 - val_loss: 0.3752 - val_acc: 0.7872\n",
            "Epoch 260/300\n",
            "\n",
            "Epoch 00260: val_loss did not improve from 0.31517\n",
            "240/240 - 0s - loss: 0.1254 - acc: 0.9875 - val_loss: 0.3481 - val_acc: 0.8723\n",
            "Epoch 261/300\n",
            "\n",
            "Epoch 00261: val_loss did not improve from 0.31517\n",
            "240/240 - 0s - loss: 0.1183 - acc: 0.9708 - val_loss: 0.3222 - val_acc: 0.9149\n",
            "Epoch 262/300\n",
            "\n",
            "Epoch 00262: val_loss did not improve from 0.31517\n",
            "240/240 - 0s - loss: 0.1142 - acc: 0.9875 - val_loss: 0.3509 - val_acc: 0.8936\n",
            "Epoch 263/300\n",
            "\n",
            "Epoch 00263: val_loss did not improve from 0.31517\n",
            "240/240 - 0s - loss: 0.1109 - acc: 0.9875 - val_loss: 0.3360 - val_acc: 0.8723\n",
            "Epoch 264/300\n",
            "\n",
            "Epoch 00264: val_loss did not improve from 0.31517\n",
            "240/240 - 0s - loss: 0.1239 - acc: 0.9833 - val_loss: 0.3396 - val_acc: 0.8936\n",
            "Epoch 265/300\n",
            "\n",
            "Epoch 00265: val_loss did not improve from 0.31517\n",
            "240/240 - 0s - loss: 0.1166 - acc: 0.9875 - val_loss: 0.4022 - val_acc: 0.8298\n",
            "Epoch 266/300\n",
            "\n",
            "Epoch 00266: val_loss did not improve from 0.31517\n",
            "240/240 - 0s - loss: 0.1388 - acc: 0.9833 - val_loss: 0.3256 - val_acc: 0.9362\n",
            "Epoch 267/300\n",
            "\n",
            "Epoch 00267: val_loss did not improve from 0.31517\n",
            "240/240 - 0s - loss: 0.1053 - acc: 0.9833 - val_loss: 0.3380 - val_acc: 0.9149\n",
            "Epoch 268/300\n",
            "\n",
            "Epoch 00268: val_loss did not improve from 0.31517\n",
            "240/240 - 0s - loss: 0.1153 - acc: 0.9833 - val_loss: 0.3889 - val_acc: 0.7872\n",
            "Epoch 269/300\n",
            "\n",
            "Epoch 00269: val_loss did not improve from 0.31517\n",
            "240/240 - 0s - loss: 0.1360 - acc: 0.9792 - val_loss: 0.4043 - val_acc: 0.8511\n",
            "Epoch 270/300\n",
            "\n",
            "Epoch 00270: val_loss did not improve from 0.31517\n",
            "240/240 - 0s - loss: 0.1232 - acc: 0.9833 - val_loss: 0.3488 - val_acc: 0.8723\n",
            "Epoch 271/300\n",
            "\n",
            "Epoch 00271: val_loss did not improve from 0.31517\n",
            "240/240 - 0s - loss: 0.1162 - acc: 0.9750 - val_loss: 0.3957 - val_acc: 0.8085\n",
            "Epoch 272/300\n",
            "\n",
            "Epoch 00272: val_loss did not improve from 0.31517\n",
            "240/240 - 0s - loss: 0.1032 - acc: 0.9792 - val_loss: 0.4449 - val_acc: 0.8298\n",
            "Epoch 273/300\n",
            "\n",
            "Epoch 00273: val_loss did not improve from 0.31517\n",
            "240/240 - 0s - loss: 0.1072 - acc: 0.9833 - val_loss: 0.4089 - val_acc: 0.7872\n",
            "Epoch 274/300\n",
            "\n",
            "Epoch 00274: val_loss did not improve from 0.31517\n",
            "240/240 - 0s - loss: 0.1045 - acc: 0.9917 - val_loss: 0.4141 - val_acc: 0.7872\n",
            "Epoch 275/300\n",
            "\n",
            "Epoch 00275: val_loss did not improve from 0.31517\n",
            "240/240 - 0s - loss: 0.0940 - acc: 0.9833 - val_loss: 0.3622 - val_acc: 0.8723\n",
            "Epoch 276/300\n",
            "\n",
            "Epoch 00276: val_loss did not improve from 0.31517\n",
            "240/240 - 0s - loss: 0.1225 - acc: 0.9833 - val_loss: 0.4978 - val_acc: 0.8298\n",
            "Epoch 277/300\n",
            "\n",
            "Epoch 00277: val_loss did not improve from 0.31517\n",
            "240/240 - 0s - loss: 0.1144 - acc: 0.9958 - val_loss: 0.4665 - val_acc: 0.8298\n",
            "Epoch 278/300\n",
            "\n",
            "Epoch 00278: val_loss did not improve from 0.31517\n",
            "240/240 - 0s - loss: 0.1046 - acc: 0.9875 - val_loss: 0.3513 - val_acc: 0.8723\n",
            "Epoch 279/300\n",
            "\n",
            "Epoch 00279: val_loss did not improve from 0.31517\n",
            "240/240 - 0s - loss: 0.0902 - acc: 0.9917 - val_loss: 0.3598 - val_acc: 0.8723\n",
            "Epoch 280/300\n",
            "\n",
            "Epoch 00280: val_loss did not improve from 0.31517\n",
            "240/240 - 0s - loss: 0.1045 - acc: 0.9875 - val_loss: 0.3305 - val_acc: 0.8723\n",
            "Epoch 281/300\n",
            "\n",
            "Epoch 00281: val_loss improved from 0.31517 to 0.31363, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.0983 - acc: 0.9917 - val_loss: 0.3136 - val_acc: 0.8936\n",
            "Epoch 282/300\n",
            "\n",
            "Epoch 00282: val_loss did not improve from 0.31363\n",
            "240/240 - 0s - loss: 0.1087 - acc: 0.9792 - val_loss: 0.3502 - val_acc: 0.8298\n",
            "Epoch 283/300\n",
            "\n",
            "Epoch 00283: val_loss did not improve from 0.31363\n",
            "240/240 - 0s - loss: 0.1016 - acc: 0.9917 - val_loss: 0.3582 - val_acc: 0.8723\n",
            "Epoch 284/300\n",
            "\n",
            "Epoch 00284: val_loss did not improve from 0.31363\n",
            "240/240 - 0s - loss: 0.1030 - acc: 0.9917 - val_loss: 0.3200 - val_acc: 0.9149\n",
            "Epoch 285/300\n",
            "\n",
            "Epoch 00285: val_loss did not improve from 0.31363\n",
            "240/240 - 0s - loss: 0.0770 - acc: 0.9958 - val_loss: 0.3585 - val_acc: 0.8936\n",
            "Epoch 286/300\n",
            "\n",
            "Epoch 00286: val_loss improved from 0.31363 to 0.29138, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.0876 - acc: 0.9917 - val_loss: 0.2914 - val_acc: 0.9149\n",
            "Epoch 287/300\n",
            "\n",
            "Epoch 00287: val_loss did not improve from 0.29138\n",
            "240/240 - 0s - loss: 0.0943 - acc: 0.9917 - val_loss: 0.2915 - val_acc: 0.9149\n",
            "Epoch 288/300\n",
            "\n",
            "Epoch 00288: val_loss did not improve from 0.29138\n",
            "240/240 - 0s - loss: 0.0845 - acc: 0.9958 - val_loss: 0.3691 - val_acc: 0.8723\n",
            "Epoch 289/300\n",
            "\n",
            "Epoch 00289: val_loss did not improve from 0.29138\n",
            "240/240 - 0s - loss: 0.0928 - acc: 0.9875 - val_loss: 0.3339 - val_acc: 0.8723\n",
            "Epoch 290/300\n",
            "\n",
            "Epoch 00290: val_loss did not improve from 0.29138\n",
            "240/240 - 0s - loss: 0.0883 - acc: 0.9875 - val_loss: 0.3818 - val_acc: 0.8723\n",
            "Epoch 291/300\n",
            "\n",
            "Epoch 00291: val_loss improved from 0.29138 to 0.28010, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.1013 - acc: 0.9833 - val_loss: 0.2801 - val_acc: 0.9149\n",
            "Epoch 292/300\n",
            "\n",
            "Epoch 00292: val_loss did not improve from 0.28010\n",
            "240/240 - 0s - loss: 0.0977 - acc: 0.9875 - val_loss: 0.3540 - val_acc: 0.8723\n",
            "Epoch 293/300\n",
            "\n",
            "Epoch 00293: val_loss did not improve from 0.28010\n",
            "240/240 - 0s - loss: 0.1060 - acc: 0.9875 - val_loss: 0.3656 - val_acc: 0.8936\n",
            "Epoch 294/300\n",
            "\n",
            "Epoch 00294: val_loss did not improve from 0.28010\n",
            "240/240 - 0s - loss: 0.0743 - acc: 0.9958 - val_loss: 0.3393 - val_acc: 0.8723\n",
            "Epoch 295/300\n",
            "\n",
            "Epoch 00295: val_loss did not improve from 0.28010\n",
            "240/240 - 0s - loss: 0.1082 - acc: 0.9875 - val_loss: 0.3720 - val_acc: 0.8936\n",
            "Epoch 296/300\n",
            "\n",
            "Epoch 00296: val_loss did not improve from 0.28010\n",
            "240/240 - 0s - loss: 0.0956 - acc: 0.9833 - val_loss: 0.3940 - val_acc: 0.8298\n",
            "Epoch 297/300\n",
            "\n",
            "Epoch 00297: val_loss did not improve from 0.28010\n",
            "240/240 - 0s - loss: 0.1151 - acc: 0.9833 - val_loss: 0.3337 - val_acc: 0.9149\n",
            "Epoch 298/300\n",
            "\n",
            "Epoch 00298: val_loss did not improve from 0.28010\n",
            "240/240 - 0s - loss: 0.0881 - acc: 1.0000 - val_loss: 0.3682 - val_acc: 0.8511\n",
            "Epoch 299/300\n",
            "\n",
            "Epoch 00299: val_loss did not improve from 0.28010\n",
            "240/240 - 0s - loss: 0.0835 - acc: 0.9958 - val_loss: 0.3972 - val_acc: 0.8298\n",
            "Epoch 300/300\n",
            "\n",
            "Epoch 00300: val_loss did not improve from 0.28010\n",
            "240/240 - 0s - loss: 0.1121 - acc: 0.9875 - val_loss: 0.3000 - val_acc: 0.9149\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n1j1H+wa8qQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "# Evaluate on test data\n",
            "288/288 [==============================] - 1s 2ms/sample - loss: 0.4579 - acc: 0.8576\n",
            "test loss, test acc: [0.4579126239595159, 0.8576389]\n",
            "EEG_Deep/Data2A/Data_A08T.mat\n",
            "(288, 22, 1875)\n",
            "(288, 1)\n",
            "(240, 22, 750)\n",
            "(240, 1)\n",
            "(47, 22, 750)\n",
            "(47, 1)\n",
            "X_train shape: (240, 1, 22, 750)\n",
            "240 train samples\n",
            "47 val samples\n",
            "EEG_Deep/Data2A/Data_A08E.mat\n",
            "(288, 22, 1875)\n",
            "(288, 1)\n",
            "(288, 22, 750)\n",
            "(288, 1)\n",
            "X_train shape: (288, 1, 22, 750)\n",
            "288 train samples\n",
            "Train on 240 samples, validate on 47 samples\n",
            "Epoch 1/300\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 1.38227, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 1s - loss: 1.3860 - acc: 0.2750 - val_loss: 1.3823 - val_acc: 0.2766\n",
            "Epoch 2/300\n",
            "\n",
            "Epoch 00002: val_loss improved from 1.38227 to 1.37805, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.3380 - acc: 0.3833 - val_loss: 1.3780 - val_acc: 0.3617\n",
            "Epoch 3/300\n",
            "\n",
            "Epoch 00003: val_loss improved from 1.37805 to 1.37242, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.2966 - acc: 0.4667 - val_loss: 1.3724 - val_acc: 0.4255\n",
            "Epoch 4/300\n",
            "\n",
            "Epoch 00004: val_loss improved from 1.37242 to 1.36148, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.2433 - acc: 0.5625 - val_loss: 1.3615 - val_acc: 0.4255\n",
            "Epoch 5/300\n",
            "\n",
            "Epoch 00005: val_loss improved from 1.36148 to 1.34324, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.1824 - acc: 0.5750 - val_loss: 1.3432 - val_acc: 0.4468\n",
            "Epoch 6/300\n",
            "\n",
            "Epoch 00006: val_loss improved from 1.34324 to 1.32217, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.1045 - acc: 0.6083 - val_loss: 1.3222 - val_acc: 0.4043\n",
            "Epoch 7/300\n",
            "\n",
            "Epoch 00007: val_loss improved from 1.32217 to 1.31099, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.0608 - acc: 0.6417 - val_loss: 1.3110 - val_acc: 0.3617\n",
            "Epoch 8/300\n",
            "\n",
            "Epoch 00008: val_loss improved from 1.31099 to 1.30693, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.0039 - acc: 0.6542 - val_loss: 1.3069 - val_acc: 0.2979\n",
            "Epoch 9/300\n",
            "\n",
            "Epoch 00009: val_loss improved from 1.30693 to 1.29894, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.9413 - acc: 0.6958 - val_loss: 1.2989 - val_acc: 0.2766\n",
            "Epoch 10/300\n",
            "\n",
            "Epoch 00010: val_loss improved from 1.29894 to 1.28641, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.9045 - acc: 0.7375 - val_loss: 1.2864 - val_acc: 0.3404\n",
            "Epoch 11/300\n",
            "\n",
            "Epoch 00011: val_loss improved from 1.28641 to 1.26383, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.8681 - acc: 0.7208 - val_loss: 1.2638 - val_acc: 0.3617\n",
            "Epoch 12/300\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 1.26383\n",
            "240/240 - 0s - loss: 0.8039 - acc: 0.7833 - val_loss: 1.2765 - val_acc: 0.3617\n",
            "Epoch 13/300\n",
            "\n",
            "Epoch 00013: val_loss improved from 1.26383 to 1.25355, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.8028 - acc: 0.7583 - val_loss: 1.2536 - val_acc: 0.4043\n",
            "Epoch 14/300\n",
            "\n",
            "Epoch 00014: val_loss improved from 1.25355 to 1.19953, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.7563 - acc: 0.7792 - val_loss: 1.1995 - val_acc: 0.4043\n",
            "Epoch 15/300\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 1.19953\n",
            "240/240 - 0s - loss: 0.7186 - acc: 0.8125 - val_loss: 1.2735 - val_acc: 0.3191\n",
            "Epoch 16/300\n",
            "\n",
            "Epoch 00016: val_loss improved from 1.19953 to 1.16553, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.7525 - acc: 0.7667 - val_loss: 1.1655 - val_acc: 0.4681\n",
            "Epoch 17/300\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 1.16553\n",
            "240/240 - 0s - loss: 0.6952 - acc: 0.8125 - val_loss: 1.2350 - val_acc: 0.3617\n",
            "Epoch 18/300\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 1.16553\n",
            "240/240 - 0s - loss: 0.6860 - acc: 0.7875 - val_loss: 1.1806 - val_acc: 0.4468\n",
            "Epoch 19/300\n",
            "\n",
            "Epoch 00019: val_loss improved from 1.16553 to 1.10926, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.6640 - acc: 0.8250 - val_loss: 1.1093 - val_acc: 0.4681\n",
            "Epoch 20/300\n",
            "\n",
            "Epoch 00020: val_loss improved from 1.10926 to 1.09559, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.6819 - acc: 0.8000 - val_loss: 1.0956 - val_acc: 0.5319\n",
            "Epoch 21/300\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 1.09559\n",
            "240/240 - 0s - loss: 0.6799 - acc: 0.7708 - val_loss: 1.1160 - val_acc: 0.4468\n",
            "Epoch 22/300\n",
            "\n",
            "Epoch 00022: val_loss improved from 1.09559 to 1.03763, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.6474 - acc: 0.8208 - val_loss: 1.0376 - val_acc: 0.5106\n",
            "Epoch 23/300\n",
            "\n",
            "Epoch 00023: val_loss improved from 1.03763 to 0.99477, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.6296 - acc: 0.8333 - val_loss: 0.9948 - val_acc: 0.5532\n",
            "Epoch 24/300\n",
            "\n",
            "Epoch 00024: val_loss improved from 0.99477 to 0.92360, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.6222 - acc: 0.8292 - val_loss: 0.9236 - val_acc: 0.5957\n",
            "Epoch 25/300\n",
            "\n",
            "Epoch 00025: val_loss improved from 0.92360 to 0.91524, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.6121 - acc: 0.8333 - val_loss: 0.9152 - val_acc: 0.5319\n",
            "Epoch 26/300\n",
            "\n",
            "Epoch 00026: val_loss improved from 0.91524 to 0.89409, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.5910 - acc: 0.8417 - val_loss: 0.8941 - val_acc: 0.6809\n",
            "Epoch 27/300\n",
            "\n",
            "Epoch 00027: val_loss improved from 0.89409 to 0.85668, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.6172 - acc: 0.7792 - val_loss: 0.8567 - val_acc: 0.5957\n",
            "Epoch 28/300\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.85668\n",
            "240/240 - 0s - loss: 0.5568 - acc: 0.8583 - val_loss: 0.8680 - val_acc: 0.5957\n",
            "Epoch 29/300\n",
            "\n",
            "Epoch 00029: val_loss improved from 0.85668 to 0.79907, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.5884 - acc: 0.8292 - val_loss: 0.7991 - val_acc: 0.6170\n",
            "Epoch 30/300\n",
            "\n",
            "Epoch 00030: val_loss improved from 0.79907 to 0.79601, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.5851 - acc: 0.8333 - val_loss: 0.7960 - val_acc: 0.6809\n",
            "Epoch 31/300\n",
            "\n",
            "Epoch 00031: val_loss improved from 0.79601 to 0.78501, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.5438 - acc: 0.8875 - val_loss: 0.7850 - val_acc: 0.6383\n",
            "Epoch 32/300\n",
            "\n",
            "Epoch 00032: val_loss improved from 0.78501 to 0.78069, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.5619 - acc: 0.8667 - val_loss: 0.7807 - val_acc: 0.7021\n",
            "Epoch 33/300\n",
            "\n",
            "Epoch 00033: val_loss improved from 0.78069 to 0.72984, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.5479 - acc: 0.8625 - val_loss: 0.7298 - val_acc: 0.7021\n",
            "Epoch 34/300\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.72984\n",
            "240/240 - 0s - loss: 0.5507 - acc: 0.8458 - val_loss: 0.7523 - val_acc: 0.6809\n",
            "Epoch 35/300\n",
            "\n",
            "Epoch 00035: val_loss improved from 0.72984 to 0.72837, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.5265 - acc: 0.8583 - val_loss: 0.7284 - val_acc: 0.7660\n",
            "Epoch 36/300\n",
            "\n",
            "Epoch 00036: val_loss improved from 0.72837 to 0.72446, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.4984 - acc: 0.8667 - val_loss: 0.7245 - val_acc: 0.6596\n",
            "Epoch 37/300\n",
            "\n",
            "Epoch 00037: val_loss improved from 0.72446 to 0.68966, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.4894 - acc: 0.8875 - val_loss: 0.6897 - val_acc: 0.6596\n",
            "Epoch 38/300\n",
            "\n",
            "Epoch 00038: val_loss improved from 0.68966 to 0.68840, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.5084 - acc: 0.8792 - val_loss: 0.6884 - val_acc: 0.7021\n",
            "Epoch 39/300\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.68840\n",
            "240/240 - 0s - loss: 0.5322 - acc: 0.8583 - val_loss: 0.7112 - val_acc: 0.6596\n",
            "Epoch 40/300\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.68840\n",
            "240/240 - 0s - loss: 0.4915 - acc: 0.8917 - val_loss: 0.6911 - val_acc: 0.7234\n",
            "Epoch 41/300\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.68840\n",
            "240/240 - 0s - loss: 0.4771 - acc: 0.8792 - val_loss: 0.6906 - val_acc: 0.7021\n",
            "Epoch 42/300\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.68840\n",
            "240/240 - 0s - loss: 0.5046 - acc: 0.8958 - val_loss: 0.7162 - val_acc: 0.7234\n",
            "Epoch 43/300\n",
            "\n",
            "Epoch 00043: val_loss improved from 0.68840 to 0.67263, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.5160 - acc: 0.8417 - val_loss: 0.6726 - val_acc: 0.7660\n",
            "Epoch 44/300\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.67263\n",
            "240/240 - 0s - loss: 0.5100 - acc: 0.8458 - val_loss: 0.6884 - val_acc: 0.7447\n",
            "Epoch 45/300\n",
            "\n",
            "Epoch 00045: val_loss improved from 0.67263 to 0.66042, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.5113 - acc: 0.8500 - val_loss: 0.6604 - val_acc: 0.7872\n",
            "Epoch 46/300\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.66042\n",
            "240/240 - 0s - loss: 0.4679 - acc: 0.8792 - val_loss: 0.6759 - val_acc: 0.7234\n",
            "Epoch 47/300\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.66042\n",
            "240/240 - 0s - loss: 0.4856 - acc: 0.8583 - val_loss: 0.6924 - val_acc: 0.7021\n",
            "Epoch 48/300\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.66042\n",
            "240/240 - 0s - loss: 0.4489 - acc: 0.9167 - val_loss: 0.6713 - val_acc: 0.7234\n",
            "Epoch 49/300\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.66042\n",
            "240/240 - 0s - loss: 0.4490 - acc: 0.9125 - val_loss: 0.6651 - val_acc: 0.7447\n",
            "Epoch 50/300\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.66042\n",
            "240/240 - 0s - loss: 0.4865 - acc: 0.8583 - val_loss: 0.6671 - val_acc: 0.7234\n",
            "Epoch 51/300\n",
            "\n",
            "Epoch 00051: val_loss improved from 0.66042 to 0.64837, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.4447 - acc: 0.9042 - val_loss: 0.6484 - val_acc: 0.7660\n",
            "Epoch 52/300\n",
            "\n",
            "Epoch 00052: val_loss improved from 0.64837 to 0.62478, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.4376 - acc: 0.9125 - val_loss: 0.6248 - val_acc: 0.8298\n",
            "Epoch 53/300\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.62478\n",
            "240/240 - 0s - loss: 0.4363 - acc: 0.9000 - val_loss: 0.6327 - val_acc: 0.7234\n",
            "Epoch 54/300\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.62478\n",
            "240/240 - 0s - loss: 0.4440 - acc: 0.9208 - val_loss: 0.6962 - val_acc: 0.7447\n",
            "Epoch 55/300\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.62478\n",
            "240/240 - 0s - loss: 0.4385 - acc: 0.8667 - val_loss: 0.6611 - val_acc: 0.7447\n",
            "Epoch 56/300\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.62478\n",
            "240/240 - 0s - loss: 0.4394 - acc: 0.8792 - val_loss: 0.6784 - val_acc: 0.7021\n",
            "Epoch 57/300\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.62478\n",
            "240/240 - 0s - loss: 0.4313 - acc: 0.8958 - val_loss: 0.6878 - val_acc: 0.7021\n",
            "Epoch 58/300\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.62478\n",
            "240/240 - 0s - loss: 0.4517 - acc: 0.8958 - val_loss: 0.6746 - val_acc: 0.7447\n",
            "Epoch 59/300\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.62478\n",
            "240/240 - 0s - loss: 0.4421 - acc: 0.9042 - val_loss: 0.6480 - val_acc: 0.7872\n",
            "Epoch 60/300\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.62478\n",
            "240/240 - 0s - loss: 0.4091 - acc: 0.9167 - val_loss: 0.6459 - val_acc: 0.7234\n",
            "Epoch 61/300\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.62478\n",
            "240/240 - 0s - loss: 0.4263 - acc: 0.9125 - val_loss: 0.6706 - val_acc: 0.7021\n",
            "Epoch 62/300\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.62478\n",
            "240/240 - 0s - loss: 0.4211 - acc: 0.9125 - val_loss: 0.6578 - val_acc: 0.7447\n",
            "Epoch 63/300\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.62478\n",
            "240/240 - 0s - loss: 0.4302 - acc: 0.9208 - val_loss: 0.6689 - val_acc: 0.7234\n",
            "Epoch 64/300\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.62478\n",
            "240/240 - 0s - loss: 0.4046 - acc: 0.9417 - val_loss: 0.6950 - val_acc: 0.7021\n",
            "Epoch 65/300\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.62478\n",
            "240/240 - 0s - loss: 0.4038 - acc: 0.9042 - val_loss: 0.6509 - val_acc: 0.7447\n",
            "Epoch 66/300\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.62478\n",
            "240/240 - 0s - loss: 0.3967 - acc: 0.9000 - val_loss: 0.6615 - val_acc: 0.6809\n",
            "Epoch 67/300\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.62478\n",
            "240/240 - 0s - loss: 0.3788 - acc: 0.9333 - val_loss: 0.6870 - val_acc: 0.7234\n",
            "Epoch 68/300\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.62478\n",
            "240/240 - 0s - loss: 0.3832 - acc: 0.9125 - val_loss: 0.6369 - val_acc: 0.7447\n",
            "Epoch 69/300\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.62478\n",
            "240/240 - 0s - loss: 0.3839 - acc: 0.9083 - val_loss: 0.6425 - val_acc: 0.8085\n",
            "Epoch 70/300\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.62478\n",
            "240/240 - 0s - loss: 0.3821 - acc: 0.9042 - val_loss: 0.6645 - val_acc: 0.7021\n",
            "Epoch 71/300\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.62478\n",
            "240/240 - 0s - loss: 0.3753 - acc: 0.9250 - val_loss: 0.6424 - val_acc: 0.7447\n",
            "Epoch 72/300\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.62478\n",
            "240/240 - 0s - loss: 0.4025 - acc: 0.8833 - val_loss: 0.6468 - val_acc: 0.8085\n",
            "Epoch 73/300\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.62478\n",
            "240/240 - 0s - loss: 0.3702 - acc: 0.9042 - val_loss: 0.6631 - val_acc: 0.7234\n",
            "Epoch 74/300\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.62478\n",
            "240/240 - 0s - loss: 0.3786 - acc: 0.9167 - val_loss: 0.6858 - val_acc: 0.7447\n",
            "Epoch 75/300\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.62478\n",
            "240/240 - 0s - loss: 0.3684 - acc: 0.9250 - val_loss: 0.6356 - val_acc: 0.7660\n",
            "Epoch 76/300\n",
            "\n",
            "Epoch 00076: val_loss improved from 0.62478 to 0.62398, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.3571 - acc: 0.9292 - val_loss: 0.6240 - val_acc: 0.7872\n",
            "Epoch 77/300\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.62398\n",
            "240/240 - 0s - loss: 0.3589 - acc: 0.9167 - val_loss: 0.6291 - val_acc: 0.7234\n",
            "Epoch 78/300\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.62398\n",
            "240/240 - 0s - loss: 0.3649 - acc: 0.9250 - val_loss: 0.6273 - val_acc: 0.7660\n",
            "Epoch 79/300\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.62398\n",
            "240/240 - 0s - loss: 0.3575 - acc: 0.9167 - val_loss: 0.6368 - val_acc: 0.7660\n",
            "Epoch 80/300\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.62398\n",
            "240/240 - 0s - loss: 0.3479 - acc: 0.9250 - val_loss: 0.6939 - val_acc: 0.7234\n",
            "Epoch 81/300\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.62398\n",
            "240/240 - 0s - loss: 0.3464 - acc: 0.9292 - val_loss: 0.6481 - val_acc: 0.7021\n",
            "Epoch 82/300\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.62398\n",
            "240/240 - 0s - loss: 0.3530 - acc: 0.9083 - val_loss: 0.6534 - val_acc: 0.7447\n",
            "Epoch 83/300\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.62398\n",
            "240/240 - 0s - loss: 0.3670 - acc: 0.9167 - val_loss: 0.6750 - val_acc: 0.6809\n",
            "Epoch 84/300\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.62398\n",
            "240/240 - 0s - loss: 0.3596 - acc: 0.9417 - val_loss: 0.6477 - val_acc: 0.7660\n",
            "Epoch 85/300\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.62398\n",
            "240/240 - 0s - loss: 0.3582 - acc: 0.9208 - val_loss: 0.6398 - val_acc: 0.7872\n",
            "Epoch 86/300\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.62398\n",
            "240/240 - 0s - loss: 0.3666 - acc: 0.9042 - val_loss: 0.6383 - val_acc: 0.7872\n",
            "Epoch 87/300\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.62398\n",
            "240/240 - 0s - loss: 0.3624 - acc: 0.9208 - val_loss: 0.6487 - val_acc: 0.7447\n",
            "Epoch 88/300\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.62398\n",
            "240/240 - 0s - loss: 0.3594 - acc: 0.9333 - val_loss: 0.6358 - val_acc: 0.7447\n",
            "Epoch 89/300\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.62398\n",
            "240/240 - 0s - loss: 0.3175 - acc: 0.9500 - val_loss: 0.6467 - val_acc: 0.7872\n",
            "Epoch 90/300\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.62398\n",
            "240/240 - 0s - loss: 0.3283 - acc: 0.9375 - val_loss: 0.6279 - val_acc: 0.7872\n",
            "Epoch 91/300\n",
            "\n",
            "Epoch 00091: val_loss improved from 0.62398 to 0.62247, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.3221 - acc: 0.9292 - val_loss: 0.6225 - val_acc: 0.7660\n",
            "Epoch 92/300\n",
            "\n",
            "Epoch 00092: val_loss improved from 0.62247 to 0.62131, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.3569 - acc: 0.9125 - val_loss: 0.6213 - val_acc: 0.7447\n",
            "Epoch 93/300\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.62131\n",
            "240/240 - 0s - loss: 0.3500 - acc: 0.9083 - val_loss: 0.6962 - val_acc: 0.7234\n",
            "Epoch 94/300\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.62131\n",
            "240/240 - 0s - loss: 0.3329 - acc: 0.9333 - val_loss: 0.6226 - val_acc: 0.7872\n",
            "Epoch 95/300\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.62131\n",
            "240/240 - 0s - loss: 0.3199 - acc: 0.9625 - val_loss: 0.6554 - val_acc: 0.7447\n",
            "Epoch 96/300\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.62131\n",
            "240/240 - 0s - loss: 0.3210 - acc: 0.9417 - val_loss: 0.6489 - val_acc: 0.7660\n",
            "Epoch 97/300\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.62131\n",
            "240/240 - 0s - loss: 0.3069 - acc: 0.9417 - val_loss: 0.6629 - val_acc: 0.7234\n",
            "Epoch 98/300\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.62131\n",
            "240/240 - 0s - loss: 0.3274 - acc: 0.9333 - val_loss: 0.6220 - val_acc: 0.7447\n",
            "Epoch 99/300\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.62131\n",
            "240/240 - 0s - loss: 0.3092 - acc: 0.9500 - val_loss: 0.6448 - val_acc: 0.7021\n",
            "Epoch 100/300\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.62131\n",
            "240/240 - 0s - loss: 0.3242 - acc: 0.9292 - val_loss: 0.6382 - val_acc: 0.7021\n",
            "Epoch 101/300\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.62131\n",
            "240/240 - 0s - loss: 0.2872 - acc: 0.9500 - val_loss: 0.6610 - val_acc: 0.7021\n",
            "Epoch 102/300\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.62131\n",
            "240/240 - 0s - loss: 0.3120 - acc: 0.9208 - val_loss: 0.6582 - val_acc: 0.7872\n",
            "Epoch 103/300\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.62131\n",
            "240/240 - 0s - loss: 0.3152 - acc: 0.9125 - val_loss: 0.6452 - val_acc: 0.7872\n",
            "Epoch 104/300\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.62131\n",
            "240/240 - 0s - loss: 0.2863 - acc: 0.9625 - val_loss: 0.6498 - val_acc: 0.7660\n",
            "Epoch 105/300\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 0.62131\n",
            "240/240 - 0s - loss: 0.3056 - acc: 0.9417 - val_loss: 0.6535 - val_acc: 0.6809\n",
            "Epoch 106/300\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 0.62131\n",
            "240/240 - 0s - loss: 0.3130 - acc: 0.9250 - val_loss: 0.7171 - val_acc: 0.6809\n",
            "Epoch 107/300\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.62131\n",
            "240/240 - 0s - loss: 0.2824 - acc: 0.9542 - val_loss: 0.6847 - val_acc: 0.7447\n",
            "Epoch 108/300\n",
            "\n",
            "Epoch 00108: val_loss improved from 0.62131 to 0.61931, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.3081 - acc: 0.9292 - val_loss: 0.6193 - val_acc: 0.7660\n",
            "Epoch 109/300\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.61931\n",
            "240/240 - 0s - loss: 0.2960 - acc: 0.9375 - val_loss: 0.6734 - val_acc: 0.7660\n",
            "Epoch 110/300\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.61931\n",
            "240/240 - 0s - loss: 0.3075 - acc: 0.9167 - val_loss: 0.7184 - val_acc: 0.7021\n",
            "Epoch 111/300\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 0.61931\n",
            "240/240 - 0s - loss: 0.3186 - acc: 0.9125 - val_loss: 0.6205 - val_acc: 0.7660\n",
            "Epoch 112/300\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 0.61931\n",
            "240/240 - 0s - loss: 0.3133 - acc: 0.9333 - val_loss: 0.6623 - val_acc: 0.6809\n",
            "Epoch 113/300\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.61931\n",
            "240/240 - 0s - loss: 0.3047 - acc: 0.9208 - val_loss: 0.6660 - val_acc: 0.6809\n",
            "Epoch 114/300\n",
            "\n",
            "Epoch 00114: val_loss improved from 0.61931 to 0.60523, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.2742 - acc: 0.9375 - val_loss: 0.6052 - val_acc: 0.7234\n",
            "Epoch 115/300\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.60523\n",
            "240/240 - 0s - loss: 0.2839 - acc: 0.9625 - val_loss: 0.6488 - val_acc: 0.7660\n",
            "Epoch 116/300\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.60523\n",
            "240/240 - 0s - loss: 0.2882 - acc: 0.9542 - val_loss: 0.6058 - val_acc: 0.7234\n",
            "Epoch 117/300\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.60523\n",
            "240/240 - 0s - loss: 0.2911 - acc: 0.9458 - val_loss: 0.6211 - val_acc: 0.7447\n",
            "Epoch 118/300\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.60523\n",
            "240/240 - 0s - loss: 0.3339 - acc: 0.9125 - val_loss: 0.6128 - val_acc: 0.7021\n",
            "Epoch 119/300\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 0.60523\n",
            "240/240 - 0s - loss: 0.2707 - acc: 0.9583 - val_loss: 0.6115 - val_acc: 0.7660\n",
            "Epoch 120/300\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.60523\n",
            "240/240 - 0s - loss: 0.2844 - acc: 0.9333 - val_loss: 0.6348 - val_acc: 0.7447\n",
            "Epoch 121/300\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.60523\n",
            "240/240 - 0s - loss: 0.2954 - acc: 0.9375 - val_loss: 0.6742 - val_acc: 0.7872\n",
            "Epoch 122/300\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.60523\n",
            "240/240 - 0s - loss: 0.3047 - acc: 0.9250 - val_loss: 0.6142 - val_acc: 0.7021\n",
            "Epoch 123/300\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 0.60523\n",
            "240/240 - 0s - loss: 0.2751 - acc: 0.9500 - val_loss: 0.6424 - val_acc: 0.7234\n",
            "Epoch 124/300\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 0.60523\n",
            "240/240 - 0s - loss: 0.2771 - acc: 0.9625 - val_loss: 0.6090 - val_acc: 0.7660\n",
            "Epoch 125/300\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 0.60523\n",
            "240/240 - 0s - loss: 0.2760 - acc: 0.9458 - val_loss: 0.6154 - val_acc: 0.7660\n",
            "Epoch 126/300\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 0.60523\n",
            "240/240 - 0s - loss: 0.2988 - acc: 0.9208 - val_loss: 0.6858 - val_acc: 0.7234\n",
            "Epoch 127/300\n",
            "\n",
            "Epoch 00127: val_loss improved from 0.60523 to 0.60360, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.2917 - acc: 0.9250 - val_loss: 0.6036 - val_acc: 0.7872\n",
            "Epoch 128/300\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 0.60360\n",
            "240/240 - 0s - loss: 0.2933 - acc: 0.9333 - val_loss: 0.6507 - val_acc: 0.7234\n",
            "Epoch 129/300\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.60360\n",
            "240/240 - 0s - loss: 0.2619 - acc: 0.9500 - val_loss: 0.6310 - val_acc: 0.6809\n",
            "Epoch 130/300\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 0.60360\n",
            "240/240 - 0s - loss: 0.2628 - acc: 0.9625 - val_loss: 0.6355 - val_acc: 0.7660\n",
            "Epoch 131/300\n",
            "\n",
            "Epoch 00131: val_loss improved from 0.60360 to 0.59256, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.2576 - acc: 0.9500 - val_loss: 0.5926 - val_acc: 0.7660\n",
            "Epoch 132/300\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 0.59256\n",
            "240/240 - 0s - loss: 0.2468 - acc: 0.9583 - val_loss: 0.6200 - val_acc: 0.7660\n",
            "Epoch 133/300\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.59256\n",
            "240/240 - 0s - loss: 0.2464 - acc: 0.9667 - val_loss: 0.6145 - val_acc: 0.7234\n",
            "Epoch 134/300\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.59256\n",
            "240/240 - 0s - loss: 0.2488 - acc: 0.9375 - val_loss: 0.6923 - val_acc: 0.7447\n",
            "Epoch 135/300\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 0.59256\n",
            "240/240 - 0s - loss: 0.2653 - acc: 0.9208 - val_loss: 0.6820 - val_acc: 0.7234\n",
            "Epoch 136/300\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.59256\n",
            "240/240 - 0s - loss: 0.2474 - acc: 0.9458 - val_loss: 0.6025 - val_acc: 0.7021\n",
            "Epoch 137/300\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 0.59256\n",
            "240/240 - 0s - loss: 0.2351 - acc: 0.9625 - val_loss: 0.6232 - val_acc: 0.7447\n",
            "Epoch 138/300\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.59256\n",
            "240/240 - 0s - loss: 0.2527 - acc: 0.9417 - val_loss: 0.6756 - val_acc: 0.7660\n",
            "Epoch 139/300\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.59256\n",
            "240/240 - 0s - loss: 0.2475 - acc: 0.9583 - val_loss: 0.5965 - val_acc: 0.7660\n",
            "Epoch 140/300\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 0.59256\n",
            "240/240 - 0s - loss: 0.2583 - acc: 0.9500 - val_loss: 0.5990 - val_acc: 0.7021\n",
            "Epoch 141/300\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.59256\n",
            "240/240 - 0s - loss: 0.2578 - acc: 0.9417 - val_loss: 0.6290 - val_acc: 0.7872\n",
            "Epoch 142/300\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.59256\n",
            "240/240 - 0s - loss: 0.2457 - acc: 0.9667 - val_loss: 0.6042 - val_acc: 0.7234\n",
            "Epoch 143/300\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 0.59256\n",
            "240/240 - 0s - loss: 0.2445 - acc: 0.9583 - val_loss: 0.7148 - val_acc: 0.7021\n",
            "Epoch 144/300\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 0.59256\n",
            "240/240 - 0s - loss: 0.2448 - acc: 0.9500 - val_loss: 0.6179 - val_acc: 0.7660\n",
            "Epoch 145/300\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.59256\n",
            "240/240 - 0s - loss: 0.2362 - acc: 0.9708 - val_loss: 0.6249 - val_acc: 0.7660\n",
            "Epoch 146/300\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.59256\n",
            "240/240 - 0s - loss: 0.2525 - acc: 0.9583 - val_loss: 0.5974 - val_acc: 0.7447\n",
            "Epoch 147/300\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.59256\n",
            "240/240 - 0s - loss: 0.2474 - acc: 0.9500 - val_loss: 0.6385 - val_acc: 0.7021\n",
            "Epoch 148/300\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 0.59256\n",
            "240/240 - 0s - loss: 0.2275 - acc: 0.9542 - val_loss: 0.7073 - val_acc: 0.7872\n",
            "Epoch 149/300\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.59256\n",
            "240/240 - 0s - loss: 0.2348 - acc: 0.9500 - val_loss: 0.6530 - val_acc: 0.7234\n",
            "Epoch 150/300\n",
            "\n",
            "Epoch 00150: val_loss improved from 0.59256 to 0.59255, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.2045 - acc: 0.9750 - val_loss: 0.5926 - val_acc: 0.7872\n",
            "Epoch 151/300\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.59255\n",
            "240/240 - 0s - loss: 0.2451 - acc: 0.9458 - val_loss: 0.6412 - val_acc: 0.7660\n",
            "Epoch 152/300\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.59255\n",
            "240/240 - 0s - loss: 0.2698 - acc: 0.9250 - val_loss: 0.6560 - val_acc: 0.7234\n",
            "Epoch 153/300\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 0.59255\n",
            "240/240 - 0s - loss: 0.2234 - acc: 0.9708 - val_loss: 0.6557 - val_acc: 0.7872\n",
            "Epoch 154/300\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.59255\n",
            "240/240 - 0s - loss: 0.2335 - acc: 0.9583 - val_loss: 0.6030 - val_acc: 0.8085\n",
            "Epoch 155/300\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.59255\n",
            "240/240 - 0s - loss: 0.2084 - acc: 0.9833 - val_loss: 0.6246 - val_acc: 0.7234\n",
            "Epoch 156/300\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.59255\n",
            "240/240 - 0s - loss: 0.2321 - acc: 0.9500 - val_loss: 0.6425 - val_acc: 0.7660\n",
            "Epoch 157/300\n",
            "\n",
            "Epoch 00157: val_loss improved from 0.59255 to 0.57660, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.2440 - acc: 0.9292 - val_loss: 0.5766 - val_acc: 0.7660\n",
            "Epoch 158/300\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.57660\n",
            "240/240 - 0s - loss: 0.2477 - acc: 0.9417 - val_loss: 0.6114 - val_acc: 0.7447\n",
            "Epoch 159/300\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.57660\n",
            "240/240 - 0s - loss: 0.2162 - acc: 0.9667 - val_loss: 0.6019 - val_acc: 0.7447\n",
            "Epoch 160/300\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.57660\n",
            "240/240 - 0s - loss: 0.2568 - acc: 0.9542 - val_loss: 0.6983 - val_acc: 0.7447\n",
            "Epoch 161/300\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.57660\n",
            "240/240 - 0s - loss: 0.2231 - acc: 0.9458 - val_loss: 0.6329 - val_acc: 0.7872\n",
            "Epoch 162/300\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.57660\n",
            "240/240 - 0s - loss: 0.2307 - acc: 0.9625 - val_loss: 0.6772 - val_acc: 0.7234\n",
            "Epoch 163/300\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.57660\n",
            "240/240 - 0s - loss: 0.2534 - acc: 0.9458 - val_loss: 0.6152 - val_acc: 0.7234\n",
            "Epoch 164/300\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.57660\n",
            "240/240 - 0s - loss: 0.2161 - acc: 0.9542 - val_loss: 0.5919 - val_acc: 0.7660\n",
            "Epoch 165/300\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.57660\n",
            "240/240 - 0s - loss: 0.2418 - acc: 0.9542 - val_loss: 0.5878 - val_acc: 0.7872\n",
            "Epoch 166/300\n",
            "\n",
            "Epoch 00166: val_loss improved from 0.57660 to 0.57342, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.1962 - acc: 0.9708 - val_loss: 0.5734 - val_acc: 0.7660\n",
            "Epoch 167/300\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.57342\n",
            "240/240 - 0s - loss: 0.2175 - acc: 0.9708 - val_loss: 0.6238 - val_acc: 0.6809\n",
            "Epoch 168/300\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.57342\n",
            "240/240 - 0s - loss: 0.2325 - acc: 0.9458 - val_loss: 0.6128 - val_acc: 0.7447\n",
            "Epoch 169/300\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 0.57342\n",
            "240/240 - 0s - loss: 0.2212 - acc: 0.9542 - val_loss: 0.6479 - val_acc: 0.7447\n",
            "Epoch 170/300\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.57342\n",
            "240/240 - 0s - loss: 0.2303 - acc: 0.9583 - val_loss: 0.5984 - val_acc: 0.7660\n",
            "Epoch 171/300\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.57342\n",
            "240/240 - 0s - loss: 0.2245 - acc: 0.9375 - val_loss: 0.5993 - val_acc: 0.7447\n",
            "Epoch 172/300\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.57342\n",
            "240/240 - 0s - loss: 0.2441 - acc: 0.9458 - val_loss: 0.6001 - val_acc: 0.7447\n",
            "Epoch 173/300\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.57342\n",
            "240/240 - 0s - loss: 0.2120 - acc: 0.9542 - val_loss: 0.6334 - val_acc: 0.7021\n",
            "Epoch 174/300\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.57342\n",
            "240/240 - 0s - loss: 0.2477 - acc: 0.9250 - val_loss: 0.6417 - val_acc: 0.7021\n",
            "Epoch 175/300\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.57342\n",
            "240/240 - 0s - loss: 0.2281 - acc: 0.9708 - val_loss: 0.5896 - val_acc: 0.7660\n",
            "Epoch 176/300\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 0.57342\n",
            "240/240 - 0s - loss: 0.2443 - acc: 0.9458 - val_loss: 0.6005 - val_acc: 0.7234\n",
            "Epoch 177/300\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.57342\n",
            "240/240 - 0s - loss: 0.2120 - acc: 0.9625 - val_loss: 0.6228 - val_acc: 0.7660\n",
            "Epoch 178/300\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.57342\n",
            "240/240 - 0s - loss: 0.1924 - acc: 0.9708 - val_loss: 0.7470 - val_acc: 0.6596\n",
            "Epoch 179/300\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.57342\n",
            "240/240 - 0s - loss: 0.2262 - acc: 0.9500 - val_loss: 0.6875 - val_acc: 0.7234\n",
            "Epoch 180/300\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.57342\n",
            "240/240 - 0s - loss: 0.2150 - acc: 0.9500 - val_loss: 0.6693 - val_acc: 0.7660\n",
            "Epoch 181/300\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.57342\n",
            "240/240 - 0s - loss: 0.1989 - acc: 0.9750 - val_loss: 0.6518 - val_acc: 0.7234\n",
            "Epoch 182/300\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.57342\n",
            "240/240 - 0s - loss: 0.2123 - acc: 0.9750 - val_loss: 0.6669 - val_acc: 0.7234\n",
            "Epoch 183/300\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.57342\n",
            "240/240 - 0s - loss: 0.2316 - acc: 0.9458 - val_loss: 0.5735 - val_acc: 0.7447\n",
            "Epoch 184/300\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 0.57342\n",
            "240/240 - 0s - loss: 0.2551 - acc: 0.9417 - val_loss: 0.6743 - val_acc: 0.7234\n",
            "Epoch 185/300\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.57342\n",
            "240/240 - 0s - loss: 0.2334 - acc: 0.9458 - val_loss: 0.6269 - val_acc: 0.7234\n",
            "Epoch 186/300\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.57342\n",
            "240/240 - 0s - loss: 0.2292 - acc: 0.9417 - val_loss: 0.6234 - val_acc: 0.7234\n",
            "Epoch 187/300\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 0.57342\n",
            "240/240 - 0s - loss: 0.2476 - acc: 0.9417 - val_loss: 0.6339 - val_acc: 0.7660\n",
            "Epoch 188/300\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.57342\n",
            "240/240 - 0s - loss: 0.2041 - acc: 0.9708 - val_loss: 0.5818 - val_acc: 0.7660\n",
            "Epoch 189/300\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.57342\n",
            "240/240 - 0s - loss: 0.2023 - acc: 0.9750 - val_loss: 0.6796 - val_acc: 0.7872\n",
            "Epoch 190/300\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.57342\n",
            "240/240 - 0s - loss: 0.2427 - acc: 0.9333 - val_loss: 0.7194 - val_acc: 0.7447\n",
            "Epoch 191/300\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.57342\n",
            "240/240 - 0s - loss: 0.1943 - acc: 0.9583 - val_loss: 0.6149 - val_acc: 0.6809\n",
            "Epoch 192/300\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 0.57342\n",
            "240/240 - 0s - loss: 0.2031 - acc: 0.9708 - val_loss: 0.6132 - val_acc: 0.7660\n",
            "Epoch 193/300\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.57342\n",
            "240/240 - 0s - loss: 0.2299 - acc: 0.9542 - val_loss: 0.6992 - val_acc: 0.7447\n",
            "Epoch 194/300\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.57342\n",
            "240/240 - 0s - loss: 0.1878 - acc: 0.9833 - val_loss: 0.6255 - val_acc: 0.7447\n",
            "Epoch 195/300\n",
            "\n",
            "Epoch 00195: val_loss improved from 0.57342 to 0.57029, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.1813 - acc: 0.9792 - val_loss: 0.5703 - val_acc: 0.7660\n",
            "Epoch 196/300\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.57029\n",
            "240/240 - 0s - loss: 0.1743 - acc: 0.9667 - val_loss: 0.7471 - val_acc: 0.7021\n",
            "Epoch 197/300\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.57029\n",
            "240/240 - 0s - loss: 0.2195 - acc: 0.9542 - val_loss: 0.6489 - val_acc: 0.7447\n",
            "Epoch 198/300\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.57029\n",
            "240/240 - 0s - loss: 0.2110 - acc: 0.9500 - val_loss: 0.5858 - val_acc: 0.7660\n",
            "Epoch 199/300\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.57029\n",
            "240/240 - 0s - loss: 0.1920 - acc: 0.9708 - val_loss: 0.5822 - val_acc: 0.7660\n",
            "Epoch 200/300\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.57029\n",
            "240/240 - 0s - loss: 0.2085 - acc: 0.9542 - val_loss: 0.6037 - val_acc: 0.7447\n",
            "Epoch 201/300\n",
            "\n",
            "Epoch 00201: val_loss did not improve from 0.57029\n",
            "240/240 - 0s - loss: 0.1979 - acc: 0.9625 - val_loss: 0.6123 - val_acc: 0.7872\n",
            "Epoch 202/300\n",
            "\n",
            "Epoch 00202: val_loss did not improve from 0.57029\n",
            "240/240 - 0s - loss: 0.1887 - acc: 0.9792 - val_loss: 0.6200 - val_acc: 0.7447\n",
            "Epoch 203/300\n",
            "\n",
            "Epoch 00203: val_loss improved from 0.57029 to 0.54625, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.1781 - acc: 0.9750 - val_loss: 0.5463 - val_acc: 0.7447\n",
            "Epoch 204/300\n",
            "\n",
            "Epoch 00204: val_loss did not improve from 0.54625\n",
            "240/240 - 0s - loss: 0.1786 - acc: 0.9625 - val_loss: 0.6362 - val_acc: 0.7447\n",
            "Epoch 205/300\n",
            "\n",
            "Epoch 00205: val_loss did not improve from 0.54625\n",
            "240/240 - 0s - loss: 0.1955 - acc: 0.9625 - val_loss: 0.5493 - val_acc: 0.7660\n",
            "Epoch 206/300\n",
            "\n",
            "Epoch 00206: val_loss did not improve from 0.54625\n",
            "240/240 - 0s - loss: 0.2517 - acc: 0.9250 - val_loss: 0.5773 - val_acc: 0.8298\n",
            "Epoch 207/300\n",
            "\n",
            "Epoch 00207: val_loss did not improve from 0.54625\n",
            "240/240 - 0s - loss: 0.1872 - acc: 0.9792 - val_loss: 0.5797 - val_acc: 0.7660\n",
            "Epoch 208/300\n",
            "\n",
            "Epoch 00208: val_loss did not improve from 0.54625\n",
            "240/240 - 0s - loss: 0.1843 - acc: 0.9667 - val_loss: 0.5649 - val_acc: 0.7660\n",
            "Epoch 209/300\n",
            "\n",
            "Epoch 00209: val_loss did not improve from 0.54625\n",
            "240/240 - 0s - loss: 0.2063 - acc: 0.9708 - val_loss: 0.5605 - val_acc: 0.8085\n",
            "Epoch 210/300\n",
            "\n",
            "Epoch 00210: val_loss did not improve from 0.54625\n",
            "240/240 - 0s - loss: 0.1899 - acc: 0.9583 - val_loss: 0.6116 - val_acc: 0.7660\n",
            "Epoch 211/300\n",
            "\n",
            "Epoch 00211: val_loss did not improve from 0.54625\n",
            "240/240 - 0s - loss: 0.1878 - acc: 0.9792 - val_loss: 0.6166 - val_acc: 0.7447\n",
            "Epoch 212/300\n",
            "\n",
            "Epoch 00212: val_loss did not improve from 0.54625\n",
            "240/240 - 0s - loss: 0.1852 - acc: 0.9583 - val_loss: 0.6249 - val_acc: 0.7234\n",
            "Epoch 213/300\n",
            "\n",
            "Epoch 00213: val_loss did not improve from 0.54625\n",
            "240/240 - 0s - loss: 0.2150 - acc: 0.9542 - val_loss: 0.5892 - val_acc: 0.7660\n",
            "Epoch 214/300\n",
            "\n",
            "Epoch 00214: val_loss did not improve from 0.54625\n",
            "240/240 - 0s - loss: 0.2109 - acc: 0.9583 - val_loss: 0.5720 - val_acc: 0.7872\n",
            "Epoch 215/300\n",
            "\n",
            "Epoch 00215: val_loss did not improve from 0.54625\n",
            "240/240 - 0s - loss: 0.1607 - acc: 0.9750 - val_loss: 0.6535 - val_acc: 0.7660\n",
            "Epoch 216/300\n",
            "\n",
            "Epoch 00216: val_loss did not improve from 0.54625\n",
            "240/240 - 0s - loss: 0.1871 - acc: 0.9542 - val_loss: 0.6039 - val_acc: 0.7447\n",
            "Epoch 217/300\n",
            "\n",
            "Epoch 00217: val_loss did not improve from 0.54625\n",
            "240/240 - 0s - loss: 0.1791 - acc: 0.9667 - val_loss: 0.6144 - val_acc: 0.7447\n",
            "Epoch 218/300\n",
            "\n",
            "Epoch 00218: val_loss did not improve from 0.54625\n",
            "240/240 - 0s - loss: 0.1902 - acc: 0.9458 - val_loss: 0.6546 - val_acc: 0.7447\n",
            "Epoch 219/300\n",
            "\n",
            "Epoch 00219: val_loss did not improve from 0.54625\n",
            "240/240 - 0s - loss: 0.2185 - acc: 0.9458 - val_loss: 0.6698 - val_acc: 0.7234\n",
            "Epoch 220/300\n",
            "\n",
            "Epoch 00220: val_loss did not improve from 0.54625\n",
            "240/240 - 0s - loss: 0.1790 - acc: 0.9583 - val_loss: 0.6096 - val_acc: 0.7234\n",
            "Epoch 221/300\n",
            "\n",
            "Epoch 00221: val_loss did not improve from 0.54625\n",
            "240/240 - 0s - loss: 0.1818 - acc: 0.9500 - val_loss: 0.6396 - val_acc: 0.7447\n",
            "Epoch 222/300\n",
            "\n",
            "Epoch 00222: val_loss did not improve from 0.54625\n",
            "240/240 - 0s - loss: 0.1651 - acc: 0.9875 - val_loss: 0.5698 - val_acc: 0.7872\n",
            "Epoch 223/300\n",
            "\n",
            "Epoch 00223: val_loss did not improve from 0.54625\n",
            "240/240 - 0s - loss: 0.1952 - acc: 0.9583 - val_loss: 0.6253 - val_acc: 0.7660\n",
            "Epoch 224/300\n",
            "\n",
            "Epoch 00224: val_loss did not improve from 0.54625\n",
            "240/240 - 0s - loss: 0.2066 - acc: 0.9542 - val_loss: 0.6095 - val_acc: 0.7021\n",
            "Epoch 225/300\n",
            "\n",
            "Epoch 00225: val_loss did not improve from 0.54625\n",
            "240/240 - 0s - loss: 0.1664 - acc: 0.9708 - val_loss: 0.5701 - val_acc: 0.7660\n",
            "Epoch 226/300\n",
            "\n",
            "Epoch 00226: val_loss did not improve from 0.54625\n",
            "240/240 - 0s - loss: 0.1730 - acc: 0.9792 - val_loss: 0.5923 - val_acc: 0.7872\n",
            "Epoch 227/300\n",
            "\n",
            "Epoch 00227: val_loss did not improve from 0.54625\n",
            "240/240 - 0s - loss: 0.1930 - acc: 0.9500 - val_loss: 0.6062 - val_acc: 0.7447\n",
            "Epoch 228/300\n",
            "\n",
            "Epoch 00228: val_loss did not improve from 0.54625\n",
            "240/240 - 0s - loss: 0.2100 - acc: 0.9458 - val_loss: 0.6074 - val_acc: 0.7234\n",
            "Epoch 229/300\n",
            "\n",
            "Epoch 00229: val_loss did not improve from 0.54625\n",
            "240/240 - 0s - loss: 0.1903 - acc: 0.9583 - val_loss: 0.6230 - val_acc: 0.7660\n",
            "Epoch 230/300\n",
            "\n",
            "Epoch 00230: val_loss did not improve from 0.54625\n",
            "240/240 - 0s - loss: 0.2257 - acc: 0.9542 - val_loss: 0.6672 - val_acc: 0.7660\n",
            "Epoch 231/300\n",
            "\n",
            "Epoch 00231: val_loss did not improve from 0.54625\n",
            "240/240 - 0s - loss: 0.2044 - acc: 0.9417 - val_loss: 0.7232 - val_acc: 0.7234\n",
            "Epoch 232/300\n",
            "\n",
            "Epoch 00232: val_loss did not improve from 0.54625\n",
            "240/240 - 0s - loss: 0.1601 - acc: 0.9750 - val_loss: 0.6992 - val_acc: 0.7447\n",
            "Epoch 233/300\n",
            "\n",
            "Epoch 00233: val_loss did not improve from 0.54625\n",
            "240/240 - 0s - loss: 0.1664 - acc: 0.9708 - val_loss: 0.5752 - val_acc: 0.8085\n",
            "Epoch 234/300\n",
            "\n",
            "Epoch 00234: val_loss did not improve from 0.54625\n",
            "240/240 - 0s - loss: 0.1835 - acc: 0.9583 - val_loss: 0.6313 - val_acc: 0.7447\n",
            "Epoch 235/300\n",
            "\n",
            "Epoch 00235: val_loss did not improve from 0.54625\n",
            "240/240 - 0s - loss: 0.1514 - acc: 0.9917 - val_loss: 0.6906 - val_acc: 0.7872\n",
            "Epoch 236/300\n",
            "\n",
            "Epoch 00236: val_loss did not improve from 0.54625\n",
            "240/240 - 0s - loss: 0.1698 - acc: 0.9625 - val_loss: 0.6133 - val_acc: 0.8085\n",
            "Epoch 237/300\n",
            "\n",
            "Epoch 00237: val_loss did not improve from 0.54625\n",
            "240/240 - 0s - loss: 0.1556 - acc: 0.9792 - val_loss: 0.6208 - val_acc: 0.7234\n",
            "Epoch 238/300\n",
            "\n",
            "Epoch 00238: val_loss did not improve from 0.54625\n",
            "240/240 - 0s - loss: 0.1800 - acc: 0.9625 - val_loss: 0.6149 - val_acc: 0.7447\n",
            "Epoch 239/300\n",
            "\n",
            "Epoch 00239: val_loss did not improve from 0.54625\n",
            "240/240 - 0s - loss: 0.1750 - acc: 0.9708 - val_loss: 0.6171 - val_acc: 0.8085\n",
            "Epoch 240/300\n",
            "\n",
            "Epoch 00240: val_loss did not improve from 0.54625\n",
            "240/240 - 0s - loss: 0.1991 - acc: 0.9458 - val_loss: 0.6027 - val_acc: 0.7660\n",
            "Epoch 241/300\n",
            "\n",
            "Epoch 00241: val_loss did not improve from 0.54625\n",
            "240/240 - 0s - loss: 0.2157 - acc: 0.9375 - val_loss: 0.6229 - val_acc: 0.7872\n",
            "Epoch 242/300\n",
            "\n",
            "Epoch 00242: val_loss improved from 0.54625 to 0.53109, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.2055 - acc: 0.9583 - val_loss: 0.5311 - val_acc: 0.8085\n",
            "Epoch 243/300\n",
            "\n",
            "Epoch 00243: val_loss did not improve from 0.53109\n",
            "240/240 - 0s - loss: 0.1592 - acc: 0.9750 - val_loss: 0.6437 - val_acc: 0.7660\n",
            "Epoch 244/300\n",
            "\n",
            "Epoch 00244: val_loss did not improve from 0.53109\n",
            "240/240 - 0s - loss: 0.1757 - acc: 0.9750 - val_loss: 0.6305 - val_acc: 0.7660\n",
            "Epoch 245/300\n",
            "\n",
            "Epoch 00245: val_loss did not improve from 0.53109\n",
            "240/240 - 0s - loss: 0.1708 - acc: 0.9708 - val_loss: 0.7869 - val_acc: 0.7234\n",
            "Epoch 246/300\n",
            "\n",
            "Epoch 00246: val_loss did not improve from 0.53109\n",
            "240/240 - 0s - loss: 0.1615 - acc: 0.9792 - val_loss: 0.5864 - val_acc: 0.7872\n",
            "Epoch 247/300\n",
            "\n",
            "Epoch 00247: val_loss did not improve from 0.53109\n",
            "240/240 - 0s - loss: 0.1651 - acc: 0.9667 - val_loss: 0.5909 - val_acc: 0.8298\n",
            "Epoch 248/300\n",
            "\n",
            "Epoch 00248: val_loss did not improve from 0.53109\n",
            "240/240 - 0s - loss: 0.1378 - acc: 0.9875 - val_loss: 0.6026 - val_acc: 0.7447\n",
            "Epoch 249/300\n",
            "\n",
            "Epoch 00249: val_loss did not improve from 0.53109\n",
            "240/240 - 0s - loss: 0.1820 - acc: 0.9625 - val_loss: 0.6150 - val_acc: 0.7660\n",
            "Epoch 250/300\n",
            "\n",
            "Epoch 00250: val_loss did not improve from 0.53109\n",
            "240/240 - 0s - loss: 0.1570 - acc: 0.9750 - val_loss: 0.5662 - val_acc: 0.7447\n",
            "Epoch 251/300\n",
            "\n",
            "Epoch 00251: val_loss did not improve from 0.53109\n",
            "240/240 - 0s - loss: 0.1714 - acc: 0.9625 - val_loss: 0.6197 - val_acc: 0.7660\n",
            "Epoch 252/300\n",
            "\n",
            "Epoch 00252: val_loss did not improve from 0.53109\n",
            "240/240 - 0s - loss: 0.1716 - acc: 0.9750 - val_loss: 0.6118 - val_acc: 0.7872\n",
            "Epoch 253/300\n",
            "\n",
            "Epoch 00253: val_loss did not improve from 0.53109\n",
            "240/240 - 0s - loss: 0.1732 - acc: 0.9750 - val_loss: 0.6082 - val_acc: 0.7872\n",
            "Epoch 254/300\n",
            "\n",
            "Epoch 00254: val_loss did not improve from 0.53109\n",
            "240/240 - 0s - loss: 0.1651 - acc: 0.9708 - val_loss: 0.6365 - val_acc: 0.7660\n",
            "Epoch 255/300\n",
            "\n",
            "Epoch 00255: val_loss did not improve from 0.53109\n",
            "240/240 - 0s - loss: 0.1661 - acc: 0.9708 - val_loss: 0.6414 - val_acc: 0.7234\n",
            "Epoch 256/300\n",
            "\n",
            "Epoch 00256: val_loss did not improve from 0.53109\n",
            "240/240 - 0s - loss: 0.1574 - acc: 0.9750 - val_loss: 0.6620 - val_acc: 0.7447\n",
            "Epoch 257/300\n",
            "\n",
            "Epoch 00257: val_loss did not improve from 0.53109\n",
            "240/240 - 0s - loss: 0.1572 - acc: 0.9792 - val_loss: 0.6371 - val_acc: 0.7660\n",
            "Epoch 258/300\n",
            "\n",
            "Epoch 00258: val_loss did not improve from 0.53109\n",
            "240/240 - 0s - loss: 0.1637 - acc: 0.9542 - val_loss: 0.5898 - val_acc: 0.7872\n",
            "Epoch 259/300\n",
            "\n",
            "Epoch 00259: val_loss did not improve from 0.53109\n",
            "240/240 - 0s - loss: 0.1817 - acc: 0.9417 - val_loss: 0.5791 - val_acc: 0.8085\n",
            "Epoch 260/300\n",
            "\n",
            "Epoch 00260: val_loss did not improve from 0.53109\n",
            "240/240 - 0s - loss: 0.1725 - acc: 0.9583 - val_loss: 0.5789 - val_acc: 0.7660\n",
            "Epoch 261/300\n",
            "\n",
            "Epoch 00261: val_loss did not improve from 0.53109\n",
            "240/240 - 0s - loss: 0.1552 - acc: 0.9833 - val_loss: 0.5538 - val_acc: 0.7447\n",
            "Epoch 262/300\n",
            "\n",
            "Epoch 00262: val_loss did not improve from 0.53109\n",
            "240/240 - 0s - loss: 0.1874 - acc: 0.9583 - val_loss: 0.6309 - val_acc: 0.7660\n",
            "Epoch 263/300\n",
            "\n",
            "Epoch 00263: val_loss did not improve from 0.53109\n",
            "240/240 - 0s - loss: 0.1744 - acc: 0.9708 - val_loss: 0.6214 - val_acc: 0.7660\n",
            "Epoch 264/300\n",
            "\n",
            "Epoch 00264: val_loss did not improve from 0.53109\n",
            "240/240 - 0s - loss: 0.1705 - acc: 0.9750 - val_loss: 0.6064 - val_acc: 0.8085\n",
            "Epoch 265/300\n",
            "\n",
            "Epoch 00265: val_loss did not improve from 0.53109\n",
            "240/240 - 0s - loss: 0.1561 - acc: 0.9750 - val_loss: 0.7340 - val_acc: 0.7021\n",
            "Epoch 266/300\n",
            "\n",
            "Epoch 00266: val_loss did not improve from 0.53109\n",
            "240/240 - 0s - loss: 0.1643 - acc: 0.9708 - val_loss: 0.5669 - val_acc: 0.7447\n",
            "Epoch 267/300\n",
            "\n",
            "Epoch 00267: val_loss did not improve from 0.53109\n",
            "240/240 - 0s - loss: 0.1685 - acc: 0.9667 - val_loss: 0.6789 - val_acc: 0.7234\n",
            "Epoch 268/300\n",
            "\n",
            "Epoch 00268: val_loss did not improve from 0.53109\n",
            "240/240 - 0s - loss: 0.1658 - acc: 0.9667 - val_loss: 0.6564 - val_acc: 0.7234\n",
            "Epoch 269/300\n",
            "\n",
            "Epoch 00269: val_loss did not improve from 0.53109\n",
            "240/240 - 0s - loss: 0.1462 - acc: 0.9625 - val_loss: 0.5973 - val_acc: 0.7447\n",
            "Epoch 270/300\n",
            "\n",
            "Epoch 00270: val_loss did not improve from 0.53109\n",
            "240/240 - 0s - loss: 0.1494 - acc: 0.9833 - val_loss: 0.5933 - val_acc: 0.8085\n",
            "Epoch 271/300\n",
            "\n",
            "Epoch 00271: val_loss did not improve from 0.53109\n",
            "240/240 - 0s - loss: 0.2006 - acc: 0.9333 - val_loss: 0.6780 - val_acc: 0.7447\n",
            "Epoch 272/300\n",
            "\n",
            "Epoch 00272: val_loss did not improve from 0.53109\n",
            "240/240 - 0s - loss: 0.1824 - acc: 0.9458 - val_loss: 0.6204 - val_acc: 0.8085\n",
            "Epoch 273/300\n",
            "\n",
            "Epoch 00273: val_loss did not improve from 0.53109\n",
            "240/240 - 0s - loss: 0.1577 - acc: 0.9708 - val_loss: 0.7028 - val_acc: 0.7447\n",
            "Epoch 274/300\n",
            "\n",
            "Epoch 00274: val_loss did not improve from 0.53109\n",
            "240/240 - 0s - loss: 0.1261 - acc: 0.9875 - val_loss: 0.6256 - val_acc: 0.7447\n",
            "Epoch 275/300\n",
            "\n",
            "Epoch 00275: val_loss did not improve from 0.53109\n",
            "240/240 - 0s - loss: 0.1593 - acc: 0.9708 - val_loss: 0.6204 - val_acc: 0.7660\n",
            "Epoch 276/300\n",
            "\n",
            "Epoch 00276: val_loss did not improve from 0.53109\n",
            "240/240 - 0s - loss: 0.1485 - acc: 0.9667 - val_loss: 0.6043 - val_acc: 0.7234\n",
            "Epoch 277/300\n",
            "\n",
            "Epoch 00277: val_loss did not improve from 0.53109\n",
            "240/240 - 0s - loss: 0.1756 - acc: 0.9750 - val_loss: 0.6391 - val_acc: 0.7660\n",
            "Epoch 278/300\n",
            "\n",
            "Epoch 00278: val_loss did not improve from 0.53109\n",
            "240/240 - 0s - loss: 0.1554 - acc: 0.9708 - val_loss: 0.7105 - val_acc: 0.7234\n",
            "Epoch 279/300\n",
            "\n",
            "Epoch 00279: val_loss did not improve from 0.53109\n",
            "240/240 - 0s - loss: 0.1850 - acc: 0.9667 - val_loss: 0.6532 - val_acc: 0.7021\n",
            "Epoch 280/300\n",
            "\n",
            "Epoch 00280: val_loss did not improve from 0.53109\n",
            "240/240 - 0s - loss: 0.1894 - acc: 0.9500 - val_loss: 0.6338 - val_acc: 0.8085\n",
            "Epoch 281/300\n",
            "\n",
            "Epoch 00281: val_loss did not improve from 0.53109\n",
            "240/240 - 0s - loss: 0.1338 - acc: 0.9792 - val_loss: 0.6570 - val_acc: 0.7447\n",
            "Epoch 282/300\n",
            "\n",
            "Epoch 00282: val_loss did not improve from 0.53109\n",
            "240/240 - 0s - loss: 0.1500 - acc: 0.9583 - val_loss: 0.6451 - val_acc: 0.7660\n",
            "Epoch 283/300\n",
            "\n",
            "Epoch 00283: val_loss did not improve from 0.53109\n",
            "240/240 - 0s - loss: 0.1389 - acc: 0.9917 - val_loss: 0.6423 - val_acc: 0.7447\n",
            "Epoch 284/300\n",
            "\n",
            "Epoch 00284: val_loss did not improve from 0.53109\n",
            "240/240 - 0s - loss: 0.1485 - acc: 0.9792 - val_loss: 0.5951 - val_acc: 0.7660\n",
            "Epoch 285/300\n",
            "\n",
            "Epoch 00285: val_loss did not improve from 0.53109\n",
            "240/240 - 0s - loss: 0.1813 - acc: 0.9625 - val_loss: 0.5803 - val_acc: 0.7872\n",
            "Epoch 286/300\n",
            "\n",
            "Epoch 00286: val_loss did not improve from 0.53109\n",
            "240/240 - 0s - loss: 0.1489 - acc: 0.9792 - val_loss: 0.6614 - val_acc: 0.7872\n",
            "Epoch 287/300\n",
            "\n",
            "Epoch 00287: val_loss did not improve from 0.53109\n",
            "240/240 - 0s - loss: 0.1301 - acc: 0.9750 - val_loss: 0.6417 - val_acc: 0.7447\n",
            "Epoch 288/300\n",
            "\n",
            "Epoch 00288: val_loss did not improve from 0.53109\n",
            "240/240 - 0s - loss: 0.1537 - acc: 0.9750 - val_loss: 0.6436 - val_acc: 0.7234\n",
            "Epoch 289/300\n",
            "\n",
            "Epoch 00289: val_loss did not improve from 0.53109\n",
            "240/240 - 0s - loss: 0.1590 - acc: 0.9708 - val_loss: 0.5905 - val_acc: 0.7872\n",
            "Epoch 290/300\n",
            "\n",
            "Epoch 00290: val_loss did not improve from 0.53109\n",
            "240/240 - 0s - loss: 0.1886 - acc: 0.9542 - val_loss: 0.6062 - val_acc: 0.7021\n",
            "Epoch 291/300\n",
            "\n",
            "Epoch 00291: val_loss did not improve from 0.53109\n",
            "240/240 - 0s - loss: 0.1482 - acc: 0.9625 - val_loss: 0.6636 - val_acc: 0.6596\n",
            "Epoch 292/300\n",
            "\n",
            "Epoch 00292: val_loss did not improve from 0.53109\n",
            "240/240 - 0s - loss: 0.1610 - acc: 0.9625 - val_loss: 0.6482 - val_acc: 0.7660\n",
            "Epoch 293/300\n",
            "\n",
            "Epoch 00293: val_loss did not improve from 0.53109\n",
            "240/240 - 0s - loss: 0.1729 - acc: 0.9667 - val_loss: 0.6602 - val_acc: 0.7660\n",
            "Epoch 294/300\n",
            "\n",
            "Epoch 00294: val_loss did not improve from 0.53109\n",
            "240/240 - 0s - loss: 0.1620 - acc: 0.9667 - val_loss: 0.5963 - val_acc: 0.7447\n",
            "Epoch 295/300\n",
            "\n",
            "Epoch 00295: val_loss did not improve from 0.53109\n",
            "240/240 - 0s - loss: 0.1738 - acc: 0.9583 - val_loss: 0.7134 - val_acc: 0.7234\n",
            "Epoch 296/300\n",
            "\n",
            "Epoch 00296: val_loss did not improve from 0.53109\n",
            "240/240 - 0s - loss: 0.1566 - acc: 0.9708 - val_loss: 0.7271 - val_acc: 0.7021\n",
            "Epoch 297/300\n",
            "\n",
            "Epoch 00297: val_loss did not improve from 0.53109\n",
            "240/240 - 0s - loss: 0.1855 - acc: 0.9542 - val_loss: 0.6484 - val_acc: 0.7660\n",
            "Epoch 298/300\n",
            "\n",
            "Epoch 00298: val_loss did not improve from 0.53109\n",
            "240/240 - 0s - loss: 0.1580 - acc: 0.9667 - val_loss: 0.6227 - val_acc: 0.7660\n",
            "Epoch 299/300\n",
            "\n",
            "Epoch 00299: val_loss did not improve from 0.53109\n",
            "240/240 - 0s - loss: 0.1777 - acc: 0.9375 - val_loss: 0.7231 - val_acc: 0.7447\n",
            "Epoch 300/300\n",
            "\n",
            "Epoch 00300: val_loss did not improve from 0.53109\n",
            "240/240 - 0s - loss: 0.1529 - acc: 0.9833 - val_loss: 0.5935 - val_acc: 0.7660\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "# Evaluate on test data\n",
            "288/288 [==============================] - 1s 2ms/sample - loss: 0.5408 - acc: 0.7882\n",
            "test loss, test acc: [0.5408015018679679, 0.7881944]\n",
            "EEG_Deep/Data2A/Data_A09T.mat\n",
            "(288, 22, 1875)\n",
            "(288, 1)\n",
            "(240, 22, 750)\n",
            "(240, 1)\n",
            "(47, 22, 750)\n",
            "(47, 1)\n",
            "X_train shape: (240, 1, 22, 750)\n",
            "240 train samples\n",
            "47 val samples\n",
            "EEG_Deep/Data2A/Data_A09E.mat\n",
            "(288, 22, 1875)\n",
            "(288, 1)\n",
            "(288, 22, 750)\n",
            "(288, 1)\n",
            "X_train shape: (288, 1, 22, 750)\n",
            "288 train samples\n",
            "Train on 240 samples, validate on 47 samples\n",
            "Epoch 1/300\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 1.36560, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 1s - loss: 1.3510 - acc: 0.3625 - val_loss: 1.3656 - val_acc: 0.2553\n",
            "Epoch 2/300\n",
            "\n",
            "Epoch 00002: val_loss improved from 1.36560 to 1.34412, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.2216 - acc: 0.5292 - val_loss: 1.3441 - val_acc: 0.3191\n",
            "Epoch 3/300\n",
            "\n",
            "Epoch 00003: val_loss improved from 1.34412 to 1.31278, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.1236 - acc: 0.5750 - val_loss: 1.3128 - val_acc: 0.4043\n",
            "Epoch 4/300\n",
            "\n",
            "Epoch 00004: val_loss improved from 1.31278 to 1.28531, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 1.0212 - acc: 0.6000 - val_loss: 1.2853 - val_acc: 0.5745\n",
            "Epoch 5/300\n",
            "\n",
            "Epoch 00005: val_loss improved from 1.28531 to 1.26309, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.9474 - acc: 0.6458 - val_loss: 1.2631 - val_acc: 0.5532\n",
            "Epoch 6/300\n",
            "\n",
            "Epoch 00006: val_loss improved from 1.26309 to 1.25091, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.9097 - acc: 0.6417 - val_loss: 1.2509 - val_acc: 0.5319\n",
            "Epoch 7/300\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 1.25091\n",
            "240/240 - 0s - loss: 0.8855 - acc: 0.6917 - val_loss: 1.2560 - val_acc: 0.4468\n",
            "Epoch 8/300\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 1.25091\n",
            "240/240 - 0s - loss: 0.8230 - acc: 0.6708 - val_loss: 1.2707 - val_acc: 0.3191\n",
            "Epoch 9/300\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 1.25091\n",
            "240/240 - 0s - loss: 0.7770 - acc: 0.7250 - val_loss: 1.2892 - val_acc: 0.2979\n",
            "Epoch 10/300\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 1.25091\n",
            "240/240 - 0s - loss: 0.7724 - acc: 0.7458 - val_loss: 1.2908 - val_acc: 0.2766\n",
            "Epoch 11/300\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 1.25091\n",
            "240/240 - 0s - loss: 0.7213 - acc: 0.7833 - val_loss: 1.2893 - val_acc: 0.3191\n",
            "Epoch 12/300\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 1.25091\n",
            "240/240 - 0s - loss: 0.7004 - acc: 0.8208 - val_loss: 1.2873 - val_acc: 0.3191\n",
            "Epoch 13/300\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 1.25091\n",
            "240/240 - 0s - loss: 0.6979 - acc: 0.7750 - val_loss: 1.2685 - val_acc: 0.3830\n",
            "Epoch 14/300\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 1.25091\n",
            "240/240 - 0s - loss: 0.6671 - acc: 0.8083 - val_loss: 1.2592 - val_acc: 0.3617\n",
            "Epoch 15/300\n",
            "\n",
            "Epoch 00015: val_loss improved from 1.25091 to 1.18049, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.6259 - acc: 0.8208 - val_loss: 1.1805 - val_acc: 0.4255\n",
            "Epoch 16/300\n",
            "\n",
            "Epoch 00016: val_loss improved from 1.18049 to 1.17344, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.6081 - acc: 0.8292 - val_loss: 1.1734 - val_acc: 0.4894\n",
            "Epoch 17/300\n",
            "\n",
            "Epoch 00017: val_loss improved from 1.17344 to 1.10491, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.5980 - acc: 0.8417 - val_loss: 1.1049 - val_acc: 0.5532\n",
            "Epoch 18/300\n",
            "\n",
            "Epoch 00018: val_loss improved from 1.10491 to 1.09792, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.5795 - acc: 0.8333 - val_loss: 1.0979 - val_acc: 0.5319\n",
            "Epoch 19/300\n",
            "\n",
            "Epoch 00019: val_loss improved from 1.09792 to 1.02224, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.5688 - acc: 0.8417 - val_loss: 1.0222 - val_acc: 0.5532\n",
            "Epoch 20/300\n",
            "\n",
            "Epoch 00020: val_loss improved from 1.02224 to 0.96282, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.5492 - acc: 0.8458 - val_loss: 0.9628 - val_acc: 0.6170\n",
            "Epoch 21/300\n",
            "\n",
            "Epoch 00021: val_loss improved from 0.96282 to 0.94898, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.5409 - acc: 0.8458 - val_loss: 0.9490 - val_acc: 0.5745\n",
            "Epoch 22/300\n",
            "\n",
            "Epoch 00022: val_loss improved from 0.94898 to 0.91185, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.5639 - acc: 0.8542 - val_loss: 0.9118 - val_acc: 0.6383\n",
            "Epoch 23/300\n",
            "\n",
            "Epoch 00023: val_loss improved from 0.91185 to 0.87982, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.5003 - acc: 0.8583 - val_loss: 0.8798 - val_acc: 0.5957\n",
            "Epoch 24/300\n",
            "\n",
            "Epoch 00024: val_loss improved from 0.87982 to 0.86657, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.5447 - acc: 0.8667 - val_loss: 0.8666 - val_acc: 0.6383\n",
            "Epoch 25/300\n",
            "\n",
            "Epoch 00025: val_loss improved from 0.86657 to 0.84680, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.5074 - acc: 0.8708 - val_loss: 0.8468 - val_acc: 0.6809\n",
            "Epoch 26/300\n",
            "\n",
            "Epoch 00026: val_loss improved from 0.84680 to 0.79198, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.4681 - acc: 0.8875 - val_loss: 0.7920 - val_acc: 0.6809\n",
            "Epoch 27/300\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.79198\n",
            "240/240 - 0s - loss: 0.4800 - acc: 0.8792 - val_loss: 0.8216 - val_acc: 0.6809\n",
            "Epoch 28/300\n",
            "\n",
            "Epoch 00028: val_loss improved from 0.79198 to 0.77751, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.4968 - acc: 0.8667 - val_loss: 0.7775 - val_acc: 0.6383\n",
            "Epoch 29/300\n",
            "\n",
            "Epoch 00029: val_loss improved from 0.77751 to 0.74322, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.4765 - acc: 0.8792 - val_loss: 0.7432 - val_acc: 0.6809\n",
            "Epoch 30/300\n",
            "\n",
            "Epoch 00030: val_loss improved from 0.74322 to 0.70318, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.4902 - acc: 0.8708 - val_loss: 0.7032 - val_acc: 0.7234\n",
            "Epoch 31/300\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.70318\n",
            "240/240 - 0s - loss: 0.4583 - acc: 0.8750 - val_loss: 0.7271 - val_acc: 0.7234\n",
            "Epoch 32/300\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.70318\n",
            "240/240 - 0s - loss: 0.4601 - acc: 0.9000 - val_loss: 0.7305 - val_acc: 0.7021\n",
            "Epoch 33/300\n",
            "\n",
            "Epoch 00033: val_loss improved from 0.70318 to 0.70169, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.4589 - acc: 0.8875 - val_loss: 0.7017 - val_acc: 0.7021\n",
            "Epoch 34/300\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.70169\n",
            "240/240 - 0s - loss: 0.4382 - acc: 0.8958 - val_loss: 0.7211 - val_acc: 0.7021\n",
            "Epoch 35/300\n",
            "\n",
            "Epoch 00035: val_loss improved from 0.70169 to 0.67670, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.4392 - acc: 0.8917 - val_loss: 0.6767 - val_acc: 0.7447\n",
            "Epoch 36/300\n",
            "\n",
            "Epoch 00036: val_loss improved from 0.67670 to 0.67019, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.4371 - acc: 0.9000 - val_loss: 0.6702 - val_acc: 0.7447\n",
            "Epoch 37/300\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.67019\n",
            "240/240 - 0s - loss: 0.4296 - acc: 0.8958 - val_loss: 0.6727 - val_acc: 0.7234\n",
            "Epoch 38/300\n",
            "\n",
            "Epoch 00038: val_loss improved from 0.67019 to 0.64958, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.4288 - acc: 0.9083 - val_loss: 0.6496 - val_acc: 0.7447\n",
            "Epoch 39/300\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.64958\n",
            "240/240 - 0s - loss: 0.4280 - acc: 0.8917 - val_loss: 0.6562 - val_acc: 0.7234\n",
            "Epoch 40/300\n",
            "\n",
            "Epoch 00040: val_loss improved from 0.64958 to 0.64104, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.4085 - acc: 0.8750 - val_loss: 0.6410 - val_acc: 0.7447\n",
            "Epoch 41/300\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.64104\n",
            "240/240 - 0s - loss: 0.4067 - acc: 0.9083 - val_loss: 0.6444 - val_acc: 0.7447\n",
            "Epoch 42/300\n",
            "\n",
            "Epoch 00042: val_loss improved from 0.64104 to 0.63274, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.3984 - acc: 0.8833 - val_loss: 0.6327 - val_acc: 0.7234\n",
            "Epoch 43/300\n",
            "\n",
            "Epoch 00043: val_loss improved from 0.63274 to 0.61744, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.4180 - acc: 0.8958 - val_loss: 0.6174 - val_acc: 0.7234\n",
            "Epoch 44/300\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.61744\n",
            "240/240 - 0s - loss: 0.3985 - acc: 0.8833 - val_loss: 0.6257 - val_acc: 0.7447\n",
            "Epoch 45/300\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.61744\n",
            "240/240 - 0s - loss: 0.3973 - acc: 0.9125 - val_loss: 0.6304 - val_acc: 0.7447\n",
            "Epoch 46/300\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.61744\n",
            "240/240 - 0s - loss: 0.4103 - acc: 0.8958 - val_loss: 0.6551 - val_acc: 0.7234\n",
            "Epoch 47/300\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.61744\n",
            "240/240 - 0s - loss: 0.3928 - acc: 0.9083 - val_loss: 0.6318 - val_acc: 0.7021\n",
            "Epoch 48/300\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.61744\n",
            "240/240 - 0s - loss: 0.3950 - acc: 0.8833 - val_loss: 0.6340 - val_acc: 0.7021\n",
            "Epoch 49/300\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.61744\n",
            "240/240 - 0s - loss: 0.3888 - acc: 0.9083 - val_loss: 0.6225 - val_acc: 0.7234\n",
            "Epoch 50/300\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.61744\n",
            "240/240 - 0s - loss: 0.3555 - acc: 0.9250 - val_loss: 0.6280 - val_acc: 0.7234\n",
            "Epoch 51/300\n",
            "\n",
            "Epoch 00051: val_loss improved from 0.61744 to 0.58391, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.3794 - acc: 0.9167 - val_loss: 0.5839 - val_acc: 0.7660\n",
            "Epoch 52/300\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.58391\n",
            "240/240 - 0s - loss: 0.3776 - acc: 0.9000 - val_loss: 0.6138 - val_acc: 0.7447\n",
            "Epoch 53/300\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.58391\n",
            "240/240 - 0s - loss: 0.3852 - acc: 0.9208 - val_loss: 0.6446 - val_acc: 0.7447\n",
            "Epoch 54/300\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.58391\n",
            "240/240 - 0s - loss: 0.3650 - acc: 0.9125 - val_loss: 0.6525 - val_acc: 0.8298\n",
            "Epoch 55/300\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.58391\n",
            "240/240 - 0s - loss: 0.3706 - acc: 0.9042 - val_loss: 0.6508 - val_acc: 0.8085\n",
            "Epoch 56/300\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.58391\n",
            "240/240 - 0s - loss: 0.3592 - acc: 0.9292 - val_loss: 0.5952 - val_acc: 0.7872\n",
            "Epoch 57/300\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.58391\n",
            "240/240 - 0s - loss: 0.3628 - acc: 0.9167 - val_loss: 0.5946 - val_acc: 0.7447\n",
            "Epoch 58/300\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.58391\n",
            "240/240 - 0s - loss: 0.3653 - acc: 0.8875 - val_loss: 0.6115 - val_acc: 0.7234\n",
            "Epoch 59/300\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.58391\n",
            "240/240 - 0s - loss: 0.3376 - acc: 0.9333 - val_loss: 0.6457 - val_acc: 0.7234\n",
            "Epoch 60/300\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.58391\n",
            "240/240 - 0s - loss: 0.3471 - acc: 0.9250 - val_loss: 0.6324 - val_acc: 0.7234\n",
            "Epoch 61/300\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.58391\n",
            "240/240 - 0s - loss: 0.3392 - acc: 0.9042 - val_loss: 0.5926 - val_acc: 0.7447\n",
            "Epoch 62/300\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.58391\n",
            "240/240 - 0s - loss: 0.3472 - acc: 0.9250 - val_loss: 0.5975 - val_acc: 0.7234\n",
            "Epoch 63/300\n",
            "\n",
            "Epoch 00063: val_loss improved from 0.58391 to 0.57691, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.3535 - acc: 0.9042 - val_loss: 0.5769 - val_acc: 0.7447\n",
            "Epoch 64/300\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.57691\n",
            "240/240 - 0s - loss: 0.3472 - acc: 0.9208 - val_loss: 0.5941 - val_acc: 0.7234\n",
            "Epoch 65/300\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.57691\n",
            "240/240 - 0s - loss: 0.3524 - acc: 0.9208 - val_loss: 0.5807 - val_acc: 0.7660\n",
            "Epoch 66/300\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.57691\n",
            "240/240 - 0s - loss: 0.3298 - acc: 0.9250 - val_loss: 0.5852 - val_acc: 0.7660\n",
            "Epoch 67/300\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.57691\n",
            "240/240 - 0s - loss: 0.3338 - acc: 0.9250 - val_loss: 0.6113 - val_acc: 0.7447\n",
            "Epoch 68/300\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.57691\n",
            "240/240 - 0s - loss: 0.3602 - acc: 0.8917 - val_loss: 0.5930 - val_acc: 0.7872\n",
            "Epoch 69/300\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.57691\n",
            "240/240 - 0s - loss: 0.3359 - acc: 0.9167 - val_loss: 0.6062 - val_acc: 0.7447\n",
            "Epoch 70/300\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.57691\n",
            "240/240 - 0s - loss: 0.3367 - acc: 0.9083 - val_loss: 0.5835 - val_acc: 0.7660\n",
            "Epoch 71/300\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.57691\n",
            "240/240 - 0s - loss: 0.3033 - acc: 0.9500 - val_loss: 0.6077 - val_acc: 0.7234\n",
            "Epoch 72/300\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.57691\n",
            "240/240 - 0s - loss: 0.3284 - acc: 0.9292 - val_loss: 0.6276 - val_acc: 0.7234\n",
            "Epoch 73/300\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.57691\n",
            "240/240 - 0s - loss: 0.3053 - acc: 0.9292 - val_loss: 0.6103 - val_acc: 0.7447\n",
            "Epoch 74/300\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.57691\n",
            "240/240 - 0s - loss: 0.3050 - acc: 0.9375 - val_loss: 0.6060 - val_acc: 0.7234\n",
            "Epoch 75/300\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.57691\n",
            "240/240 - 0s - loss: 0.3462 - acc: 0.9083 - val_loss: 0.5994 - val_acc: 0.7660\n",
            "Epoch 76/300\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.57691\n",
            "240/240 - 0s - loss: 0.3277 - acc: 0.9208 - val_loss: 0.5870 - val_acc: 0.7234\n",
            "Epoch 77/300\n",
            "\n",
            "Epoch 00077: val_loss improved from 0.57691 to 0.56250, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.3229 - acc: 0.9250 - val_loss: 0.5625 - val_acc: 0.7447\n",
            "Epoch 78/300\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.56250\n",
            "240/240 - 0s - loss: 0.3055 - acc: 0.9208 - val_loss: 0.5696 - val_acc: 0.7234\n",
            "Epoch 79/300\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.56250\n",
            "240/240 - 0s - loss: 0.3180 - acc: 0.9208 - val_loss: 0.5720 - val_acc: 0.7872\n",
            "Epoch 80/300\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.56250\n",
            "240/240 - 0s - loss: 0.3185 - acc: 0.9417 - val_loss: 0.6313 - val_acc: 0.7872\n",
            "Epoch 81/300\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.56250\n",
            "240/240 - 0s - loss: 0.3152 - acc: 0.9208 - val_loss: 0.5850 - val_acc: 0.7660\n",
            "Epoch 82/300\n",
            "\n",
            "Epoch 00082: val_loss improved from 0.56250 to 0.55231, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.3057 - acc: 0.9417 - val_loss: 0.5523 - val_acc: 0.8085\n",
            "Epoch 83/300\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.55231\n",
            "240/240 - 0s - loss: 0.2907 - acc: 0.9458 - val_loss: 0.6059 - val_acc: 0.8085\n",
            "Epoch 84/300\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.55231\n",
            "240/240 - 0s - loss: 0.2880 - acc: 0.9250 - val_loss: 0.5728 - val_acc: 0.7660\n",
            "Epoch 85/300\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.55231\n",
            "240/240 - 0s - loss: 0.3051 - acc: 0.9292 - val_loss: 0.5537 - val_acc: 0.7660\n",
            "Epoch 86/300\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.55231\n",
            "240/240 - 0s - loss: 0.2678 - acc: 0.9417 - val_loss: 0.5609 - val_acc: 0.7447\n",
            "Epoch 87/300\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.55231\n",
            "240/240 - 0s - loss: 0.2947 - acc: 0.9292 - val_loss: 0.5778 - val_acc: 0.7234\n",
            "Epoch 88/300\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.55231\n",
            "240/240 - 0s - loss: 0.2808 - acc: 0.9542 - val_loss: 0.6080 - val_acc: 0.7872\n",
            "Epoch 89/300\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.55231\n",
            "240/240 - 0s - loss: 0.2936 - acc: 0.9167 - val_loss: 0.5760 - val_acc: 0.7447\n",
            "Epoch 90/300\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.55231\n",
            "240/240 - 0s - loss: 0.2804 - acc: 0.9375 - val_loss: 0.6077 - val_acc: 0.7021\n",
            "Epoch 91/300\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.55231\n",
            "240/240 - 0s - loss: 0.2954 - acc: 0.9292 - val_loss: 0.5725 - val_acc: 0.7872\n",
            "Epoch 92/300\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.55231\n",
            "240/240 - 0s - loss: 0.2822 - acc: 0.9458 - val_loss: 0.5636 - val_acc: 0.7234\n",
            "Epoch 93/300\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.55231\n",
            "240/240 - 0s - loss: 0.2677 - acc: 0.9458 - val_loss: 0.5670 - val_acc: 0.7660\n",
            "Epoch 94/300\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.55231\n",
            "240/240 - 0s - loss: 0.2639 - acc: 0.9458 - val_loss: 0.6647 - val_acc: 0.7021\n",
            "Epoch 95/300\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.55231\n",
            "240/240 - 0s - loss: 0.2747 - acc: 0.9250 - val_loss: 0.5875 - val_acc: 0.8298\n",
            "Epoch 96/300\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.55231\n",
            "240/240 - 0s - loss: 0.2685 - acc: 0.9333 - val_loss: 0.6014 - val_acc: 0.7660\n",
            "Epoch 97/300\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.55231\n",
            "240/240 - 0s - loss: 0.2584 - acc: 0.9500 - val_loss: 0.5917 - val_acc: 0.7660\n",
            "Epoch 98/300\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.55231\n",
            "240/240 - 0s - loss: 0.2734 - acc: 0.9125 - val_loss: 0.5819 - val_acc: 0.7234\n",
            "Epoch 99/300\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.55231\n",
            "240/240 - 0s - loss: 0.2795 - acc: 0.9458 - val_loss: 0.6049 - val_acc: 0.7660\n",
            "Epoch 100/300\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.55231\n",
            "240/240 - 0s - loss: 0.2600 - acc: 0.9333 - val_loss: 0.6073 - val_acc: 0.7234\n",
            "Epoch 101/300\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.55231\n",
            "240/240 - 0s - loss: 0.2535 - acc: 0.9333 - val_loss: 0.5691 - val_acc: 0.7660\n",
            "Epoch 102/300\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.55231\n",
            "240/240 - 0s - loss: 0.2728 - acc: 0.9458 - val_loss: 0.5966 - val_acc: 0.7447\n",
            "Epoch 103/300\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.55231\n",
            "240/240 - 0s - loss: 0.2701 - acc: 0.9333 - val_loss: 0.5922 - val_acc: 0.7660\n",
            "Epoch 104/300\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.55231\n",
            "240/240 - 0s - loss: 0.2676 - acc: 0.9292 - val_loss: 0.5843 - val_acc: 0.8298\n",
            "Epoch 105/300\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 0.55231\n",
            "240/240 - 0s - loss: 0.2565 - acc: 0.9500 - val_loss: 0.5728 - val_acc: 0.7447\n",
            "Epoch 106/300\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 0.55231\n",
            "240/240 - 0s - loss: 0.2521 - acc: 0.9333 - val_loss: 0.6165 - val_acc: 0.7447\n",
            "Epoch 107/300\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.55231\n",
            "240/240 - 0s - loss: 0.2520 - acc: 0.9333 - val_loss: 0.6022 - val_acc: 0.7447\n",
            "Epoch 108/300\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 0.55231\n",
            "240/240 - 0s - loss: 0.2381 - acc: 0.9625 - val_loss: 0.5897 - val_acc: 0.7447\n",
            "Epoch 109/300\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.55231\n",
            "240/240 - 0s - loss: 0.2755 - acc: 0.9375 - val_loss: 0.5889 - val_acc: 0.7872\n",
            "Epoch 110/300\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.55231\n",
            "240/240 - 0s - loss: 0.2322 - acc: 0.9542 - val_loss: 0.6482 - val_acc: 0.8085\n",
            "Epoch 111/300\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 0.55231\n",
            "240/240 - 0s - loss: 0.2380 - acc: 0.9500 - val_loss: 0.5934 - val_acc: 0.7872\n",
            "Epoch 112/300\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 0.55231\n",
            "240/240 - 0s - loss: 0.2506 - acc: 0.9417 - val_loss: 0.5850 - val_acc: 0.7660\n",
            "Epoch 113/300\n",
            "\n",
            "Epoch 00113: val_loss improved from 0.55231 to 0.54223, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.2342 - acc: 0.9458 - val_loss: 0.5422 - val_acc: 0.7872\n",
            "Epoch 114/300\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.54223\n",
            "240/240 - 0s - loss: 0.2757 - acc: 0.9208 - val_loss: 0.5754 - val_acc: 0.7660\n",
            "Epoch 115/300\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.54223\n",
            "240/240 - 0s - loss: 0.2230 - acc: 0.9542 - val_loss: 0.5502 - val_acc: 0.8085\n",
            "Epoch 116/300\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.54223\n",
            "240/240 - 0s - loss: 0.2476 - acc: 0.9417 - val_loss: 0.6633 - val_acc: 0.7234\n",
            "Epoch 117/300\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.54223\n",
            "240/240 - 0s - loss: 0.2343 - acc: 0.9500 - val_loss: 0.5861 - val_acc: 0.7234\n",
            "Epoch 118/300\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.54223\n",
            "240/240 - 0s - loss: 0.2214 - acc: 0.9667 - val_loss: 0.5690 - val_acc: 0.7872\n",
            "Epoch 119/300\n",
            "\n",
            "Epoch 00119: val_loss improved from 0.54223 to 0.53954, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.2358 - acc: 0.9417 - val_loss: 0.5395 - val_acc: 0.7872\n",
            "Epoch 120/300\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.53954\n",
            "240/240 - 0s - loss: 0.2276 - acc: 0.9708 - val_loss: 0.5777 - val_acc: 0.7872\n",
            "Epoch 121/300\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.53954\n",
            "240/240 - 0s - loss: 0.2343 - acc: 0.9500 - val_loss: 0.6193 - val_acc: 0.7660\n",
            "Epoch 122/300\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.53954\n",
            "240/240 - 0s - loss: 0.2354 - acc: 0.9542 - val_loss: 0.5596 - val_acc: 0.8085\n",
            "Epoch 123/300\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 0.53954\n",
            "240/240 - 0s - loss: 0.2522 - acc: 0.9167 - val_loss: 0.6229 - val_acc: 0.7660\n",
            "Epoch 124/300\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 0.53954\n",
            "240/240 - 0s - loss: 0.2542 - acc: 0.9375 - val_loss: 0.6078 - val_acc: 0.7660\n",
            "Epoch 125/300\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 0.53954\n",
            "240/240 - 0s - loss: 0.2217 - acc: 0.9583 - val_loss: 0.5438 - val_acc: 0.7660\n",
            "Epoch 126/300\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 0.53954\n",
            "240/240 - 0s - loss: 0.2077 - acc: 0.9458 - val_loss: 0.6018 - val_acc: 0.7660\n",
            "Epoch 127/300\n",
            "\n",
            "Epoch 00127: val_loss improved from 0.53954 to 0.52060, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.1969 - acc: 0.9792 - val_loss: 0.5206 - val_acc: 0.8085\n",
            "Epoch 128/300\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 0.52060\n",
            "240/240 - 0s - loss: 0.2192 - acc: 0.9542 - val_loss: 0.5511 - val_acc: 0.7872\n",
            "Epoch 129/300\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.52060\n",
            "240/240 - 0s - loss: 0.2016 - acc: 0.9708 - val_loss: 0.5862 - val_acc: 0.7660\n",
            "Epoch 130/300\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 0.52060\n",
            "240/240 - 0s - loss: 0.2151 - acc: 0.9583 - val_loss: 0.5902 - val_acc: 0.7660\n",
            "Epoch 131/300\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 0.52060\n",
            "240/240 - 0s - loss: 0.1887 - acc: 0.9708 - val_loss: 0.5701 - val_acc: 0.7872\n",
            "Epoch 132/300\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 0.52060\n",
            "240/240 - 0s - loss: 0.2121 - acc: 0.9417 - val_loss: 0.5914 - val_acc: 0.7660\n",
            "Epoch 133/300\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.52060\n",
            "240/240 - 0s - loss: 0.1962 - acc: 0.9708 - val_loss: 0.5433 - val_acc: 0.7872\n",
            "Epoch 134/300\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.52060\n",
            "240/240 - 0s - loss: 0.2224 - acc: 0.9583 - val_loss: 0.5536 - val_acc: 0.7872\n",
            "Epoch 135/300\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 0.52060\n",
            "240/240 - 0s - loss: 0.2208 - acc: 0.9500 - val_loss: 0.5790 - val_acc: 0.7447\n",
            "Epoch 136/300\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.52060\n",
            "240/240 - 0s - loss: 0.2263 - acc: 0.9500 - val_loss: 0.5668 - val_acc: 0.7660\n",
            "Epoch 137/300\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 0.52060\n",
            "240/240 - 0s - loss: 0.2187 - acc: 0.9542 - val_loss: 0.5574 - val_acc: 0.8085\n",
            "Epoch 138/300\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.52060\n",
            "240/240 - 0s - loss: 0.1997 - acc: 0.9792 - val_loss: 0.6506 - val_acc: 0.7660\n",
            "Epoch 139/300\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.52060\n",
            "240/240 - 0s - loss: 0.2199 - acc: 0.9417 - val_loss: 0.6138 - val_acc: 0.7660\n",
            "Epoch 140/300\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 0.52060\n",
            "240/240 - 0s - loss: 0.1864 - acc: 0.9833 - val_loss: 0.5729 - val_acc: 0.7660\n",
            "Epoch 141/300\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.52060\n",
            "240/240 - 0s - loss: 0.2200 - acc: 0.9417 - val_loss: 0.5433 - val_acc: 0.7447\n",
            "Epoch 142/300\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.52060\n",
            "240/240 - 0s - loss: 0.1865 - acc: 0.9667 - val_loss: 0.5649 - val_acc: 0.7660\n",
            "Epoch 143/300\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 0.52060\n",
            "240/240 - 0s - loss: 0.2099 - acc: 0.9583 - val_loss: 0.5807 - val_acc: 0.7872\n",
            "Epoch 144/300\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 0.52060\n",
            "240/240 - 0s - loss: 0.1932 - acc: 0.9625 - val_loss: 0.5464 - val_acc: 0.7872\n",
            "Epoch 145/300\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.52060\n",
            "240/240 - 0s - loss: 0.1797 - acc: 0.9708 - val_loss: 0.5772 - val_acc: 0.7660\n",
            "Epoch 146/300\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.52060\n",
            "240/240 - 0s - loss: 0.2101 - acc: 0.9292 - val_loss: 0.6014 - val_acc: 0.7447\n",
            "Epoch 147/300\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.52060\n",
            "240/240 - 0s - loss: 0.1901 - acc: 0.9583 - val_loss: 0.6158 - val_acc: 0.8085\n",
            "Epoch 148/300\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 0.52060\n",
            "240/240 - 0s - loss: 0.2042 - acc: 0.9583 - val_loss: 0.6315 - val_acc: 0.7660\n",
            "Epoch 149/300\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.52060\n",
            "240/240 - 0s - loss: 0.1873 - acc: 0.9542 - val_loss: 0.5811 - val_acc: 0.7660\n",
            "Epoch 150/300\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.52060\n",
            "240/240 - 0s - loss: 0.1780 - acc: 0.9875 - val_loss: 0.5739 - val_acc: 0.7660\n",
            "Epoch 151/300\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.52060\n",
            "240/240 - 0s - loss: 0.1929 - acc: 0.9583 - val_loss: 0.5363 - val_acc: 0.7872\n",
            "Epoch 152/300\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.52060\n",
            "240/240 - 0s - loss: 0.1911 - acc: 0.9667 - val_loss: 0.5932 - val_acc: 0.8085\n",
            "Epoch 153/300\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 0.52060\n",
            "240/240 - 0s - loss: 0.1785 - acc: 0.9708 - val_loss: 0.5357 - val_acc: 0.7872\n",
            "Epoch 154/300\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.52060\n",
            "240/240 - 0s - loss: 0.1904 - acc: 0.9542 - val_loss: 0.6223 - val_acc: 0.7660\n",
            "Epoch 155/300\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.52060\n",
            "240/240 - 0s - loss: 0.1704 - acc: 0.9708 - val_loss: 0.6621 - val_acc: 0.8085\n",
            "Epoch 156/300\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.52060\n",
            "240/240 - 0s - loss: 0.1854 - acc: 0.9792 - val_loss: 0.5514 - val_acc: 0.7660\n",
            "Epoch 157/300\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.52060\n",
            "240/240 - 0s - loss: 0.1872 - acc: 0.9750 - val_loss: 0.5616 - val_acc: 0.7660\n",
            "Epoch 158/300\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.52060\n",
            "240/240 - 0s - loss: 0.1678 - acc: 0.9583 - val_loss: 0.5690 - val_acc: 0.7872\n",
            "Epoch 159/300\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.52060\n",
            "240/240 - 0s - loss: 0.1947 - acc: 0.9667 - val_loss: 0.5550 - val_acc: 0.7447\n",
            "Epoch 160/300\n",
            "\n",
            "Epoch 00160: val_loss improved from 0.52060 to 0.51085, saving model to /tmp/checkpoint.h5\n",
            "240/240 - 0s - loss: 0.1859 - acc: 0.9625 - val_loss: 0.5108 - val_acc: 0.8085\n",
            "Epoch 161/300\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.51085\n",
            "240/240 - 0s - loss: 0.1959 - acc: 0.9583 - val_loss: 0.5424 - val_acc: 0.7660\n",
            "Epoch 162/300\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.51085\n",
            "240/240 - 0s - loss: 0.1870 - acc: 0.9625 - val_loss: 0.5263 - val_acc: 0.7872\n",
            "Epoch 163/300\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.51085\n",
            "240/240 - 0s - loss: 0.1735 - acc: 0.9708 - val_loss: 0.5814 - val_acc: 0.7872\n",
            "Epoch 164/300\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.51085\n",
            "240/240 - 0s - loss: 0.2190 - acc: 0.9333 - val_loss: 0.6092 - val_acc: 0.8085\n",
            "Epoch 165/300\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.51085\n",
            "240/240 - 0s - loss: 0.1801 - acc: 0.9500 - val_loss: 0.6245 - val_acc: 0.7872\n",
            "Epoch 166/300\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 0.51085\n",
            "240/240 - 0s - loss: 0.1790 - acc: 0.9625 - val_loss: 0.5781 - val_acc: 0.7447\n",
            "Epoch 167/300\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.51085\n",
            "240/240 - 0s - loss: 0.1860 - acc: 0.9667 - val_loss: 0.6108 - val_acc: 0.7872\n",
            "Epoch 168/300\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.51085\n",
            "240/240 - 0s - loss: 0.1522 - acc: 0.9750 - val_loss: 0.6425 - val_acc: 0.7872\n",
            "Epoch 169/300\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 0.51085\n",
            "240/240 - 0s - loss: 0.1687 - acc: 0.9667 - val_loss: 0.5951 - val_acc: 0.8085\n",
            "Epoch 170/300\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.51085\n",
            "240/240 - 0s - loss: 0.1528 - acc: 0.9750 - val_loss: 0.6074 - val_acc: 0.8085\n",
            "Epoch 171/300\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.51085\n",
            "240/240 - 0s - loss: 0.1762 - acc: 0.9708 - val_loss: 0.6072 - val_acc: 0.7447\n",
            "Epoch 172/300\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.51085\n",
            "240/240 - 0s - loss: 0.1671 - acc: 0.9750 - val_loss: 0.6127 - val_acc: 0.7234\n",
            "Epoch 173/300\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.51085\n",
            "240/240 - 0s - loss: 0.1560 - acc: 0.9792 - val_loss: 0.6266 - val_acc: 0.7872\n",
            "Epoch 174/300\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.51085\n",
            "240/240 - 0s - loss: 0.1612 - acc: 0.9667 - val_loss: 0.5855 - val_acc: 0.7660\n",
            "Epoch 175/300\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.51085\n",
            "240/240 - 0s - loss: 0.1532 - acc: 0.9625 - val_loss: 0.5886 - val_acc: 0.7872\n",
            "Epoch 176/300\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 0.51085\n",
            "240/240 - 0s - loss: 0.1587 - acc: 0.9750 - val_loss: 0.5730 - val_acc: 0.8085\n",
            "Epoch 177/300\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.51085\n",
            "240/240 - 0s - loss: 0.1654 - acc: 0.9750 - val_loss: 0.6050 - val_acc: 0.7872\n",
            "Epoch 178/300\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.51085\n",
            "240/240 - 0s - loss: 0.1638 - acc: 0.9708 - val_loss: 0.5918 - val_acc: 0.7872\n",
            "Epoch 179/300\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.51085\n",
            "240/240 - 0s - loss: 0.1516 - acc: 0.9917 - val_loss: 0.5693 - val_acc: 0.8085\n",
            "Epoch 180/300\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.51085\n",
            "240/240 - 0s - loss: 0.1459 - acc: 0.9708 - val_loss: 0.6216 - val_acc: 0.8085\n",
            "Epoch 181/300\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.51085\n",
            "240/240 - 0s - loss: 0.1713 - acc: 0.9625 - val_loss: 0.5962 - val_acc: 0.7872\n",
            "Epoch 182/300\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.51085\n",
            "240/240 - 0s - loss: 0.1595 - acc: 0.9750 - val_loss: 0.5531 - val_acc: 0.7660\n",
            "Epoch 183/300\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.51085\n",
            "240/240 - 0s - loss: 0.1697 - acc: 0.9792 - val_loss: 0.5887 - val_acc: 0.7660\n",
            "Epoch 184/300\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 0.51085\n",
            "240/240 - 0s - loss: 0.1682 - acc: 0.9583 - val_loss: 0.6089 - val_acc: 0.8085\n",
            "Epoch 185/300\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.51085\n",
            "240/240 - 0s - loss: 0.1506 - acc: 0.9708 - val_loss: 0.5493 - val_acc: 0.8085\n",
            "Epoch 186/300\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.51085\n",
            "240/240 - 0s - loss: 0.1403 - acc: 0.9750 - val_loss: 0.5587 - val_acc: 0.7872\n",
            "Epoch 187/300\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 0.51085\n",
            "240/240 - 0s - loss: 0.1540 - acc: 0.9750 - val_loss: 0.6145 - val_acc: 0.7660\n",
            "Epoch 188/300\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.51085\n",
            "240/240 - 0s - loss: 0.1256 - acc: 0.9833 - val_loss: 0.6030 - val_acc: 0.7872\n",
            "Epoch 189/300\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.51085\n",
            "240/240 - 0s - loss: 0.1525 - acc: 0.9708 - val_loss: 0.7020 - val_acc: 0.7660\n",
            "Epoch 190/300\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.51085\n",
            "240/240 - 0s - loss: 0.1731 - acc: 0.9750 - val_loss: 0.5811 - val_acc: 0.7660\n",
            "Epoch 191/300\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.51085\n",
            "240/240 - 0s - loss: 0.1478 - acc: 0.9792 - val_loss: 0.6588 - val_acc: 0.7872\n",
            "Epoch 192/300\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 0.51085\n",
            "240/240 - 0s - loss: 0.1498 - acc: 0.9625 - val_loss: 0.6000 - val_acc: 0.8085\n",
            "Epoch 193/300\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.51085\n",
            "240/240 - 0s - loss: 0.1164 - acc: 0.9875 - val_loss: 0.6262 - val_acc: 0.7872\n",
            "Epoch 194/300\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.51085\n",
            "240/240 - 0s - loss: 0.1806 - acc: 0.9583 - val_loss: 0.6368 - val_acc: 0.7660\n",
            "Epoch 195/300\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.51085\n",
            "240/240 - 0s - loss: 0.1577 - acc: 0.9625 - val_loss: 0.6341 - val_acc: 0.7660\n",
            "Epoch 196/300\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.51085\n",
            "240/240 - 0s - loss: 0.1301 - acc: 0.9750 - val_loss: 0.6219 - val_acc: 0.7872\n",
            "Epoch 197/300\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.51085\n",
            "240/240 - 0s - loss: 0.1340 - acc: 0.9917 - val_loss: 0.6084 - val_acc: 0.7660\n",
            "Epoch 198/300\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.51085\n",
            "240/240 - 0s - loss: 0.1511 - acc: 0.9667 - val_loss: 0.5810 - val_acc: 0.7660\n",
            "Epoch 199/300\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.51085\n",
            "240/240 - 0s - loss: 0.1694 - acc: 0.9625 - val_loss: 0.5897 - val_acc: 0.7660\n",
            "Epoch 200/300\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.51085\n",
            "240/240 - 0s - loss: 0.1665 - acc: 0.9583 - val_loss: 0.5395 - val_acc: 0.7872\n",
            "Epoch 201/300\n",
            "\n",
            "Epoch 00201: val_loss did not improve from 0.51085\n",
            "240/240 - 0s - loss: 0.1634 - acc: 0.9667 - val_loss: 0.6186 - val_acc: 0.7447\n",
            "Epoch 202/300\n",
            "\n",
            "Epoch 00202: val_loss did not improve from 0.51085\n",
            "240/240 - 0s - loss: 0.1597 - acc: 0.9708 - val_loss: 0.5742 - val_acc: 0.7660\n",
            "Epoch 203/300\n",
            "\n",
            "Epoch 00203: val_loss did not improve from 0.51085\n",
            "240/240 - 0s - loss: 0.1286 - acc: 0.9750 - val_loss: 0.5871 - val_acc: 0.7660\n",
            "Epoch 204/300\n",
            "\n",
            "Epoch 00204: val_loss did not improve from 0.51085\n",
            "240/240 - 0s - loss: 0.1648 - acc: 0.9667 - val_loss: 0.5607 - val_acc: 0.7872\n",
            "Epoch 205/300\n",
            "\n",
            "Epoch 00205: val_loss did not improve from 0.51085\n",
            "240/240 - 0s - loss: 0.1495 - acc: 0.9750 - val_loss: 0.5140 - val_acc: 0.7660\n",
            "Epoch 206/300\n",
            "\n",
            "Epoch 00206: val_loss did not improve from 0.51085\n",
            "240/240 - 0s - loss: 0.1706 - acc: 0.9583 - val_loss: 0.5437 - val_acc: 0.7872\n",
            "Epoch 207/300\n",
            "\n",
            "Epoch 00207: val_loss did not improve from 0.51085\n",
            "240/240 - 0s - loss: 0.1390 - acc: 0.9792 - val_loss: 0.5665 - val_acc: 0.7872\n",
            "Epoch 208/300\n",
            "\n",
            "Epoch 00208: val_loss did not improve from 0.51085\n",
            "240/240 - 0s - loss: 0.1292 - acc: 0.9833 - val_loss: 0.5783 - val_acc: 0.8085\n",
            "Epoch 209/300\n",
            "\n",
            "Epoch 00209: val_loss did not improve from 0.51085\n",
            "240/240 - 0s - loss: 0.1416 - acc: 0.9750 - val_loss: 0.5737 - val_acc: 0.7872\n",
            "Epoch 210/300\n",
            "\n",
            "Epoch 00210: val_loss did not improve from 0.51085\n",
            "240/240 - 0s - loss: 0.1255 - acc: 0.9792 - val_loss: 0.5783 - val_acc: 0.7872\n",
            "Epoch 211/300\n",
            "\n",
            "Epoch 00211: val_loss did not improve from 0.51085\n",
            "240/240 - 0s - loss: 0.1280 - acc: 0.9750 - val_loss: 0.5771 - val_acc: 0.7872\n",
            "Epoch 212/300\n",
            "\n",
            "Epoch 00212: val_loss did not improve from 0.51085\n",
            "240/240 - 0s - loss: 0.1555 - acc: 0.9500 - val_loss: 0.5811 - val_acc: 0.7660\n",
            "Epoch 213/300\n",
            "\n",
            "Epoch 00213: val_loss did not improve from 0.51085\n",
            "240/240 - 0s - loss: 0.1694 - acc: 0.9708 - val_loss: 0.6496 - val_acc: 0.7660\n",
            "Epoch 214/300\n",
            "\n",
            "Epoch 00214: val_loss did not improve from 0.51085\n",
            "240/240 - 0s - loss: 0.1438 - acc: 0.9708 - val_loss: 0.5814 - val_acc: 0.7872\n",
            "Epoch 215/300\n",
            "\n",
            "Epoch 00215: val_loss did not improve from 0.51085\n",
            "240/240 - 0s - loss: 0.1363 - acc: 0.9792 - val_loss: 0.6003 - val_acc: 0.7447\n",
            "Epoch 216/300\n",
            "\n",
            "Epoch 00216: val_loss did not improve from 0.51085\n",
            "240/240 - 0s - loss: 0.1593 - acc: 0.9625 - val_loss: 0.5691 - val_acc: 0.7660\n",
            "Epoch 217/300\n",
            "\n",
            "Epoch 00217: val_loss did not improve from 0.51085\n",
            "240/240 - 0s - loss: 0.1428 - acc: 0.9625 - val_loss: 0.6531 - val_acc: 0.7660\n",
            "Epoch 218/300\n",
            "\n",
            "Epoch 00218: val_loss did not improve from 0.51085\n",
            "240/240 - 0s - loss: 0.1591 - acc: 0.9708 - val_loss: 0.5806 - val_acc: 0.7872\n",
            "Epoch 219/300\n",
            "\n",
            "Epoch 00219: val_loss did not improve from 0.51085\n",
            "240/240 - 0s - loss: 0.1435 - acc: 0.9750 - val_loss: 0.5697 - val_acc: 0.7660\n",
            "Epoch 220/300\n",
            "\n",
            "Epoch 00220: val_loss did not improve from 0.51085\n",
            "240/240 - 0s - loss: 0.1276 - acc: 0.9708 - val_loss: 0.5856 - val_acc: 0.7872\n",
            "Epoch 221/300\n",
            "\n",
            "Epoch 00221: val_loss did not improve from 0.51085\n",
            "240/240 - 0s - loss: 0.1313 - acc: 0.9833 - val_loss: 0.6663 - val_acc: 0.7660\n",
            "Epoch 222/300\n",
            "\n",
            "Epoch 00222: val_loss did not improve from 0.51085\n",
            "240/240 - 0s - loss: 0.1545 - acc: 0.9583 - val_loss: 0.6580 - val_acc: 0.7660\n",
            "Epoch 223/300\n",
            "\n",
            "Epoch 00223: val_loss did not improve from 0.51085\n",
            "240/240 - 0s - loss: 0.1584 - acc: 0.9625 - val_loss: 0.6188 - val_acc: 0.7872\n",
            "Epoch 224/300\n",
            "\n",
            "Epoch 00224: val_loss did not improve from 0.51085\n",
            "240/240 - 0s - loss: 0.1428 - acc: 0.9750 - val_loss: 0.6755 - val_acc: 0.7660\n",
            "Epoch 225/300\n",
            "\n",
            "Epoch 00225: val_loss did not improve from 0.51085\n",
            "240/240 - 0s - loss: 0.1405 - acc: 0.9708 - val_loss: 0.6353 - val_acc: 0.8085\n",
            "Epoch 226/300\n",
            "\n",
            "Epoch 00226: val_loss did not improve from 0.51085\n",
            "240/240 - 0s - loss: 0.1228 - acc: 0.9917 - val_loss: 0.6802 - val_acc: 0.7660\n",
            "Epoch 227/300\n",
            "\n",
            "Epoch 00227: val_loss did not improve from 0.51085\n",
            "240/240 - 0s - loss: 0.1163 - acc: 0.9792 - val_loss: 0.6782 - val_acc: 0.7872\n",
            "Epoch 228/300\n",
            "\n",
            "Epoch 00228: val_loss did not improve from 0.51085\n",
            "240/240 - 0s - loss: 0.1347 - acc: 0.9875 - val_loss: 0.6116 - val_acc: 0.7872\n",
            "Epoch 229/300\n",
            "\n",
            "Epoch 00229: val_loss did not improve from 0.51085\n",
            "240/240 - 0s - loss: 0.1355 - acc: 0.9792 - val_loss: 0.6671 - val_acc: 0.7872\n",
            "Epoch 230/300\n",
            "\n",
            "Epoch 00230: val_loss did not improve from 0.51085\n",
            "240/240 - 0s - loss: 0.1494 - acc: 0.9625 - val_loss: 0.6263 - val_acc: 0.7660\n",
            "Epoch 231/300\n",
            "\n",
            "Epoch 00231: val_loss did not improve from 0.51085\n",
            "240/240 - 0s - loss: 0.1318 - acc: 0.9792 - val_loss: 0.5343 - val_acc: 0.8085\n",
            "Epoch 232/300\n",
            "\n",
            "Epoch 00232: val_loss did not improve from 0.51085\n",
            "240/240 - 0s - loss: 0.1284 - acc: 0.9833 - val_loss: 0.6597 - val_acc: 0.7660\n",
            "Epoch 233/300\n",
            "\n",
            "Epoch 00233: val_loss did not improve from 0.51085\n",
            "240/240 - 0s - loss: 0.1332 - acc: 0.9833 - val_loss: 0.6570 - val_acc: 0.7660\n",
            "Epoch 234/300\n",
            "\n",
            "Epoch 00234: val_loss did not improve from 0.51085\n",
            "240/240 - 0s - loss: 0.1018 - acc: 0.9917 - val_loss: 0.5848 - val_acc: 0.7872\n",
            "Epoch 235/300\n",
            "\n",
            "Epoch 00235: val_loss did not improve from 0.51085\n",
            "240/240 - 0s - loss: 0.1327 - acc: 0.9625 - val_loss: 0.5765 - val_acc: 0.7872\n",
            "Epoch 236/300\n",
            "\n",
            "Epoch 00236: val_loss did not improve from 0.51085\n",
            "240/240 - 0s - loss: 0.1117 - acc: 0.9875 - val_loss: 0.6015 - val_acc: 0.7234\n",
            "Epoch 237/300\n",
            "\n",
            "Epoch 00237: val_loss did not improve from 0.51085\n",
            "240/240 - 0s - loss: 0.1290 - acc: 0.9833 - val_loss: 0.5581 - val_acc: 0.7872\n",
            "Epoch 238/300\n",
            "\n",
            "Epoch 00238: val_loss did not improve from 0.51085\n",
            "240/240 - 0s - loss: 0.1307 - acc: 0.9708 - val_loss: 0.6920 - val_acc: 0.7447\n",
            "Epoch 239/300\n",
            "\n",
            "Epoch 00239: val_loss did not improve from 0.51085\n",
            "240/240 - 0s - loss: 0.1312 - acc: 0.9792 - val_loss: 0.6276 - val_acc: 0.7447\n",
            "Epoch 240/300\n",
            "\n",
            "Epoch 00240: val_loss did not improve from 0.51085\n",
            "240/240 - 0s - loss: 0.1471 - acc: 0.9667 - val_loss: 0.6838 - val_acc: 0.7872\n",
            "Epoch 241/300\n",
            "\n",
            "Epoch 00241: val_loss did not improve from 0.51085\n",
            "240/240 - 0s - loss: 0.1230 - acc: 0.9833 - val_loss: 0.6570 - val_acc: 0.7660\n",
            "Epoch 242/300\n",
            "\n",
            "Epoch 00242: val_loss did not improve from 0.51085\n",
            "240/240 - 0s - loss: 0.1467 - acc: 0.9625 - val_loss: 0.5720 - val_acc: 0.7872\n",
            "Epoch 243/300\n",
            "\n",
            "Epoch 00243: val_loss did not improve from 0.51085\n",
            "240/240 - 0s - loss: 0.1172 - acc: 0.9875 - val_loss: 0.6169 - val_acc: 0.8085\n",
            "Epoch 244/300\n",
            "\n",
            "Epoch 00244: val_loss did not improve from 0.51085\n",
            "240/240 - 0s - loss: 0.1318 - acc: 0.9833 - val_loss: 0.5817 - val_acc: 0.7234\n",
            "Epoch 245/300\n",
            "\n",
            "Epoch 00245: val_loss did not improve from 0.51085\n",
            "240/240 - 0s - loss: 0.1330 - acc: 0.9792 - val_loss: 0.5267 - val_acc: 0.7234\n",
            "Epoch 246/300\n",
            "\n",
            "Epoch 00246: val_loss did not improve from 0.51085\n",
            "240/240 - 0s - loss: 0.1293 - acc: 0.9833 - val_loss: 0.5720 - val_acc: 0.7872\n",
            "Epoch 247/300\n",
            "\n",
            "Epoch 00247: val_loss did not improve from 0.51085\n",
            "240/240 - 0s - loss: 0.1043 - acc: 0.9958 - val_loss: 0.6327 - val_acc: 0.7447\n",
            "Epoch 248/300\n",
            "\n",
            "Epoch 00248: val_loss did not improve from 0.51085\n",
            "240/240 - 0s - loss: 0.1171 - acc: 0.9792 - val_loss: 0.6456 - val_acc: 0.7447\n",
            "Epoch 249/300\n",
            "\n",
            "Epoch 00249: val_loss did not improve from 0.51085\n",
            "240/240 - 0s - loss: 0.1241 - acc: 0.9792 - val_loss: 0.7425 - val_acc: 0.7234\n",
            "Epoch 250/300\n",
            "\n",
            "Epoch 00250: val_loss did not improve from 0.51085\n",
            "240/240 - 0s - loss: 0.1159 - acc: 0.9833 - val_loss: 0.6087 - val_acc: 0.7660\n",
            "Epoch 251/300\n",
            "\n",
            "Epoch 00251: val_loss did not improve from 0.51085\n",
            "240/240 - 0s - loss: 0.1153 - acc: 0.9792 - val_loss: 0.6929 - val_acc: 0.7872\n",
            "Epoch 252/300\n",
            "\n",
            "Epoch 00252: val_loss did not improve from 0.51085\n",
            "240/240 - 0s - loss: 0.1206 - acc: 0.9750 - val_loss: 0.6110 - val_acc: 0.7872\n",
            "Epoch 253/300\n",
            "\n",
            "Epoch 00253: val_loss did not improve from 0.51085\n",
            "240/240 - 0s - loss: 0.1175 - acc: 0.9833 - val_loss: 0.5981 - val_acc: 0.7872\n",
            "Epoch 254/300\n",
            "\n",
            "Epoch 00254: val_loss did not improve from 0.51085\n",
            "240/240 - 0s - loss: 0.0987 - acc: 0.9875 - val_loss: 0.6194 - val_acc: 0.7872\n",
            "Epoch 255/300\n",
            "\n",
            "Epoch 00255: val_loss did not improve from 0.51085\n",
            "240/240 - 0s - loss: 0.1150 - acc: 0.9792 - val_loss: 0.6163 - val_acc: 0.7447\n",
            "Epoch 256/300\n",
            "\n",
            "Epoch 00256: val_loss did not improve from 0.51085\n",
            "240/240 - 0s - loss: 0.1182 - acc: 0.9917 - val_loss: 0.6673 - val_acc: 0.7660\n",
            "Epoch 257/300\n",
            "\n",
            "Epoch 00257: val_loss did not improve from 0.51085\n",
            "240/240 - 0s - loss: 0.1248 - acc: 0.9833 - val_loss: 0.5847 - val_acc: 0.7660\n",
            "Epoch 258/300\n",
            "\n",
            "Epoch 00258: val_loss did not improve from 0.51085\n",
            "240/240 - 0s - loss: 0.1193 - acc: 0.9708 - val_loss: 0.6269 - val_acc: 0.7660\n",
            "Epoch 259/300\n",
            "\n",
            "Epoch 00259: val_loss did not improve from 0.51085\n",
            "240/240 - 0s - loss: 0.1026 - acc: 0.9958 - val_loss: 0.6115 - val_acc: 0.7872\n",
            "Epoch 260/300\n",
            "\n",
            "Epoch 00260: val_loss did not improve from 0.51085\n",
            "240/240 - 0s - loss: 0.1643 - acc: 0.9542 - val_loss: 0.7052 - val_acc: 0.8085\n",
            "Epoch 261/300\n",
            "\n",
            "Epoch 00261: val_loss did not improve from 0.51085\n",
            "240/240 - 0s - loss: 0.1307 - acc: 0.9833 - val_loss: 0.5721 - val_acc: 0.7447\n",
            "Epoch 262/300\n",
            "\n",
            "Epoch 00262: val_loss did not improve from 0.51085\n",
            "240/240 - 0s - loss: 0.1516 - acc: 0.9792 - val_loss: 0.6337 - val_acc: 0.7447\n",
            "Epoch 263/300\n",
            "\n",
            "Epoch 00263: val_loss did not improve from 0.51085\n",
            "240/240 - 0s - loss: 0.1312 - acc: 0.9708 - val_loss: 0.6407 - val_acc: 0.7872\n",
            "Epoch 264/300\n",
            "\n",
            "Epoch 00264: val_loss did not improve from 0.51085\n",
            "240/240 - 0s - loss: 0.1290 - acc: 0.9792 - val_loss: 0.5956 - val_acc: 0.7660\n",
            "Epoch 265/300\n",
            "\n",
            "Epoch 00265: val_loss did not improve from 0.51085\n",
            "240/240 - 0s - loss: 0.1065 - acc: 0.9917 - val_loss: 0.6144 - val_acc: 0.7872\n",
            "Epoch 266/300\n",
            "\n",
            "Epoch 00266: val_loss did not improve from 0.51085\n",
            "240/240 - 0s - loss: 0.1044 - acc: 0.9833 - val_loss: 0.5740 - val_acc: 0.7234\n",
            "Epoch 267/300\n",
            "\n",
            "Epoch 00267: val_loss did not improve from 0.51085\n",
            "240/240 - 0s - loss: 0.1268 - acc: 0.9708 - val_loss: 0.5850 - val_acc: 0.7660\n",
            "Epoch 268/300\n",
            "\n",
            "Epoch 00268: val_loss did not improve from 0.51085\n",
            "240/240 - 0s - loss: 0.1371 - acc: 0.9750 - val_loss: 0.5819 - val_acc: 0.7447\n",
            "Epoch 269/300\n",
            "\n",
            "Epoch 00269: val_loss did not improve from 0.51085\n",
            "240/240 - 0s - loss: 0.1104 - acc: 0.9875 - val_loss: 0.6084 - val_acc: 0.7021\n",
            "Epoch 270/300\n",
            "\n",
            "Epoch 00270: val_loss did not improve from 0.51085\n",
            "240/240 - 0s - loss: 0.1161 - acc: 0.9792 - val_loss: 0.6693 - val_acc: 0.7447\n",
            "Epoch 271/300\n",
            "\n",
            "Epoch 00271: val_loss did not improve from 0.51085\n",
            "240/240 - 0s - loss: 0.1329 - acc: 0.9792 - val_loss: 0.6947 - val_acc: 0.7234\n",
            "Epoch 272/300\n",
            "\n",
            "Epoch 00272: val_loss did not improve from 0.51085\n",
            "240/240 - 0s - loss: 0.1224 - acc: 0.9708 - val_loss: 0.7542 - val_acc: 0.8085\n",
            "Epoch 273/300\n",
            "\n",
            "Epoch 00273: val_loss did not improve from 0.51085\n",
            "240/240 - 0s - loss: 0.1000 - acc: 0.9875 - val_loss: 0.6151 - val_acc: 0.7872\n",
            "Epoch 274/300\n",
            "\n",
            "Epoch 00274: val_loss did not improve from 0.51085\n",
            "240/240 - 0s - loss: 0.1021 - acc: 0.9833 - val_loss: 0.7258 - val_acc: 0.7660\n",
            "Epoch 275/300\n",
            "\n",
            "Epoch 00275: val_loss did not improve from 0.51085\n",
            "240/240 - 0s - loss: 0.1141 - acc: 0.9750 - val_loss: 0.6243 - val_acc: 0.7872\n",
            "Epoch 276/300\n",
            "\n",
            "Epoch 00276: val_loss did not improve from 0.51085\n",
            "240/240 - 0s - loss: 0.0968 - acc: 0.9792 - val_loss: 0.6158 - val_acc: 0.7660\n",
            "Epoch 277/300\n",
            "\n",
            "Epoch 00277: val_loss did not improve from 0.51085\n",
            "240/240 - 0s - loss: 0.0954 - acc: 0.9875 - val_loss: 0.5984 - val_acc: 0.8085\n",
            "Epoch 278/300\n",
            "\n",
            "Epoch 00278: val_loss did not improve from 0.51085\n",
            "240/240 - 0s - loss: 0.0984 - acc: 0.9917 - val_loss: 0.5873 - val_acc: 0.7872\n",
            "Epoch 279/300\n",
            "\n",
            "Epoch 00279: val_loss did not improve from 0.51085\n",
            "240/240 - 0s - loss: 0.1180 - acc: 0.9875 - val_loss: 0.6065 - val_acc: 0.7872\n",
            "Epoch 280/300\n",
            "\n",
            "Epoch 00280: val_loss did not improve from 0.51085\n",
            "240/240 - 0s - loss: 0.1325 - acc: 0.9667 - val_loss: 0.7430 - val_acc: 0.7872\n",
            "Epoch 281/300\n",
            "\n",
            "Epoch 00281: val_loss did not improve from 0.51085\n",
            "240/240 - 0s - loss: 0.1471 - acc: 0.9542 - val_loss: 0.6756 - val_acc: 0.7234\n",
            "Epoch 282/300\n",
            "\n",
            "Epoch 00282: val_loss did not improve from 0.51085\n",
            "240/240 - 0s - loss: 0.1040 - acc: 0.9917 - val_loss: 0.5767 - val_acc: 0.7447\n",
            "Epoch 283/300\n",
            "\n",
            "Epoch 00283: val_loss did not improve from 0.51085\n",
            "240/240 - 0s - loss: 0.1073 - acc: 0.9875 - val_loss: 0.6418 - val_acc: 0.7872\n",
            "Epoch 284/300\n",
            "\n",
            "Epoch 00284: val_loss did not improve from 0.51085\n",
            "240/240 - 0s - loss: 0.1008 - acc: 0.9958 - val_loss: 0.6853 - val_acc: 0.7660\n",
            "Epoch 285/300\n",
            "\n",
            "Epoch 00285: val_loss did not improve from 0.51085\n",
            "240/240 - 0s - loss: 0.1026 - acc: 0.9875 - val_loss: 0.6510 - val_acc: 0.7660\n",
            "Epoch 286/300\n",
            "\n",
            "Epoch 00286: val_loss did not improve from 0.51085\n",
            "240/240 - 0s - loss: 0.1084 - acc: 0.9875 - val_loss: 0.6165 - val_acc: 0.7660\n",
            "Epoch 287/300\n",
            "\n",
            "Epoch 00287: val_loss did not improve from 0.51085\n",
            "240/240 - 0s - loss: 0.1057 - acc: 0.9875 - val_loss: 0.6510 - val_acc: 0.7872\n",
            "Epoch 288/300\n",
            "\n",
            "Epoch 00288: val_loss did not improve from 0.51085\n",
            "240/240 - 0s - loss: 0.0972 - acc: 0.9833 - val_loss: 0.7395 - val_acc: 0.7660\n",
            "Epoch 289/300\n",
            "\n",
            "Epoch 00289: val_loss did not improve from 0.51085\n",
            "240/240 - 0s - loss: 0.1054 - acc: 0.9875 - val_loss: 0.6770 - val_acc: 0.7447\n",
            "Epoch 290/300\n",
            "\n",
            "Epoch 00290: val_loss did not improve from 0.51085\n",
            "240/240 - 0s - loss: 0.1084 - acc: 0.9833 - val_loss: 0.5982 - val_acc: 0.7872\n",
            "Epoch 291/300\n",
            "\n",
            "Epoch 00291: val_loss did not improve from 0.51085\n",
            "240/240 - 0s - loss: 0.1324 - acc: 0.9667 - val_loss: 0.7933 - val_acc: 0.7872\n",
            "Epoch 292/300\n",
            "\n",
            "Epoch 00292: val_loss did not improve from 0.51085\n",
            "240/240 - 0s - loss: 0.1190 - acc: 0.9708 - val_loss: 0.6653 - val_acc: 0.7872\n",
            "Epoch 293/300\n",
            "\n",
            "Epoch 00293: val_loss did not improve from 0.51085\n",
            "240/240 - 0s - loss: 0.1081 - acc: 0.9875 - val_loss: 0.6852 - val_acc: 0.7872\n",
            "Epoch 294/300\n",
            "\n",
            "Epoch 00294: val_loss did not improve from 0.51085\n",
            "240/240 - 0s - loss: 0.0972 - acc: 0.9833 - val_loss: 0.6249 - val_acc: 0.7447\n",
            "Epoch 295/300\n",
            "\n",
            "Epoch 00295: val_loss did not improve from 0.51085\n",
            "240/240 - 0s - loss: 0.1123 - acc: 0.9750 - val_loss: 0.6710 - val_acc: 0.8085\n",
            "Epoch 296/300\n",
            "\n",
            "Epoch 00296: val_loss did not improve from 0.51085\n",
            "240/240 - 0s - loss: 0.1125 - acc: 0.9708 - val_loss: 0.7550 - val_acc: 0.7660\n",
            "Epoch 297/300\n",
            "\n",
            "Epoch 00297: val_loss did not improve from 0.51085\n",
            "240/240 - 0s - loss: 0.1105 - acc: 0.9792 - val_loss: 0.7421 - val_acc: 0.7234\n",
            "Epoch 298/300\n",
            "\n",
            "Epoch 00298: val_loss did not improve from 0.51085\n",
            "240/240 - 0s - loss: 0.0957 - acc: 0.9875 - val_loss: 0.6506 - val_acc: 0.7660\n",
            "Epoch 299/300\n",
            "\n",
            "Epoch 00299: val_loss did not improve from 0.51085\n",
            "240/240 - 0s - loss: 0.1146 - acc: 0.9708 - val_loss: 0.7710 - val_acc: 0.7660\n",
            "Epoch 300/300\n",
            "\n",
            "Epoch 00300: val_loss did not improve from 0.51085\n",
            "240/240 - 0s - loss: 0.1126 - acc: 0.9833 - val_loss: 0.6777 - val_acc: 0.7872\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n3P8D0x6TvSGtfysAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "# Evaluate on test data\n",
            "288/288 [==============================] - 1s 2ms/sample - loss: 0.6833 - acc: 0.7743\n",
            "test loss, test acc: [0.6832529088754471, 0.7743056]\n",
            "[[0.50939753 0.78819442]\n",
            " [1.11725642 0.59722221]\n",
            " [0.51194876 0.83680558]\n",
            " [1.00790531 0.55902779]\n",
            " [2.13269825 0.41319445]\n",
            " [1.1196956  0.59027779]\n",
            " [0.45791262 0.8576389 ]\n",
            " [0.5408015  0.78819442]\n",
            " [0.68325291 0.77430558]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AST-KRZ7WXEX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "df_accl_all = pd.DataFrame({'Loss ': 100*acc_all[:, 1], 'Eval Acc': 100*acc_all[:, 1]})\n",
        "df_accl_all.to_csv (r'df_accl_8_12_Hz4_Class.csv', index = None, header=True) #Don't forget to add '.csv' at the end of the path"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}